instruction,input,output,Categories
return coefficients from pipeline object in sklearn,"I've fit a Pipeline object with RandomizedSearchCV
pipe_sgd = Pipeline([('scl', StandardScaler()),
                    ('clf', SGDClassifier(n_jobs=-1))])

param_dist_sgd = {'clf__loss': ['log'],
                 'clf__penalty': [None, 'l1', 'l2', 'elasticnet'],
                 'clf__alpha': np.linspace(0.15, 0.35),
                 'clf__n_iter': [3, 5, 7]}

sgd_randomized_pipe = RandomizedSearchCV(estimator = pipe_sgd, 
                                         param_distributions=param_dist_sgd, 
                                         cv=3, n_iter=30, n_jobs=-1)

sgd_randomized_pipe.fit(X_train, y_train)

I want to access the coef_ attribute of the best_estimator_ but I'm unable to do that. I've tried accessing coef_ with the code below.
sgd_randomized_pipe.best_estimator_.coef_
However I get the following AttributeError... 
AttributeError: 'Pipeline' object has no attribute 'coef_'
The scikit-learn docs say that coef_ is an attribute of SGDClassifier, which is the class of my base_estimator_. 
What am I doing wrong?
","You can always use the names you assigned to them while making the pipeline by using the named_steps dict.
scaler = sgd_randomized_pipe.best_estimator_.named_steps['scl']
classifier = sgd_randomized_pipe.best_estimator_.named_steps['clf']

and then access all the attributes like coef_, intercept_ etc. which are available to corresponding fitted estimator.
This is the formal attribute exposed by the Pipeline as specified in the documentation:

named_steps : dict
Read-only attribute to access any step parameter by user given name. Keys are step names and values are steps parameters.

",cross-validation
setting rgb value for a numpy array using boolean indexing,"I have an array with shape (100, 80, 3) which is an rgb image.
I have a boolean mask with shape (100, 80).
I want each pixel where the mask is True to have value of pix_val = np.array([0.1, 0.2, 0.3]).
cols = 100
rows = 80
img = np.random.rand(rows, cols, 3)
mask = np.random.randint(2, size=(rows, cols), dtype=np.bool_)
px = np.array([0.1, 0.2, 0.3])

for ch in range(3):
  img[:, :, ch][mask] = px[ch]

I thought broadcasting:
img[mask[:, :, None]] = px

would work. But it did not.
I am looking for a vectorized (efficient) way to implement it.
","I'll attempt to explain why your indexing attempt didn't work.
Make a smaller 3d array, and 2d mask:
In [1]: import numpy as np

In [2]: img = np.arange(24).reshape(2,3,4)

In [3]: mask = np.array([[1,0,1],[0,1,1]],bool);mask
Out[3]: 
array([[ True, False,  True],
       [False,  True,  True]])

Using @mozway's indexing, produces a (4,4) array.  The first first 4 is the number of True values in the mask, the second is the trailing dimension:
In [4]: img[mask]
Out[4]: 
array([[ 0,  1,  2,  3],
       [ 8,  9, 10, 11],
       [16, 17, 18, 19],
       [20, 21, 22, 23]])

With your indexing attempt, we get an error.  (You really should have shown the error message):
In [5]: img[mask[:,:,None]]
---------------------------------------------------------------------------
IndexError                                Traceback (most recent call last)
Cell In[5], line 1
----> 1 img[mask[:,:,None]]

IndexError: boolean index did not match indexed array 
along dimension 2; dimension is 4 but corresponding 
boolean dimension is 1

With this None, mask dimension is (2,3,1).  That last 1 doesn't match the 4 of img.  broadcasting doesn't apply in this context.
Now if we attempt to use mask in a multiplication, the (2,3,4) and (2,3) don't broadcast together:
In [6]: img*mask
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
Cell In[6], line 1
----> 1 img*mask

ValueError: operands could not be broadcast together 
with shapes (2,3,4) (2,3) 

But (2,3,1) does broadcast with (2,3,4), producing a select number of 0 rows:
In [7]: img*mask[:,:,None]
Out[7]: 
array([[[ 0,  1,  2,  3],
        [ 0,  0,  0,  0],
        [ 8,  9, 10, 11]],

       [[ 0,  0,  0,  0],
        [16, 17, 18, 19],
        [20, 21, 22, 23]]])

As I commented, using a boolean mask is equivalent to indexing with nonzero arrays:
In [13]: I,J = np.nonzero(mask); I,J
Out[13]: (array([0, 0, 1, 1], dtype=int64), array([0, 2, 1, 2], dtype=int64))

In [14]: img[I,J,:]
Out[14]: 
array([[ 0,  1,  2,  3],
       [ 8,  9, 10, 11],
       [16, 17, 18, 19],
       [20, 21, 22, 23]])

In the assignment expresion, a size (4,) value can broadcast to the (n,4) indexed img[mask].  Now if we were attempting to mask other dimensions we might need to make a px[:,None,:] or something like that.
",numpy
rag using langchain  chroma  unable to save more than 99 records to database,"I'm using the following code to load the content of markdown files (only one file, in my case), split it into chunks and then embed and store the chunks one by one. My file is split into 801 chunks. However, this code is unable to save the embeddings to disk in the vector db.
def load_documents():
loader = DirectoryLoader(DATA_PATH, glob=""*.md"")

documents = loader.load()

return documents

def split_text(documents: list[Document]):
text_splitter = RecursiveCharacterTextSplitter(

    chunk_size=300,

    chunk_overlap=100,

    length_function=len,

    add_start_index=True,

)

chunks = text_splitter.split_documents(documents)

print(f""Split {len(documents)} documents into {len(chunks)} chunks."")



document = chunks[10]

print(document.page_content)

print(document.metadata)



return chunks

def save_to_chroma(chunks: list[Document]):
# Clear out the database first.

if os.path.exists(CHROMA_PATH):

    shutil.rmtree(CHROMA_PATH)



# Create a new DB from the documents.

db = Chroma.from_documents(

    chunks, OpenAIEmbeddings(), persist_directory=CHROMA_PATH

)

While analysing this problem, I attempted to save the chunks one by one instead, using a for loop:
for i, chunk in enumerate(chunks):
db = Chroma.from_documents(
chunks, OpenAIEmbeddings(), persist_directory=CHROMA_PATH
)
I found that the code does save up to 99 chunks / embeddings, but always crashes when it tries to save further data. To investigate further, I opened the underlying database using DB Browser for SQLite and saw that Chroma was saving a max of 99 records in the 'embeddings' table. However, I was able to manually add more records to it i.e. exceeding 99.
I also tried a few variations i.e. by:

saving the chunks in batches of 10 or 30
not deleting the existing database already containing 99 records, and then attempting to write more data
playing around with the chunk size and chunk overlap

However, none of the above made any difference.
Does anybody know why this is happening and how to solve this problem?
","My problem was solved when I re-installed python on my pc due to some other problem. Now, when I run the code, it works like magic! All the chunks are being saved now regardless of whether I save them all in one go or in batches.
EDIT:
The problem was actually due to a conflict in the installed libraries. It gets resolved when you use a virtual environment, so no need to re-install python.
",langchain
how do i chopslicetrim off last character in string using javascript,"I have a string, 12345.00, and I would like it to return 12345.0.
I have looked at trim, but it looks like it is only trimming whitespace and slice which I don't see how this would work. Any suggestions?
","You can use the substring function:


let str = ""12345.00"";
str = str.substring(0, str.length - 1);
console.log(str);



This is the accepted answer, but as per the conversations below, the slice syntax is much clearer:


let str = ""12345.00"";
str = str.slice(0, -1); 
console.log(str);



Neither method mutates
",javascript
how to do a scatter plot with empty circles in python,"In Python, with Matplotlib, how can a scatter plot with empty circles be plotted?  The goal is to draw empty circles around some of the colored disks already plotted by scatter(), so as to highlight them, ideally without having to redraw the colored circles.
I tried facecolors=None, to no avail.
","From the documentation for scatter:
Optional kwargs control the Collection properties; in particular:

    edgecolors:
        The string ‘none’ to plot faces with no outlines
    facecolors:
        The string ‘none’ to plot unfilled outlines

Try the following:
import matplotlib.pyplot as plt 
import numpy as np 

x = np.random.randn(60) 
y = np.random.randn(60)

plt.scatter(x, y, s=80, facecolors='none', edgecolors='r')
plt.show()


Note: For other types of plots see this post on the use of markeredgecolor and markerfacecolor.
",matplotlib
speeding up load time of llms,"I am currently only able to play around with a V100 on GCP. I understand that I can load a LLM in 4bit quantization as shown below. However, (assuming due to the quantization) it is taking up to 10 minutes to load this model.
Is there a way to speed up this loading process?

I see that there is GGUF file format which may help in this regard (although I am not sure why/ how).
Would doing torch.compile somehow help me load the model next time in a fast manner. My hypothesis being that when compiled, I can save the resulting model in a binary format that can load faster?
Should I be baking the loaded model into the docker image somehow to speed this up? The downside being due to cuda the docker image is already at 4GB.

import torch
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig

device = ""cuda"" # the device to load the model onto
model_id = ""mistralai/Mistral-7B-Instruct-v0.2""
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type=""nf4"",
    bnb_4bit_compute_dtype=torch.bfloat16
)

model = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=bnb_config, device_map=""auto"")

","You havent provided much details about you docker setup. But yes every time you run this docker you will have to download files, until you build your own image which will copy model files into docker, then you can use cache_dir parameter in from_pretrained to point to location of your model.
I am able to load llama3 8b into Tesla M40 in few seconds.
",huggingface-transformers
error  failed to create temp directory quotcusersuserappdatalocaltempcondaltrandomgtquot,"When I try to Activate ""conda activate tensorflow_cpu""
conda activate tensorflow_cpu

Error : Failed to create temp directory ""C:\Users\user\AppData\Local\Temp\conda-\""
","It is due to a bug from conda developers. The bug is the temp path is having names with spaces, so to overcome please reassign the Env Variables TEMP, TMP.
(for windows)

go to environment variables
In ""User Variables for "" section look for TEMP, TMP
double click on TMP and in ""variable value"", type ""C:\conda_tmp""
similarly do it for TEMP
close env variables section
Restart the anaconda prompt, the error should vanish

",tensorflow
how to extract number from a string in javascript,"I have an element in javascript like follows:
 <span>280ms</span>

I want to extract 280 from the span element. How can I do it? The content within the span element will be any number followed by ms.
","parseInt() is pretty sweet.
HTML
<span id=""foo"">280ms</span>

JS
var text = $('#foo').text();
var number = parseInt(text, 10);
alert(number);

parseInt() will process any string as a number and stop when it reaches a non-numeric character.  In this case the m in 280ms.  After have found the digits 2, 8, and 0, evaluates those digits as base 10 (that second argument) and returns the number value 280.  Note this is an actual number and not a string.
Edit: 
@Alex Wayne's comment. 
Just filter out the non numeric characters first.
parseInt('ms120'.replace(/[^0-9\.]/g, ''), 10);

",javascript
how to add total amount from list of lists using the java stream api,"I have a class:
public class Customer{
private List<Order> orders;
private String name;
...
}

public class Order{
private long orderId;
private String customerId;
private double amount;
...
}


public class CustomerService{

@Autowired
private CustomerRepository customerRepo;

List<Customer> custList = customerRepo.findAllCustomers();

double amount = custList.stream().map(c -> {
return c.getOrders().stream().mapToDouble(o -> o.getAmount()).sum()}).findAny().get();

}

So I tried as like above in CustomerService. But the amount returned is only for the first item in the list. So in the same way how can I write to get the total amount of all the orders for that customer?
Any suggestions/input will be helpful.
Note: Sorry for any compile errors. I cannot copy my code and so I need to write in the question instead.
","You use flatMap():
double total = custList.stream()
        .flatMap(c -> c.getOrders().stream())
        .mapToDouble(Order::getAmount)
        .sum();

Also note the use of method reference to get the amount from the order.
",java
how can i get a value from a cell of a dataframe,"I have constructed a condition that extracts exactly one row from my dataframe:
d2 = df[(df['l_ext']==l_ext) & (df['item']==item) & (df['wn']==wn) & (df['wd']==1)]

Now I would like to take a value from a particular column:
val = d2['col_name']

But as a result, I get a dataframe that contains one row and one column (i.e., one cell). It is not what I need. I need one value (one float number). How can I do it in pandas?
","If you have a DataFrame with only one row, then access the first (only) row as a Series using iloc, and then the value using the column name:
In [3]: sub_df
Out[3]:
          A         B
2 -0.133653 -0.030854

In [4]: sub_df.iloc[0]
Out[4]:
A   -0.133653
B   -0.030854
Name: 2, dtype: float64

In [5]: sub_df.iloc[0]['A']
Out[5]: -0.13365288513107493

",pandas
how to merge two data frames based on particular column in pandas python,"I have to merge two dataframes:
df1
company,standard
tata,A1
cts,A2
dell,A3

df2
company,return
tata,71
dell,78
cts,27
hcl,23

I have to unify both dataframes to one dataframe. I need output like:
company,standard,return
tata,A1,71
cts,A2,27
dell,A3,78

","Use merge:
print (pd.merge(df1, df2, on='company'))

Sample:
print (df1)
  company standard
0    tata       A1
1     cts       A2
2    dell       A3

print (df2)
  company  return
0    tata      71
1    dell      78
2     cts      27
3     hcl      23

print (pd.merge(df1, df2, on='company'))
  company standard  return
0    tata       A1      71
1     cts       A2      27
2    dell       A3      78

",pandas
python generating a list of dates between two dates,"I want to generate a list of dates between two dates and store them in a list in string format. This list is useful to compare with other dates I have. 
My code is given below: 
from datetime import date, timedelta

sdate = date(2019,3,22)   # start date
edate = date(2019,4,9)   # end date

def dates_bwn_twodates(start_date, end_date):
    for n in range(int ((end_date - start_date).days)):
        yield start_date + timedelta(n)
print(dates_bwn_twodates(sdate,edate))

My present output: 
<generator object dates_bwn_twodates at 0x000002A8E7929410>

My expected output: 
['2019-03-22',.....,'2019-04-08']

Something wrong in my code. 
","You can use pandas.date_range() for this:
import pandas
pandas.date_range(sdate,edate-timedelta(days=1),freq='d')


DatetimeIndex(['2019-03-22', '2019-03-23', '2019-03-24', '2019-03-25',
           '2019-03-26', '2019-03-27', '2019-03-28', '2019-03-29',
           '2019-03-30', '2019-03-31', '2019-04-01', '2019-04-02',
           '2019-04-03', '2019-04-04', '2019-04-05', '2019-04-06',
           '2019-04-07', '2019-04-08'],
          dtype='datetime64[ns]', freq='D')

",pandas
can39t change row text in docx file once row is added to table,"I have the problem with the following code:
XWPFTable table = <get table somehow>;
CTRow firstRow = table.getRow(0).getCtRow();

for (int i = 0; i < data.getRowCount(); i++) {
    CTRow ctRow = (CTRow) firstRow.copy();
    XWPFTableRow row = new XWPFTableRow(ctRow, table);
    XWPFRun[] cellRuns = row.getTableCells()
            .stream()
            .map(c -> c.getParagraphs().get(0))
            .map(p -> p.getRuns().isEmpty() ? p.createRun() : p.getRuns().get(0))
            .toArray(XWPFRun[]::new);
    for (int j = 0; j < cellRuns.length; j++) {
        cellRuns[j].setText(data.getValueAt(i, j).toString(), 0);
    }
    table.addRow(row);
}


table.getRow(1).getTableCells()
.get(0).getParagraphs()
.get(0).getRuns()
.get(0).setText(""FooBar"", 0); //change text in some added row

This code is copying the first row of the table several times and then copying values from data. Works perfectly fine (except text style) except the last operator, which was supposed to change the text in some added table row. Also, the ""FooBar"" string doesn't even appear in document.xml of created WORD document. I failed to see any clues from debug, because it seems, that table.addRow(row); operator just copies row pointer to it's internal list of rows. Also, I didn't have problems with altering already existing rows. So do you have any ideas why this could happen?
","To reproducing the problem do having a source.docx having a first table having at least two rows.
Then do running following code:
import java.io.FileInputStream;
import java.io.FileOutputStream;
import org.apache.poi.xwpf.usermodel.*;

import org.openxmlformats.schemas.wordprocessingml.x2006.main.CTRow;

public class WordInsertTableRow {

 static XWPFTableRow insertNewTableRow(XWPFTableRow sourceTableRow, int pos) throws Exception {
  XWPFTable table = sourceTableRow.getTable();
  CTRow newCTRrow = CTRow.Factory.parse(sourceTableRow.getCtRow().newInputStream());
  XWPFTableRow tableRow = new XWPFTableRow(newCTRrow, table);
  table.addRow(tableRow, pos);
  return tableRow;
 }

 static void commitTableRows(XWPFTable table) {
  int rowNr = 0;
  for (XWPFTableRow tableRow : table.getRows()) {
   table.getCTTbl().setTrArray(rowNr++, tableRow.getCtRow());
  }
 }

 public static void main(String[] args) throws Exception {

  XWPFDocument doc = new XWPFDocument(new FileInputStream(""source.docx""));
  boolean weMustCommitTableRows = false;

  XWPFTable table = doc.getTableArray(0);

  // insert new row, which is a copy of row 2, as new row 3:
  XWPFTableRow sourceTableRow = table.getRow(1);
  XWPFTableRow newRow3 = insertNewTableRow(sourceTableRow, 2);

  // now changing something in that new row:
  int i = 1;
  for (XWPFTableCell cell : newRow3.getTableCells()) {
   for (XWPFParagraph paragraph : cell.getParagraphs()) {
    for (XWPFRun run : paragraph.getRuns()) {
     run.setText(""New row 3 run "" + i++, 0);
    }
   }
  }
System.out.println(newRow3.getCtRow()); // was changed
System.out.println(table.getRow(2).getCtRow()); // even this is changed
System.out.println(table.getCTTbl().getTrArray(2)); // but this was not changed, why not?
  weMustCommitTableRows = true;

  if (weMustCommitTableRows) commitTableRows(table); // now it is changed

  FileOutputStream out = new FileOutputStream(""result.docx"");
  doc.write(out);
  out.close();
  doc.close();

 }
}

This code creates a copy of second row and inserts it as third row in the table. Then it does changing something in that new third row.
The issue ist, that the changings do appearing in low level CTRow of the row itself but do not appearing in low Level CTTbl of the table. For me this is not logically and I cannot get the reason of that. It looks as if the new CTRow elements are not part of the CTTbl at all. But they were added to it using ctTbl.setTrArray in XWPFTable.addRow. So I suspect there is something wrong with setTrArray in org.openxmlformats.schemas.wordprocessingml.x2006.main.CTTbl. It seems updating the XML correctly but losing the object relations in the array  (or list) of CTRows in CTTbl. But this is very hard to determining because of the kind of programming the org.openxmlformats.schemas classes. At least I was not able to do so. Maybe another of the professional and enthusiast programmers here may be able?
I am using the same approach for inserting rows having tthe same styling as a given source row. But after I have done this, I am setting boolean weMustCommitTableRows = true; and then I am doing if (weMustCommitTableRows) commitTableRows(table); before writing out the document. Then all changings will be committed.
",java
identifying dead code in large code repository,"I have a large C code base, with >100 binaries, >3000 files and > 30 libraries. There is a lot of dead code that was accumulated and I'm looking for ways to identify and remove that code. The code is simple - no complex macros and (very little) automatically generated code (lex/bison/...).
To identify ""static"" dead code (and variables) gcc does a good job (using -Wunused-* options identifies all unused static variables, static functions, ...). My challenge is with non-static global functions and variables (and the code base has lot of them!)
I've lot of mileage using 'nm' across all the objects files, practically create a list of all defined global symbols (types 'T', 'D' and 'B' for code, data and uninitialized). I then removed every 'U' symbols. That process identified all unreferenced global. At this point, I have to manually make each symbol static, compile with gcc -Werror -Wunused, and see if it raises any error.
# Omitting some details for brevity.
nm --undefined-only lib1.a lib2.a ... obj1 obj2.o obj3.o | sort > refs.txt
nm --extern-only --defined-only lib1.a lib2.a ... obj1 obj2.o obj3.o  | sort > defs.txt
join -12 -23 -v2 refs.txt defs.txt

My question - is it possible to use ""nm"" (or other object analysis tool like objdump) to identify which global symbols in object file are also used inside the same object. This will speed up the dead code elimination by separating dead code in global function from global functions that are actually used (but may become static).
Alternatively, is there any other existing tool that will do the job?
","I suggest to use GNU ld's dead symbol removal functionality for this.
For this you need to compile your code with -fdata-sections -ffunction-sections and then link with -Wl,--gc-sections -Wl,--print-gc-sections flags. It will print information about functions which have been removed.
Here is an example for sample program
/usr/bin/ld: removing unused section '.text.foo' in file '/tmp/ccXZWJ2X.o'

(.text.foo is section generated for unused function foo).
As a side note, if you use these options there may be no need to manually sanitize your codebase (apart from making it cleaner) because the toolchain will remove dead code automatically.
",c
prompting for bluetooth permission on big sur 1101,"Upon upgrading to Big Sur (macOS 11.0.1), our app is now asking the user to grant permission to use Bluetooth. However, we are not running any bluetooth code to my knowledge. We do link to CoreBluetooth, but it's been like that for a long time, and this prompt has not appeared.
When running in Xcode 12.2 it does crash with this message:
2020-11-13 13:21:58.685610-0800 Fuze[31049:200367] [access] This app has crashed because it attempted to access privacy-sensitive data without a usage description.  The app's Info.plist must contain an NSBluetoothAlwaysUsageDescription key with a string value explaining to the user how the app uses this data.

But to my knowledge, NSBluetoothAlwaysUsageDescription is only applicable for iOS devices.
I suspect this is something new with Big Sur, because the System Preferences UI for Security & Privacy -> Privacy, did not have a Bluetooth section in Catalina, and it does in Bug Sur. But the Apple documentation does not make mention of this key being applicable for macOS.
https://developer.apple.com/documentation/bundleresources/information_property_list/nsbluetoothalwaysusagedescription
So my questions are:

Is this key now applicable for macOS and we should have it, even though the documentation says it's only for other platforms, or is there a different key for bluetooth for macOS? I haven't found one.
Is there a way to control this prompting or not, because we aren't actually using the bluetooth code. We need the framework for a particular feature, but it's not applicable for many users, so it's confusing to ask permission for it right when the app launches.

Thank you!
","I'm not confident about this, but I've been running into some Bluetooth on Big Sur strangeness today as well. In my case, I run a Bluetooth peripheral from the Terminal, and I was also recently prompted to provide the Terminal with Bluetooth permission (which I'd never seen before).
For testing purposes, you can try the ""Privacy — Bluetooth Peripheral Usage Description"" or ""Privacy - Bluetooth Always Usage Description"" - and see whether that resolves the crash (I assume it would). It's strange, as I didn't think either of these were required in the past - but it looks like Big Sur is definitely cracking down on permissions.
When you go to ""Signing and Capabilities"", if there is an option to enable ""Hardware -> Bluetooth"" in the ""App Sandbox"" - I would try that out too (you may also need to enable Location, but I can't recall). I'm curious if modifying that flag would change anything regarding permissions.
When you link to CoreBluetooth, are you using the API at all? Because the permission check should happen on the first API call. But, Bluetooth is weird, so it might kick in as soon as the app launches...
In thinking about it, I've actually never optionally used Bluetooth in an app - it's always initialized from app start, so I can't say when the permission check occurs.
",prompt
how does one create a pandas column binary based on whether or not another column contains dates,"I am analyzing customer churn. My dataset contains years of customers that have stayed or left. I need to create a ['CHURN_FLAG'] column based on whether or not there is a date in the ['CHURN_DATE'] column. If there is a date in the ['CHURN_DATE'] column then the customer churned.
Current data frame:




CHURNdate




2023-1-1


NaT




Desired:




CHURNdate
CHURNflag




2023-1-1
1


NaT
0




I created a column ['TODAY_DATE'] and have attempted to solve by assessing if the ['CHURNdate'] < ['TODAY_DATE'] then the binary 1 would populate, else 0. Here is the code:
df2['CHURNflag'] = np.where(df2['CHURNdate']<df2['TODAY_DATE'], 0, 1)

Naturally, it didn't work. :( The datatypes are datetime64
","
churn['CHURNflag'] = np.where(churn['CHURNdate'].isna(), 0, 1)

Out[24]: 
    CHURNdate  CHURNflag
0  2023-01-01          1
1         NaN          0

",data-science
draw a circle with periodic boundary conditions matplotlib,"I am doing a project that involves lattices. A point of coordinates (x0, y0) is chosen randomly and I need to color blue all the points that are in the circle of center (x0, y0) and radius R and red all the other points and then draw a circle around.
The tricky part is that there is periodic boundary conditions, meaning that if my circle is near the left border then I need to draw the rest of it on the right side, the same goes for up and down.
Here is my code that plots the lattice, I have managed to color the points depending on whether or not they are in the circle but I am yet to draw the circle.
from matplotlib import pyplot as plt
import numpy as np

class lattice:

    def __init__(self, L):
        self.L = L
        self.positions = np.array([[[i, j] for i in range(L)] for j in range(L)])
    
    def draw_lattice(self, filename):
        X = self.positions[:, :, 0].flatten()
        Y = self.positions[:, :, 1].flatten()
        plt.scatter(X, Y, s=10)
        plt.xticks([])
        plt.yticks([])
        plt.title(""Lattice"")
        plt.savefig(filename)
    
    def dist_centre(self):
        x0, y0 = np.random.randint(0, self.L), np.random.randint(0, self.L)
        self.c0 = (x0, y0)
        self.distance = np.zeros((self.L, self.L))

        for i in range(self.L):
            for j in range(self.L):
                x = self.positions[i, j, 0]
                y = self.positions[i, j, 1]
                # Distance with periodic boundary conditions.
                Dx = -self.L/2 + ((x0-x)+self.L/2)%self.L
                Dy = -self.L/2 + ((y0-y)+self.L/2)%self.L
                dist = np.sqrt(Dx**2 + Dy**2)
                self.distance[i, j] = dist

    def draw_zone(self, filename, R):
        colormap = np.where(self.distance <= R, ""blue"", ""red"").flatten()

        X = self.positions[:, :, 0].flatten()
        Y = self.positions[:, :, 1].flatten()
        plt.clf()
        plt.scatter(X, Y, s=10, color=colormap)
        plt.xticks([])
        plt.yticks([])
        plt.title(""Lattice"")
        plt.savefig(filename)

if __name__ == ""__main__"":
    L = 10
    R = 3
    filename = ""test.pdf""
    latt = lattice(L)
    latt.draw_lattice(filename)
    latt.dist_centre()
    latt.draw_zone(filename, R)

The formula for the distance is modified because of the periodic boundary conditions.

","The comment from Tino_D gave me the answer. I imagined a bigged lattice, my lattice and 8 lattices surrounding it and drew a total of 9 circles with a centers that were translated to another sub lattice and then I restricted my plot to the original lattice.
    def draw_zone(self, filename, R):
        colormap = np.where(self.distance <= R, ""blue"", ""red"").flatten()

        X = self.positions[:, :, 0].flatten()
        Y = self.positions[:, :, 1].flatten()

        x0, y0 = self.c0
        centers = [(x0-L, y0-L), (x0-L, y0), (x0-L, y0+L),
                   (x0, y0-L), (x0, y0), (x0, y0+L),
                   (x0+L, y0-L), (x0+L, y0), (x0+L, y0+L)]
        
        plt.clf()

        for (x,y) in centers:
            circle = plt.Circle((x, y), R, alpha=0.2, color=""grey"")
            plt.gca().add_patch(circle)

        plt.scatter(X, Y, s=10, color=colormap)
        plt.xticks([])
        plt.yticks([])
        plt.xlim(0, self.L-1)
        plt.ylim(0, self.L-1)
        plt.title(""Lattice"")
        plt.savefig(filename)


",matplotlib
how to increase the space between the subplots and the figure,"I'm using a python code to plot 3D surface. However, the z-axis label get cut by the figure. Here is the code :
import matplotlib.pyplot as plt
import numpy as np

fig = plt.figure(figsize=(12, 10), facecolor='lightblue')
x = np.linspace(0, 10)
y = np.linspace(0, 10)
X, Y = np.meshgrid(x, y)

for idx in range(4):
    Z = np.cos(X) - np.sin(np.pi*idx/4 * Y)
    ax3D =  fig.add_subplot(2, 2, idx+1, projection='3d')
    ax3D.plot_surface(X, Y, Z, cmap=""viridis"")
    ax3D.set_zlabel(""Title"")

plt.show()

The result : 3D surface plots
Is it a possible to include the axis label in the axe ? Or to increase the space subplots and the figure ?
I have tried different options such as : plt.subplots_adjust(left=0, bottom=0, right=0.8, top=0.7, wspace=0.5, hspace=0.2) or fig.tight_layout(); but nothing seems to resolve my problem.
","One solution is to zoom out to decrease the size of each subplot (set_box_aspect).
One can also play with the three angles that defines the view: elevation, azimuth, and roll (view_init).
fig = plt.figure(figsize=(12/2, 10/2), facecolor='lightblue')
x = np.linspace(0, 10)
y = np.linspace(0, 10)
X, Y = np.meshgrid(x, y)

for idx in range(4):
    Z = np.cos(X) - np.sin(np.pi*idx/4 * Y)
    ax3D =  fig.add_subplot(2, 2, idx+1, projection='3d')
    ax3D.view_init(elev=30, azim=70, roll=0)  
    ax3D.set_box_aspect(aspect=(1,1,1), zoom=0.8)
    ax3D.plot_surface(X, Y, Z, cmap=""viridis"")
    ax3D.set_zlabel(""Title"")
fig.tight_layout()
plt.show()


",matplotlib
is there a module or something similar for interactive prompts in powershell,"Is there something we can use in PowerShell to ask users to select one item from an array of items? For example, I like how Inquirer.js can do it. 

I have also seen PoshGui, but it seems too much work to create just a simple prompt.
The reason we want something similar is that we need to provide deployment scripts for our clients and make deployment guides as easy as possible. Asking users to select one item on a screen is much better than asking them to insert some guid to a config file.
Do you have any suggestions for user prompts for arrays?
","All of the answers are correct, but I also wrote a few reusable PowerShell helper functions. Readme. I auto-generate basic looking WinForms. Looks ugly, but works.
https://github.com/Zerg00s/powershell-forms
$selectedItem = Get-FormArrayItem (Get-ChildItem)


$Delete = Get-FormBinaryAnswer ""Delete file?""


$newFileName = Get-FormStringInput ""Enter new file name"" -defaultValue ""My new file""


# -------------------------------------------------------------------------------
# Prepare the list of inputs that user needs to populate using an interactive form    
# -------------------------------------------------------------------------------
$preDeployInputs = @{
    suffix                       = """"
    SPSiteUrl                    = ""https://ENTER_SHAREPOINT_SITE.sharepoint.com""
    TimeZone                     = ""Central Standard Time""
    sendGridRegistrationEmail    = ""ENTER_VALID_EMAIL_ADDRESS""
    sendGridRegistrationPassword = $sendGridPassword
    sendGridRegistrationCompany  = ""Contoso & Tailspin""
    sendGridRegistrationWebsite  = ""https://www.company.com""
    fromEmail                    = ""no-reply@DOMAIN.COM""
}

$preDeployInputs = Get-FormItemProperties -item $preDeployInputs -dialogTitle ""Fill these required fields""


",prompt
pandas create crossvalidation based on specific columns,"I have a dataframe of few hundreds rows , that can be grouped to ids as follows:
df = Val1 Val2 Val3 Id
      2     2   8    b
      1     2   3    a
      5     7   8    z
      5     1   4    a
      0     9   0    c
      3     1   3    b
      2     7   5    z
      7     2   8    c
      6     5   5    d
...
      5     1   8    a
      4     9   0    z
      1     8   2    z

I want to use GridSearchCV , but with a custom CV that will assure that all the rows from the same ID will always be on the same set. 
So either all the rows if a are in the test set , or all of them are in the train set - and so for all the different IDs.
I want to have 5 folds - so 80% of the ids will go to the train and 20% to the test.
I understand that it can't guarentee that all folds will have the exact same amount of rows - since one ID might have more rows than the other.
What is the best way to do so?
","As stated, you can provide cv with an iterator. You can use GroupShuffleSplit(). For example, once you use it to split your dataset, you can put the result within GridSearchCV() for the cv parameter.
",cross-validation
in nodejs how can i get a password from input without the user39s keypresses being echoed to the console,"I'm trying to add a simple setup prompt to my program, where it prompts for some basic details before starting the app for the first time. One of these inputs I need is a password. When I read this password, I want to ensure that (much like when you enter a password for sudo) it doesn't appear in the user's terminal as they type.
How can I do so within NodeJS? I'm looking for something I can drop into the following code.
console.log('Failed to find config file');
// This echoes to the console - how can I do the same thing without the echo?
const password = prompt('Enter a password: ');

I'm happy to install a dependency from NPM if required.
","You can use the prompt-sync library to prompt for passwords.
import createPrompt from 'prompt-sync';
const prompt = createPrompt({});

// The prompt object's hide method gives a prompt where the input 
// is hidden
const password = prompt.hide('Enter a password: ');

",prompt
call 39npm init39 via execsync in nodejs application and automatically enter required console input,"I'm writing a Node application that automatically spins up new templated Node applications (with various configurations), including executing the 'npm init' step, but this is failing because npm init requires multiple follow-up inputs from the user.
This is what I'm executing so far, and it never completes:
execSync('cd ' + targetFolder + '; npm init;');

Is it possible for me to provide an array of Strings (empty or otherwise) to sequentially be input automatically when prompted?
","Thanks @jonrsharpe. Just executing 'npm init --yes' achieves what I wanted.
",prompt
value error y contains previously unseen labels 8 9,"I have a dataset, that I use label_encoding for data preprocessing before feed into the model.
I first label_encoding all of my data, then split the data into training and testing.
And I want to use ""label_encoder.inverse_transform"" to reverse the code, but it has this error message:
y contains previously unseen labels: [8 9]

The error message was clear, but that's the part that I am confused, because I label-encoded all of the data before splitting.
I've checked the other post: Getting ValueError: y contains new labels when using scikit learn's LabelEncoder, but that post was about the author had new dataset as testing data, mine on the otherhand, I did label_encoding the whole dataset and then split. So the encoder should have already seen all of my data, including x_test.
from sklearn.preprocessing import LabelEncoder
label_encoder = LabelEncoder()
category =[]
data_encoded = data_cleaned
for i in data_cleaned.columns:
    if data_cleaned[i].dtype == 'category':
        category.append(i)
        data_encoded[i] = label_encoder.fit_transform(data_cleaned[i])

Split the training and testing data
target_variable = ['target']
data_encoded = data_encoded.dropna(subset=target_variable)
x = data_encoded.drop(target_variable, axis=1)
y = data_encoded[target_variable]

And try to reverse the encoded label
//only want to reverse the category column
for i in category:
    X_test[i] = label_encoder.inverse_transform(X_test[i])
    print(X_test[i])

If anyone could point some direction that would be great!
","The issue is that in your loop, the label_encoder keep getting fit_transform to a new column, in which it relearns the new encoding. So when you do inverse_transform, the label_encoder only has the information from the last column it encoded. You will need to create a LE for each column and can use it to inverse_transform for each column.
Here is what I would suggest you to do:
# List of Label Encoder for each categorical feature
le = [LabelEncoder() for i in range(len(category))]
data_encoded = data_cleaned
for i in range(len(category)): 
    feat = cat_feat[i]
    data_encoded[feat] = le[i].fit_transform(data_cleaned[feat])

# After splitting:
for i in range(len(category)): 
    feat = category[i]
    X_test[feat] = le[i].inverse_transform(X_test[feat])
    print(X_test[feat])

",scikit-learn
how to implement tfgather_nd in pytorch with the argument batch_dims,"I have been doing a project on image matching, so I need to find correspondences between 2 images. To get descriptors, I will need a interpolate function. However, when I read about a equivalent function which is done in Tensorflow, I still don’t get how to implement tf.gather_nd(parmas, indices, barch_dims) in Pytorch. Especially when there is a argument: batch_dims. I have gone through stackoverflow and there is no perfect equivalence yet.
The referred interpolate function in Tensorflow is below and I have been trying to implement this in Pytorch Arguments' information is below:
inputs is a dense feature map[i] from a for loop of batch size, which means it is 3D[H, W, C](in pytorch is [C, H, W])
pos is a set of random point coordinate shapes like [[i, j], [i, j],...,[i, j]], so it is 2D when it goes in interpolate function(in pytorch is [[i,i,...,i], [j,j,...,j]])
and it then expands both of their dimensions when they get into this function
I just want a perfect implement of tf.gather_nd with argument batch_dims. Thank you!
And here's a simple example of using it:
pos = tf.ones((12, 2)) ## stands for a set of coordinates [[i, i,…, i], [j, j,…, j]]
inputs = tf.ones((4, 4, 128)) ## stands for [H, W, C] of dense feature map
outputs = interpolate(pos, inputs, batched=False)
print(outputs.get_shape()) # We get (12, 128) here

interpolate function (tf version):
def interpolate(pos, inputs, nd=True):

    pos = tf.expand_dims(pos, 0)
    inputs = tf.expand_dims(inputs, 0)

    h = tf.shape(inputs)[1]
    w = tf.shape(inputs)[2]

    i = pos[:, :, 0]
    j = pos[:, :, 1]

    i_top_left = tf.clip_by_value(tf.cast(tf.math.floor(i), tf.int32), 0, h - 1)
    j_top_left = tf.clip_by_value(tf.cast(tf.math.floor(j), tf.int32), 0, w - 1)

    i_top_right = tf.clip_by_value(tf.cast(tf.math.floor(i), tf.int32), 0, h - 1)
    j_top_right = tf.clip_by_value(tf.cast(tf.math.ceil(j), tf.int32), 0, w - 1)

    i_bottom_left = tf.clip_by_value(tf.cast(tf.math.ceil(i), tf.int32), 0, h - 1)
    j_bottom_left = tf.clip_by_value(tf.cast(tf.math.floor(j), tf.int32), 0, w - 1)

    i_bottom_right = tf.clip_by_value(tf.cast(tf.math.ceil(i), tf.int32), 0, h - 1)
    j_bottom_right = tf.clip_by_value(tf.cast(tf.math.ceil(j), tf.int32), 0, w - 1)

    dist_i_top_left = i - tf.cast(i_top_left, tf.float32)
    dist_j_top_left = j - tf.cast(j_top_left, tf.float32)
    w_top_left = (1 - dist_i_top_left) * (1 - dist_j_top_left)
    w_top_right = (1 - dist_i_top_left) * dist_j_top_left
    w_bottom_left = dist_i_top_left * (1 - dist_j_top_left)
    w_bottom_right = dist_i_top_left * dist_j_top_left

    if nd:
        w_top_left = w_top_left[..., None]
        w_top_right = w_top_right[..., None]
        w_bottom_left = w_bottom_left[..., None]
        w_bottom_right = w_bottom_right[..., None]

    interpolated_val = (
        w_top_left * tf.gather_nd(inputs, tf.stack([i_top_left, j_top_left], axis=-1), batch_dims=1) +
        w_top_right * tf.gather_nd(inputs, tf.stack([i_top_right, j_top_right], axis=-1), batch_dims=1) +
        w_bottom_left * tf.gather_nd(inputs, tf.stack([i_bottom_left, j_bottom_left], axis=-1), batch_dims=1) +
        w_bottom_right * tf.gather_nd(inputs, tf.stack([i_bottom_right, j_bottom_right], axis=-1), batch_dims=1)
    )

    interpolated_val = tf.squeeze(interpolated_val, axis=0)
    return interpolated_val

","As far as I'm aware there is no directly equivalent of tf.gather_nd in PyTorch and implementing a generic version with batch_dims is not that simple. However, you likely don't need a generic version, and given the context of your interpolate function, a version for [C, H, W] would suffice.
At the beginning of interpolate you add a singular dimension to the front, which is the batch dimension. Setting batch_dims=1 in tf.gather_nd means there is one batch dimension at the beginning, therefore it applies it per batch, i.e. it indexes inputs[0] with pos[0] etc. There is no benefit of adding a singular batch dimension, because you could have just used the direct computation.
# Adding singular batch dimension
# Shape: [1, num_pos, 2]
pos = tf.expand_dims(pos, 0)
# Shape: [1, H, W, C]
inputs = tf.expand_dims(inputs, 0)


batched_result = tf.gather_nd(inputs, pos, batch_dims=1)
single_result = tf.gater_nd(inputs[0], pos[0])

# The first element in the batched result is the same as the single result
# Hence there is no benefit to adding a singular batch dimension.
tf.reduce_all(batched_result[0] == single_result) # => True

Single version
In PyTorch the implementation for [H, W, C] can be done with Python's indexing. While PyTorch usually uses [C, H, W] for images, it's only a matter of what dimension to index, but let's keep them the same as in TensorFlow for the sake of comparison. If you were to index them manually, you would do it as such: inputs[pos_h[0], pos_w[0]],  inputs[pos_h[1], pos_w[1]] and so on. PyTorch allows you to do that automatically by providing the indices as lists: inputs[pos_h, pos_w], where pos_h and pos_w have the same length. All you need to do is split your pos into two separate tensors, one for the indices along the height dimension and the other along the width dimension, which you also did in the TensorFlow version.
inputs = torch.randn(4, 4, 128)
# Random positions 0-3, shape: [12, 2]
pos = torch.randint(4, (12, 2))

# Positions split by dimension
pos_h = pos[:, 0]
pos_w = pos[:, 1]

# Index the inputs with the indices per dimension
gathered = inputs[pos_h, pos_w]

# Verify that it's identical to TensorFlow's output
inputs_tf = tf.convert_to_tensor(inputs.numpy())
pos_tf = tf.convert_to_tensor(pos.numpy())
gathered_tf = tf.gather_nd(inputs_tf, pos_tf)
gathered_tf = torch.from_numpy(gathered_tf.numpy())

torch.equal(gathered_tf, gathered) # => True

If you want to apply it to a tensor of size [C, H, W] instead, you only need to change the dimensions you want to index:
# For [H, W, C]
gathered = inputs[pos_h, pos_w]

# For [C, H, W]
gathered = inputs[:, pos_h, pos_w]

Batched version
Making it a batched batched version (for [N, H, W, C] or [N, C, H, W]) is not that difficult, and using that is more appropriate, since you're dealing with batches anyway. The only tricky part is that each element in the batch should only be applied to the corresponding batch. For this the batch dimensions needs to be enumerated, which can be done with torch.arange. The batch enumeration is just the list with the batch indices, which will be combined with the pos_h and pos_w indices, resulting in inputs[0, pos_h[0, 0], pos_h[0, 0]], inputs[0, pos_h[0, 1], pos_h[0, 1]] ... inputs[1, pos_h[1, 0], pos_h[1, 0]] etc.
batch_size = 3
inputs = torch.randn(batch_size, 4, 4, 128)
# Random positions 0-3, different for each batch, shape: [3, 12, 2]
pos = torch.randint(4, (batch_size, 12, 2))

# Positions split by dimension
pos_h = pos[:, :, 0]
pos_w = pos[:, :, 1]

batch_enumeration = torch.arange(batch_size) # => [0, 1, 2]
# pos_h and pos_w have shape [3, 12], so the batch enumeration needs to be
# repeated 12 times per batch.
# Unsqueeze to get shape [3, 1], now the 1 could be repeated to 12, but
# broadcasting will do that automatically.
batch_enumeration = batch_enumeration.unsqueeze(1)
# Index the inputs with the indices per dimension
gathered = inputs[batch_enumeration, pos_h, pos_w]

# Again, verify that it's identical to TensorFlow's output
inputs_tf = tf.convert_to_tensor(inputs.numpy())
pos_tf = tf.convert_to_tensor(pos.numpy())
# This time with batch_dims=1
gathered_tf = tf.gather_nd(inputs_tf, pos_tf, batch_dims=1)
gathered_tf = torch.from_numpy(gathered_tf.numpy())

torch.equal(gathered_tf, gathered) # => True

Again, for [N, C, H, W], only the dimensions that are indexed need to be changed:
# For [N, H, W, C]
gathered = inputs[batch_enumeration, pos_h, pos_w]

# For [N, C, H, W]
gathered = inputs[batch_enumeration, :, pos_h, pos_w]


Just a little side note on the interpolate implementation, rounding the positions (floor and ceil respectively) doesn't make sense, because indices must be integers, so it has no effect, as long as your positions are actual indices. That also results in i_top_left and i_bottom_left being the same value, but even if they are to be rounded differently, they are always 1 position apart. Furthermore, i_top_left and i_top_right are literally the same. I don't think that this function produces a meaningful output. I don't know what you're trying to achieve, but if you're looking for image interpolation you could have a look at torch.nn.functional.interpolate.
",pytorch
403 error when trying to web scrape online job sites using python,"I am trying to web scrape online job offering web sites for my coursera project.
I keep getting a 403 error which, after I searched for its meaning online, I found out that it means that the web site has anti-web scraping protection.
Does anyone any countermeasure for this ?
PS: I have tried web scraping on indeed and weworkremotely web sites with the same error after executing my code.
Here's my code:
import requests
from bs4 import BeautifulSoup
import pandas as pd

url = 'https://weworkremotely.com/remote-jobs'

# We Work Remotely website blocks traffic from non-browsers, so we add extra parameters
headers = {
    'User-Agent': 'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:128.0) Gecko/20100101 Firefox/128.0'
}

# Send a request to the website and get the HTML content
response = requests.get(url, headers=headers)

# Check if the request was successful
if response.status_code == 200:
    soup = BeautifulSoup(response.content, 'html.parser')

    # Creating empty lists to store the data
    job_titles = []
    companies = []
    locations = []
    job_links = []

    job_sections = soup.find_all('section', class_='jobs')

    for section in job_sections:
        jobs = section.find_all('li', class_='feature')  # Ensure this class matches the site's HTML

        for job in jobs:
            # Job title
            title_tag = job.find('span', class_='title')
            title = title_tag.text.strip() if title_tag else 'N/A'
            job_titles.append(title)

            # Company name
            company_tag = job.find('span', class_='company')
            company = company_tag.text.strip() if company_tag else 'N/A'
            companies.append(company)

            # Location
            location_tag = job.find('span', class_='region company')
            location = location_tag.text.strip() if location_tag else 'Remote'
            locations.append(location)

            # Job link
            job_link_tag = job.find('a', href=True)
            job_link = 'https://weworkremotely.com' + job_link_tag['href'] if job_link_tag else 'N/A'
            job_links.append(job_link)

    # Create a DataFrame using the extracted data
    job_data = pd.DataFrame({
        'Job Title': job_titles,
        'Company': companies,
        'Location': locations,
        'Job Link': job_links
    })

    # Save the data to a CSV file
    job_data.to_csv('we_work_remotely_jobs.csv', index=False)
    print(""Job listings have been successfully saved to we_work_remotely_jobs.csv"")
else:
    print(f""Failed to retrieve the webpage. Status code: {response.status_code}"")


","To address a 403 Forbidden error when web scraping:

Use a Valid User-Agent: Set a common User-Agent header to mimic a browser.

Use Proxies: Rotate IP addresses using proxies to avoid IP blocking.

Respect robots.txt: Check and follow the website's scraping rules.

Add Delays: Introduce delays between requests to mimic human behavior.

Handle JavaScript: Use tools like Selenium for websites with JavaScript-rendered content.


Source: ScrapingBee - How to Handle a 403 Forbidden Error in Web Scraping
",python
assets folder in android studio unit test,"I have a Gradle project with the following structure:
project/
    src/
        androidTest/
            java/
        main/
            java/
            res/
            AndroidManifest.xml
    build.gradle

Now I want to add a unit test which uses a resource (either ""raw"" or ""asset""). 
I put my resource into project/androidTest/assets/test_file and access it with getContext().getResources().getAssets().open(""test_file""); (in an AndroidTestCase).
However, this gives me a FileNotFoundException. How can I fix this?
","It looks like you're trying to create an instrumented unit test, since you want to create it in the androidTest folder.
You can use one of these two options in your test to get the context:

Context ctx = InstrumentationRegistry.getTargetContext();
this one will give you your app's context. You can use it to grab assets that are in src/main/assets for example.

Context ctx = InstrumentationRegistry.getContext();
this one will give you the test app's context. You can use it to grab assets that are in src/androidTest/assets


If you want to know more about assets in unit testing you can read this post. In this github file you have an example.
Deprecation Note: As pointed out in the comments, these methods are now deprecated. This is the new recommended way:

First, instead of importing the old InstrumentationRegistry class, use the new one.
Instead of InstrumentationRegistry.getTargetContext(); use ApplicationProvider.getApplicationContext(). Source
Instead of InstrumentationRegistry.getTargetContext(); use ApplicationProvider.getApplicationContext(). Source
For InstrumentationRegistry.getTargetContext();: In most scenarios, ApplicationProvider.getApplicationContext() should be used instead of the instrumentation test context. If you do need access to the test context for to access its resources, it is recommended to use PackageManager.getResourcesForApplication(String) instead.
Source

",java
how to annotate maths equation in graph using both latex and fstring formatting,"Here's the code snippet : annotation of mathematical equation of the cuve.
# equation annotation 
plt.annotate(s =f""$exp({poly_regr.intercept_[0] : 0.2f} - {-poly_regr.coef_[0,0] : 0.2f}x + {poly_regr.coef_[0,1]: 0.4f}x^2)$"",
             xy = (1975, 0.4*(10**13)),
             color = ""black"",
             fontsize = 13,
             fontweight = ""normal""
             )


what's the possible way to update the f-string code to get the desired anotation with a beautiful mathematical equations.
or even how to write more complex things , for example :
","You can do it using LaTeX syntax. For example, replace the string in plt.annotate with
fr""$e^{{({poly_regr.intercept_[0] : 0.2f} - {-poly_regr.coef_[0,0] : 0.2f}x + {poly_regr.coef_[0,1]: 0.4f}x^2)}}$""

or with
r""$e^{(\alpha + \beta + 48x)}$""

",matplotlib
select rows if a condition is met in any two columns,"Please help me filter my dataframe for a condition, that should be fulfilled in any two columns.
Imagine a list of students with grades in different sports. I want to filter the list so the new list passed_students shows only those who have scored a 4 or greater in at least two different sports.
Students = {
  ""Names"": [""Tom"", ""Rick"", ""Sally"",""Sarah""],
  ""Football"": [4, 5, 2,1],
  ""Basketball"": [4, 2, 4,2],
  ""Volleyball"": [6, 1, 6,1],
  ""Foosball"": [4, 3, 4,3],
}

The Code should return this:
passed_Students = {
  ""Names"": [""Tom"", ""Sally""],
  ""Football"": [4,2],
  ""Basketball"": [4,4],
  ""Volleyball"": [6,6],
  ""Foosball"": [4,4],
}

I can make it work if one grade above 4 is sufficient:
import numpy as np
import pandas as pd


Students = {
  ""Names"": [""Tom"", ""Rick"", ""Sally"",""Sarah""],
  ""Football"": [4, 5, 2,1],
  ""Basketball"": [4, 2, 4,2],
  ""Volleyball"": [6, 1, 6,1],
  ""Foosball"": [4, 3, 4,3],
}
Students = pd.DataFrame(Students)


passed_Students= Students[(Students[""Football""]>3) |(Students[""Basketball""]>3)|(Students[""Volleyball""]>3)|(Students[""Foosball""]>3) ]

print(passed_Students)


This returns:
Students = {
  ""Names"": [""Tom"", ""Rick"", ""Sally""],
  ""Football"": [4, 5, 2],
  ""Basketball"": [4, 2, 4],
  ""Volleyball"": [6, 1, 6],
  ""Foosball"": [4, 3, 4],
}

But how can I make it such, that any two grades of 4 or above qualify for passed_students, thereby returning only this?
passed_Students = {
  ""Names"": [""Tom"", ""Sally""],
  ""Football"": [4,2],
  ""Basketball"": [4,4],
  ""Volleyball"": [6,6],
  ""Foosball"": [4,4],
}

","drop the ""Names"", then compare to 4 with ge, sum to count the number of True per row and filter with boolean indexing:
passed_Students = Students[Students.drop(columns=['Names'])
                                   .ge(4).sum(axis=1).ge(2)]

Output:
   Names  Football  Basketball  Volleyball  Foosball
0    Tom         4           4           6         4
2  Sally         2           4           6         4

Intermediates:
# Students.drop(columns=['Names']).ge(4)
   Football  Basketball  Volleyball  Foosball
0      True        True        True      True
1      True       False       False     False
2     False        True        True      True
3     False       False       False     False

# Students.drop(columns=['Names']).ge(4).sum(axis=1)
0    4
1    1
2    3
3    0
dtype: int64

# Students.drop(columns=['Names']).ge(4).sum(axis=1).ge(2)
0     True
1    False
2     True
3    False
dtype: bool

",pandas
detecting unsaved changes,"I have a requirement to implement an ""Unsaved Changes"" prompt in an ASP .Net application.  If a user modifies controls on a web form, and attempts to navigate away before saving, a prompt should appear warning them that they have unsaved changes, and give them the option to cancel and stay on the current page.  The prompt should not display if the user hasn't touched any of the controls.
Ideally I'd like to implement this in JavaScript, but before I go down the path of rolling my own code, are there any existing frameworks or recommended design patterns for achieving this?  Ideally I'd like something that can easily be reused across multiple pages with minimal changes.
","Using jQuery:
var _isDirty = false;
$(""input[type='text']"").change(function(){
  _isDirty = true;
});
// replicate for other input types and selects

Combine with onunload/onbeforeunload methods as required.
From the comments, the following references all input fields, without duplicating code:
$(':input').change(function () {

Using $("":input"") refers to all input, textarea, select, and button elements.
",prompt
igraph graph from numpy or pandas adjacency matrix,"I have an adjacency matrix stored as a pandas.DataFrame:
node_names = ['A', 'B', 'C']
a = pd.DataFrame([[1,2,3],[3,1,1],[4,0,2]],
    index=node_names, columns=node_names)
a_numpy = a.as_matrix()

I'd like to create an igraph.Graph from either the pandas or the numpy adjacency matrices. In an ideal world the nodes would be named as expected.
Is this possible? The tutorial seems to be silent on the issue.
","In igraph you can use igraph.Graph.Adjacency to create a graph from an adjacency matrix without having to use zip. There are some things to be aware of when a weighted adjacency matrix is used and stored in a np.array or pd.DataFrame.

igraph.Graph.Adjacency can't take an np.array as argument, but that is easily solved using tolist.

Integers in adjacency-matrix are interpreted as number of edges between nodes rather than weights, solved by using adjacency as boolean.


An example of how to do it:
import igraph
import pandas as pd

node_names = ['A', 'B', 'C']
a = pd.DataFrame([[1,2,3],[3,1,1],[4,0,2]], index=node_names, columns=node_names)

# Get the values as np.array, it's more convenenient.
A = a.values

# Create graph, A.astype(bool).tolist() or (A / A).tolist() can also be used.
g = igraph.Graph.Adjacency((A > 0).tolist())

# Add edge weights and node labels.
g.es['weight'] = A[A.nonzero()]
g.vs['label'] = node_names  # or a.index/a.columns

You can reconstruct your adjacency dataframe using get_adjacency by:
df_from_g = pd.DataFrame(g.get_adjacency(attribute='weight').data,
                         columns=g.vs['label'], index=g.vs['label'])
(df_from_g == a).all().all()  # --> True

",numpy
simulating scikitlearn39s onehotencoder mixed data type error,"My focus is on the caveat mentioned in OneHotEncoder's list parameter doc.

list : categories[i] holds the categories expected in the ith column.
The passed categories should not mix strings and numeric values within
a single feature, and should be sorted in case of numeric values.

I'm trying two different approaches. The first one has a hard-coded data frame.
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import OneHotEncoder
import pandas as pd

X = pd.DataFrame(
    {'city': ['London', 'London', 'Paris', 'NewYork'],
     'country': ['UK', 0.2, 'FR', 'US'],
     'user_rating': [4, 5, 4, 3]}
)
categorical_features = ['city', 'country']
one_hot = OneHotEncoder()
transformer = ColumnTransformer([(""one_hot"", one_hot, categorical_features)], remainder=""passthrough"")
transformed_X = transformer.fit_transform(X)

This code throws a TypeError at transformed_X = transformer.fit_transform(X).
I wanted to try the same, but this time reading data from a CSV file instead of hard-coding it.
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import OneHotEncoder
import pandas as pd

X = pd.read_csv('data.csv', header=0)
categorical_features = ['city', 'country']
one_hot = OneHotEncoder()
transformer = ColumnTransformer([(""one_hot"", one_hot, categorical_features)], remainder=""passthrough"")
transformed_X = transformer.fit_transform(X)

The csv file looks like this.
city,country,user_rating
London,UK,4
London,0.2,5
Paris,FR,4
NewYork,US,3

However, this code does not throw any errors. I can see four different encodings when I print the transformed_X. It seems like scikit-learn treated 0.2 as a string instead of a float.
Can the mixed data type error be simulated when reading CSV files? Or it is not possible, because pandas infer the column type when reading the data, so the entire column gets the type object unlike the time of hard-coding data.
","Option 1 (pd.to_numeric)
After reading the data with pd.read_csv, use pd.to_numeric:
import pandas as pd
from io import StringIO

s = """"""city,country,user_rating
London,UK,4
London,0.2,5
Paris,FR,4
NewYork,US,3
""""""

X = pd.read_csv(StringIO(s), header=0)
X[categorical_features] = X[categorical_features].apply(
    lambda x: pd.to_numeric(x, errors='coerce').fillna(x)
        )

Output:
X['country'].tolist()

['UK', 0.2, 'FR', 'US']

Here, we attempt to convert all values to numeric data types; where it fails, we get NaN values, which we fill again with the original values.

Option 2 (converters parameter)
With pd.read_csv, you can pass a custom function to the converters parameter:
def convert(val):
    try:
        return float(val)
    except ValueError:
        return val

categorical_features = ['city', 'country']
converters = {feature: convert for feature in categorical_features}
X = pd.read_csv(StringIO(s), header=0, converters=converters)

# same result

Here, we define a function convert that attempts to convert each value in a column to float; when it fails, it returns the original value. We use a dictionary comprehension to map the categorical columns to this function:
converters = {feature: convert for feature in categorical_features}
converters

{'city': <function __main__.convert(val)>,
 'country': <function __main__.convert(val)>}

Via converters=converters, the function gets applied to the applicable columns from the source.

Note that option 1 will be much faster on a sizeable dataset, as pd.to_numeric is vectorized, meaning its logic is applied to an entire column at once, while converters will apply a function (like convert) to each value in a column individually.
Reproducing the error:
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import OneHotEncoder

one_hot = OneHotEncoder()
transformer = ColumnTransformer([(""one_hot"", one_hot, categorical_features)], 
                                remainder=""passthrough"")
transformed_X = transformer.fit_transform(X)

Result:
TypeError: Encoders require their input to be uniformly strings or numbers. Got ['float', 'str']

",scikit-learn
how to force gcc to do printf checks on nonliteral format strings,"const char* const non_literal_string = ""Hello %d"";

void my_print()
{
    /* 
     * I would like GCC to throw a warning when compiling this line because the
     * argument is not of int type.
     */
    printf(non_literal_string, ""World!"");
    
    //Like it does here
    printf(""Hello %d"", ""World!"");
}

Above I have declared non_literal_string as const char* const so neither the pointer nor the pointed string can change, so there are the conditions for which GCC can check that the supplied arguments have types appropriate to the specified format string.
With clang compiler these checks are performed even on non-literal format strings, this is the output if I try to compile this code with clang:
<source>:11:32: warning: format specifies type 'int' but the argument has type 'char *' [-Wformat]
   11 |     printf(non_literal_string, ""World!"");
      |            ~~~~~~~~~~~~~~~~~~  ^~~~~~~~
<source>:3:47: note: format string is defined here
    3 | const char* const non_literal_string = ""Hello %d"";
      |                                               ^~
      |                                               %s
<source>:14:24: warning: format specifies type 'int' but the argument has type 'char *' [-Wformat]
   14 |     printf(""Hello %d"", ""World!"");
      |                   ~~   ^~~~~~~~
      |                   %s

","
How to force gcc to do printf checks on non-literal format strings?

A program does not do something that you would want the program to do. The only possible way to ""force"" gcc to do anything is to change it. You can modify gcc source code to do the check and implement it. You could post it as a bug or feature request to gcc bugzilla and also fund gcc development https://gcc.gnu.org/wiki/GNUToolchainFund .
The feature is just not implemented, it's not there. There is no ""forcing"".
",c
does pytorch cnn care about image size,"I am playing with CNNs these days, and I have code like pasted below. My question is, would this work on any image size? It is not clear to me what parameter or channel, if any, cares about the image size? And if that's the case, how does the model know how many neurons it needs, isn't that a function of image size?
Related point on pretrained models - if I use pretrained models, do I need to reformat my images to be same as what the model was trained on in the first place, or how does that work?
class CNN(nn.Module):
    def __init__(self, num_classes, num_channels=1):
        super(CNN, self).__init__()
        self.num_classes = num_classes
        self.conv1 = nn.Conv2d(num_channels, 32, kernel_size=3, padding=1)
        self.relu1 = nn.ReLU()
        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)
        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)
        self.relu2 = nn.ReLU()
        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)
        self.fc = nn.Linear(64*7*7, num_classes)

","
No, but the Conv2D() layer can work on variable image sizes. The way the Conv2D() layer works is by applying the kernel on each set of kernel_size x kernel_size pixels. The padding adds to the dimensions before the convolution as well. So as long as the image dimensions are at least kernel_size x kernel_size after padding, the convolution will work. In this case, kernel_size is 3 and padding is 1 so even a single pixel will work because after padding, the image will be 3x3. The reason this CNN can't take variable image sizes is because of the linear layer. The linear layer requires a 7x7 image after max-pooling twice, so a 28x28 image to start with.

It depends on the type of layer, linear layers take in a set input size and a set output size, which determines the number of parameters (neurons). The number of parameters for convolution layers are determined by the kernel size and number of output channels so it doesn't rely on input size.

It depends on the model, some models allow variable image sizes even if they were trained on a specific resolution.


",pytorch
spring boot can a springboot app be deployed to upcloud or vultr cloud services,"I am trying to host my spring boot app, based on the documentation of spring boot they mention Heroku, AWS, Google Cloud, google cloud as some of the host providers. I want to know if Springboot apps can be hosted also on Vultr and Upcloud and any other. 
","Spring boot is a simple java application can be deployed in any cloud. please specify if you have some specific concern.
Please look into this link:
https://docs.spring.io/spring-boot/docs/current/reference/html/deployment-install.html
",java
google playintegrity api a nightmare,"I need some help guys!! I am a self-taught, newbie in encryption, and after reading, testing, and error for more than two weeks on how to solve this, and finding very little crowd knowledge and almost no documentation from Google.
I am trying to read the integrity verdict, that I have managed to get it IntegrityTokenRequest doing
    String nonce = Base64.encodeToString(""this_is_my_nonce"".getBytes(), Base64.URL_SAFE | Base64.NO_WRAP | Base64.NO_PADDING);
    IntegrityManager myIntegrityManager =   IntegrityManagerFactory
          .create(getApplicationContext());
    // Request the integrity token by providing a nonce.
    Task<IntegrityTokenResponse> myIntegrityTokenResponse = myIntegrityManager
          .requestIntegrityToken(IntegrityTokenRequest
          .builder()
          .setNonce(nonce)
          .build());

    myIntegrityTokenResponse.addOnSuccessListener(new OnSuccessListener<IntegrityTokenResponse>() {
        @Override
        public void onSuccess(IntegrityTokenResponse myIntegrityTokenResponse) {
            String token = myIntegrityTokenResponse.token();
            // so here I have my Integrity token.
            // now how do I read it??
        }
    }

As per the documentation, it's all set up in the Play Console, and created the Google Cloud project accordingly. Now here comes the big hole in the documentation:
a) The JWT has 4 dots that divide the JWT into 5 sections, not in 3 sections as described here https://jwt.io/
b) Developer.Android.com recommends to Decrypt and Verify on Google Servers

I have no idea on how or were to execute this command... :-(
c) if I choose to decrypt and verify the returned token it's more complicated as I don't have my own secure server environment, only my App and the Google Play Console.
d) I found in the Google Clound Platform OAuth 2.0 Client IDs ""Android client for com.company.project"" JSON file that I have downloaded, but no clue (again) on how to use it in my App for getting the veredict from the Integrity Token.
{""installed"":
    {""client_id"":""123456789012-abcdefghijklmnopqrstuvwxyza0g2ahk.apps.googleusercontent.com"",
        ""project_id"":""myproject-360d3"",
        ""auth_uri"":""https://accounts.google.com/o/oauth2/auth"",
        ""token_uri"":""https://oauth2.googleapis.com/token"",
        ""auth_provider_x509_cert_url"":https://www.googleapis.com/oauth2/v1/certs
    }
}

I'm sure I am missing a lot, please help
","Using a cloud server to decode and verify the token is better.
For example, if you going with Java service then the below code will send the integrity token to the google server hence you can verify the response.
Enable PlayIntegrity API in Google Cloud Platform against the app and download the JSON file and configure in the code.
Similarly, you should enable PlayIntegrity API in Google PlayConsole against the app
Add Google Play Integrity Client Library to your project
Maven Dependency
<project>
 <dependencies>
   <dependency>
     <groupId>com.google.apis</groupId>
     <artifactId>google-api-services-playintegrity</artifactId>
     <version>v1-rev20220211-1.32.1</version>
   </dependency>
 </dependencies>

Gradle
repositories {
   mavenCentral()
}
dependencies {
   implementation 'com.google.apis:google-api-services-playintegrity:v1-rev20220211-1.32.1'
}

Token decode
DecodeIntegrityTokenRequest requestObj = new DecodeIntegrityTokenRequest();
requestObj.setIntegrityToken(request.getJws());
//Configure downloaded Json file
GoogleCredentials credentials = GoogleCredentials.fromStream(new FileInputStream(""<Path of JSON file>\\file.json""));
HttpRequestInitializer requestInitializer = new HttpCredentialsAdapter(credentials);

 HttpTransport HTTP_TRANSPORT = new NetHttpTransport();
 JsonFactory JSON_FACTORY = new JacksonFactory();
 GoogleClientRequestInitializer initialiser = new PlayIntegrityRequestInitializer();
 
 
Builder playIntegrity = new PlayIntegrity.Builder(HTTP_TRANSPORT, JSON_FACTORY, requestInitializer).setApplicationName(""testapp"")
        .setGoogleClientRequestInitializer(initialiser);
             PlayIntegrity play = playIntegrity.build();
    
DecodeIntegrityTokenResponse response = play.v1().decodeIntegrityToken(""com.test.android.integritysample"", requestObj).execute();

Then the response will be as follows
{
""tokenPayloadExternal"": {
    ""accountDetails"": {
        ""appLicensingVerdict"": ""LICENSED""
    },
    ""appIntegrity"": {
        ""appRecognitionVerdict"": ""PLAY_RECOGNIZED"",
        ""certificateSha256Digest"": [""pnpa8e8eCArtvmaf49bJE1f5iG5-XLSU6w1U9ZvI96g""],
        ""packageName"": ""com.test.android.integritysample"",
        ""versionCode"": ""4""
    },
    ""deviceIntegrity"": {
        ""deviceRecognitionVerdict"": [""MEETS_DEVICE_INTEGRITY""]
    },
    ""requestDetails"": {
        ""nonce"": ""SafetyNetSample1654058651834"",
        ""requestPackageName"": ""com.test.android.integritysample"",
        ""timestampMillis"": ""1654058657132""
    }
}
}

Check for License
String licensingVerdict = response.getTokenPayloadExternal().getAccountDetails().getAppLicensingVerdict();
    if(!licensingVerdict.equalsIgnoreCase(""LICENSED"")) {
         throw new Exception(""Licence is not valid."");
            
    }

Verify App Integrity
public void checkAppIntegrity(DecodeIntegrityTokenResponse response,  String appId) throws Exception {
    AppIntegrity appIntegrity = response.getTokenPayloadExternal().getAppIntegrity();
    
    if(!appIntegrity.getAppRecognitionVerdict().equalsIgnoreCase(""PLAY_RECOGNIZED"")) {
        throw new Exception(""The certificate or package name does not match Google Play records."");
    }
     if(!appIntegrity.getPackageName().equalsIgnoreCase(appId)) {
         throw new Exception(""App package name mismatch."");
        
     }
     
     if(appIntegrity.getCertificateSha256Digest()!= null) {
        //If the app is deployed in Google PlayStore then Download the App signing key certificate from Google Play Console (If you are using managed signing key). 
        //otherwise download Upload key certificate and then find checksum of the certificate.
         Certificate cert = getCertificate(""<Path to Signing certificate>\deployment_cert.der"");
         MessageDigest md = MessageDigest.getInstance(""SHA-256""); 

        byte[] der = cert.getEncoded(); 
        md.update(der);
        byte[] sha256 = md.digest();
        
        //String checksum = Base64.getEncoder().encodeToString(sha256);
       String checksum = Base64.getUrlEncoder().encodeToString(sha256);
       /** Sometimes checksum value ends with '=' character, you can avoid this character before perform the match **/
       checksum = checksum.replaceAll(""="","""");        
        if(!appIntegrity.getCertificateSha256Digest().get(0).contains(checksum)) {
             throw new Exception(""App certificate mismatch."");
        }
     }
}
public static Certificate getCertificate(String certificatePath)
        throws Exception {
    CertificateFactory certificateFactory = CertificateFactory
            .getInstance(""X509"");
    FileInputStream in = new FileInputStream(certificatePath);

    Certificate certificate = certificateFactory
            .generateCertificate(in);
    in.close();

    return certificate;
}

Verify Device integrity
//Check Device Integrity
public void deviceIntegrity(DecodeIntegrityTokenResponse response) {
    DeviceIntegrity deviceIntegrity = response.getTokenPayloadExternal().getDeviceIntegrity();
    if(!deviceIntegrity.getDeviceRecognitionVerdict().contains(""MEETS_DEVICE_INTEGRITY"")) {
        throw new Exception(""Does not meet Device Integrity."");
        
    }
}

Similary you can verify the Nonce and App Package name with previously stored data in server
",java
how to show current directory in ipython prompt,"Is there is way to show the current directory in IPython prompt?
Instead of this:
In [1]:

Something like this:
In<~/user/src/proj1>[1]:

","Assuming you're interested in configuring this for all subsequent invocations of ipython, run the following (in a traditional shell, like bash :) ). It appends to your default ipython configuration, creating it if necessary. The last line of the configuration file will also automatically make all the executables in your $PATH available to simply run in python, which you probably also want if you're asking about cwd in the prompt. So you can run them without a ! prefix. Tested with IPython 7.18.1.
mkdir -p ~/.ipython/profile_default
cat >> ~/.ipython/profile_default/ipython_config.py <<EOF

from IPython.terminal.prompts import Prompts, Token
import os

class MyPrompt(Prompts):
    def cwd(self):
        cwd = os.getcwd()
        if cwd.startswith(os.environ['HOME']):
            cwd = cwd.replace(os.environ['HOME'], '~')
            cwd_list = cwd.split('/')
            for i,v in enumerate(cwd_list):
                if i not in (1,len(cwd_list)-1): #not last and first after ~
                    cwd_list[i] = cwd_list[i][0] #abbreviate
            cwd = '/'.join(cwd_list)
        return cwd

    def in_prompt_tokens(self, cli=None):
        return [
                (Token.Prompt, 'In ['),
                (Token.PromptNum, str(self.shell.execution_count)),
                (Token.Prompt, '] '),
                (Token, self.cwd()),
                (Token.Prompt, ': ')]

c.TerminalInteractiveShell.prompts_class = MyPrompt
c.InteractiveShellApp.exec_lines = ['%rehashx']
EOF

(c.PromptManager only works in older versions of ipython.)
",prompt
connection with netssh2php to huawei olt ma5680t is very slow,"I implemented a php script to communicate with my Huawei Olt MA5680T through the phpseclib SSH2.php library but the execution time of any remote operation on the Olt is extremely slow, about 40 seconds.
I tried sending simpler commands but the result doesn't change.
I'm currently using this function to connect via ssh to the device:
function sessione_ssh($olt_id) {
global $ssh;
global $ip_olt;

$ssh = new Net_SSH2($row_olt['olt_ip']);
if (!$ssh->login($row_olt['olt_user'], $row_olt['olt_pass'])) {
exit('Login Failed');}
$ssh->setWindowColumns(200);
$ssh->read('MA5680T>');
$ssh->write(""enable\n"");
$ssh->read('MA5680T#');
$ssh->write(""mmi-mode enable\n"");
$ssh->read('MA5680T#');
}





function check($pppoe) {
global $ssh;
global $frame;
global $board;
global $port;
global $ontid;
global $ifindex;
global $ip_olt;
$ont_eth_port='1';
// restituisce solo il valore senza tipo stringa
snmp_set_quick_print(1);
snmp_read_mib ( 'ma5600t.txt');
ricava_info_ont($pppoe);
$ssh->disconnect();
//ont_descrizione
echo 'PPPoE Cliente: ' . str_replace('""','',snmp2_get ($ip_olt, ""private"", ""1.3.6.1.4.1.2011.6.128.1.1.2.43.1.9."" . $ifindex . ""."" . $ontid)) . ""<br>"";
echo 'Indirizzo Ip: ' . snmp2_get ($ip_olt, ""private"", ""1.3.6.1.4.1.2011.6.3.27.1.3."" . $ifindex . ""."" . $ontid) . ""<br>"";
//ont_seriale
echo 'Seriale ONT: ' . str_replace('""','',str_replace("" "","""",snmp2_get ($ip_olt, ""private"", ""1.3.6.1.4.1.2011.6.128.1.1.2.43.1.3."" . $ifindex . ""."" . $ontid))) . ""<br>"";
$ont_status= snmp2_get ($ip_olt, ""private"", ""1.3.6.1.4.1.2011.6.128.1.1.2.46.1.15."" . $ifindex . ""."" . $ontid);  //1 online  2 offline
if ($ont_status=='1') {
echo 'Stato: Online<br>';
}
if ($ont_status=='2') {
echo 'Stato: Offline<br>';
}
//ont_rx_segnale
echo 'Segnale RX: ' . (snmp2_get ($ip_olt, ""private"", ""1.3.6.1.4.1.2011.6.128.1.1.2.51.1.4."" . $ifindex . ""."" . $ontid))/100 . "" db<br>"";
//ont_distanza
echo 'Distanza: ' . snmp2_get ($ip_olt, ""private"", ""1.3.6.1.4.1.2011.6.128.1.1.2.46.1.20."" . $ifindex . ""."" . $ontid) . "" metri<br>"";
//ont_ultimo_uptime
echo 'Ultimo Uptime: ' . converti_data(snmp2_get ($ip_olt, ""private"", ""1.3.6.1.4.1.2011.6.128.1.1.2.46.1.22."" . $ifindex . ""."" . $ontid)) . ""<br>"";
//ont_ultimo_downtime
echo 'Ultimo Downtime: ' . converti_data(snmp2_get ($ip_olt, ""private"", ""1.3.6.1.4.1.2011.6.128.1.1.2.46.1.23."" . $ifindex . ""."" . $ontid)) . ""<br>"";
$ont_motivo_disconnessione=snmp2_get ($ip_olt, ""private"", ""1.3.6.1.4.1.2011.6.128.1.1.2.46.1.24."" . $ifindex . ""."" . $ontid); //,15 LOKI(Loss of key synch with ONUi),18 deactived ONT due to the ring,30 shut down ONT optical module,31 reset ONT by ONT command,32 reset ONT by ONT reset button,33 reset ONT by ONT software,34 deactived ONT due to broadcast attack,35 operator check fail,37 rogue ONT detected by itself,-1 query fails.
if ($ont_motivo_disconnessione=='1') {
echo 'Causa disconnessione: LOS (Loss of signal)<br>';
}
if ($ont_motivo_disconnessione=='2') {
echo 'Causa disconnessione: LOSi (Loss of signal for ONUi) or LOBi (Loss of burst for ONUi)<br>';
}
if ($ont_motivo_disconnessione=='3') {
echo 'Causa disconnessione: LOFI (Loss of frame of ONUi)<br>';
}
if ($ont_motivo_disconnessione=='4') {
echo 'Causa disconnessione: SFI (Signal fail of ONUi)<br>';
}
if ($ont_motivo_disconnessione=='5') {
echo 'Causa disconnessione: LOAI (Loss of acknowledge with ONUi)<br>';
}
if ($ont_motivo_disconnessione=='6') {
echo 'Causa disconnessione: LOAMI (Loss of PLOAM for ONUi)<br>';
}
if ($ont_motivo_disconnessione=='7') {
echo 'Causa disconnessione: Deactive ONT fails<br>';
}
if ($ont_motivo_disconnessione=='8') {
echo 'Causa disconnessione: Deactive ONT success<br>';
}
if ($ont_motivo_disconnessione=='9') {
echo 'Causa disconnessione: Reset ONT<br>';
}
if ($ont_motivo_disconnessione=='10') {
echo 'Causa disconnessione: Re-register ONT<br>';
}
if ($ont_motivo_disconnessione=='11') {
echo 'Causa disconnessione: Pop up fail<br>';
}
if ($ont_motivo_disconnessione=='13') {
echo 'Causa disconnessione: Dying-Gasp<br>';
}
//ont_uptime_tratta
echo 'Uptime tratta Pon: ' . date_format (new DateTime(str_replace('""','',snmp2_get ($ip_olt, ""private"", ""1.3.6.1.4.1.2011.6.128.1.1.2.101.1.6."" . $ifindex . ""."" . $ontid . "".0""))), 'd/m/Y H:i:s') . ""<br>"";
$ont_power_eth=snmp2_get ($ip_olt, ""private"", ""1.3.6.1.4.1.2011.6.128.1.1.2.62.1.5."" . $ifindex . ""."" . $ontid . ""."" . $ont_eth_port); //1on,2off
if ($ont_power_eth=='1') {
echo 'Porta Eth: On<br>';
}
if ($ont_power_eth=='2') {
echo 'Porta Eth: Off<br>';
}
$ont_registrazione_eth=snmp2_get ($ip_olt, ""private"", ""1.3.6.1.4.1.2011.6.128.1.1.2.62.1.22."" . $ifindex . ""."" . $ontid . ""."" . $ont_eth_port); //1linkup,2linkdown
if ($ont_registrazione_eth=='1') {
echo 'Registrazione Eth: On<br>';
}
if ($ont_registrazione_eth=='2') {
echo 'Registrazione Eth: Off<br>';
}
$ont_speed_eth=snmp2_get ($ip_olt, ""private"", ""1.3.6.1.4.1.2011.6.128.1.1.2.62.1.4."" . $ifindex . ""."" . $ontid . ""."" . $ont_eth_port);
if ($ont_speed_eth=='1') {
echo 'Negoziazione Eth: 10M manuale<br>';
}
if ($ont_speed_eth=='2') {
echo 'Negoziazione Eth: 100M manuale<br>';
}
if ($ont_speed_eth=='3') {
echo 'Negoziazione Eth: 1000M manuale<br>';
}
if ($ont_speed_eth=='4') {
echo 'Negoziazione Eth: Autonegoziazione<br>';
}
if ($ont_speed_eth=='5') {
echo 'Negoziazione Eth: 10M Autonegoziazione<br>';
}
if ($ont_speed_eth=='6') {
echo 'Negoziazione Eth: 100M Autonegoziazione<br>';
}
if ($ont_speed_eth=='7') {
echo 'Negoziazione Eth: 1000M Autonegoziazione<br>';
}
if ($ont_speed_eth=='8') {
echo 'Negoziazione Eth: 10000M manuale<br>';
}
if ($ont_speed_eth=='9') {
echo 'Negoziazione Eth: 10000M Autonegoziazione<br>';
}
if ($ont_speed_eth=='10') {
echo 'Negoziazione Eth: 2500M manuale<br>';
}
if ($ont_speed_eth=='11') {
echo 'Negoziazione Eth: 2500M Autonegoziazione<br>';
}
if ($ont_speed_eth=='-1') {
echo 'Negoziazione Eth: invalid<br>';
}
//ont_password_pon
echo 'Password Pon: ' . snmp2_get ($ip_olt, ""private"", ""1.3.6.1.4.1.2011.6.128.1.1.2.43.1.4."" . $ifindex . ""."" . $ontid) . ""<br>"";
}
// fine function check



unction ricava_info_ont($pppoe) {
global $ssh;
global $frame;
global $board;
global $port;
global $ontid;
global $ifindex;

//determina $id_olt

//per carcolare id olt se si tratta di cambio impianto usare var session per impianto vecchio e primaria vecchia altrimenti procedi normalmente
if($_SESSION['vecchioimpianto']) {
//decodifica impianto
$primariavecchia=explode(""-"",$_SESSION['vecchioimpianto']);
$primariavecchia=$primariavecchia['0'];
unset($_SESSION['vecchioimpianto']);
$sql_id_olt=""select id_olt from ftth_fibre_primarie where id_fibra_primaria ='"" . $primariavecchia . ""'"";
}
else {
$sql_id_olt=""select id_olt from ftth_fibre_primarie where id_fibra_primaria = (select SUBSTR(impianto, 1, 4) from ftth_ont where pppoe='"" . $pppoe . ""')"";
}
// fine se si tratta di cambio pacchetto
$rs_id_olt=CustomQuery($sql_id_olt);
$row_id_olt = db_fetch_array($rs_id_olt);
sessione_ssh($row_id_olt['id_olt']);
$ssh->write(""display ont info by-desc "" . $pppoe . ""\n"");
$out=nl2br($ssh->read());
// crea array con righe output
$logline = explode(""\n"",$out);
//ricava $ontid, $frame, $board e $port da descrizione (pppoe)
$parziale = explode("" "",$logline['9']);
$ontid=$parziale['9'];
$parziale2=explode(""/"",$parziale['2']);
$frame=$parziale2['0'];
$board=$parziale2['1'];
$port=$parziale2['2'];
$ifindex=4194304000 + ($board * (256 * 32) + $port * 256);
}
// fine function ricava_info_ont



With the same library I connect to other devices and execute scripts that are executed quickly, the problem occurs only with this device. I tried reading the logs but I don't detect any errors in the communication between the ssh2.php library and the olt MA5680T.
What could be the problem?
this is the logs:
https://pastebin.com/n9wHdLaj
Thank you so much for the support I receive from you for my project
UPDATE
The suggestion received from @neubert seems like the right way to decrease the script execution time. With the function I posted above I now actually have an execution time of about 3 seconds (absolutely acceptable). so using the same logic I inserted the correct prompt into all read() calls but it doesn't always work! when I enter a submenu of the prompt example: $ssh->write(""interface gpon "" . $frame . ""/"" . $board . ""\n""); then I set the read call like this: $ssh->read('MA5680T(config-if-gpon-' . $frame_ont . '/' . $board_ont . ')#'); but I always get long times with 10 second timeouts like before. This is the link to the prompt from the terminal https://pasteboard.co/ldCa4JlnsfnL.jpg
What could be the problem?
this is the log:
<- NET_SSH2_MSG_CHANNEL_DATA (since last: 0.03, network: 0.0003s)
00000000  00:00:00:02:00:00:00:14:0d:0a:0d:0a:4d:41:35:36  ............MA56
00000010  38:30:54:28:63:6f:6e:66:69:67:29:23              80T(config)#

<- NET_SSH2_MSG_CHANNEL_DATA (since last: 0.0122, network: 0.0001s)
00000000  00:00:00:02:00:00:00:13:69:6e:74:65:72:66:61:63  ........interfac
00000010  65:20:67:70:6f:6e:20:30:2f:31:33                 e gpon 0/13

<- NET_SSH2_MSG_CHANNEL_DATA (since last: 0.0181, network: 0.0003s)
00000000  00:00:00:02:00:00:00:21:0d:0a:0d:0a:4d:41:35:36  .......!....MA56
00000010  38:30:54:28:63:6f:6e:66:69:67:2d:69:66:2d:67:70  80T(config-if-gp
00000020  6f:6e:2d:30:2f:31:33:29:23                       on-0/13)#

-> NET_SSH2_MSG_CHANNEL_DATA (since last: 9.9108, network: 0.0002s)
00000000  00:00:00:01:00:00:00:0f:6f:6e:74:20:64:65:6c:65  ........ont dele
00000010  74:65:20:31:20:30:0a                             te 1 0.

<- NET_SSH2_MSG_CHANNEL_DATA (since last: 0.0303, network: 0.0001s)
00000000  00:00:00:02:00:00:00:0e:6f:6e:74:20:64:65:6c:65  ........ont dele
00000010  74:65:20:31:20:30                                te 1 0

<- NET_SSH2_MSG_CHANNEL_DATA (since last: 0.0744, network: 0.0001s)
00000000  00:00:00:02:00:00:00:35:0d:0a:20:20:4e:75:6d:62  .......5..  Numb
00000010  65:72:20:6f:66:20:4f:4e:54:73:20:74:68:61:74:20  er of ONTs that 
00000020  63:61:6e:20:62:65:20:64:65:6c:65:74:65:64:3a:20  can be deleted: 
00000030  31:2c:20:73:75:63:63:65:73:73:3a:20:31           1, success: 1

<- NET_SSH2_MSG_CHANNEL_DATA (since last: 0.0241, network: 0.0001s)
00000000  00:00:00:02:00:00:00:21:0d:0a:0d:0a:4d:41:35:36  .......!....MA56
00000010  38:30:54:28:63:6f:6e:66:69:67:2d:69:66:2d:67:70  80T(config-if-gp
00000020  6f:6e:2d:30:2f:31:33:29:23                       on-0/13)#

-> NET_SSH2_MSG_DISCONNECT (since last: 9.8742, network: 0.0002s)
00000000  00:00:00:0b:00:00:00:00:00:00:00:00              ............


I notice the presence of this strange exclamation point before each timeout
.......!....MA5680T(config-if-gpon-0/13)#
","I solved it thanks to @nubert by setting the correct prompt for each $ssh->read();
Example:
$ssh->read('MA5680T>');,$ssh->read('MA5680T(config)#');
",prompt
ltrandomizedsearchcvgt pass the estimator obtained after fitting to scoring function as a parameter,"Suppose I want to do a RandomizedSearchCV with custom both estimator and scorer :
RandomizedSearchCV(cxCustomLogReg(), search_space,
                   n_iter=50, scoring=scorer) 

May it be possible, during randomized search, to pass the estimator obtained after fitting cxCustomLogReg() to scorer function as a parameter (and not only y_true/y_pred values as usual)?
","This is actually the expected format. When scoring is a callable, it should have signature (fitted_estimator, X_test, y_test).
You may be using the helper make_scorer, which converts a metric function with signature (y_true, y_pred) into a scoring function as described above. Just don't use that helper, and define your scorer directly.
",cross-validation
analysing documents with llava on ollama not working,"I am currently testing LLAVA for use in Document Understanding Tasks. I found some promissing results in some scientific papers and on some websites. I installed the Model on Ollama (on Windows) and tried accessing it with this C# Code.
using System.Text;
using System.Text.Json;

public class Program
{
    private static readonly HttpClient client = new HttpClient();
    private static string? imageBase64;

    static async Task Main(string[] args)
    {
        Console.WriteLine(""Welcome to the Document Analysis Application!"");

        while (true)
        {
            Console.Write(""Enter the path to the image file (or 'exit' to quit): "");
            string imagePath;
            do
            {
                imagePath = Console.ReadLine() ?? """";
            } while (String.IsNullOrEmpty(imagePath));

            if (imagePath.ToLower() == ""exit"")
                break;

            if (!File.Exists(imagePath))
            {
                Console.WriteLine(""File not found. Please try again."");
                continue;
            }

            imageBase64 = Convert.ToBase64String(File.ReadAllBytes(imagePath));
            Console.WriteLine(""Image loaded successfully."");

            while (true)
            {
                Console.Write(""Enter your question about the document (or 'new' for a new image, 'exit' to quit): "");
                string question;
                do
                {
                    question = Console.ReadLine() ?? """";
                } while (String.IsNullOrEmpty(question));

                if (question.ToLower() == ""new"")
                    break;
                if (question.ToLower() == ""exit"")
                    return;

                Console.WriteLine(""Response:"");
                _ = await AnalyzeDocument(question);
                Console.WriteLine(""\nEnd of response."");
            }
        }
    }

    static async Task<string> AnalyzeDocument(string question)
    {
        var requestBody = new
        {
            model = ""llava:13b-v1.6"",
            prompt = $""Analyze this invoice image carefully. Pay close attention to all numerical values, especially totals and subtotals. If the question is about a total or sum, make sure to double-check your calculation. After your analysis, provide a clear, concise answer to this specific question: {question}"",
            images = new[] { imageBase64 },
            stream = true
        };

        var content = new StringContent(JsonSerializer.Serialize(requestBody), Encoding.UTF8, ""application/json"");

        try
        {
            HttpResponseMessage response = await client.PostAsync(""http://localhost:11434/api/generate"", content);
            response.EnsureSuccessStatusCode();

            using (var reader = new StreamReader(await response.Content.ReadAsStreamAsync()))
            {
                StringBuilder fullResponse = new StringBuilder();
                string? line;
                while ((line = await reader.ReadLineAsync()) != null)
                {
                    if (string.IsNullOrWhiteSpace(line)) continue;

                    try
                    {
                        using (JsonDocument doc = JsonDocument.Parse(line))
                        {
                            JsonElement root = doc.RootElement;
                            if (root.TryGetProperty(""response"", out JsonElement responseElement))
                            {
                                string responsePart = responseElement.GetString() ?? """";
                                fullResponse.Append(responsePart);
                                Console.Write(responsePart); // Print each part as it's received
                            }
                            if (root.TryGetProperty(""done"", out JsonElement doneElement) && doneElement.GetBoolean())
                            {
                                break;
                            }
                        }
                    }
                    catch (JsonException)
                    {
                        Console.WriteLine($""Failed to parse JSON: {line}"");
                    }
                }
                return fullResponse.ToString();
            }
        }
        catch (HttpRequestException e)
        {
            return $""Error: {e.Message}"";
        }
    }
}

Unfortuntely the results are really bad and are mostly hallucinations. Sometimes the LLM complains that it would need a clearer view of the document to answer the questions, i tried downscaling the image, but it still did not work, is there maybe a way to process the image in multiple chunks.
","I cannot comment yet so i will be answering instead.
I have tried using Llava for Policies.pdf and stuff for RAG FAQ chatbot, however that multimodal model have also said the same thing to me, either the image is not good enough or it just takes a guess. It is able to understand images though, like dogs, or graphs (in my case it just tells me the image is a graph but cannot give details about it). Even when upscaled with pdf to image to upscaler, it still say the same thing, but can now identify more text on the image.
Ultimately to reach some weird deadline i threw that Model and implemented a pre-processing, PDF to Image to OCR. And yes it makes Llava redundant in here. But you can use other models to process the text output of OCR to reach the same ""Document Understanding"" task.
Another thing: PDF Documents are a pain, some of them may be all text and can be scraped easily with already available PDFpackages. But some may contain Images, or just all Scanned Images of the actual printed document. That is where Image to OCR will be of use to you. You can try other vision models or OCR packages but currently from all i've tested winocr works best for image text extraction.(yes, its using the OCR engine of windows snipping tool)
",large-language-model
why does shuffling training data for cross validation increase performance,"I am working on unbalanced dataset and I noticed that strangely if I shuffle the data during cross validation I get a high value of the f1 score while if i do not shuffle it f1 is low. Here is the function I use for cross validation:
def train_cross_v(md,df_train,n_folds=5,shuffl=False):

        X,y=df_train.drop([variable],axis=1),df_train[variable]
    
        cv =StratifiedKFold(n_splits=n_folds,shuffle=shuffl)

        scores = cross_val_score(md,X,y, scoring='f1', cv=cv, n_jobs=-1)

        y_pred=cross_val_predict(md,X,y, cv=cv, n_jobs=-1)
        print(' f1: ',scores,np.mean(scores))
        print(confusion_matrix(y_pred,y))
        return np.mean(scores)

Now shuffling I get f1 around 0.82:
nfolds=5
train_cross_v(XGBClassifier(),df_train,n_folds=nfolds,shuffl=True)
f1:  [0.81469793 0.82076749 0.82726257 0.82379249 0.82484862] 0.8222738195197493
[[23677  2452]
[ 1520  9126]]
0.8222738195197493

While not shuffling leads to:
nfolds=5
train_cross_v(XGBClassifier(),df_train,n_folds=nfolds,shuffl=False) 

f1:  [0.67447073 0.55084022 0.4166443  0.52759421 0.64819164] 0.5635482198057791
[[21621  5624]
[ 3576  5954]]
0.5635482198057791

As I understand it, shuffling is preferred to assess the real performance of the model as it allows us to neglect any dependencies related to the ordering of the data, and usually the post shuffling value of the performance metric is lower than that without shuffling. In my case however the behavior is the exact opposite and I get a high value if I shuffle, and the values ​​of the predictions on the test set remain unchanged. What could be the problem here?
","Because the order of your data is important.
Let's consider the following example:

Suppose we have completely balanced labels:

[0, 1, 0, 1, 0, 1, 0, 1, 0, ...]

And the features matrix that matches the labels, i.e:

[
[0, 1, 0, 1, ..],
]


Suppose the first 25% of the data are noisy and have incorrect labels:

n_noisy = int(n_examples * 0.25)
X[:n_noisy] = 1 - X[:n_noisy]

So we have:
[25% noisy, 25% normal, 25% normal, 25% normal]

Now we are using 2-fold cross validation (2 for simplicity).

4.1 without shuffling we will have the following metrics:
 f1:  [0.5 0. ] 0.25  # metrics for the second fold is zero

The first fold will be trained on the second half of data ([25% normal, 25% normal]) which have no noise in it and tested on the first half ([25%noisy, 25% normal]) which have 50% of noise in it which results in f1=0.5.
The second fold will be trained on the first half of data which were inverted as a result f1=0
4.2 with shuffling:
 f1:  [0.74903475 0.75103734] 0.7500360467165447

As expected we have f1=75% because 25% are noise.
Source code:
from xgboost import XGBClassifier
import numpy as np
from sklearn.model_selection import StratifiedKFold, cross_val_score, cross_val_predict
from sklearn.metrics import confusion_matrix


def train_cross_v(md, X, y, n_folds=5, shuffl=False):

    cv = StratifiedKFold(n_splits=n_folds, shuffle=shuffl)

    scores = cross_val_score(md, X, y, scoring=""f1"", cv=cv, n_jobs=-1)

    y_pred = cross_val_predict(md, X, y, cv=cv, n_jobs=-1)
    print("" f1: "", scores, np.mean(scores))
    print(confusion_matrix(y_pred, y))
    return np.mean(scores)


nfolds = 2
n_examples = 1000

y = np.tile([0, 1], 500)
X = y.copy().reshape(-1, 1)

n_noisy = int(n_examples * 0.25)
X[:n_noisy] = 1 - X[:n_noisy]


train_cross_v(XGBClassifier(), X, y, n_folds=nfolds, shuffl=False)
train_cross_v(XGBClassifier(), X, y, n_folds=nfolds, shuffl=True)

So the order matters and shuffling can both increase or decrease performance.
",cross-validation
calculate the average of each metric in cross validation,"I'm trying to calculate some metrics using StratifiedKFold cross validation.
skfold = StratifiedKFold(n_splits=5, random_state=42, shuffle=True)

dtc_score = cross_validate(models[0], X, y, scoring=('accuracy', 'precision', 'recall', 'f1'), cv=skfold, n_jobs=-1, verbose=1)
rfc_score = cross_validate(models[1], X, y, scoring=('accuracy', 'precision', 'recall', 'f1'), cv=skfold, n_jobs=-1, verbose=1)
abc_score = cross_validate(models[2], X, y, scoring=('accuracy', 'precision', 'recall', 'f1'), cv=skfold, n_jobs=-1, verbose=1)
etc_score = cross_validate(models[3], X, y, scoring=('accuracy', 'precision', 'recall', 'f1'), cv=skfold, n_jobs=-1, verbose=1)
gbc_score = cross_validate(models[4], X, y, scoring=('accuracy', 'precision', 'recall', 'f1'), cv=skfold, n_jobs=-1, verbose=1)
bgc_score = cross_validate(models[5], X, y, scoring=('accuracy', 'precision', 'recall', 'f1'), cv=skfold, n_jobs=-1, verbose=1)
knn_score = cross_validate(models[6], X, y, scoring=('accuracy', 'precision', 'recall', 'f1'), cv=skfold, n_jobs=-1, verbose=1)
logreg_score = cross_validate(models[7], X, y, scoring=('accuracy', 'precision', 'recall', 'f1'), cv=skfold, n_jobs=-1, verbose=1)
nb_score = cross_validate(models[8], X, y, scoring=('accuracy', 'precision', 'recall', 'f1'), cv=skfold, n_jobs=-1, verbose=1)
svm_score = cross_validate(models[9], X, y, scoring=('accuracy', 'precision', 'recall', 'f1'), cv=skfold, n_jobs=-1, verbose=1)
xgb_score = cross_validate(models[10], X, y, scoring=('accuracy', 'precision', 'recall', 'f1'), cv=skfold, n_jobs=-1, verbose=1)
mlp_score = cross_validate(models[11], X, y, scoring=('accuracy', 'precision', 'recall', 'f1'), cv=skfold, n_jobs=-1, verbose=1)

After that I put the results of each calculation into the dataframe, but the results don't match what I want.
cv_result = [
    dtc_score, rfc_score, abc_score, etc_score, gbc_score, bgc_score, 
    knn_score, logreg_score, nb_score, svm_score, xgb_score, mlp_score]


df_cv_result = pd.DataFrame(cv_result, index=model_name)
df_cv_result

The result is like this:





fit_time
score_time
test_accuracy
test_precision
test_recall
test_f1




DecisionTreeClassifier
[0.06297850608825684, 0.06297850608825684, 0.1...
[0.025590181350708008, 0.025590181350708008, 0...
[0.783008658008658, 0.7943722943722944, 0.7662...
[0.7193229901269393, 0.7398843930635838, 0.708...
[0.7162921348314607, 0.7191011235955056, 0.668...
[0.7178043631245602, 0.7293447293447292, 0.687...


RandomForestClassifier
[1.759207010269165, 1.774831771850586, 1.75920...
[0.10936832427978516, 0.10936856269836426, 0.1...
[0.8257575757575758, 0.8138528138528138, 0.806...
[0.7809798270893372, 0.7697947214076246, 0.769...
[0.7612359550561798, 0.7373595505617978, 0.709...
[0.7709815078236132, 0.7532281205164992, 0.738...


AdaBoostClassifier
[0.7297384738922119, 0.7453627586364746, 0.721...
[0.07508277893066406, 0.07508540153503418, 0.0...
[0.8235930735930735, 0.8295454545454546, 0.818...
[0.796923076923077, 0.7976011994002998, 0.8006...
[0.7275280898876404, 0.7471910112359551, 0.705...
[0.7606461086637298, 0.7715736040609138, 0.749...


ExtraTreesClassifier
[1.8575339317321777, 1.888782024383545, 1.8731...
[0.12499260902404785, 0.12499213218688965, 0.1...
[0.808982683982684, 0.8008658008658008, 0.7916...
[0.760522496371553, 0.7478386167146974, 0.7599...
[0.7359550561797753, 0.7289325842696629, 0.671...
[0.7480371163454677, 0.7382645803698435, 0.712...


GradientBoostingClassifier
[2.078220844268799, 2.150218963623047, 2.11822...
[0.02400040626525879, 0.027779102325439453, 0....
[0.8365800865800865, 0.8344155844155844, 0.825...
[0.8005865102639296, 0.8038922155688623, 0.791...
[0.7668539325842697, 0.7542134831460674, 0.742...
[0.7833572453371592, 0.7782608695652172, 0.766...


BaggingClassifier
[0.486358642578125, 0.486358642578125, 0.47073...
[0.015625953674316406, 0.039700984954833984, 0...
[0.8143939393939394, 0.808982683982684, 0.7927...
[0.786046511627907, 0.7765793528505393, 0.7683...
[0.7120786516853933, 0.7078651685393258, 0.661...
[0.7472365512159176, 0.7406318883174137, 0.710...


KNeighborsClassifier
[0.01562809944152832, 0.01562809944152832, 0.0...
[0.9189648628234863, 0.9033389091491699, 0.918...
[0.8041125541125541, 0.8122294372294372, 0.803...
[0.7708978328173375, 0.77526395173454, 0.76687...
[0.699438202247191, 0.7219101123595506, 0.7022...
[0.7334315169366716, 0.7476363636363637, 0.733...


LogisticRegression
[0.12395191192626953, 0.14353656768798828, 0.1...
[0.01958465576171875, 0.04000663757324219, 0.0...
[0.8079004329004329, 0.8181818181818182, 0.811...
[0.7856, 0.7955974842767296, 0.793214862681744...
[0.6896067415730337, 0.7106741573033708, 0.689...
[0.7344801795063575, 0.7507418397626112, 0.737...


GaussianNB
[0.02400040626525879, 0.02400040626525879, 0.0...
[0.02399587631225586, 0.02399587631225586, 0.0...
[0.7738095238095238, 0.7797619047619048, 0.764...
[0.7963709677419355, 0.8105906313645621, 0.812...
[0.5547752808988764, 0.5589887640449438, 0.505...
[0.6539735099337749, 0.6616791354945969, 0.623...


SVC
[5.1321375370025635, 5.216028690338135, 4.9741...
[2.7087018489837646, 2.7156307697296143, 2.633...
[0.8327922077922078, 0.8295454545454546, 0.824...
[0.7933042212518195, 0.794074074074074, 0.7833...
[0.7654494382022472, 0.7528089887640449, 0.751...
[0.7791279485346676, 0.7728911319394376, 0.767...


XGBClassifier
[2.78363299369812, 2.78363299369812, 2.7516334...
[0.03450345993041992, 0.03450345993041992, 0.0...
[0.8262987012987013, 0.8208874458874459, 0.808...
[0.7879234167893961, 0.7822222222222223, 0.775...
[0.7514044943820225, 0.7415730337078652, 0.707...
[0.7692307692307693, 0.7613554434030282, 0.740...


MLPClassifier
[20.106656074523926, 20.14053773880005, 19.947...
[0.023995399475097656, 0.0240020751953125, 0.0...
[0.8327922077922078, 0.8295454545454546, 0.819...
[0.7899280575539568, 0.7815602836879433, 0.773...
[0.7710674157303371, 0.773876404494382, 0.7528...
[0.7803837953091683, 0.7776993648553283, 0.762...




I don't want to include every k-fold and just want to calculate the average. How do I calculate the average for each metric?
","You can simply do as follow to average your score:
cv_result = [
    dtc_score, rfc_score, abc_score, etc_score, gbc_score, bgc_score, 
    knn_score, logreg_score, nb_score, svm_score, xgb_score, mlp_score]

results = [{k : v.mean() for k, v in result.items()} for result in cv_result]
df_cv_result = pd.DataFrame(results, index=model_name)

Note: you might want to calculate the standard deviation as it might help you choose to best model.
EDIT:
It might not be the most optimal solution but you could do as follows to add both mean and std:
results = []
for result in cv_result:
    tmp = dict()
    for k, v in result.items():
        tmp.update({f'{k}_mean()': v.mean(), f'{k}_std' : v.std()})
    results.append(tmp)
df_cv_result = pd.DataFrame(results, index=model_name)

",cross-validation
how do i get rid of this errortraceback most recent call last file quotltstringgtquot line 1 in ltmodulegt file quotcprogram,"I keep trying to start my python backend with  uvicorn main:app --reload but I keep getting this error:
INFO:     Will watch for changes in these directories: ['C:\\Users\\darkg\\OneDrive\\Desktop\\loginpage\\FastAP_BackEnd\\books']
INFO:     Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit)
INFO:     Started reloader process [532] using StatReload
Traceback (most recent call last):
  File ""<string>"", line 1, in <module>
  File ""C:\Program Files\WindowsApps\PythonSoftwareFoundation.Python.3.10_3.10.1776.0_x64__qbz5n2kfra8p0\lib\multiprocessing\spawn.py"", line 116, in spawn_main
    exitcode = _main(fd, parent_sentinel)
  File ""C:\Program Files\WindowsApps\PythonSoftwareFoundation.Python.3.10_3.10.1776.0_x64__qbz5n2kfra8p0\lib\multiprocessing\spawn.py"", line 126, in _main
    self = reduction.pickle.load(from_parent)
  File ""C:\Users\darkg\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\LocalCache\local-packages\Python310\site-packages\uvicorn\__init__.py"", line 1, in <module>
    from uvicorn.config import Config
  File ""C:\Users\darkg\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\LocalCache\local-packages\Python310\site-packages\uvicorn\config.py"", line 1, in <module>
    import asyncio
  File ""C:\Program Files\WindowsApps\PythonSoftwareFoundation.Python.3.10_3.10.1776.0_x64__qbz5n2kfra8p0\lib\asyncio\__init__.py"", line 8, in <module>
    from .base_events import *
  File ""C:\Program Files\WindowsApps\PythonSoftwareFoundation.Python.3.10_3.10.1776.0_x64__qbz5n2kfra8p0\lib\asyncio\base_events.py"", line 18, in <module>
    import concurrent.futures
  File ""C:\Program Files\WindowsApps\PythonSoftwareFoundation.Python.3.10_3.10.1776.0_x64__qbz5n2kfra8p0\lib\concurrent\futures\__init__.py"", line 8, in <module>
    from concurrent.futures._base import (FIRST_COMPLETED,
  File ""C:\Program Files\WindowsApps\PythonSoftwareFoundation.Python.3.10_3.10.1776.0_x64__qbz5n2kfra8p0\lib\concurrent\futures\_base.py"", line 7, in <module>
    import logging
  File ""C:\Program Files\WindowsApps\PythonSoftwareFoundation.Python.3.10_3.10.1776.0_x64__qbz5n2kfra8p0\lib\logging\__init__.py"", line 26, in <module>
    import sys, os, time, io, re, traceback, warnings, weakref, collections.abc
  File ""C:\Program Files\WindowsApps\PythonSoftwareFoundation.Python.3.10_3.10.1776.0_x64__qbz5n2kfra8p0\lib\traceback.py"", line 5, in <module>
    import linecache
  File ""C:\Program Files\WindowsApps\PythonSoftwareFoundation.Python.3.10_3.10.1776.0_x64__qbz5n2kfra8p0\lib\linecache.py"", line 11, in <module>
    import tokenize
  File ""C:\Program Files\WindowsApps\PythonSoftwareFoundation.Python.3.10_3.10.1776.0_x64__qbz5n2kfra8p0\lib\tokenize.py"", line 35, in <module>
    from token import *
  File ""C:\Users\darkg\OneDrive\Desktop\loginpage\FastAP_BackEnd\books\.\token.py"", line 5, in <module>
    from . import schemas
ImportError: attempted relative import with no known parent package


I tried starting it with   python -m uvicorn main:app --reload
but I get this error  instead

Traceback (most recent call last):
  File ""C:\Program Files\WindowsApps\PythonSoftwareFoundation.Python.3.10_3.10.1776.0_x64__qbz5n2kfra8p0\lib\runpy.py"", line 187, in _run_module_as_main
    mod_name, mod_spec, code = _get_module_details(mod_name, _Error)
  File ""C:\Program Files\WindowsApps\PythonSoftwareFoundation.Python.3.10_3.10.1776.0_x64__qbz5n2kfra8p0\lib\runpy.py"", line 146, in _get_module_details
    return _get_module_details(pkg_main_name, error)
  File ""C:\Program Files\WindowsApps\PythonSoftwareFoundation.Python.3.10_3.10.1776.0_x64__qbz5n2kfra8p0\lib\runpy.py"", line 110, in _get_module_details
    __import__(pkg_name)
  File ""C:\Users\darkg\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\LocalCache\local-packages\Python310\site-packages\uvicorn\__init__.py"", line 1, in <module>
    from uvicorn.config import Config
  File ""C:\Users\darkg\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\LocalCache\local-packages\Python310\site-packages\uvicorn\config.py"", line 1, in <module>
    import asyncio
  File ""C:\Program Files\WindowsApps\PythonSoftwareFoundation.Python.3.10_3.10.1776.0_x64__qbz5n2kfra8p0\lib\asyncio\__init__.py"", line 8, in <module>
    from .base_events import *
  File ""C:\Program Files\WindowsApps\PythonSoftwareFoundation.Python.3.10_3.10.1776.0_x64__qbz5n2kfra8p0\lib\asyncio\base_events.py"", line 18, in <module>
    import concurrent.futures
  File ""C:\Program Files\WindowsApps\PythonSoftwareFoundation.Python.3.10_3.10.1776.0_x64__qbz5n2kfra8p0\lib\concurrent\futures\__init__.py"", line 8, in <module>
    from concurrent.futures._base import (FIRST_COMPLETED,
  File ""C:\Program Files\WindowsApps\PythonSoftwareFoundation.Python.3.10_3.10.1776.0_x64__qbz5n2kfra8p0\lib\concurrent\futures\_base.py"", line 7, in <module>
    import logging
  File ""C:\Program Files\WindowsApps\PythonSoftwareFoundation.Python.3.10_3.10.1776.0_x64__qbz5n2kfra8p0\lib\logging\__init__.py"", line 26, in <module>
    import sys, os, time, io, re, traceback, warnings, weakref, collections.abc
  File ""C:\Program Files\WindowsApps\PythonSoftwareFoundation.Python.3.10_3.10.1776.0_x64__qbz5n2kfra8p0\lib\traceback.py"", line 5, in <module>
    import linecache
  File ""C:\Program Files\WindowsApps\PythonSoftwareFoundation.Python.3.10_3.10.1776.0_x64__qbz5n2kfra8p0\lib\linecache.py"", line 11, in <module>
    import tokenize
  File ""C:\Program Files\WindowsApps\PythonSoftwareFoundation.Python.3.10_3.10.1776.0_x64__qbz5n2kfra8p0\lib\tokenize.py"", line 35, in <module>
    from token import *
  File ""C:\Users\darkg\OneDrive\Desktop\loginpage\FastAP_BackEnd\books\token.py"", line 5, in <module>
    from . import schemas
ImportError: attempted relative import with no known parent package

Here is the main code
from . import models
from fastapi import FastAPI

from .database import engine
from .routers import books, users,authentication

app=FastAPI()
models.Base.metadata.create_all(engine)

app.include_router(books.router)
app.include_router(users.router)
app.include_router(authentication.router)




Here is my Code File Path If that helps solve the issue :
Code File Structure
I have tried everything but cant seem to find a way to fix this . I've redowloaded python. I've reinstalled every package, ive added paths to my packages, but nothing seems to work. Any help would be greatly appreciated. I am completely lost
","Python doesn't know that the directory containing your modules (including main.py) is a package, therefore it can't properly locate imports. Create an empty __init__.py file in this directory to mark it as a regular package.
A rule of thumb is to mark all your directories that contain python modules as packages with this method.
There's another type of Python packages that may make your code sometimes work: namespace packages. They don't have to contain an __init__.py file. I recommend to just avoid them, unless you understand exactly what they're for and why you need them. [Doc reference]
For a quick reference, search through the FastAPI documentation for ""project file structure"" (it may be called something similar).
",python
stacked bar with values,"    +------------+------+------+---------+
    | promo_name | in   | out  | model   |
    +------------+------+------+---------+
    | A          | 0,87 | 0,13 | plan    |
    +------------+------+------+---------+
    | B          | 0,77 | 0,23 | plan    |
    +------------+------+------+---------+
    | C          | 0,54 | 0,46 | plan    |
    +------------+------+------+---------+
    | A          | 0,59 | 0,41 | predict |
    +------------+------+------+---------+
    | B          | 0,50 | 0,50 | predict |
    +------------+------+------+---------+
    | C          | 0,46 | 0,54 | predict |
    +------------+------+------+---------+

Hello. I have a dataframe as in the table above, I would like to draw a stacked barplot with values, for example:

But so far it only works like this, my code is below:
ratio.set_index(['promo_name', 'model'], inplace=True)
ratio = ratio.reindex(index = [('A', 'plan'), ('A', 'predict'),
                              ('B', 'plan'), ('B', 'predict'),
                              ('C', 'plan'), ('C', 'predict')])
plt.rcParams[""figure.figsize""] = (12,5)
ratio.plot(kind='bar', stacked=True)
plt.xticks(rotation=30)
plt.show()

But until I get the wrong thing, how do I fix it?

","Using matplotlib you can easily play around with the locations and widths of the bars: 
 import matplotlib.pyplot as plt

# plan, predict
Aout = (0.87, 0.59)
Ain = (0.13, 0.41)
Bout = (0.77, 0.50)
Bin = (0.23, 0.50)  
Cout = (0.54, 0.46)
Cin = (0.46, 0.54)
width = 1.0       # the width of the bars

A_positions = [0, 1]  # Positions for A bars
p1A = plt.bar([0, 1], (1.0, 1.0), width, color='g', label='out')
p2A = plt.bar([0, 1], Ain, width, color='b', label='in')

B_positions = [3, 4]  # Positions for B bars
p1B = plt.bar(B_positions, (1.0, 1.0), width, color='g')
p2B = plt.bar(B_positions, Bin, width, color='b')

C_positions = [6, 7]  # Positions for C bars
p1C = plt.bar(C_positions, (1.0, 1.0), width, color='g')
p2C = plt.bar(C_positions, Cin, width, color='b')

positions = A_positions + B_positions + C_positions  # All together for ticks
plt.xticks(positions, ('A (plan)', 'A (predict)', 'B (plan)', 'B (predict)', 'C (plan)', 'C (predict)'))
plt.xticks(rotation=70)
plt.yticks([1, 0])
plt.legend()
plt.tight_layout()

And add text labels (you can play around with the location):
plt.text(A_positions[0] - 0.25, Ain[0] / 2, str(Ain[0]))
plt.text(A_positions[0] - 0.25, Aout[0] / 2, str(Aout[0]))
plt.text(A_positions[1] - 0.25, Ain[1] / 2, str(Ain[1]))
plt.text(A_positions[1] - 0.25, Aout[1] / 2 + 0.5, str(Aout[1]))

plt.text(B_positions[0] - 0.25, Bin[0] / 2, str(Bin[0]))
plt.text(B_positions[0] - 0.25, Bout[0] / 2, str(Bout[0]))
plt.text(B_positions[1] - 0.25, Bin[1] / 2, str(Bin[1]))
plt.text(B_positions[1] - 0.25, Bout[1] / 2 + 0.5, str(Bout[1]))


plt.text(C_positions[0] - 0.25, Cin[0] / 2, str(Cin[0]))
plt.text(C_positions[0] - 0.25, Cout[0] / 2 + 0.5, str(Cout[0]))
plt.text(C_positions[1] - 0.25, Cin[1] / 2, str(Cin[1]))
plt.text(C_positions[1] - 0.25, Cout[1] / 2 + 0.5, str(Cout[1]))


",matplotlib
numpy grayscale image to black and white,"I use the MNIST dataset that contains 28x28 grayscale images represented as numpy arrays with 0-255 values. I'd like to convert images to black and white only (0 and 1) so that pixels with a value over 128 will get the value 1 and pixels with a value under 128 will get the value 0.
Is there a simple method to do so?
","Yes. Use (arr > 128) to get a boolean mask array of the same shape as your image, then .astype(int) to cast the bools to ints:
>>> import numpy as np
>>> arr = np.random.randint(0, 255, (5, 5))
>>> arr
array([[153, 167, 141,  79,  58],
       [184, 107, 152, 215,  69],
       [221,  90, 172, 147, 125],
       [ 93,  35, 125, 186, 187],
       [ 19,  72,  28,  94, 132]])
>>> (arr > 128).astype(int)
array([[1, 1, 1, 0, 0],
       [1, 0, 1, 1, 0],
       [1, 0, 1, 1, 0],
       [0, 0, 0, 1, 1],
       [0, 0, 0, 0, 1]])

",numpy
using tfkerasmetricsr2score results in an error in tensorflow,"I'm making a regression model with Tensorflow, but when I use tf.keras.metrics.R2Score() as a metric, it fails with ValueError: Tensor conversion requested dtype int32 for Tensor with dtype float32: <tf.Tensor: shape=(), dtype=float32, numpy=0.0> after the first epoch. (But works fine up until then) However, if I use a different metric (tf.keras.metrics.RootMeanSquaredError(), it works fine.
import pandas as pd

weather_states = pd.read_sql(""SELECT stations.id, stations.capacity_kw, start, wind_speed_10m, wind_direction_10m, wind_speed_80m, wind_direction_80m, wind_speed_180m, wind_direction_180m FROM stations INNER JOIN weather_states ON stations.id = weather_states.station WHERE weather_states.source = 'openmeteo_forecast/history/best' AND stations.source = 'wind'"", db_client)
 
grid_states = pd.read_sql(""SELECT start, wind FROM grid_states"", db_client)
 
def create_x_y(df: tuple[Any, pd.DataFrame]):
    start = df[1][""start""].iloc[0]
    res = df[1].sort_values(""id"").drop([""id"", ""start""], axis=1)
    temp_wind = grid_states.loc[grid_states[""start""] == start][""wind""].to_list()
    wind_kw = temp_wind if len(temp_wind) >= 1 else None
    res_flat_df = pd.DataFrame(res.to_numpy().reshape((1, -1)))
    res_flat_df[""wind_kw""] = wind_kw
    return res_flat_df
 
data = pd.concat(map(create_x_y, weather_states.groupby(""start""))).dropna()
from sklearn.model_selection import train_test_split
 
 
data = data.astype(""float32"")
train, test = train, test = train_test_split(data.dropna(), test_size=0.2)
 
train_y = train.pop(""wind_kw"")
train_x = train
 
 
test_y = test.pop(""wind_kw"")
test_x = test
 
norm = tf.keras.layers.Normalization()
norm.adapt(train_x)
 
model = tf.keras.Sequential([
    norm,
    tf.keras.layers.Dense(16, activation=""linear""),
    tf.keras.layers.Dropout(0.3),
    tf.keras.layers.Dense(1, activation=""linear""),
])
 
 
model.compile(
    optimizer=tf.keras.optimizers.legacy.Adam(0.001),
    metrics=[tf.keras.metrics.R2Score(dtype=tf.float32)],
    loss=tf.keras.losses.MeanSquaredError(),
)
 
model.fit(train_x, train_y, epochs=7, batch_size=2)
 
tf.keras.models.save_model(model, 'wind.keras')

print(data.describe())
              0            1            2            3            4  ...          241          242          243          244      wind_kw
count    1896.0  1896.000000  1896.000000  1896.000000  1896.000000  ...  1896.000000  1896.000000  1896.000000  1896.000000  1896.000000
mean   144000.0     4.315717   189.610759     5.791377   193.830169  ...     3.881292   145.420359     4.572205   143.642405  1292.576958
std         0.0     2.482439   113.178764     2.926497   113.685887  ...     2.612259    93.293471     2.775681    94.721086   611.333721
min    144000.0     0.100000     1.000000     0.100000     1.000000  ...     0.100000     2.000000     0.000000     1.000000    34.263000
25%    144000.0     2.110000    88.000000     3.487500    90.000000  ...     1.900000    67.000000     2.500000    63.000000   793.109500
50%    144000.0     4.110000   199.000000     5.500000   231.000000  ...     3.075000   137.000000     3.940000   135.000000  1251.590000
75%    144000.0     6.220000   291.000000     7.882500   294.000000  ...     5.502500   205.000000     6.082500   205.000000  1761.926750
max    144000.0    11.670000   360.000000    15.210000   360.000000  ...    14.460000   360.000000    16.980000   360.000000  3008.125000

print(type(data))
#<class 'pandas.core.frame.DataFrame'>
print(data.dtypes)
#0          float32
#1          float32
#2          float32
#3          float32
#4          float32
#            ...   
#241        float32
#242        float32
#243        float32
#244        float32
#wind_kw    float32
#Length: 246, dtype: object
print(data.shape)
#(1896, 246)

I can't seem to find any information online about this error when using R2Score- any ideas as to what could be the issue?
","It is a bug of TensorFlow 2.13.
Use TensorFlow 2.15 or higher.
",tensorflow
using curl to view contents of file,"I am trying to figure out the command to display the contents of a file (text file) in command prompt using cURL and get. I had the right command but now I can't seem to get the correct syntax right to display the proper file information:
curl -g host:port@path to file
curl -d filename-g host:port
I have tried many variations of the above and have consulted the cURL manual:
https://curl.haxx.se/docs/manpage.html
And other SO resources...Just need a little boost here as I had the correct syntax but now have spent several hours trying to get the proper terminology. 
","I use ...
curl http://example.com/file

Or eventually
 curl http://example.com:8888/file

",prompt
matplotlib plots lose transparency when saving as pseps,"I'm having an issue with attempting to save some plots with transparent ellipsoids on them if I attempt to save them with .ps/.eps extensions. 
Here's the plot saved as a .png:

If I choose to save it as a .ps/.eps here is what it looks like:

How I got around this, was to use ImageMagick to convert the original png to a ps. The only problem is that the image in png format is about 90k, and it becomes just under 4M after conversion. This is not good since I have a lot of these images, and it will take too much time to compile my latex document. Does anyone have a solution to this?
","The problem is that eps does not support transparencies natively.
There are few options:

rasterize the image and embed in a eps file (like @Molly suggests) or exporting to pdf and converting with some external tool (like gs) (which usually relies as well on rasterization)
'mimic' transparency, giving a colour that looks like the transparent one on a given background.

I discussed this for sure once on the matplotlib mailing list, and I got the suggestion to rasterize, which is not feasible as you get either pixellized or huge figures. And they don't scale very nicely when put into, e.g., a publication.
I personally use the second approach, and although not ideal, I found it good enough. I wrote a small python script that implements the algorithm from this SO post to obtain a solid RGB representation of a colour with a give transparency
EDIT
In the specific case of your plot try to use the zorder keyword to order the parts plotted. Try to use zorder=10 for the blue ellipse, zorder=11 for the green and zorder=12 for the hexbins. 
This way the blue should be below everything, then the green ellipse and finally the hexbins. And the plot should be readable also with solid colors. And if you like the shades of blue and green that you have in png, you can try to play with mimic_alpha.py.
EDIT 2
If you are 100% sure that you have to use eps, there are a couple of workarounds that come to my mind (and that are definitely uglier than your plot): 

Just draw the ellipse borders on top of the hexbins. 
Get centre and amplitude of each hexagon, (possibly discard all zero bins) and make a scatter plot using the same colour map as in hexbin and adjusting the marker size and shape as you like. You might want to redraw the ellipses borders on top of that

",matplotlib
error quotbus errorquot running the simplest example on hugging face transformers pipeline macos m1,"I'm trying to follow the quick tour example here: https://huggingface.co/docs/transformers/quicktour
and i'm getting a ""bus error"".
My env is:

MacOS Sonoma 14.7, Apple M1 Max chip
Python 3.11.5
pip install transformers datasets evaluate accelerate
pip install tf-keras

Running this code:
from transformers import pipeline

classifier = pipeline(""sentiment-analysis"")
result = classifier(""We are very happy to show you the 🤗 Transformers library."")
print(result)

And getting this result:

No model was supplied, defaulted to
distilbert/distilbert-base-uncased-finetuned-sst-2-english and
revision 714eb0f
(https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).
Using a pipeline without specifying a model name and revision in
production is not recommended. [1]
92883 bus error  python main.py

Any ideas?
Thanks for your help.
","using device=0 (the first GPU) solved this:
classifier = pipeline(""sentiment-analysis"", device=0)
",huggingface-transformers
how do i format an openai api request in swift,"I need to use the OpenAI API and am struggling to format the request. I started by referencing the curl code that is listed in the OpenAI API playground, and this code worked in my terminal. That request is here:
curl https://api.openai.com/v1/chat/completions \
  -H ""Content-Type: application/json"" \
  -H ""Authorization: Bearer $OPENAI_API_KEY"" \
  -d '{
  ""model"": ""gpt-4o"",
  ""messages"": [
    {
      ""role"": ""user"",
      ""content"": [
        {
          ""type"": ""text"",
          ""text"": ""Write a haiku about weightlifting""
        }
      ]
    }
  ],
  ""temperature"": 1,
  ""max_tokens"": 2048,
  ""top_p"": 1,
  ""frequency_penalty"": 0,
  ""presence_penalty"": 0,
  ""response_format"": {
    ""type"": ""text""
  }
}'

Then, I started working on translating that request into swift instead of curl. I've been referencing some YouTube tutorials, and here is my code so far.
let openAiUrl = URL(string: ""https://api.openai.com/v1/chat/completions"")!
var request = URLRequest(url: openAiUrl)
request.setValue(""application/json"", forHTTPHeaderField: ""Content-Type"")
request.addValue(""Bearer \(openAiApiKey)"", forHTTPHeaderField: ""Authorization"")
request.httpMethod = ""POST""
let httpBody: [String: Any] = [
    ""messages"" : [
        {
            ""role"": ""user"", //Error: consecutive statements on a line must be separated by ';'
            ""content"": ""Write a haiku about weightlifting""
        }
    ],
    ""model"": ""gpt-4o"",
    ""max_tokens"" : 100,
    ""temperature"": String(temperature),
    ""frequency_penalty"": 0,
    ""presence_penalty"": 0,
    ""response_format"": { //Error: consecutive statements on a line must be separated by ';'
        ""type"": ""text""
    }
]

I am getting the error that says ""Consecutive statements on a line must be separated by ';'"" on the two lines that are using curly braces.
What do I need to do to format the API request? The temperature variable is a Double that is currently set to 1.0.
","Try this approach using a JSONEncoder to encode the json parameters, such as:
            let body = """"""
{
                    ""messages"" : [
                        {
                            ""role"": ""user"",
                            ""content"": ""Write a haiku about weightlifting""
                        }
                    ],
                    ""model"": ""gpt-4o"",
                    ""max_tokens"" : 100,
                    ""temperature"": 1.0,
                    ""frequency_penalty"": 0,
                    ""presence_penalty"": 0,
                    ""response_format"": { 
                        ""type"": ""text""
                    }
}
""""""
            
            do {
                request.httpBody = try JSONEncoder().encode(body)

                let (responseData, response) = try await URLSession.shared.data(for: request)

                print(""-----> \n \(String(data: responseData, encoding: .utf8) as AnyObject) \n"")

            } catch {
                print(""---> error: \(error)"")
            }

EDIT-1
Here is my test code using SwiftUI. Since I don't have a key I cannot fully test this code, but it compiles and gives some info about the response.
import SwiftUI

struct ContentView: View {
    let openAiApiKey = ""for testing""
    
    var body: some View {
        Text(""testing"")
            .task {
                await doPost()
            }
    }
    
    func doPost() async {
        let openAiUrl = URL(string: ""https://api.openai.com/v1/chat/completions"")!
        
        var request = URLRequest(url: openAiUrl)
        request.setValue(""application/json"", forHTTPHeaderField: ""Content-Type"")
        request.addValue(""Bearer \(openAiApiKey)"", forHTTPHeaderField: ""Authorization"")
        request.httpMethod = ""POST""
        
        let body = """"""
{
""messages"": [
{
""role"": ""user"",
""content"": ""Write a haiku about weightlifting""
}
],
""model"": ""gpt-4o"",
""max_tokens"" : 100,
""temperature"": 1.0,
""frequency_penalty"": 0,
""presence_penalty"": 0,
""response_format"": { 
    ""type"": ""text""
}
}
""""""
        do {
            request.httpBody = try JSONEncoder().encode(body)
            
            let (responseData, response) = try await URLSession.shared.data(for: request)
            print(""-----> responseData \n \(String(data: responseData, encoding: .utf8) as AnyObject) \n"")
        }
        catch { print(error) }
    }
}

EDIT-2
another alternative,
struct ContentView: View {
    let openAiApiKey = ""for testing""
    
    var body: some View {
        Text(""testing"")
            .task {
                let url = URL(string: ""https://api.openai.com/v1/chat/completions"")!
                var request = URLRequest(url: url)
                request.httpMethod = ""POST""
                request.setValue(""application/json"", forHTTPHeaderField: ""Content-Type"")
                request.setValue(""Bearer \(openAiApiKey)"", forHTTPHeaderField: ""Authorization"")
                
                let requestBody: [String: Any] = [
                    ""model"": ""gpt-4o"",
                    ""messages"": [
                        [
                            ""role"": ""user"",
                            ""content"": [
                                [
                                    ""type"": ""text"",
                                    ""text"": ""Write a haiku about weightlifting""
                                ]
                            ]
                        ]
                    ],
                    ""temperature"": 1,
                    ""max_tokens"": 2048,
                    ""top_p"": 1,
                    ""frequency_penalty"": 0,
                    ""presence_penalty"": 0,
                    ""response_format"": [
                        ""type"": ""text""
                    ]
                ]
                
                do {
                    request.httpBody = try JSONSerialization.data(withJSONObject: requestBody)
                    
                    let (responseData, response) = try await URLSession.shared.data(for: request)
                    print(""-----> responseData \n \(String(data: responseData, encoding: .utf8) as AnyObject) \n"")
                }
                catch { print(error) }
                
            }
    }
}

",chatgpt
select all c file in a directory on a makefile,"i've got a Makefile and i want to modify it, to select all c files in some directory instead of specify all c file separately.
Thanks, this is the code: (i will paste only the slice that regard the topic)
#MQTT PATH
MQTT_DIR = $(SAMPLE_DIR)/mqtt_lib

#TRSMIO PATH
TRSMIO_DIR = $(SAMPLE_DIR)/trsmio_lib

#UTIL PATH
UTIL_DIR = $(SAMPLE_DIR)/util_lib

# Location of the SDK ""startup.c"" file used to launch custom (sample) code.
SDK_CS = $(SDK_DIR)/startup.c


#DEPENDECIES OF MAINS TASK
MQTT_CS = $(MQTT_DIR)/libmqtt_util.c
MQTT_LIB = $(MQTT_DIR)/libmqtt.c
MQTT_CGI = $(MQTT_DIR)/libmqtt_cgi.c
MQTT_MGR = $(MQTT_DIR)/libmqtt_config_mgr.c
LIB_TIMER = $(UTIL_DIR)/lib_timer.c
TRSMIO_HOSTLINK = $(TRSMIO_DIR)/trsmio_hostlink.c
TRSMIO_HOSTLINK_CONFIG = $(TRSMIO_DIR)/trsmio_hostlink_config_mgr.c
LIB_NET_DEBUG = $(UTIL_DIR)/lib_netdebug.c
TASK_MGR = $(UTIL_DIR)/task_mgr.c

#MAIN TASKS
TRSMIO_TEST = $(SAMPLE_DIR)/trsmio_test.c
#MQTT_TEST = $(SAMPLE_DIR)/libmqtt_test.c


# Location and name of the sample source code file:
SAMPLE_CS = $(SAMPLE_DIR)/$(MAKECMDGOALS).c

# Location of the Evolution OS object module library, statically linked with 
# the sample code.
EVOS_LIB = $(SDK_DIR)/$(PLATFORM_DIR)/evolution.lib

EVOS_WEB_LIB = $(SDK_DIR)/$(PLATFORM_DIR)/evolution_with_web.lib
#EVOS_WEB_LIB = $(SDK_DIR)/$(PLATFORM_DIR)/evolution_no_web.lib

# All C source code files to be built and included in the image.
ALL_C_COMPILE_SOURCES =         \
    $(SDK_CS)                   \
    $(TASK_MGR)                 \
    $(LIB_NET_DEBUG)        \
    $(MQTT_CS)                  \
    $(MQTT_CGI)         \
    $(MQTT_LIB)                 \
    $(MQTT_MGR)                 \
    $(TRSMIO_HOSTLINK)          \
    $(TRSMIO_HOSTLINK_CONFIG)   \
    $(LIB_TIMER)            \
    $(TRSMIO_TEST)          \
    $(SAMPLE_CS)

   work/$(notdir $(SAMPLE_CS:.c=.o)): $(SAMPLE_CS)
@echo $<
@$(CC) $(CFLAGS) -c $< -o $@

work/$(notdir $(LIB_TIMER:.c=.o)): $(LIB_TIMER)
    @echo $<
    @$(CC) $(CFLAGS) -c $< -o $@

work/$(notdir $(TASK_MGR:.c=.o)): $(TASK_MGR)
    @echo $<
    @$(CC) $(CFLAGS) -c $< -o $@

work/$(notdir $(LIB_NET_DEBUG:.c=.o)): $(LIB_NET_DEBUG)
    @echo $<
    @$(CC) $(CFLAGS) -c $< -o $@

work/$(notdir $(MQTT_MGR:.c=.o)): $(MQTT_MGR)
    @echo $<
    @$(CC) $(CFLAGS) -c $< -o $@

work/$(notdir $(TRSMIO_HOSTLINK_CONFIG:.c=.o)): $(TRSMIO_HOSTLINK_CONFIG)
    @echo $<
    @$(CC) $(CFLAGS) -c $< -o $@    

work/$(notdir $(TRSMIO_HOSTLINK:.c=.o)): $(TRSMIO_HOSTLINK)
    @echo $<
    @$(CC) $(CFLAGS) -c $< -o $@

work/$(notdir $(TRSMIO_TEST:.c=.o)): $(TRSMIO_TEST)
    @echo $<
    @$(CC) $(CFLAGS) -c $< -o $@    

work/$(notdir $(MQTT_CGI:.c=.o)): $(MQTT_CGI)
    @echo $<
    @$(CC) $(CFLAGS) -c $< -o $@

work/$(notdir $(MQTT_CS:.c=.o)): $(MQTT_CS)
    @echo $<
    @$(CC) $(CFLAGS) -c $< -o $@    

work/$(notdir $(MQTT_LIB:.c=.o)): $(MQTT_LIB)
    @echo $<
    @$(CC) $(CFLAGS) -c $< -o $@        

work/$(notdir $(SDK_CS:.c=.o)): $(SDK_CS)
    @echo $<
    @$(CC) $(CFLAGS) -c $< -o $@

","I'm not sure that what you're trying is wise, but here's how to do it:
ALL_C_FILES = $(wildcard $(SOME_DIR)/*.c)

",c
how do i install sklearn module properly,"I'm trying to install sklearn module using pip command but after the installation is completed , all I can see is this folder
C:\Users\Aditi\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\LocalCache\local-packages\Python38\site-packages\sklearn-0.0-py3.8.egg-info 

In my directory and even the error says module name sklearn not found.
I've tried reinstalling it many times but still I'm not able to see the main sklearn folder in the above directory.Only 1 folder is installed i.e sklearn-0.0-py3.8.egg-info .Can anyone please help?
","Try to install using command pip install scikit-learn.
If it still not work for you, you can update the numpy or reinstall the numpy.
You can check here all the help related to installation and verifying the installation of scikit-learn
",data-science
a logarithmic colorbar in matplotlib scatter plot,"I would like to make the colors of the points on the scatter plot correspond to the value of the void fraction, but on a logarithmic scale to amplify differences. I did this, but now when I do plt.colorbar(), it displays the log of the void fraction, when I really want the actual void fraction. How can I make a log scale on the colorbar with the appropriate labels of the void fraction, which belongs to [0.00001,1]?
Here is an image of the plot I have now, but the void fraction colorbar is not appropriately labeled to correspond to the true void fraction, instead of the log of it.

fig = plt.figure()
plt.scatter(x,y,edgecolors='none',s=marker_size,c=np.log(void_fraction))
plt.colorbar()
plt.title('Colorbar: void fraction')

Thanks for your help.
","There is now a section of the documentation describing how color mapping and normalization works
The way that matplotlib does color mapping is in two steps, first a Normalize function (wrapped up by the sub-classes of matplotlib.colors.Normalize) which maps the data you hand in to [0, 1].  The second step maps values in [0,1] -> RGBA space.
You just need to use the LogNorm normalization class, passed in with the norm kwarg.
plt.scatter(x,y,edgecolors='none',s=marker_size,c=void_fraction,
                norm=matplotlib.colors.LogNorm())

When you want to scale/tweak data for plotting, it is better to let matplotlib do the transformations than to do it your self.

Normalize doc
LogNorm doc
matplotlib.color doc

",matplotlib
catelog sentences into 5 words that represent them,"I have dataframe with 1000 text rows. df['text']
I also have 5 words that I want to know for each one of them how much they represnt the text  (between 0 to 1)
every score will be in df[""word1""] ,df[""word2""] and etc
I will glad for recomendations how to do that
edit
represnt = the semantic distance between the word to the text.
for example -
lets say in row 1 the text is ""i want to eat""
and I have 2 words : food and house.
so in df[""food ""] it would be higher score than in df[""house""]
","You could use a pre-trained sentence transformer model from sentence_transformers:
import pandas as pd
from sentence_transformers import SentenceTransformer, util


class SemanticSimilarityCalculator:
  def __init__(self, model_name: str = 'all-MiniLM-L6-v2') -> None:
    self.model = SentenceTransformer(model_name)
    self.word_embeddings = None

  def encode_words(self, words: list[str]) -> None:
    self.word_embeddings = self.model.encode(words, convert_to_tensor=True)
    self.words = words

  def calculate_similarity(self, text: str) -> list[float]:
    if self.word_embeddings is None:
      raise ValueError('Words must be encoded before calculating similarity.')
    text_embedding = self.model.encode(text, convert_to_tensor=True)
    similarities = util.cos_sim(text_embedding, self.word_embeddings)[
      0
    ].tolist()
    return similarities

  def add_similarity_scores_to_df(
    self, df: pd.DataFrame, text_column: str
  ) -> pd.DataFrame:
    if self.words is None:
      raise ValueError(
        'Words must be encoded before adding scores to the DataFrame.'
      )
    similarity_columns = ['word_' + word for word in self.words]
    df[similarity_columns] = df[text_column].apply(
      lambda text: pd.Series(self.calculate_similarity(text))
    )
    return df


def main():
  data = {'text': ['I want to eat', 'The house is big', 'I need to sleep']}
  df = pd.DataFrame(data)
  words = ['food', 'house', 'sleep', 'drink', 'run']
  calculator = SemanticSimilarityCalculator()
  calculator.encode_words(words)
  df_with_scores = calculator.add_similarity_scores_to_df(
    df, text_column='text'
  )
  print(df_with_scores)


if __name__ == '__main__':
  main()

Output:
               text  word_food  word_house  word_sleep  word_drink  word_run
0     I want to eat   0.592410    0.215032    0.254065    0.370329  0.259350
1  The house is big   0.243262    0.672110    0.170785    0.213780  0.119716
2   I need to sleep   0.253703    0.222462    0.725105    0.358372  0.303838

",pandas
can39t figure out why numpylog10 outputs nan,"So I have an 500k array of floating values. When I am trying to:
np.log10(my_long_array)

270k numbers getting replaced to nan, and they are not that small. For example:
In [1]: import numpy as np

In [2]: t = -0.055488893531690543

In [3]: np.log10(t)
/home/aydar/anaconda3/bin/ipython:1: RuntimeWarning: invalid value encountered in log10
  #!/home/aydar/anaconda3/bin/python3
Out[3]: nan

In [4]: type(t)
Out[4]: float

What am I missing?
","the logarithm of a negative number is undefined, hence the nan
From the docs to numpy.log10:

Returns: y : ndarray
The logarithm to the base 10 of x, element-wise. NaNs are returned where x is negative.

",numpy
how to send large videos to gemini ai api 15 pro for inference,"I'm currently working with the Gemini AI API 1.5 Pro (latest version) and need to send large video files for inference. These videos are several hundred megabytes each (~700MB) but are within the API's constraints (e.g., less than 1 hour in length). I want to upload them once and perform inference without re-uploading.
In GPT-4o, there was an option to use image_urls to reference images. Is there a similar method or best practice for handling large video files with the Gemini AI API 1.5 Pro?
The videos are too large to send repeatedly, so an efficient method for uploading and referencing them is crucial.
Any guidance on API endpoints, required parameters, or example code snippets would be greatly appreciated.
","In your situation, how about the following sample script?
Sample script 1:
Before you test the following script, please update google-generativeai to the latest version.
import google.generativeai as genai
import time

apiKey = ""###"" # Please set your API key.
video_file_name = ""sample.mp4"" # Please set your video file with the path.
display_name = ""sampleDisplayName"" # Please set the display name of the uploaded file on Gemini. The file is searched from the file list using this value.

genai.configure(api_key=apiKey)

# Get file list in Gemini
fileList = genai.list_files(page_size=100)

# Check uploaded file.
video_file = next((f for f in fileList if f.display_name == display_name), None)
if video_file is None:
    print(f""Uploading file..."")
    video_file = genai.upload_file(path=video_file_name, display_name=display_name, resumable=True)
    print(f""Completed upload: {video_file.uri}"")
else:
    print(f""File URI: {video_file.uri}"")

# Check the state of the uploaded file.
while video_file.state.name == ""PROCESSING"":
    print(""."", end="""")
    time.sleep(10)
    video_file = genai.get_file(video_file.name)

if video_file.state.name == ""FAILED"":
    raise ValueError(video_file.state.name)

# Generate content using the uploaded file.
prompt = ""Describe this video.""
model = genai.GenerativeModel(model_name=""models/gemini-1.5-pro-latest"")
print(""Making LLM inference request..."")
response = model.generate_content([video_file, prompt], request_options={""timeout"": 600})
print(response.text)

In this sample script, when the file has already been uploaded, the existing file is used. On the other hand, when the file is not found, the file is uploaded and the uploaded file is used. In order to search the file, in this sample, display_name is used.
Sample script 2:
As another approach, when the value of name can be directly given, the following sample script can be also used. In this case, the value of name is required to be the unique value in the uploaded files.
import google.generativeai as genai
import time

apiKey = ""###"" # Please set your API key.
video_file_name = ""sample.mp4"" # Please set your video file with the path.
name = ""sample-name-1"" # Please set the name of the uploaded file on Gemini. The file is searched from the file list using this value.

genai.configure(api_key=apiKey)

# Check uploaded file.
try:
    video_file = genai.get_file(f""files/{name}"")
    print(f""File URI: {video_file.uri}"")
except:
    print(f""Uploading file..."")
    video_file = genai.upload_file(path=video_file_name, name=name, resumable=True)
    print(f""Completed upload: {video_file.uri}"")

# Check the state of the uploaded file.
while video_file.state.name == ""PROCESSING"":
    print(""."", end="""")
    time.sleep(10)
    video_file = genai.get_file(video_file.name)

if video_file.state.name == ""FAILED"":
    raise ValueError(video_file.state.name)

# Generate content using the uploaded file.
prompt = ""Describe this video.""
model = genai.GenerativeModel(model_name=""models/gemini-1.5-pro-latest"")
print(""Making LLM inference request..."")
response = model.generate_content([video_file, prompt], request_options={""timeout"": 600})
print(response.text)

This script is the same result with the above script.
Note:

This is a simple sample script. So, please modify this to your actual situation.

Reference:

Prompting with media files

",chatgpt
why am i receiving quotattributeerror 39str39 object has no attribute 39page_content39quot when trying to add my embeddings to azure cognitive search,"I am extracting text from pdf documents and load it to Azure Cognitive Search for a RAG approach. Unfortunately this does not work. I am receiving the error message

AttributeError: 'str' object has no attribute 'page_content'

What I want to do is

Extract text from pdf via pymupdf - works
Upload it to Azuer Vector search as embeddings with vectors and `filename``
Query this through ChatGPT model

This is my code:
!pip install cohere tiktoken
!pip install openai==0.28.1
!pip install pymupdf
!pip install azure-storage-blob azure-identity
!pip install azure-search-documents --pre --upgrade
!pip install langchain

import fitz
import time
import uuid
import os
import openai

from PIL import Image
from io import BytesIO
from IPython.display import display

from azure.identity import DefaultAzureCredential
from azure.storage.blob import BlobServiceClient, BlobClient, ContainerClient

from langchain.embeddings import OpenAIEmbeddings
from langchain.text_splitter import RecursiveCharacterTextSplitter

from langchain.chat_models import AzureChatOpenAI
from langchain.vectorstores import AzureSearch
from langchain.document_loaders import DirectoryLoader
from langchain.document_loaders import TextLoader
from langchain.text_splitter import TokenTextSplitter
from langchain.chains import ConversationalRetrievalChain
from langchain.prompts import PromptTemplate

from google.colab import drive

OPENAI_API_BASE = ""https://xxx.openai.azure.com""
OPENAI_API_KEY = ""xxx""
OPENAI_API_VERSION = ""2023-05-15""

openai.api_type = ""azure""
openai.api_key = OPENAI_API_KEY
openai.api_base = OPENAI_API_BASE
openai.api_version = OPENAI_API_VERSION

AZURE_COGNITIVE_SEARCH_SERVICE_NAME = ""https://xxx.search.windows.net""
AZURE_COGNITIVE_SEARCH_API_KEY = ""xxx""
AZURE_COGNITIVE_SEARCH_INDEX_NAME = ""test""

llm = AzureChatOpenAI(deployment_name=""gpt35"", openai_api_key=OPENAI_API_KEY, openai_api_base=OPENAI_API_BASE, openai_api_version=OPENAI_API_VERSION)
embeddings = OpenAIEmbeddings(deployment_id=""ada002"", chunk_size=1, openai_api_key=OPENAI_API_KEY, openai_api_base=OPENAI_API_BASE, openai_api_version=OPENAI_API_VERSION)

acs = AzureSearch(azure_search_endpoint=AZURE_COGNITIVE_SEARCH_SERVICE_NAME,
                  azure_search_key = AZURE_COGNITIVE_SEARCH_API_KEY,
                  index_name = AZURE_COGNITIVE_SEARCH_INDEX_NAME,
                  embedding_function = embeddings.embed_query)
    
def generate_tokens(s):
  text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)
  splits = text_splitter.split_text(s)

  return splits

drive.mount('/content/drive')
folder = ""/content/drive/.../pdf/""

page_content = ''
doc_content = ''
    
for filename in os.listdir(folder):
    file_path = os.path.join(folder, filename)
    if os.path.isfile(file_path):
        print(f""Processing file: {file_path}"")

        doc = fitz.open(file_path)
        for page in doc: # iterate the document pages
          page_content += page.get_text() # get plain text encoded as UTF-8
          doc_content += page_content

          d = generate_tokens(doc_content)

          # the following line throws the error
          # how can i add the chunks + filename to 
          # Azure Cognitive Search?

          acs.add_documents(documents=d)
    
        print(metadatas)
        print(""----------"")
        print(doc_content)
        count = len(doc_content.split())
        print(""Number of tokens: "", count)


---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
<ipython-input-33-d9eaff7ee027> in <cell line: 10>()
     31           all_texts.extend(d)
     32 
---> 33           acs.add_documents(documents=d)
     34 
     35           metadatas = [{""Source"": f""{i}-pl""} for i in range(len(all_texts))]

1 frames
/usr/local/lib/python3.10/dist-packages/langchain/schema/vectorstore.py in <listcomp>(.0)
    118         """"""
    119         # TODO: Handle the case where the user doesn't provide ids on the Collection
--> 120         texts = [doc.page_content for doc in documents]
    121         metadatas = [doc.metadata for doc in documents]
    122         return self.add_texts(texts, metadatas, **kwargs)

AttributeError: 'str' object has no attribute 'page_content'

","I believe the issue is in your generate_tokens method. Instead of returning a list of documents, it is returning a list of string. Please see the documentation for split_text here: https://api.python.langchain.com/en/latest/text_splitter/langchain.text_splitter.RecursiveCharacterTextSplitter.html#langchain.text_splitter.RecursiveCharacterTextSplitter.split_text.
I believe the fix should be to convert this list of string into a list of Document objects. Can you please try the following:
def generate_tokens(s):
  text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)
  splits = text_splitter.split_text(s)

  return text_splitter.create_documents(splits) #this should return the list of documents.

",langchain
is there a way to connect the lines of a scatterline plot with the ends of the markers instead of the centers in python,"I use this code to make a scatter+line plot with Matplotlib:
import matplotlib.pyplot as plt
    
zero = [1,2,3,4,5]
eq = [1.2,3,5,7,11]     
plt.plot(zero,'r_--', eq,'k_--',ms = 30, markeredgewidth=5)
plt.xlabel('Steps',fontsize=20, fontname='Helvetica', fontweight='bold')
plt.ylabel('Steps',fontweight='bold',fontsize=20, fontname='Helvetica')
plt.xticks([0,1,2,3,4], fontsize=20, fontname='Helvetica')
plt.yticks(fontsize =18, fontname='Helvetica')
plt.figure(figsize=(8,6))
plt.show()     
plt.close()

However, the lines in the resulting graph connect to the centers of the markers, as in the red example in the image. How can I get a result like the black example instead, where the lines connect to the endpoints of the markers?

","As per @J_H suggestion, you have to cheat a bit and -for each datapoint- introduce two datapoints corresponding to the ends of the markers.
You can use the following list comprehensions to define the new datapoints
import matplotlib.pyplot as plt
    
zero = [1,2,3,4,5]
eq = [1.2,3,5,7,11]

# For each datapoint in your initial dataset, define two points, with same Y of the original point, and X values one slightly smaller and one slightly larger than the actual datapoint (i.e. x - offset and x + offset).
offset = 0.175 # adjust the offset value manually or -better- based on marker size

zeroX = [i
         for x in range(len(zero)) 
         for i in (x-offset, x+offset)]
zeroY = [i
         for y in zero
         for i in (y, y)]
eqX =   [i
         for x in range(len(eq)) 
         for i in (x-offset, x+offset)]
eqY =   [i
         for y in eq 
         for i in (y, y)]

plt.plot(zeroX, zeroY, eqX, eqY) # plot the lines without markers
plt.plot(zero,'r_', eq,'k_',ms = 30, markeredgewidth=5) # plot the markers only 
plt.xlabel('Steps',fontsize=20, fontname='Helvetica', fontweight='bold')
plt.ylabel('Steps',fontweight='bold',fontsize=20, fontname='Helvetica')
plt.xticks([0,1,2,3,4], fontsize=20, fontname='Helvetica')
plt.yticks(fontsize =18, fontname='Helvetica')
plt.show()     
plt.close()

and get the plot below as a result

As commented in the code, it would be better to have the offset automatically adjusted based on the marker size rather than manually.
Hope this helps!
",matplotlib
how to find the first nonnan data before nan in one column in pandas,"For example,
I have some data like this:
column = pd.Series([1,2,3,np.nan,4,np.nan,7])
print column

Executing the command, the result looks like:
0    1.0
1    2.0
2    3.0
3    NaN
4    4.0
5    NaN
6    7.0

Now I want to know what is the first value before each NaN value, such as 3.0 which is before the first NaN. And 4.0 is the result before the second NaN value. Is there any built-in function in pandas which can complete this or shall I write a for loop to do this? 
","Same idea as @jezrael... numpyfied.
column[np.append(np.isnan(column.values)[1:], False)]

2    3.0
4    4.0
dtype: float64

Complete with pd.Series reconstruction
m = np.append(np.isnan(column.values)[1:], False)
pd.Series(column.values[m], column.index[m])

2    3.0
4    4.0
dtype: float64


Not nearly as quick but intuitive.  Group by the the cumsum of isnull and take the last value.  Of this result, get rid of last row.
column.groupby(column.isnull().cumsum()).last().iloc[:-1]

0    3.0
1    4.0
dtype: float64

",pandas
how to disable neptune callback in transformers trainer runs,"After installing Neptune.ai for occasional ML experiments logging, it became included by default into the list of callbacks in all transformers.trainer runs. As a result, it requires proper initialisation with token or else throws NeptuneMissingConfiguration error, demanding token and project name.
This is really annoying, I'd prefer Neptune callback to limit itself to warning or just have it disabled if no token is provided.
Unfortunately there is no obvious way to disable this callback, short of uninstalling Neptune.ai altogether. The doc page at https://huggingface.co/docs/transformers/main_classes/callback states that this callback is enabled by default and gives no way to disable it (unlike some other callbacks that can be disabled by environment variable).
Question: how to disable Neptune callback on per run basis?
","The reason Neptune is included is because the default value of report_to in TrainingArguments is ""all"", which implicitly includes all installed loggers from the officially supported list of loggers. You should either uninstall Neptune from the environment you use for the project, or pass report_to=""none"" to the TrainingArguments instance you use to initialize the Trainer (n.b.: that's the string literal ""none"", NOT a Python None).
The other answers here, including the accepted answer, are either poor workarounds for this problem, or simply do not work at all. The proper way to handle this issue is as above.
",huggingface-transformers
pandas equivalent of group by having in sql,"What is the most efficient way to use groupby and in parallel apply a filter in pandas?
Basically I am asking for the equivalent in SQL of
select *
...
group by col_name
having condition

I think there are many uses cases ranging from conditional means, sums, conditional probabilities, etc. which would make such a command very powerful.
I need a very good performance, so ideally such a command would not be the result of several layered operations done in python.
","As mentioned in unutbu's comment, groupby's filter is the equivalent of SQL'S HAVING:
In [11]: df = pd.DataFrame([[1, 2], [1, 3], [5, 6]], columns=['A', 'B'])

In [12]: df
Out[12]:
   A  B
0  1  2
1  1  3
2  5  6

In [13]: g = df.groupby('A')  #  GROUP BY A

In [14]: g.filter(lambda x: len(x) > 1)  #  HAVING COUNT(*) > 1
Out[14]:
   A  B
0  1  2
1  1  3

You can write more complicated functions (these are applied to each group), provided they return a plain ol' bool:
In [15]: g.filter(lambda x: x['B'].sum() == 5)
Out[15]:
   A  B
0  1  2
1  1  3

Note: potentially there is a bug where you can't write you function to act on the columns you've used to groupby... a workaround is the groupby the columns manually i.e. g = df.groupby(df['A'])).
",pandas
detect if the element im currently in is the youngest songrandsongrandgrand son of an element,"I am looping around span elements which belongs to frag class. I wanted to detect if the frag element im currently in is the youngest son/grandson/grandgrand son(from left to right) of the a span element which belongs to frag class and belongs to cond class and has an ""if"" or ""else if"" in its text
contenteditable div (textarea)
if( ((b>0) && (b<10)) || b==15 )

HTML equivalent
<div id=""board"">
  <div>
  <span class=""frag cond"">if<span class=""openParen bm1"">(</span> <span class=""frag cond""><span class=""openParen bm2"">(</span><span class=""frag cond""><span class=""openParen bm3"">(</span><span class=""frag"">b</span>&gt;0<span class=""closeParen bm3"">)</span></span> && <span class=""frag cond""><span class=""openParen bm4"">(</span><span class=""frag"">b</span>&lt;10<span class=""closeParen bm4"">)</span></span><span class=""closeParen bm2"">)</span></span> || <span class=""frag"">b</span>==15 <span class=""closeParen bm1"">)</span></span>
  </div>
</div>

(My system is like a code editor by the way, so i am using a contenteditable div) Here, i wanted to detect if im on the 3rd b.
byossarian has helped me with this:
var $elem = $('.frag').eq(fragNum);

var validAncest = $elem.parents('.cond').filter(function(i){
    return /^\s*(if|else\s+if)/.test($(this).text());
}).filter(function(i){
    var generations = 0,
    $elemCrawl = $('.frag').eq(fragNum);

    while ($elemCrawl[0] && $elemCrawl[0] !== this) {
        generations++;
        $elemCrawl = $elemCrawl.parent();
    }

    if ($elemCrawl[0] && generations < 4) {
        return true;
    }
});

finds all the ancestors of the element, it then filters then once to narrow down to those starting with 'if' or 'else if'. It then filters these to those that are either the parent, grandparent or great-grandparent (generations < 4) of the original element.
the condition to check will be:
if (validAncest.length && !$elem.nextAll('.frag').length) {
    alert('target acquired');
}

It checks that there is at least one valid ancestor (validAncest.length) AND that the original element is the last child of it's IMMEDIATE parent that has the class 'frag' (!$elem.nextAll('.frag').length)
The problem
IMMEDIATE parent should also be the last child/grandchild of the element with ""if"" or ""else if""
So from the example above, the first b will already give me the alert where it should be on the 3rd b
UPDATE should pop the alert at the 2nd b not at (b<10)

","Try changing validAncest with:
var validAncest = $elem.parents('.cond').filter(function(i){
    return /^\s*(if|else\s+if)/.test($(this).text());
}).filter(function(i){
    var generations = 1,
    $elemCrawl = $('.frag').eq(fragNum).parent();

    while ($elemCrawl[0] && $elemCrawl[0] !== this) {
        if (!$elemCrawl.is(':last-child')) {return false;}
        generations++;
        $elemCrawl = $elemCrawl.parent();
    }

    if ($elemCrawl[0] && generations < 4) {
        return true;
    }
});

This should include:

The parent of $elem provided it starts with 'if'/'else if'
The grandparent provided it starts with 'if'/'else if' AND the parent of $elem is the last child of this grandparent
The great-grandparent provided it starts with 'if'/'else if' AND the parent of $elem is the last child of the intermediate grandparent AND that grandparent is the last child of the great-grandparent

",javascript
sklearn multiclass problem and reporting sensitivity and specificity,"I have a three-class problem and I'm able to report precision and recall for each class with the below code:
from sklearn.metrics import classification_report
print(classification_report(y_test, y_pred))

which gives me the precision and recall nicely for each of the 3 classes in a table format.
My question is how can I now get sensitivity and specificity for each of the 3 classes? I looked at sklearn.metrics and I didn't find anything for reporting sensitivity and specificity.
","If we check the help page for classification report:

Note that in binary classification, recall of the positive class is
also known as “sensitivity”; recall of the negative class is
“specificity”.

So we can convert the pred into a binary for every class, and then use the recall results from precision_recall_fscore_support.
Using an example:
from sklearn.metrics import classification_report
y_true = [0, 1, 2, 2, 2]
y_pred = [0, 0, 2, 2, 1]
target_names = ['class 0', 'class 1', 'class 2']
print(classification_report(y_true, y_pred, target_names=target_names))

Looks like:
              precision    recall  f1-score   support

     class 0       0.50      1.00      0.67         1
     class 1       0.00      0.00      0.00         1
     class 2       1.00      0.67      0.80         3

    accuracy                           0.60         5
   macro avg       0.50      0.56      0.49         5
weighted avg       0.70      0.60      0.61         5

Using sklearn:
from sklearn.metrics import precision_recall_fscore_support
res = []
for l in [0,1,2]:
    prec,recall,_,_ = precision_recall_fscore_support(np.array(y_true)==l,
                                                      np.array(y_pred)==l,
                                                      pos_label=True,average=None)
    res.append([l,recall[0],recall[1]])

put the results into a dataframe:
pd.DataFrame(res,columns = ['class','sensitivity','specificity'])

    class   sensitivity specificity
0   0   0.75    1.000000
1   1   0.75    0.000000
2   2   1.00    0.666667

",scikit-learn
transformersjs in reactjs,"I'm building a component in React and I want to use a model from huggingface. I found the package @xenova/transformers that allows to use of these models in JavaScript, but when I try to create my pipeline instance like this:
class MyExtractorPipeline {
    static task = ""feature-extraction"";
    static model = ""Xenova/all-MiniLM-L6-v2"";
    static instance = null;

    static async getInstance(model, progress_callback = null) {
        if (this.instance === null) {
            try {
                this.instance = await pipeline(this.task, model, { progress_callback }); //<= THIS IS FAILING
            } catch (error) {
                throw new Error(error)
            }
        }
        return this.instance;
    }
}

I get this error:
worker.js:17 Uncaught (in promise) Error: SyntaxError: Unexpected token '<', ""<!DOCTYPE ""... is not valid JSON
at MyExtractorPipeline.getInstance (worker.js:17:1)
at async worker.js:77:1
I'm sure it comes from the 'throw new Error(error)' that you can see above, but I don't know how to solve it, I need HELP.
I tried to do console.log() of almost all the variables, and I tried to use other hugginface models...
Anything has been useful.
","Is your intention to load the model locally? Have you used create-react-app to initiate your project?
If so (and this might not be unique to create-react-app) I've noticed that on build, if you reference a file in your 'src' folder, it does not properly find the files (causing 'index.html' to be returned).
Try putting your local models under your project's 'public' folder. Then set the environment variable like this:
env.localModelPath = process.env.PUBLIC_URL + '/models/'

Also, apply env setting for local loading of the model:
env.allowRemoteModels = false;
env.allowLocalModels = true;

Further reference here: https://create-react-app.dev/docs/using-the-public-folder/
",huggingface-transformers
manipulation of a pandas dataframe most time and memoryefficiently,"Please imagine I have a dataframe like this:
df = pd.DataFrame(index=pd.Index(['1', '1', '2', '2'], name='from'), columns=['to'], data= ['2', '2', '4', '5'])

df:

Now, I would like to calculate a matrix comprising of the percentage of times each value in the index ""from"" transitions to each value in column 'to', which is known as a transition matrix. I can achieve this by creating an empty transition matrix first and then populating it with the percentages using a for loop:
#Create an empty matrix to populate later (using sparse dtype to save memory):
matrix = pd.DataFrame(index=df.index.unique(), columns=df.to.unique(), data=0, dtype=pd.SparseDtype(dtype=np.float16, fill_value=0)) 

matrix:

for i in range(len(df)):
    from_, to = df.index[i], df.to.iloc[i]     
    matrix[to] = matrix[to].sparse.to_dense() # Convert to dense format because sparse dtype does not allow value assignment with .loc in the next line:  
    matrix.loc[from_, to] += 1     # Do a normal insertion with .loc[]
    matrix[to] = matrix[to].astype(pd.SparseDtype(dtype=np.float16, fill_value=0)) # Back to the original sparse format

matrix = (matrix.div(matrix.sum(axis=1), axis=0)*100) # converting counts to percentages

matrix:

This works. For example, index ""1"" only transitioned to ""2"" (100% of the time) and index ""2"" transitioned to ""4"" 50% of the time and to ""5"" the other 50% of the time, as can be verified in df.
Issue: The actual matrix is about 500K by 500K and the for loop takes a really long time to finish. So, is there a vectorized or other efficient way of calculating matrix from df
Note: I would get MemoryError without using the whole Sparse dtype thing even with dtype=float16 in pd.DataFrame() so I prefer to keep that if possible. It would be great if the 500K by 500K matrix will not take up more than 10-12Gb of RAM. Also, if it matters, these percentages will always have a 0-100 range, obviously.
","Option 1: pd.crosstab

Use pd.crosstab with normalize='index'.
Add df.mul and df.rename_axis for formatting.

out = (pd.crosstab(index=df.index, 
                   columns=df['to'], 
                   normalize='index'
                   )
       .mul(100)
       .rename_axis(index='from', columns=None)
       )

Output:
          2     4     5
from                   
1     100.0   0.0   0.0
2       0.0  50.0  50.0

Option 2: df.pivot_table

Use df.reset_index to use df.pivot_table with aggfunc='size' and fill_value=0.
Chain df.div to divide by df.value_counts along axis=0.
Again, add df.mul and df.rename_axis.

out2 = (df.reset_index()
        .pivot_table(index='from', columns='to', 
                     values='to', aggfunc='size', fill_value=0)
        .div(df.index.value_counts(), axis=0)
        .mul(100)
        .rename_axis(columns=None)
        )

out2.equals(out)
# True

Option 3: df.groupby

Use df.groupby with df.index and get groupby.value_counts with normalize=True.
Apply df.unstack with fill_value=0 to move level='to' (i.e. -1) to columns.
Again, add df.mul + df.rename_axis.

out3 = (df.groupby(df.index)
        .value_counts(normalize=True)
        .unstack(-1, fill_value=0)
        .mul(100)
        .rename_axis(columns=None)
        )

out3.equals(out)
# True


Edit:
With a very sizeable df, I would consider creating a csr_matrix to avoid the MemoryError. In this case, we only use df.groupby + value_counts and use Index.factorize to pass compatible row_ind and col_ind values.
from scipy.sparse import csr_matrix

g = df.groupby(df.index).value_counts(normalize=True).mul(100)

idx_codes, idx_unique = g.index.get_level_values(0).factorize()
col_codes, col_unique = g.index.get_level_values(1).factorize() 

m = csr_matrix((g.values, (idx_codes, col_codes)))

If you want, you can turn that back into a df with df.sparse.from_spmatrix:
df_m = pd.DataFrame.sparse.from_spmatrix(m, 
                                         index=idx_unique, 
                                         columns=col_unique)

Output:
       2     4     5
1  100.0   0.0   0.0
2    0.0  50.0  50.0

But understanding this, you can also continue with m and still use idx_unique and col_unique for slicing. E.g.:
bool_idx = col_unique.isin(['4','5'])

m[:, bool_idx].sum(axis=1)

matrix([[  0.],
        [100.]])

Compare:
df_m.loc[:, bool_idx].sum(axis=1)

1        0
2    100.0
dtype: Sparse[float64, 0]

",numpy
maximum call stack size exceeded error,"I am using a Direct Web Remoting (DWR) JavaScript library file and am getting an error only in Safari (desktop and iPad)
It says 

Maximum call stack size exceeded.

What exactly does this error mean and does it stop processing completely?
Also any fix for Safari browser (Actually on the iPad Safari, it says 

JS:execution exceeded timeout

which I am assuming is the same call stack issue)
","It means that somewhere in your code, you are calling a function which in turn calls another function and so forth, until you hit the call stack limit.
This is almost always because of a recursive function with a base case that isn't being met.
Viewing the stack
Consider this code...
(function a() {
    a();
})();

Here is the stack after a handful of calls...

As you can see, the call stack grows until it hits a limit: the browser hardcoded stack size or memory exhaustion.
In order to fix it, ensure that your recursive function has a base case which is able to be met...
(function a(x) {
    // The following condition 
    // is the base case.
    if ( ! x) {
        return;
    }
    a(--x);
})(10);

",javascript
recalling data user has input and showing if it is a number,"This is a two part question, I currently have the below code and I'm trying to have it so that the console will tell me if it is a lower case letter or an upper case letter or of it is a number. I cant seem to get the number.isInteger to work so please tell me where I am going wrong with that.
Also I would like to have it so that there is a call back of what the user entered. So instead of it just saying ""This is a upper case letter"" I would like it to state ""The letter g you entered is lowercase"" and vice versa for upperCase and numbers.
Hope that makes sense, please find below my current code. I am new to coding and javascript so please try dumb it down as much as possible for me. Thanks!
Please see below code I currently have:
    let upperLower = prompt(""please enter either a uppercase letter, lowercase letter or a number"");

if (upperLower == upperLower.toLowerCase()) {

  console.log(""The character is lowercase"");
}
else if (upperLower == upperLower.toUpperCase()) {
    
    console.log(""The character is uppercase"");
}
else if (upperLower == Number.isInteger()){

    console.log(""This is a number"");
}

","Or you can check if input converted to number is not a number (isNaN) is false


let upperLower = prompt(""please enter either a uppercase letter, lowercase letter or a number"");
if (!isNaN(parseInt(upperLower))){

    console.log(upperLower + "" is a number"");
}
else if (upperLower == upperLower.toLowerCase()) {

  console.log(upperLower + "" character is lowercase"");
}
else if (upperLower == upperLower.toUpperCase()) {
    
    console.log(upperLower + "" character is uppercase"");
}



",prompt
where the value of variables are stored in c,"In the following code segment:
int func()
{
    int a=7;
    return a;
}

Is the code segment where the value 7 is stored in the executable? Or is it in data segment or in the code segment?  Will the answer depends on the Operating system or the compiler?
","Each executable format has some sections. One of them is text, contains the assembly - binary code. One of them is heap where malloc-ed data is found and on is stack where local variables are stored. There are several others but it doesn't matter now. The above three are common everywhere.
Now, local data like your a resides on the stack. In the executable file, the value is stored in the text section.
I've added a main to your code (returning 0), compiled with -g then did objdump -CDgS a.out and searched for 0x424242 (I've replaced your 7 with a value with lesser chance of randomly occurring in code).
00000000004004ec <func>:
int func()
{
  4004ec:       55                      push   %rbp
  4004ed:       48 89 e5                mov    %rsp,%rbp
        int a=0x42424242;
  4004f0:       c7 45 fc 42 42 42 42    movl   $0x42424242,-0x4(%rbp)
        return a;
  4004f7:       8b 45 fc                mov    -0x4(%rbp),%eax
}
  4004fa:       5d                      pop    %rbp
  4004fb:       c3                      retq 

As you see, c7 45 fc 42 42 42 42 means that the value is stored in the generated file. Indeed, this is the case when looking at the binary via xxd:
$ xxd a.out | grep 4242
00004f0: c745 fc42 4242 428b 45fc 5dc3 5548 89e5  .E.BBBB.E.].UH..

You can recognize the above assembly line in the xxd snippet.
",c
fastest exponentiation of numpy 3d matrix,"Q is a 3D matrix and could for example have the following shape:

(4000, 25, 25)

I want raise Q to the power n for {0, 1, ..., k} and sum it all.
Basically, I want to calculate

\sum_{i=0}^{k-1}Q^n

I have the following function that works as expected:
def sum_of_powers(Q: np.ndarray, k: int) -> np.ndarray:
    Qs = np.sum([
        np.linalg.matrix_power(Q, n) for n in range(k)
    ], axis=0)

    return Qs

Is it possible to speed up my function or is there a faster method to obtain the same output?
","We can perform this calculation in O(log k) matrix operations.
Let M(k) represent the k'th power of the input, and S(k) represent the sum of those powers from 0 to k. Let I represent an appropriate identity matrix.
Approach 1
If you expand the product, you'll find that (M(1) - I) * S(k) = M(k+1) - I. That means we can compute M(k+1) using a standard matrix power (which takes O(log k) matrix multiplications), and compute S(k) by using numpy.linalg.solve to solve the equation (M(1) - I) * S(k) = M(k+1) - I:
import numpy.linalg

def option1(Q, k):
    identity = numpy.eye(Q.shape[-1])
    A = Q - identity
    B = numpy.linalg.matrix_power(Q, k+1) - identity
    return numpy.linalg.solve(A, B)

Approach 2
The standard exponentation by squaring algorithm computes M(2*k) as M(k)*M(k) and M(2*k+1) as M(2*k)*M(1).
We can alter the algorithm to track both S(k-1) and M(k), by computing S(2*k-1) as S(k-1)*M(k) + S(k-1) and S(2*k) as S(2*k-1) + M(2*k):
import numpy

def option2(Q, k):
    identity = numpy.eye(Q.shape[-1])

    if k == 0:
        res = numpy.empty_like(Q)
        res[:] = identity
        return res

    power = Q
    sum_of_powers = identity

    # Looping over a string might look dumb, but it's actually the most efficient option,
    # as well as the simplest. (It wouldn't be the bottleneck even if it wasn't efficient.)
    for bit in bin(k+1)[3:]:
        sum_of_powers = (sum_of_powers @ power) + sum_of_powers
        power = power @ power
        if bit == ""1"":
            sum_of_powers += power
            power = power @ Q
    return sum_of_powers

",numpy
tensorflow  invalidargumenterror shapes of all inputs must match,"I am currently following the TensorFlow guide/tutorial on seq2seq NMT models (https://www.tensorflow.org/text/tutorials/nmt_with_attention) using a Jupyter Notebook.
Upon running the following code,
# Setup the loop variables.
next_token, done, state = decoder.get_initial_state(ex_context)
tokens = []

for n in range(10):
  # Run one step.
  next_token, done, state = decoder.get_next_token(
      ex_context, next_token, done, state, temperature=1.0)
  # Add the token to the output.
  tokens.append(next_token)

# Stack all the tokens together.
tokens = tf.concat(tokens, axis=-1) # (batch, t)

# Convert the tokens back to a a string
result = decoder.tokens_to_text(tokens)
result[:3].numpy()

I receive an InvalidArgument Error as follows:
---------------------------------------------------------------------------
InvalidArgumentError                      Traceback (most recent call last)
Cell In[31], line 2
      1 # Setup the loop variables.
----> 2 next_token, done, state = decoder.get_initial_state(ex_context)
      3 tokens = []
      5 for n in range(10):
      6   # Run one step.

Cell In[28], line 8
      6 embedded = self.embedding(start_tokens)
      7 print(embedded)
----> 8 return start_tokens, done, self.rnn.get_initial_state(embedded)[0]

File ~/Library/Python/3.11/lib/python/site-packages/keras/src/layers/rnn/rnn.py:309, in RNN.get_initial_state(self, batch_size)
    307 get_initial_state_fn = getattr(self.cell, ""get_initial_state"", None)
    308 if get_initial_state_fn:
--> 309     init_state = get_initial_state_fn(batch_size=batch_size)
    310 else:
    311     return [
    312         ops.zeros((batch_size, d), dtype=self.cell.compute_dtype)
    313         for d in self.state_size
    314     ]

File ~/Library/Python/3.11/lib/python/site-packages/keras/src/layers/rnn/gru.py:326, in GRUCell.get_initial_state(self, batch_size)
    324 def get_initial_state(self, batch_size=None):
    325     return [
--> 326         ops.zeros((batch_size, self.state_size), dtype=self.compute_dtype)
    327     ]

File ~/Library/Python/3.11/lib/python/site-packages/keras/src/ops/numpy.py:5968, in zeros(shape, dtype)
   5957 @keras_export([""keras.ops.zeros"", ""keras.ops.numpy.zeros""])
   5958 def zeros(shape, dtype=None):
   5959     """"""Return a new tensor of given shape and type, filled with zeros.
   5960 
   5961     Args:
   (...)
   5966         Tensor of zeros with the given shape and dtype.
   5967     """"""
-> 5968     return backend.numpy.zeros(shape, dtype=dtype)

File ~/Library/Python/3.11/lib/python/site-packages/keras/src/backend/tensorflow/numpy.py:619, in zeros(shape, dtype)
--> 617 return tf.zeros(shape, dtype=dtype)

File ~/Library/Python/3.11/lib/python/site-packages/tensorflow/python/util/traceback_utils.py:153, in filter_traceback.<locals>.error_handler(*args, **kwargs)
    151 except Exception as e:
    152   filtered_tb = _process_traceback_frames(e.__traceback__)
--> 153   raise e.with_traceback(filtered_tb) from None
    154 finally:
    155   del filtered_tb

File ~/Library/Python/3.11/lib/python/site-packages/tensorflow/python/framework/ops.py:5983, in raise_from_not_ok_status(e, name)
   5981 def raise_from_not_ok_status(e, name) -> NoReturn:
   5982   e.message += ("" name: "" + str(name if name is not None else """"))
-> 5983   raise core._status_to_exception(e) from None

InvalidArgumentError: {{function_node __wrapped__Pack_N_2_device_/job:localhost/replica:0/task:0/device:CPU:0}} Shapes of all inputs must match: values[0].shape = [64,1,256] != values[1].shape = [] [Op:Pack] name: 

Any ideas? I'm pretty sure I'm following the guide to the letter.
","Turns out my only problem was versioning - use python 3.10 and tensorflow 2.11 and tensorflow-text 2.11. Creating a virtual environment with pyenv seemed to solve the problem. The wheels for the tensorflow and tensorflow-text packages can be found on PyPi, since pip didn't have those versions.
The other packages I used were:

einops==0.6.0
matplotlib==3.6.1
numpy==1.23.3

",tensorflow
nextjs 13 build error for private identifiers when using openai 40 package,"I added the new 4.0 openai package in my next.js app, i'm using it in the server of the app only and when building i get the error.
- info Linting and checking validity of types .Failed to compile.

./node_modules/openai/src/core.ts:539:3
Type error: Private identifiers are only available when targeting ECMAScript 2015 and higher.

  537 |
  538 | export abstract class AbstractPage<Item> implements AsyncIterable<Item> {
> 539 |   #client: APIClient;
      |   ^
  540 |   protected options: FinalRequestOptions;
  541 |
  542 |   protected response: Response;



I tried changing the tsconfig target to es6,es2015,esnext, but it doesn't seem to work, skipLibCheck is enabled so i'm even more confused as of why it happens
This is my tsconfig:
{
  ""compilerOptions"": {
    ""target"": ""es5"",
    ""lib"": [
      ""dom"",
      ""dom.iterable"",
      ""esnext""
    ],
    ""allowJs"": true,
    ""skipLibCheck"": true,
    ""strict"": true,
    ""forceConsistentCasingInFileNames"": true,
    ""noEmit"": true,
    ""esModuleInterop"": true,
    ""module"": ""esnext"",
    ""moduleResolution"": ""node"",
    ""resolveJsonModule"": true,
    ""isolatedModules"": true,
    ""jsx"": ""preserve"",
    ""incremental"": true,
    ""plugins"": [
      {
        ""name"": ""next""
      }
    ],
    ""paths"": {
      ""$/*"": [
        ""./src/*""
      ],
      ""$app/*"": [
        ""./src/app/*""
      ],
      ""$cmp/*"": [
        ""./src/components/*""
      ],
      ""$types/*"": [
        ""./src/types/*""
      ],
      ""$assets/*"": [
        ""./src/assets/*""
      ],
      ""$lib/*"": [
        ""./src/lib/*""
      ],
    }
  },
  ""include"": [
    ""next-env.d.ts"",
    ""**/*.ts"",
    ""**/*.tsx"",
    "".next/types/**/*.ts""
  ],
  ""exclude"": [
    ""node_modules""
  ]
}

","Use this import (no ""src"" in import):
import { type ChatCompletionMessageParam } from ""openai/resources/index.mjs"";

instead of :
import { ChatCompletionMessageParam } from ""openai/src/resources/chat/completions"";

",chatgpt
how can i link the records in the training dataset to the corresponding model predictions,"Using scikit-learn, I've set up a regression model to predict customers' maximum spend per transaction. The dataset I'm using looks a bit like this; the target column is maximum spend per transaction during the previous year:
customer_number | metric_1 | metric_2 | target
----------------|----------|----------|-------
111             | A        | X        | 15
222             | A        | Y        | 20
333             | B        | Y        | 30

I split the dataset into training & testing sets, one-hot encode the features, train the model, and make some test predictions:
target = pd.DataFrame(dataset, columns = [""target""])
features = dataset.drop(""target"", axis = 1)
train_features, test_features, train_target, test_target = train_test_split(features, target, test_size = 0.25)

train_features = pd.get_dummies(train_features)
test_features = pd.get_dummies(test_features)

model = RandomForestRegressor()
model.fit(X = train_features, y = train_target)

test_prediction = model.predict(X = test_features)

I can output various measures of the model's accuracy (mean average error, mean squared error etc) using the relevant functions in scikit-learn. However, I'd like to be able to tell which customers' predictions are the most inaccurate. So I want to be able to create a dataframe which looks like this:
customer_number | target | prediction | error
----------------|--------|----------- |------
111             | 15     | 17         | 2
222             | 20     | 19         | 1
333             | 30     | 50         | 20

I can use this to investigate if there is any correlation between the features and the model making inaccurate predictions. In this example, I can see that customer 333 has the biggest error by far, so I could potentially infer that customers with metric_1 = B end up with less accurate predictions.
I think I can calculate errors like this (please correct me if I'm wrong on this), but I don't know how to tie them back to customer number.
error = abs(test_target - test_prediction) 

How can I get the desired result?
","The error you are computing is the absolute error. When averaged it gives the Mean Absolute Error which is commonly used to evaluate regression models. You can read about the choice of an error metric here.
This error vector is the length of your test dataset and its elements are in the same order as your records. Many people assign them back into the dataframe. Then, if you leave customer number in there, everything should line up.
Starting with the DataFrame df and using idiomatic names for things:
df_train, df_test = train_test_split(df)

y_train, y_test = df_train[""target""], df_test[""target""]

X_train = df_train.drop([""customer_number"", ""target""], axis=1)
X_test = df_test.drop([""customer_number"", ""target""], axis=1)

X_train = pd.get_dummies(X_train)
X_test = pd.get_dummies(X_test)

model = RandomForestRegressor()
model.fit(X_train, y_train)

df_test[""prediction""] = model.predict(X_test)
df_test[""error""] = abs(df_test[""target""] - df_test[""prediction""])

",scikit-learn
filling out an html img tags 39src39 form based on information derived from api javascript code,"So, i am trying to add an img element from openweatherAPI, that shows an icon relative to what is found in the JSON results when the user gets the current web stats from typing a city, (i.e an image of scattered clouds, clear skies, etc). in order to display the img, i understand i need to paste the url into the ""src"" section of the img tag. the URL would look something like:
 const png = ""http://openweathermap.org/img/wn/"" + icon + ""@2x.png""

however, in order to make this dynamic, the img tags ""src"" would have to change based on what the image file is from the typed in city.
I have the logic defined from the ""icon"" and ""png"" variables in the js file. My question is, how to i get the html img 'src' to populate with the results of my ""png"" variable, based on the city the user inputs on the page?
I have included both html and javasript codes below


const button = document.querySelector("".button"")
const inputValue = document.querySelector("".inputValue"")
const name = document.querySelector("".name"")
const desc = document.querySelector("".desc"")
const temp = document.querySelector("".temp"")
const img = document.querySelector("".image"")

button.addEventListener('click', function (){

    fetch('http://api.openweathermap.org/data/2.5/weather?q='+ inputValue.value +'&units=imperial&appid=61dcc0033e94c4172d2bb94bb607fc5d')
.then(response => response.json())
.then(data => {
    const nameValue = data['name']
    const tempValue = data['main']['temp']
    const descValue = data['weather'][0]['description']
    const icon = weatherData.weather[0].icon
    const png = ""http://openweathermap.org/img/wn/"" + icon + ""@2x.png""

    name.innerHTML = nameValue
    temp.innerHTML = tempValue
    desc.innerHTML = descValue
    img.innerHTML = 
})


.catch(err => alert(""Wrong City name!""))
})
<!DOCTYPE html>
<html>
<head>
    <meta charset='utf-8'>
    <meta http-equiv='X-UA-Compatible' content='IE=edge'>
    <title>OpenWeatherAPI</title>
    <meta name='viewport' content='width=device-width, initial-scale=1'>
    <link rel='stylesheet' type='text/css' media='screen' href='style.css'>
    
</head>
<body>
    <div class=""input"">
        <input type=""text"" class=""inputValue"" placeholder=""Enter a city"">
        <input type=""submit"" value=""submit"" class=""button"">

    </div>

    <div class=""display"">
        <h1 class=""name""></h1>
        <p class=""desc""></p>
        <p class=""temp""></p>
        <img class=""image"" src="""">
    </div>

    <script src='main.js'></script>
</body>
</html>



","If I am not mistaken I do not see anywhere in your Java Script where you change the <img> src.
EDIT
You can change the src by simply getting that element then setting it's source like blow:
document.getElementById(""myImg"").src = png;


This is assuming you add an id of ""myImg"" to the <img> tag like so:
<img class=""image"" src="""" id=""myImg"">


EDIT 2
I did not realize you already got the element earlier on so all you need to do is:
img.src = png;


",javascript
what is quotlanguage modeling headquot in bertformaskedlm,"I have recently read about BERT and want to use BertForMaskedLM for fill_mask task. I know about BERT architecture. Also, as far as I know, BertForMaskedLM is built from BERT with a language modeling head on top, but I have no idea about what language modeling head means here. Can anyone give me a brief explanation.
","The BertForMaskedLM, as you have understood correctly uses a Language Modeling(LM) head .
Generally, as well as in this case, LM head is a linear layer having input dimension of hidden state (for BERT-base it will be 768) and output dimension of vocabulary size. Thus, it maps to hidden state output of BERT model to a specific token in the vocabulary. The loss is calculated based on the scores obtained of a given token with respect to the target token.
",huggingface-transformers
matplotlib plotting all columns on the xaxis and values on the yaxis grouped by a third variable,"I have data that looks like this:
group    var1    var2    var3
0        0.6    0.001    0.11  
1       -0.36  -0.007   -0.066481 

I want to create a dot graph where the 3 different variables form the x-axis and the y-axis represent the values in the data frame, with different symbols/colors for the dots depending on the group on the leftmost column.
I tried melting the data into the format
group var  value
0     var1 0.6
1     var1 -0.36
0     var2 0.001
1     var2 -0.007
0     var3 0.11
1     var3 -0.066481

But I was only able to make separate plots by variable instead of one big plot. Any help would be appreciated, thank you!
","Here is the code that plots based on two values and each group has specific color:
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

# Sample data
data = {
    'group': [0, 1],
    'var1': [0.6, -0.36],
    'var2': [0.001, -0.007],
    'var3': [0.11, -0.066481]
}

df = pd.DataFrame(data)

# Reshape the data using melt
df_melted = df.melt(id_vars='group', var_name='variable', value_name='value')

# Create the dot plot using seaborn
sns.set(style=""ticks"")
sns.stripplot(x='variable', y='value', hue='group', data=df_melted, dodge=True, size=10, palette=""Set2"")

# Set labels and title
plt.xlabel('Variable')
plt.ylabel('Value')
plt.title('Dot Graph')

# Display the plot
plt.show()

",data-science
how to add addeventlistener to from parent to child in pure javasvript,"I need your help. My code structure is like this:


<div class=""header"">
  <p>Button</p>
  <ul>
    <li>Option 1</>
    <li>Option 2</>
    <li>Option 3</>
  </ul>
</div>



I want to use addEventListener here on the p tag, on clicking it we need to hide and show ul.
This is one of my friends challenge question, please anyone help me how to achieve this.  p tag is inside header class. I cannot change the format, need to implement like this. I want to use pure Javascript for this problem
Codepen Link:
","Select the DOM through the selector, add click events, switch variables


const header = document.querySelector('.header')
const btn = header.querySelector('p')
const ul = header.querySelector('ul')
let isShow = false
btn.addEventListener('click', () => {
  isShow = !isShow
  ul.style.display = isShow ? 'none' : 'block'
})
<div class=""header""> <p>Button</p> <ul> <li>Option 1</> <li>Option 2</> <li>Option 3</> </ul> </div>



",javascript
why do i get write access violation,"I get a write access violation when I debug the following code. In two lines, the exception is thrown, according to the debugger. These lines are commented as write access violation in the code below.
#include <stdio.h>

struct Lnode {
    int value;
    struct Lnode* next;
};

struct List {
    struct Lnode* head;
    struct Lnode* tail;
};

void listAdd(struct List* x, int val) {
    struct Lnode* y;
    y = (struct Lnode*)malloc(sizeof(struct Lnode)); // write access violation
    y->value = val; // write access violation

    if (x->head == NULL) {
        x->head = y;
        x->tail = y;
    }
    else {
        struct Lnode *currentNode;
        currentNode = x->head->next;
        while (currentNode) {
            if (currentNode->next == NULL) {
                currentNode->next = y;
                x->tail = y;
                break;
            }
            currentNode = currentNode->next;
        }
    }
}

struct Lnode* listFind(struct List* x, int value) {
    if (x->head->value == value) {
        return x->head;
    }
    else if (x->tail->value == value) {
        return x->tail;
    }
    else {
        struct Lnode* currentNode = x->head->next;
        while (currentNode) {
            if (currentNode->value == value) {
                return currentNode;
            }
            currentNode = currentNode->next;
        }
    }
    return NULL;
}

int main() {
    struct List* myList = (struct List*)malloc(sizeof(struct List));

    listAdd(myList, 10); 
    listAdd(myList, 20);
    printf(""%d"", listFind(myList, 10)->value);
    printf(""%d"", listFind(myList, 20)->value);
    return 0;
}

I am trying to use malloc to allocate memory for a struct. In this example I am trying to create a linked list. However, the exception is causing my program to fail.
","Here are the bugs that I see.  It is not necessarily a complete list.  I leave it to you to work out how to fix them.

You forgot #include <stdlib.h> to use malloc.  (You should get a lot of compiler warnings about that.)

After struct List* myList = malloc(...) the contents of *myList are uninitialized (garbage), but your program expects both its head and tail members to be NULL.  You have to set them explicitly.

In listAdd, if the list is empty, you add the new node to it, but its next member remains uninitialized, rather than being NULL as your program needs.  Again, you need to initialize it.

In listAdd, if the list contains exactly one node, then the next member of that node will be NULL, so setting currentNode = x->head->next;  results in currentNode = NULL.  This causes the while (currentNode) loop to terminate immediately without adding the new node.

You're missing \n at the end of your printf format strings, so the values printed will be run together, and your program outputs 1020 which I don't think is what you want.


And some general code review advice:

Your listFind as written won't work if passed an empty list.

Your listFind could be considerably simpler; the head and tail nodes do not need to be special cases, if you write it properly.

Should I cast the result of malloc (in C)? No.

listFind(myList, 10)->value is dangerous in general: if the value is not found, then listFind() returns NULL, and so dereferencing it (by applying the -> operator) would crash.  You should check the return value before dereferencing.  This is basic error checking practice.
I realize that in this case, if everything is working then the value will be found - but your code does not actually work yet.  As a result, after fixing bugs 1-3 above, bug 4, which by itself only results in incorrect list contents rather than a crash, still causes the program to crash because 20 is not found (because listAdd didn't insert it properly).  If you had proper error checking, the program could fail more gracefully, and would be easier to debug.

Likewise, as the other answer said, you should also check the return value of malloc, every time, and fail gracefully if it is NULL (out of memory).

One of the main reasons to have a tail pointer in a linked list is so that you can add a node at the end without needing to iterate through the entire list to locate the end.  Such an operation could be O(1), but in your code it is O(n), which is needlessly inefficient.


",c
c structserror parameter name omitted,"I am receiving the following error for the following code:
kernel/proc.c: In function ‘getpinfo’:
kernel/proc.c:495: error: parameter name omitted

The code is as follows:
int 
getpinfo(struct pstat *)
{
}

Can you please tell me what I am missing about the struct or the code?
","     int 
     getpinfo(struct pstat *)   
      {
      }

Did not given any parameter Name.
Function definition should contain List of parameters, with valid type and parameters names.where as in declarations parameter Names are optional 
This should be
     int 
     getpinfo(struct pstat *some_name)
       {
       }

",c
is there a way in nodejs to display multiple options kind of like a prompt but with preset options,"I want the user to be able to have a drop-down and use arrow keys to control their option with enter as the selecting. Kind of like this.
Pick A Color:
Red <
Green
Blue

I want the user to use arrow keys to navigate from red to blue, how do I do this?
","There are a few popular prompt libraries you could try:

https://github.com/enquirer/enquirer
https://github.com/terkelg/prompts
https://github.com/SBoudrias/Inquirer.js

You should read each one's README to see which is easiest for you to setup. I typically use Inquirer but all three are great. With Inquirer, it would look something like this:
npm install @inquirer/select

import select from '@inquirer/select';

const answer = await select({
  message: 'Pick a color:',
  choices: [
    {
      value: 'red',
    },
    {
      value: 'green',
    },
    {
      value: 'blue',
    },
  ],
});

You can read more about this specific select prompt here: https://github.com/SBoudrias/Inquirer.js/tree/master/packages/select
If you get an error like this:
import select from ""@inquirer/select"";
^^^^^^

SyntaxError: Cannot use import statement outside a module

That's because the example I gave uses ESM-style (ES module) imports. You can just add ""type"": ""module"", to your package.json to fix it.
",prompt
input image is not compatible with tensorflow model input shape,"I was building a model and I wanted to test its performance, thus I imported a local file and load it and try to predict its label with the following code:
from tensorflow.preprocessing import image
# Other imports for tensorlfow etc.

#...

# Sample image
img_path = ""./Model/data/brain/train/Glioma/images/gg (2).jpg""
img = image.load_img(img_path,target_size=(256,256))
arr = image.img_to_array(img)
t_img = tf.convert_to_tensor(arr)
print(t_img.shape) # Returns (256,256,3)
# Client testing
client = Client(""brain"") # Custom Class. Contains model: Sequential (compiled and trained)
client.predict(img=t_img) # Calls self.model.predict(t_img)

However I get the following error:

Invalid input shape for input Tensor(""data:0"", shape=(32, 256, 3), dtype=float32). Expected shape (None, 256, 256, 3), but input has incompatible shape (32, 256, 3)

I have an input layer in the trained model which has input_shape=[256,256,3] (comes from image width, height, and rgb values)
Can you help me understand the issue and solve it?
","Dr. Snoopy already gave the answer in the comments, but for the sake of completeness a short solution copied from the TF load_image page:
image = keras.utils.load_img(image_path)
input_arr = keras.utils.img_to_array(image)
input_arr = np.array([input_arr])  # Convert single image to a batch.
predictions = model.predict(input_arr)

model.predict() expects batches of images. This solultion would transform your (256, 256, 3) shape to (1, 256, 256, 3). There are also other solutions, e.g. with tf.expand_dims(image, 0) if you rather want to work with tensors directly instead of arrays.
",tensorflow
pandas grouped list aggregation using transform fails with key error,"How can I apply a list function to a grouped pandas dataframe which is not aggregated using apply but rather transform?
For me the following fails with: KeyError: ""None of [Index(['v1', 'v2'], dtype='object')] are in the [index]""
import pandas as pd
df = pd.DataFrame({'key':[1,1,1,2,3,2], 'v1': [1,4,6,7,4,9], 'v2':[0.3, 0.6, 0.4, .1, .2, .8]})
display(df)

def list_function(x):
    #display(x)
    all_values = x[['v1','v2']].drop_duplicates()
    #display(all_values)
    #result = all_values.to_json()
    result = all_values.values
    return result


display(df.groupby(['key']).apply(list_function))
df['list_result'] = df.groupby(['key']).transform(list_function)
df


NOTICE: I know that a join would be possible with the aggregated data, but in this particular case I would prefer not having to do the JOIN afterwards.

","It is not possible, in pandas GroupBy.transform and also GroupBy.agg working with each column separately, so cannot select by multiple columns names like you need.
It is possible only by GroupBy.apply.
So instead transform is possible use Series.map if match one column, for multiple column use DataFrame.join:
df['list_result'] = df['key'].map(df.groupby(['key']).apply(list_function))
print (df)

   key  v1   v2                           list_result
0    1   1  0.3  [[1.0, 0.3], [4.0, 0.6], [6.0, 0.4]]
1    1   4  0.6  [[1.0, 0.3], [4.0, 0.6], [6.0, 0.4]]
2    1   6  0.4  [[1.0, 0.3], [4.0, 0.6], [6.0, 0.4]]
3    2   7  0.1              [[7.0, 0.1], [9.0, 0.8]]
4    3   4  0.2                          [[4.0, 0.2]]
5    2   9  0.8              [[7.0, 0.1], [9.0, 0.8]]


#added one column for match by 2 columns sample
df['new'] = 1

s = df.groupby(['key', 'new']).apply(list_function)
df = df.join(s.rename('list_result'), on=['key','new'])
print (df)
   key  v1   v2  new                           list_result
0    1   1  0.3    1  [[1.0, 0.3], [4.0, 0.6], [6.0, 0.4]]
1    1   4  0.6    1  [[1.0, 0.3], [4.0, 0.6], [6.0, 0.4]]
2    1   6  0.4    1  [[1.0, 0.3], [4.0, 0.6], [6.0, 0.4]]
3    2   7  0.1    1              [[7.0, 0.1], [9.0, 0.8]]
4    3   4  0.2    1                          [[4.0, 0.2]]
5    2   9  0.8    1              [[7.0, 0.1], [9.0, 0.8]]

",pandas
how do i get tailwind39s active breakpoint in javascript,"I am building Tailwind with config file and including it in a React project.
I would like to get the active breakpoint value in JavaScript/React. How can I achieve the same?


 <div class=""block  sm:hidden md:hidden lg:hidden xl:hidden"">al</div>
  <div class=""hidden sm:block  md:hidden lg:hidden xl:hidden"">sm</div>
  <div class=""hidden sm:hidden md:block  lg:hidden xl:hidden"">md</div>
  <div class=""hidden sm:hidden md:hidden lg:block  xl:hidden"">lg</div>
  <div class=""hidden sm:hidden md:hidden lg:hidden xl:block"">xl</div>
</div>



The above shows the active breakpoints. But how do I get the same in JS without including any of the above markup?
","Here's what I wrote in Typescript that returns the current breakpoint based on device width. You can place it in a standalone file and import the methods whenever needed in any file:
import resolveConfig from 'tailwindcss/resolveConfig';
import tailwindConfig from './tailwind.config'; // Fix the path

const fullConfig = resolveConfig(tailwindConfig);

export const getBreakpointValue = (value: string): number =>
  +fullConfig.theme.screens[value].slice(
    0,
    fullConfig.theme.screens[value].indexOf('px')
  );

export const getCurrentBreakpoint = (): string => {
  let currentBreakpoint: string;
  let biggestBreakpointValue = 0;
  for (const breakpoint of Object.keys(fullConfig.theme.screens)) {
    const breakpointValue = getBreakpointValue(breakpoint);
    if (
      breakpointValue > biggestBreakpointValue &&
      window.innerWidth >= breakpointValue
    ) {
      biggestBreakpointValue = breakpointValue;
      currentBreakpoint = breakpoint;
    }
  }
  return currentBreakpoint;
};


Edit:
in newer Typescript versions you have to add these two parameters to tsconfig.json under compilerOptions in order to be able import js files:
""compilerOptions"": {
  ""allowJs"": true,
  ""allowsyntheticdefaultimports"": true
}

Also, if you are on Angular and get the error that process is not defined, you have to add these lines to the end of your polyfills.ts file (you have to install the process package of course):
import * as process from 'process';
window['process'] = process;

",javascript
how to step through python code to help debug issues,"In Java/C# you can easily step through code to trace what might be going wrong, and IDE's make this process very user friendly.
Can you trace through python code in a similar fashion?
","Yes! There's a Python debugger called pdb just for doing that!
You can launch a Python program through pdb via python -m pdb myscript.py.
There are a few commands you can then issue, which are documented on the pdb page.
Some useful ones to remember are:

b: set a breakpoint
c: continue debugging until you hit a breakpoint
s: step through the code
n: to go to next line of code
l: list source code for the current file (default: 11 lines including the line being executed)
u: navigate up a stack frame
d: navigate down a stack frame
p: to print the value of an expression in the current context

If you don't want to use a command line debugger, some IDEs like Pydev, Wing IDE or PyCharm have a GUI debugger. Wing and PyCharm are commercial products, but Wing has a free ""Personal"" edition, and PyCharm has a free community edition.
",python
how to annotate end of lines using python and matplotlib,"With a dataframe and basic plot such as this:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

np.random.seed(123456)
rows = 75
df = pd.DataFrame(np.random.randint(-4,5,size=(rows, 3)), columns=['A', 'B', 'C'])
datelist = pd.date_range(pd.datetime(2017, 1, 1).strftime('%Y-%m-%d'), periods=rows).tolist()
df['dates'] = datelist 
df = df.set_index(['dates'])
df.index = pd.to_datetime(df.index)
df = df.cumsum()

df.plot()


What is the best way of annotating the last points on the lines so that you get the result below?

","In order to annotate a point use ax.annotate(). In this case it makes sense to specify the coordinates to annotate separately. I.e. the y coordinate is the data coordinate of the last point of the line (which you can get from line.get_ydata()[-1]) while the x coordinate is independent of the data and should be the right hand side of the axes (i.e. 1 in axes coordinates). You may then also want to offset the text a bit such that it does not overlap with the axes.
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

rows = 75
df = pd.DataFrame(np.random.randint(-4,5,size=(rows, 3)), columns=['A', 'B', 'C'])
datelist = pd.date_range(pd.datetime(2017, 1, 1).strftime('%Y-%m-%d'), periods=rows).tolist()
df['dates'] = datelist 
df = df.set_index(['dates'])
df.index = pd.to_datetime(df.index)
df = df.cumsum()

ax = df.plot()

for line, name in zip(ax.lines, df.columns):
    y = line.get_ydata()[-1]
    ax.annotate(name, xy=(1,y), xytext=(6,0), color=line.get_color(), 
                xycoords = ax.get_yaxis_transform(), textcoords=""offset points"",
                size=14, va=""center"")

plt.show()


",matplotlib
define macro for debug printing in c,"Trying to create a macro which can be used for print debug messages when DEBUG is defined, like the following pseudo code:
#define DEBUG 1
#define debug_print(args ...) if (DEBUG) fprintf(stderr, args)

How is this accomplished with a macro?
","If you use a C99 or later compiler
#define debug_print(fmt, ...) \
            do { if (DEBUG) fprintf(stderr, fmt, __VA_ARGS__); } while (0)

It assumes you are using C99 (the variable argument list notation is not supported in earlier versions).  The do { ... } while (0) idiom ensures that the code acts like a statement (function call).  The unconditional use of the code ensures that the compiler always checks that your debug code is valid — but the optimizer will remove the code when DEBUG is 0.
If you want to work with #ifdef DEBUG, then change the test condition:
#ifdef DEBUG
#define DEBUG_TEST 1
#else
#define DEBUG_TEST 0
#endif

And then use DEBUG_TEST where I used DEBUG.
If you insist on a string literal for the format string (probably a good idea anyway), you can also introduce things like __FILE__, __LINE__ and __func__ into the output, which can improve the diagnostics:
#define debug_print(fmt, ...) \
        do { if (DEBUG) fprintf(stderr, ""%s:%d:%s(): "" fmt, __FILE__, \
                                __LINE__, __func__, __VA_ARGS__); } while (0)

This relies on string concatenation to create a bigger format string than the programmer writes.
If you use a C89 compiler
If you are stuck with C89 and no useful compiler extension, then there isn't a particularly clean way to handle it.  The technique I used to use was:
#define TRACE(x) do { if (DEBUG) dbg_printf x; } while (0)

And then, in the code, write:
TRACE((""message %d\n"", var));

The double-parentheses are crucial — and are why you have the funny notation in the macro expansion.  As before, the compiler always checks the code for syntactic validity (which is good) but the optimizer only invokes the printing function if the DEBUG macro evaluates to non-zero.
This does require a support function — dbg_printf() in the example — to handle things like 'stderr'.  It requires you to know how to write varargs functions, but that isn't hard:
#include <stdarg.h>
#include <stdio.h>

void dbg_printf(const char *fmt, ...)
{
    va_list args;
    va_start(args, fmt);
    vfprintf(stderr, fmt, args);
    va_end(args);
}

You can also use this technique in C99, of course, but the __VA_ARGS__ technique is neater because it uses regular function notation, not the double-parentheses hack.
Why is it crucial that the compiler always see the debug code?
[Rehashing comments made to another answer.]
One central idea behind both the C99 and C89 implementations above is that the compiler proper always sees the debugging printf-like statements.  This is important for long-term code — code that will last a decade or two.
Suppose a piece of code has been mostly dormant (stable) for a number of years, but now needs to be changed.  You re-enable debugging trace - but it is frustrating to have to debug the debugging (tracing) code because it refers to variables that have been renamed or retyped, during the years of stable maintenance. If the compiler (post pre-processor) always sees the print statement, it ensures that any surrounding changes have not invalidated the diagnostics. If the compiler does not see the print statement, it cannot protect you against your own carelessness (or the carelessness of your colleagues or collaborators). See 'The Practice of Programming' by Kernighan and Pike, especially Chapter 8 (see also Wikipedia on TPOP).
This is 'been there, done that' experience — I used essentially the technique described in other answers where the non-debug build does not see the printf-like statements for a number of years (more than a decade). But I came across the advice in TPOP (see my previous comment), and then did enable some debugging code after a number of years, and ran into problems of changed context breaking the debugging. Several times, having the printing always validated has saved me from later problems.
I use NDEBUG to control assertions only, and a separate macro (usually DEBUG) to control whether debug tracing is built into the program. Even when the debug tracing is built in, I frequently do not want debug output to appear unconditionally, so I have mechanism to control whether the output appears (debug levels, and instead of calling fprintf() directly, I call a debug print function that only conditionally prints so the same build of the code can print or not print based on program options).  I also have a 'multiple-subsystem' version of the code for bigger programs, so that I can have different sections of the program producing different amounts of trace - under runtime control.
I am advocating that for all builds, the compiler should see the diagnostic statements; however, the compiler won't generate any code for the debugging trace statements unless debug is enabled.  Basically, it means that all of your code is checked by the compiler every time you compile - whether for release or debugging.  This is a good thing!
debug.h - version 1.2 (1990-05-01)
/*
@(#)File:            $RCSfile: debug.h,v $
@(#)Version:         $Revision: 1.2 $
@(#)Last changed:    $Date: 1990/05/01 12:55:39 $
@(#)Purpose:         Definitions for the debugging system
@(#)Author:          J Leffler
*/

#ifndef DEBUG_H
#define DEBUG_H

/* -- Macro Definitions */

#ifdef DEBUG
#define TRACE(x)    db_print x
#else
#define TRACE(x)
#endif /* DEBUG */

/* -- Declarations */

#ifdef DEBUG
extern  int     debug;
#endif

#endif  /* DEBUG_H */

debug.h - version 3.6 (2008-02-11)
/*
@(#)File:           $RCSfile: debug.h,v $
@(#)Version:        $Revision: 3.6 $
@(#)Last changed:   $Date: 2008/02/11 06:46:37 $
@(#)Purpose:        Definitions for the debugging system
@(#)Author:         J Leffler
@(#)Copyright:      (C) JLSS 1990-93,1997-99,2003,2005,2008
@(#)Product:        :PRODUCT:
*/

#ifndef DEBUG_H
#define DEBUG_H

#ifdef HAVE_CONFIG_H
#include ""config.h""
#endif /* HAVE_CONFIG_H */

/*
** Usage:  TRACE((level, fmt, ...))
** ""level"" is the debugging level which must be operational for the output
** to appear. ""fmt"" is a printf format string. ""..."" is whatever extra
** arguments fmt requires (possibly nothing).
** The non-debug macro means that the code is validated but never called.
** -- See chapter 8 of 'The Practice of Programming', by Kernighan and Pike.
*/
#ifdef DEBUG
#define TRACE(x)    db_print x
#else
#define TRACE(x)    do { if (0) db_print x; } while (0)
#endif /* DEBUG */

#ifndef lint
#ifdef DEBUG
/* This string can't be made extern - multiple definition in general */
static const char jlss_id_debug_enabled[] = ""@(#)*** DEBUG ***"";
#endif /* DEBUG */
#ifdef MAIN_PROGRAM
const char jlss_id_debug_h[] = ""@(#)$Id: debug.h,v 3.6 2008/02/11 06:46:37 jleffler Exp $"";
#endif /* MAIN_PROGRAM */
#endif /* lint */

#include <stdio.h>

extern int      db_getdebug(void);
extern int      db_newindent(void);
extern int      db_oldindent(void);
extern int      db_setdebug(int level);
extern int      db_setindent(int i);
extern void     db_print(int level, const char *fmt,...);
extern void     db_setfilename(const char *fn);
extern void     db_setfileptr(FILE *fp);
extern FILE    *db_getfileptr(void);

/* Semi-private function */
extern const char *db_indent(void);

/**************************************\
** MULTIPLE DEBUGGING SUBSYSTEMS CODE **
\**************************************/

/*
** Usage:  MDTRACE((subsys, level, fmt, ...))
** ""subsys"" is the debugging system to which this statement belongs.
** The significance of the subsystems is determined by the programmer,
** except that the functions such as db_print refer to subsystem 0.
** ""level"" is the debugging level which must be operational for the
** output to appear. ""fmt"" is a printf format string. ""..."" is
** whatever extra arguments fmt requires (possibly nothing).
** The non-debug macro means that the code is validated but never called.
*/
#ifdef DEBUG
#define MDTRACE(x)  db_mdprint x
#else
#define MDTRACE(x)  do { if (0) db_mdprint x; } while (0)
#endif /* DEBUG */

extern int      db_mdgetdebug(int subsys);
extern int      db_mdparsearg(char *arg);
extern int      db_mdsetdebug(int subsys, int level);
extern void     db_mdprint(int subsys, int level, const char *fmt,...);
extern void     db_mdsubsysnames(char const * const *names);

#endif /* DEBUG_H */

Single argument variant for C99 or later
Kyle Brandt asked:

Anyway to do this so debug_print still works even if there are no arguments? For example:


    debug_print(""Foo"");


There's one simple, old-fashioned hack:
debug_print(""%s\n"", ""Foo"");

The GCC-only solution shown below also provides support for that.
However, you can do it with the straight C99 system by using:
#define debug_print(...) \
            do { if (DEBUG) fprintf(stderr, __VA_ARGS__); } while (0)

Compared to the first version, you lose the limited checking that requires the 'fmt' argument, which means that someone could try to call 'debug_print()' with no arguments (but the trailing comma in the argument list to fprintf() would fail to compile).  Whether the loss of checking is a problem at all is debatable.
GCC-specific technique for a single argument
Some compilers may offer extensions for other ways of handling variable-length argument lists in macros.  Specifically, as first noted in the comments by Hugo Ideler, GCC allows you to omit the comma that would normally appear after the last 'fixed' argument to the macro.  It also allows you to use ##__VA_ARGS__ in the macro replacement text, which deletes the comma preceding the notation if, but only if, the previous token is a comma:
#define debug_print(fmt, ...) \
            do { if (DEBUG) fprintf(stderr, fmt, ##__VA_ARGS__); } while (0)

This solution retains the benefit of requiring the format argument while accepting optional arguments after the format.
This technique is also supported by Clang for GCC compatibility.

C23 and __VA_OPT__
Both C++20 (and later) and C23 (and later) add the __VA_OPT__ mechanism to handle the problem with __VA_ARGS__ that is handled by GCC using the , ## __VA_ARGS__ notation.
You simply use:
#define debug_print(fmt, ...) \
        do { if (DEBUG) fprintf(stderr, fmt, __VA_OPT__(,) __VA_ARGS__); } while (0)

The argument to __VA_OPT__ is added to the output if __VA_ARGS__ is not empty (and nothing is added if __VA_ARGS__ is empty).  This should be available if __STDC_VERSION__ >= 202311L — but be aware that GCC 14.1.0 (still) sets __STDC_VERSION__ == 202000 when compiling with -std=c23 or -std=iso9899:2024.

Why the do-while loop?

What's the purpose of the do while here?

You want to be able to use the macro so it looks like a function call, which means it will be followed by a semi-colon.  Therefore, you have to package the macro body to suit.  If you use an if statement without the surrounding do { ... } while (0), you will have:
/* BAD - BAD - BAD */
#define debug_print(...) \
            if (DEBUG) fprintf(stderr, __VA_ARGS__)

Now, suppose you write:
if (x > y)
    debug_print(""x (%d) > y (%d)\n"", x, y);
else
    do_something_useful(x, y);

Unfortunately, that indentation doesn't reflect the actual control of flow, because the preprocessor produces code equivalent to this (indented and braces added to emphasize the actual meaning):
if (x > y)
{
    if (DEBUG)
        fprintf(stderr, ""x (%d) > y (%d)\n"", x, y);
    else
        do_something_useful(x, y);
}

The next attempt at the macro might be:
/* BAD - BAD - BAD */
#define debug_print(...) \
            if (DEBUG) { fprintf(stderr, __VA_ARGS__); }

And the same code fragment now produces:
if (x > y)
    if (DEBUG)
    {
        fprintf(stderr, ""x (%d) > y (%d)\n"", x, y);
    }
; // Null statement from semi-colon after macro
else
    do_something_useful(x, y);

And the else is now a syntax error.  The do { ... } while(0) loop avoids both these problems.
There's one other way of writing the macro which might work:
/* BAD - BAD - BAD */
#define debug_print(...) \
            ((void)((DEBUG) ? fprintf(stderr, __VA_ARGS__) : 0))

This leaves the program fragment shown as valid.  The (void) cast prevents it being used in contexts where a value is required — but it could be used as the left operand of a comma operator where the do { ... } while (0) version cannot.  If you think you should be able to embed debug code into such expressions, you might prefer this.  If you prefer to require the debug print to act as a full statement, then the do { ... } while (0) version is better.  Note that if the body of the macro involved any semi-colons (roughly speaking), then you can only use the do { ... } while(0) notation.  It always works; the expression statement mechanism can be more difficult to apply.  You might also get warnings from the compiler with the expression form that you'd prefer to avoid; it will depend on the compiler and the flags you use.


_TPOP was previously at http://plan9.bell-labs.com/cm/cs/tpop and http://cm.bell-labs.com/cm/cs/tpop but both are now (2015-08-10) broken._


Code in GitHub
If you're curious, you can look at this code in GitHub in my SOQ (Stack
Overflow Questions) repository as files debug.c, debug.h, mddebug.c and mddebug.h in the
src/libsoq
sub-directory.
",c
finding a point and its angle on a cubic bezier curve in javascript,"I need to find a point and its angle on a cubic Bezier curve that can be dynamically changed using JavaScript.
I asked ChatGPT about this, to which it generated the following code, but the angle is not calculated correctly, where am I or is ChatGPT wrong?
      // Initialize with some initial control points
      let points = [
        { x: 50, y: 100 }, // Start point
        { x: 150, y: 50 }, // First control point
        { x: 250, y: 150 }, // Second control point
        { x: 350, y: 100 } // End point
      ];

      function deCasteljau(points, t) {
        if (points.length === 1) {
          return points[0];
        }

        const newPoints = [];
        for (let i = 0; i < points.length - 1; i++) {
          const x = (1 - t) * points[i].x + t * points[i + 1].x;
          const y = (1 - t) * points[i].y + t * points[i + 1].y;
          newPoints.push({ x, y });
        }

        return deCasteljau(newPoints, t);
      }

      function cubicBezierDerivative(points, t) {
        const derivativePoints = [];
        const n = points.length - 1;
        for (let i = 0; i < n; i++) {
            const dx = n * (points[i + 1].x - points[i].x);
            const dy = n * (points[i + 1].y - points[i].y);
            derivativePoints.push({ x: dx, y: dy });
        }
        return derivativePoints;
      }


      function bezierAngle(points, t) {
        const dPoints = cubicBezierDerivative(points, t);
        const point = deCasteljau(points, t);
        const dx = dPoints[0].x;
        const dy = dPoints[0].y;
        const radian = Math.atan2(dy, dx);
        //const angle = radian*180/Math.PI;
        return radian;
      }
      const point = deCasteljau(points, 0.9);
          
      const angle = bezierAngle(points, 0.9);



live demo:


const canvas = document.getElementById('splineCanvas');
const ctx = canvas.getContext('2d');

let points = []; // Array to hold control points
let selectedPointIndex = -1; // Index of the currently selected control point

// Event listener for mouse down to select control point
canvas.addEventListener('mousedown', function(event) {
    const rect = canvas.getBoundingClientRect();
    const mouseX = event.clientX - rect.left;
    const mouseY = event.clientY - rect.top;

    // Check if mouse is over any control point
    for (let i = 0; i < points.length; i++) {
        const dx = points[i].x - mouseX;
        const dy = points[i].y - mouseY;
        const dist = Math.sqrt(dx * dx + dy * dy);
        if (dist < 6) { // 6 is the radius for selecting control point
            selectedPointIndex = i;
            canvas.addEventListener('mousemove', onMouseMove);
            canvas.addEventListener('mouseup', onMouseUp);
            break;
        }
    }
});

// Event listener for mouse move to update control point position
function onMouseMove(event) {
    const rect = canvas.getBoundingClientRect();
    const mouseX = event.clientX - rect.left;
    const mouseY = event.clientY - rect.top;

    points[selectedPointIndex].x = mouseX;
    points[selectedPointIndex].y = mouseY;
    drawSpline();
}

// Event listener for mouse up to stop updating control point position
function onMouseUp() {
    canvas.removeEventListener('mousemove', onMouseMove);
    canvas.removeEventListener('mouseup', onMouseUp);
    selectedPointIndex = -1;
}

let testAngle = 65;

// Draw spline function
function drawSpline() {
    ctx.clearRect(0, 0, canvas.width, canvas.height);
    ctx.beginPath();
    ctx.moveTo(points[0].x, points[0].y);

    for (let i = 1; i < points.length - 2; i+=3) {
        ctx.bezierCurveTo(
          points[i].x, 
          points[i].y,
          points[i+1].x,
          points[i+1].y,
          points[i+2].x,
          points[i+2].y,
        );
    }

    ctx.stroke();

    // Draw control points
    for (const point of points) {
        ctx.beginPath();
        ctx.arc(point.x, point.y, 6, 0, Math.PI * 2);
        ctx.fillStyle = ""#ff0000"";
        ctx.fill();
        ctx.closePath();
    }

    const point = deCasteljau(points, 0.9);
    //console.log('point = ', point);
    const angle = bezierAngle(points, 0.9);
    ctx.save();
    ctx.translate(point.x, point.y);
    ctx.rotate(angle);
    ctx.translate(-point.x, -point.y);

    ctx.fillStyle = ""green"";
    ctx.fillRect(point.x-5, point.y-5, 10, 10);
    ctx.restore();
}

// Initialize with some initial control points
points = [
  { x: 50, y: 100 }, // Start point
  { x: 150, y: 50 }, // First control point
  { x: 250, y: 150 }, // Second control point
  { x: 350, y: 100 } // End point
];

function deCasteljau(points, t) {
  if (points.length === 1) {
    return points[0];
  }

  const newPoints = [];
  for (let i = 0; i < points.length - 1; i++) {
    const x = (1 - t) * points[i].x + t * points[i + 1].x;
    const y = (1 - t) * points[i].y + t * points[i + 1].y;
    newPoints.push({ x, y });
  }

  return deCasteljau(newPoints, t);
}

function cubicBezierDerivative(points, t) {
  const derivativePoints = [];
  const n = points.length - 1;
  for (let i = 0; i < n; i++) {
      const dx = n * (points[i + 1].x - points[i].x);
      const dy = n * (points[i + 1].y - points[i].y);
      derivativePoints.push({ x: dx, y: dy });
  }
  return derivativePoints;
}


function bezierAngle(points, t) {
  const dPoints = cubicBezierDerivative(points, t);
  const point = deCasteljau(points, t);
  const dx = dPoints[0].x;
  const dy = dPoints[0].y;
  const radian = Math.atan2(dy, dx);
  //const angle = radian*180/Math.PI;
  return radian;
}

drawSpline();
<canvas id=""splineCanvas"" width=""600"" height=""300""></canvas>



","Just from a math perspective I question what you mean by ""finding a point and its angle"" points in 2D space, like you represented in the points array, are just {x, y} there is no angle ...

What we can do is calculate the angle between two points, and looks like that is what the functions cubicBezierDerivative & bezierAngle attempted to do, I'm assuming that is what you need/ask, I would base my code below on that, also I'm going to assume that the point returned by function deCasteljau is correct, I'm not going to spend any research time into how is that doing what is doing.
So we can modify the function bezierAngle to return the angle between two given points, the current point and and what I call ""next"" point:
    const point = deCasteljau(points, x);
    const next = deCasteljau(points, x + 0.01);
    const angle = bezierAngle([point, next]);

with the angle between those two the square we draw is ""facing"" the right way.
In the code below you will see a new function drawRect that is where we draw the squares, and since it is now a function we can have multiple square with different colors


const canvas = document.getElementById('splineCanvas');
const ctx = canvas.getContext('2d');

let points = [
  { x: 50,  y: 100 }, // Start point
  { x: 170, y: 20  }, // First control point
  { x: 240, y: 170 }, // Second control point
  { x: 350, y: 10  }  // End point
]
let selectedPointIndex = -1;

// Event listener for mouse down to select control point
canvas.addEventListener('mousedown', function(event) {
    const rect = canvas.getBoundingClientRect();
    const mouseX = event.clientX - rect.left;
    const mouseY = event.clientY - rect.top;

    // Check if mouse is over any control point
    for (let i = 0; i < points.length; i++) {
        const dx = points[i].x - mouseX;
        const dy = points[i].y - mouseY;
        const dist = Math.sqrt(dx * dx + dy * dy);
        if (dist < 6) { // 6 is the radius for selecting control point
            selectedPointIndex = i;
            canvas.addEventListener('mousemove', onMouseMove);
            canvas.addEventListener('mouseup', onMouseUp);
            break;
        }
    }
});

// Event listener for mouse move to update control point position
function onMouseMove(event) {
    const rect = canvas.getBoundingClientRect();
    const mouseX = event.clientX - rect.left;
    const mouseY = event.clientY - rect.top;

    points[selectedPointIndex].x = mouseX;
    points[selectedPointIndex].y = mouseY;
    drawSpline();
}

// Event listener for mouse up to stop updating control point position
function onMouseUp() {
    canvas.removeEventListener('mousemove', onMouseMove);
    canvas.removeEventListener('mouseup', onMouseUp);
    selectedPointIndex = -1;
}

let testAngle = 65;

// Draw spline function
function drawSpline() {
    ctx.clearRect(0, 0, canvas.width, canvas.height);
    ctx.beginPath();
    ctx.moveTo(points[0].x, points[0].y);

    ctx.bezierCurveTo(
      points[1].x, points[1].y,
      points[2].x, points[2].y,
      points[3].x, points[3].y,
    );
    ctx.stroke();

    // Draw control points
    for (const point of points) {
        ctx.beginPath();
        ctx.arc(point.x, point.y, 6, 0, Math.PI * 2);
        ctx.fillStyle = ""#ff0000"";
        ctx.fill();
        ctx.closePath();
    }
   drawRect(0.2, ""cyan"")
   drawRect(0.5, ""blue"")
   drawRect(0.8, ""green"")
}

function drawRect(x, color) {
    const point = deCasteljau(points, x);
    const next = deCasteljau(points, x + 0.01);
    const angle = bezierAngle([point, next]);
    ctx.save();
    ctx.globalAlpha = 0.6
    ctx.translate(point.x, point.y);
    ctx.rotate(angle);
    ctx.translate(-point.x, -point.y);

    ctx.fillStyle = color;
    ctx.fillRect(point.x-15, point.y-15, 30, 30);
    //ctx.fillStyle = ""black"";
    //ctx.fillText(angle, point.x, point.y);
    ctx.restore();
}

function deCasteljau(points, t) {
  if (points.length === 1) {
    return points[0];
  }

  const newPoints = [];
  for (let i = 0; i < points.length - 1; i++) {
    const x = (1 - t) * points[i].x + t * points[i + 1].x;
    const y = (1 - t) * points[i].y + t * points[i + 1].y;
    newPoints.push({ x, y });
  }

  return deCasteljau(newPoints, t);
}

function bezierAngle(points) {
  const dx = points[1].x - points[0].x;
  const dy = points[1].y - points[0].y;
  const radian = Math.atan2(dy, dx);
  return radian;
}

drawSpline();
<canvas id=""splineCanvas"" width=""400"" height=""180""></canvas>



",javascript
java  execute multiple runnables in parallel and trigger to timeout some of long running tasks,"I've a list of Runnable tasks (e.g. it contains 100 tasks, and each task takes randomly 1 - 10 seconds). The tasks must run in parallel and from the thread pool provided by ExecutorService (e.g. my system has 4 CPUs, then it should run 4 tasks at the same time).
The question is: I wanted to know which tasks took longer than 5 seconds to run from the list of 100 tasks and they should be terminated (with logs of task ids) after 5 seconds to have places for other tasks.
I've looked at Future with executorService.submit(Runnable task) but the Future.get() method will block the main thread and it is not what I wanted. Any suggestions would be great.
public class TestExecutorService {

    private static final ExecutorService executorService = Executors.newFixedThreadPool(Runtime.getRuntime().availableProcessors() * 10);

    public static void main(String[] args) throws InterruptedException {
        List<Callable<Object>> tasks = new ArrayList<>();

        for (int i = 0; i < 100; i++) {
            int finalI = i;
            int min = 1;
            int max = 8;

            Runnable runnable = new Runnable() {
                @Override
                public void run() {
                    try {
                        int sleepTime = min + (int)(Math.random() * ((max - min) + 1));
                        System.out.println(""## Thread: "" + finalI + "" will sleep: "" + sleepTime + "" seconds."");
                        Thread.sleep(sleepTime * 1000);
                        System.out.println(""## Thread: "" + finalI + "" finished after: "" + sleepTime + "" seconds"");
                    } catch (InterruptedException e) {
                        System.out.println(""Thread is cancelled!"");
                    }
                }
            };

            tasks.add(Executors.callable(runnable));
        }

        // How to make a Runnable task timeout after 5 seconds when running other tasks in parallel
        // instead of total time for 100 tasks in 5 seconds?
        executorService.invokeAll(tasks, 5, TimeUnit.SECONDS);

        executorService.shutdown();
    }
}

","Lets say you have some list of tasks, and an Executor to
List<Runnable> tasks = ...;
ExecutorService executor = ...;

Now you want to perform each task, get the time it takes, and cancel the task if it takes too long. I would suggest scheduling a time out action.
ScheduledExecutorService timeoutService = Executors.newSingleThreadScheduledExecutor();

Now when you're submitting your tasks.
List<Future<Long>> results = new ArrayList<>();
for(int i = 0; i<tasks.size(); i++){
    Runnable task = tasks.get(i);
    Future<Long> future = executor.submit( () ->{
        long start = System.currentTimeMillis();
        task.run();
        return System.currentTimeMillis() - start;
    });
    Future<?> timeout = timeoutService.schedule( ()->{
        if(!future.isDone()){
            future.cancel(true);
        }
    }, 5, TimeUnit.SECONDS);
    results.add(future);
}

Now you can just go through results and call get when all of the tasks have finished, either exceptionally or normally, you will finish going through the results list. This assumes your tasks can be cancelled or interrupted If they cannot, then you can use the timeout futures.
",java
resolve hostnames using etchosts only,"I'm trying to resolve a hostname to an IP address using the /etc/hosts file only.
That is, if the name cannot be locally resolved, it must fail (ie. no DNS request sent out).
The standard way to resolve a name is with getaddrinfo, but this will fallback to DNS.
gethostent_r seems a good candidate since it returns records from the local hosts file, but unfortunately, I do not get IPv6 records, only IPv4.
How can I resolve a name to an IPv4 or IPv6 address based on the local /etc/hosts file only ?
","
gethostent_r seems a good candidate since it returns records from the local hosts file, but unfortunately, I do not get IPv6 records, only IPv4.

That doesn't sound right to me. It should return both types, assuming they are actually in /etc/hosts – but it will likely return them as two separate entries, as it goes through the file line-by-line.
Other methods:
If you are using glibc, you might be able to call __nss_configure_lookup(""hosts"", ""files"") – an internal glibc function which the getent tool uses to implement its -s option (as in {""getent"", ""-s"", ""hosts"", arg}).
A slightly better method, also for glibc, is to directly dlopen() the corresponding libnss ""backend"" module that is responsible for providing hostname lookup from /etc/hosts; that is, libnss_files.so.2. The interface between glibc and the libnss modules is a bit messy (especially the several different ""hostname"" functions), but it is stable.
Either of those might be preferable to writing your own parser, although not due to CPU usage – rather, due to the high chance that your custom parser will be incomplete and won't match the way glibc itself parses /etc/hosts. Make sure to test your parser with multiple address mappings for the same name; multiple names for the same address; mixed IPv4 and IPv6 mappings for the same name; etc. Otherwise users/sysadmins may be very annoyed.

it takes more CPU time for something the system has already done

The system hasn't really ""already done"" it. On most Linux systems there is no central cache for /etc/hosts; it is parsed independently by each process. (Some systems might have the 'nscd' general-purpose name-lookup cache daemon running but it's uncommon.) So if you do gethostbyname() and let libc read /etc/hosts, that's really the same as parsing it yourself.
",c
how to generate an array which is a multiple of original,"I'm trying to upsize OpenCV images in Python in such a manner that the individual pixels are spread out by an integral factor; I use this to visually examine deep detail and individual pixel values can be seen (using cv2.imshow in this instance).
For example, an array:
    [[1,2],
     [3,4]]


And a factor of 2 means I'd get:
    [[1,1,2,2],
     [1,1,2,2],
     [3,3,4,4], 
     [3,3,4,4]]


I've done this by generating an array of size*factor using np.zeros, then iterating each point in the original array and copying it to the target array using (for example)
for y in range(src.shape[0]):
    for x in range(src.shape[1]):
        tgt[y*f:y*f+f, x*f:x*f+f, :] = src[y,x,:]

But as you can imagine, it's not the fastest approach, and I'm hoping I'm just not finding the right thing.
OpenCV (and PIL) do not have a resize capability that doesn't interpolate by one method or another, which seems weird all by itself.
I looked over & tried numpy broadcast*, numpy stride_trickks, opencv functions, PIL functions.
The semi-manual method works as long as I don't need interactivity, but I'm trying to adjust parameters to several opencv functions quickly so I can find the right combinations to solve my problem.  (Which is proprietary, so I can't share imagery...)  Waiting a significant time between results is counterproductive.
","You can use opencv's cv.resize with nearest-neighbor as interpolation method (cv.INTER_NEAREST) to achieve what you need:
import cv2 as cv
import numpy as np

src = np.array([[1,2], [3,4]])
dst = cv.resize(src, (4,4), interpolation=cv.INTER_NEAREST)
print(dst)

Output:
[[1 1 2 2]
 [1 1 2 2]
 [3 3 4 4]
 [3 3 4 4]]

Live demo
",numpy
how to use a different cnn without losing accuracy,"I have been given a task to implement a Convolutional neural network that can evaluate hand-written digits found in the MNIST dataset with the architecture of the network looking like this:

I have implemented a CNN that matches the architecture, unfortunately it only has about a 10% accuracy to it. I've looked online and tried other example CNNs to make sure if anything else causing the issue, however they seem to work fine and give me a ~99% accuracy. I've placed both CNNs in my code and made a boolean switch to show the difference between the two:
import tensorflow
from tensorflow.keras.datasets import mnist
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout, Flatten
from tensorflow.keras.layers import Conv2D, MaxPooling2D

batch_size = 128
num_classes = 10
epochs = 1
img_rows, img_cols = 28, 28


(x_train, y_train), (x_test, y_test) = mnist.load_data()

x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)
x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)
input_shape = (img_rows, img_cols, 1)

x_train = x_train.astype('float32')
x_test = x_test.astype('float32')
x_train /= 255
x_test /= 255
print('x_train shape:', x_train.shape)
print(x_train.shape[0], 'train samples')
print(x_test.shape[0], 'test samples')

y_train = tensorflow.keras.utils.to_categorical(y_train, num_classes)
y_test = tensorflow.keras.utils.to_categorical(y_test, num_classes)

exampleModel = False  # Use to toggle which CNN goes into the model

if exampleModel:  # An example CNN that I found for MNIST
    model = Sequential()
    model.add(Conv2D(32, kernel_size=(3, 3),
                     activation='relu',
                     input_shape=input_shape))
    model.add(Conv2D(64, (3, 3), activation='relu'))
    model.add(MaxPooling2D(pool_size=(2, 2)))
    model.add(Dropout(0.25))
    model.add(Flatten())
    model.add(Dense(128, activation='relu'))
    model.add(Dropout(0.5))
    model.add(Dense(num_classes, activation='softmax'))
else:  # The CNN I created
    input_layer = tensorflow.keras.layers.Input(shape=input_shape)
    conv1 = Conv2D(32, (1, 1), activation='relu')(input_layer)
    pool1 = MaxPooling2D(2, 2)(conv1)
    conv2_1 = Conv2D(64, (1, 1), activation='relu', padding='same')(pool1)
    pool2_1 = MaxPooling2D(2, 2)(conv2_1)
    drop2_1 = Dropout(0.5)(pool2_1)
    conv2_2 = Conv2D(64, (1, 1), activation='relu', padding='same')(pool1)
    pool2_2 = MaxPooling2D(2, 2)(conv2_2)
    drop2_2 = Dropout(0.5)(pool2_2)
    conv3_1 = Conv2D(256, (1, 1), activation='relu', padding='same')(drop2_1)
    conv3_2 = Conv2D(256, (1, 1), activation='relu', padding='same')(drop2_2)
    merged = tensorflow.keras.layers.concatenate([conv3_1, conv3_2], axis=-1)
    merged = Dropout(0.5)(merged)
    merged = Flatten()(merged)
    fc1 = Dense(1000, activation='relu')(merged)
    fc2 = Dense(500, activation='relu')(fc1)
    out = Dense(10)(fc2)
    model = tensorflow.keras.models.Model(input_layer, out)

model.compile(loss=tensorflow.keras.losses.categorical_crossentropy,
              optimizer=tensorflow.keras.optimizers.Adadelta(),
              metrics=['accuracy'])

model.fit(x_train, y_train,
          batch_size=batch_size,
          epochs=epochs,
          verbose=1,
          validation_data=(x_test, y_test))
score = model.evaluate(x_test, y_test, verbose=0)
print('Test loss:', score[0])
print('Test accuracy:', score[1])

In order to complete my task, I believe I have to convert my example CNN piece-by-piece into the required architecture. Although I have no idea how to do this,  they look completely different from each other (one is purely sequential, the other uses parallel layers and merging).
","You simply have to add an softmax activation to the last, out layer:
out = Dense(10, activation=""softmax"")(fc2)

Thus your model in completed form:
input_layer = tensorflow.keras.layers.Input(shape=input_shape)
conv1 = Conv2D(32, (1, 1), activation='relu')(input_layer)
pool1 = MaxPooling2D(2, 2)(conv1)
conv2_1 = Conv2D(64, (1, 1), activation='relu', padding='same')(pool1)
pool2_1 = MaxPooling2D(2, 2)(conv2_1)
drop2_1 = Dropout(0.5)(pool2_1)
conv2_2 = Conv2D(64, (1, 1), activation='relu', padding='same')(pool1)
pool2_2 = MaxPooling2D(2, 2)(conv2_2)
drop2_2 = Dropout(0.5)(pool2_2)
conv3_1 = Conv2D(256, (1, 1), activation='relu', padding='same')(drop2_1)
conv3_2 = Conv2D(256, (1, 1), activation='relu', padding='same')(drop2_2)
merged = tensorflow.keras.layers.concatenate([conv3_1, conv3_2], axis=-1)
merged = Dropout(0.5)(merged)
merged = Flatten()(merged)
fc1 = Dense(1000, activation='relu')(merged)
fc2 = Dense(500, activation='relu')(fc1)
out = Dense(10, activation=""softmax"")(fc2)

Out:
x_train shape: (60000, 28, 28, 1)
60000 train samples
10000 test samples
Train on 60000 samples, validate on 10000 samples
Epoch 1/1
60000/60000 [==============================] - 25s 416us/step - loss: 0.6394 - acc: 0.7858 - val_loss: 0.2956 - val_acc: 0.9047
Test loss: 0.29562548571825026
Test accuracy: 0.9047

",tensorflow
matlab39s treebagger and kfold cross validation,"I am trying to get the 5-fold cross validation error of a model created with TreeBagger using the function crossval but I keep getting an error

Error using crossval>evalFun
The function 'regrTree' generated the following error:
Too many input arguments.

My code is below. Can anyone point me in the right direction? Thanks
%Random Forest
%%XX is training data matrix, Y is training labels vector
XX=X_Tbl(:,2:end);
Forest_Mdl = TreeBagger(1000,XX,Y,'Method','regression');

err_std = crossval('mse',XX,Y,'Predfun',@regrTree, 'kFold',5);


function yfit_std = regrTree(Forest_Mdl,XX) 
yfit_std = predict(Forest_Mdl,XX);
end

","Reading the documentation helps a lot!:
The function has to be defined as:
(note that it takes 3 arguments, not 2)

function yfit = myfunction(Xtrain,ytrain,Xtest)
% Calculate predicted response
...
end

Xtrain — Subset of the observations in X used as training predictor
data. The function uses Xtrain and ytrain to construct a
classification or regression model.
ytrain — Subset of the responses in y used as training response data.
The rows of ytrain correspond to the same observations in the rows of
Xtrain. The function uses Xtrain and ytrain to construct a
classification or regression model.
Xtest — Subset of the observations in X used as test predictor data.
The function uses Xtest and the model trained on Xtrain and ytrain to
compute the predicted values yfit.
yfit — Set of predicted values for observations in Xtest. The yfit
values form a column vector with the same number of rows as Xtest.

",cross-validation
modulenotfounderror no module named 39distutilsmsvccompiler39 when trying to install numpy 116,"I'm working inside a conda environment and I'm trying to downgrade numpy to version 1.16, but when running pip install numpy==1.16 I keep getting the following error:
$ pip install numpy==1.16
Collecting numpy==1.16
  Downloading numpy-1.16.0.zip (5.1 MB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 5.1/5.1 MB 10.8 MB/s eta 0:00:00
  Preparing metadata (setup.py) ... done
Building wheels for collected packages: numpy
  Building wheel for numpy (setup.py) ... error
  error: subprocess-exited-with-error
  
  × python setup.py bdist_wheel did not run successfully.
  │ exit code: 1
  ╰─> [17 lines of output]
      Running from numpy source directory.
      /tmp/pip-install-jdof0z8r/numpy_4597057bbb504aa18b7bda112f0aa37f/numpy/distutils/misc_util.py:476: SyntaxWarning: ""is"" with a literal. Did you mean ""==""?
        return is_string(s) and ('*' in s or '?' is s)
      Traceback (most recent call last):
        File ""<string>"", line 2, in <module>
        File ""<pip-setuptools-caller>"", line 34, in <module>
        File ""/tmp/pip-install-jdof0z8r/numpy_4597057bbb504aa18b7bda112f0aa37f/setup.py"", line 415, in <module>
          setup_package()
        File ""/tmp/pip-install-jdof0z8r/numpy_4597057bbb504aa18b7bda112f0aa37f/setup.py"", line 394, in setup_package
          from numpy.distutils.core import setup
        File ""/tmp/pip-install-jdof0z8r/numpy_4597057bbb504aa18b7bda112f0aa37f/numpy/distutils/core.py"", line 26, in <module>
          from numpy.distutils.command import config, config_compiler, \
        File ""/tmp/pip-install-jdof0z8r/numpy_4597057bbb504aa18b7bda112f0aa37f/numpy/distutils/command/config.py"", line 19, in <module>
          from numpy.distutils.mingw32ccompiler import generate_manifest
        File ""/tmp/pip-install-jdof0z8r/numpy_4597057bbb504aa18b7bda112f0aa37f/numpy/distutils/mingw32ccompiler.py"", line 34, in <module>
          from distutils.msvccompiler import get_build_version as get_build_msvc_version
      ModuleNotFoundError: No module named 'distutils.msvccompiler'
      [end of output]
  
  note: This error originates from a subprocess, and is likely not a problem with pip.
  ERROR: Failed building wheel for numpy
  Running setup.py clean for numpy
  error: subprocess-exited-with-error
  
  × python setup.py clean did not run successfully.
  │ exit code: 1
  ╰─> [10 lines of output]
      Running from numpy source directory.
      
      `setup.py clean` is not supported, use one of the following instead:
      
        - `git clean -xdf` (cleans all files)
        - `git clean -Xdf` (cleans all versioned files, doesn't touch
                            files that aren't checked into the git repo)
      
      Add `--force` to your command to use it anyway if you must (unsupported).
      
      [end of output]
  
  note: This error originates from a subprocess, and is likely not a problem with pip.
  ERROR: Failed cleaning build dir for numpy
Failed to build numpy
ERROR: ERROR: Failed to build installable wheels for some pyproject.toml based projects (numpy)

How can I resolve this?
","What seemed to solve this issue was installing a specific version of setuptools (<65) into the conda environment:
conda install ""setuptools <65""

",numpy
custom crossvalidation and pycaret,"I've been working with hierarchical time series, and, as a result, I needed to create my own CV to make sure that all timestamps and products are included evenly in the test (validation) set. It worked just fine for sklearn, but I can't make it work in pycaret: best = compare_models() yields nothing at all. Here is the custom CV I used:
class custom_cv:
    def __init__(self, train_end, test_size, n_splits): # val_size
        self.train_end = train_end
        # self.val_size = val_size
        self.test_size = test_size
        self.n_splits = n_splits
        
    def split(self, X):
        self.X = X
        
        for i in range(self.n_splits, 0, -1): # range(start, stop, step)
            tr_threshol = self.train_end - self.test_size*i
            te_threshol = tr_threshol + self.test_size
    
            tr_idx = np.array(self.X.reset_index(drop = True).index[self.X['N_month'] <= tr_threshol])
            te_idx = np.array(self.X.index[(self.X['N_month'] > tr_threshol) & (self.X['N_month'] <= te_threshol)])
        
            yield(tr_idx, te_idx)

    custom_CV = custom_cv(train_end = 365, test_size = 28, n_splits = 5)
    # custom_CV = custom_CV.split(X = df)

My Data looks like this: 1
For sklearn I used the following loop:
def custom_cv(df, train_end = 36, test_size = 4, n_splits = 4):
    cv_idx = []

    for i in range(n_splits, 0, -1): # range(start, stop, step)
        tr_threshol = train_end - test_size*i
        te_threshol = tr_threshol + test_size
    
        tr_idx = list(df.reset_index(drop = True).index[df['N_month'] <= tr_threshol])
        te_idx = list(df.index[(df['N_month'] > tr_threshol) & (df['N_month'] <= te_threshol)])
    
        cv_idx.append((tr_idx, te_idx))
    
    return cv_idx

custom_CV = custom_cv(df = df, train_end = 365, test_size = 28, n_splits = 5)

However, pycaret requires a custom CV generator object compatible with scikit-learn (something I've never dealt with before). I can't figure out what's wrong exactly, and I hope you can kindly help me out.
","Your class for Pycaret is probably missing the get_n_splits method.I had similar problem and solved with the class structure like here:
How to generate a custom cross-validation generator in scikit-learn?
",cross-validation
implement a search engine chain using tavily in langchain,"I want to implement a search engine chain using tavily in langchain. This chain gives user's query as an input and returns up to 5 related documents. Each retrieved document must have the content of the document as page_content and the url of the corresponding site as metadata under the definition of LangChain Documents. I must use langchain_core.documents.base.Document class to define documents. So this chain will have two main parts:

Tavily search platform
Parser with the aim of converting search output data into standard LangChai documents.

I wrote this code but I don't know how to change tavily output format into standard form of document:
from langchain_core.documents.base import Document
from langchain_community.tools.tavily_search import TavilySearchResults

search = TavilySearchResults(max_results=5)

class ParsedDocument(BaseModel):
    content: str = Field(description=""This refers to the content of the search."")
    url: str = Field(description=""This refers to the url of the search."")

search_parser = PydanticOutputParser(pydantic_object=ParsedDocument)
search_engine_chain = search | search_parser


I would be grateful if you could help me how to change this code.
","I finally found the answer:
class ParsedDocument(BaseModel):
    content: str = Field(description=""This refers to the content of the search."")
    url: str = Field(description=""This refers to the url of the search."")

# Define a custom parser
def custom_parser(search_results):
    parsed_documents = []
    for result in search_results:  # Adjust this line based on the actual structure of search_results
        parsed_document = ParsedDocument(content=result['content'], url=result['url'])
        document = Document(page_content=parsed_document.content, metadata={'url': parsed_document.url})
        parsed_documents.append(document)
    return parsed_documents

search_engine_chain = search | custom_parser

",langchain
customize header pandas to html,"I created a web application to search content in CSV files.for now i only able to put all content into 1 column with comma as delimiter ,
I want to display the data as shown in the image.
My python code
selected_columns = [202, 9, 10, 135, 13, 127, 126, 26, 28,128,129,31,33]
df = df.iloc[:, selected_columns]
df.columns = ['CDRID', 'ANumber', 'To', 'SipStatus', 'ISrcIP', 'IDstIP', 'ISrcRTP', 'IDstRTP', 'OSrcIP', 'ODstIP', 'OSrcRTP', 'ODstRTP']]
df['Ingress'] = df[['ISrcIP', 'IDstIP', 'ISrcRTP', 'IDstRTP']].apply(lambda x: ', '.join(x.dropna().astype(str)), axis=1)
df['Egress'] = df[[ 'OSrcIP', 'ODstIP', 'OSrcRTP', 'ODstRTP']].apply(lambda x: ', '.join(x.dropna().astype(str)), axis=1)
df = df.drop(columns=['ISrcIP', 'IDstIP', 'ISrcRTP', 'IDstRTP', 'OSrcIP', 'ODstIP', 'OSrcRTP', 'ODstRTP'])    
df = df.drop(columns=['ISrcIP', 'IDstIP', 'ISrcRTP', 'IDstRTP', 'OSrcIP', 'ODstIP', 'OSrcRTP', 'ODstRTP'])
combined_df = pd.concat([combined_df, df], ignore_index=True)
results.append(('Combined Results', combined_df.to_html(index=False)))

html frontend code
        {% for file, table in results %}
    <div class=""container-fluid"">
    <div class=""table-responsive"">
        <table class=""table table-striped table-bordered table-hover"">
            {{ table|safe }}
        </table>
    </div>
{% endfor %}

current output

expected html header

","You can use multi-index to have several levels of column headers and you can use .style.set_table_styles to customize the display of the dataframe:
cols = ['CDRID', 'ANumber', 'To', 'SipStatus', 'ISrcIP', 'IDstIP', 'ISrcRTP', 'IDstRTP', 
        'OSrcIP', 'ODstIP', 'OSrcRTP', 'ODstRTP']
df = pd.DataFrame(np.round(100*np.random.random((3, 12))), columns=cols)

# Use multi-index
df.columns = pd.MultiIndex.from_arrays([4*[""""] + 4*[""Ingress""] + 4*[""Outgress""], df.columns])

# Add borders and hide the index
df_styled = df.style.set_table_styles([
     {""selector"": ""td"", ""props"": [(""border"", ""1px solid black"")]},  # cells borders
     {""selector"": ""th"", ""props"": [(""border"", ""1px solid black"")]},  # headers borders
     {""selector"": ""th"", ""props"": [(""text-align"", ""center"")]},  # text headers alignment
])
display(df_styled.hide(axis=""index""))
# print(df_styled.to_html())  # to get the HTML/CSS code


",pandas
gridsearchcv  xgboost  early stopping,"i am trying to do hyperparemeter search with using scikit-learn's GridSearchCV on XGBoost. During gridsearch i'd like it to early stop, since it reduce search time drastically and (expecting to) have better results on my prediction/regression task. I am using XGBoost via its Scikit-Learn API.
    model = xgb.XGBRegressor()
    GridSearchCV(model, paramGrid, verbose=verbose ,fit_params={'early_stopping_rounds':42}, cv=TimeSeriesSplit(n_splits=cv).get_n_splits([trainX, trainY]), n_jobs=n_jobs, iid=iid).fit(trainX,trainY)

I tried to give early stopping parameters with using fit_params, but then it throws this error which is basically because of lack of validation set which is required for early stopping:
/opt/anaconda/anaconda3/lib/python3.5/site-packages/xgboost/callback.py in callback(env=XGBoostCallbackEnv(model=<xgboost.core.Booster o...teration=4000, rank=0, evaluation_result_list=[]))
    187         else:
    188             assert env.cvfolds is not None
    189 
    190     def callback(env):
    191         """"""internal function""""""
--> 192         score = env.evaluation_result_list[-1][1]
        score = undefined
        env.evaluation_result_list = []
    193         if len(state) == 0:
    194             init(env)
    195         best_score = state['best_score']
    196         best_iteration = state['best_iteration']

How can i apply GridSearch on XGBoost with using early_stopping_rounds?
note: model is working without gridsearch, also GridSearch works without 'fit_params={'early_stopping_rounds':42}
","An update to @glao's answer and a response to @Vasim's comment/question, as of sklearn 0.21.3 (note that fit_params has been moved out of the instantiation of GridSearchCV and been moved into the fit() method; also, the import specifically pulls in the sklearn wrapper module from xgboost):
import xgboost.sklearn as xgb
from sklearn.model_selection import GridSearchCV
from sklearn.model_selection import TimeSeriesSplit

cv = 2

trainX= [[1], [2], [3], [4], [5]]
trainY = [1, 2, 3, 4, 5]

# these are the evaluation sets
testX = trainX 
testY = trainY

paramGrid = {""subsample"" : [0.5, 0.8]}

fit_params={""early_stopping_rounds"":42, 
            ""eval_metric"" : ""mae"", 
            ""eval_set"" : [[testX, testY]]}

model = xgb.XGBRegressor()

gridsearch = GridSearchCV(model, paramGrid, verbose=1,             
         cv=TimeSeriesSplit(n_splits=cv).get_n_splits([trainX, trainY]))

gridsearch.fit(trainX, trainY, **fit_params)

",data-science
huggingface  valueerror connection error and we cannot find the requested files in the cached path please try again or make sure your internet con,"Not always, but occasionally when running my code this error appears. At first, I doubted it was a connectivity issue but to do with cashing issue, as discussed on an older Git Issue.
Clearing cache didn't help runtime:
$ rm ~/.cache/huggingface/transformers/*

Traceback references:

NLTK also gets Error loading stopwords: <urlopen error [Errno -2] Name or service not known.
Last 2 lines re cached_path and get_from_cache.


Cache (before cleared):
$ cd ~/.cache/huggingface/transformers/
(sdg) me@PF2DCSXD:~/.cache/huggingface/transformers$ ls
16a2f78023c8dc511294f0c97b5e10fde3ef9889ad6d11ffaa2a00714e73926e.cf2d0ecb83b6df91b3dbb53f1d1e4c311578bfd3aa0e04934215a49bf9898df0
16a2f78023c8dc511294f0c97b5e10fde3ef9889ad6d11ffaa2a00714e73926e.cf2d0ecb83b6df91b3dbb53f1d1e4c311578bfd3aa0e04934215a49bf9898df0.json
16a2f78023c8dc511294f0c97b5e10fde3ef9889ad6d11ffaa2a00714e73926e.cf2d0ecb83b6df91b3dbb53f1d1e4c311578bfd3aa0e04934215a49bf9898df0.lock
4029f7287fbd5fa400024f6bbfcfeae9c5f7906ea97afcaaa6348ab7c6a9f351.723d8eaff3b27ece543e768287eefb59290362b8ca3b1c18a759ad391dca295a.h5
4029f7287fbd5fa400024f6bbfcfeae9c5f7906ea97afcaaa6348ab7c6a9f351.723d8eaff3b27ece543e768287eefb59290362b8ca3b1c18a759ad391dca295a.h5.json
4029f7287fbd5fa400024f6bbfcfeae9c5f7906ea97afcaaa6348ab7c6a9f351.723d8eaff3b27ece543e768287eefb59290362b8ca3b1c18a759ad391dca295a.h5.lock
684fe667923972fb57f6b4dcb61a3c92763ad89882f3da5da9866baf14f2d60f.c7ed1f96aac49e745788faa77ba0a26a392643a50bb388b9c04ff469e555241f
684fe667923972fb57f6b4dcb61a3c92763ad89882f3da5da9866baf14f2d60f.c7ed1f96aac49e745788faa77ba0a26a392643a50bb388b9c04ff469e555241f.json
684fe667923972fb57f6b4dcb61a3c92763ad89882f3da5da9866baf14f2d60f.c7ed1f96aac49e745788faa77ba0a26a392643a50bb388b9c04ff469e555241f.lock
c0c761a63004025aeadd530c4c27b860ec4ecbe8a00531233de21d865a402598.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b
c0c761a63004025aeadd530c4c27b860ec4ecbe8a00531233de21d865a402598.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b.json
c0c761a63004025aeadd530c4c27b860ec4ecbe8a00531233de21d865a402598.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b.lock
fc674cd6907b4c9e933cb42d67662436b89fa9540a1f40d7c919d0109289ad01.7d2e0efa5ca20cef4fb199382111e9d3ad96fd77b849e1d4bed13a66e1336f51
fc674cd6907b4c9e933cb42d67662436b89fa9540a1f40d7c919d0109289ad01.7d2e0efa5ca20cef4fb199382111e9d3ad96fd77b849e1d4bed13a66e1336f51.json
fc674cd6907b4c9e933cb42d67662436b89fa9540a1f40d7c919d0109289ad01.7d2e0efa5ca20cef4fb199382111e9d3ad96fd77b849e1d4bed13a66e1336f51.lock

Code:
from transformers import pipeline, set_seed
generator = pipeline('text-generation', model='gpt2')  # Error
set_seed(42)

Traceback:
2022-03-03 10:18:06.803989: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory
2022-03-03 10:18:06.804057: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
[nltk_data] Error loading stopwords: <urlopen error [Errno -2] Name or
[nltk_data]     service not known>
2022-03-03 10:18:09.216627: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory
2022-03-03 10:18:09.216700: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)
2022-03-03 10:18:09.216751: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (PF2DCSXD): /proc/driver/nvidia/version does not exist
2022-03-03 10:18:09.217158: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-03-03 10:18:09.235409: W tensorflow/python/util/util.cc:368] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.
All model checkpoint layers were used when initializing TFGPT2LMHeadModel.

All the layers of TFGPT2LMHeadModel were initialized from the model checkpoint at gpt2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.
Traceback (most recent call last):
  File ""/home/me/miniconda3/envs/sdg/lib/python3.8/runpy.py"", line 194, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File ""/home/me/miniconda3/envs/sdg/lib/python3.8/runpy.py"", line 87, in _run_code
    exec(code, run_globals)
  File ""/mnt/c/Users/me/Documents/GitHub/project/foo/bar/__main__.py"", line 26, in <module>
    nlp_setup()
  File ""/mnt/c/Users/me/Documents/GitHub/project/foo/bar/utils/Modeling.py"", line 37, in nlp_setup
    generator = pipeline('text-generation', model='gpt2')
  File ""/home/me/miniconda3/envs/sdg/lib/python3.8/site-packages/transformers/pipelines/__init__.py"", line 590, in pipeline
    tokenizer = AutoTokenizer.from_pretrained(
  File ""/home/me/miniconda3/envs/sdg/lib/python3.8/site-packages/transformers/models/auto/tokenization_auto.py"", line 463, in from_pretrained
    tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)
  File ""/home/me/miniconda3/envs/sdg/lib/python3.8/site-packages/transformers/models/auto/tokenization_auto.py"", line 324, in get_tokenizer_config
    resolved_config_file = get_file_from_repo(
  File ""/home/me/miniconda3/envs/sdg/lib/python3.8/site-packages/transformers/file_utils.py"", line 2235, in get_file_from_repo
    resolved_file = cached_path(
  File ""/home/me/miniconda3/envs/sdg/lib/python3.8/site-packages/transformers/file_utils.py"", line 1846, in cached_path
    output_path = get_from_cache(
  File ""/home/me/miniconda3/envs/sdg/lib/python3.8/site-packages/transformers/file_utils.py"", line 2102, in get_from_cache
    raise ValueError(
ValueError: Connection error, and we cannot find the requested files in the cached path. Please try again or make sure your Internet connection is on.


Failed Attempts

I closed my IDE and bash terminal. Ran wsl.exe --shutdown in PowerShell. Relaunched IDE and bash terminal with same error.
Disconnecting and using a different VPN.
Clear cache $ rm ~/.cache/huggingface/transformers/*.

","Since I am working in a conda venv and using Poetry for handling dependencies, I needed to re-install torch - a dependency for Hugging Face 🤗 Transformers.

First, install torch:
PyTorch's website lets you chose your exact setup/ specification for install. In my case, the command was
conda install pytorch torchvision torchaudio cudatoolkit=10.2 -c pytorch

Then add to Poetry:
poetry add torch

Both take ages to process. Runtime was back to normal :)
",huggingface-transformers
prettyprint an entire pandas series  dataframe,"I work with Series and DataFrames on the terminal a lot. The default __repr__ for a Series returns a reduced sample, with some head and tail values, but the rest missing.
Is there a builtin way to pretty-print the entire Series / DataFrame?  Ideally, it would support proper alignment, perhaps borders between columns, and maybe even color-coding for the different columns.
","You can also use the option_context, with one or more options:
with pd.option_context('display.max_rows', None, 'display.max_columns', None):  # more options can be specified also
    print(df)

This will automatically return the options to their previous values.
If you are working on jupyter-notebook, using display(df) instead of print(df) will use jupyter rich display logic (like so).
",pandas
matplotlib cmget_cmapname num_steps,"My python code uses
plt.cm.getcmap(""coolwarm"", num_steps)
This causes a deprecation warning.
The get_cmap function was deprecated in Matplotlib 3.7 and will be removed in 3.11. Use matplotlib.colormaps[name] or matplotlib.colormaps.get_cmap() or pyplot.get_cmap() instead.
however - both suggested methods do not provide a ""num_steps"" parameter. How do I need to update my statement to be compatible?
","In [29]: import matplotlib
    ...: 
    ...: cw = matplotlib.colormaps['coolwarm']
    ...: cw10 = matplotlib.colormaps['coolwarm'].resampled(10)
    ...: print(cw.N, cw10.N)
256 10

In [30]: 

",matplotlib
return arguments from function calling with openai api when streaming,"I've made a simple OpenAI API example with function calling. I'm only using function calling to format the response, I'm not calling multiple functions or any external APIs.
When I don't stream the response I can return the function arguments, which is the data that I need.
In my NextJS route handler:
export async function POST(request: Request) {
  try {
    const openai = new OpenAI({
      apiKey: process.env[""OPENAI_API_KEY""],
    });
    const response = await openai.chat.completions.create({
      model: ""gpt-4"",
      // stream: true,
      messages: [
        {
          role: ""user"",
          content: ""Give me 5 questions and answers for a pub quiz"",
        },
      ],
      tools: [
        {
          type: ""function"",
          function: {
            name: ""get_questions_and_answers"",
            description: ""Get questions and answers"",
            parameters: simpleJsonSchema,
          },
        },
      ],
      tool_choice: {
        type: ""function"",
        function: { name: ""get_questions_and_answers"" },
      },
    });
    return Response.json(
       JSON.parse(
         response.choices[0].message.tool_calls?.[0].function.arguments || """",
       ),
    );
  } catch (serverError) {
    console.error({ serverError });
    throw new Error();
  }
}

simpleJsonSchema.json:
{
  ""type"": ""object"",
  ""properties"": {
    ""getQuestions"": {
      ""type"": ""array"",
      ""items"": {
        ""type"": ""object"",
        ""properties"": {
          ""Question"": {""type"": ""string""},
          ""Answer"": {""type"": ""string""}
        },
        ""required"": [""Question"", ""Answer""]
      }
    }
  },
  ""required"": [""getQuestions""]
}

Response from API:
{""getQuestions"":[{""Question"":""What is the capital of Australia?"",""Answer"":""Canberra""},{""Question"":""Who wrote 'To Kill a Mockingbird'?"",""Answer"":""Harper Lee""},{""Question"":""What is the highest peak in the world?"",""Answer"":""Mount Everest""},{""Question"":""Who is known as the 'Father of Computers'?"",""Answer"":""Charles Babbage""},{""Question"":""What is the largest ocean in the world?"",""Answer"":""Pacific Ocean""}]}

This is fine when developing locally, however when deployed to Vercel the request sometimes times out. I've tried to add streaming as this is the recommended solution:
const response = await openai.chat.completions.create({
  model: ""gpt-4"",
  stream: true,
  messages: [
    {
      role: ""user"",
      content: ""Give me 5 questions and answers for a pub quiz"",
    },
  ],
  tools: [
    {
      type: ""function"",
      function: {
        name: ""get_questions_and_answers"",
        description: ""Get questions and answers"",
        parameters: simpleJsonSchema,
      },
    },
  ],
  tool_choice: {
    type: ""function"",
    function: { name: ""get_questions_and_answers"" },
  },
});

const stream = OpenAIStream(response);
return new StreamingTextResponse(stream);

However now the response has a lot of unnecessary data. And when I try to JSON.parse on the client I get errors.
Response from API:
{""tool_calls"":[ {""id"": ""call_IhxvzkZ5EsmZpHc6tOznTmzb"", ""type"": ""function"", ""function"": {""name"": ""get_questions_and_answers"", ""arguments"": ""{\n  \""getQuestions\"": [\n    {\n      \""Question\"": \""Question 1\"",\n      \""Answer\"": \""Answer 1\""\n    },\n    {\n      \""Question\"": \""Question 2\"",\n      \""Answer\"": \""Answer 2\""\n    },\n    {\n      \""Question\"": \""Question 3\"",\n      \""Answer\"": \""Answer 3\""\n    },\n    {\n      \""Question\"": \""Question 4\"",\n      \""Answer\"": \""Answer 4\""\n    },\n    {\n      \""Question\"": \""Question 5\"",\n      \""Answer\"": \""Answer 5\""\n    }\n  ]\n}""}}

As far as I can see the docs only cover using useChat but I have some particular requirements so I need to handle the fetching and form state myself: https://sdk.vercel.ai/docs/api-reference/use-chat
Why am I getting invalid JSON?
Here is a repository which reproduces the issue: https://github.com/jameschetwood/openai-function-calling
","this is the response you are getting:
{""tool_calls"":[ {""id"": ""call_HRxqlP3yzeHsoN43tMyZjMlr"", ""type"": ""function"", ""function"": {""name"": ""get_questions_and_answers"", ""arguments"": ""{\n  \""getQuestions\"": [\n    {\n      \""Question\"": \""What is the capital city of France?\"",\n      \""Answer\"": \""Paris\""\n    },\n    {\n      \""Question\"": \""Who painted the Mona Lisa?\"",\n      \""Answer\"": \""Leonardo da Vinci\""\n    },\n    {\n      \""Question\"": \""What is the largest planet in our solar system?\"",\n      \""Answer\"": \""Jupiter\""\n    },\n    {\n      \""Question\"": \""What is the national flower of England?\"",\n      \""Answer\"": \""Rose\""\n    },\n    {\n      \""Question\"": \""Which country is famous for its tulips?\"",\n      \""Answer\"": \""Netherlands\""\n    }\n  ]\n}""}}

I used https://jsoneditoronline.org/ to auto correct the json and it just adds ""]}"". for some reason openai is not sending correct json response. you have to add it
accumulatedText += ""]}"";

then response works:

this is too specific error. if openai updates its response api, it might send the json data correctly. so a better approach would be parsing in try/catch
try {
      const parsed = JSON.parse(accumulatedText);
      console.log({ parsed });
    } catch (error) {
      // you should error for each specific case
      accumulatedText += ""]}"";
      console.log(""correct accumulatedText in catch block"", accumulatedText);
    }

",chatgpt
pca in python reproducing pcafit_transform results using pcafit,"I have a data frame called data_principal_components with dimensions (306x21154), so 306 observations and 21154 features. Using PCA, I want to project the data into 10 dimensions.
As far as I understand, the following code does this. The resulting matrix (projected) has a dimension of (306x10).
import numpy as np
import pandas as pd
from sklearn.decomposition import PCA

# Sample data:
# Define the dimensions of the DataFrame
num_rows = 306
num_cols = 21154

# Generate random numbers from a normal distribution
data = np.random.randn(num_rows, num_cols)

# Create a DataFrame from the random data
data_principal_components = pd.DataFrame(data)

pca = PCA(10)  
projected = pca.fit_transform(data_principal_components)
```

To better understand how the code works, I wanted to reproduce the result of pca.fit_transform() manually.
Based on my research, I found the following steps:
pc_components = pca.components_  # This gives the eigenvectors
pc_components = pc_components.transpose()  # Transposes the eigenvectors, so it has the dimensions (21154x10)
eigenvalues = pca.explained_variance_  # These are the eigenvalues with dimensions (1x10)

Now, as I understand, one can calculate the loadings using the following code based on the formula $\text{loadings} = \text{eigenvectors} \times \sqrt{\text{eigenvalues}}$ :
# Create an empty DataFrame
df = pd.DataFrame()

# Iterate over eigenvalues
for i in range(len(eigenvalues)):
    result = np.dot(pc_components[:, i], np.sqrt(eigenvalues[i]))
    df[f'Result_{i+1}'] = result  # Assign result as a new column in the DataFrame
    
loadings = df
```

After obtaining the loadings with dimensions (21154x10), I wanted to use them to obtain the projected values with $ \text{Actual values} \times \text{loadings}$ resulting in dimensions (306x21154) $\times$ (21154x10) = (306x10):
test = np.dot(data_principal_components, loadings)

However, when I compare test to projected, the values differ substantially. Where am I wrong?
EDIT
I found this way to extract the loadings. However, I still want to derive them semi-manually, can somone help?:
pca = PCA(10)  # project from 64 to 2 dimensions
projected = pca.fit_transform(data_principal_components)

loadings = pd.DataFrame(pca.components_.T, columns=['PC1', 'PC2','PC3', 'PC4','PC5', 'PC6','PC7', 'PC8','PC9', 'PC10'], index=data_principal_components.columns)
loadings

","You ask for two different things here:

I wanted to reproduce the result of pca.fit_transform()

When you ask about the result of pca.fit_transform(), you're asking about the principal component (PC) scores.

I found this way to extract the loadings. However, I still want to
derive them semi-manually, can somone help?

Here you ask about the loadings.
In a nutshell, using singular value decomposition, you can decompose your data matrix as X = USV.T where

U -> normalized PC scores
S -> singular values which are directly linked to the eigenvalues
V -> eigenvectors (often called loadings)

So this allows you to answer your second question: You obtain the loadings by applying a SVD to your data matrix:
data = data - data.mean(0)  # don't forget to center your data
U, S, VT = np.linalg.svd(data)
loadings = VT.T  # your loadings

Now, the result of pca.fit_transform() is just the projection of your centered data onto these loadings:
PC_scores = xr.dot(data, loadings[:, :10)  # use the first 10 components only

",scikit-learn
is there an ascii arrow that will fit seamlessly with the boxdrawing characters,"I'm getting creative with my prompt and on part of it I'm trying to do the following:
└→
Though it looks better than this, even in the console it has a small gap.

Does anyone know of an arrow character that would not do this?
","This seems to work with most fonts:

►
9658
U+25Ba



",prompt
llama321binstruct generate inconsistent output,"I want to use Llama-3.2-1B-Instruct model, and although I have set ""temperature"": 0.0, ""top_p"":0.0 and ""top_k"":0, it still generates inconsistent output. This is how my pipeline looks like:
pipe = pipeline(
    ""text-generation"",
    model=model_id,
    torch_dtype=torch.bfloat16,
    device_map=""mps"",
        model_kwargs={""temperature"": 0.0,
                  ""do_sample"":True,
                              ""top_p"":0.0,
                              ""top_k"":0,},
)

Any idea how to solve this issue?
","The model inconsistent output can be due to two main factors:
1. Temperature:
setting temperature to zero give more inconsistent result. You can refer Opeani discussion page for detail.
So the best option is to set temperature to very low values such as 0.00001 instead of zero.
2. do_sample
You already set it false, and it should remain that way only.
",huggingface-transformers
what does 39 page 39 do in rrstudio,"What does ::Page{} do in R/RStudio ?
I'm studying Data Science through IBM certification course in coursera and the notes contain this line of code in all the code blocks and no explanation to what the ""Page"" function is doing
#load ggplot package

library(ggplot2)

::page{title=""create a scatterplot of displacement (disp) and miles per gallon (mpg)""}

ggplot(aes(x=disp,y=mpg,),data=mtcars)+geom_point()


I tried googling but found no help, there was something similar ""page()"" instead of ""Page{}"" as in my notes, but that was something completely different.
","Ig it was just a code mistake on the author side, cuz if I run without those colons it runs fine as intended.
",data-science
how to speed up pandas string function why is str slower than applylambda,"I am using the pandas vectorized str.split() method to extract the first element returned from a split on ""~"".  I also have also tried using df.apply() with a lambda and str.split() to produce equivalent results.  When using %timeit, I'm finding that df.apply() is performing faster than the vectorized version.
Everything that I have read about vectorization seems to indicate that the first version should have better performance.  Can someone please explain why I am getting these results? Example:
    id     facility      
0   3466   abc~24353  
1   4853   facility1~3.4.5.6   
2   4582   53434_Facility~34432~cde   
3   9972   facility2~FACILITY2~343
4   2356   Test~23 ~FAC1  

The above dataframe has about 500,000 rows and I have also tested at around 1 million with similar results. Here is some example input and output:
Vectorization
In [1]: %timeit df['facility'] = df['facility'].str.split('~').str[0]
1.1 s ± 54.4 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)

Lambda Apply
In [2]: %timeit df['facility'] = df['facility'].astype(str).apply(lambda s: s.split('~')[0])
650 ms ± 52.1 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)

Does anyone know why I am getting this behavior?
","Pandas string methods are only ""vectorized"" in the sense that you don't have to write the loop yourself. There isn't actually any parallelization going on, because string (especially regex problems) are inherently difficult (impossible?) to parallelize. If you really want speed, you actually should fall back to python here. 
%timeit df['facility'].str.split('~', n=1).str[0]
%timeit [x.split('~', 1)[0] for x in df['facility'].tolist()]

411 ms ± 10.2 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)
132 ms ± 302 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)

For more information on when loops are faster than pandas functions, take a look at For loops with pandas - When should I care?.
As for why apply is faster, I'm of the belief that the function apply is applying (i.e., str.split) is a lot more lightweight than the string splitting happening in the bowels of Series.str.split.
",pandas
pytorch optimizer adamw and adam with weight decay,"Is there any difference between torch.optim.Adam(weight_decay=0.01) and torch.optim.AdamW(weight_decay=0.01)?
Link to the docs: torch.optim.
","Yes, Adam and AdamW weight decay are different.

Hutter pointed out in their paper (Decoupled Weight Decay Regularization) that the way weight decay is implemented in Adam in every library seems to be wrong, and proposed a simple way (which they call AdamW) to fix it.

In Adam, the weight decay is usually implemented by adding wd*w (wd is weight decay here) to the gradients (Ist case), rather than actually subtracting from weights (IInd case).
# Ist: Adam weight decay implementation (L2 regularization)
final_loss = loss + wd * all_weights.pow(2).sum() / 2
# IInd: equivalent to this in SGD
w = w - lr * w.grad - lr * wd * w


These methods are same for vanilla SGD, but as soon as we add momentum, or use a more sophisticated optimizer like Adam, L2 regularization (first equation) and weight decay (second equation) become different.

AdamW follows the second equation for weight decay.
In Adam

weight_decay (float, optional) – weight decay (L2 penalty) (default: 0)

In AdamW

weight_decay (float, optional) – weight decay coefficient (default: 1e-2)

Read more on the fastai blog.
",pytorch
in c what are the rules governing how compilers merge the same strings into the executable,"I am trying to find what the rules are for c and c++ compilers putting strings into the data section of executables and don't know where to look. I would like to know if the address of all of the following are guaranteed to be the same in c/c++ by the spec:
char * test1 = ""hello"";
const char * test2 = ""hello"";
static char * test3 = ""hello"";
static const char * test4 = ""hello"";
extern const char * test5; // Defined in another compilation unit as ""hello""
extern const char * test6; // Defined in another shared object as ""hello""

Testing on windows, they are all the same. However I do not know if they would be on all operating systems.
","I would like to know if the address of all of the following are guaranteed to be the same in c/c++ by the spec
String literals are allowed to be the same object but are not required to.
C++ says:

(C++11, 2.14.5p12) ""Whether all string literals are distinct (that is, are stored in nonoverlapping objects) is implementation-defined. The effect of attempting to modify a string literal is undefined.""

C says:

(C11, 6.5.2.5p7) ""String literals, and compound literals with const-qualified types, need not designate distinct objects.101) This allows implementations to share storage for string literals and constant compound literals with the same or overlapping representations.""

And C99 Rationale says:

""This specification allows implementations to share copies of strings with identical text, to place string literals in read-only memory, and to perform certain optimizations""

",c
cannot compare tznaive and tzaware timestamps,"I'm finding the error below:
Cannot compare tz-naive and tz-aware timestamps
How can I convert dates to fix the issue? The error appear in the end of the syntax below.
from datetime import datetime, timedelta
import pytz
# Define start and end dates
start = datetime(2024, 6, 1)
end = datetime(2024, 12, 10)
current_date = start
# List to store valid dates
valid_dates = []

while current_date <= end:
    valid_dates.append(current_date.strftime('%Y-%m-%d'))
    current_date += timedelta(days=1)

df['OpenDate'] = pd.to_datetime(df['OpenDate'], utc=True)
df['CloseDate'] = pd.to_datetime(df['CloseDate'], errors='coerce', utc=True)

date_range = pd.date_range(start=start, end=end)

results = []

for index, row in df.iterrows():
    for current_date in date_range:
        if row['OpenDate'] < current_date and (pd.isnull(row['CloseDate']) or 
row['CloseDate'] > current_date) and row['CloseType'] in [0, 1, 3, 7, 8]:
            result_row = {
                'CurrentDate': current_date,
                'User_Id': row['User_Id'],
                'OpenDate': row['OpenDate'],
                'CloseDate': row['CloseDate'],
                'CloseType': row['CloseType']
            }
            results.append(result_row)

","try this to convert to tz-aware:
start = datetime(2024, 6, 1, tzinfo=pytz.UTC)
end = datetime(2024, 12, 10, tzinfo=pytz.UTC)

And to create a tz-aware time range:
date_range = pd.date_range(start=start, end=end, tz='UTC')

Complete code:
from datetime import datetime, timedelta
import pytz
import pandas as pd

# Define start and end dates (timezone-aware)
start = datetime(2024, 6, 1, tzinfo=pytz.UTC)
end = datetime(2024, 12, 10, tzinfo=pytz.UTC)
current_date = start

# List to store valid dates
valid_dates = []
while current_date <= end:
    valid_dates.append(current_date.strftime('%Y-%m-%d'))
    current_date += timedelta(days=1)

# Convert 'OpenDate' and 'CloseDate' to timezone-aware datetimes
df['OpenDate'] = pd.to_datetime(df['OpenDate'], utc=True)
df['CloseDate'] = pd.to_datetime(df['CloseDate'], errors='coerce', utc=True)

# Create a timezone-aware date range
date_range = pd.date_range(start=start, end=end, tz='UTC')

results = []
for index, row in df.iterrows():
    for current_date in date_range:
        if row['OpenDate'] < current_date and (pd.isnull(row['CloseDate']) or 
row['CloseDate'] > current_date) and row['CloseType'] in [0, 1, 3, 7, 8]:
            result_row = {
                'CurrentDate': current_date,
                'User_Id': row['User_Id'],
                'OpenDate': row['OpenDate'],
                'CloseDate': row['CloseDate'],
                'CloseType': row['CloseType']
            }
            results.append(result_row)

Hope this helps.
",pandas
real time object detection with yolo model not working,"I have trained a custom yolo model to detect square slots on a board, and is working with more than 95 % accuracy on images.
But as soon as I switch to video detection it seems to not detect even a single thing
I am using the following code to run real time object detection
cap = cv2.VideoCapture('../video/1st/output.mp4')
while cap.isOpened():
    ret, frame = cap.read()

    results = model(frame)
    final_img = np.squeeze(results.render())


    cv2.imshow(""YOLO"", final_img)
    if cv2.waitKey(10) & 0XFF == ord(""q""):
        break
cap.release()
cv2.destroyAllWindows()


I load the model using this code
model = torch.hub.load(""ultralytics/yolov5"", ""custom"", path=""yolov5/runs/train/exp36/weights/best.pt"",force_reload=True)
model.conf = 0.6

I have even tried splitting the available video into jpegs and running the model on individual images, saving the output and then merging the output images into a new video file.
that works perfectly, so the model is detecting something.
but as soon as I switch to video it seems to go back to nothing.
","Yes, You cannot see anything due to the none type of object return in the results.renders()/
You can change the code script like this
cap = cv2.VideoCapture('../video/1st/output.mp4')
while cap.isOpened():
    ret, frame = cap.read()

    results = model(frame)
    bboxes = results.xyxy[0].cpu().tolist()
    for bbox in bboxes:
      conf = f'{bbox[4]:.4f}' #Confidance of that prediction
      bbox = list(map(lambda x: int(x), bbox)) #To convert float to integer
      class_id = bbox[5] #Class_id 
      bbox =bbox[:4] 
      cv2.rectangle(frame,(bbox[0],bbox[1]),(bbox[2],bbox[3]),color=(255,255,255),thickness=3)
    cv2.imshow(""YOLO"", frame)
    if cv2.waitKey(10) & 0XFF == ord(""q""):
        break
cap.release()
cv2.destroyAllWindows()

and write the frames in the video
the full code should look like this
input_video_path = #Enter your video path
cap = cv2.VideoCapture(input_video_path)
fps = int(cap.get(cv2.CAP_PROP_FPS))
frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))

output_video_path = 'output_video.mp4' # Output video path
fourcc = cv2.VideoWriter_fourcc(*'XVID')
out = cv2.VideoWriter(output_video_path, fourcc, fps, (frame_width, frame_height))

model = #Here load your model
while True:
    ret, frame = cap.read()
    if not ret:
      break
    results = model(frame)
    bboxes = results.xyxy[0].cpu().tolist()
    for bbox in bboxes:
      conf = f'{bbox[4]:.4f}' #Confidance of that prediction
      bbox = list(map(lambda x: int(x), bbox)) #To convert float to integer
      class_id = bbox[5] #Class_id 
      bbox =bbox[:4] 
      cv2.rectangle(frame,(bbox[0],bbox[1]),(bbox[2],bbox[3]),color=(255,255,255),thickness=3)
    out.write(frame)
    #cv2_imshow(frame)
    if cv2.waitKey(10) & 0XFF == ord(""q""):
        break

cap.release()
out.release()

# Close all OpenCV windows
cv2.destroyAllWindows()

References:
https://docs.ultralytics.com/ 
https://docs.ultralytics.com/yolov5/ 
https://docs.ultralytics.com/yolov5/tutorials/pytorch_hub_model_loading/
",pytorch
calculating n mod m when m is not prime,"I have read a lot of good algos to calculate n! mod m but they were usually valid when m was prime . I wanted to know whether some good algo exists when m is not prime .I would be helpful if someone could write the basic function of the algo too.I have been using 
long long factMOD(long long n,long long mod)
{
    long long res = 1; 
    while (n > 0)
    {
        for (long long i=2, m=n%mod; i<=m; i++)
        res = (res * i) % mod;
        if ((n/=mod)%2 > 0) 
        res = mod - res;
    }
    return res;
}

but getting wrong answer when I try to print factMOD(4,3) even. source of this algo is :
http://comeoncodeon.wordpress.com/category/algorithm/
","This is what I've come up with:
#include <stdio.h>
#include <stdlib.h>

unsigned long long nfactmod(unsigned long long n, unsigned long long m)
{
    unsigned long long i, f;
    for (i = 1, f = 1; i <= n; i++) {
        f *= i;
        if (f > m) {
            f %= m;
        }
    }
    return f;
}

int main(int argc, char *argv[])
{
    unsigned long long n = strtoull(argv[1], NULL, 10);
    unsigned long long m = strtoull(argv[2], NULL, 10);

    printf(""%llu\n"", nfactmod(n, m));

    return 0;
}

and this:
h2co3-macbook:~ h2co3$ ./mod 1000000 1001001779
744950559
h2co3-macbook:~ h2co3$

runs in a fraction of a second.
",c
partialdependencedisplayfrom_estimator plots having lines with 0 values,"Need to evaluate the two way interaction between two variables after regressor model.
Used PartialDependenceDisplay.from_estimator to plot but the contour  lines inside the plot all have value 0.Not sure what might cause this.
Checked the data and model and there  are no problems while loading the model and data.
Checked the other two variable combinations and they have same issue.
from sklearn.inspection import partial_dependence, PartialDependenceDisplay
model = load_model(model_path)
model_features = model.feature_name_

fig, ax = plt.subplots(figsize=(10,5))
X = training_data[model_features]
PartialDependenceDisplay.from_estimator(model, X, features=[('temperature',  'speed')], ax=ax, n_jobs=-1, grid_resolution=20)


","Most probably your contour values are all < 0.005.
Contour labels are formatted as ""%2.2f"" and there appears to be no documented way of changing this format. The only workaround I could think of is to retrieve the labels and their values and replace the label texts:
import matplotlib.pyplot as plt
from matplotlib.text import Text
import numpy as np
from sklearn.datasets import make_friedman1
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.inspection import PartialDependenceDisplay

X, y = make_friedman1()
clf = GradientBoostingRegressor(n_estimators=10).fit(X, y)

pdd = PartialDependenceDisplay.from_estimator(clf, X, [0, (0, 1)])

for c in pdd.axes_[0][1].get_children():
  if isinstance(c, Text):
    try:
       label_value = float(c.get_text())
    except ValueError:
       continue
    idx = np.argmin(abs(pdd.contours_[0][1].levels - label_value))
    c.set_text(f'{pdd.contours_[0][1].levels[idx]:g}')


Update 1
The above method doesn't work if all existing labels are identical. A somewhat unreliable quick and dirty workaround would be to rely on the fact that the label texts are added to the Axes in ascending order. The first and last level are not labelled. This leads to the following example:
import matplotlib.pyplot as plt
from matplotlib.text import Text
from sklearn.datasets import make_friedman1
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.inspection import PartialDependenceDisplay

X, y = make_friedman1(random_state=42)
clf = GradientBoostingRegressor(n_estimators=10).fit(X, y)

pdd = PartialDependenceDisplay.from_estimator(clf, X, [0, (0, 1)])

i = 1
for c in pdd.axes_[0][1].get_children():
  if isinstance(c, Text) and c.get_text():
    c.set_text(f'{pdd.contours_[0][1].levels[i]:g}')
    i += 1


Update 2
Another (reliable but still hacky) possibility is to overwrite the clabel function used by Scikit with your own version that uses an appropriate format specification. In order to get hold of this function you'll have to provide your own Axes instance to PartialDependenceDisplay.from_estimator:
import matplotlib.pyplot as plt
from sklearn.datasets import make_friedman1
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.inspection import PartialDependenceDisplay

fig, axes = plt.subplots(ncols=2)

original_clabel = axes[1].clabel
def new_clabel(CS, **kwargs):
  del kwargs['fmt']
  return original_clabel(CS, fmt='%2.5f', **kwargs)
axes[1].clabel = new_clabel

X, y = make_friedman1(random_state=42)
clf = GradientBoostingRegressor(n_estimators=10).fit(X, y)

pdd = PartialDependenceDisplay.from_estimator(clf, X, [0, (0, 1)], ax=axes)


",scikit-learn
pandas change values of a dataframe based on an override,"I have a pandas dataframe which looks something like this.
orig |  dest |  type | class |  BKT   | BKT_order | value   | fc_Cap | sc_Cap
-----+-------+-------+-------+--------+-----------+---------+--------+---------
 AMD |  TRY  |   SA  | fc    |  MA    |   1       |   12.04 |   20   |   50
 AMD |  TRY  |   SA  | fc    |  TY    |   2       |   11.5  |   20   |   50
 AMD |  TRY  |   SA  | fc    |  NY    |   3       |   17.7  |   20   |   50
 AMD |  TRY  |   SA  | fc    |  MU    |   4       |   09.7  |   20   |   50
 AMD |  TRY  |   PE  | fc    |  RE    |   1       |   09.7  |   20   |   50
 AMD |  TRY  |   PE  | sc    |  EW    |   5       |   07.7  |   20   |   50
 NCL |  MNK  |   PE  | sc    |  PO    |   2       |   08.7  |   20   |   50
 NCL |  MNK  |   PE  | sc    |  TU    |   3       |   12.5  |   20   |   50
 NCL |  MNK  |   PE  | sc    |  MA    |   1       |   16.7  |   20   |   50

Also i have an override Dataframe which may look something like this:
orig |  dest |  type |  max_BKT 
-----+-------+-------+-----------
 AMD |  TRY  |   SA  |  TY
 NCL |  MNK  |   PE  |  PO
 NCL |  AGZ  |   PE  |  PO

what i want to do is modify the original dataframe such that after comparison of  orig dest type & BKT ( with max_BKT)  values, the value column for any rows which have the BKT_order higher than or equal to the max_BKT in override DF is set to either fc_Cap or sc_Cap depending on the class value.
For Example in above scenario,
Since the Override DF sets max_BKT as TY for AMD |  TRY  |   SA and the bucket order for TY is 2 in original Df, i need to set the value column equal to fc_Cap or sc_Cap
depending on the value of class for all rows where BKT_order >= 2
So basically:

filter the rows for orig dest type combination
Get the BKT_order of max_BKT from the Original DF
for each row that matches the above criteria

if class == fc update value column with fc_Cap
if class == sc update value column with sc_Cap



So our original DF looks something like this:
orig |  dest |  type | class |  BKT   | BKT_order | value   | fc_Cap | sc_Cap
-----+-------+-------+-------+--------+-----------+---------+--------+---------
 AMD |  TRY  |   SA  | fc    |  MA    |   1       |   12.04 |   20   |   50
 AMD |  TRY  |   SA  | fc    |  TY    |   2       |   20    |   20   |   50
 AMD |  TRY  |   SA  | fc    |  NY    |   3       |   20    |   20   |   50
 AMD |  TRY  |   SA  | fc    |  MU    |   4       |   20    |   20   |   50
 AMD |  TRY  |   PE  | fc    |  RE    |   1       |   09.7  |   20   |   50
 AMD |  TRY  |   PE  | sc    |  EW    |   5       |   07.7  |   20   |   50
 NCL |  MNK  |   PE  | sc    |  PO    |   2       |   50    |   20   |   50
 NCL |  MNK  |   PE  | sc    |  TU    |   3       |   50    |   20   |   50
 NCL |  MNK  |   PE  | sc    |  MA    |   1       |   16.7  |   20   |   50

I have tried an approach to iterate over the override df and try to handle 1 row at a time but, i get stuck when i need to do a reverse lookup to get the BKT_order  of the max_BKT from original df.
Hope that makes sense... i am fairly new to pandas.
","That's a fairly complex task. The individual steps are straightforward though.
You need:

indexing lookup to find the cap values based on class
merge to match the max_BKT
mask+groupby.transform to identify the rows to mask

idx, cols = pd.factorize(df['class']+'_Cap')

group = ['orig', 'dest', 'type']

out = (
    df.merge(override, on=group, how='left')
    .assign(
        value=lambda x: x['value'].mask(
            x['BKT_order'].ge(
                x['BKT_order']
                .where(x['BKT'].eq(x['max_BKT']))
                .groupby([x[c] for c in group])
                .transform('first')
            ),
            x.reindex(cols, axis=1).to_numpy()[np.arange(len(x)), idx],
        )
    )
    .reindex(columns=df.columns)
)

Output:
  orig dest type class BKT  BKT_order  value  fc_Cap  sc_Cap
0  AMD  TRY   SA    fc  MA          1  12.04      20      50
1  AMD  TRY   SA    fc  TY          2  20.00      20      50
2  AMD  TRY   SA    fc  NY          3  20.00      20      50
3  AMD  TRY   SA    fc  MU          4  20.00      20      50
4  AMD  TRY   PE    fc  RE          1   9.70      20      50
5  AMD  TRY   PE    sc  EW          5   7.70      20      50
6  NCL  MNK   PE    sc  PO          2  50.00      20      50
7  NCL  MNK   PE    sc  TU          3  50.00      20      50
8  NCL  MNK   PE    sc  MA          1  16.70      20      50

Intermediates:
tmp = df.merge(override, on=group, how='left')
tmp['cap'] = tmp.reindex(cols, axis=1).to_numpy()[np.arange(len(tmp)), idx]
tmp['mask'] = tmp['BKT'].eq(tmp['max_BKT'])
tmp['masked_BKT'] = tmp['BKT_order'].where(tmp['mask'])
tmp['ref_BKT'] = tmp.groupby(group)['masked_BKT'].transform('first')
tmp['>= ref_BKT'] = tmp['BKT_order'].ge(tmp['ref_BKT'])

  orig dest type class BKT  BKT_order  value  fc_Cap  sc_Cap max_BKT  cap   mask  masked_BKT  ref_BKT  >= ref_BKT
0  AMD  TRY   SA    fc  MA          1  12.04      20      50      TY   20  False         NaN      2.0       False
1  AMD  TRY   SA    fc  TY          2  11.50      20      50      TY   20   True         2.0      2.0        True
2  AMD  TRY   SA    fc  NY          3  17.70      20      50      TY   20  False         NaN      2.0        True
3  AMD  TRY   SA    fc  MU          4   9.70      20      50      TY   20  False         NaN      2.0        True
4  AMD  TRY   PE    fc  RE          1   9.70      20      50     NaN   20  False         NaN      NaN       False
5  AMD  TRY   PE    sc  EW          5   7.70      20      50     NaN   50  False         NaN      NaN       False
6  NCL  MNK   PE    sc  PO          2   8.70      20      50      PO   50   True         2.0      2.0        True
7  NCL  MNK   PE    sc  TU          3  12.50      20      50      PO   50  False         NaN      2.0        True
8  NCL  MNK   PE    sc  MA          1  16.70      20      50      PO   50  False         NaN      2.0       False

",pandas
emulating a packed structure in portable c,"I have the following structure:
typedef struct Octree {
    uint64_t *data;
    uint8_t alignas(8) alloc;
    uint8_t dataalloc;
    uint16_t size, datasize, node0;
    // Node8 is a union type with of size 16 omitted for brevity
    Node8 alignas(16) node[]; 
} Octree;

In order for the code that operates on this structure to work as intended, it is necessary that node0 immediately precedes the first node such that ((uint16_t *)Octree.node)[-1] will access Octree.node0. Each Node8 is essentially a union holding 8 uint16_t. With GCC I could force pack the structure with #pragma pack(push) and #pragma pack(pop). However this is non-portable. Another option is to:

Assume sizeof(uint64_t *) <= sizeof(uint64_t)
Store the structure as just 2 uint64_t followed immediately by the node data, and the members are accessed manually via bitwise arithmetic and pointer casts

This option is quite impractical. How else could I define this 'packed' data structure in a portable way? Are there any other ways?
","The C language standard does not allow you to specify a struct's memory layout down to the last bit. Other languages do (Ada and Erlang come to mind), but C does not.
So if you want actual portable standard C, you specify a C struct for your data, and convert from and to specific memory layout using pointers, probably composing from and decomposing into a lot of uint8_t values to avoid endianness issues. Writing such code is error prone, requires duplicating memory, and depending on your use case, it can be relatively expensive in both memory and processing.
If you want direct access to a memory layout via a struct in C, you need to rely on compiler features which are not in the C language specification, and therefore are not ""portable C"".
So the next best thing is to make your C code as portable as possible while at the same time preventing compilation of that code for incompatible platforms. You define the struct and provide platform/compiler specific code for each and every supported combination of platform and compiler, and the code using the struct can be the same on every platform/compiler.
Now you need to make sure that it is impossible to accidentally compile for a platform/compiler where the memory layout is not exactly the one your code and your external interface require.
Since C11, that is possible using static_assert, sizeof and offsetof.
So something like the following should do the job if you can require C11 (I presume you can require C11 as you are using alignas which is not part of C99 but is part of C11). The ""portable C"" part here is you fixing the code for each platform/compiler where the compilation fails due to one of the static_assert declarations failing.
#include <assert.h>
#include <stdalign.h>
#include <stddef.h>
#include <stdint.h>
#include <stdio.h>

typedef uint16_t Node8[8];

typedef struct Octree {
    uint64_t *data;
    uint8_t alignas(8) alloc;
    uint8_t dataalloc;
    uint16_t size, datasize, node0;
    Node8 alignas(16) node[];
} Octree;

static_assert(0x10 == sizeof(Octree),              ""Octree size error"");
static_assert(0x00 == offsetof(Octree, data),      ""Octree data position error"");
static_assert(0x08 == offsetof(Octree, alloc),     ""Octree alloc position error"");
static_assert(0x09 == offsetof(Octree, dataalloc), ""Octree dataalloc position error"");
static_assert(0x0a == offsetof(Octree, size),      ""Octree size position error"");
static_assert(0x0c == offsetof(Octree, datasize),  ""Octree datasize position error"");
static_assert(0x0e == offsetof(Octree, node0),     ""Octree node0 position error"");
static_assert(0x10 == offsetof(Octree, node),      ""Octree node[] position error"");

The series of static_assert declarations could be written more concisely with less redundant source code typing for the error messages using a preprocessor macro stringifying the struct name, member name, and maybe size/offset value.
Now that we have nailed down the struct member sizes and offsets within the struct, two aspects still need checks.

The integer endianness your code expects is the same endianness your memory structure contains. If the endianness happens to be ""native"", you have nothing to check for or to handle conversions. If the endianness is ""big endian"" or ""little endian"", you need to add some checks and/or do conversions.

As noted in the comments to the question, you will need to verify separately that the undefined behaviour &(((uint16_t *)octree.node)[-1]) == &octree.node0 actually is what you expect it to be on this compiler/platform.
Ideally, you would find a way to write this as a separate static_assert declaration. However, such a test is quick and short enough that you can add such a check to the runtime code in a rarely but guaranteed to be run function like a global initialization function, a library initialization functions, or maybe even a constructor. Do be cautious though if you use the assert() macro for that check, as that runtime check will turn into a no-op if the NDEBUG macro is defined.


",c
how do i subdividerefine a dimension in an xarray dataset,"Summary: I have a dataset that is collected in such a way that the dimensions are not initially available. I would like to take what is essentially a big block of undifferentiated data and add dimensions to it so that it can be queried, subsetted, etc. That is the core of the following question.
Here is an xarray DataSet that I have:
<xarray.Dataset>
Dimensions:  (chain: 1, draw: 2000, rows: 24000)
Coordinates:
  * chain    (chain) int64 0
  * draw     (draw) int64 0 1 2 3 4 5 6 7 ... 1993 1994 1995 1996 1997 1998 1999
  * rows     (rows) int64 0 1 2 3 4 5 6 ... 23994 23995 23996 23997 23998 23999
Data variables:
    obs      (chain, draw, rows) float64 4.304 3.985 4.612 ... 6.343 5.538 6.475
Attributes:
    created_at:                 2019-12-27T17:16:13.847972
    inference_library:          pymc3
    inference_library_version:  3.8

The rows dimension here corresponds to a number of subdimensions that I need to restore to the data.  In particular, the 24,000 rows correspond to 100 samples each from 240 conditions (these 100 samples are in contiguous blocks).  These conditions are combinations of gate, input, growth medium, and od.
I would like to end up with something like this:
<xarray.Dataset>
Dimensions:  (chain: 1, draw: 2000, gate: 1, input: 4, growth_medium: 3, sample: 100, rows: 24000)
Coordinates:
  * chain    (chain) int64 0
  * draw     (draw) int64 0 1 2 3 4 5 6 7 ... 1993 1994 1995 1996 1997 1998 1999
  * rows     *MultiIndex*
  * gate     (gate) int64 'AND'
  * input    (input) int64 '00', '01', '10', '11'
  * growth_medium (growth_medium) 'standard', 'rich', 'slow'
  * sample   (sample) int64 0 1 2 3 4 5 6 7 ... 95 96 97 98 99
Data variables:
    obs      (chain, draw, gate, input, growth_medium, samples) float64 4.304 3.985 4.612 ... 6.343 5.538 6.475
Attributes:
    created_at:                 2019-12-27T17:16:13.847972
    inference_library:          pymc3
    inference_library_version:  3.8

I have a pandas dataframe that specifies the values of gate, input, and growth medium -- each row gives a set of values of gate, input, and growth medium, and an index that specifies where (in the rows) the corresponding set of 100 samples appears.  The intent is that this data frame is a guide for labeling the Dataset.
I looked at the xarray docs on ""Reshaping and Reorganizing Data"", but I don't see how to combine those operations to do what I need.  I suspect somehow I need to combine these with GroupBy, but I don't get how. Thanks!
Later: I have a solution to this problem, but it is so disgusting that I am hoping someone will explain how wrong I am, and what a more elegant approach is possible.
So, first, I extracted all the data in the original Dataset into raw numpy form:
foo = qm.idata.posterior_predictive['obs'].squeeze('chain').values.T
foo.shape # (24000, 2000)

Then I reshaped it as needed:
bar = np.reshape(foo, (240, 100, 2000))

This gives me roughly the shape I want: there are 240 different experimental conditions, each has 100 variants, and for each of these variants, I have 2000 Monte Carlo samples in my data set.
Now, I extract the information about the 240 experimental conditions from the Pandas DataFrame:
import pandas as pd
# qdf is the original dataframe with the experimental conditions and some
# extraneous information in other columns
new_df = qdf[['gate', 'input', 'output', 'media', 'od_lb', 'od_ub', 'temperature']]
idx = pd.MultiIndex.from_frame(new_df)

Finally, I reassembled a DataArray from the numpy array and the pandas MultiIndex:
xr.DataArray(bar, name='obs', dims=['regions', 'conditions', 'draws'],
             coords={'regions': idx, 'conditions': range(100), 'draws': range(2000)})

The resulting DataArray has these coordinates, as I wished:
Coordinates:
  * regions      (regions) MultiIndex
  - gate         (regions) object 'AND' 'AND' 'AND' 'AND' ... 'AND' 'AND' 'AND'
  - input        (regions) object '00' '10' '10' '10' ... '01' '01' '11' '11'
  - output       (regions) object '0' '0' '0' '0' '0' ... '0' '0' '0' '1' '1'
  - media        (regions) object 'standard_media' ... 'high_osm_media_five_percent'
  - od_lb        (regions) float64 0.0 0.001 0.001 ... 0.0001 0.0051 0.0051
  - od_ub        (regions) float64 0.0001 0.0051 0.0051 2.0 ... 0.0003 2.0 2.0
  - temperature  (regions) int64 30 30 37 30 37 30 37 ... 37 30 37 30 37 30 37
  * conditions   (conditions) int64 0 1 2 3 4 5 6 7 ... 92 93 94 95 96 97 98 99
  * draws        (draws) int64 0 1 2 3 4 5 6 ... 1994 1995 1996 1997 1998 1999

That was pretty horrible, though, and it seems wrong that I had to punch through all the nice layers of xarray abstraction to get to this point.  Especially since this does not seem like an unusual piece of a scientific workflow: getting a relatively raw data set together with a spreadsheet of metadata that needs to be combined with the data.  So what am I doing wrong? What's the more elegant solution?
","Given the starting Dataset, similar to:
<xarray.Dataset>
Dimensions:  (draw: 2, row: 24)
Coordinates:
  * draw     (draw) int32 0 1
  * row      (row) int32 0 1 2 3 4 5 6 7 8 9 ... 14 15 16 17 18 19 20 21 22 23
Data variables:
    obs      (draw, row) int32 0 1 2 3 4 5 6 7 8 ... 39 40 41 42 43 44 45 46 47

You can concatenate several pure xarray commands to subdivide the dimensions (get the data in the same shape but using a multiindex) or even reshape the Dataset. To subdivide the dimensions, the following code can be used:
multiindex_ds = ds.assign_coords(
    dim_0=[""a"", ""b"", ""c""], dim_1=[0,1], dim_2=range(4)
).stack(
    stacked_dim=(""dim_0"", ""dim_1"", ""dim_2"")
).reset_index(
    ""row"", drop=True
).rename(
    row=""stacked_dim""
)
multiindex_ds

whose output is:
<xarray.Dataset>
Dimensions:  (stacked_dim: 24, draw: 2)
Coordinates:
  * draw     (draw) int32 0 1
  * stacked_dim      (stacked_dim) MultiIndex
  - dim_0    (stacked_dim) object 'a' 'a' 'a' 'a' 'a' 'a' ... 'c' 'c' 'c' 'c' 'c' 'c'
  - dim_1    (stacked_dim) int64 0 0 0 0 1 1 1 1 0 0 0 0 1 1 1 1 0 0 0 0 1 1 1 1
  - dim_2    (stacked_dim) int64 0 1 2 3 0 1 2 3 0 1 2 3 0 1 2 3 0 1 2 3 0 1 2 3
Data variables:
    obs      (draw, stacked_dim) int32 0 1 2 3 4 5 6 7 8 ... 39 40 41 42 43 44 45 46 47

Moreover, the multiindex can then be unstacked, effectively reshaping the Dataset:
reshaped_ds = multiindex_ds.unstack(""stacked_dim"")
reshaped_ds

with output:
<xarray.Dataset>
Dimensions:  (dim_0: 3, dim_1: 2, dim_2: 4, draw: 2)
Coordinates:
  * draw     (draw) int32 0 1
  * dim_0    (dim_0) object 'a' 'b' 'c'
  * dim_1    (dim_1) int64 0 1
  * dim_2    (dim_2) int64 0 1 2 3
Data variables:
    obs      (draw, dim_0, dim_1, dim_2) int32 0 1 2 3 4 5 ... 42 43 44 45 46 47

I think that this alone does not completely cover your needs because you want to convert a dimension into two dimensions, one of which is to be a multiindex. All the building blocks are here though.
For example, you can follow this steps (including unstacking) with regions and conditions and then follow this steps (no unstacking now) to convert regions to multiindex. Another option would be to use all dimensions from the start, unstack them and then stack them again leaving conditions outside of the final multiindex.

Detailed answer
The answer combines several quite unrelated commands, and it might be tricky to see what each of them is doing.
assign_coords
The first step is to create new dimensions and coordinates and add them to the Dataset. This is necessary because the next methods need the dimensions and coordinates to already be present in the Dataset.
Stopping right after assign_coords yields the following Dataset:
<xarray.Dataset>
Dimensions:  (dim_0: 3, dim_1: 2, dim_2: 4, draw: 2, row: 24)
Coordinates:
  * draw     (draw) int32 0 1
  * row      (row) int32 0 1 2 3 4 5 6 7 8 9 ... 14 15 16 17 18 19 20 21 22 23
  * dim_0    (dim_0) <U1 'a' 'b' 'c'
  * dim_1    (dim_1) int32 0 1
  * dim_2    (dim_2) int32 0 1 2 3
Data variables:
    obs      (draw, row) int32 0 1 2 3 4 5 6 7 8 ... 39 40 41 42 43 44 45 46 47

stack
The Dataset now contains 3 dimensions that add up to 24 elements, however, as the data is currently flat with respect to these 24 elements, we have to stack them into a single 24 element multiindex to make their shapes compatible.
I find the assign_coords followed by stack the most natural solution, however, another possibility would be to generate a multiindex similarly to how it is done above and directly call assign_coords with the multiindex, rendering the stack unnecessary.
This step combines all 3 new dimensions into a single one:
<xarray.Dataset>
Dimensions:  (stacked_dim: 24, draw: 2, row: 24)
Coordinates:
  * draw     (draw) int32 0 1
  * row      (row) int32 0 1 2 3 4 5 6 7 8 9 ... 14 15 16 17 18 19 20 21 22 23
  * stacked_dim      (stacked_dim) MultiIndex
  - dim_0    (stacked_dim) object 'a' 'a' 'a' 'a' 'a' 'a' ... 'c' 'c' 'c' 'c' 'c' 'c'
  - dim_1    (stacked_dim) int64 0 0 0 0 1 1 1 1 0 0 0 0 1 1 1 1 0 0 0 0 1 1 1 1
  - dim_2    (stacked_dim) int64 0 1 2 3 0 1 2 3 0 1 2 3 0 1 2 3 0 1 2 3 0 1 2 3
Data variables:
    obs      (draw, row) int32 0 1 2 3 4 5 6 7 8 ... 39 40 41 42 43 44 45 46 47

Note that as desired now we have 2 dimensions with size 24 as desired.
reset_index
Now we have our final dimension present in the Dataset as a coordinate, and we want this new coordinate to be the one used to index the variable obs. set_index seems like the correct choice, however, each of our coordinates indexes itself (unlike the example in set_index docs where x indexes both x and a coordinates) which means that set_index cannot be used in this particular case. The method to use is reset_index to remove the coordinate row without removing the dimension row.
In the following output it can be seen how now row is a dimension without coordinates:
<xarray.Dataset>
Dimensions:  (stacked_dim: 24, draw: 2, row: 24)
Coordinates:
  * draw     (draw) int32 0 1
  * stacked_dim      (stacked_dim) MultiIndex
  - dim_0    (stacked_dim) object 'a' 'a' 'a' 'a' 'a' 'a' ... 'c' 'c' 'c' 'c' 'c' 'c'
  - dim_1    (stacked_dim) int64 0 0 0 0 1 1 1 1 0 0 0 0 1 1 1 1 0 0 0 0 1 1 1 1
  - dim_2    (stacked_dim) int64 0 1 2 3 0 1 2 3 0 1 2 3 0 1 2 3 0 1 2 3 0 1 2 3
Dimensions without coordinates: row
Data variables:
    obs      (draw, row) int32 0 1 2 3 4 5 6 7 8 ... 39 40 41 42 43 44 45 46 47

rename
The current Dataset is nearly the final one, the only issue is that the obs variable still has the row dimension instead of the desired one: stacked_dim. It does not really look like this is intended usage of rename but it can be used to get stacked_dim to absorb row, yielding the desired final result (called multiindex_ds above).
Here again, set_index seems to be the method to choose, however, if instead of rename(row=""stacked_dim""), set_index(row=""stacked_dim"") is used, the multiindex is collapsed into an index made of tuples:
<xarray.Dataset>
Dimensions:  (draw: 2, row: 24)
Coordinates:
  * draw     (draw) int32 0 1
  * row      (row) object ('a', 0, 0) ('a', 0, 1) ... ('c', 1, 2) ('c', 1, 3)
Data variables:
    obs      (draw, row) int32 0 1 2 3 4 5 6 7 8 ... 39 40 41 42 43 44 45 46 47

",pandas
problems with updating colorbar with matplotlibslider listener in jupyter,"The problem that I am having is during updating the colorbar of an plt.imshow plot. Here's the code I will try to break it down and to explain some stuff in it afterwards.

For the first cell in Jupyter, I have the functions, the imports and the input parameters:

# Imports ##############################################################
import numpy as np
%matplotlib notebook
import matplotlib.pyplot as plt
import matplotlib.colors as colors
from matplotlib.widgets import Slider
from scipy.ndimage import label, find_objects
# Functions ############################################################
def intensity_distribution(r, z, PMax, w0, zR):
    wZ = w0 * np.sqrt(1 + (z / zR)**2)  # beam radius at z (Gaussian beam spreading)
    I0 = 2 * PMax / (np.pi * wZ**2) # peak intensity at radius wZ
    return I0 * np.exp(-2 * r**2 / wZ**2), wZ  # Gaussian intensity distribution, beam radius
def get_circle_ROI(I, r, threshold):
    ROI = (I > threshold).astype(""uint8"")  # binary mask for regions above the threshold
    labels, features = label(ROI) # label connected regions
    slices = find_objects(labels) # get bounding box slices for labeled regions
    xSlice, ySlice = slices[0] # extract x and y slices of the largest feature
    ROIHeight = (xSlice.stop - xSlice.start) * (r[1] - r[0]) * 1e6 # convert height to micrometers
    ROIWidth = (ySlice.stop - ySlice.start) * (r[1] - r[0]) * 1e6 # convert width to micrometers
    cx = (ySlice.start + ySlice.stop) // 2 # x-coordinate of the center
    cy = (xSlice.start + xSlice.stop) // 2 # y-coordinate of the center
    centre = (r[cy] * 1e6, r[cx] * 1e6) # convert center coordinates to micrometers
    radius = min(ROIWidth, ROIHeight) / 2 # radius is the smaller dimension's half-width
    return centre, radius
def update_plot(PMax, zOffset):
    """"""Update the heatmap based on new parameters.""""""
    global colorbar
    # Calculate intensity distribution at given z offset
    I, wZ = intensity_distribution(np.sqrt(R**2 + Z**2), zOffset, PMax, BEAM_RADIUS_AT_FOCUS, zR)
    I /= 1e6  # convert intensity from W/m² to W/mm²
    I += 0.01  # small offset for better visualization contrast
    max_intensity = I.max()  # maximum intensity in the current distribution
    # Calculate the on-axis peak intensity at focus in W/mm²
    I0 = (2 * PMax) / (np.pi * BEAM_RADIUS_AT_FOCUS**2)  # peak intensity in W/m² at z = 0
    I0 /= 1e6  # convert peak intensity to W/mm²
    # Calculate the Full Width at Half Maximum (FWHM) in micrometers
    centre, fwhm = get_circle_ROI(I, r, max_intensity / 2)
    _, tenth = get_circle_ROI(I, r, max_intensity / 10)
    # Clear and update plot
    ax.clear()  # clear current axes
    # Display the updated intensity distribution as a heatmap
    im = ax.imshow(I, extent=[r[0]*1e6, r[-1]*1e6, r[0]*1e6, r[-1]*1e6], norm=colors.LogNorm(vmax=14000))
    ax.set_xlabel(""x (μm)"")  # label for x-axis
    ax.set_ylabel(""y (μm)"")  # label for y-axis
    # Add plot title with z offset, FWHM, and max intensity in W/mm²
    ax.set_title(f""FWHM = {fwhm:.1f} μm\n""
                 f""Radius at 10% of total power = {tenth:.2f} μm\n""
                 f""Max power = {I.max():.2f} W/mm²"",
                 loc=""left"")
    # Draw a circle representing the FWHM boundary
    cirlcefwhm = plt.Circle(centre, fwhm, color='white', fill=False, linestyle='--', linewidth=2, label=""FWHM"")
    cirlce10 = plt.Circle(centre, tenth, color='white', fill=False, linestyle='--', linewidth=2, label=""10% of I$_max$"")
    ax.add_patch(cirlcefwhm)  # add the FWHM circle to the plot
    ax.add_patch(cirlce10)  # add the circle where power is 10% of max
    #### Problematic starts here ####
    if colorbar is not None:   # if colorbar already exists, remove it
        colorbar.remove()
    colorbar = plt.colorbar(im, ax=ax, label=""Intensity (W/mm²)"")  # create new colorbar in W/mm²
    fig.draw_without_rendering() # redraw based on the recommendation of matplotlib instead of colorbar.draw_all()
    fig.canvas.draw()  # redraw figure to reflect updates
    #### Problematic ends here ####
def sliders_on_changed(val):
    ''' Slider update function '''
    power = power_slider.val * MAX_LASER_POWER / 100  # calculate current power level in watts
    z_offset = z_offset_slider.val / 1000 # convert slider z offset from mm to meters
    update_plot(power, z_offset) # update the plot with new parameters
# Inputs ###############################################################
WAVELENGTH = 10.6e-6  # wavelength in meters
MAX_LASER_POWER = 80  # max laser power in watts
BEAM_WIDTH_AT_FOCUS = 120e-6  # beam width at focus in meters
BEAM_RADIUS_AT_FOCUS = BEAM_WIDTH_AT_FOCUS / 2  # beam radius at focus in meters
zR = np.pi * BEAM_RADIUS_AT_FOCUS**2 / WAVELENGTH  # Rayleigh range in meters
gridSize = 100 # resolution
r = np.linspace(-500e-6, 500e-6, gridSize)  # range for spatial coordinates in meters
R, Z = np.meshgrid(r, r) # create grid for spatial coordinates
colorbar = None # init colorbar

The problematic part would be this one:
#### Problematic starts here ####
if colorbar is not None:   # if colorbar already exists, remove it
    colorbar.remove()
colorbar = plt.colorbar(im, ax=ax, label=""Intensity (W/mm²)"")  # create new colorbar in W/mm²
fig.draw_without_rendering() # redraw based on the recommendation of matplotlib instead of colorbar.draw_all()
fig.canvas.draw()  # redraw figure to reflect updates
#### Problematic ends here ####

Am not so sure why it is not working. What happens is that the colorbar appears for the first update and then dissapears after a change in the Z-offset or the power percentage. Basically after a change in any of the values.

The second cell of the jupyter notebook has the call for the functions:

fig, ax = plt.subplots()
plt.subplots_adjust(left=0.25, bottom=0.35)  # leave space for sliders
update_plot(50, 0)
ax_power = plt.axes([0.25, 0.2, 0.65, 0.03], facecolor = ""lightgray"")
ax_z_offset = plt.axes([0.25, 0.15, 0.65, 0.03], facecolor = ""lightgray"")
power_slider = Slider(ax_power, 'Power (%)', 0.1, 100, valinit=50)
z_offset_slider = Slider(ax_z_offset, 'Z-Offset (mm)', 0, 5.0, valinit=0)
power_slider.on_changed(sliders_on_changed)
z_offset_slider.on_changed(sliders_on_changed)

Running this will get you to a UI that looks like this:

And then when I move any of the sliders the colorbar disappears:

The updating of the colorbar seems to be a common problem, I saw a lot of stackoverflow questions about it, I even answered one once.
The solution however is not really working with the slider listeners.
P.S.: Changing the backend does not really help. However, using %matplotlib qt revealed a traceback:
Traceback (most recent call last):
  File ""C:\ProgramData\anaconda3\Lib\site-packages\matplotlib\cbook\__init__.py"", line 309, in process
    func(*args, **kwargs)
  File ""C:\ProgramData\anaconda3\Lib\site-packages\matplotlib\widgets.py"", line 603, in <lambda>
    return self._observers.connect('changed', lambda val: func(val))
                                                          ^^^^^^^^^
  File ""C:\Users\User\AppData\Local\Temp\ipykernel_26252\2170461726.py"", line 66, in sliders_on_changed
    update_plot(power, z_offset) # update the plot with new parameters
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""C:\Users\User\AppData\Local\Temp\ipykernel_26252\2170461726.py"", line 57, in update_plot
    colorbar.remove()
  File ""C:\ProgramData\anaconda3\Lib\site-packages\matplotlib\colorbar.py"", line 1041, in remove
    self.ax.remove()
  File ""C:\ProgramData\anaconda3\Lib\site-packages\matplotlib\artist.py"", line 242, in remove
    self._remove_method(self)
  File ""C:\ProgramData\anaconda3\Lib\site-packages\matplotlib\figure.py"", line 944, in delaxes
    self._axstack.remove(ax)
  File ""C:\ProgramData\anaconda3\Lib\site-packages\matplotlib\figure.py"", line 92, in remove
    self._axes.pop(a)
KeyError: <Axes: label='<colorbar>', ylabel='Intensity (W/mm²)'>

Some stuff that I tried:

There is a method for colorbar called draw_all()
Matplotlib also suggests using fig.draw_without_rendering()
My own solution
Setting the limits of colorbar without deleting the colorbar

Can someone please help me in figuring out what's wrong? I understand ofc that the code is quite long, I can shorten it to a simple example if necessary.

I am using

Jupyter 6.5.4
Matplotlib 3.7.2

","I managed to solve this problem on my old version by using the following lines to update the colormap:
if colorbar is not None: # if colorbar already exists, update it
    colorbar.mappable.set_clim(vmax=max_intensity)
else:    
    colorbar = plt.colorbar(im, ax=ax, label=""Intensity (W/mm²)"") # create new colorbar in W/mm²

Updating Anaconda was not needed in this case. The code above works both for the old version and the old version (under the %matplotlib qt backend)
Proof:


The advice about using ipympl lead to a RuntimeError under a clean new anaconda installation. So, really no need for that.

Using interact() does not solve anything.
",matplotlib
deciding on how to scale data and which scaler to use,"I am trying to train an MLP model with two Dense layers in keras to do prediction for a small data set of around 100 uni-variate time series. This model should get values of 6 days and predict the 7th day value. As an input to the model, I first concatenate these time series after each other in a dataframe like follow: 
 ts1    val1 
 ts1    val2 
   ...     
 ts1    varN 
 ts2    val1 
 ts2    val2 
   ...     
 ts3    varN 
 ts3    val1 
 ts3    val2 
   ...     
 ts3    varN 
   ...
 ts100  val1 
 ts100  val2 
   ...     
 ts100  varN 

I wonder what is the best way to scale this data? First of all, Should I scale each time series (ts_n) independently, so there will be 100 scalers at the end? Or should I better scale them all together (one acaler at the end) so that I won't lose the correlation between them? Or since all of these time series are considered as the same feature, then there is no point in having correlation?! 
My second question is about which scaling method I should choose? min-max or StandardScaler (from sklearn)? Some time series behave quite different from the others, and they have big variations in their values. If I use min-max scaler, it will ignore these differences, right? So isn't it better to use StandardScaler that (hopefully) considers the differences in score between each time series?
P.S. I should mention that 'after' the scaling is done, I will create timesteps and will have the final results like this:
        timestep1 | timestep2 | timestep3 | timestep4 | timestep5 | timestep6 | timestep7
 ts1      var1    |   var2    |   var3    |   var4    |   var5    |   var6    |   var7    
 ts1      var2    |   var3    |   var4    |   var5    |   var6    |   var7    |   var8    
 ts1      var3    |   var4    |   var5    |   var6    |   var7    |   var8    |   var9
 ...
 ts2      var1    |   var2    |   var3    |   var4    |   var5    |   var6    |   var7    
 ts2      var2    |   var3    |   var4    |   var5    |   var6    |   var7    |   var8    
 ts2      var3    |   var4    |   var5    |   var6    |   var7    |   var8    |   var9
 ...
 ts100      var1    |   var2    |   var3    |   var4    |   var5    |   var6    |   var7    
 ts100      var2    |   var3    |   var4    |   var5    |   var6    |   var7    |   var8    
 ts100      var3    |   var4    |   var5    |   var6    |   var7    |   var8    |   var9
 ...

","In general, I've found very little difference in performance between MinMaxScaler and StandardScaler.  Of course, since (it appears) you'll be scaling your target variable, as well, you should make sure you use a scaler that is consistent with your output activation function.  For example, if you're output activation function is a ReLU, you wont be able to predict any negative values.  In that case, I would lean towards the MinMaxScaler since all of your targets will end up in the interval [0,1]. 
In terms of whether to scale the time series together or independently, it may depend on the specific setting.  If the scales tend to have different time-dependent behaviors, it's likely good to scale them together so the difference is preserved.  If they all behave with a similar pattern, scaling them independently will likely work best.  
It's also worth considering other network architectures for time-series forecasting, e.g. RNNs.
",data-science
what is the meaning of numpy reduceat in python,"I am very new in Python and I read at this moment just the tutorial. 
I am confused about the reduceat() function. 
I saw the example:
np.add.reduceat([0,1,2,3,4,5,6,7],[0,4,1,5,2,6,3,7])[::2]

and result is:
array([ 6, 10, 14, 18])

How does it come out? Can some one explain for me?
","It is sort of like a rolling apply, see:
In [59]:
np.add.reduceat([0,1,2,3,4,5,6,7],[0,4])
Out[59]:
array([ 6, 22])

In [65]:    
np.add.reduceat([0,1,2,3,4,5,6,7],[4,1])
Out[65]:
array([ 4, 28])

In [66]:
np.add.reduceat([0,1,2,3,4,5,6,7],[1,5])
Out[66]:
array([10, 18])

In [64]:
np.add.reduceat([0,1,2,3,4,5,6,7],[5,2])
Out[64]:
array([ 5, 27])

In [61]:
np.add.reduceat([0,1,2,3,4,5,6,7],[2,6])
Out[61]:
array([14, 13])

In [67]:
np.add.reduceat([0,1,2,3,4,5,6,7],[6,3])
Out[67]:
array([ 6, 25])

In [62]:
np.add.reduceat([0,1,2,3,4,5,6,7],[3,7])
Out[62]:
array([18,  7])

If you want just the 1st value, you can get it done in just one shot:
In [63]:    
np.add.reduceat([0,1,2,3,4,5,6,7],[0,4,1,5,2,6,3,7])
Out[63]:
array([ 6,  4, 10,  5, 14,  6, 18,  7])

",numpy
why does the gstreamer quotplay a section of a media filequot example block,"I'm in the process of learning how to build an application in C using gstreamer, and I'm working my way through examples.  In particular, I'm having an issue with the play a section of a media file example on the ""Pipeline manipulation"" section.
Under Ubuntu 24.04, it builds just fine, and runs - but nothing is ever played.  I added some code to the cb_message() callback to see what other messages it's getting - and there are some GST_MESSAGE_BUFFERING messages after setting the pipeline to GST_STATE_PLAYING. I added some debug output there, and it does eventually get to 100% - but still nothing plays, even if I add a call to gst_element_set_state(pipeline, GST_STATE_PLAYING) there.  In case it matters, the URI I've been using is the same one used in other tutorials on the gstreamer site: https://gstreamer.freedesktop.org/data/media/sintel_trailer-480p.webm
So... seems like maybe this example needs updating.  Any suggestions?  It's kind of frustrating when trying to learn something if the examples don't work.  :) :) :)
EDIT: looks like it's only meant to be used with a local file, not a stream?  The same file works fine locally.  I'd love to know why this is the case, and if it's possible to play a section of a file retrieved over the network...
","As Florian Zwoch commented - this example only works with a local file, not a stream.  The same file works fine if retrieved then played via a file:/// URI.
",c
prince quotfamd object has no attribute 39plot_row_coordinates39quot,"As far as I can tell, this is the same implementation as every example of this function that I could find.
Calling famd.plot_row_coordinates() produces: 'FAMD' object has no attribute 'plot_row_coordinates'.
The prior lines run fine.
#FAMD
famd = FAMD(n_components = 10, n_iter = 3, random_state = 101)
famd.fit(df.drop('Target', axis=1)) #leave out response variable 'Target'
famd.eigenvalues_summary

#plot components
famd.transform(df)
famd.plot_row_coordinates(
    df.head(200),
    figsize = (15,10),
    color_labels=['Completion status: {}'.format(t) for t in df['Target']]
)

","The 'plot_row_coordinates' function got removed from the package on 22nd of February. You should now either use 'famd.plot' function or plot coordinates yourself, with code looking something like this:
coordinates = famd.row_coordinates(X)
x = coordinates[0].astype(np.float)
y = coordinates[1].astype(np.float)
plt.plot(x,y)

",data-science
how to debug the llama2 inference command with vscode,"I am trying to run the LLAMA2 inference script (shown below) with vscode debugging mode:
    torchrun --nproc_per_node 1 example_text_completion.py \
    --ckpt_dir models/7B-Chat \
    --tokenizer_path tokenizer.model \
    --max_seq_len 128 --max_batch_size 4

Before this, I can successfully run it with my command line interface, which shows my python environment is correct.
I have tried these two debug configs below:


 {
     ""name"": ""Python: run_llama2_inference"",
     ""type"": ""python"",
     ""request"": ""launch"",
     ""module"": ""torchrun"",
     ""args"": [
         ""--nproc_per_node=1"",
         ""example_chat_completion.py"",
         ""--ckpt_dir=models/7B-Chat/"",
         ""--tokenizer_path=tokenizer.model"",
         ""--max_seq_len=512"",
         ""--max_batch_size=4"",
     ],
     ""console"": ""integratedTerminal"",
     ""justMyCode"": true,
     ""env"": {
         ""PYTHONPATH"": ""${workspaceFolder}""
     }
 },

Corresponding Error Messsage: ""No module named torchrun""


 {
     ""name"": ""Python: run_llama2_inference"",
     ""type"": ""python"",
     ""request"": ""launch"",
     ""module"": ""torch.distributed.launch"",
     ""args"": [
         ""--use-env"",
         ""example_chat_completion.py"",
         ""--nproc_per_node=1"",
         ""--ckpt_dir=models/7B-Chat/"",
         ""--tokenizer_path=tokenizer.model"",
         ""--max_seq_len=512"",
         ""--max_batch_size=4"",
     ],
     ""console"": ""integratedTerminal"",
     ""justMyCode"": true,
     ""env"":
         ""PYTHONPATH"": ""${workspaceFolder}""
     }
 },

Corresponding Error Messsage: ""Could not consume arg: --nproc_per_node=1""


Both configs do not work as expected. I would like to seek the advice from online experts. Appreciate your ideas or advice in advance!
","You can use the ""program"" field to specify the Python script you want to run (example_text_completion.py), and pass the rest of the arguments using the ""args"" field.
Here's an example of how you can modify your launch configuration:
{
    ""name"": ""Python: run_llama2_inference"",
    ""type"": ""python"",
    ""request"": ""launch"",
    ""program"": ""${workspaceFolder}/example_chat_completion.py"",
    ""args"": [
        ""--ckpt_dir=models/7B-Chat/"",
        ""--tokenizer_path=tokenizer.model"",
        ""--max_seq_len=512"",
        ""--max_batch_size=4"",
    ],
    ""console"": ""integratedTerminal"",
    ""justMyCode"": true,
    ""env"": {
        ""PYTHONPATH"": ""${workspaceFolder}""
    }
}

",llama
python script iterates over whole folder but skips files in the folder,"I tried running the following code. The code should read hdf5 files from a directory and create for every hdf5 file a png and a txt file with the same name (btw. I need it as input for the CNN YOLO).
The code does what I described but only for 20 images! I added print(i) to see if the for-loop is working proper... and it is. It prints every file in the directory (over 200 files). But it just creates 20 .png and 20 .txt files.
def process_fpath(path1, path2):
    sensor_dim = (101, 101)
    onlyfiles = [f for f in os.listdir(path1) if isfile(join(path1, f))]

    for i in onlyfiles:
        if i.endswith("".hdf""):
            print(i)
            #cut "".hdf""
            name = str(i[0:-5]) 
            # create png
            im = h5py.File(path1 + str(i), 'r')
            labels_im = im['labels']
            image = im['image']
            plt.imsave(path2 + name + '.png', image)
            
            # create txt
            exp = np.column_stack((np.zeros(np.size(labels_im,0)) , labels_im[:,0]/sensor_dim[0], labels_im[:,1]/sensor_dim[1], labels_im[:,3]/sensor_dim[0], labels_im[:,3]/sensor_dim[0]))
            np.savetxt(path2 + name + '.txt', exp, delimiter = '  ', fmt=['%d', '%8f', '%8f', '%8f', '%8f'])


This is my first post so if something isn't proper please let me know.
","
Maybe it's because of the name variable? You remove 5 characters but you want to remove only 4: name = str(i[0:-4])
Not related to your question, the last 3 lines are useless. you can remove them.

            continue
        else:
            continue


Try to run on a given file that is not working to understand what the problem is instead of looping on each of them.

",data-science
how to download the testdata dataset of open3d python,"I have installed open3d in Raspberry Pi 4 using the documentation:
Command: pip3 install open3d
I can successfully import open3d in a Python program. But if I run the program from the tutorial:
import numpy as np
import open3d as o3d

if __name__ == ""__main__"":
    print(""Load a ply point cloud, print it, and render it"")
    pcd = o3d.io.read_point_cloud(""../../TestData/fragment.ply"")
    print(pcd)
    print(np.asarray(pcd.points))
    o3d.visualization.draw_geometries([pcd])

It gives error:
Load a ply point cloud, print it, and render it
RPly: Unable to open file
[Open3D WARNING] Read PLY failed: unable to open file: ../../TestData/fragment.ply
PointCloud with 0 points.
[]
[Open3D WARNING] The number of points is 0 when creating axis-aligned bounding box.

It looks like the TestData files are not downloaded.
So how can I download the TestData files?
","The latest version of Open3D downloads all data needed in o3d.data module. Hence, just use latest version docs available here.
P.S. Note that URL should have latest keyword in it for the most recent version docs:
https://www.open3d.org/docs/latest/tutorial/geometry/pointcloud.html
Also, notice the theme change between the old version and the new version. The new version uses furo theme which provides light/dark mode along with a right pane for easier view of class methods. For example: See https://www.open3d.org/docs/latest/python_api/open3d.geometry.TriangleMesh.html#open3d.geometry.TriangleMesh.
",python
what does void mean in c c and c,"Looking to get the fundamentals on where the term ""void"" comes from, and why it is called void. The intention of the question is to assist someone who has no C experience, and is suddenly looking at a C-based codebase. 
","Basically it means ""nothing"" or ""no type""
There are 3 basic ways that void is used:

Function argument: int myFunc(void)
-- the function takes nothing. 
Function return value: void myFunc(int)
-- the function returns nothing
Generic data pointer: void* data
-- 'data' is a pointer to data of unknown type, and cannot be dereferenced

Note: the void in a function argument is optional in C++, so int myFunc() is exactly the same as int myFunc(void), and it is left out completely in C#. It is always required for a return value.
",c
autocomplete combobox in javafx,"I'm looking for a way to add autocomplete to a JavaFX ComboBox.
This AutoFillBox is known but not what I'm searching.
What I want is a editable ComboBox, and while typing the list should filtered. But I want also to open the list without typing and seeing the whole items.
Any idea?
","I found a solution that's working for me:
public class AutoCompleteComboBoxListener<T> implements EventHandler<KeyEvent> {

    private ComboBox comboBox;
    private StringBuilder sb;
    private ObservableList<T> data;
    private boolean moveCaretToPos = false;
    private int caretPos;

    public AutoCompleteComboBoxListener(final ComboBox comboBox) {
        this.comboBox = comboBox;
        sb = new StringBuilder();
        data = comboBox.getItems();

        this.comboBox.setEditable(true);
        this.comboBox.setOnKeyPressed(new EventHandler<KeyEvent>() {

            @Override
            public void handle(KeyEvent t) {
                comboBox.hide();
            }
        });
        this.comboBox.setOnKeyReleased(AutoCompleteComboBoxListener.this);
    }

    @Override
    public void handle(KeyEvent event) {

        if(event.getCode() == KeyCode.UP) {
            caretPos = -1;
            moveCaret(comboBox.getEditor().getText().length());
            return;
        } else if(event.getCode() == KeyCode.DOWN) {
            if(!comboBox.isShowing()) {
                comboBox.show();
            }
            caretPos = -1;
            moveCaret(comboBox.getEditor().getText().length());
            return;
        } else if(event.getCode() == KeyCode.BACK_SPACE) {
            moveCaretToPos = true;
            caretPos = comboBox.getEditor().getCaretPosition();
        } else if(event.getCode() == KeyCode.DELETE) {
            moveCaretToPos = true;
            caretPos = comboBox.getEditor().getCaretPosition();
        }

        if (event.getCode() == KeyCode.RIGHT || event.getCode() == KeyCode.LEFT
                || event.isControlDown() || event.getCode() == KeyCode.HOME
                || event.getCode() == KeyCode.END || event.getCode() == KeyCode.TAB) {
            return;
        }

        ObservableList list = FXCollections.observableArrayList();
        for (int i=0; i<data.size(); i++) {
            if(data.get(i).toString().toLowerCase().startsWith(
                AutoCompleteComboBoxListener.this.comboBox
                .getEditor().getText().toLowerCase())) {
                list.add(data.get(i));
            }
        }
        String t = comboBox.getEditor().getText();

        comboBox.setItems(list);
        comboBox.getEditor().setText(t);
        if(!moveCaretToPos) {
            caretPos = -1;
        }
        moveCaret(t.length());
        if(!list.isEmpty()) {
            comboBox.show();
        }
    }

    private void moveCaret(int textLength) {
        if(caretPos == -1) {
            comboBox.getEditor().positionCaret(textLength);
        } else {
            comboBox.getEditor().positionCaret(caretPos);
        }
        moveCaretToPos = false;
    }

}

You can call it with
new AutoCompleteComboBoxListener<>(comboBox);


It's based on this and I customized it to fit my needs.
Feel free to use it and if anybody can improve it, tell me.
",java
why does summing data grouped by dfiloc 0 also sum up the column names,"I have a DataFrame with a species column and four arbitrary data columns. I want to group it by species and sum up the four data columns for each one. I've tried to do this in two ways: once by grouping by df.columns[0] and once by grouping by df.iloc[:, 0].
data = {
    'species': ['a', 'b', 'c', 'd', 'e', 'rt', 'gh', 'ed', 'e', 'd', 'd', 'q', 'ws', 'f', 'fg', 'a', 'a', 'a', 'a', 'a'],
    's1': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20],
    's2': [9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9],
    's3': [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21],
    's4': [10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10]
}
df = pd.DataFrame(data)

grouped_df1 = df.groupby(df.columns[0], as_index=False).sum()
grouped_df2 = df.groupby(df.iloc[:, 0], as_index=False).sum()

Both methods correctly sum the data in the four rightmost columns. But for some reason, the second method also sums up the names of the species, concatenating them into one long, repeating string.
Here's the result from the first method, which is what I'm looking for:
print(grouped_df1)
   species  s1  s2  s3  s4
0        a  91  54  97  60
1        b   2   9   3  10
2        c   3   9   4  10
3        d  25  27  28  30
4        e  14  18  16  20
5       ed   8   9   9  10
6        f  14   9  15  10
7       fg  15   9  16  10
8       gh   7   9   8  10
9        q  12   9  13  10
10      rt   6   9   7  10
11      ws  13   9  14  10

And here's the result from the df.iloc method, which incorrectly sums up the species data:
print(grouped_df2)
   species  s1  s2  s3  s4
0   aaaaaa  91  54  97  60
1        b   2   9   3  10
2        c   3   9   4  10
3      ddd  25  27  28  30
4       ee  14  18  16  20
5       ed   8   9   9  10
6        f  14   9  15  10
7       fg  15   9  16  10
8       gh   7   9   8  10
9        q  12   9  13  10
10      rt   6   9   7  10
11      ws  13   9  14  10

Why is the second method summing up the species names as well as the numerical data?
","In groupby - column name is treated as an intrinsic grouping key, while a Series is treated as an external key.
Reference - https://pandas.pydata.org/docs/reference/groupby.html
When using df.iloc[:, 0]:
Pandas considers the string values in the species column as a separate grouping key independent of the DataFrame structure.
When using df.columns[0]:
Pandas directly uses the column 'species' within the DataFrame as the grouping key. This allows Pandas to manage the grouping and summation correctly.
Code COrrection
You should always reference the column name explicitly
grouped_df1 = df.groupby('species', as_index=False).sum()

Or this also works
grouped_df1 = df.groupby(df[df.columns[0]], as_index=False).sum()

",pandas
kmean get good group separation,"I have a basic code for use Kmeans with sklearn
    scaled = MinMaxScaler().fit_transform(points)

    kmeans = KMeans(n_clusters=nb_clusters) # , random_state=42 , init='random', algorithm='elkan'
    kmeans.fit(scaled)
    labels = kmeans.labels_
    centroids = kmeans.cluster_centers_

But i don't success to have my two groups well separated

Does someone know what's wrong or what is lacking please ?
Thank you in advance.
","The k-means algorithm is based on the assumption of spherically shaped clusters with a common diameter, e. g. isotropic Gaussian-distributed clusters. Your data does not fit this assumption.
First you should plot your data with a 1:1 aspect ratio to better understand what is going on. You will see that the separation of your two groups circled in blue is smaller as compared to their spread in y-direction.
Second, you should observe that the group to the right includes much, much less data points as compared to the group on the left. To the algorithm, the points on the right are mere outliers of the distribution it tries to model.
To sum it up, k-means is probably not a good fit for your data. Mean shift, although maybe overkill, should be a much better fit.
",scikit-learn
empty result apache opennlp onnx model,"I trying to convert huggingface model to onnx for classifying text in Java app, but i can't undestand why i don't see result(result array is just empty). readme.md has link on the model, and it's working pretty well, but i have to use some another one because it is not support language which  i need.
Working sample python code:
from transformers import AutoTokenizer, AutoModelForSequenceClassification
import torch

model_name = ""tabularisai/multilingual-sentiment-analysis""
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForSequenceClassification.from_pretrained(model_name)

def predict_sentiment(texts):
    inputs = tokenizer(texts, return_tensors=""pt"", truncation=True, padding=True, max_length=512)
    with torch.no_grad():
        outputs = model(**inputs)
    probabilities = torch.nn.functional.softmax(outputs.logits, dim=-1)
    sentiment_map = {0: ""Very Negative"", 1: ""Negative"", 2: ""Neutral"", 3: ""Positive"", 4: ""Very Positive""}
    return [sentiment_map[p] for p in torch.argmax(probabilities, dim=-1).tolist()]

print(predict_sentiment([""I absolutely love the new design of this app!"", ""The customer service was disappointing.""]))

I tried to convert a few models with two ways:
python -m optimum.exporters.onnx --model tabularisai/multilingual-sentiment-analysis --task sequence-classification onnx_model
and
from optimum.onnxruntime import ORTModelForFeatureExtraction

model = ORTModelForFeatureExtraction.from_pretrained(""tabularisai/multilingual-sentiment-analysis"", from_transformers=True)
model.save_pretrained(""onnx_model"")

Same result - onnx_model folder with model, vocab etc
nlptown_bert-base-multilingual-uncased-sentiment - model from readme and it works as expect even i use converted model vocab file
Java code sample:
    public void def() {
        try (final DocumentCategorizerDL documentCategorizerDL =
                     new DocumentCategorizerDL(
                             new File(""onnx_model/model.onnx""),
//                             new File(""nlptown_bert-base-multilingual-uncased-sentiment.onnx""),
                             new File(""onnx_model/vocab.txt""),
                             getCategories(),
                             new AverageClassificationScoringStrategy(),
                             new InferenceOptions())) {

            final double[] result = documentCategorizerDL.categorize(new String[] {""I absolutely love the new design of this app!"", ""The customer service was disappointing.""});
            System.out.println(""done"");
        } catch (Exception e) {
            e.printStackTrace();
        }
    }

    private Map<Integer, String> getCategories() {
        final Map<Integer, String> categories = new HashMap<>();
        categories.put(0, ""Very Negative"");
        categories.put(1, ""Negative"");
        categories.put(2, ""Neutral"");
        categories.put(3, ""Positive"");
        categories.put(4, ""Very Positive"");
        return categories;
    }

I don't know how is important but opset version of working model is 11, but my is 14.
My opennlp-dl library version is 2.5.1
","setIncludeTokenTypeIds(true) was passed to options and this helped to solve this problem
    public void def() {
    var options = new InferenceOptions();
    options.setIncludeTokenTypeIds(false);
    try (
         final DocumentCategorizerDL documentCategorizerDL =
                 new DocumentCategorizerDL(
                         new File(""onnx_model/model.onnx""),
                         new File(""onnx_model/vocab.txt""),
                         getCategories(),
                         new AverageClassificationScoringStrategy(),
                         options)) {

        final double[] result = documentCategorizerDL.categorize(new String[] {""I absolutely love the new design of this app!"", ""The customer service was disappointing.""});
        System.out.println(""s"");
    } catch (Exception e) {
        log.error(e.getMessage(), e);
    }
}

private Map<Integer, String> getCategories() {
    final Map<Integer, String> categories = new HashMap<>();
    categories.put(0, ""Very Negative"");
    categories.put(1, ""Negative"");
    categories.put(2, ""Neutral"");
    categories.put(3, ""Positive"");
    categories.put(4, ""Very Positive"");
    return categories;
}

",huggingface-transformers
how to get the first day of the current week and month,"I have the date of several events expressed in milliseconds[1], and I want to know which events are inside the current week and the current month, but I can't figure out how to obtain the first day (day/month/year) of the running week and convert it to milliseconds, the same for the first day of the month.
[1]Since January 1, 1970, 00:00:00 GMT

","This week in milliseconds:    
// get today and clear time of day
Calendar cal = Calendar.getInstance();
cal.set(Calendar.HOUR_OF_DAY, 0); // ! clear would not reset the hour of day !
cal.clear(Calendar.MINUTE);
cal.clear(Calendar.SECOND);
cal.clear(Calendar.MILLISECOND);

// get start of this week in milliseconds
cal.set(Calendar.DAY_OF_WEEK, cal.getFirstDayOfWeek());
System.out.println(""Start of this week:       "" + cal.getTime());
System.out.println(""... in milliseconds:      "" + cal.getTimeInMillis());

// start of the next week
cal.add(Calendar.WEEK_OF_YEAR, 1);
System.out.println(""Start of the next week:   "" + cal.getTime());
System.out.println(""... in milliseconds:      "" + cal.getTimeInMillis());

This month in milliseconds:
// get today and clear time of day
Calendar cal = Calendar.getInstance();
cal.set(Calendar.HOUR_OF_DAY, 0); // ! clear would not reset the hour of day !
cal.clear(Calendar.MINUTE);
cal.clear(Calendar.SECOND);
cal.clear(Calendar.MILLISECOND);

// get start of the month
cal.set(Calendar.DAY_OF_MONTH, 1);
System.out.println(""Start of the month:       "" + cal.getTime());
System.out.println(""... in milliseconds:      "" + cal.getTimeInMillis());

// get start of the next month
cal.add(Calendar.MONTH, 1);
System.out.println(""Start of the next month:  "" + cal.getTime());
System.out.println(""... in milliseconds:      "" + cal.getTimeInMillis());

",java
efficiently custom array creation routines in jax,"I'm still getting a handle of best practices in jax. My broad question is the following:
What are best practices for the implementation of custom array creation routines in jax?
For instance, I want to implement a function that creates a matrix with zeros everywhere except with ones in a given column. I went for this (Jupyter notebook):
import numpy as np
import jax.numpy as jnp

def ones_at_col(shape_mat, idx):
    idxs = jnp.arange(shape_mat[1])[None,:]
    mat = jnp.where(idx==idxs, 1, 0)
    mat = jnp.repeat(mat, shape_mat[0], axis=0)
    return mat

shape_mat = (5,10)

print(ones_at_col(shape_mat, 5))

%timeit np.zeros(shape_mat)

%timeit jnp.zeros(shape_mat)

%timeit ones_at_col(shape_mat, 5)

The output is
[[0 0 0 0 0 1 0 0 0 0]
 [0 0 0 0 0 1 0 0 0 0]
 [0 0 0 0 0 1 0 0 0 0]
 [0 0 0 0 0 1 0 0 0 0]
 [0 0 0 0 0 1 0 0 0 0]]
127 ns ± 0.717 ns per loop (mean ± std. dev. of 7 runs, 10,000,000 loops each)
31.3 µs ± 331 ns per loop (mean ± std. dev. of 7 runs, 10,000 loops each)
123 µs ± 1.79 µs per loop (mean ± std. dev. of 7 runs, 10,000 loops each)

My function is a factor of 4 slower than the jnp.zeros() routine, which is not too bad. This tells me that what I'm doing is not crazy.
But then both jax routines are much slower than the equivalent numpy routines. These functions cannot be jitted because they take the shape as an argument, and so cannot be traced. I presume this is why they are inherently slower? I guess that if either of them appeared within the scope of another jitted function, they could be traced and sped up?
Is there something better I can do or am I pushing the limits of what is possible in jax?
","The best way to do this is probably something like this:
mat = jnp.zeros(shape_mat).at[:, 5].set(1)

Regarding timing comparisons with NumPy, relevant reading is JAX FAQ: is JAX faster than NumPy? The summary is that for this particular case (creating a simple array) you would not expect JAX to match NumPy performance-wise, due to JAX's per-operation dispatch overhead.
If you wish for faster performance in JAX, you should always use jax.jit to just-in-time compile your function. For example, this version of the function should be pretty optimal (though again, not nearly as fast as NumPy for the reasons discussed at the FAQ link):
@partial(jax.jit, static_argnames=['shape_mat', 'idx'])
def ones_at_col(shape_mat, idx):
  return jnp.zeros(shape_mat).at[:, idx].set(1)

You could leave idx non-static if you'll be calling this function multiple times with different index values, and if you're creating these arrays within another function, you should just put the code inline and JIT-compile that outer function.
Another side-note: your microbenchmarks may not be measuring what you think they're measuring: for tips on this see JAX FAQ: benchmarking JAX code. In particular, be careful of compilation time and asynchronous dispatch effects.
",numpy
fastest way to get hamming distance for integer array,"Let a and b be vectors of the same size with 8-bit integers (0-255). I want to compute the number of bits where those vectors differs i.e. a Hamming distance between vectors formed by concatenation of binary representations of those numbers. For example:
a = [127,255]
b= [127,240]

Using numpy library
np.bitwise_xor(a,b)
# Output: array([ 0, 15])

What I need is now to binary represent each element of the above array and count number of 1s in all the elements of the array. The above example will give hamming distance of 0+4 = 4. Any fast and elegant solution for this in Python?
","Approach #1 : We could broadcast them into binary bits & count number of different bits, like so -
def hamming_distance(a, b):
    r = (1 << np.arange(8))[:,None]
    return np.count_nonzero( (a & r) != (b & r) )

Sample run -
In [144]: a = [127,255]
     ...: b = [127,240]
     ...: 

In [145]: hamming_distance(a, b)
Out[145]: 4

Approach #2 : Using bitwise-xor operation, we can find out the number of different binary bits between a and b -
def hamming_distance_v2(a, b):
    r = (1 << np.arange(8))[:,None]
    return np.count_nonzero((np.bitwise_xor(a,b) & r) != 0)

",numpy
preferred block size for efficient file io,"Is the value of st_blksize field of struct stat type filled inside function stat()'s output parameter, hard coded in some system config file? If it is, can somebody tell me the system config file path?
","Converting my comments into an answer.
The preferred block size may vary by file system type, so there's no easy way to determine one size at compile time for all files on a system — let alone if the executable will be copied across systems.

Can you get that information non-programmatically?

No — certainly not portably between Linux and non-Linux systems. There's the POSIX statvfs() function, but it provides minimal reliable information. The various stat() functions may provide more information, but that's all programmatic because the answer depends on which file system a given file resides on. You can't rely on a value derived at compile time.
",c
how to reference a json list using an api,"I would to refer to the pressure field in the List using the getPressure function
{
    ""cod"": ""200"",
    ""message"": 0,
    ""cnt"": 40,
    ""list"": [
        {
            ""dt"": 1612558800,
            ""main"": {
                ""temp"": -3.29,
                ""feels_like"": -6.55,
                ""temp_min"": -3.29,
                ""temp_max"": -3.16,
                ""pressure"": 1021,




 public String getPressure() {
            JsonArray weatherArray = obj.getJsonArray(""main"");
            JsonObject weatherObject = weatherArray.getJsonObject(0);
            return weatherObject.getString(""pressure"");
        }

How to write correctly getPressure functions to get the pressure value from the list?
","Try this:
    public String getPressure() {
       JsonArray weatherArray = obj.getJsonArray(""list""); // list: []
       JsonObject weatherObject = weatherArray.getJsonObject(0); // first element of list
       JsonObject mainObject = weatherObject.getJsonObject(""main""); // get main object
       return mainObject.getString(""pressure""); // get Pressure of Main Object
    }

",java
assign value to paragraph or p tag through jquery for the data obtained from php,"I need to set the text of Paragraph or P tag to the value obtained though AJAX.
So I have the HTML page somewhat like this where I have declared the paragraph tab.
<p class=""card-text"">Client Type<p id=""Client_Type"" name = ""Client_Type""></p></p>

Onclick of the button I am making the AJAX call to HOME_CARD.PHP page.
The PHP is working properly and its returning me the data to jQuery. When I use console.log(data); it displays me all the data correctly.
$.ajax({
  url: ""Home_Card.php"",
  method: ""POST"",
  data: {
    search_client_id: search_client_id
  },
  success: function(data) {
    console.log(data);
    $('#Client_Type').val(data.CLIENT_MNEMONIC);
    //$('#Client_Type').text(""HELLO"");
    //$('#Client_Type').attr(data.CLIENT_MNEMONIC);
    //$('#card').show();
    //$('#Client_Type').(""HELLOE"");
  }
});

So I tried using val function to assign the value in CLIENT_TYPE to p tag in HTML page but its not assigning. When I use $('#Client_Type').text(""HELLO""); it assigns the value ""HELLO"" properly so I am guessing nothing wrong with my program.
I wanted to know is there any other way of assigning the value to paragraph tag in jQuery?
How to assign the specific value obtained from PHP in JSON format to paragraph p tag using jQuery.
","During the AJAX call, I did not mention the type of data I getting in return:
datatype: ""json"",
 $.ajax({
  url: ""Home_Card.php"",
  method: ""POST"",
datatype: ""json"",

  data: {
    search_client_id: search_client_id
  },
  success: function(data) {
    console.log(data);
    $('#Client_Type').val(data.CLIENT_MNEMONIC);
    //$('#Client_Type').text(""HELLO"");
    //$('#Client_Type').attr(data.CLIENT_MNEMONIC);
    //$('#card').show();
    //$('#Client_Type').(""HELLOE"");
  }
});

",javascript
right way to use simple imputer,"I have a test- and a train-dataset with nan-values. I would like to use simple imputer in order to deal with them using the following code.
#numerical values
imp= SImp(strategy= 'median') #or 'mean', 'median', 'most_frequent' 
my_df.loc[:, num_cols_nan]= imp.fit_transform(def_df.loc[:, num_cols_nan])

#categorical values
imp= SImp(strategy= 'most_frequent')
my_df.loc[:, cat_cols_nan]= imp.fit_transform(def_df.loc[:, cat_cols_nan])`

My idea was to use median and most_frequent to fill in the NaNs.
However, browsing the web I find to different information on how to use the imputer:

use fit_transform for the train data-set and transform for the test-set in order to perform the replacement based on the distribution on the train set.
use fit_transform for train and test-set in order to perform the replacement based on the distribution of each set separately.

Just to clarify version 1. seems to be the most frequent case but since I'm a total beginner I'm kind of confused here.
So my question is which one is the ""right"" version to use?
","Both approaches that you mentioned are valid and can be used for imputing missing values in both train and test sets. The difference lies in the way the imputer is trained and applied to the data.
Approach 1: fit_transform on train set and transform on test set
In this approach, you fit the imputer on the train set using fit_transform, which learns the parameters (e.g. mean, median, most frequent) of the imputation model based on the distribution of the training data. Then, you use transform on the test set to impute missing values in the test set using the same learned parameters. This approach ensures that the test set is imputed based on the distribution of the training data, which is the goal in most cases.
Approach 2: fit_transform on train set and test set separately
In this approach, you fit the imputer separately on both the train and test sets using fit_transform. This means that the imputation parameters will be learned separately based on the distribution of each set, which may lead to different imputation results between the two sets. This approach is less common and is generally not recommended because it may lead to overfitting to the test set.
In general, it is recommended to use Approach 1 for imputing missing values in both the train and test sets. This ensures that the imputation is done based on the distribution of the training data, which is what we want to learn from. Additionally, it ensures that the same imputation model is used for both the train and test sets, which is important for ensuring consistency in the modeling process.
",data-science
which variable typessizes are atomic on stm32 microcontrollers,"Here are the data types on STM32 microcontrollers: http://www.keil.com/support/man/docs/armcc/armcc_chr1359125009502.htm.
These microcontrollers use 32-bit ARM core processors.
Which data types have automatic atomic read and atomic write access?
I'm pretty sure all 32-bit data types do (since the processor is 32-bits), and all 64-bit data types do NOT (since it would take at least 2 processor operations to read or write a 64-bit word), but what about bool (1 byte), and uint16_t/int16_t (2 bytes)?
Context: I'm sharing variables between multiple threads (single core, but multiple threads, or ""tasks"" as they are called, in FreeRTOS) on the STM32 and need to know if I need to enforce atomic access by turning off interrupts, using mutexes, etc.
UPDATE:
Refering to this sample code:
volatile bool shared_bool;
volatile uint8_t shared u8;
volatile uint16_t shared_u16;
volatile uint32_t shared_u32;
volatile uint64_t shared_u64;
volatile float shared_f; // 32-bits
volatile double shared_d; // 64-bits

// Task (thread) 1
while (true)
{
    // Write to the values in this thread.
    //
    // What I write to each variable will vary. Since other threads are reading
    // these values, I need to ensure my *writes* are atomic, or else I must
    // use a mutex to prevent another thread from reading a variable in the
    // middle of this thread's writing.
    shared_bool = true;
    shared_u8 = 129;
    shared_u16 = 10108;
    shared_u32 = 130890;
    shared_f = 1083.108;
    shared_d = 382.10830;
}

// Task (thread) 2
while (true)
{
    // Read from the values in this thread.
    //
    // What thread 1 writes into these values can change at any time, so I need
    // to ensure my *reads* are atomic, or else I'll need to use a mutex to
    // prevent the other thread from writing to a variable in the midst of
    // reading it in this thread.
    if (shared_bool == whatever)
    {
        // do something
    }
    if (shared_u8 == whatever)
    {
        // do something
    }
    if (shared_u16 == whatever)
    {
        // do something
    }
    if (shared_u32 == whatever)
    {
        // do something
    }
    if (shared_u64 == whatever)
    {
        // do something
    }
    if (shared_f == whatever)
    {
        // do something
    }
    if (shared_d == whatever)
    {
        // do something
    }
}

In the code above, which variables can I do this for without using a mutex? My suspicion is as follows:

volatile bool: safe--no mutex required
volatile uint8_t: safe--no mutex required
volatile uint16_t: safe--no mutex required
volatile uint32_t: safe--no mutex required
volatile uint64_t: UNSAFE--YOU MUST USE A Critical section or MUTEX!
volatile float: safe--no mutex required
volatile double: UNSAFE--YOU MUST USE A Critical section or MUTEX!

Example critical section with FreeRTOS:

https://www.freertos.org/taskENTER_CRITICAL_taskEXIT_CRITICAL.html
// Force atomic access with these critical section atomic access guards.
taskENTER_CRITICAL();
// do the (now guaranteed to be safe) read or write here
taskEXIT_CRITICAL();


Related, but not answering my question:

Atomic operations in ARM
ARM: Is writing/reading from int atomic?
(My own question and answer on atomicity in 8-bit AVR [and Arduino] microcontrollers): https://stackoverflow.com/a/39693278/4561887
https://stm32f4-discovery.net/2015/06/how-to-properly-enabledisable-interrupts-in-arm-cortex-m/

","For the final, definitive answer to this question, jump straight down to the section below titled ""Final answer to my question"".
The generic answer
A single read or a single write, but not multiples, and not increment/decrement, is typically (but not guaranteed by the C standard) automatically atomic on a microcontroller so long as the variable size is <= the CPU size, and is naturally aligned.
Therefore, for an 8-bit mcu, only 8-bit variable types have naturally atomic reads or writes, and on an STM32 32-bit mcu, all 32-bit or smaller variable types have naturally atomic reads or writes.
So, on STM32, all types <= 4 bytes (all bolded types in the list of 9 rows below) have naturally atomic reads and writes.
Caveats:

If you have a packed struct or another unaligned variable, don't expect atomicity on any unaligned types.
This ""natural atomicity"" concept does not exist in the C standard whatsoever, so it's not guaranteed by the C standard. It's just a common feature of microcontrollers. So, pedantically, you can't rely on it in C code, but in practice, you can. So I, and FreeRTOS, and many others, do.

The ARM v7-M Architecture Reference Manual's documentation
UPDATE 30 Oct. 2018: I was accidentally referencing the (slightly) wrong documents (but which said the exact same thing), so I've fixed them in my answer here. See ""Notes about the 30 Oct. 2018 changes"" at bottom of this answer for details.
I definitely don't understand every word here, but the ARM v7-M Architecture Reference Manual (Online source; PDF file direct download) (NOT the Technical Reference Manual [TRM], since it doesn't discuss atomicity) validates my assumptions:

So...I think my 7 assumptions at the bottom of my question are all correct. [30 Oct. 2018: Yes, that is correct. See below for details.]

UPDATE 29 Oct. 2018:
One more little tidbit: FreeRTOS is sure on this
...and it's used in thousands of safety-critical applications world-wide.
Richard Barry, FreeRTOS founder, expert, and core developer, states in tasks.c in two different places (ex: here in the official FreeRTOS V11.0.1 release) that:
/* A critical section is not required because the variables are of type BaseType_t. */

And, for most (all?) 32-bit microcontrollers, such as STM32F4 ARM Cortex-M4 with floating point unit (hence the folder name ARM_CM4F), you can see here in FreeRTOS-Kernel/portable/GCC/ARM_CM4F/portmacro.h that BaseType_t is typedefed as long, and UBaseType_t is typedefed as unsigned long:
typedef long             BaseType_t;
typedef unsigned long    UBaseType_t;

...and in the code where the above ""critical section is not required"" comments are, the variables in question are of type UBaseType_t. Furthermore, long for these chips is int32_t (4 bytes), and unsigned long is uint32_t (4 bytes). So, this means that Richard Barry is saying that 4-byte reads and writes are atomic on these 32-bit microcontrollers. This means that he, at least, is 100% sure 4-byte reads and writes are atomic on STM32. He doesn't mention smaller-byte reads, but for 4-byte reads he is conclusively sure. I have to assume that 4-byte variables being the native processor width, and also, word-aligned, is critical to this being true.
Note that the FreeRTOS version number is found in task.h, here. Here are the two code and comment snippets from tasks.c in FreeRTOS V11.0.1 where he states that a critical section is not required because the variables are of type BaseType_t (or UBaseType_t):
void vTaskSuspendAll( void )
{
    traceENTER_vTaskSuspendAll();

    #if ( configNUMBER_OF_CORES == 1 )
    {
        /* A critical section is not required as the variable is of type
         * BaseType_t.  Please read Richard Barry's reply in the following link to a
         * post in the FreeRTOS support forum before reporting this as a bug! -
         * https:// goo.gl/wu4acr */

        /* portSOFTWARE_BARRIER() is only implemented for emulated/simulated ports that
         * do not otherwise exhibit real time behaviour. */
        portSOFTWARE_BARRIER();

        /* The scheduler is suspended if uxSchedulerSuspended is non-zero.  An increment
         * is used to allow calls to vTaskSuspendAll() to nest. */
        ++uxSchedulerSuspended;

        /* Enforces ordering for ports and optimised compilers that may otherwise place
         * the above increment elsewhere. */
        portMEMORY_BARRIER();
    }
...

UBaseType_t uxTaskGetNumberOfTasks( void )
{
    traceENTER_uxTaskGetNumberOfTasks();

    /* A critical section is not required because the variables are of type
     * BaseType_t. */
    traceRETURN_uxTaskGetNumberOfTasks( uxCurrentNumberOfTasks );

    return uxCurrentNumberOfTasks;
}

The short goo.gl link in the first comment above leads to this full link: FreeRTOS Support Archive: Concerns about the atomicity of vTaskSuspendAll(). The key here is that Richard is relying on each individual 4-byte read or write being naturally atomic on this hardware.
Final answer to my question: all types <= 4 bytes (all bolded types in the list of 9 rows below) are atomic.
Furthermore, upon closer inspection of the TRM on p141 as shown in my screenshot above, the key sentences I'd like to point out are:

In ARMv7-M, the single-copy atomic processor accesses are:
• all byte accesses.
• all halfword accesses to halfword-aligned locations.
• all word accesses to word-aligned locations.

And, per this link, the following is true for ""basic data types implemented in ARM C and C++"" (ie: on STM32):

bool/_Bool is ""byte-aligned"" (1-byte-aligned)
int8_t/uint8_t is ""byte-aligned"" (1-byte-aligned)
int16_t/uint16_t is ""halfword-aligned"" (2-byte-aligned)
int32_t/uint32_t is ""word-aligned"" (4-byte-aligned)
int64_t/uint64_t is ""doubleword-aligned"" (8-byte-aligned) <-- NOT GUARANTEED ATOMIC
float is ""word-aligned"" (4-byte-aligned)
double is ""doubleword-aligned"" (8-byte-aligned) <-- NOT GUARANTEED ATOMIC
long double is ""doubleword-aligned"" (8-byte-aligned) <-- NOT GUARANTEED ATOMIC
all pointers are ""word-aligned"" (4-byte-aligned)

This means that I now have and understand the evidence I need to conclusively state that all bolded rows just above have automatic atomic read and write access (but NOT increment/decrement of course, which is multiple operations). This is the final answer to my question. The only exception to this atomicity might be in packed structs I think, in which case these otherwise-naturally-aligned data types may not be naturally aligned.
Also note that when reading the Technical Reference Manual, ""single-copy atomicity"" apparently just means ""single-core-CPU atomicity"", or ""atomicity on a single-CPU-core architecture."" This is in contrast to ""multi-copy atomicity"", which refers to a ""mutliprocessing system"", or multi-core-CPU architecture. Wikipedia states ""multiprocessing is the use of two or more central processing units (CPUs) within a single computer system"" (https://en.wikipedia.org/wiki/Multiprocessing).
My architecture in question, STM32F767ZI (with ARM Cortex-M7 core), is a single-core architecture, so apparently ""single-copy atomicity"", as I've quoted above from the TRM, applies.
Further Reading:

ARM: Is writing/reading from int atomic?
What is the difference between atomic / volatile / synchronized?
Can variables inside packed structures be read atomically?

Notes about the 30 Oct. 2018 changes:

I had this reference: ARMv7 TRM (Technical Reference Manual). However, this is wrong in 2 ways: 1) This isn't a TRM at all! The TRM is a short (~200 pgs) Technical Reference Manual. This, however, is the ""Architecture Reference Manual"", NOT the TRM. It is a much longer and more generic document, as Architecture reference manuals are on the order of ~1000~2000 pgs it turns out. 2) This is for the ARMv7-A and ARMv7-R processors, but the manual I need for the STM32 mcu in question is for the ARMv7-M processor.
Here is the correct link to the ARM Cortex-M7 Processor Technical Reference Manual. Online: https://developer.arm.com/docs/ddi0489/latest. PDF: https://static.docs.arm.com/ddi0489/d/DDI0489D_cortex_m7_trm.pdf.
The correct TRM just above, on p99 (5-36) says, ""For more
information on atomicity, see the ARM®v7-M Architecture Reference Manual."" So, here is that manual. Online download link: https://developer.arm.com/products/architecture/cpu-architecture/m-profile/docs/ddi0403/latest/armv7-m-architecture-reference-manual. PDF: https://static.docs.arm.com/ddi0489/d/DDI0489D_cortex_m7_trm.pdf. It discusses atomicity on p79-80 (A3-79 to A3-80).

To create atomic access guards (usually by turning off interrupts when reads and writes are not atomic) see:

[my Q&A] What are the various ways to disable and re-enable interrupts in STM32 microcontrollers in order to implement atomic access guards?
My doAtomicRead() func here which can do atomic reads withOUT turning off interrupts

",c
problem with index out of bounds in stringbuilder matrix,"I am making a program to learn how to manage text strings and files in Java, where the user will enter university modules in blocks of time. It's still a prototype, and for now I'm trying to fill an array of StringBuilders with the base structure, so I can then mutate it with the data the user enters.
My method was to create an empty StringBuilder array, of dimension 6X10, fill it with empty strings, and then ""fill"" it with an array of Strings with the basic structure:
import java.util.Arrays;

public class Main {
    public static void main(String[] args) {
        
        StringBuilder[][] baseStructure = new StringBuilder[6][10];
        for(int i = 0; i < 6; i++){
            for(int j = 0; j < 10; j++){
                baseStructure[i][j] = new StringBuilder();
            }
        }


        String[][] baseStructure2 = {
            {""HORA"", ""Lunes"", ""Martes"", ""Miercoles"", ""Jueves"", ""Viernes""},
            {""  8:30 - 9:30"", ""1"", ""2"", ""3"", ""4"", ""5""},
            {"" 9:40 - 10:40"", ""1"", ""2"", ""3"", ""4"", ""5""},
            {""10:50 - 11:50"", ""1"", ""2"", ""3"", ""4"", ""5""},
            {""12:00 - 13:00"", ""1"", ""2"", ""3"", ""4"", ""5""},
            {""13:10 - 14:10"", ""1"", ""2"", ""3"", ""4"", ""5""},
            {""14:20 - 15:20"", ""1"", ""2"", ""3"", ""4"", ""5""},
            {""15:30 - 16:30"", ""1"", ""2"", ""3"", ""4"", ""5""},
            {""16:40 - 17:40"", ""1"", ""2"", ""3"", ""4"", ""5""},
            {""17:50 - 18:50"", ""1"", ""2"", ""3"", ""4"", ""5""}
        };

        for(int m = 0; m < 6; m++){
            for(int n = 0; n < 10; n++){
                printMatrix(baseStructure);
                baseStructure[m][n] = new StringBuilder(baseStructure2[m][n]);
            }
        }

    }

    public static void printMatrix(StringBuilder[][] strMatrix){
        for (StringBuilder[] strings : strMatrix) {
            System.out.println(Arrays.deepToString(strings));
        }
    }

However, even though my loops have the same dimension values, when running the program I get index access errors out of range 6.
WHAT I TRIED:
I tried to populate a StringBuilder matrix, ""copying"" data from a String matrix, printing it as the loop went to see it grow with each iteration.
The program works until it reaches the end of the first row, where it manages to copy the data until the Friday, and then breaks:
[HORA, Lunes, Martes, Miercoles, Jueves, Viernes, , , , ]
[, , , , , , , , , ]
[, , , , , , , , , ]
[, , , , , , , , , ]
[, , , , , , , , , ]
[, , , , , , , , , ]

Exception in thread ""main"" java.lang.ArrayIndexOutOfBoundsException: Index 6 out of bounds for length 6 at Main.main(Main.java:44)

I can tell there's something wonky because the first row should NOT have 4 extra empty spaces, but I'm not sure what's causing it.
(Developed on Java 23.0.1 on IntellliJ IDEA Community Edition 2024.3.1.1)
","It seems like you inverted the dimensions of your 2d array. Which also reflected in both your loops.
You have 10 lines who each have 6 elements per line, not the other way round. :)
The 2d array, instead of 6, 10:
StringBuilder[][] baseStructure = new StringBuilder[10][6];

Loop 1:
for(int i = 0; i < 10; i++){
        for(int j = 0; j < 6; j++){

And loop 2:
for(int m = 0; m < 10; m++){
            for(int n = 0; n < 6; n++){

",java
fast exponential implementation in c  based on 2nd degree polynomial,"I made a fast approximation of the exponential function (e^x) based on a 2nd-degree polynomial. However, the program doesn't give reasonable numbers if x<0.0 .
The code is like this:
// Compile with: gcc -std=c99 fastexp.c -o fastexp

#include <stdio.h>
#include <math.h>

#define ALPHA 0.50617024f
#define ALPHAc 0.49382976f
#define INV_LN2 1.4426950408889634f

float fast2pxm1_m(float x) { // 0.0 <= x <= 1.0
  return x*(ALPHA+ALPHAc*x);
}

float fast2px(float x) {
  float m, mp;
  float e;

  m = modff(x, &e);
  mp = fast2pxm1_m(m) + 1.0f;
  
  return ldexpf(mp, e);
}

float fastexp(float x) {  // exportable
  return fast2px(x*INV_LN2);
}

int main(int argc, char *argv[]) {
  float x;

  for (int i = -10; i <= 10; i++) {
     x= (float) i/10.0*2;
     printf(""%.8f => %.8f\n"",x,fastexp(x));
  }
  return 0;
}


The results of the function fastexp are not good for x<0.0: its evolution along x is erroneous, with local minima in several places (x). See the results of the program:
-2.00000000 => 0.23474069
-1.79999995 => 0.21845233
-1.60000002 => 0.22272082
-1.39999998 => 0.24754614
-1.20000005 => 0.44696173
-1.00000000 => 0.43635058
-0.80000001 => 0.46685311
-0.60000002 => 0.93187356
-0.40000001 => 0.87235498
-0.20000000 => 0.89506382
0.00000000 => 1.00000000
0.20000000 => 1.18716359
0.40000001 => 1.45655441
0.60000002 => 1.80817270
0.80000001 => 2.17952919
1.00000000 => 2.64171839
1.20000005 => 3.26836252
1.39999998 => 4.04080629
1.60000002 => 4.81200027
1.79999995 => 5.91210270
2.00000000 => 7.34111547

EDIT:
The constants below give a better approximation:
#define ALPHA 0.6563658616549711f               
#define ALPHAc 0.34363413834502887f             

","The polynomial you're using in fast2pxm1_m approximates the function pow(2, x) - 1, but it does this well only in the range [0, 1], as evidenced by the comment // 0.0 <= x <= 1.0.
The value you're passing to that function is the return value from modff which has the same sign as the input (and therefore is in the range (-1, 1)), so the result is inaccurate for negative inputs.
One way to fix this is to modify the output of modff so that the fractional part is always non-negative:
m = modff(x, &e);
if (m < 0.0f) {
    m += 1.0f; // fractional part
    e -= 1.0f; // integral part
}

This gives more reasonable outputs:
     x         fastexp(x)       exp(x)
----------------------------------------
-2.00000000 => 0.13306235     0.13533528
-1.79999995 => 0.16054048     0.16529890
-1.60000002 => 0.19829696     0.20189651
-1.39999998 => 0.24633193     0.24659697
-1.20000005 => 0.29292828     0.30119420
-1.00000000 => 0.35886729     0.36787944
-0.80000001 => 0.44536310     0.44932896
-0.60000002 => 0.53846931     0.54881162
-0.40000001 => 0.65119916     0.67032004
-0.20000000 => 0.80504274     0.81873075
 0.00000000 => 1.00000000     1.00000000
 0.20000000 => 1.18716359     1.22140276
 0.40000001 => 1.45655441     1.49182471
 0.60000002 => 1.80817270     1.82211884
 0.80000001 => 2.17952919     2.22554096
 1.00000000 => 2.64171839     2.71828183
 1.20000005 => 3.26836252     3.32011708
 1.39999998 => 4.04080629     4.05519987
 1.60000002 => 4.81200027     4.95303254
 1.79999995 => 5.91210270     6.04964718
 2.00000000 => 7.34111547     7.38905610

",c
how to calculate rolling  moving average using python  numpy  scipy,"There seems to be no function that simply calculates the moving average on numpy/scipy, leading to convoluted solutions.
My question is two-fold:

What's the easiest way to (correctly) implement a moving average with numpy?
Since this seems non-trivial and error prone, is there a good reason not to have the batteries included in this case?

","A simple way to achieve this is by using np.convolve.
The idea behind this is to leverage the way the discrete convolution is computed and use it to return a rolling mean. This can be done by convolving with a sequence of np.ones of a length equal to the sliding window length we want.
In order to do so we could define the following function:
def moving_average(x, w):
    return np.convolve(x, np.ones(w), 'valid') / w

This function will be taking the convolution of the sequence x and a sequence of ones of length w. Note that the chosen mode is valid so that the convolution product is only given for points where the sequences overlap completely.

Some examples:
x = np.array([5,3,8,10,2,1,5,1,0,2])

For a moving average with a window of length 2 we would have:
moving_average(x, 2)
# array([4. , 5.5, 9. , 6. , 1.5, 3. , 3. , 0.5, 1. ])

And for a window of length 4:
moving_average(x, 4)
# array([6.5 , 5.75, 5.25, 4.5 , 2.25, 1.75, 2.  ])


How does convolve work?
Lets have a more in depth look at the way the discrete convolution is being computed.
The following function aims to replicate the way np.convolve is computing the output values:
def mov_avg(x, w):
    for m in range(len(x)-(w-1)):
        yield sum(np.ones(w) * x[m:m+w]) / w 

Which, for the same example above would also yield:
list(mov_avg(x, 2))
# [4.0, 5.5, 9.0, 6.0, 1.5, 3.0, 3.0, 0.5, 1.0]

So what is being done at each step is to take the inner product between the array of ones and the current  window. In this case the multiplication by np.ones(w) is superfluous given that we are directly taking the sum of the sequence.
Bellow is an example of how the first outputs are computed so that it is a little clearer. Lets suppose we want a window of w=4:
[1,1,1,1]
[5,3,8,10,2,1,5,1,0,2]
= (1*5 + 1*3 + 1*8 + 1*10) / w = 6.5

And the following output would be computed as:
  [1,1,1,1]
[5,3,8,10,2,1,5,1,0,2]
= (1*3 + 1*8 + 1*10 + 1*2) / w = 5.75

And so on, returning a moving average of the sequence once all overlaps have been performed.
",numpy
how to solve an overflowerror when exporting pandas dataframe to json,"In Jupyter, I have a dataframe of 400 000 objects that I can't entirely export to a JSON file without facing the following error.
The export is working great as long as I limit the exportation to the first 141 000 objects , whatever the order of these first objects.
Should I be aware of any size limitation dealing with large JSON files ?
Thank you.
OverflowError                             Traceback (most recent call last)
<ipython-input-254-b59373f1eeb2> in <module>
----> 1 df4.to_json('test.json', orient = 'records')

~/anaconda3/lib/python3.7/site-packages/pandas/core/generic.py in to_json(self, path_or_buf, orient, date_format, double_precision, force_ascii, date_unit, default_handler, lines, compression, index)
   1889                             default_handler=default_handler,
   1890                             lines=lines, compression=compression,
-> 1891                             index=index)
   1892 
   1893     def to_hdf(self, path_or_buf, key, **kwargs):

~/anaconda3/lib/python3.7/site-packages/pandas/io/json/json.py in to_json(path_or_buf, obj, orient, date_format, double_precision, force_ascii, date_unit, default_handler, lines, compression, index)
     56         double_precision=double_precision, ensure_ascii=force_ascii,
     57         date_unit=date_unit, default_handler=default_handler,
---> 58         index=index).write()
     59 
     60     if lines:

~/anaconda3/lib/python3.7/site-packages/pandas/io/json/json.py in write(self)
     99         return self._write(self.obj, self.orient, self.double_precision,
    100                            self.ensure_ascii, self.date_unit,
--> 101                            self.date_format == 'iso', self.default_handler)
    102 
    103     def _write(self, obj, orient, double_precision, ensure_ascii,

~/anaconda3/lib/python3.7/site-packages/pandas/io/json/json.py in _write(self, obj, orient, double_precision, ensure_ascii, date_unit, iso_dates, default_handler)
    154                                                double_precision,
    155                                                ensure_ascii, date_unit,
--> 156                                                iso_dates, default_handler)
    157 
    158 

~/anaconda3/lib/python3.7/site-packages/pandas/io/json/json.py in _write(self, obj, orient, double_precision, ensure_ascii, date_unit, iso_dates, default_handler)
    110             date_unit=date_unit,
    111             iso_dates=iso_dates,
--> 112             default_handler=default_handler
    113         )
    114 

OverflowError: int too big to convert

","There is no inherent limitation on data size in JSON, so this isn't your problem: the message suggests some difficulty with a particular integer value.
This underlines the difficulty of working with such large files, since you now have to isolate the particular record that's causing the problems in the middle of the to_json call.
Since you know roughly where the problem is you could try converting subsets of your data frame in a bisection technique to home in on the row that's causing the issues.
",pandas
all intermediate steps should be transformers and implement fit and transform,"I am implementing a pipeline using important features selection and then using the same features to train my random forest classifier. Following is my code.
m = ExtraTreesClassifier(n_estimators = 10)
m.fit(train_cv_x,train_cv_y)
sel = SelectFromModel(m, prefit=True)
X_new = sel.transform(train_cv_x)
clf = RandomForestClassifier(5000)

model = Pipeline([('m', m),('sel', sel),('X_new', X_new),('clf', clf),])
params = {'clf__max_features': ['auto', 'sqrt', 'log2']}

gs = GridSearchCV(model, params)
gs.fit(train_cv_x,train_cv_y)

So X_new are the new features selected via SelectFromModel and sel.transform. Then I want to train my RF using the new features selected.
I am getting the following error:
All intermediate steps should be transformers and implement fit and transform, 
ExtraTreesClassifier ...

","Like the traceback says: each step in your pipeline needs to have a fit() and transform() method (except the last, which just needs fit().  This is because a pipeline chains together transformations of your data at each step.
sel.transform(train_cv_x) is not an estimator and doesn't meet this criterion.
In fact, it looks like based on what you're trying to do, you can leave this step out.  Internally, ('sel', sel) already does this transformation--that's why it's included in the pipeline.
Secondly, ExtraTreesClassifier (the first step in your pipeline), doesn't have a transform() method, either.  You can verify that here, in the class docstring.  Supervised learning models aren't made for transforming data; they're made for fitting on it and predicting based off that.
What type of classes are able to do transformations?

Ones that scale your data.  See preprocessing and normalization.
Ones that transform your data (in some other way than the above).  Decomposition and other unsupervised learning methods do this.

Without reading between the lines too much about what you're trying to do here, this would work for you:

First split x and y using train_test_split.  The test dataset produced by this is held out for final testing, and the train dataset within GridSearchCV's cross-validation will be further broken out into smaller train and validation sets.
Build a pipeline that satisfies what your traceback is trying to tell you.
Pass that pipeline to GridSearchCV, .fit() that grid search on X_train/y_train, then .score() it on X_test/y_test.

Roughly, that would look like this:
from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.33, random_state=444)

sel = SelectFromModel(ExtraTreesClassifier(n_estimators=10, random_state=444), 
                      threshold='mean')
clf = RandomForestClassifier(n_estimators=5000, random_state=444)

model = Pipeline([('sel', sel), ('clf', clf)])
params = {'clf__max_features': ['auto', 'sqrt', 'log2']}

gs = GridSearchCV(model, params)
gs.fit(X_train, y_train)

# How well do your hyperparameter optimizations generalize
# to unseen test data?
gs.score(X_test, y_test)

Two examples for further reading:

Pipelining: chaining a PCA and a logistic regression
Sample pipeline for text feature extraction and evaluation

",scikit-learn
azure open ai while developing python script using api key getting error,"import openai

# Setup
openai.api_key = 'xxxxxxxxxxxxxxxxxxxxxx'
openai.api_base = ""xxxxxxxxxxxxxxxxxxxxxx""

openai.api_version = '2024-08-20'  # Ensure this is correct

def test_openai():
    try:
        response = openai.Image.create(
            prompt=""A dog in rain image"",
            model=""dall-e-3"",  # Try with a different model ifneeded
            n=1,
            size=""1024x1024""
        )
        print(response)
    except Exception as e:
        print(f""Error: {e}"")

test_openai()


Error: Resource not found
I have created azure open ai model dall-e-3
the api key and api base both worked with model gpt-35-turbo
","The error you are getting because you need to use the Deployment name instead of the Model name.

You need to create client to connect  to generate result.
Below code worked for me.
I am using a Flask app to use the openai code and view the generated image in index.html file.
app.py:
from flask import Flask,render_template
import os
from openai import AzureOpenAI
import json

app = Flask(__name__)

@app.route('/', methods=['GET'])
def fetch_image_url():

    try:
        client = AzureOpenAI(
        api_version=""2024-05-01-preview"",
        azure_endpoint=""https://xxxxxxxxxx.openai.azure.com/"",
        api_key=""xxxxxxxxxxxxxxxxxx""
        )

        result = client.images.generate(
            model=""dallechatgpt"",
            prompt=""A dog in rain image"",
            n=1
        )

        image_url = json.loads(result.model_dump_json())['data'][0]['url']

        return render_template('index.html',image_url=image_url)
    except Exception as e:
        print(f""error{e}"")
        

if __name__== ""__main__"":
    app.run()

templates/index.html
<!DOCTYPE html>
<html lang=""en"">
<head>
    <meta charset=""UTF-8"">
    <meta name=""viewport"" content=""width=device-width, initial-scale=1.0"">
    <title>Document</title>
</head>
<body>
    {% if image %}
    <h2>Generated Image:</h2>
    <img src=""{{ image }}"" alt=""Generated Image"" style=""width: auto; height: 50vh;"">
    {% endif %}
</body>
</html>

requirements.txt:
flask
openai

OUTPUT:

",chatgpt
move span center to click position not showing range when clicking in subplot with shown range,"I have created a 2 plot subplot. Selecting the range in both subplots works fine. Clicking into the upper subplot (the zoomed plot) shifts both views fine, but when clicking in the lower (total view) plot, the selected range (SpanSelector) region vanishes. What am I missing.
As a next step I intend to ass a cross-hair cursor to the top (zoomed) plot.
Attaches the code-example:
import sys
import numpy as np
from matplotlib.backends.backend_qt5agg import FigureCanvasQTAgg as FigureCanvas
import matplotlib.pyplot as plt
from matplotlib.widgets import MultiCursor
from PyQt5.QtWidgets import QMainWindow,QVBoxLayout
from PyQt5.QtWidgets import QApplication
from PyQt5 import QtCore, QtGui, QtWidgets
from matplotlib.ticker import FuncFormatter
from matplotlib.widgets import SpanSelector
import matplotlib.ticker as ticker

class MainWindow_code_serarch(object):

    def setup_code_serarch(self, MainWindow):
        MainWindow.setObjectName(""MainWindow"")
        MainWindow.resize(1024, 800)
        self.centralwidget = QtWidgets.QWidget(MainWindow)
        self.centralwidget.setObjectName(""centralwidget"")
        self.verticalLayoutWidget = QtWidgets.QWidget(self.centralwidget)
        self.verticalLayoutWidget.setGeometry(QtCore.QRect(0, 0, 1024, 800))
        self.verticalLayoutWidget.setObjectName(""verticalLayoutWidget"")
        self.verticalLayout = QtWidgets.QVBoxLayout(self.verticalLayoutWidget)
        self.verticalLayout.setContentsMargins(0,0,0,0)
        self.verticalLayout.setObjectName(""verticalLayout"")


        self.figure = plt.figure()
        self.canvas = FigureCanvas(self.figure)
        self.verticalLayout.addWidget(self.canvas)
        axes, axes2 = self.figure.subplots(nrows=2, sharex=True)
    
        datacount = 100000
    
        data_x = []
        data_y = []
    
        for i in range(1, datacount):
            data_x.append(i)
            if i % 250 <= 50:
                data_y.append(np.nan)
            else:
                data_y.append(np.sin(i/100)+0.05*np.sin(i))
    
        #** matplotlib subplot *********************************************
        self.figure.subplots_adjust(left=0.03, right=0.97, top=0.975, bottom=0.075, wspace=0, hspace=0.2)
    
        ax1, ax2 = self.figure.subplots(2, height_ratios=[0.8,0.2])
        ax1.grid()
        ax1.tick_params(axis = 'x', length=0)
        ax1.tick_params(axis = 'y', length=0)

        ax2.tick_params(axis = 'x', length=0)
        ax2.tick_params(axis = 'y', length=0)
        ax2.grid()
    
        def format_tick_labels(x, pos):
            return '{0:2.0e}'.format(x)

        ax2.xaxis.set_major_formatter(FuncFormatter(format_tick_labels))
        
        ax2.plot(data_x, data_y, linewidth = 0.5)
        ax2.set_xlim(0, datacount)
        ax2.set_ylim(-1.2, 1.2)
    
        self.line2, = ax1.plot([], [], linewidth=0.5)
    
        def onselect_span(xmin, xmax):
            print('onselect_span')
            indmin, indmax = np.searchsorted(data_x, (xmin, xmax))
        
            print(f'Span: select indmin = {indmin}, indmax = {indmax}')

            region_x = data_x[indmin:indmax]   
            region_y = data_y[indmin:indmax]   
        

            if len(region_x)>=2:
                self.line2.set_data(region_x, region_y)
                ax1.set_xlim(region_x[0], region_x[-1])     #select from region start till region end...
                ax1.set_ylim(-1.2,1.2)#region_y[0], region_y[-1])
                self.span2.extents = (region_x[0], region_x[-1])
                self.canvas.draw_idle()
                self.mouse_SpanSelected = True;    
            else: 
                print('onselect_span: region too small') 
            
            self.span1.clear()
            print('onselect_span end')
  
    
        self.span1 = SpanSelector(
            ax1,
            onselect_span,
            ""horizontal"",
            useblit=True,
            props=dict(alpha=0.3, facecolor=""tab:red""),
            interactive=True,
            drag_from_anywhere=True,
            grab_range = 3,
        )
    
        self.span2 = SpanSelector(
            ax2,
            onselect_span,
            ""horizontal"",
            useblit=True,
            props=dict(alpha=0.3, facecolor=""tab:red""),
            interactive=True,
            drag_from_anywhere=True,
            grab_range = 3,
        )
    
        def onclick(event):
            global ix
            ix = event.xdata
            print('onclick end')

                
        def onrelease(event):
            print('onrelease')
            global ix, ixrel
            ixrel = event.xdata

            if abs(ix-ixrel)<=2:
                print('Release: region too small')
                width_half = int ((self.line2._x[-1] - self.line2._x[0])/2)
                self.span2.extents = (ixrel - width_half, ixrel + width_half)
                onselect_span(ixrel - width_half, ixrel + width_half)
                self.span2.update()
            
            self.canvas.draw_idle()
            
            print('onrelease end')
        
        
        click_id = self.figure.canvas.mpl_connect('button_press_event', onclick)
        relaese_id = self.figure.canvas.mpl_connect('button_release_event', onrelease)

        self.span2.extents = (4000,15501)  #set selectred region
        onselect_span(4000,15501)

        self.canvas.draw()
    
        MainWindow.setCentralWidget(self.centralwidget)


if __name__ == ""__main__"":
    import sys
    app = QtWidgets.QApplication(sys.argv)
    MainWindow = QtWidgets.QMainWindow()
    ui = MainWindow_code_serarch()
    ui.setup_code_serarch(MainWindow)
    MainWindow.show()
    sys.exit(app.exec_())

Image: at start:
this is how it looks after starting the app

when I click in the upper graph the range shifts

when I click in the lower graph the range disappears => this is my problem

","The thing is that when you click, that is like selecting an empty span (I take that you are already aware of that, since you instrument the code with a lot of print that tell you exactly what happen, with what values).
And SpanSelector is removed when you select an empty span. And this occurs even before you had a chance to call your callbacks.
(I have a vague feeling, but very vague, I didn't take time to understand why you did all these things with a callback not only for select but also for click and release, that you are recoding what SpanSelector is already doing).
You can prevent that removal by playing with parameter minspan of SpanSelectors. Make it minspan=-1, and then they will never be removed. Sure, it is a aberration theoretically (default minspan=0 is supposed to be the least value that makes sense), but in your case, since you will then do what it takes to put it back if needed...
At any rate, it seems to be working like I think  you expect
So, my minimal reproducible answer :-) (it is only a copy of your question, with two additional minspan=-1)
import sys
import numpy as np
from matplotlib.backends.backend_qt5agg import FigureCanvasQTAgg as FigureCanvas
import matplotlib.pyplot as plt
from matplotlib.widgets import MultiCursor
from PyQt5.QtWidgets import QMainWindow,QVBoxLayout
from PyQt5.QtWidgets import QApplication
from PyQt5 import QtCore, QtGui, QtWidgets
from matplotlib.ticker import FuncFormatter
from matplotlib.widgets import SpanSelector
import matplotlib.ticker as ticker

class MainWindow_code_serarch(object):

    def setup_code_serarch(self, MainWindow):
        MainWindow.setObjectName(""MainWindow"")
        MainWindow.resize(1024, 800)
        self.centralwidget = QtWidgets.QWidget(MainWindow)
        self.centralwidget.setObjectName(""centralwidget"")
        self.verticalLayoutWidget = QtWidgets.QWidget(self.centralwidget)
        self.verticalLayoutWidget.setGeometry(QtCore.QRect(0, 0, 1024, 800))
        self.verticalLayoutWidget.setObjectName(""verticalLayoutWidget"")
        self.verticalLayout = QtWidgets.QVBoxLayout(self.verticalLayoutWidget)
        self.verticalLayout.setContentsMargins(0,0,0,0)
        self.verticalLayout.setObjectName(""verticalLayout"")


        self.figure = plt.figure()
        self.canvas = FigureCanvas(self.figure)
        self.verticalLayout.addWidget(self.canvas)
        axes, axes2 = self.figure.subplots(nrows=2, sharex=True)
    
        datacount = 100000
    
        data_x = []
        data_y = []
    
        for i in range(1, datacount):
            data_x.append(i)
            if i % 250 <= 50:
                data_y.append(np.nan)
            else:
                data_y.append(np.sin(i/100)+0.05*np.sin(i))
    
        #** matplotlib subplot *********************************************
        self.figure.subplots_adjust(left=0.03, right=0.97, top=0.975, bottom=0.075, wspace=0, hspace=0.2)
    
        ax1, ax2 = self.figure.subplots(2, height_ratios=[0.8,0.2])
        ax1.grid()
        ax1.tick_params(axis = 'x', length=0)
        ax1.tick_params(axis = 'y', length=0)

        ax2.tick_params(axis = 'x', length=0)
        ax2.tick_params(axis = 'y', length=0)
        ax2.grid()
    
        def format_tick_labels(x, pos):
            return '{0:2.0e}'.format(x)

        ax2.xaxis.set_major_formatter(FuncFormatter(format_tick_labels))
        
        ax2.plot(data_x, data_y, linewidth = 0.5)
        ax2.set_xlim(0, datacount)
        ax2.set_ylim(-1.2, 1.2)
    
        self.line2, = ax1.plot([], [], linewidth=0.5)
    
        def onselect_span(xmin, xmax):
            print('onselect_span')
            indmin, indmax = np.searchsorted(data_x, (xmin, xmax))
        
            print(f'Span: select indmin = {indmin}, indmax = {indmax}')

            region_x = data_x[indmin:indmax]   
            region_y = data_y[indmin:indmax]   
        

            if len(region_x)>=2:
                self.line2.set_data(region_x, region_y)
                ax1.set_xlim(region_x[0], region_x[-1])     #select from region start till region end...
                ax1.set_ylim(-1.2,1.2)#region_y[0], region_y[-1])
                self.span2.extents = (region_x[0], region_x[-1])
                self.canvas.draw_idle()
                self.mouse_SpanSelected = True;    
            else: 
                print('onselect_span: region too small') 
            
            self.span1.clear()
            print('onselect_span end')
  
    
        self.span1 = SpanSelector(
            ax1,
            onselect_span,
            ""horizontal"",
            useblit=True,
            props=dict(alpha=0.3, facecolor=""tab:red""),
            interactive=True,
            drag_from_anywhere=True,
            minspan=-1,                      ## <<<<<<<< HERE
            grab_range = 3,
        )
    
        self.span2 = SpanSelector(
            ax2,
            onselect_span,
            ""horizontal"",
            useblit=True,
            props=dict(alpha=0.3, facecolor=""tab:red""),
            interactive=True,
            drag_from_anywhere=True,
            minspan=-1,                            ## <<<<<<<< AND HERE
            grab_range = 3,
        )
    
        def onclick(event):
            global ix
            ix = event.xdata
            print('onclick end')

                
        def onrelease(event):
            print('onrelease')
            global ix, ixrel
            ixrel = event.xdata

            if abs(ix-ixrel)<=2:
                print('Release: region too small')
                width_half = int ((self.line2._x[-1] - self.line2._x[0])/2)
                self.span2.extents = (ixrel - width_half, ixrel + width_half)
                onselect_span(ixrel - width_half, ixrel + width_half)
                self.span2.update()
            
            self.canvas.draw_idle()
            
            print('onrelease end')
        
        
        click_id = self.figure.canvas.mpl_connect('button_press_event', onclick)
        relaese_id = self.figure.canvas.mpl_connect('button_release_event', onrelease)

        self.span2.extents = (4000,15501)  #set selectred region
        onselect_span(4000,15501)

        self.canvas.draw()
    
        MainWindow.setCentralWidget(self.centralwidget)


if __name__ == ""__main__"":
    import sys
    app = QtWidgets.QApplication(sys.argv)
    MainWindow = QtWidgets.QMainWindow()
    ui = MainWindow_code_serarch()
    ui.setup_code_serarch(MainWindow)
    MainWindow.show()
    sys.exit(app.exec_())

",matplotlib
nextjs edge function module not found error 39path39 and internal server error when using openai integration,"Description
I'm encountering an issue where the path module is not being found in my Node.js environment. This problem occurs when trying to send message in the chatbox.
Error Message
Module not found: Can't resolve 'path'

https://nextjs.org/docs/messages/module-not-found

Import trace for requested module:
./node_modules/dotenv/config.js
./src/lib/db/index.ts
./src/app/api/chat/route.ts
./node_modules/next/dist/build/webpack/loaders/next-edge-app-route-loader/index.js?absolutePagePath=C%3A%5CUsers%5CDell%5COneDrive%5CDesktop%5C100xDevs%5Csummarize-my-pdf-ai%5Csrc%5Capp%5Capi%5Cchat%5Croute.ts&page=%2Fapi%2Fchat%2Froute&appDirLoader=bmV4dC1hcHAtbG9hZGVyP25hbWU9YXBwJTJGYXBpJTJGY2hhdCUyRnJvdXRlJnBhZ2U9JTJGYXBpJTJGY2hhdCUyRnJvdXRlJmFwcFBhdGhzPSZwYWdlUGF0aD1wcml2YXRlLW5leHQtYXBwLWRpciUyRmFwaSUyRmNoYXQlMkZyb3V0ZS50cyZhcHBEaXI9QyUzQSU1Q1VzZXJzJTVDRGVsbCU1Q09uZURyaXZlJTVDRGVza3RvcCU1QzEwMHhEZXZzJTVDc3VtbWFyaXplLW15LXBkZi1haSU1Q3NyYyU1Q2FwcCZwYWdlRXh0ZW5zaW9ucz10c3gmcGFnZUV4dGVuc2lvbnM9dHMmcGFnZUV4dGVuc2lvbnM9anN4JnBhZ2VFeHRlbnNpb25zPWpzJnJvb3REaXI9QyUzQSU1Q1VzZXJzJTVDRGVsbCU1Q09uZURyaXZlJTVDRGVza3RvcCU1QzEwMHhEZXZzJTVDc3VtbWFyaXplLW15LXBkZi1haSZpc0Rldj10cnVlJnRzY29uZmlnUGF0aD10c2NvbmZpZy5qc29uJmJhc2VQYXRoPSZhc3NldFByZWZpeD0mbmV4dENvbmZpZ091dHB1dD0mcHJlZmVycmVkUmVnaW9uPSZtaWRkbGV3YXJlQ29uZmlnPWUzMCUzRCE%3D&nextConfigOutput=&preferredRegion=&middlewareConfig=e30%3D!

Environment

Node.js Version: v18.17.1
Framework: Next.js
Operating System: WINDOWS 11

Code
route.ts
import { getContext } from ""@/lib/context"";
import { db } from ""@/lib/db"";
import { chats, messages as _messages } from ""@/lib/db/schema"";
import { openai } from ""@ai-sdk/openai"";
import { streamText } from ""ai"";
import { eq } from ""drizzle-orm"";
import { NextResponse } from ""next/server"";
import { Message } from ""ai/react"";

export const runtime = ""edge"";

export async function POST(req: Request) {
  console.log(""Called api"");

  try {
    const { messages,chatId } = await req.json();
    console.log(""Messages:"", messages);
    console.log(""Chat ID:"", chatId);
    const _chats = await db.select().from(chats).where(eq(chats.id, chatId));
    // Log retrieved chats
    console.log(""_chats:"", _chats);
    if (_chats.length != 1) {
      return NextResponse.json({ Error: "" Chat not found"" }, { status: 404 });
    }
    const fileKey = _chats[0].fileKey;
    const lastMessage = messages[messages.length - 1];
    // Log fileKey and lastMessage
    console.log(""File Key:"", fileKey);
    console.log(""Last Message:"", lastMessage);
    const context = await getContext(lastMessage.content, fileKey);
    console.log(""Context:"", context);
    const prompt = {
      role: ""system"",
      content: `AI assistant is a brand new, powerful, human-like artificial intelligence.
      The traits of AI include expert knowledge, helpfulness, cleverness, and articulateness.
      AI is a well-behaved and well-mannered individual.
      AI is always friendly, kind, and inspiring, and he is eager to provide vivid and thoughtful responses to the user.
      AI has the sum of all knowledge in their brain, and is able to accurately answer nearly any question about any topic in conversation.
      AI assistant is a big fan of Pinecone and Vercel.
      START CONTEXT BLOCK
      ${context}
      END OF CONTEXT BLOCK
      AI assistant will take into account any CONTEXT BLOCK that is provided in a conversation.
      If the context does not provide the answer to question, the AI assistant will say, ""I'm sorry, but I don't know the answer to that question"".
      AI assistant will not apologize for previous responses, but instead will indicated new information was gained.
      AI assistant will not invent anything that is not drawn directly from the context.
      `,
    };
    const response = await streamText({
      model: openai(""gpt-4o-mini""),
      messages: [
        prompt,
        ...messages.filter((message: Message) => message.role === ""user""),
      ],
    });
    return response.toDataStreamResponse();
  } catch (error) {
    console.log(error);
    return NextResponse.json(
      { error: ""Internal Server Error"" },
      { status: 500 }
    );
  }
}

ChatComponent.tsx
""use client"";
import React from ""react"";
import { Input } from ""./ui/input"";
import { useChat } from ""ai/react"";
import { Button } from ""./ui/button"";
import { SendIcon } from ""lucide-react"";
import MessageList from ""./MessageList"";

type Props = { chatId: number };

const ChatComponent = ({ chatId }: Props) => {
  console.log(""Chat ID in ChatComponent:"", chatId);

  const { input, handleInputChange, handleSubmit, messages } = useChat({
    api: ""/api/chat"",

    body: {
      chatId,
    },
  });
  // React.useEffect(() => {
  //   const messageContainer = document.getElementById(""message-container"");
  //   if (messageContainer) {
  //     messageContainer.scrollTo({
  //       top: messageContainer.scrollHeight,
  //       behavior: ""smooth"",
  //     });
  //   }
  // }, [messages]);

  return (
    <div
      className=""relative max-h-screen overflow-scroll""
      id=""message-container""
    >
      {/* Header */}
      <div className=""sticky top-0 inset-x-0 p-2 bg-white h-fit"">
        <h3 className=""text-xl font-bold"">Chat</h3>
      </div>
      {/* Message List */}
      <MessageList messages={messages} />
      <form
        onSubmit={handleSubmit}
        className=""sticky  bottom-0 px-2 py-4 inset-x-0 bg-white""
      >
        <div className=""flex"">
          <Input
            value={input}
            onChange={handleInputChange}
            placeholder=""Ask any question...""
            className=""w-full""
          />
          <Button className=""bg-gradient-to-r from-sky-400 to-blue-500 ml-2"">
            <SendIcon className=""h-4 w-4"" />
          </Button>
        </div>
      </form>
    </div>
  );
};

export default ChatComponent;


Logs
PS C:\Users\Dell\OneDrive\Desktop\100xDevs\summarize-my-pdf-ai> node -v
v18.17.1
PS C:\Users\Dell\OneDrive\Desktop\100xDevs\summarize-my-pdf-ai>

this is console GET /chat/9?_rsc=a12k2 200 in 227ms
GET /chat/8?_rsc=18zah 200 in 383ms
○ Compiling /api/chat ...
⨯ ./node_modules/dotenv/lib/main.js:2:1
Module not found: Can't resolve 'path'

https://nextjs.org/docs/messages/module-not-found

Import trace for requested module:
./node_modules/dotenv/config.js
./src/lib/db/index.ts
./src/app/api/chat/route.ts
./node_modules/next/dist/build/webpack/loaders/next-edge-app-route-loader/index.js?absolutePagePath=C%3A%5CUsers%5CDell%5COneDrive%5CDesktop%5C100xDevs%5Csummarize-my-pdf-ai%5Csrc%5Capp%5Capi%5Cchat%5Croute.ts&page=%2Fapi%2Fchat%2Froute&appDirLoader=bmV4dC1hcHAtbG9hZGVyP25hbWU9YXBwJTJGYXBpJT

JGY2hhdCUyRnJvdXRlJnBhZ2U9JTJGYXBpJTJGY2hhdCUyRnJvdXRlJmFwcFBhdGhzPSZwYWdlUGF0aD1wcml2YXRlLW5leHQtYXBwLWRpciUyRmFwaSUyRmNoYXQlMkZyb3V0ZS50cyZhcHBEaXI9QyUzQSU1Q1VzZXJzJTVDRGVsbCU1Q09uZURyaXZlJTVDRGVza3RvcCU1QzEwMHhEZXZzJTVDc3VtbWFyaXplLW15LXBkZi1haSU1Q3NyYyU1Q2FwcCZwYWdlRXh0ZW5zaW9ucz10c3gmcGFnZUV4dGVuc2lvbnM9dHMmcGFnZUV4dGVuc2lvbnM9anN4JnBhZ2VFeHRlbnNpb25zPWpzJnJvb3REaXI9QyUzQSU1Q1VzZXJzJTVDRGVsbCU1Q09uZURyaXZlJTVDRGVza3RvcCU1QzEwMHhEZXZzJTVDc3VtbWFyaXplLW15LXBkZi1haSZpc0Rldj10cnVlJnRzY29uZmlnUGF0aD10c2NvbmZpZy5qc29uJmJhc2VQYXRoPSZhc3NldFByZWZpeD0mbmV4dENvbmZpZ091dHB1dD0mcHJlZmVycmVkUmVnaW9uPSZtaWRkbGV3YXJlQ29uZmlnPWUzMCUzRCE%3D&nextConfigOutput=&preferredRegion=&middlewareConfig=e30%3D!


Working Code
When using the following simplified code, I am able to get a response from the chatbox:
import { openai } from ""@ai-sdk/openai"";
import { streamText } from ""ai"";
import { NextResponse } from ""next/server"";

export const runtime = ""edge"";

export async function POST(req: Request) {
  try {
    const { messages } = await req.json();
    const response = await streamText({
      model: openai(""gpt-4o-mini""),
      messages,
    });
    return response.toDataStreamResponse();
  } catch (error) {
    console.log(error);
    return NextResponse.json(
      { error: ""Internal Server Error"" },
      { status: 500 }
    );
  }
}


This should provide a clear context for the issue and indicate that the problem might be related to the configuration or dependencies rather than the core functionality of fetching chat responses.
Steps to Reproduce

Run the application with the provided route.ts and ChatComponent.tsx code.
Observe the error when attempting to start or compile the project.

I attempted to implement a chat functionality in my Next.js application using the streamText function from the ai package and the openai model. Specifically, I used the following code to handle POST requests and fetch chat responses:
import { openai } from ""@ai-sdk/openai"";
import { streamText } from ""ai"";
import { NextResponse } from ""next/server"";

export const runtime = ""edge"";

export async function POST(req: Request) {
  try {
    const { messages } = await req.json();
    const response = await streamText({
      model: openai(""gpt-4o-mini""),
      messages,
    });
    return response.toDataStreamResponse();
  } catch (error) {
    console.log(error);
    return NextResponse.json(
      { error: ""Internal Server Error"" },
      { status: 500 }
    );
  }
}

I expected this implementation to correctly process incoming messages and return a valid response from the chatbox.
What actually resulted?
Although the code runs without errors and returns a response in a simplified setup, I encountered issues with the more complex implementation, which includes additional logic such as database interactions and context handling. In those cases, I am receiving a 500 Internal Server Error and the response is not as expected. The simplified code works as intended and provides the expected chat responses.

","I removed this and it worked!!!!!
export const runtime = ""edge"";

",chatgpt
get all documents from chromadb using python and langchain,"I'm using langchain to process a whole bunch of documents which are in an Mongo database.
I can load all documents fine into the chromadb vector storage using langchain. Nothing fancy being done here. This is my code:

from langchain.embeddings.openai import OpenAIEmbeddings
embeddings = OpenAIEmbeddings()

from langchain.vectorstores import Chroma
db = Chroma.from_documents(docs, embeddings, persist_directory='db')
db.persist()


Now, after storing the data, I want to get a list of all the documents and embeddings WITH id's.
This is so I can store them back into MongoDb.
I also want to put them through Bertopic to get the topic categories.
Question 1 is: how do I get all documents I've just stored in the Chroma database? I want the documents, and all the metadata.
Many thanks for your help!
","Looking at the source code (https://github.com/hwchase17/langchain/blob/master/langchain/vectorstores/chroma.py)
You can just call below
db.get()

and you will get a json output with the id's, embeddings and docs data.
",langchain
prevent ncurses c library from creating subwindow,"I want to prevent the C++ ncurses library from creating its own sub-window when activated. Whenever you use initscr() with ncurses, it creates its own sub-window (text area in terminal). Not using initscr() and proceeding to use ncurses will lead to a segmentation fault. Does anyone know how to use ncurses without it creating its own sub-window?
In my case I am using ncurses to do a ""selection prompt"" where options are displayed, and the user can go though them with arrow keys until the find one they want (enter key twice to select).
#include <iostream>
#include <vector>
#include <string>
#include <ncurses.h>

std::string prompt(const std::vector<std::string> &options, const std::string &message)
{
    // * NCURSES init
    initscr();
    cbreak();
    keypad(stdscr, true);
    noecho();

    int choice;
    int highlight = 0;
    int num_options = options.size();

    while (true)
    {
        // * Clear line
        int y, x;
        getyx(stdscr, y, x);
        move(y, 0);
        clrtoeol();

        // * Display options
        printw(""%s: "", (message).c_str());
        for (int i = 0; i < num_options; ++i)
        {
            if (i == highlight)
                attron(A_REVERSE);

            printw(""%s "", options[i].c_str());
            attroff(A_REVERSE);
        }

        // * Get user input
        choice = getch();

        // * Decoding selection
        switch (choice)
        {
        case KEY_RIGHT:
            highlight = (highlight - 1 + num_options) % num_options;
            break;
        case KEY_LEFT:
            highlight = (highlight + 1) % num_options;
            break;
        case '\n':
            refresh();
            getch();
            endwin();
            printf(""\n"");
            return options[highlight];
        default:
            break;
        }
    }

    // * NCURSES cleanup
    refresh();
    getch();
    endwin();
}

int main()
{
    std::cout << ""Hello World!\n"";
    prompt({""agree"", ""disagree""}, ""choose one"");
    std::cout << ""Hello World #2\n"";

    return 0;
}

If you run this code, you will notice that in the ncurses sub window, only the stuff in the prompt function will appear. After selection, you will see the first message (""Hello World""), then a blank line where the prompt should be, then the second message (""Hello World #2"").
","Ncurses does not work without using it's own sub-window.
",prompt
how to change the look and feel in netbeans gui designer preview,"When using the NetBeans GUI Builder the ""Preview Design"" feature shows the panel with the system look and feel (e.g. Windows). Now I want to preview my panel with a different LaF to get all the gaps and spaces right. Is there a way to tell the gui builder to display the panel with a different LaF?
","The only thing I can find is: 
Inspector > Right click on your JFrame > Preview Design

",java
embed javascript as base64,"I'm working on a small GreaseMonkey script where I would like to embed a jQuery plugin (Markitup) so that the script is fully self contained (images + js) except for jQuery which is served from google.
I found the site http://www.greywyvern.com/code/php/binary2base64 which says that you can embed javascript with the href if you base64 encode the script, much like serving images as basse64 from CSS.
<script type=""text/javascript"" href=""data:text/javascript;base64,dmFyIHNjT2JqMSA9IG5ldyBzY3Jv...""></script>

So i tried that but couldn't get it to work at all, using Firefox 3.0.5 on OS X.
I put together a small test page to isolate the problem but couldn't get it to work at that page either.
<!DOCTYPE HTML>
<html>
   <head>
      <title>Title</title>
      <meta charset=""utf-8"">
   </head>
   <body>
      <script href=""data:text/javascript;base64,YWxlcnQoJ2FzYWRhc2QnKTsK""></script> 
      <script>alert('a');</script>  
   </body>
</html>

The base64 string (YWxlcnQoJ2FzYWRhc2QnKTsK) says alert('asadasd'); so I'm supposed to get two alerts but the only one I see is the second one where the alert is located as text inside the script tag.
Am I doing something wrong or why isn't this working, any ideas?
","maybe just a thought but maybe the ""href"" should be ""src"" instead.
",javascript
equivalent of alert and prompt in php,"In JavaScript, we have Alert() and Prompt() which open up a popup box for the user.
Is there an equivalent for PHP?
$Get_['asdf'] is one way to get user input... any others?
Also, one more question. Is it a requirement that PHP always be executed all at once? Or can it be like JavaScript, where it waits for the user input (e.g. popup box), then executes the rest of the code after that.
","PHP is a server side language, it can't do alert messages on the client side. But you can use javascript within the php to do the alert.
<script type=""text/javascript"">
window.alert(""Hi There, I am the Alert Box!"")
</script>

For Prompt you can do something like this -
<?php

    //prompt function
    function prompt($prompt_msg){
        echo(""<script type='text/javascript'> var answer = prompt('"".$prompt_msg.""'); </script>"");

        $answer = ""<script type='text/javascript'> document.write(answer); </script>"";
        return($answer);
    }

    //program
    $prompt_msg = ""Please type your name."";
    $name = prompt($prompt_msg);

    $output_msg = ""Hello there "".$name.""!"";
    echo($output_msg);

?>

",prompt
quotattributeerror 39index39 object has no attribute 39tz_localize39quot while using the function history to extract stock information,"I'm trying to extract information of the Tesla stock but, I always get the error while coding it. Here's the code so far:
import plotly.graph_objects as go
import yfinance as yf
import pandas as pd
import requests

from bs4 import BeautifulSoup
from plotly.subplots import make_subplots

def make_graph(stock_data, revenue_data, stock):

    fig = make_subplots(rows=2, cols=1, shared_xaxes=True, subplot_titles=(""Historical Share Price"", ""Historical Revenue""), vertical_spacing = .3)
    stock_data_specific = stock_data[stock_data.Date <= '2021--06-14']
    revenue_data_specific = revenue_data[revenue_data.Date <= '2021-04-30']
    fig.add_trace(go.Scatter(x=pd.to_datetime(stock_data_specific.Date, infer_datetime_format=True), y=stock_data_specific.Close.astype(""float""), name=""Share Price""), row=1, col=1)
    fig.add_trace(go.Scatter(x=pd.to_datetime(revenue_data_specific.Date, infer_datetime_format=True), y=revenue_data_specific.Revenue.astype(""float""), name=""Revenue""), row=2, col=1)
    fig.update_xaxes(title_text=""Date"", row=1, col=1)
    fig.update_xaxes(title_text=""Date"", row=2, col=1)
    fig.update_yaxes(title_text=""Price ($US)"", row=1, col=1)
    fig.update_yaxes(title_text=""Revenue ($US Millions)"", row=2, col=1)
    fig.update_layout(showlegend=False,
    height= 900,
    title= stock,
    xaxis_rangeslider_visible=True)
    fig.show()

Tesla= yf.Ticker('TSLA')
tesla_database= Tesla.history(period= ""max"")

In the tesla_database part is where the following error comes:
Traceback (most recent call last):
  File ""<pyshell#23>"", line 1, in <module>
    tesla_database= Tesla.history(period= ""max"")
  File ""/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/yfinance/base.py"", line 295, in history
    df.index = df.index.tz_localize(""UTC"").tz_convert(
AttributeError: 'Index' object has no attribute 'tz_localize'

How can I fix this? According to the IBM Watson Course in Coursera, this is the correct answer but I get this error all the time. Can someone correct it, please?
","I had the same issue with pandas 1.4.1 For me reverting back to a previous pandas version solved the problem:
Try this on command line:
pip install pandas==1.2.2

Try this from whithin Jupyter Board:
!pip install pandas==1.2.2

",data-science
modify cas overlay 713 to add custom restcontroller,"I am new to CAS codebase.
Would really appreciate some pointers?
Initialized cas overlay project from https://apereo.github.io/cas/development/installation/WAR-Overlay-Initializr.html
I am trying to add my own own class with annotation @SpringBootApplication
https://github.com/apereo/cas-overlay-template/tree/master/src/main so that it scans my packages for bean initialization and Controller class so that i can override functions of cas overlay.
https://github.com/apereo/cas-overlay-template/tree/master/src/main
I modified main entry class here https://github.com/apereo/cas-overlay-template/blob/master/gradle/springboot.gradle#L81
I can see this in the logs
2025-01-18 08:48:11,371 DEBUG [org.springframework.boot.devtools.restart.Restarter] - <Starting application com.test.AuthenticationApplication with URLs [file:/Users/Desktop/AE/ae_authentication-service/src/main/resources/, file:/Users/Desktop/AE/ae_authentication-service/build/classes/java/main/, file:/Users/Desktop/AE/ae_authentication-service/build/resources/main/]>
but why is it not scanning other packages.
Am i missing something
","The problem is that in CAS version 7, authors (AFAIK quietly) decided to disable Spring's automatic scanning/discovery of annotated components, probably for performance reasons. Instead, CAS relies on Spring's Component Indexer now. Even though it looks like all its components are still being created in Java via @Bean methods, so there should be no components scanning or indexing necessary for CAS own components anyway.
Source: CAS 7.0.0, how to scan self created beans, and manage by spring
So, the solution is either to create your component (whether it is RestController or anything else) as CAS does it, i.e. in your own Java Spring configuration class (which is documented at Extending CAS Configuration).
Or, try to put the name of your component class into the META-INF/spring.components file in your project, which you might want to automate by using the Spring indexer tools discussed in the SO page linked above. The current format of one line in that file seems to be something like org.example.YourComponentClass=org.springframework.stereotype.Component (value after the = sign might generally differ).
",java
c programming difference between i and ii1 from an assembler point of view,"This was an interview question. I said they were the same, but this was adjudged an incorrect response. From the assembler point of view, is there any imaginable difference? I have compiled two short C programs using default gcc optimization and -S to see the assembler output, and they are the same.
","The interviewer may have wanted an answer something like this:

i=i+1 will have to load the value of i, add one to it, and then store the result back to i.  In contrast, ++i may simply increment the value using a single assembly instruction, so in theory it could be more efficient.  However, most compilers will optimize away the difference, and the generated code will be exactly the same.

FWIW, the fact that you know how to look at assembly makes you a better programmer than 90% of the people I've had to interview over the years.  Take solace in the fact that you won't have to work with the clueless loser who interviewed you.
",c
how to load a quantized finetuned llama 38b model in vllm for faster inference,"I am working on deploying a quantized fine-tuned LLaMA 3-8B model and I aim to use vLLM  to achieve faster inference. I am currently using the following Python code to load the model:
import torch
import transformers
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig
import bitsandbytes as bnb
import accelerate

# model_id = ""meta-llama/Meta-Llama-3-8B"" #""mistralai/Mistral-7B-Instruct-v0.1""
model_id = ""meta-llama/Meta-Llama-3-8B-Instruct""
quantization_config = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_compute_dtype=torch.bfloat16)

base_model = AutoModelForCausalLM.from_pretrained(
    model_id,
    trust_remote_code=True,
    quantization_config=quantization_config, #load_in_8bit=True,#
    device_map='auto',
    token=MYTOKEN
)
peft_model = ""BojanaBas/Meta-Llama-3-8B-Instruct-pqa-10""
model = PeftModel.from_pretrained(base_model, peft_model)

The code successfully loads the model, but I am not sure how to integrate this with vLLM to optimize for faster inference. I read that it is not possible to load a model using PEFT in vLLM; instead, the PEFT model needs to be merged and loaded on Hugging Face.
I have merged and loaded the model on Hugging Face as described in the
article, after that, I am trying to use the model pushed to Hugging Face to load it on vLLM  with the following code:

from vllm import LLM

merged_peft_model_name=""lcass00/Meta-Llama-3-8B-Instruct-pqa-10-merged-peft""
model_id = ""meta-llama/Meta-Llama-3-8B-Instruct""

llm = LLM(model=merged_peft_model_name, tokenizer=model_id)

but when I try to load the model on vLLM I get the following error:
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
<ipython-input-14-c306a36d9c21> in <cell line: 3>()
      1 from vllm import LLM
      2 
----> 3 llm = LLM(model=merged_peft_model_name, tokenizer=model_id)

4 frames
/usr/local/lib/python3.10/dist-packages/vllm/entrypoints/llm.py in __init__(self, model, tokenizer, tokenizer_mode, skip_tokenizer_init, trust_remote_code, tensor_parallel_size, dtype, quantization, revision, tokenizer_revision, seed, gpu_memory_utilization, swap_space, enforce_eager, max_context_len_to_capture, max_seq_len_to_capture, disable_custom_all_reduce, **kwargs)
    142             **kwargs,
    143         )
--> 144         self.llm_engine = LLMEngine.from_engine_args(
    145             engine_args, usage_context=UsageContext.LLM_CLASS)
    146         self.request_counter = Counter()

/usr/local/lib/python3.10/dist-packages/vllm/engine/llm_engine.py in from_engine_args(cls, engine_args, usage_context)
    333         """"""Creates an LLM engine from the engine arguments.""""""
    334         # Create the engine configs.
--> 335         engine_config = engine_args.create_engine_config()
    336         distributed_executor_backend = (
    337             engine_config.parallel_config.distributed_executor_backend)

/usr/local/lib/python3.10/dist-packages/vllm/engine/arg_utils.py in create_engine_config(self)
    557     def create_engine_config(self, ) -> EngineConfig:
    558         device_config = DeviceConfig(self.device)
--> 559         model_config = ModelConfig(
    560             self.model, self.tokenizer, self.tokenizer_mode,
    561             self.trust_remote_code, self.dtype, self.seed, self.revision,

/usr/local/lib/python3.10/dist-packages/vllm/config.py in __init__(self, model, tokenizer, tokenizer_mode, trust_remote_code, dtype, seed, revision, code_revision, rope_scaling, tokenizer_revision, max_model_len, quantization, quantization_param_path, enforce_eager, max_context_len_to_capture, max_seq_len_to_capture, max_logprobs, disable_sliding_window, skip_tokenizer_init, served_model_name)
    141             self._verify_tokenizer_mode()
    142         self._verify_embedding_mode()
--> 143         self._verify_quantization()
    144         self._verify_cuda_graph()
    145 

/usr/local/lib/python3.10/dist-packages/vllm/config.py in _verify_quantization(self)
    201         if self.quantization is not None:
    202             if self.quantization not in supported_quantization:
--> 203                 raise ValueError(
    204                     f""Unknown quantization method: {self.quantization}. Must ""
    205                     f""be one of {supported_quantization}."")

ValueError: Unknown quantization method: bitsandbytes. Must be one of ['aqlm', 'awq', 'deepspeedfp', 'fp8', 'marlin', 'gptq_marlin_24', 'gptq_marlin', 'gptq', 'squeezellm', 'sparseml'].

How can I load a quantized finetuned model on vLLM?
","Unfortunately vLLM does not support bitsandbytes quantization technique yet. You may want to use Mixtral-8x7B-Instruct-v0.1-GPTQ tough, as GPTQ and  AWQ quantization techniques are already supported.
",llama
implementing gridsearchcv with scorer for leave one out crossvalidation,"I am attempting to implement scikit-learn's GridSearchCV for Gaussian Process Regression (GPR). I'm using a small dataset of ~200 points, and would like to use LOOCV as a performance evaluator for my model. My setup is:
from sklearn.model_selection import *
from sklearn.ensemble import *
from sklearn.gaussian_process import *

param_grid = {
    'kernel':[kernels.RBF(),kernels.Matern(length_scale=0.1)],
    'n_restarts_optimizer':[5,10,20,25],
    'random_state':[30]
}
res_GPR = GridSearchCV(estimator=GaussianProcessRegressor(),param_grid=param_grid,cv=LeaveOneOut(),verbose=20,n_jobs=-1)
res_GPR.fit(X,y)

where X and y are my data points and target values respectively.
I know that the scoring method returned by GPR is r^2, which is undefinable for the LOOCV case (since there is only one test element) - this is verified by obtaining NaN for the .best_score_ attribute of the fitted model.
As such, I would like the model to be scored with just the Root Mean Squared Error (RMSE) for each test case, averaged over all the iterations. How can I do that?
","GridSearchCV includes a scoring argument, which you may use to set your score to negative RMSE:
res_GPR = GridSearchCV(estimator=GaussianProcessRegressor(),
                       param_grid=param_grid,
                       cv=LeaveOneOut(),
                       verbose=20,
                       n_jobs=-1, 
                       scoring = 'neg_root_mean_squared_error')

See the documentation and the list of available scores for more.
",cross-validation
androidunable to connect to chromecast from app in development when in kiosk mode,"I am developing an app for android12.
The app is a kiosk mode and uses startLockTask.
To use the standard Android cast function from the app,
to call the cast settings screen.
startActivity(new Intent(ACTION_CAST_SETTINGS));

I can display the settings screen, and I can connect to FireTV,
I can connect to Chromecast, but I cannot connect to Chromecast.
The connection is still in progress.
I found that a confirmation message “start recording or casting?” is displayed at the time of connection, but it was not displayed on my app.
It does appear when I execute stopLockTask just before the connection,
Is there any way to make it appear while using startLockTask?
","This is actually a known limitation with Android's kiosk mode(Lock Task) mode and system dialogs. However you can handle this by adding cast permission to allowed activities.
You can whitelist specific system UI components in your Device owner settings. You need to use DevicePolicyManager to set this up.
DevicePolicyManager dpm = (DevicePolicyManager) getSystemService(Context.DEVICE_POLICY_SERVICE);
String[] packages = {
    ""com.google.android.gms"",

*// Google Play Services*

""com.google.android.apps.chromecast.app""

*// Cast related*

};
dpm.setLockTaskPackages(admin, packages);

Please check and update if this works for you.
",java
make python3 as my default python on mac,"What I'm trying to do here is to make python3 as my default python. Except the python 2.7 which automatically installed on mac, I installed python3 with homebrew. This is the website that I'm following. http://docs.python-guide.org/en/latest/starting/install3/osx/#install3-osx
I guess I followed every instruction well, got xcode freshly installed, Command line tools, and homebrew. But here's my little confusion occurs.

The script will explain what changes it will make and prompt you before the installation begins. Once you’ve installed Homebrew, insert the Homebrew directory at the top of your PATH environment variable. You can do this by adding the following line at the bottom of your ~/.profile file
export PATH=/usr/local/bin:/usr/local/sbin:$PATH

I was really confused what this was, but I concluded that I should just add this following line at the bottom of ~/.profile file. So I opened the ~/.profile file by open .profile in the terminal, and added following line at the bottom. And now it looks like this.
export PATH=/usr/local/bin:/usr/local/sbin:$PATH
# Setting PATH for Python 3.6
# The original version is saved in .profile.pysave
export PATH=/usr/local/bin:/usr/local/sbin:$PATH

And then I did brew install python, and was hoping to see python3 when I do python --version.
But it just shows me python 2.7.10. I want my default python to be python3 not 2.7
And I found a little clue from the website.

Do I have a Python 3 installed?

$ python --version
Python 3.6.4


If you still see 2.7 ensure in PATH /usr/local/bin/ takes pecedence over /usr/bin/

Maybe it has to do something with PATH? Could someone explain in simple English what PATH exactly is and how I could make my default python to be python3 when I run python --version in the terminal?
","Probably the safest and easy way is to use brew and then just modify your PATH:
First update brew:
brew update

Next install python:
brew install python

That will install and symlink python3 to python, for more details do:
brew info python

Look for the Caveats:
==> Caveats
Python has been installed as
  /usr/local/bin/python3

Unversioned symlinks `python`, `python-config`, `pip` etc. pointing to
`python3`, `python3-config`, `pip3` etc., respectively, have been installed into
  /usr/local/opt/python/libexec/bin

Then add to your path /usr/local/opt/python/libexec/bin:
export PATH=/usr/local/opt/python/libexec/bin:$PATH

The order of the PATH is important, by putting first the /usr/local/opt/python/libexec/bin will help to give preference to the brew install (python3) than the one is in your system located in /usr/bin/python
",python
why is the size of the data type different when the value is directly passed to the sizeof operator,"#include <stdio.h>
int main() {
    char a = 'A';
    int b = 90000;
    float c = 6.5;
    printf(""%d "",sizeof(6.5));
    printf(""%d "",sizeof(90000));
    printf(""%d "",sizeof('A'));
    printf(""%d "",sizeof(c));
    printf(""%d "",sizeof(b));
    printf(""%d"",sizeof(a));
    return 0;
}

The output is:
8 4 4 4 4 1

Why is the output different for the same values?
","Character constants in C (opposite to C++) have the type int. So this call 
printf(""%d"",sizeof('A'));

outputs 4. That is sizeof( 'A' ) is equal to sizeof( int ).
From the C Standard (6.4.4.4 Character constants)

10 An integer character constant has type int....

On the other hand (6.5.3.4  The sizeof and alignof operators)

4 When sizeof is applied to an operand that has type char, unsigned
  char, or signed char, (or a qualiﬁed version thereof) the result is 1.

So the operand of the sizeof operator in this expression sizeof( 'A' ) has the type int while in this expression sizeof( a ) where a is declared like
char a = 'A';

the operand has the type char.
Pay attention to that calls like this
printf(""%d"",sizeof(6.5));

use incorrect conversion format specifier. You have to write
printf(""%zu"",sizeof(6.5));

Also in the above call there is used a constant of the type double while in this call
printf(""%zu"",sizeof(c));

the variable c has the type float.
You could get the same result for these calls if the first call used a constant of the type float like
printf(""%zu"",sizeof(6.5f));

",c
valueerror percolumn arrays must each be 1dimensional when trying to create a pandas dataframe from a dictionary why,"I'm trying to create a very simple Pandas DataFrame from a dictionary. The dictionary has 3 items, and the DataFrame as well. They are:

a list with the 'shape' (3,)
a list/np.array (in different attempts) with the shape(3, 3)
a constant of 100 (same value to the whole column)


Here is the code that succeeds and displays the preferred df

​
# from a dicitionary
>>>dict1 = {""x"": [1, 2, 3],
...         ""y"": list(
...             [
...                 [2, 4, 6], 
...                 [3, 6, 9], 
...                 [4, 8, 12]
...             ]
...             ),
...         ""z"": 100}

>>>df1 = pd.DataFrame(dict1)
>>>df1
   x           y    z
0  1   [2, 4, 6]  100
1  2   [3, 6, 9]  100
2  3  [4, 8, 12]  100


But then I assign a Numpy ndarray (shape 3, 3 )to the key y, and try to create a DataFrame from the dictionary. The line I try to create the DataFrame errors out. Below is the code I try to run, and the error I get (in separate code blocks for ease of reading.)


code

​
>>>dict2 = {""x"": [1, 2, 3],
...         ""y"": np.array(
...             [
...                 [2, 4, 6], 
...                 [3, 6, 9], 
...                 [4, 8, 12]
...             ]
...             ),
...         ""z"": 100}

>>>df2 = pd.DataFrame(dict2)  # see the below block for error


error

​
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
d:\studies\compsci\pyscripts\study\pandas-realpython\data-delightful\01.intro.ipynb Cell 10' in <module>
      1 # from a dicitionary
      2 dict1 = {""x"": [1, 2, 3],
      3          ""y"": np.array(
      4              [
   (...)
      9              ),
     10          ""z"": 100}
---> 12 df1 = pd.DataFrame(dict1)

File ~\anaconda3\envs\dst\lib\site-packages\pandas\core\frame.py:636, in DataFrame.__init__(self, data, index, columns, dtype, copy)
    630     mgr = self._init_mgr(
    631         data, axes={""index"": index, ""columns"": columns}, dtype=dtype, copy=copy
    632     )
    634 elif isinstance(data, dict):
    635     # GH#38939 de facto copy defaults to False only in non-dict cases
--> 636     mgr = dict_to_mgr(data, index, columns, dtype=dtype, copy=copy, typ=manager)
    637 elif isinstance(data, ma.MaskedArray):
    638     import numpy.ma.mrecords as mrecords

File ~\anaconda3\envs\dst\lib\site-packages\pandas\core\internals\construction.py:502, in dict_to_mgr(data, index, columns, dtype, typ, copy)
    494     arrays = [
    495         x
    496         if not hasattr(x, ""dtype"") or not isinstance(x.dtype, ExtensionDtype)
    497         else x.copy()
    498         for x in arrays
    499     ]
    500     # TODO: can we get rid of the dt64tz special case above?
--> 502 return arrays_to_mgr(arrays, columns, index, dtype=dtype, typ=typ, consolidate=copy)

File ~\anaconda3\envs\dst\lib\site-packages\pandas\core\internals\construction.py:120, in arrays_to_mgr(arrays, columns, index, dtype, verify_integrity, typ, consolidate)
    117 if verify_integrity:
    118     # figure out the index, if necessary
    119     if index is None:
--> 120         index = _extract_index(arrays)
    121     else:
    122         index = ensure_index(index)

File ~\anaconda3\envs\dst\lib\site-packages\pandas\core\internals\construction.py:661, in _extract_index(data)
    659         raw_lengths.append(len(val))
    660     elif isinstance(val, np.ndarray) and val.ndim > 1:
--> 661         raise ValueError(""Per-column arrays must each be 1-dimensional"")
    663 if not indexes and not raw_lengths:
    664     raise ValueError(""If using all scalar values, you must pass an index"")

ValueError: Per-column arrays must each be 1-dimensional

Why is it ending in error like that in the second attempt, even though the dimensions of both arrays are the same? What is a workaround for this issue?
","If you look closer at the error message and quick look at the source code here:
    elif isinstance(val, np.ndarray) and val.ndim > 1:
        raise ValueError(""Per-column arrays must each be 1-dimensional"")

You will find that if the dictionay value is a numpy array and has more than one dimension as your example, it throws an error based on the source code. Therefore, it works very well with list because a list has no more than one dimension even if it is a list of list.
lst = [[1,2,3],[4,5,6],[7,8,9]]
len(lst) # print 3 elements or (3,) not (3,3) like numpy array.

You can try to use np.array([1,2,3]), it will work because number of dimensions is 1 and try:
arr = np.array([1,2,3])
print(arr.ndim)  # output is 1

If it is necessary to use numpy array inside a dictionary, you can use .tolist() to convert numpy array to a list.
",numpy
are locale specific whitespaces skipped with strtod compliant,"Is my C library compliant?
In testing strtod(), my code reported an interesting inconsistency:
strtod(""\240"" ""123"", ...)  --> 0.0.
In locales that identified character 160 '\240' as a white-space, strtod() did not skip character 160 as a leading white-space, yet strtol() did.
I suspect my library has a corner bug as I expected strtod() to follow the current locale's isspace().
Is it a bug, allowed behavior or a fixed bug?

Sample code:
#include <ctype.h>
#include <errno.h>
#include <limits.h>
#include <locale.h>
#include <stdio.h>
#include <stdlib.h>

void test_locale_name(const char *locale_name) {
  const char *current_locale = setlocale(LC_ALL, locale_name);
  if (current_locale) {
    printf(""Current locale name \""%s\""\n"", current_locale);
  }
  puts(""White spaces in this locale\n"");
  for (int i = UCHAR_MAX; i > 0; i--) {
    if (isspace(i) && i != '\n' && i != '\r') {
      char buf[100];
      snprintf(buf, sizeof buf, ""%c123"", i);
      char *endptr;
      errno = 0;
      long val = strtol(buf, &endptr, 0);
      printf(""Character code %3d:   strtol(\""%s\"") converts to %3ld, length = %d\n"", i,
          buf, val, (int) (endptr - buf));
      errno = 0;
      double value_d = strtod(buf, &endptr);
      printf(""Character code %3d:   strtod(\""%s\"") converts to %3g, length = %d\n"", i,
          buf, value_d, (int) (endptr - buf));
    }
  }
  puts(""\n"");
}

int main(void) {
  /*
   *  A null pointer for locale causes the setlocale function to return a pointer
   *  to the string associated with the category for the program’s current locale;
   *  the program’s locale is not changed.
   */
  const char *locale_names[] = { NULL, ""POSIX"", ""af_ZA"" };
  size_t locale_names_n = sizeof locale_names / sizeof locale_names[0];
  for (size_t i = 0; i < locale_names_n; i++) {
    test_locale_name(locale_names[i]);
  }
  return 0;
}

Sample output
Current locale name ""C""
White spaces in this locale

Character code  32:   strtol("" 123"") converts to 123, length = 4
Character code  32:   strtod("" 123"") converts to 123, length = 4
Character code  12:   strtol(""123"") converts to 123, length = 4
Character code  12:   strtod(""123"") converts to 123, length = 4
Character code  11:   strtol(""123"") converts to 123, length = 4
Character code  11:   strtod(""123"") converts to 123, length = 4
Character code   9:   strtol(""  123"") converts to 123, length = 4
Character code   9:   strtod(""  123"") converts to 123, length = 4


Current locale name ""C""
White spaces in this locale

Character code  32:   strtol("" 123"") converts to 123, length = 4
Character code  32:   strtod("" 123"") converts to 123, length = 4
Character code  12:   strtol(""123"") converts to 123, length = 4
Character code  12:   strtod(""123"") converts to 123, length = 4
Character code  11:   strtol(""123"") converts to 123, length = 4
Character code  11:   strtod(""123"") converts to 123, length = 4
Character code   9:   strtol(""  123"") converts to 123, length = 4
Character code   9:   strtod(""  123"") converts to 123, length = 4


Current locale name ""af_ZA""
White spaces in this locale

Character code 160:   strtol(""�123"") converts to 123, length = 4
Character code 160:   strtod(""�123"") converts to   0, length = 0  ***!!!!***
Character code  32:   strtol("" 123"") converts to 123, length = 4
Character code  32:   strtod("" 123"") converts to 123, length = 4
Character code  12:   strtol(""123"") converts to 123, length = 4
Character code  12:   strtod(""123"") converts to 123, length = 4
Character code  11:   strtol(""123"") converts to 123, length = 4
Character code  11:   strtod(""123"") converts to 123, length = 4
Character code   9:   strtol(""  123"") converts to 123, length = 4
Character code   9:   strtod(""  123"") converts to 123, length = 4



C23 spec has:

In this clause, ""white-space character"" refers to (execution) white-space character as defined by isspace.  C23dr § 7.1.1 5


7.24.1.5 The strtod, strtof, and strtold functions
First, they decompose the input string into three parts: an initial, possibly empty, sequence of white-space characters, a subject sequence resembling a floating constant
...
§ 7.24.1.5 2


In other than the ""C"" locale, additional locale-specific subject sequence forms may be accepted.
§ 7.24.1.5 7

and similar specs for strtol()

7.24.1.7 The strtol, strtoll, strtoul, and strtoull functions
First, they decompose the input string into three parts: an initial, possibly empty, sequence of white-space characters, a subject sequence resembling an integer ...
C23dr § 7.24.1.7 6


In other than the ""C"" locale, additional locale-specific subject sequence forms may be accepted.
C23dr § 7.24.1.7 3

Compiler output includes:
Invoking: Cygwin C Compiler
gcc -O0 -g3 -pedantic -Wall -Wextra -Wconversion -Wsign-conversion -c -std=c17  -fmessage-length=0 -Wformat -Wformat-security -Wformat=2 -Wmaybe-uninitialized -Werror=stringop-truncation -Wcast-align=strict -v -MMD -MP -MF""strtod7.d"" -MT""strtod7.o"" -o ""strtod7.o"" ""../strtod7.c""
Using built-in specs.
COLLECT_GCC=gcc
Target: x86_64-pc-cygwin
Configured with: /mnt/share/cygpkgs/gcc/gcc.x86_64/src/gcc-12.4.0/configure --srcdir=/mnt/share/cygpkgs/gcc/gcc.x86_64/src/gcc-12.4.0 --prefix=/usr --exec-prefix=/usr --localstatedir=/var --sysconfdir=/etc --docdir=/usr/share/doc/gcc --htmldir=/usr/share/doc/gcc/html -C --build=x86_64-pc-cygwin --host=x86_64-pc-cygwin --target=x86_64-pc-cygwin --without-libiconv-prefix --without-libintl-prefix --libexecdir=/usr/lib --with-gcc-major-version-only --enable-shared --enable-shared-libgcc --enable-static --enable-version-specific-runtime-libs --enable-bootstrap --enable-__cxa_atexit --enable-clocale=newlib --with-dwarf2 --with-tune=generic --enable-languages=ada,c,c++,fortran,lto,objc,obj-c++,jit --enable-graphite --enable-threads=posix --enable-libatomic --enable-libgomp --enable-libquadmath --enable-libquadmath-support --disable-libssp --enable-libada --disable-symvers --disable-multilib --with-gnu-ld --with-gnu-as --with-cloog-include=/usr/include/cloog-isl --without-libiconv-prefix --without-libintl-prefix --with-system-zlib --enable-linker-build-id --with-default-libstdcxx-abi=gcc4-compatible --enable-libstdcxx-filesystem-ts
Thread model: posix
Supported LTO compression algorithms: zlib zstd
gcc version 12.4.0 (GCC) 
COLLECT_GCC_OPTIONS='-O0' '-g3' '-Wpedantic' '-Wall' '-Wextra' '-Wconversion' '-Wsign-conversion' '-c' '-std=c17' '-fmessage-length=0' '-Wformat=1' '-Wformat-security' '-Wformat=2' '-Wmaybe-uninitialized' '-Werror=stringop-truncation' '-Wcast-align=strict' '-v' '-MMD' '-MP' '-MF' 'strtod7.d' '-MT' 'strtod7.o' '-o' 'strtod7.o' '-mtune=generic' '-march=x86-64'
 /usr/lib/gcc/x86_64-pc-cygwin/12/cc1.exe -quiet -v -MMD strtod7.d -MF strtod7.d -MP -MT strtod7.o -dD -idirafter /usr/lib/gcc/x86_64-pc-cygwin/12/../../../../lib/../include/w32api -idirafter /usr/lib/gcc/x86_64-pc-cygwin/12/../../../../x86_64-pc-cygwin/lib/../lib/../../include/w32api ../strtod7.c -quiet -dumpbase strtod7.c -dumpbase-ext .c -mtune=generic -march=x86-64 -g3 -O0 -Wpedantic -Wall -Wextra -Wconversion -Wsign-conversion -Wformat=1 -Wformat-security -Wformat=2 -Wmaybe-uninitialized -Werror=stringop-truncation -Wcast-align=strict -std=c17 -version -fmessage-length=0 -o /cygdrive/c/Users/TPC/AppData/Local/Temp/cch4QUWQ.s
GNU C17 (GCC) version 12.4.0 (x86_64-pc-cygwin)
    compiled by GNU C version 12.4.0, GMP version 6.3.0, MPFR version 4.2.1, MPC version 1.3.1, isl version isl-0.27-GMP

Although this C lib is not the latest gcc, newer   (14.2?), perhaps less stable ones did not list a related fix that I could find.
","I would consider this to be a bug: white-space is well defined in the C Standard for these functions and must be locale specific and consistent with the behavior of the isspace() macro.
The implementation of strtod in the newlib, originally written by David M. Gay at AT&T has this (somewhat cryptic) code to parse the initial portion of the argument string:
    for(s = s00;;s++) switch(*s) {
        case '-':
            sign = 1;
            /* no break */
        case '+':
            if (*++s)
                goto break2;
            /* no break */
        case 0:
            goto ret0;
        case '\t':
        case '\n':
        case '\v':
        case '\f':
        case '\r':
        case ' ':
            continue;
        default:
            goto break2;
        }
 break2:

ASCII white space characters are tested explicitly and the locale information is only used to recognize the decimal separator.
Conversely, strtol redirects to the much older _strtol_l implementation from the BSD code that has a naive loop to skip the Standard conforming locale dependent optional white space:
    /*
     * Skip white space and pick up leading +/- sign if any.
     * If base is 0, allow 0x for hex and 0 for octal, else
     * assume decimal; if base is already 16, allow 0x.
     */
    do {
        c = *s++;
    } while (isspace_l(c, loc));
    if (c == '-') {
        neg = 1;
        c = *s++;
    } else if (c == '+')
        c = *s++;
    if ((base == 0 || base == 16) &&
        c == '0' && (*s == 'x' || *s == 'X')) {
        c = s[1];
        s += 2;
        base = 16;
    }
    if (base == 0)
        base = c == '0' ? 8 : 10;

Note however that this implementation is incorrect too as the 0x prefix is skipped unconditionally, which is non conforming if not followed by a hex digit.
You should file at least one bug report with the newlib support team.
",c
linux kernel generate compilecommandsjson for module,"The problem: Most of macro definition and even header files are not looked up by an IDE because include path is not specified in the IDE configuration. It inhibits autocompletion and navigation.
Here is my Makefile:
#-Wno-declaration-after-statement
ccflags-y := -std=gnu11 -Wno-declaration-after-statement -Werror
obj-m += pfsw.o
pfsw-objs := src/init.o

all:
    make -C /lib/modules/$(shell uname -r)/build M=$(PWD) modules

clean:
    make -C /lib/modules/$(shell uname -r)/build M=$(PWD) clean

I ran make V=1 and noticed that the compile command along with include path is actually pretty cumbersome (counting Linux Specific -include for parameters):
gcc -Wp,-MD,/home/memyself/lkm/procfs_write/src/.init.o.d -nostdinc \
   -isystem /usr/lib/gcc/x86_64-linux-gnu/7/include \
   -I./arch/x86/include -I./arch/x86/include/generated \
   -I./include -I./arch/x86/include/uapi \
   -I./arch/x86/include/generated/uapi -I./include/uapi \
   -I./include/generated/uapi \
   -include ./include/linux/kconfig.h -Iubuntu/include \
   -include ./include/linux/compiler_types.h \
   -D__KERNEL__ \
   ...tons of options ommitted...
   -c -o /home/memyself/lkm/procfs_write/src/init.o \
   /home/memyself/lkm/procfs_write/src/init.c

Question: Is there a way to generate compile-command.json to inform IDE about include paths? Or the only solution is to manually pass the include path to the IDE one by one?
","Due to CLang has a lot of different tools, including some to analyze the code, the compile-command.json is required. That's why Tom Roeder from Google provided a
scripts/clang-tools/gen_compile_commands.py in the commit b30204640192 (""scripts: add a tool to produce a compile_commands.json file"") for this.
Note, kernel must be compiled at least once to make this script work.
P.S. I suppose you are trying MS Visual Studio Code?
Kudos to colleague of mine, Alex, who is user of it and told me about existence of such script.
",c
how to find out all palindromic numbers,"A palindromic number or numeral palindrome is a ""symmetrical"" number like 16461, that remains the same when its digits are reversed.
The term palindromic is derived from palindrome, which refers to a word like rotor that remains unchanged under reversal of its letters.
The first palindromic numbers (in decimal) are:
0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 11, 22,
33, 44, 55, 66, 77, 88, 99, 101, 111,
121, 131, 141, 151, 161, 171, 181,
191, ...

How to find out all palindromic numbers below, say, 10000?
","Generating all palindromes up to a specific limit.
public static Set<Integer> allPalindromic(int limit) {

    Set<Integer> result = new HashSet<Integer>();

    for (int i = 0; i <= 9 && i <= limit; i++)
        result.add(i);

    boolean cont = true;
    for (int i = 1; cont; i++) {
        StringBuffer rev = new StringBuffer("""" + i).reverse();
        cont = false;
        for (String d : "",0,1,2,3,4,5,6,7,8,9"".split("","")) {
            int n = Integer.parseInt("""" + i + d + rev);
            if (n <= limit) {
                cont = true;
                result.add(n);
            }
        }
    }

    return result;
}


Testing for palindromicity
Using Strings
public static boolean isPalindromic(String s, int i, int j) {
    return j - i < 1 || s.charAt(i) == s.charAt(j) && isPalindromic(s,i+1,j-1);
}

public static boolean isPalindromic(int i) {
    String s = """" + i;
    return isPalindromic(s, 0, s.length() - 1);
}

Using integers
public static boolean isPalindromic(int i) {
    int len = (int) Math.ceil(Math.log10(i+1));
    for (int n = 0; n < len / 2; n++)
        if ((i / (int) Math.pow(10, n)) % 10 !=
            (i / (int) Math.pow(10, len - n - 1)) % 10)
            return false;
    return true;
}

",java
error generated while creating a nextjs project,"Good morning, I am a student in software programming, and this is my last session. We are taking a course on web programming with Next.js. During the installation, I encountered errors, which are shown in the screenshots. The errors are related to missing dependencies and several other issues. The error messages are too long.
This are my versions:
PS C:...\Desktop\Seesion 4\WebServer\nextjs>
PS C:...\Desktop\Seesion 4\WebServer\nextjs> npm -v
11.0.0
PS C:...\Desktop\Seesion 4\WebServer\nextjs> nvm -v
1.2.2
PS C:...\Desktop\Seesion 4\WebServer\nextjs> node -v
v22.13.0
PS C:...Desktop\Seesion 4\WebServer\nextjs> nvm list
22.13.0 (Currently using 64-bit executable)
PS C:\Users\abots\Desktop\Seesion 4\WebServer\nextjs>
I encountered these warnings and errors during installation:
(node:5052) [DEP0040] DeprecationWarning: The punycode module is deprecated. Please use a userland alternative instead.
(Use node --trace-deprecation ... to show where the warning was created)
npm WARN peerDependencies The peer dependency typescript@>=3.3.1 included from eslint-config-next will no
npm WARN peerDependencies longer be automatically installed to fulfill the peerDependency
npm WARN peerDependencies in npm 3+. Your application will need to depend on it explicitly.
npm WARN peerDependencies The peer dependency eslint-plugin-import-x@* included from eslint-import-resolver-typescript will no
npm WARN peerDependencies longer be automatically installed to fulfill the peerDependency
npm WARN peerDependencies in npm 3+. Your application will need to depend on it explicitly.
npm WARN peerDependencies The peer dependency typescript@>=4.8.4 <5.8.0 included from @typescript-eslint/parser will no
npm WARN peerDependencies longer be automatically installed to fulfill the peerDependency
npm WARN peerDependencies in npm 3+. Your application will need to depend on it explicitly.
npm WARN peerDependencies The peer dependency ts-node@>=9.0.0 included from postcss-load-config will no
npm WARN peerDependencies longer be automatically installed to fulfill the peerDependency
npm WARN peerDependencies in npm 3+. Your application will need to depend on it explicitly.
Please what is the issue?
I tried everything possible, i changed versions, reinstall everything, i spent all my night on it.
","I see 3 issues

make sure next.js is compatible with Node.js versions like 18.x or 20.x
Install Peer Dependencies Manually npm install typescript eslint-plugin-import eslint-plugin-react eslint-plugin-jsx-a11y ts-node --save-dev 
There is Deprecated Punycode Module so install npm install --legacy-peer-deps 

",javascript
is there any way to work with netcdf files in nim,"I've discovered Nim some days ago and am really interested in it.
However, I need to know if I'd able to work with NetCDF files (or at least GRB files) to see if it is suitable for my every-day life.
I've searched the web for some library or code, but couldn't find any, so if anybody knows a way to do it, please let me know.
","Possible options (as of September 2020) to work with netcdf files using nim:

since netcdf4 is based on hdf5, maybe nimhdf5 could be useful
using nimpy bridge to access python libraries that access netcdf, such as netcdf4-python (your local python environment will be a dependency)
wrap netcdf C api using tools like c2nim or nimterop

",data-science
matplotlib 3d line plot,"I am trying to plot a two series of data in 3d as lines but cant find a good example of how to structure the data to do this in python.
My example data is:
x = np.array([[1,1,1,1,5,1,1,1,1], [4.5,5,5,5,5.5,5,5,5,4.5]])

But I presume to do this I need 3 series of data.
Here is a sample of what the output should look like

I am assuming I need to add extra rows to the array, but not sure if I should try to build a 3d array or plot each axis with its own separate arrays?
In which case axis
y1 = np.ones(9)
z = np.array([0,1,2,3,4,5,6,7,8])

I did have a look here and read the documentation here but still could not work out how to apply it to what I am trying to do.
My attempt:
import matplotlib.pylab as pl
from mpl_toolkits.mplot3d import Axes3D
import numpy as np
pl.figure()

ax = pl.subplot(projection='3d')
data = np.array([[1,1,1,1,5,1,1,1,1], [4,5,5,5,5,5,5,5,4]])
y1 = np.ones(9)
z = np.array([1,2,3,4,5,6,7,8,9])

ax.plot(x, y, z, color = 'r')

UPDATE
This code displays the pseudo data, but ideally x would only have two axes - does it have to be axes of matching size? x just wants to have two ticks at 1, and another at 2.
import matplotlib.pyplot as pl
from mpl_toolkits.mplot3d import Axes3D
import numpy as np
pl.figure()

ax = pl.subplot(projection='3d')
z = np.array([[1,1,1,1,5,1,1,1,1], [1,4.5,5,5,5,5,5,5,4]])
x = np.ones(9)
y1 = np.array([1,2,3,4,5,6,7,8,9])
ax.plot(x, y1, z[0], color = 'r')
ax.plot(x*2, y1, z[1], color = 'g')

ax.set_xlabel('x')
ax.set_box_aspect(aspect = (1,1,2))


","Try this:
import matplotlib.pyplot as pl
from mpl_toolkits.mplot3d import Axes3D
import numpy as np
pl.figure(figsize = (12,12))

ax = pl.subplot(projection='3d')
z = np.array([[1,1,1,1,5,1,1,1,1], [0.5,4.5,5,4.8,5.1,5,5,5,4]])
x = np.ones(9)
y1 = np.array([1,2,3,4,5,6,7,8,9])
ax.plot(x, y1, z[0], color = 'r')
ax.plot(x*2, y1, z[1], color = 'g')

ax.set_xlabel('x \n freq')
ax.set_ylabel('y \n time (s)')
ax.set_zlabel('z\n magnitude')

ax.set_box_aspect(aspect = (0.5,1,1))
ticks = np.array([0,1,2])
ax.set_xticks(ticks) 


",matplotlib
sampling without replacement with unequal weights,"I am trying to find an algorithm that selects 12 elements from a list of 25 according to the probability of each one of them. These elements cannot be repeated.
I've tried using the function without replacement, but I can't get it to keep the weights when I do 1000 iterations to check that it was done correctly. I have only succeeded if I use the function with replace = True.
Since that function returns duplicate items to me, then I clean the list removing the duplicates. By doing this, the probability that I get each number is not the one that I had defined at the beginning. I am aware that the error is here and that I should be using the function with replace = False, but I can't get it to give me the result.
import numpy as np
from collections import Counter 

nameList = ['AAAAAA', 'B', 'C', 'D','E','F','G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P','Q','R','S','T','U','V','W','X','ZZZZZZZZZZZZZZZZ']
probability_nameList = [0.10, 0.01, 0.02, 0.03, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04]

sampleNames =[]
sampleNamesControl =[]
cleaned_result = []
test_list = []

for i in range(1000):
    sampleNames += np.random.choice(nameList, 100,replace=True, p=probability_nameList)
    for item in sampleNames:    
        if not item in cleaned_result:
            cleaned_result += [item] 
    sampleNames =[]
    test_list += cleaned_result[0:12]
    cleaned_result = []

print(Counter(test_list))


Response:
Counter({'AAAAAA': 832, 'F': 522, 'X': 513, 'ZZZZZZZZZZZZZZZZ': 506, 'I': 505, 'L': 504, 'N': 501, 'S': 499, 'T': 498, 'U': 496, 'R': 492, 'O': 491, 'J': 489, 'P': 488, 'E': 487, 'V': 485, 'K': 482, 'Q': 479, 'H': 473, 'G': 471, 'M': 468, 'W': 461, 'D': 404, 'C': 297, 'B': 157})

As you can see, AAAAA had to be 10 times bigger than B, and that is not happening.
P(AAAAA)= 10xP(B)
","This is not a programming issue but fundamental maths: what you want is not possible.
You need to take into account conditional probabilities.
I'll give you a small example. Let's take 4 letters A,B,C,D and pick 2 of them, without replacement, with A having a 97% probability of being picked and the others 1%.
Given A's high probability, it will be picked most of the time. But once we have picked it, as we are without replacement, there is no more A and a 1/3 chance to pick one of the other 3 values.
Indeed:
from itertools import chain
from collections import Counter

Counter(chain.from_iterable(np.random.choice(list('ABCD'),
                                             2, replace=False,
                                             p=[.97, .01, .01, .01])
                            for i in range(10000)))

Output:
Counter({'A': 9995, 'C': 3358, 'B': 3317, 'D': 3330})

A is picked almost all the time and the others are picked about one third of the time.
NB. The end frequency of B,C,D is roughly 1/3 because the probability of A is close to 1, if p(A) was smaller, the calculation of the final frequencies would be a bit more complex. The final frequency of B (or C or D) is 0.97*1/3+0.01*(0.01/0.99)*2+0.01 so a bit above 1/3, and that of A is 0.97+0.01*(0.97/0.99)*3 so ~0.9994
",numpy
alternative to receptive field in transformers and what factors impact it,"I have two transformer networks. One with 3 heads per attention and 15 layers in total and second one with 5 heads per layer and 30 layers in total. Given an arbitrary set of documents (2048 tokens per each), how to find out, which network is going to be better to use and is less prone to overfitting?
In computer vision we have concept called: ""receptive field"", that allows us to understand how big or small network we need to use. For instance, if we have CNN with 120 layers and CNN with 70 layers, we can calculate their receptive fields and understand which one is going to perform better on a particular dataset of images.
Do you guys have something similar in NLP? How do you understand whether one architecture is more optimal to use versus another，having a set of text documents with unique properties?
","
How do you understand whether one architecture is more optimal to use versus another, having a set of text documents with unique properties?

For modern Transformer-based Language Models (LMs), there are some empirical ""scaling laws,"" such as the Chinchilla scaling laws (Wikipedia), that essentially say that larger (deeper) models with more layers, i.e., with more parameters tend to perform better. So far, most LMs seem to roughly follow Chinchilla scaling. There is another kind of scaling, which is closer to a ""receptive field"", that I talk about below.

Do you guys have something similar in NLP?

Kind of. Transformer-based LMs can be thought to have a ""receptive field"" similar to CNN layers, as the attention mechanism in the Transformer operates on a pre-defined ""context window"" or ""context length"", which is the maximum number of tokens the layer can look at (""attend to"") at any given time, similar to a CNN kernel. However, with the introduction of new positional encoding (PE) approaches, such as Rotary Positional Encoding (RoPE), and modified attention architectures, like Sliding Window Attention (SWA), this is not strictly accurate.
Scaling in terms of ""context length"" is of much interest, but usually, it is very difficult to scale Transformers this way, because of attention being a ($\mathcal{O}(N^2)$) (O(N^2)) operation. So, usually, researchers go towards deeper architectures with more parameters (""over-parameterization"") that can allow the model to ""memorize"" as much of the large training corpus as it can (""overfitting""), so that it can perform reasonably well, when fine-tuned for most down-stream tasks (that have at least some representative examples in the training corpus).
",huggingface-transformers
how to fix attributeerror 39series39 object has no attribute 39append39,"Hi I'm still learning pandas and numpy in python
I learn from e-learning that you could combine 2 series with append but when I tried it It give me error
students_classes = pd.Series({'Alice': 'Physics',
                   'Jack': 'Chemistry',
                   'Molly': 'English',
                   'Sam': 'History'})
students_classes

kelly_classes = pd.Series(['Philosophy', 'Arts', 'Math'], index=['Kelly', 'Kelly', 'Kelly'])
kelly_classes

all_students_classes = students_classes.append(kelly_classes)

all_students_classes

and It give me error like this
AttributeError                            Traceback (most recent call last)
Cell In\[35\], line 3
1 # Finally, we can append all of the data in this new Series to the first using the .append()
2 # function.
\----\> 3 all_students_classes.str = students_classes.append(kelly_classes)
5 # This creates a series which has our original people in it as well as all of Kelly's courses
6 all_students_classes

File \~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\pandas\\core\\generic.py:5989, in NDFrame.__getattr__(self, name)
5982 if (
5983     name not in self.\_internal_names_set
5984     and name not in self.\_metadata
5985     and name not in self.\_accessors
5986     and self.\_info_axis.\_can_hold_identifiers_and_holds_name(name)
5987 ):
5988     return self\[name\]
\-\> 5989 return object.__getattribute__(self, name)

AttributeError: 'Series' object has no attribute 'append'

I expect something like this from the e-learning
","In relatively recent pandas version, 1.5.2, append works, but gives a warning.
In pd 2.0, append has been removed
https://pandas.pydata.org/docs/dev/whatsnew/v2.0.0.html#deprecations
In [14]: students_classes = pd.Series({'Alice': 'Physics',
    ...:                    'Jack': 'Chemistry',
    ...:                    'Molly': 'English',
    ...:                    'Sam': 'History'})
    ...: kelly_classes = pd.Series(['Philosophy', 'Arts', 'Math'], index=['Kelly', 'Kelly', 'Kelly'])

In [15]: students_classes.append(kelly_classes)
C:\Users\paul\AppData\Local\Temp\ipykernel_6072\990183765.py:1: FutureWarning: The series.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  students_classes.append(kelly_classes)
Out[15]: 
Alice       Physics
Jack      Chemistry
Molly       English
Sam         History
Kelly    Philosophy
Kelly          Arts
Kelly          Math
dtype: object

Under the covers, append method uses _append, which works without raising the warning:
In [16]: students_classes._append(kelly_classes)
Out[16]: 
Alice       Physics
Jack      Chemistry
Molly       English
Sam         History
Kelly    Philosophy
Kelly          Arts
Kelly          Math
dtype: object

And using the recommended concat:
In [18]: pd.concat([students_classes,kelly_classes])
Out[18]: 
Alice       Physics
Jack      Chemistry
Molly       English
Sam         History
Kelly    Philosophy
Kelly          Arts
Kelly          Math
dtype: object

Python lists have an efficient append method.  numpy has a np.append function which is a poorly named cover for calling np.concatenate, and is often misused (it shouldn't be used iteratively).  pandas may be trying to avoid similar problems by getting rid of the append method.  With pd.concat you can join many Series (or frames) at once, and aren't (as) tempted to use it in a loop.
Looking up the code for _append (which is still in 2.0), I see it ends up using pd.concat.  So there's no value in using this 'work-around'.  Use concat as recommended.
",numpy
how to stop adblocks from blocking my revive adserver banner promotions,"I am using revive adserver to show my users promotions banners and track conversion. The problem is that adblock completely blocks my banners. I would like to change my banner code so that my users who are using adblock still will be able to know about promotions. 
","This is very complicated because new filters are being written and available every day. However, removing any code or markup and signifies that your ad is an ad can help.
Ie. Remove word like: ad, popup, banner, giveaway, etc.
Further, the best way is to determine what filter is blocking your ad, then alter the ad so that it isn't matched. Although if the site serving the ad has a lot of traffic a new filter will be written very quickly, and then at that point it's just a you against the world. So it's sort of pointless to even try.
An ad is an ad and will eventually be blocked.
",javascript
scikitlearn model giving 39localoutlierfactor39 object has no attribute 39predict39 error,"I'm new to machine learning world and I have built and trained a ml model using ScikitLearn library.It works perfectly well in the Jupyter notebook but when I deployed this model to Google Cloud ML and try to serve it using a Python script, it throws an error.
Here's a snippet from my model code:

Updated:

from sklearn.metrics import classification_report, accuracy_score
from sklearn.ensemble import IsolationForest
from sklearn.neighbors import LocalOutlierFactor

# define a random state
state = 1

classifiers = {
    ""Isolation Forest"": IsolationForest(max_samples=len(X),
                                       contamination=outlier_fraction,
                                       random_state=state),
    # ""Local Outlier Factor"": LocalOutlierFactor(
    # n_neighbors = 20,
    # contamination = outlier_fraction)
}

import pickle
# fit the model
n_outliers = len(Fraud)

for i, (clf_name, clf) in enumerate(classifiers.items()):

    # fit te data and tag outliers
    if clf_name == ""Local Outlier Factor"":
        y_pred = clf.fit_predict(X)
        print(""LOF executed"")
        scores_pred = clf.negative_outlier_factor_
        # Export the classifier to a file
        with open('model.pkl', 'wb') as model_file:
            pickle.dump(clf, model_file)
    else:
        clf.fit(X)
        scores_pred = clf.decision_function(X)
        y_pred = clf.predict(X)
        print(""IF executed"")
        # Export the classifier to a file
        with open('model.pkl', 'wb') as model_file:
            pickle.dump(clf, model_file)
    # Reshape the prediction values to 0 for valid and 1 for fraudulent
    y_pred[y_pred == 1] = 0
    y_pred[y_pred == -1] = 1

    n_errors = (y_pred != Y).sum()

# run classification metrics 
print('{}:{}'.format(clf_name, n_errors))
print(accuracy_score(Y, y_pred ))
print(classification_report(Y, y_pred ))

and here's the output in the Jupyter Notebook:

Isolation Forest:7
0.93
               precision    recall  f1-score   support


         0       0.97      0.96      0.96        94
         1       0.43      0.50      0.46         6

  avg / total    0.94      0.93      0.93       100


I have deployed this model to Google  Cloud ML-Engine and then try to serve it using the following python script:
import os
from googleapiclient import discovery
from oauth2client.service_account import ServiceAccountCredentials
credentials = ServiceAccountCredentials.from_json_keyfile_name('Machine Learning 001-dafe42dfb46f.json')

PROJECT_ID = ""machine-learning-001-201312""
VERSION_NAME = ""v1""
MODEL_NAME = ""mlfd""
service = discovery.build('ml', 'v1', credentials=credentials)
name = 'projects/{}/models/{}'.format(PROJECT_ID, MODEL_NAME)
name += '/versions/{}'.format(VERSION_NAME)

data = [[265580, 7, 68728, 8.36, 4.76, 84.12, 79.36, 3346, 1, 11.99, 1.14,655012, 0.65, 258374, 0, 84.12] ]

response = service.projects().predict(
    name=name,
    body={'instances': data}
).execute()

if 'error' in response:
  print (response['error'])
else:
  online_results = response['predictions']
  print(online_results)

Here is the output of this script:

Prediction failed: Exception during sklearn prediction: 'LocalOutlierFactor' object has no attribute 'predict'

","LocalOutlierFactor does not have a predict method, but only a private _predict method.  Here is the justification from the source.
def _predict(self, X=None):
    """"""Predict the labels (1 inlier, -1 outlier) of X according to LOF.
    If X is None, returns the same as fit_predict(X_train).
    This method allows to generalize prediction to new observations (not
    in the training set). As LOF originally does not deal with new data,
    this method is kept private.

https://github.com/scikit-learn/scikit-learn/blob/a24c8b46/sklearn/neighbors/lof.py#L200
",scikit-learn
fatalgpu_data_manager_impl_privatecc439 gpu process isn39t usable goodbye,"I am working on Ubuntu 18.04 and getting the following exception in latest version of intellij FATAL:gpu_data_manager_impl_private.cc(439)] GPU process isn't usable. Goodbye. error. This is occuring on both Ultimate and Community versions when I am opening any remote repository although new helloworld project works fine for me. I can clone the repo successfully but intellij crashes with this exception
I have also tried running intellij with --disable-gpu but didn't work for me.
","disable Markdown plugin.
it's work for me.
",java
trl sfttrainer  llama2 finetuning on alpaca  datasettext field,"I am trying to finetune the Llama2 model using the alpaca dataset.  I have loaded the model in 4-bit and apply the peft config to the model for Lora training. Then I am trying to do TRL’s SFTTrainer to fine-tune the model.
The train_dataset is
Dataset({
    features: ['instruction', 'input', 'output', 'input_ids', 'attention_mask'],
    num_rows: 50002
})

This is the error that I get:
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
Cell In[28], line 3
      1 # Step 8 :Set supervised fine-tuning parameters
      2 from transformers import DataCollatorForLanguageModeling
----> 3 trainer = SFTTrainer(
      4     model=model,
      5     train_dataset=train_data,
      6     #eval_dataset=val_data,
      7     #peft_config=peft_config,
      8     #dataset_text_field=""train"",
      9     max_seq_length=max_seq_length,
     10     tokenizer=tokenizer,
     11     args=training_arguments,
     12     #packing=True,
     13     #data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False),
     14 )


ValueError: You passed `packing=False` to the SFTTrainer, but you didn't pass a `dataset_text_field` or `formatting_func` argument.

If I try to pass the packing = True then I get this error:
ValueError: You need to pass a `dataset_text_field` or `formatting_func` argument to the SFTTrainer if you want to use the `ConstantLengthDataset`.

If I provide the dataset_text_field, which I do not know what it is.  I tried with ""train"" or ""text"" keywords and I am getting this error:
ValueError: the `--group_by_length` option is only available for `Dataset`, not `IterableDataset

I appreciate if someone can help me to understand the ""dataset_text_filed"", where do I set ConstantLengthDataset (does it come from packing?).  I also tried with packing = False and provide the dataset_text_field with 'train' and 'text' and they are incorrect.
based on the documentation:
dataset_text_field (Optional[str]): The name of the text field of the dataset, in case this is passed by a user, the trainer will automatically create a ConstantLengthDataset based on the dataset_text_field argument.

","dataset_text_field (Optional[str]) is the name of the field in the training dataset that contains the text that will be used for training only if formatting_func is None.
You should be careful because if you do this:
`dataset_text_field='instruction'

SFTTrainer will only read the text saved in train_dataset['instruction'].
So that Llama2 will only learn to predict the instructions without the answers.
Each line of your dataset(train_data) should have a string saved in 'text' field like this:
train_data[0]['text']=""<s>[INST]<<SYS>>You are an expert in math.<</SYS>>Compute 2+2 [/INST]4 </s>""

And you should do this:
`dataset_text_field='text'

",llama
cross_val_score returns nan when put in fit_params,"I am doing SVC for classification task with cross validation using cross_val_score in slearn, but turns out it return list of nan value when I put in parameters for fit_params but working fine if I dont put in the parameters for fit_params.
Code:
# define parameter
param_grid = {
    'C' : [1,5,10,20],
    'gamma' : ['auto','scale']
}

svc = SVC(kernel = ""rbf"")

scores = cross_val_score(svc, x_train, y_train, cv=10, fit_params = param_grid)
# scores output array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan])

scores = cross_val_score(svc, x_train, y_train, cv=10)
# scores output array([0.95833333, 0.95833333, 0.95454545, 0.93181818, 0.95454545, 0.96197719, 0.96197719, 0.94676806, 0.96197719, 0.95057034])

","fit_params is designated for fit methods (e.g., array of sample weights for training data), but you pass your parameter grid to cross_val_score, which is incompatible with your data (x_train, y_train, etc.). Indeed, if you specify error_score='raise' in your cross_val_score, you will receive the corresponding error. Parameter grids should be used with GridSearchCV or similar tools.
",cross-validation
restrictions on non volatile variables in c,"I Would like to understand what Restrictions if any does the compiler have with regards to non volatile variables in C.
I'm not sure if its true or not, but I've been told that if you have the following code:
int x;
...
void update_x() {
  lock();
  x = x*5+3;
  unlock();
} 

You must acquire the lock to read x because even tough the compiler is unlikely to do it is technically legal for it to store intermediate calculation such as x*5 into x, and so the read might read an intermediate value. so my first question is whether it is indeed the case? if not, why not?
If it is, I have a followup question, is there anything that's prevents to compiler from using x as a temporary storage before or after taking the lock? (Assuming the compiler can prove that a single thread executing the program will not notice it).
If not, does that mean that any program that has non volatile shared variables is technically undefined even if all the accesses are protected by locks?
Thanks,
Ilya
","Prior to C11, the answer is No, as the spec doesn't define anything about what multiple threads do, so any program that uses multiple threads where one thread writes an object and another thread reads it is undefined behavior.
With C11, there's actually a memory model that talks about multiple threads and data races, so the answer is Yes, as long as the lock/unlock routines do certain synchronization operations (involving either library functions that do the synchronization or operations on special _Atomic objects).
Since the C11 spec is attempting to codify behavior of existing implementations (for the most part), it is likely that any code that does what it requires (ie, using a implementation provided library for locking, or implementation provided extensions for atomic operations) will work correctly even on pre-C11 implementations.
Section 5.2.1.4 of the C11 spec covers this.
",c
with python how to apply vector operations to a neighborhood in an nd image,"I have a 3D image with vector components (i.e., a mapping from R3 to R3). My goal is to replace each vector with the vector of maximum norm within its 3x3x3 neighborhood.
This task is proving to be unexpectedly challenging. I attempted to use scipy.ndimage.generic_filter, but despite its name, this filter only handles scalar inputs and outputs. I also briefly explored skimage and numpy's sliding_window_view, but neither seemed to provide a straightforward solution.
What would be the correct way to implement this?
Here's what I ended up writing. It's not very elegant and pretty slow, but should help understand what I'm trying to do.
import numpy as np
import matplotlib.pyplot as plt

def max_norm_vector(data):
    """"""Return the vector with the maximum norm.""""""
    data = data.reshape(-1, 3)
    norms = np.linalg.norm(data, axis=-1)
    idx_max = np.argmax(norms)
    return data[idx_max]

if __name__ == '__main__':
    # Load the image
    range_ = np.linspace(-5, 5, 30)
    x, y, z = np.meshgrid(range_, range_, range_, indexing='ij')
    data = 1 - (x ** 2)

    # Compute the gradient
    grad = np.gradient(data)
    grad = np.stack(grad, axis=-1)  # Stack gradients along a new last axis


    # grad = grad[:5, :5, :5, :]  # Crop the gradient for testing
    max_grad = np.zeros_like(grad)
    for i in range(1,grad.shape[0]-1):
        for j in range(1,grad.shape[1]-1):
            for k in range(2,grad.shape[2]-1):
                max_grad[i, j, k] = max_norm_vector(grad[i-1:i+2, j-1:j+2, k-1:k+2,:])

    # Visualization
    fig = plt.figure(figsize=(12, 6))

    # Plot original data
    ax1 = fig.add_subplot(121, projection='3d')
    ax1.scatter(x.ravel(), y.ravel(), z.ravel(), c=data.ravel(), cmap='viridis', alpha=0.5)
    ax1.set_title('Original Data')

    # Plot maximum gradient vectors
    ax2 = fig.add_subplot(122, projection='3d')

    # Downsample for better performance
    step = 3
    x_down = x[::step, ::step, ::step]
    y_down = y[::step, ::step, ::step]
    z_down = z[::step, ::step, ::step]
    max_grad_down = max_grad[::step, ::step, ::step]

    ax2.quiver(x_down.ravel(), y_down.ravel(), z_down.ravel(),
               max_grad_down[:, :, :, 0].ravel(), max_grad_down[:, :, :, 1].ravel(), max_grad_down[:, :, :, 2].ravel(),
               length=0.1, color='red', alpha=0.7)
    ax2.set_title('Maximum Gradient Vectors')

    plt.tight_layout()
    plt.show()

","DIPlib has this function implemented: dip.SelectionFilter().
This is how you'd use it:
grad = ...  # OP's grad array
norm = dip.Norm(grad)
out = dip.SelectionFilter(grad, norm, dip.Kernel(3, ""rectangular""), mode=""maximum"")

You can cast the dip.Image object out to a NumPy array with np.asarray(out) (no copy of the data will be made). NumPy functions will accept the dip.Image object as input, but many functions in scikit-image expect the input array to have a .shape method or similar, which will fail if you don't do the cast explicitly.
Install the package with pip install diplib.
Disclaimer: I'm an author of DIPlib, but I didn't implement this function.
",numpy
accept4 blocks though sock_nonblock is set,"I'm trying to accept a socket non-blockingly:
accept4(s, (struct sockaddr *) &peerAddress, &len,SOCK_NONBLOCK);

where as s is a fd, peerAddres is an address, and len, is its length.
I wish that accept won't block the thread.
Though, once I debug, the process is stuck at this line, while no connection is pending.
What is my mistake?
","SOCK_NONBLOCK just sets the newly accepted socket to non-blocking. It does not make accept itself non-blocking. For this one would need to set the listen socket non-blocking before calling accept.
",c
how to convert character indices to bert token indices,"I am working with a question-answer dataset UCLNLP/adversarial_qa.
from datasets import load_dataset
ds = load_dataset(""UCLNLP/adversarial_qa"", ""adversarialQA"")

How do I map character-based answer indices to token-based indices after tokenizing the context and question together using a tokenizer like BERT. Here's an example row from my dataset:
d0 = ds['train'][0]
d0

{'id': '7ba1e8f4261d3170fcf42e84a81dd749116fae95',
 'title': 'Brain',
 'context': 'Another approach to brain function is to examine the consequences of damage to specific brain areas. Even though it is protected by the skull and meninges, surrounded by cerebrospinal fluid, and isolated from the bloodstream by the blood–brain barrier, the delicate nature of the brain makes it vulnerable to numerous diseases and several types of damage. In humans, the effects of strokes and other types of brain damage have been a key source of information about brain function. Because there is no ability to experimentally control the nature of the damage, however, this information is often difficult to interpret. In animal studies, most commonly involving rats, it is possible to use electrodes or locally injected chemicals to produce precise patterns of damage and then examine the consequences for behavior.',
 'question': 'What sare the benifts of the blood brain barrir?',
 'answers': {'text': ['isolated from the bloodstream'], 'answer_start': [195]},
 'metadata': {'split': 'train', 'model_in_the_loop': 'Combined'}}

After tokenization, the answer indices are 56  and 16:
from transformers import BertTokenizerFast
bert_tokenizer = BertTokenizerFast.from_pretrained('bert-large-uncased', return_token_type_ids=True)

bert_tokenizer.decode(bert_tokenizer.encode(d0['question'], d0['context'])[56:61])
'isolated from the bloodstream'

I want to create a new dataset with the answer's token indices, e.g., 56 ad 60.
This is from a linkedin learning class. The instructor did the conversion and created the csv file but he did not share it or the code to do that. This is the expected result:
","You should encode both the question and context, locate the token span for the answer within the tokenized context, and update the dataset with the token-level indices.
The following function does the above for you:
def get_token_indices(example):
    # Tokenize with `return_offsets_mapping=True` to get character offsets for each token
    encoded = tokenizer(
        example['question'], 
        example['context'], 
        return_offsets_mapping=True
    )

    # Find character start and end from the original answer
    char_start = example['answers']['answer_start'][0]
    char_end = char_start + len(example['answers']['text'][0])

    # Identify token indices for the answer
    start_token_idx = None
    end_token_idx = None
    
    for i, (start, end) in enumerate(encoded['offset_mapping']):
        if start <= char_start < end: 
            start_token_idx = i
        if start < char_end <= end:
            end_token_idx = i
            break

    example['answer_start_token_idx'] = start_token_idx
    example['answer_end_token_idx'] = end_token_idx
    return example

Here's how you can use and test this function:
ds = load_dataset(""UCLNLP/adversarial_qa"", ""adversarialQA"")
tokenizer = BertTokenizerFast.from_pretrained('bert-large-uncased', return_token_type_ids=True)

tokenized_ds = ds['train'].map(get_token_indices)


# Example
d0_tokenized = tokenized_ds[0]
print(""Tokenized start index:"", d0_tokenized['answer_start_token_idx'])
print(""Tokenized end index:"", d0_tokenized['answer_end_token_idx'])

answer_tokens = tokenizer.decode(
    tokenizer.encode(d0_tokenized['question'], d0_tokenized['context'])[d0_tokenized['answer_start_token_idx']:d0_tokenized['answer_end_token_idx']+1]
)
print(""Tokenized answer:"", answer_tokens)

Output:
Tokenized start index: 56
Tokenized end index: 60
Tokenized answer: isolated from the bloodstream

",large-language-model
printf wprintf s s ls char and wchar errors not announced by a compiler warning,"I have tried the following code:
wprintf(L""1 %s\n"",""some string""); //Good
wprintf(L""2 %s\n"",L""some string""); //Not good -> print only first character of the string
printf(""3 %s\n"",""some string""); //Good
//printf(""4 %s\n"",L""some string""); //Doesn't compile
printf(""\n"");
wprintf(L""1 %S\n"",""some string""); //Not good -> print some funny stuff
wprintf(L""2 %S\n"",L""some string""); //Good
//printf(""3 %S\n"",""some string""); //Doesn't compile
printf(""4 %S\n"",L""some string"");  //Good

And I get the following output:
1 some string
2 s
3 some string

1 g1 %s

2 some string
4 some string

So: it seems that both wprintf and printf are able to print correctly both a char* and a wchar*, but only if the exact specifier is used. If the wrong specifier is used, you might not get a compiling error (nor warning!) and end up with wrong behavior. Do you experience the same behaviour?
Note: This was tested under Windows, compiled with MinGW and g++ 4.7.2 (I will check gcc later)
Edit: I also tried %ls (result is in the comments)
printf(""\n"");
wprintf(L""1 %ls\n"",""some string""); //Not good -> print funny stuff
wprintf(L""2 %ls\n"",L""some string""); //Good
// printf(""3 %ls\n"",""some string""); //Doesn't compile
printf(""4 %ls\n"",L""some string"");  //Good

","I suspect GCC (mingw) has custom code to disable the checks for the wide printf functions on Windows. This is because Microsoft's own implementation (MSVCRT) is badly wrong and has %s and %ls backwards for the wide printf functions; since GCC can't be sure whether you will be linking with MS's broken implementation or some corrected one, the least-obtrusive thing it can do is just shut off the warning.
",c
stop seaborn changing matplotlib plot style,"I have already found an entry that deals with a similar topic, but the suggestion doesn't work here. 
How can I use seaborn without changing the matplotlib defaults?
If I missed something I am grateful for every link.
I want to create a plot with matplotlib after having created a plot with seaborn. However, the settings of seaborn seem to affect the matplotlib appearance (I realize that seaborn is an extension of matplotlib). This happens even though I clear, close the plot etc with.
    sns.reset_orig()
    plt.clf()
    plt.close()

Complete example code:
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import numpy as np
# data
df = pd.DataFrame(np.array([[1, 1], [2, 2], [3, 3]]),columns=['x', 'y'])

###### seaborn plot #######
fig=sns.JointGrid(x=df['x'],
                  y=df['y'],
                  )
#fill with scatter and distribution plot
fig = fig.plot_joint(plt.scatter, color=""b"")                            
fig = fig.plot_marginals(sns.distplot, kde=False, color=""b"")

#axis labels
fig.set_axis_labels('x','y')        

#set title
plt.subplots_adjust(top=0.92)
title='some title'
fig.fig.suptitle(title)

#clear and close figure
sns.reset_orig()
plt.clf()
plt.close()        

###### matplotlib plot #######
#define data to plot
x = df['x']
y = df['y']

#create figure and plot
fig_mpl, ax = plt.subplots()
ax.plot(x,y,'.')
ax.grid(True)
ax.set_xlabel('x')
ax.set_ylabel('y')
title='some title'
ax.set_title(title)
plt.close()

The seaborn plot always looks the same:
seaborn plot
But the apperance of the matplotlib plot differs. The normal one without creating a seaborn plot in front:
mpl plot normal
and how it changes if using the shown code: 
mpl with sns in front
how do I stop this behaviour, avoid the seaborn influencing the other plots?
","When you import seaborn the default styling is changed.
You can change the style that matplotlib applies to plots with the plt.style.use command.
To get a list of available styles you can use plt.style.available. To change back to the classic matplotlib style you'll want to use plt.style.use('classic').
",matplotlib
hyperparameter tuning job in sagemaker with cross valdiation,"I managed to get something along those lines to work. This is great but to be more on the save side (i.e. not rely too much on the train validation split) one should really use cross validation. I am curious, if this can also be achieved via Sagemaker hyperparameter tuning jobs? I googled extensively ...
","It is not possible through HPO.
You need to add additional step in your workflow to achieve cross-validation.
",cross-validation
how to chain azureopenai in langchain,"from openai import AzureOpenAI
client = AzureOpenAI(
        azure_endpoint=os.getenv(""AZURE_OPENAI_ENDPOINT""),
        api_key=os.getenv(""AZURE_OPENAI_API_KEY""),
        api_version=""2024-05-01-preview""
    )

normal_chain = (
    ChatPromptTemplate.from_messages([(""system"", ""write a tweet about {topic} in the style of Elon Musk"") ])
    | client
    | StrOutputParser()
)


In this code, I am getting Expected type 'Runnable[Any, Other] | (Any) -> Other | (Iterator) -> Iterator[Other] | Mapping[str, Runnable[Any, Other] | (Any) -> Other | Any]', got 'AzureOpenAI' instead. How to make the client as Runnable? Is there any other way to do the same thing?
I tried all the available options from LangChain, but it is not working.
","    client = AzureOpenAI(
        azure_endpoint=os.getenv(""AZURE_OPENAI_ENDPOINT""),
        api_key=os.getenv(""AZURE_OPENAI_API_KEY""),
        api_version=""2024-05-01-preview""
    )

    completion = client.chat.completions.create(
        model=finetuned_deployment,
        messages=[
            {
                ""role"": ""user"",
                ""content"": f""Write a tweet about {topic}""
            }],
        max_tokens=800,
        temperature=0.7,
        top_p=0.95,
    )
    
completion.to_json()    

This code snippet worked.
",chatgpt
spring security form don39t want to login,"I created an form to log in in Spring Security. Registration is working fine but there is something wrong with loggin in.
When I try to log in there is an log:
2025-01-19T23:46:34.220+01:00  WARN 388 --- [TrippiApp] [nio-8080-exec-4] .w.s.m.s.DefaultHandlerExceptionResolver : Resolved [org.springframework.web.HttpRequestMethodNotSupportedException: Request method 'POST' is not supported]
Login into default loging page is available so I assume there is something wrong with SecurityFilterChain
@Configuration
@EnableWebSecurity
public class SecurityConfig {

    @Bean
    public SecurityFilterChain securityFilterChain(HttpSecurity http) throws Exception {
        http.csrf(csrf -> csrf.disable()).authorizeHttpRequests(requests -> requests
                .dispatcherTypeMatchers().permitAll()
                .requestMatchers(""/CSS/**"").permitAll()
                .requestMatchers(""/login /register /search"").hasAnyRole(""USER"", ""ADMIN"")
                .anyRequest().authenticated()
        )
        .formLogin(login -> login
                .loginPage(""/login"")
                .loginProcessingUrl(""/userAuth"")
                .defaultSuccessUrl(""/register"")
                .failureForwardUrl(""/login?error"")
                .permitAll())
        .logout(logout -> logout.logoutRequestMatcher(new AntPathRequestMatcher(""/logout""))
                .permitAll());

        return http.build();
    }
}

@Controller
public class AuthorisationController {

    private UserService userService;

    public AuthorisationController(UserService userService) {
        this.userService = userService;
    }


    @GetMapping(""/login"")
    public String login() {
        return ""login"";
    }

<!DOCTYPE html>
<html xmlns:th=""http://thymeleaf.org""
      xmlns:layout=""http://www.ultraq.net.nz/thymeleaf/layout""
      layout:decorate=""~{layout}"">
<head>
    <link rel=""stylesheet"" type=""text/css"" href=""/CSS/style.css"" />
    <meta charset=""UTF-8"" />
    <title>Register</title>
</head>

<body>
<div class=""navbar"">
    <div class=""button""><a href="""" class=""nav-btn"">Konto</a></div>
    <div class=""button""><a href="""" class=""nav-btn"">Szukaj</a></div>
    <div class=""button""><a href="""" class=""nav-btn"">Wiadomości</a></div>
    <div class=""button""><a href="""" class=""nav-btn"">Logowanie</a></div>
    <div class=""button""><a href="""" class=""nav-btn"">Rejestracja</a></div>
    <div class=""button""><a href="""" class=""nav-btn"">Wyloguj</a></div>
</div>
<div class=""main"">
    <div class=""sidebar"">
        <h2>Użytkownik</h2>
    </div>

    <div class=""mainpage"">
        <h2>Logowanie</h2>

        <div th:if=""${param.error}"" class=""alert alert-success"">
            Zły nick lub hasło!
        </div>
        <div th:if=""${param.logout}"" class=""alert alert-success"">
            Wylogowałeś się!
        </div>
        <div class=""register"">
            <form method=""POST"" role=""form"" th:action=""@{/login}"">

                <div class=""info"">
                    Nick:
                    <input
                            id=""nickName""
                            type=""text""
                            name=""nickName""
                            placeholder=""Podaj nick""
                            class=""form-in""
                    />
                </div>

                <div class=""info"">
                    Hasło:
                    <input
                            id=""password""
                            type=""password""
                            name=""password""
                            placeholder=""Podaj hasło""
                            class=""form-in""
                    />
                </div>

                <button value =""Log in"" type=""submit"" class=""btn-send"">Zaloguj</button>
            </form>
        </div>
    </div>
</div>
</body>
</html>

I have tried to toggle in filter parameters but so far with no results.
","This happens because you specified the loginProcessingUrl to be /userAuth, and not /login. You're then sending the form to /login, through
<form method=""POST"" role=""form"" th:action=""@{/login}"">

but Spring already configured a controller at /userAuth for you, because you explicitly specified that.
One solution would be to also change the th:action to send the requests at @{/userAuth}.
Another workaround would be to change the loginProcessingUrl to /login. However, you can also simply remove the statement that manually configures the login processing URL. Not specifying any loginProcessingUrl automatically makes Spring configure it as the /login endpoint.
",java
how to apply labelencoder to a polars dataframe column,"I'm trying to use scikit-learn's LabelEncoder with a Polars DataFrame to encode a categorical column. I am using the following code.
import polars as pl

from sklearn.preprocessing import LabelEncoder

df = pl.DataFrame({
    ""Color"" : [""red"",""white"",""blue""]
})

enc = LabelEncoder()

However, an error is raised.

ValueError: y should be a 1d array, got an array of shape () instead.

Next, I tried converting the column to a NumPy.
df.with_columns(
    enc.fit_transform(pl.col(""Color"").to_numpy()) 
)

Now, a different error is raised.

AttributeError: 'Expr' object has no attribute 'to_numpy'

Note. I found that .cast(pl.Categorical).to_physical() could be used to obtain the desired result. Still, I'd prefer using something like transform() on my test dataset.
df.with_columns(
    pl.col(""Color"").cast(pl.Categorical).to_physical().alias(""Color_encoded"")
)

","For such a call to an external API taking an entire sequence of values, such as enc.fit_transform, pl.Expr.map_batches could be used.
df.with_columns(
    pl.col(""Color"").map_batches(enc.fit_transform)
)

shape: (3, 1)
┌───────┐
│ Color │
│ ---   │
│ i64   │
╞═══════╡
│ 1     │
│ 2     │
│ 0     │
└───────┘

Note. It would be nice if enc.set_output(""polars"") (as outlined in this answer) was available for the LabelEncoder. However, this is not implemented.

You already shared an approach to label encoding a column using polars' native expression API. A cleaner way could rely on dense ranking as follows.
df.with_columns(
    pl.col(""Color"").rank(""dense"") - 1
)

Subtraction is used only to obtain an output with lowest label being 0.
",scikit-learn
why is my data not getting properly concatenated,"I split the data using train_test_split after preprocessing:
from sklearn.model_selection import train_test_split
X_train,X_test,y_train,y_test= train_test_split(X,y,test_size=0.2,random_state=42)

Then did robust scaling separately for the numerical columns in test and train:
from sklearn.preprocessing import RobustScaler
robust = RobustScaler()
X_train_ = robust.fit_transform(X_train[numeric_columns])
X_test_ = robust.transform(X_test[numeric_columns])
X_train_sc_num=pd.DataFrame(X_train_,columns=[numeric_columns])
X_test_sc_num=pd.DataFrame(X_test_,columns=[numeric_columns])

Then did concatenation:
X_train_scaled=pd.concat([X_train_sc_num,X_train[categoric_columns]],axis=1)
X_test_scaled=pd.concat([X_test_sc_num,X_test[categoric_columns]],axis=1)

but the shape got broken and so many 'nan' values added in the categorical columns of the output data.
The sahpe was (466,17)+(466,11),
it should be (466,28),
but it became (560,28).
How can I solve this issue?
I want to do Robust scale my data after train_test_split, but without touching my OHE columns.
","Your issue might be arising from a couple of things.

First of all, you're using columns=[numeric_columns], which would treat the list as a single column name. Instead, it should be just columns=numeric_columns.
When creating the scaled DataFrames, you weren't preserving the index from the original data. To do this, you simply add an additional parameter index=X_train.index (or X_test, depending on the case) to the pd.DataFrame() initialization.

Here is a reproducible example using this example data illustrating the steps you'd need to follow with your data:
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import RobustScaler

# Create example data
np.random.seed(42)
n_samples = 466

numerical_data = {""temperature"": np.random.normal(14, 3, n_samples), ""moisture"": np.random.normal(96, 2, n_samples)}
categorical_data = {""color"": np.random.choice([""green"", ""yellow"", ""purple""], size=n_samples, p=[0.8, 0.1, 0.1])}

# Create DataFrame
df = pd.DataFrame({**numerical_data, **categorical_data})

# Define numeric and categorical columns
numerical_columns = numerical_data.keys()
categorical_columns = categorical_data.keys()

# One-hot encode categorical columns
df_encoded = pd.get_dummies(df, columns=categorical_columns)

# Split features and target (creating dummy target for example)
y = np.random.randint(0, 2, n_samples)
X = df_encoded

# Split the data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Get the one-hot encoded column names
# In general, these will be different than categorical_columns, since you're doing OHE
categorical_columns_encoded = [col for col in X_train.columns if col not in numerical_columns]

# Initialize RobustScaler
robust = RobustScaler()

# Scale only numeric columns
X_train_scaled_numeric = robust.fit_transform(X_train[numerical_columns])
X_test_scaled_numeric = robust.transform(X_test[numerical_columns])

# Create DataFrames with correct column names for scaled numeric data
X_train_scaled_numeric_df = pd.DataFrame(
    X_train_scaled_numeric,
    columns=numerical_columns,
    index=X_train.index,  # Preserve the index
)

X_test_scaled_numeric_df = pd.DataFrame(
    X_test_scaled_numeric,
    columns=numerical_columns,
    index=X_test.index,  # Preserve the index
)

# Concatenate with categorical columns
X_train_scaled = pd.concat([X_train_scaled_numeric_df, X_train[categorical_columns_encoded]], axis=1)
X_test_scaled = pd.concat([X_test_scaled_numeric_df, X_test[categorical_columns_encoded]], axis=1)

# Verify the shapes
print(""Original shapes:"")
print(f""X_train: {X_train.shape}"")
print(f""X_test: {X_test.shape}"")
print(""\nScaled shapes:"")
print(""X_train_scaled: {X_train_scaled.shape} = {X_train_scaled_numeric.shape} + {X_train[categorical_columns_encoded].shape}"")
print(f""X_test_scaled: {X_test_scaled.shape} = {X_test_scaled_numeric.shape} + {X_test[categorical_columns_encoded].shape}"")

# Verify no NaN values
print(""\nNaN check:"")
print(""NaN in X_train_scaled:"", X_train_scaled.isna().sum().sum())
print(""NaN in X_test_scaled:"", X_test_scaled.isna().sum().sum())

That would print:
Original shapes:
X_train: (372, 5)
X_test: (94, 5)

Scaled shapes:
X_train_scaled: (372, 5) = (372, 2) + (372, 3)
X_test_scaled: (94, 5) = (94, 2) + (94, 3)

",scikit-learn
png conversion into scikit learn digit format,"I would need some help for converting a PNG RGBA into the correct format for digit recognition in Scikit learn. This is my code
image = Image.open(image_path)
print (image.size)
print (image.mode)
print (image.format)


image = image.resize((8, 8)).convert(""L"")
print (image.size)
print (image.mode)
print (image.format)


data = np.array(image)

data = data / 16.0
data = data.reshape(1, -1)
print (data.shape)
print (data)

print (model.predict(data))

The convert method always converts to an array of the correct size with all 0s.
Some articles online suggested to use convert(""LA"") because of the RGBA format, but if I do so the size of the resulting array is wrong and I do not understand how to convert it to a correct 8x8.
This is the link to the test image (png file, zipped to avoid any alterations).
","That's because your image is solid black in the RGB channels and the digit 7 that you are looking for is contained only in the A channel (A=alpha or transparency).
If I split your image into its constituent R, G, B, A channels and put a yellow border around each then ""squidge"" them together across the page left-to-right with ImageMagick:
magick YOURIMAGE -separate -bordercolor yellow -border 3 +append result.png


Hopefully you can see R (on left) is empty, G (second box from left) is empty, B (third box from left) is empty and your 7 is in the rightmost A channel.

So, in Python, basically you need to just use the A channel:
from PIL import Image

# Load image
RGBA = Image.open('YOURIMAGE')

# Extract just A channel as single channel L-mode image
A = RGBA.getchannel('A')

",scikit-learn
adding a labeled point to a venn diagram in matplotlibvenn,"Say, I am using python and the matplotlib-venn package to create some Venn diagrams. However, I wanted to include a labeled point inside one of the circles. That way I can show that point x is an element of a set A. Is there a way to simply add a point to a diagram in matplotlib-venn?
EDIT: I added a little picture to demonstrate.

Minimal Working Example:
This code will just create the venn diagram but without the point
from matplotlib import pyplot as plt
import numpy as np
from matplotlib_venn import venn2
plt.figure(figsize=(4,4))
v = venn2(subsets = (3, 2, 1))
plt.show()

","The Venn diagram is centered at x,y = 0,0. Just plot your point at the desired x,y.
from matplotlib import pyplot as plt
from matplotlib_venn import venn2
plt.figure(figsize=(4,4))
v = venn2(subsets = (3, 2, 1))

plt.axhline(0, linestyle='--')
plt.axvline(0, linestyle='--')

plt.plot(-0.5,0.2,'bo')
plt.text(-0.6,0.2, 'A')

plt.show()

",matplotlib
numerically obtaining response of a damped driven oscillator,"I am trying to plot the response of a periodically-driven damped oscillator whose dynamics is governed by,
x''+ 2Gx' + f0^2 x = F cos(ft)
where the constants denote the following.
G: Damping coefficient
f0: Natural frequency
f: Driving frequently
F: Strength of the drive
To do so, I solved the above differential equation for x(t). Next, I extracted the steady-state part from x(t), took its Fourier transform, and plotted its magnitude to visualize the response of the oscillator.
Here is the code that attempts to achieve it.
import numpy as np
import matplotlib.pyplot as plt
from scipy.fft import fft, fftfreq

G=1.0
f0=2
f1=5
F=1

N=500000
T=50
dt=T/N

t=np.linspace(0,T,N)
u=np.zeros(N,dtype=float)  # Position
v=np.zeros(N,dtype=float)  # Velocity

u[0]=0
v[0]=0.5

for i in range(N-1):
    u[i+1] = u[i] + v[i]*dt
    v[i+1] = v[i] - 2*G*v[i]*dt - (f0*f0)*u[i]*dt + F*np.cos(f1*t[i])*dt 

slice_index=int(20/dt)
U=u[slice_index:]

X_f = fft(U)
frequencies = fftfreq(len(U), dt)
psd = np.abs(X_f)
positive_freqs = frequencies[frequencies > 0]  

plt.plot(positive_freqs, psd[frequencies > 0], label=""Simulated PSD"")
plt.plot(frequencies, psd)


Since the oscillator is forced and reaches a steady state, I expect the response to peak around the driving frequency. However, the above code gives a peak located nowhere near f. What am I doing wrong?
","Your frequencies f0 and f1 are applied in the finite-difference model in rad/s. This may or may not have been your intention.
However, your frequencies from the FFT are in cycles/s.
Since you are using the symbol f, rather than omega, I would guess that you want them in cycles/s. In your finite-difference model then you would have to use 2.PI.f in both locations where you put an f before. Specifically in the line
v[i+1] = v[i] - 2*G*v[i]*dt - (2 * np.pi * f0 ) ** 2 * u[i]*dt + F*np.cos( 2 * np.pi * f1*t[i] ) * dt

Then you get peak energy at a frequency of 5 Hz. (Trim the x-axis scale.)
You are very heavily damped, BTW. Also, you aren't strictly plotting PSD.

import numpy as np
import matplotlib.pyplot as plt
from scipy.fft import fft, fftfreq

G=1.0
f0=2
f1=5
F=1

N=500000
T=50
dt=T/N

t=np.linspace(0,T,N)
u=np.zeros(N,dtype=float)  # Position
v=np.zeros(N,dtype=float)  # Velocity

u[0]=0
v[0]=0.5

for i in range(N-1):
    u[i+1] = u[i] + v[i]*dt
    v[i+1] = v[i] - 2*G*v[i]*dt - (2 * np.pi * f0 ) ** 2 * u[i]*dt + F*np.cos(2 * np.pi * f1*t[i] ) * dt

slice_index=int(20/dt)
U=u[slice_index:]

X_f = fft(U)
frequencies = fftfreq(len(U), dt)
psd = np.abs(X_f)
positive_freqs = frequencies[frequencies > 0]  

plt.plot(positive_freqs, psd[frequencies > 0], label=""Simulated PSD"")
plt.xlim(0,10)
plt.show()

",matplotlib
preprovided parameters to command with required prompt in shell,"So I have to make deployment of AWS Elastic Beanstalk application with AWSEB CLI on Jenkins. When i try to use command
eb init

It requires some information and credentials. Credentials are stored as parameters or could be a secret file on Jenkins instance. Command have no such things like --parameter to provide it at start. Is there any solution to provide all parameters in the code that in runtime this command will now okay now this is provided now this and so on? Something like this:
eb init --username XXX --password XXX --others XXX

Here is documentation for that command
https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/eb-cli3-configuration.html
","Will this answer help your issue? It seems you can set some of the parameters as Environment variables and rest as flags.. For example.
$ export AWS_ACCESS_KEY_ID=""xxx""
$ export AWS_SECRET_ACCESS_KEY=""xxx""
$ export AWS_DEFAULT_REGION=""xxx""

Then
eb init --region eu-west-1 --platform <platform-name> appname

",prompt
i am getting an error while trying to run autogen using a local llm,"Below is the Code I am running. Note that I am using LM Studio (LLM: Llama 2) and I have double checked to make sure that the server number is correct.
from autogen import AssistantAgent, UserProxyAgent

config_list = [
    {
        ""api_type"": ""open_ai"",
        ""api_base"": ""http://localhost:1234/v1"",
        ""api_key"": ""NULL""
    }
]

llm_config = {'config_list': config_list}

assistant = AssistantAgent(
    name=""assistant"",
    llm_config = llm_config
)

user_proxy = UserProxyAgent(
    name=""user_proxy"",
    human_input_mode=""NEVER"",
    max_consecutive_auto_reply=100,
)

task = """"""write a python method to output numbers 1 to 100""""""

user_proxy.initiate_chat(
    assistant,
    message=task
)

This is the exact result I get after running python app.py (name of the program):
user_proxy (to assistant):
write a python method to output numbers 1 to 100
--------------------------------------------------------------------------------
Traceback (most recent call last):
  File ""app.py"", line 26, in <module>
    user_proxy.initiate_chat(
  File ""C:\Users\Rohun\Development\AutoGen\env\lib\site-packages\autogen\agentchat\conversable_agent.py"", line 550, in initiate_chat
    self.send(self.generate_init_message(**context), recipient, silent=silent)
  File ""C:\Users\Rohun\Development\AutoGen\env\lib\site-packages\autogen\agentchat\conversable_agent.py"", line 348, in send  
    recipient.receive(message, self, request_reply, silent)
  File ""C:\Users\Rohun\Development\AutoGen\env\lib\site-packages\autogen\agentchat\conversable_agent.py"", line 481, in receive
    reply = self.generate_reply(messages=self.chat_messages[sender], sender=sender)
  File ""C:\Users\Rohun\Development\AutoGen\env\lib\site-packages\autogen\agentchat\conversable_agent.py"", line 906, in generate_reply
    final, reply = reply_func(self, messages=messages, sender=sender, config=reply_func_tuple[""config""])
  File ""C:\Users\Rohun\Development\AutoGen\env\lib\site-packages\autogen\agentchat\conversable_agent.py"", line 625, in generate_oai_reply
    response = client.create(
  File ""C:\Users\Rohun\Development\AutoGen\env\lib\site-packages\autogen\oai\client.py"", line 247, in create
    response = self._completions_create(client, params)
  File ""C:\Users\Rohun\Development\AutoGen\env\lib\site-packages\autogen\oai\client.py"", line 327, in _completions_create    
    response = completions.create(**params)
  File ""C:\Users\Rohun\Development\AutoGen\env\lib\site-packages\openai\_utils\_utils.py"", line 299, in wrapper
    return func(*args, **kwargs)
TypeError: create() got an unexpected keyword argument 'api_type'

I am unsure what the error above is referring as I looked on the autogen github link and it shows them using 'api_type' multiple times in the config list. How to resolve this?
","the api_type was removed recently from autogen to retain compatibility with the openai api.
see autogen roadmap
",large-language-model
how to add eos when training t5,"I'm a little puzzled where (and if) EOS tokens are being added when using Huggignface's trainer classes to train a T5 (LongT5 actually) model.
The data set contains pairs of text like this:



from
to




some text
some corresponding text


some other text
some other corresponding text



The tokenizer has been custom trained:
tokenizer = SentencePieceUnigramTokenizer()
tokenizer.train_from_iterator(iterator=iterator, vocab_size=32_128, show_progress=True, unk_token=""<unk>"")

and is loaded like this:
tokenizer = T5TokenizerFast(tokenizer_file=""data-rb-25000/tokenizer.json"",  
                            padding=True, bos_token=""<s>"", 
                            eos_token=""</s>"",unk_token=""<unk>"", 
                            pad_token=""<pad>"")

Before training, the data set is tokenized and examples that have a too high token count are filtered out, like so:
MAX_SEQUENCE_LENGTH = 16_384 / 2

def preprocess_function(examples):
    inputs = tokenizer(
        examples['from'],
        truncation=False,  # Don't truncate yet
        padding=False,     # Don't pad yet
        return_length=True,
    )
    labels = tokenizer(
        examples['to'],
        truncation=False,
        padding=False,
        return_length=True,
    )

    inputs[""input_length""] = inputs[""length""]
    inputs[""labels""] = labels[""input_ids""]
    inputs[""label_length""] = labels[""length""]

    inputs.pop(""length"", None)

    return inputs

tokenized_data = dataset.map(preprocess_function, batched=True, remove_columns=dataset[""train""].column_names)

def filter_function(example):
    return example['input_length'] <= MAX_SEQUENCE_LENGTH and example['label_length'] <= MAX_SEQUENCE_LENGTH

filtered_data = tokenized_data.filter(filter_function)

Training is done like this:
from transformers import DataCollatorForSeq2Seq

data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=""google/long-t5-tglobal-base"")

from transformers import AutoModelForSeq2SeqLM, AutoConfig

config = AutoConfig.from_pretrained(
    ""google/long-t5-tglobal-base"",
    vocab_size=len(tokenizer),
    pad_token_id=tokenizer.pad_token_id,
    eos_token_id=tokenizer.eos_token_id,
    decoder_start_token_id=tokenizer.pad_token_id,
)

model = AutoModelForSeq2SeqLM.from_config(config)

from transformers import GenerationConfig

generation_config = GenerationConfig.from_model_config(model.config)
generation_config._from_model_config = False
generation_config.max_new_tokens = 16_384

from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments

training_args = Seq2SeqTrainingArguments(
    output_dir=""rb-25000-model"",
    eval_strategy=""epoch"",
    save_strategy=""epoch"",
    learning_rate=2e-5,
    per_device_train_batch_size=1,
    per_device_eval_batch_size=1,
    gradient_accumulation_steps=16,
    gradient_checkpointing=True,
    weight_decay=0.01,
    save_total_limit=3,
    num_train_epochs=5,
    logging_steps=1,
    predict_with_generate=True,
    load_best_model_at_end=True,
    bf16=True,
)

trainer = Seq2SeqTrainer(
    model=model,
    args=training_args,
    train_dataset=filtered_data[""train""],
    eval_dataset=filtered_data[""test""],
    tokenizer=tokenizer,
    data_collator=data_collator,
    compute_metrics=compute_metrics,
    generation_config=generation_config,
)

trainer.train()

I know that the tokenizer doesn't add the EOS token:
inputs = tokenizer(['Hello world', 'Hello'], padding=True, truncation=True, max_length=100, return_tensors=""pt"")
labels = inputs[""input_ids""]

print(labels)
print(tokenizer.convert_tokens_to_ids(['<s>'])[0])
print(tokenizer.convert_tokens_to_ids(['<pad>'])[0])
print(tokenizer.convert_tokens_to_ids(['<unk>'])[0])
print(tokenizer.convert_tokens_to_ids(['</s>'])[0])

print(tokenizer.convert_ids_to_tokens([1]))

Output:
tensor([[1, 10356, 1, 5056],
        [1, 10356, 16002, 16002]])
16000
16002
0
16001
['▁']

(I don't really understand what's that strange token with index 1.
Anyway, I was wondering if the Trainer class or the DataCollator actually adds the EOS. I did not find any examples online of how and where to add EOS.
I suspect it's not there, because after training the model it doesn't stop generating until it reaches max_new_tokens (set to pretty high).
What's the best practice here? Where should I add EOS? Is there anything else about this code that should be checked or that looks weird for more experienced eyes?
","The T5 tokenizer should end sequences by EOS token by default. Pretrained T5 tokenizer on HuggingFace does that by default. In fact, I found the function that is responsible for that in the source code on line 256:
def _add_eos_if_not_present(self, token_ids: List[int]) -> List[int]:
        """"""Do not add eos again if user already added it.""""""
        if len(token_ids) > 0 and token_ids[-1] == self.eos_token_id:
            warnings.warn(
                f""This sequence already has {self.eos_token}. In future versions this behavior may lead to duplicated""
                "" eos tokens being added.""
            )
            return token_ids
        else:
            return token_ids + [self.eos_token_id]

If EOS token is not appended by default, you can add a post processor to your tokenizer using TemplateProcessing:
from tokenizers.processors import TemplateProcessing

tokenizer._tokenizer.post_processor = TemplateProcessing(
    single=""$A </s>"",
    pair=""$A </s> $B </s>"",
    special_tokens=[(""</s>"", tokenizer.eos_token_id)]
)

inputs = tokenizer(['Hello world', 'Hello'], padding=True, truncation=True, max_length=100, return_tensors=""pt"")
labels = inputs[""input_ids""]
print(labels)

This should give:
tensor([[1, 10356, 1, 5056, 16001],
        [1, 10356, 16001, 16002, 16002]])

",huggingface-transformers
how to use wasm tiktoken in react webpack js,"The build issues due to wasm instantiate or wasm not being imported.
wasm.__wbindgen_add_to_stack_pointer is not a function
","To use tiktoken wasm in react for better performance, follow this for webpack 5.
In webpack config override add the following. You can directly use wasm from js cdn as well. Skip this step if you want to use cdn:
module.exports = (config, env) => {

  config.experiments = {
    asyncWebAssembly: true,
    layers: true,
  };

}

To use tiktoken with config override:
import { get_encoding, init } from 'tiktoken/init';
import wasm from 'tiktoken/tiktoken_bg.wasm';

  let tikInit = false;
  const countTokens= (str) =>{
    if (!tikInit) {
    tikInit = true;

      const wasmFile = await fetch(wasm);
      const buffer = await wasmFile.arrayBuffer();
      await init((imports) => WebAssembly.instantiate(buffer, imports));
    }

    const e = get_encoding('cl100k_base');
    const t = e.encode(str);
    e.free();
    return t.length;
  }

To use tiktoken with js cdn for wasm and without config override:
import { get_encoding, init } from 'tiktoken/init';
import wasm from 'tiktoken/tiktoken_bg.wasm';

  let tikInit = false;
  const countTokens= (str) =>{
    if (!tikInit) {
    tikInit = true;
}
   const wasmFile = await fetch(
      'https://cdn.jsdelivr.net/npm/tiktoken@1.0.15/tiktoken_bg.wasm'
    );
 
      const buffer = await wasmFile.arrayBuffer();
      await init((imports) => WebAssembly.instantiate(buffer, imports));
    }

    const e = get_encoding('cl100k_base');
    const t = e.encode(str);
    e.free();
    return t.length;
  }

",chatgpt
how to update pandas dataframe column using string concatenation in function,"I have a dataframe where I would like to add a full address column, which would be the combination of 4 other columns (street, city, county, postalcode) from that dataframe. Example output of the address column would be:
5 Test Street, Worthing, West Sussex, RH5 3BX

Or if the city was empty as an example:
5 Test Street, West Sussex, RH5 3BX

This is my code, which after testing I see I might need to use something like apply, but I can't workout how to do it.
def create_address(street: str, city: str, county: str, postalcode: str) -> str:
    
    list_address = []
    
    if street:
        list_address.append(street)
    if city:
        list_address.append(city)
    if county:
        list_address.append(county)
    if postalcode:
        list_address.append(postalcode)

    address = "", "".join(list_address).rstrip("", "")

    return address

df[""address""] = create_address(df[""Street""], df[""City""], df[""County""], df[""PostalCode""])

","you can use lambda and apply to get the concatenated of full address
Example input
EDIT : postalcode with None
data = {
'street': ['street1', 'street2', 'street3'],
'city': ['city1', '', 'city2'],
'county': ['county1', 'county2', 'county3'],
'postalcode': ['postalcode1', 'postalcode2', '']
}

Sample code
df['full_address'] = df.apply(
    lambda row: ', '.join(filter(None, [row['street'], row['city'], row['county'], row['postalcode']])),
    axis=1
)

None is used as a filter so that unavailable elements are removed.
Output
    street   city   county   postalcode                          full_address
0  street1  city1  county1  postalcode1  street1, city1, county1, postalcode1
1  street2         county2  postalcode2         street2, county2, postalcode2
2  street3  city2  county3                            street3, city2, county3

",pandas
get paths of saved checkpoints from pytorchlightning modelcheckpoint,"I am using PytorchLightning and a ModelCheckpoint which saves models with a formatted filename like filename=""model_{epoch}-{val_acc:.2f}""
I then want to load these checkpoints again, for simplicity I want the best from save_top_k=N.
As the filename is dynamic I wonder how can I retrieve the checkpoint files easily.
Is there a built-in attribute in the ModelCheckpoint or the trainer that gives me the saved checkpoints?
For example like
checkpoint_callback.get_top_k_paths()


I know I can do it with glob and model_dir, as I assume that callback keeps track of them anyway I wonder if there is a solution built in already and how to use it.
","-> All stored checkpoints can be found in ModelCheckpoint.best_k_models : Dict[str, Tensor] where the keys are the paths and the values the metric that is tracked.
Additionally does ModelCheckpoint have these attributes:
best_model_path
best_model_score, kth_best_model_path, kth_value, last_model_path and best_k_models.

Note: when loading a checkpoint
These values are only guaranteed when model_checkpoint.dirpath matches the one in in the checkpoints_state_dict[""dirpath""], i.e. you did not change the directory, otherwise only best_model_path is restored.
Otherwise as Aniket Maurya states you have to look at dirpath or the parallel files in best_model_path.
",pytorch
pandas dfitertuples renaming dataframe columns when printing,"I know that normally pandas' itertuples() will return the values of each including the column names as follows:
ab=pd.DataFrame(np.random.random([3,3]),columns=['hi','low','med'])
for i in ab.itertuples():
    print(i)

and the output is as follows:
Pandas(Index=0, hi=0.05421443, low=0.2456833, med=0.491185)
Pandas(Index=1, hi=0.28670429, low=0.5828551, med=0.279305)
Pandas(Index=2, hi=0.53869406, low=0.3427290, med=0.750075)

However, I have no idea why it doesn't shows the columns as I expected for my another set of code as below:
            us qqq equity  us spy equity
date                                    
2017-06-19            0.0            1.0
2017-06-20            0.0           -1.0
2017-06-21            0.0            0.0
2017-06-22            0.0            0.0
2017-06-23            1.0            0.0
2017-06-26            0.0            0.0
2017-06-27           -1.0            0.0
2017-06-28            1.0            0.0
2017-06-29           -1.0            0.0
2017-06-30            0.0            0.0

the above is a Pandas Dataframe with Timestamp as index, float64 as the values in the list, and a list of string ['us qqq equity','us spy equity'] as the columns.
When I do this:
for row in data.itertuples():
    print (row)

It shows the columns as _1 and _2 as follows:
Pandas(Index=Timestamp('2017-06-19 00:00:00'), _1=0.0, _2=1.0)
Pandas(Index=Timestamp('2017-06-20 00:00:00'), _1=0.0, _2=-1.0)
Pandas(Index=Timestamp('2017-06-21 00:00:00'), _1=0.0, _2=0.0)
Pandas(Index=Timestamp('2017-06-22 00:00:00'), _1=0.0, _2=0.0)
Pandas(Index=Timestamp('2017-06-23 00:00:00'), _1=1.0, _2=0.0)
Pandas(Index=Timestamp('2017-06-26 00:00:00'), _1=0.0, _2=0.0)
Pandas(Index=Timestamp('2017-06-27 00:00:00'), _1=-1.0, _2=0.0)
Pandas(Index=Timestamp('2017-06-28 00:00:00'), _1=1.0, _2=0.0)
Pandas(Index=Timestamp('2017-06-29 00:00:00'), _1=-1.0, _2=0.0)
Pandas(Index=Timestamp('2017-06-30 00:00:00'), _1=0.0, _2=0.0)

Does anyone has any clue about what have I done wrong? Does it have to do with some variable referencing issue when creating the original dataframe? (Also, as a side question, I learnt from the community that the type of data generated from itertuples() should be tuples, but it seems (as shown above), the return type is  as I verified from the type statement?)
Thank you for all your patience as I am still trying to master the application of DataFrame. 
","This seems to be an issue with handling column names having spaces in them. If you replace the column names with different ones without spaces, it will work:
df.columns = ['us_qqq_equity', 'us_spy_equity'] 
# df.columns = df.columns.str.replace(r'\s+', '_', regex=True)  # Courtesy @MaxU  
for r in df.head().itertuples():
    print(r)

# Pandas(Index='2017-06-19', us_qqq_equity=0.0, us_spy_equity=1.0)
# Pandas(Index='2017-06-20', us_qqq_equity=0.0, us_spy_equity=-1.0)
# ...

Column names with spaces cannot effectively be represented in named tuples, so they are renamed automatically when printing.
",pandas
check the difference in pretrained and finetuned model,"So I am finetuning a pretrained LLaMa2 model. I want to check if the model that I have finetuned is different from the original. I want to check the difference between base_model and model. Is there a way to check if there is a difference in weights or parameters after training?
from google.colab import drive

# Mount Google Drive
drive.mount('/content/drive')

# Path to your saved model in Google Drive
model_path_in_drive = '/content/drive/MyDrive/Mod/llama-2-7b-miniguanaco'

# Reload model in FP16 and merge it with LoRA weights
base_model = AutoModelForCausalLM.from_pretrained(
    model_name,
    low_cpu_mem_usage=True,
    return_dict=True,
    torch_dtype=torch.float16,
    device_map=device_map,
)

# Load your PeftModel from the saved checkpoint in Google Drive
model = PeftModel.from_pretrained(base_model, model_path_in_drive)
model = model.merge_and_unload()
#mark_only_lora_as_trainable(lora_model)
# Reload tokenizer to save it
tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
tokenizer.pad_token = tokenizer.eos_token
tokenizer.padding_side = ""right""

I tried some methods in this article but it didn't help me at all.
","Just loop over the parameters and compare them with torch.allclose. I used DistilBertModel for the answer, please use the respective classes from your example that are also mentioned in the comments:
import torch
from transformers import  DistilBertModel

# AutoModelForCausalLM in your case
base_model =  DistilBertModel.from_pretrained(""distilbert/distilbert-base-uncased"")
# PeftModel.merge_and_unload() in your case
finetuned_model = DistilBertModel.from_pretrained(""distilbert/distilbert-base-uncased-finetuned-sst-2-english"")

for base_param, finetuned_param in zip(base_model.named_parameters(), finetuned_model.named_parameters()):
  if not torch.allclose(base_param[1], finetuned_param[1]):
    print(base_param[0])

Output:
embeddings.word_embeddings.weight
embeddings.position_embeddings.weight
embeddings.LayerNorm.weight
embeddings.LayerNorm.bias
transformer.layer.0.attention.q_lin.weight
transformer.layer.0.attention.q_lin.bias
transformer.layer.0.attention.k_lin.weight
transformer.layer.0.attention.k_lin.bias
transformer.layer.0.attention.v_lin.weight
transformer.layer.0.attention.v_lin.bias
transformer.layer.0.attention.out_lin.weight
transformer.layer.0.attention.out_lin.bias
transformer.layer.0.sa_layer_norm.weight
transformer.layer.0.sa_layer_norm.bias
transformer.layer.0.ffn.lin1.weight
transformer.layer.0.ffn.lin1.bias
transformer.layer.0.ffn.lin2.weight
transformer.layer.0.ffn.lin2.bias
transformer.layer.0.output_layer_norm.weight
transformer.layer.0.output_layer_norm.bias
...
transformer.layer.5.attention.q_lin.weight
transformer.layer.5.attention.q_lin.bias
transformer.layer.5.attention.k_lin.weight
transformer.layer.5.attention.k_lin.bias
transformer.layer.5.attention.v_lin.weight
transformer.layer.5.attention.v_lin.bias
transformer.layer.5.attention.out_lin.weight
transformer.layer.5.attention.out_lin.bias
transformer.layer.5.sa_layer_norm.weight
transformer.layer.5.sa_layer_norm.bias
transformer.layer.5.ffn.lin1.weight
transformer.layer.5.ffn.lin1.bias
transformer.layer.5.ffn.lin2.weight
transformer.layer.5.ffn.lin2.bias
transformer.layer.5.output_layer_norm.weight
transformer.layer.5.output_layer_norm.bias

",llama
how to calculate daily weights which satisfy certain conditions,"I have the following pandas dataframe which represents the consumption of 7 days (day_0 is today, day_-1 is yesterday etc) of 10 people (ids):
import pandas as pd
import numpy as np

df = pd.DataFrame(np.random.randint(8, 15, size=(10, 7)))
df.columns = ['day_0', 'day_-1', 'day_-2', 'day_-3', 'day_-4', 'day_-5', 'day_-6']
df.index.name = 'id'

print(df.reset_index())

   id  day_0  day_-1  day_-2  day_-3  day_-4  day_-5  day_-6
0   0    10      10      14       8      14      14      14
1   1    10      13      11      11       8      10      10
2   2    10      12       9      12       9      10      10
3   3    12      12       9      11       9      12      13
4   4    12      13       8      12       8      11       9
5   5    13       9       8      13       9      12      10
6   6     8       9       8      14       8      13      14
7   7    13      10      14      12       8       9      11
8   8     8       8      10      12      11      14      14
9   9    14      13      13       9      11      14      13

I would like to find daily weights (so in total 7 weights: w_0, w_-1, w_-2, w_-3, w_-4, w_-5, w_-6) which need to have the following properties:

w_0 > w_-1 > w_-2 > ... > w_-6 > 0
w_0 + w_-1 + w_-2 + ... + w_-6 = 7
the weighted average for exactly 7 out of 10 ids to be below a threshold (e.g. 11)

I can achieve prerequisites 1 & 2 by using the exponential decay function and later normalizing:
import numpy as np

n = 7

_lambda = 0.5

# Calculate the weights using exponential decay
weights = np.exp(-_lambda * np.arange(n))

# Normalize the weights so that their sum is equal to the length of the time series
weights *= n / np.sum(weights)

But I don't know how I could apply also prerequisite 3.
Is that possible? How can I do that in python?
","This does not use exponential decay, because that doesn't seem particularly useful to meet your requirements. Define an ILP with a disjunction constraint that at exactly one combination of n out of m IDs has a weighted mean less than or equal to a threshold:
import io

import numpy as np
import pandas as pd
import scipy.sparse
from scipy.optimize import milp, Bounds, LinearConstraint

with io.StringIO(
'''id,  0, -1, -2, -3, -4, -5, -6
    0, 10, 10, 14,  8, 14, 14, 14
    1, 10, 13, 11, 11,  8, 10, 10
    2, 10, 12,  9, 12,  9, 10, 10
    3, 12, 12,  9, 11,  9, 12, 13
    4, 12, 13,  8, 12,  8, 11,  9
    5, 13,  9,  8, 13,  9, 12, 10
    6,  8,  9,  8, 14,  8, 13, 14
    7, 13, 10, 14, 12,  8,  9, 11
    8,  8,  8, 10, 12, 11, 14, 14
    9, 14, 13, 13,  9, 11, 14, 13
''') as f:
    df = pd.read_csv(f, skipinitialspace=True, index_col=0)
df.columns = pd.Index(name='day', data=df.columns.astype(int))


m, n = df.shape  # number of IDs, days

'''
LP variables:
n weights
m weighted mean threshold binary predicates
'''

# The weight sum must be equal to n
sum_constraint = LinearConstraint(
    A=np.concatenate((
        np.ones(n), np.zeros(m),
    )),
    lb=n, ub=n,
)

# The weights must be strictly decreasing by this amount
min_decrease = 1e-2  # chosen fully arbitrarily
antimonotonic_constraint = LinearConstraint(
    A=scipy.sparse.diags_array(
        (
            np.ones(shape=n - 1),
            np.full(shape=n - 1, fill_value=-1),
        ),
        offsets=(0, 1), shape=(n - 1, n + m), format='csc',
    ),
    lb=min_decrease,
)

'''
For each binary threshold predicate,
pred = 1 iff weights.df_values/n <= threshold
lower bound:
pred >= 1 - (weights.values)/n/threshold
weights.values/threshold + pred*n >= n
upper bound:
pred <= 2 - (weights.values)/n/threshold
weights.values/threshold + pred*n <= 2*n
'''
threshold = 11
mean_constraint = LinearConstraint(
    A=scipy.sparse.hstack(
        (
            df.values/threshold,
            scipy.sparse.diags_array(
                np.full(shape=m, fill_value=n),
            ),
        ),
        format='csc',
    ),
    lb=n, ub=2*n,
)

# Exactly this many out of m IDs must be at the threshold or lower
n_up_to_threshold = 6
disjunction_constraint = LinearConstraint(
    A=np.concatenate((np.zeros(n), np.ones(m))),
    lb=n_up_to_threshold, ub=n_up_to_threshold,
)

result = milp(
    c=np.zeros(n + m),  # no optimisation objective
    integrality=np.concatenate((
        np.zeros(shape=n, dtype=np.uint8),  # weights are continuous
        np.ones(shape=m, dtype=np.uint8),   # predicates are binary
    )),
    bounds=Bounds(
        lb=np.concatenate((
            np.full(shape=n, fill_value=1e-2),  # minimum weight, arbitrary
            np.zeros(shape=m),  # binary predicate
        )),
        ub=np.concatenate((
            np.full(shape=n, fill_value=np.inf),
            np.ones(shape=m),  # binary predicate
        )),
    ),
    constraints=(
        sum_constraint,
        antimonotonic_constraint,
        mean_constraint,
        disjunction_constraint,
    ),
)
if not result.success:
    raise ValueError(result.message)

weights, threshold_preds = np.split(result.x, (n,))
means = df @ (weights/n)

print('weights =')
print(weights)
print('threshold predicates =')
print(threshold_preds)
print('means =')
print(means)

weights =
[2.96714286 1.01571429 1.00571429 0.99571429 0.98571429 0.02
 0.01      ]
threshold predicates =
[1. 1. 1. 0. 1. 0. 1. 0. 1. 0.]
means =
id
0    10.870612
1    10.439592
2    10.290204
3    11.005714
4    11.000000
5    11.130816
6     9.021429
7    11.847755
8     9.304490
9    12.576122
dtype: float64

Approximate solutions
To turn the hard threshold constraint into a soft constraint where the threshold moves to accommodate the input system,
import io

import numpy as np
import pandas as pd
import scipy.sparse
from scipy.optimize import milp, Bounds, LinearConstraint

with io.StringIO(
'''id,  0, -1, -2, -3, -4, -5, -6
    0, 10, 10, 14,  8, 14, 14, 14
    1, 10, 13, 11, 11,  8, 10, 10
    2, 10, 12,  9, 12,  9, 10, 10
    3, 12, 12,  9, 11,  9, 12, 13
    4, 12, 13,  8, 12,  8, 11,  9
    5, 13,  9,  8, 13,  9, 12, 10
    6,  8,  9,  8, 14,  8, 13, 14
    7, 13, 10, 14, 12,  8,  9, 11
    8,  8,  8, 10, 12, 11, 14, 14
    9, 14, 13, 13,  9, 11, 14, 13
''') as f:
    df = pd.read_csv(f, skipinitialspace=True, index_col=0)
df.columns = pd.Index(name='day', data=df.columns.astype(int))


m, n = df.shape  # number of IDs, days

'''
LP variables:
n weights
m weighted mean threshold binary predicates
1 threshold
1 threshold error
'''

# The weight sum must be equal to n
sum_constraint = LinearConstraint(
    A=np.concatenate((
        np.ones(n),           # weights
        np.zeros(m + 1 + 1),  # preds, threshold, error
    )),
    lb=n, ub=n,
)

# The weights must be strictly decreasing by this amount
min_decrease = 1e-3  # chosen fully arbitrarily
antimonotonic_constraint = LinearConstraint(
    A=scipy.sparse.diags_array(
        (
            np.ones(shape=n - 1),                 # current weight
            np.full(shape=n - 1, fill_value=-1),  # next weight
        ),
        offsets=(0, 1),                # current and next weight index
        shape=(n - 1, n + m + 1 + 1),  # after the weight positions, everything is zero
        format='csc',
    ),
    lb=min_decrease,
)

'''
For each binary threshold predicate,
pred = 1 iff weights.df_values/n <= threshold
Upper bound (forces predicate to 0):
weights.df_values + pM - n*threshold <= M
Lower bound (forces predicate to 1):
weights.df_values + pM - n*threshold >= 0
'''
M = 2*df.sum(axis=1).max()
threshold_constraint = LinearConstraint(
    A=scipy.sparse.hstack(
        (
            df.values,                 # weights
            scipy.sparse.diags_array(  # predicates
                np.full(shape=m, fill_value=M),
            ),
            np.full(shape=(m, 1), fill_value=-n),  # threshold
            scipy.sparse.csc_array((m, 1)),        # error (0)
        ),
        format='csc',
    ),
    lb=0, ub=M,
)

# Exactly this many out of m IDs must be at the threshold or lower
n_up_to_threshold = 5
count_constraint = LinearConstraint(
    A=np.concatenate((
        np.zeros(n),      # weights
        np.ones(m),       # predicates
        np.zeros(1 + 1),  # threshold, error
    )),
    lb=n_up_to_threshold, ub=n_up_to_threshold,
)

'''
The absolute error must be relative to the threshold
error >= target - threshold    threshold + error >= target
error >= threshold - target    threshold - error <= target
'''
target_threshold = 12
error_constraint = LinearConstraint(
    A=scipy.sparse.hstack(
        (
            scipy.sparse.csc_array((2, n + m)),  # weights, predicates
            np.array((
                (1, +1),  # threshold, error
                (1, -1),
            )),
        ),
        format='csc',
    ),
    lb=(target_threshold, -np.inf),
    ub=(np.inf, target_threshold),
)

result = milp(
    c=np.concatenate((
        np.zeros(n + m + 1),  # weights, preds, threshold are non-optimised
        np.ones(1),           # threshold error is minimised
    )),
    integrality=np.concatenate((
        np.zeros(shape=n, dtype=np.uint8),  # weights are continuous
        np.ones(shape=m, dtype=np.uint8),   # predicates are binary
        (0, 0),                             # threshold, error are continuous
    )),
    bounds=Bounds(
        lb=np.concatenate((
            np.full(shape=n, fill_value=1e-3),  # minimum weight, arbitrary
            np.zeros(shape=m),                  # binary predicate
            (-np.inf, -np.inf),                 # threshold, error unbounded
        )),
        ub=np.concatenate((
            np.full(shape=n, fill_value=np.inf),  # weight unbounded
            np.ones(shape=m),                     # binary predicate
            (np.inf, np.inf),                     # threshold, error unbounded
        )),
    ),
    constraints=(
        sum_constraint,
        antimonotonic_constraint,
        threshold_constraint,
        count_constraint,
        error_constraint,
    ),
)
if not result.success:
    raise ValueError(result.message)

weights, preds, (threshold,), (error,) = np.split(result.x, (n, n+m, n+m+1))
means = df @ (weights/n)

print(f'threshold = {threshold:.3f}')
print(f'error from target of {target_threshold} = {error:.6f}')
print('weights =')
print(weights)
print('threshold predicates =')
print(preds)
print('means =')
print(means.values)

threshold = 11.996
error from target of 12 = 0.003857
weights =
[6.975e+00 1.000e-02 5.000e-03 4.000e-03 3.000e-03 2.000e-03 1.000e-03]
threshold predicates =
[1. 1. 1. 0. 0. 0. 1. 0. 1. 0.]
means =
[10.00514286 10.00471429 10.00285714 11.99614286 11.99614286 12.98828571
  8.00714286 12.99228571  8.00757143 13.99357143]

",numpy
parallel querying indices for a list of filter expressions in polars dataframe,"I want to get the indices for a list of filters in polars and get a sparse matrix from it, how can I parallel the process? This is what I have right now, a pretty naive and brute force way for achieving what I need, but this is having some serious performance issue
def get_sparse_matrix(exprs: list[pl.Expr]) -> scipy.sparse.csc_matrix:
    df = df.with_row_index('_index')
    rows: list[int] = []
    cols: list[int] = []
    for col, expr in enumerate(exprs):
        r = self.df.filter(expr)['_index']
        rows.extend(r)
        cols.extend([col] * len(r))

    X = csc_matrix((np.ones(len(rows)), (rows, cols)), shape= 
   (len(self.df), len(rules)))

    return X

Example Input:
# df is a polars dataframe with size 8 * 3
df = pl.DataFrame(
[[1,2,3,4,5,6,7,8], 
[3,4,5,6,7,8,9,10], 
[5,6,7,8,9,10,11,12],
[5,6,41,8,21,10,51,12],
])

# three polars expressions
exprs = [pl.col('column_0') > 3, pl.col('column_1') < 6, pl.col('column_4') > 11]

Example output:
X is a sparse matrix of size 8 (number of records) X 3 (number of expressions), where the element at i,j equals to 1 if ith record matches the jth expression
","So I am not completely sure what exactly you want, but I hope that satisfies your needs
import polars as pl
from scipy.sparse import csc_matrix
import numpy as np

df = pl.DataFrame(
    [[1,2,3,4,5,6,7,8], 
    [3,4,5,6,7,8,9,10], 
    [5,6,7,8,9,10,11,12],
    [5,6,41,8,21,10,51,12],
])


exprs = [(pl.col('column_0') > 3).cast(pl.Int8), 
         (pl.col('column_1') < 6).cast(pl.Int8), 
         (pl.col('column_3') > 11).cast(pl.Int8)]

X = df.select(exprs)
csc_matrix(X.to_numpy())

",data-science
eslint prettier parsing error unexpected token with,"I am fairly new to ESLint, Prettier, and Babel; still learning how they work together.  I am using Visual Studio Code 1.96.4, with ESLint and Prettier extensions.
In one of my javascript files, I have an import statement with a type assertion.  For example:
import * as manifest from './package.json' with { type: 'json' };
                                           ~~~~

My IDE is telling me that ESLint has an issue with the above import statement:
Parsing error: Unexpected token with | eslint

I have an eslint.config.js file, which exports an eslint.Linter.Config array that specifies my own config object (which has language options and rules), followed by a recommended config from @eslint/js, and the ESLint Prettier plugin recommended config.
import pluginJs from '@eslint/js';
import eslintPluginPrettier from 'eslint-plugin-prettier/recommended';
import globals from 'globals';

/** @type {import('eslint').Linter.Config[]} */
export default [
  {
    languageOptions: {
      globals: {
        ...globals.browser,
        ...globals.jest
      },
      ecmaVersion: 2024,
      sourceType: 'module'
    },
    rules: {
      ...
    }
  },
  pluginJs.configs.recommended,
  eslintPluginPrettier    // <-- Removing this fixes the problem, but then am I still running Prettier at lint-time?
];

These are the installed packages that (I think?) are relevant:
""devDependencies"": {
  ""@babel/core"": ""^7.26.0"",
  ""@babel/eslint-parser"": ""^7.26.5"",
  ""@babel/plugin-syntax-import-assertions"": ""^7.26.0"",
  ...
  ""@eslint/eslintrc"": ""^3.2.0"",
  ""@eslint/js"": ""^9.18.0"",
  ...
  ""eslint"": ""^9.18.0"",
  ""eslint-config-prettier"": ""^10.0.1"",
  ""eslint-plugin-prettier"": ""^5.2.3"",
  ...
  ""prettier"": ""^3.4.2"",
  ...
},

I also have a .prettierrc file that is a simple JSON object with only rules in it, no plugins or anything.  And I don't know if .babelrc has anything to do with this?
How can I make the IDE error go away, while still enforcing both ESLint and Prettier rules?
","You've already installed @babel/plugin-syntax-import-assertions, but you also need to make ESLint use @babel/eslint-parser with the correct configuration:
import pluginJs from '@eslint/js';
import eslintPluginPrettier from 'eslint-plugin-prettier/recommended';
import globals from 'globals';

/** @type {import('eslint').Linter.Config[]} */
export default [
  {
    parser: '@babel/eslint-parser',  // Set Babel as the parser
    parserOptions: {
      requireConfigFile: false,  // Prevents needing a separate Babel config file
      babelOptions: {
        plugins: ['@babel/plugin-syntax-import-assertions'],  // Ensure the plugin is enabled
      },
    },
    languageOptions: {
      globals: {
        ...globals.browser,
        ...globals.jest,
      },
      ecmaVersion: 2024,
      sourceType: 'module',
    },
    rules: {
      // Your custom rules
    },
  },
  pluginJs.configs.recommended,
  eslintPluginPrettier,  // Ensures Prettier runs as well
];

",javascript
gradle will not build in command prompt gradle v,"Gradle shows up in echo %PATH% no problem.. 
It has has it's own directory in Program Files, however when I attempt:
gradle -v .. 

It responds: 'gradle' is not recognised as an internal or external command, operable program or batch file..
I am trying to get this to work in conjunction with an Eclipse Simpleservlet project..
","I just solved the problem. My PATH variable value was like: 

path1;path2,path3,gradlebinpath

I replaced , with ; and added a ; at the end of the value and it just worked. The final value looked like:

path1;path2;path3;gradlebinpath;

",prompt
attributeerror the layer has never been called and thus has no defined output shape,"I am trying to define a model happyModel()
# GRADED FUNCTION: happyModel

def happyModel():
    """"""
    Implements the forward propagation for the binary classification model:
    ZEROPAD2D -> CONV2D -> BATCHNORM -> RELU -> MAXPOOL -> FLATTEN -> DENSE

Note that for simplicity and grading purposes, you'll hard-code all the values
such as the stride and kernel (filter) sizes. 
Normally, functions should take these values as function parameters.

Arguments:
None

Returns:
model -- TF Keras model (object containing the information for the entire training process) 
""""""
model = tf.keras.Sequential(
    [
        ## ZeroPadding2D with padding 3, input shape of 64 x 64 x 3
        tf.keras.layers.ZeroPadding2D(padding=(3,3), data_format=(64,64,3)),
    
        ## Conv2D with 32 7x7 filters and stride of 1            
        tf.keras.layers.Conv2D(32, (7, 7), strides = (1, 1), name = 'conv0'),
        
        ## BatchNormalization for axis 3
        
        tf.keras.layers.BatchNormalization(axis = 3, name = 'bn0'),
        
        ## ReLU            
        tf.keras.layers.Activation('relu'),
        
        ## Max Pooling 2D with default parameters            
        tf.keras.layers.MaxPooling2D((2, 2), name='max_pool0'),
    
        ## Flatten layer            
        tf.keras.layers.Flatten(),
    
        ## Dense layer with 1 unit for output & 'sigmoid' activation            
        tf.keras.layers.Dense(1, activation='sigmoid', name='fc'),
        
        # YOUR CODE STARTS HERE
        
        
        # YOUR CODE ENDS HERE
    ]
)

return model

and following code is for creating the object of this model defined above:
happy_model = happyModel()
# Print a summary for each layer
for layer in summary(happy_model):
    print(layer)
    
output = [['ZeroPadding2D', (None, 70, 70, 3), 0, ((3, 3), (3, 3))],
            ['Conv2D', (None, 64, 64, 32), 4736, 'valid', 'linear', 'GlorotUniform'],
            ['BatchNormalization', (None, 64, 64, 32), 128],
            ['ReLU', (None, 64, 64, 32), 0],
            ['MaxPooling2D', (None, 32, 32, 32), 0, (2, 2), (2, 2), 'valid'],
            ['Flatten', (None, 32768), 0],
            ['Dense', (None, 1), 32769, 'sigmoid']]
    
comparator(summary(happy_model), output)

I got following error:
---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
<ipython-input-67-f33284fd82fe> in <module>
      1 happy_model = happyModel()
      2 # Print a summary for each layer
----> 3 for layer in summary(happy_model):
      4     print(layer)
      5 

~/work/release/W1A2/test_utils.py in summary(model)
     30     result = []
     31     for layer in model.layers:
---> 32         descriptors = [layer.__class__.__name__, layer.output_shape, layer.count_params()]
     33         if (type(layer) == Conv2D):
     34             descriptors.append(layer.padding)

/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py in output_shape(self)
   2177     """"""
   2178     if not self._inbound_nodes:
-> 2179       raise AttributeError('The layer has never been called '
   2180                            'and thus has no defined output shape.')
   2181     all_output_shapes = set(

AttributeError: The layer has never been called and thus has no defined output shape.

I suspect my calling of ZeroPadding2D() is not right. The project seems to require the input shape of ZeroPadding2D() to be 64X64X3. I tried many formats but could not fix the problem. Anyone can give a pointer? Thanks a lot.
","In your model definition, there's an issue with the following layer:
tf.keras.layers.ZeroPadding2D(padding=(3,3), data_format=(64,64,3)),

First, you didn't define any input layer also, the data_format is a string, one of channels_last (default) or channels_first, source. The correct way to define the above model as follows:
def happyModel():
    model = tf.keras.Sequential(
        [
            ## ZeroPadding2D with padding 3, input shape of 64 x 64 x 3
            tf.keras.layers.ZeroPadding2D(padding=(3,3), 
                         input_shape=(64, 64, 3), data_format=""channels_last""),
           ....
           ....


happy_model = happyModel()
happy_model.summary()
Model: ""sequential_2""
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
zero_padding2d_4 (ZeroPaddin (None, 70, 70, 3)         0         
_________________________________________________________________
conv0 (Conv2D)               (None, 64, 64, 32)        4736      
_________________________________________________________________
bn0 (BatchNormalization)     (None, 64, 64, 32)        128       
_________________________________________________________________
activation_2 (Activation)    (None, 64, 64, 32)        0         
_________________________________________________________________
max_pool0 (MaxPooling2D)     (None, 32, 32, 32)        0         
_________________________________________________________________
flatten_16 (Flatten)         (None, 32768)             0         
_________________________________________________________________
fc (Dense)                   (None, 1)                 32769     
=================================================================
Total params: 37,633
Trainable params: 37,569
Non-trainable params: 64

",tensorflow
break out of while loop that has an array inside,"Want to let the user make an array until they press cancel on the prompt
But the nested if statement won't trigger and break the while loop.
The goal of the program is to show the first ten items in the array with the message
'Only the first ten guests will be recorded'
This is part of my course which is why I am letting the user input as much as they like
and then reducing it to 10 after.
let guests = []
i = 1
j = 1

while (j = 1) {
    input = guests.push(prompt('Enter Guest' + i + '(press cancel to stop)'))
    i++

    if(input === null) {
        j++
        //break;
    }
}

if (guests.length > 10) {
    console.log('Only the first ten guests will be recorded')

    while (guests.length > 10) {
        guests.pop()
    }
}

console.log(guests)

If I remove the if() brackets the program stops after the user inputs one array item
And if I try break;
it stops after the user types in 1 array item.
I have also tried adding
    else {
        continue;
    }

but to no avail
","Store the result of prompt to test for null (instead of comparing the return value of Array#push).
while (true) {
    input = prompt('Enter Guest' + i + '(press cancel to stop)');
    if(input === null) break;
    guests.push(input);
    i++;
}

",prompt
how to align toprotated ylabel on the righthand side on a plot with matplotlib,"In matplotlib, how can I get the y-label to align with the top of the plot, when the label is rotated, moved to the right and top?
import matplotlib.pyplot as plt
import numpy as np

#create plot
fig, ax = plt.subplots()
ax.plot(np.sin(np.linspace(-np.pi, np.pi, 1001)))

ax.tick_params(
                    which=""both"",
                    direction=""out"",
                    left=False,
                    right=True,
                    labelleft=False,
                    labelright=True,
                )

ax.set_ylabel(""y-label"", loc=""top"", rotation=-90, labelpad=10)
ax.yaxis.set_label_position(""right"")


","You could fix the horizontal alignment after defining the label (unfortunately, you can't define it directly in set_ylabel if loc is used):
ax.set_ylabel('y-label', loc='top', rotation=-90, labelpad=10)
ax.yaxis.set_label_position('right')
ax.yaxis.label.set_horizontalalignment('left')

Output:

",matplotlib
fastapi runs apicalls in serial instead of parallel fashion,"I have the following code:
import time
from fastapi import FastAPI, Request
    
app = FastAPI()
    
@app.get(""/ping"")
async def ping(request: Request):
        print(""Hello"")
        time.sleep(5)
        print(""bye"")
        return {""ping"": ""pong!""}

If I run my code on localhost—e.g., http://localhost:8501/ping—in different tabs of the same browser window, I get:
Hello
bye
Hello
bye

instead of:
Hello
Hello
bye
bye

I have read about using httpx, but, still, I cannot have a true parallelization. What's the problem?
","As per FastAPI's documentation:

When you declare a path operation function (also known as endpoint) with normal def instead
of async def, it is run in an external threadpool that is then
awaited, instead of being called directly (as it would block the
server).

also, as described here:

If you are using a third party library that communicates with
something (a database, an API, the file system, etc.) and doesn't have
support for using await, (this is currently the case for most
database libraries), then declare your path operation functions as
normally, with just def.
If your application (somehow) doesn't have to communicate with
anything else and wait for it to respond, use async def.
If you just don't know, use normal def.
Note: You can mix def and async def in your path operation functions as much as you need and define each one using the best
option for you. FastAPI will do the right thing with them.
Anyway, in any of the cases above, FastAPI will still work
asynchronously and be extremely fast.
But by following the steps above, it will be able to do some
performance optimizations.

Thus, in order to avoid blocking the server, a def endpoint—in the context of asynchronous programming, a function defined with just def is called synchronous function—in FastAPI, will still run in the event loop, but instead of calling it directly, FastAPI will run a def endpoint in a separate thread from an external threadpool that is then awaited (more details on the external threadpool are given later on), and hence, FastAPI will still work asynchronously. In other words, the server will process requests to such endpoints concurrently (at the cost, though, of spawning a new thread or reusing an existing thread from the threadpool, for every incoming request to such endpoints). Whereas, async def endpoints run directly in the event loop—which runs in a single thread, typically the main thread of a process/worker, and, in this case, is created when calling, for instance, uvicorn.run(), or the equivalent method of some other ASGI server—that is, the server will also process requests to such endpoints concurrently/asynchronously, as long as there is an await call to non-blocking I/O-bound operations inside such async def endpoints/routes, such as waiting for (1) data from the client to be sent through the network, (2) contents of a file in the disk to be read, (3) a database operation to finish, etc., (have a look here).
However, if an endpoint defined with async def does not await for some coroutine inside (i.e., a coroutine object is the result of calling an async def function), in order to give up time for other tasks in the event loop to run (e.g., requests to the same or other endpoints, background tasks, etc.), each request to such an endpoint will have to be completely finished (i.e., exit the endpoint), before returning control back to the event loop and allowing other tasks in the event loop to run (see this answer, if you would like to get and monitor all pending tasks in an event loop). In other words, in such cases, the server would be ""blocked"", and hence any requests would be processed sequentially.
Having said that, you should still consider defining an endpoint with async def, if your endpoint does not have to execute a blocking operation inside that has to wait for it to respond, but is instead used to return simple JSON data, a simple HTMLResponse (see FastAPI docs as well) or even a FileResponse (in which case the file contents will be read asynchronously and in chunks, regardless, using await anyio.open_file(), as can be seen in the relevant FileResponse class implementation), even if there is not an await statement inside the endpoint in such cases, as FastAPI would likely perform better, when running such a simple endpoint directly in the event loop, rather than running the endpoint in a separate thread from the external threadpool (which would be the case, if the endpoint was instead defined with normal def). If, however, you had to return some complex and large JSON data, either encoding them on your own within the endpoint, as shown in the linked answer earlier, or using Starlette's JSONResponse or FastAPI's ORJSONResponse/UJSONResponse (see this related answer as well), that all these classes would encode the data in a synchronous way, using json.dumps() and orjson.dumps()/ujson.dumps() respectively, in that case, you should might consider having the endpoint defined with normal def (related answers could be found here and here). Otherwise, you could keep using an async def endpoint, but have any blocking operations taking place inside (e.g., orjson.dumps() or df.to_json()) run in a separate thread/process, as described in the solutions provided later on (in such cases, always perform benchmark tests, similar to this answer, and compare the results to find the best-performing approach).
Note that the same concept not only applies to endpoints, but also to functions that are used as StreamingResponse's generators (see StreamingResponse class implementation) or Background Tasks (see BackgroundTask class implementation and this answer), meaning that FastAPI, behind the scenes, will also run such functions defined with normal def in a separate thread from the same external threadpool; whereas, if such functions were defined with async def instead, they would run directly in the event loop. In order to run an endpoint or a function described above in a separate thread and await it, FastAPI uses Starlette's asynchronous run_in_threadpool() function, which, under the hood, calls anyio.to_thread.run_sync(). The default number of worker threads of that external threadpool is 40 and can be adjusted as required—please have a look at this answer for more details on the external threadpool and how to adjust the number of threads. Hence, after reading this answer to the end, you should be able to decide whether you should define a FastAPI endpoint, StreamingResponse's generator or BackgroundTask function with def or async def, as well as, based on the requirements of your project, whether or not you should increase the number of threads of the external threadpool.
Python's async def function and await
The keyword await (which only works within an async def function) passes function control back to the event loop. In other words, it suspends the execution of the surrounding coroutine, and tells the event loop to let some other task run, until that awaited task is completed. Note that just because you may define a custom function with async def and then await it inside your async def endpoint, it doesn't mean that your code will work asynchronously, if that custom function contains, for example, calls to time.sleep(), CPU-bound tasks, non-async I/O libraries, or any other blocking call that is incompatible with asynchronous Python code. In FastAPI, for example, when using the async methods of UploadFile, such as await file.read() and await file.close(), FastAPI/Starlette, behind the scenes, actually calls the corresponding synchronous File methods in a separate thread from the external threadpool described earlier (using run_in_threadpool()) and awaits it; otherwise, such methods/operations would block the event loop—you could find out more by looking at the implementation of the UploadFile class.
Note  that async does not mean parallel, but concurrently. As mentioned earlier, asynchronous code with async and await is many times summarized as using coroutines. Coroutines are collaborative (or cooperatively multitasked), meaning that ""at any given time, a program with coroutines is running only one of its coroutines, and this running coroutine suspends its execution only when it explicitly requests to be suspended"" (see here and here for more info on coroutines).
As described in this article:

Specifically, whenever execution of a currently-running coroutine
reaches an await expression, the coroutine may be suspended, and
another previously-suspended coroutine may resume execution if what it
was suspended on has since returned a value. Suspension can also
happen when an async for block requests the next value from an
asynchronous iterator or when an async with block is entered or
exited, as these operations use await under the hood.

If, however, a blocking I/O-bound or CPU-bound operation was directly executed/called inside an async def function/endpoint, it would then block the event loop, and hence, the main thread would be blocked as well. Hence, a blocking operation such as time.sleep() in an async def endpoint would block the entire server (as in the code example provided in your question). Thus, if your endpoint is not going to make any async calls, you could declare it with normal def instead, in which case, FastAPI would run it in a separate thread from the external threadpool and await it, as explained earlier (more solutions are given in the following sections). Example:
@app.get(""/ping"")
def ping(request: Request):
    #print(request.client)
    print(""Hello"")
    time.sleep(5)
    print(""bye"")
    return ""pong""

Otherwise, if the functions that you had to execute inside the endpoint are async functions that you had to await, you should define your endpoint with async def. To demonstrate this, the example below uses the asyncio.sleep() function (from the asyncio library), which provides a non-blocking sleep operation. The await asyncio.sleep() method will suspend the execution of the surrounding coroutine (until the sleep operation is completed), thus allowing other tasks in the event loop to run. Similar examples are given here and here as well.
import asyncio
 
@app.get(""/ping"")
async def ping(request: Request):
    #print(request.client)
    print(""Hello"")
    await asyncio.sleep(5)
    print(""bye"")
    return ""pong""

Both the endpoints above will print out the specified messages to the screen in the same order as mentioned in your question—if two requests arrived at (around) the same time—that is:
Hello
Hello
bye
bye

Important Note
When using a web browser to call the same endpoint for the second (third, and so on) time, please remember to do that from a tab that is isolated from the browser's main session; otherwise, succeeding requests (i.e., coming after the first one) might be blocked by the browser (on client side), as the browser might be waiting for a response to the previous request from the server, before sending the next request. This is a common behaviour for the Chrome web browser at least, due to waiting to see the result of a request and check if the result can be cached, before requesting the same resource again.
You could confirm that by using print(request.client) inside the endpoint, where you would see the hostname and port number being the same for all incoming requests—in case the requests were initiated from tabs opened in the same browser window/session; otherwise, the port number would normally be different for every request—and hence, those requests would be processed sequentially by the server, because of the browser/client sending them sequentially in the first place. To overcome this, you could either:

Reload the same tab (as is running), or

Open a new tab in an Incognito Window, or

Use a different web browser/client to send the request, or

Use the httpx library to make asynchronous HTTP requests, along with the awaitable asyncio.gather(), which allows executing multiple asynchronous operations concurrently and then returns a list of results in the same order the awaitables (tasks) were passed to that function (have a look at this answer for more details).
Example:
import httpx
import asyncio

URLS = ['http://127.0.0.1:8000/ping'] * 2

async def send(url, client):
    return await client.get(url, timeout=10)

async def main():
    async with httpx.AsyncClient() as client:
        tasks = [send(url, client) for url in URLS]
        responses = await asyncio.gather(*tasks)
        print(*[r.json() for r in responses], sep='\n')

asyncio.run(main())

In case you had to call different endpoints that may take different time to process a request, and you would like to print the response out on client side as soon as it is returned from the server—instead of waiting for asyncio.gather() to gather the results of all tasks and print them out in the same order the tasks were passed to the send() function—you could replace the send() function of the example above with the one shown below:
async def send(url, client):
    res = await client.get(url, timeout=10)
    print(res.json())
    return res



Async/await and Blocking I/O-bound or CPU-bound Operations
If you are required to define a FastAPI endpoint (or a StreamingResponse's generator, or a background task function)  with async def (as you might need to await for some coroutines inside it), but also have some synchronous blocking I/O-bound or CPU-bound operation (computationally intensive task) that would block the event loop (essentially, the entire server) and wouldn't let other requests to go through, for example:
@app.post(""/ping"")
async def ping(file: UploadFile = File(...)):
    print(""Hello"")
    try:
        contents = await file.read()
        res = cpu_bound_task(contents)  # this would block the event loop
    finally:
        await file.close()
    print(""bye"")
    return ""pong""

then:

You should check whether you could change your endpoint's definition to normal def instead of async def. One way, if the only method in your endpoint that had to be awaited was the one reading the file contents would be to declare the file contents parameter as bytes, i.e., contents: bytes = File(). Using that definition, FastAPI would read the file for you and you would receive the contents as bytes. Hence, there would be no need to use an async def endpoint with await file.read() inside. Please note that this approach (i.e., using contents: bytes = File()) should work fine for small files; however, for larger files, and always depending on your server's resources, this might cause issues, as the enitre file contents would be stored to memory (see the documentation on File Parameters). Hence, if your system does not have enough RAM available to accommodate the accumulated data, your application may end up crashing—if, for instance, you have 8GB of RAM (the available RAM will always be less than the amount installed on your device, as other apps/services will be using it as well), you can't load a 50GB file.
Alternatively, you could use file: UploadFile = File(...) definition in your endpoint, but this time call the synchronous .read() method of the SpooledTemporaryFile directly, which can be accessed through the .file attribute of the UploadFile object. In this way, you will be able to declare your endpoint with a normal def instead, and hence, each request will run in a separate thread from the external threadpool and then be awaited (as explained earlier). Example is given below. For more details on how to upload a File, as well as how FastAPI/Starlette uses the SpooledTemporaryFile behind the scenes when uploading a File, please have a look at this answer and this answer.
@app.post(""/ping"")
def ping(file: UploadFile = File(...)):
    print(""Hello"")
    try:
        contents = file.file.read()
        res = cpu_bound_task(contents)
    finally:
        file.file.close()
    print(""bye"")
    return ""pong""

Another way, when you would like having the endpoint defined with normal def, as you would need to run blocking operations inside and would like having the endpoint run in a separate thread instead of calling it directly in the event loop, but at the same time you would have to await for coroutines, is to await such coroutines inside an async dependency instead, as explained and demonstrated in Update 2 of this answer, and have the result returned to the def endpoint.

Use FastAPI's (Starlette's) run_in_threadpool() function from the concurrency module—as @tiangolo suggested here—which ""will run the function in a separate thread to ensure that the main thread (where coroutines are run) does not get blocked"" (see here). run_in_threadpool is an awaitable function, where its first parameter is a normal function, and the following parameters are passed to that function directly. It supports both sequence and keyword arguments.
from fastapi.concurrency import run_in_threadpool

res = await run_in_threadpool(cpu_bound_task, contents)


Alternatively, use asyncio's loop.run_in_executor()—after obtaining the running event loop using asyncio.get_running_loop()—to run the task, which, in this case, you can await for it to complete and return the result(s), before moving on to the next line of code. Passing None to the executor argument, the default executor will be used; which is a ThreadPoolExecutor:
import asyncio

loop = asyncio.get_running_loop()
res = await loop.run_in_executor(None, cpu_bound_task, contents)

or, if you would like to pass keyword arguments instead, you could use a lambda expression (e.g., lambda: cpu_bound_task(some_arg=contents)), or, preferably, functools.partial(), which is specifically recommended in the documentation for loop.run_in_executor():
import asyncio
from functools import partial

loop = asyncio.get_running_loop()
res = await loop.run_in_executor(None, partial(cpu_bound_task, some_arg=contents))

In Python 3.9+, you could also use asyncio.to_thread() to asynchronously run a synchronous function in a separate thread—which, essentially, uses await loop.run_in_executor(None, func_call) under the hood, as can been seen in the implementation of asyncio.to_thread(). The to_thread() function takes the name of a blocking function to execute, as well as any arguments (*args and/or **kwargs) to the function, and then returns a coroutine that can be awaited. Example:
import asyncio

res = await asyncio.to_thread(cpu_bound_task, contents)

Note that as explained in this answer, passing None to the executor argument does not create a new ThreadPoolExecutor every time you call await loop.run_in_executor(None, ...), but instead re-uses the default executor with the default number of worker threads (i.e., min(32, os.cpu_count() + 4)). Thus, depending on the requirements of your application, that number might not be enough. In that case, you should rather use a custom ThreadPoolExecutor. For instance:
import asyncio
import concurrent.futures

loop = asyncio.get_running_loop()
with concurrent.futures.ThreadPoolExecutor() as pool:
    res = await loop.run_in_executor(pool, cpu_bound_task, contents)

I would strongly recommend having a look at the linked answer above to learn about the difference between using run_in_threadpool() and run_in_executor(), as well as how to create a re-usable custom ThreadPoolExecutor at the application startup, and adjust the number of maximum worker threads as needed.

ThreadPoolExecutor will successfully prevent the event loop from being blocked, but won't give you the performance improvement you would expect from running code in parallel; especially, when one needs to perform CPU-bound tasks, such as the ones described here (e.g., audio or image processing, machine learning, and so on). It is thus preferable to run CPU-bound tasks in a separate process—using ProcessPoolExecutor, as shown below—which, again, you can integrate with asyncio, in order to await it to finish its work and return the result(s). As described here, it is important to protect the entry point of the program to avoid recursive spawning of subprocesses, etc. Basically, your code must be under if __name__ == '__main__'.
import concurrent.futures

loop = asyncio.get_running_loop()
with concurrent.futures.ProcessPoolExecutor() as pool:
    res = await loop.run_in_executor(pool, cpu_bound_task, contents) 

Again, I'd suggest having a look at the linked answer earlier on how to create a re-usable ProcessPoolExecutor at application startup—you should find this answer helpful as well.

More solutions, as shown in this answer, include using asyncio.create_task() (if your task returns a coroutine and you wouldn't like to await for the task to complete), as well as spawning a new thread or process, using the threading or multiprocessing module, respectively, instead of using concurrent.futures. Moreover, if you had to perform some heavy background computation task that wouldn't necessarily have to be run by the same process (for example, you don't need to share memory, variables, etc.), you could also benefit from using other bigger tools like Celery, as described in FastAPI's documentation. Using apscheduler, as demonstrated in this answer, might be another option as well—always choose what suits you best.

Use more server workers to take advantage of multi-core CPUs, in order to run multiple processes in parallel and be able to serve more requests. For example, uvicorn main:app --workers 4 (if you are using Gunicorn as a process manager with Uvicorn workers, please have a look at this answer). When using 1 worker, only one process is run. When using multiple workers, this will spawn multiple processes (all single threaded). Each process has a separate Global Interpreter Lock (GIL), as well as its own event loop, which runs in the main thread of each process and executes all tasks in its thread. That means, there is only one thread that can take a lock on the interpreter of each process; unless, of course, you employ additional threads, either outside or inside the event loop, e.g., when using a ThreadPoolExecutor with loop.run_in_executor, or defining endpoints/background tasks/StreamingResponse's generators with normal def instead of async def, as well as when calling UploadFile's methods (see the first two paragraphs of this answer for more details).
Note that each worker ""has its own things, variables and memory"". This means that global variables/objects, etc., won't be shared across the processes/workers. In this case, you should consider using a database storage, or  Key-Value stores (Caches), as described here and here. Additionally, note that ""if you are consuming a large amount of memory in your code, each process will consume an equivalent amount of memory"".


",python
display product data from billingclient,"I have made the following BillingManager class to handle the Google Play Billing Library for my Android app:
public class BillingManager {

    private List<ProductDetails> productDetailsList;

    private PurchasesUpdatedListener purchasesUpdatedListener = (billingResult, purchases) -> {
        // To be implemented in a later section.
    };

    private BillingClient billingClient = BillingClient.newBuilder(MyApplication.getAppContext())
            .setListener(purchasesUpdatedListener)
            .enablePendingPurchases(PendingPurchasesParams.newBuilder().enableOneTimeProducts().build())
            .build();

    public void establishConnection() {
        billingClient.startConnection(new BillingClientStateListener() {
            @Override
            public void onBillingSetupFinished(BillingResult billingResult) {
                if (billingResult.getResponseCode() == BillingClient.BillingResponseCode.OK) {
                    Log.i(""BillingManager"", ""Connected to Google Play."");
                    getProductDetails();
                }
            }
            @Override
            public void onBillingServiceDisconnected() {
                Log.i(""BillingManager"", ""Disconnected from Google Play."");
                establishConnection();
            }
        });
    }

    private void getProductDetails() {
        QueryProductDetailsParams queryProductDetailsParams =
                QueryProductDetailsParams.newBuilder()
                        .setProductList(List.of(
                                QueryProductDetailsParams.Product.newBuilder()
                                        .setProductId(""full_version"")
                                        .setProductType(BillingClient.ProductType.INAPP)
                                        .build()))
                        .build();

        billingClient.queryProductDetailsAsync(
                queryProductDetailsParams,
                (billingResult, productDetailsList) -> {
                    this.productDetailsList = productDetailsList;
                }
        );

    }

    public List<ProductDetails> getProductDetailsList() {
        return productDetailsList;
    }

    public void launchPurchaseFlow(Activity activity, ProductDetails productDetails) {
        List<BillingFlowParams.ProductDetailsParams> productDetailsParamsList =
                List.of(
                        BillingFlowParams.ProductDetailsParams.newBuilder()
                                // retrieve a value for ""productDetails"" by calling queryProductDetailsAsync()
                                .setProductDetails(productDetails)
                                .build()
                );

        BillingFlowParams billingFlowParams = BillingFlowParams.newBuilder()
                .setProductDetailsParamsList(productDetailsParamsList)
                .build();

    // Launch the billing flow
        BillingResult billingResult = billingClient.launchBillingFlow(activity, billingFlowParams);
    }
}

And this is an example of how I would want to use it in one of my FragmentDialog class:
BillingManager billingManager = new BillingManager();
    billingManager.establishConnection();

    MaterialButton button = view.findViewById(R.id.button);

// This causes an outOfBounds exception
button.setText(billingManager.getProductDetailsList().get(0).getOneTimePurchaseOfferDetails().getFormattedPrice());

    button.setOnClickListener(v -> {
        billingManager.launchPurchaseFlow(getActivity(), billingManager.getProductDetailsList().get(0));
    });

}

Question:
I'm encountering an IndexOutOfBoundsException when trying to set the text of a button with a product price retrieved from the Google Play Billing Library. The line causing the issue is:
button.setText(billingManager.getProductDetailsList().get(0).getOneTimePurchaseOfferDetails().getFormattedPrice());
The problem stems from the asynchronous establishConnection and getProductDetails methods within my BillingManager class. When the fragment starts and attempts to access the product details to set the button's text, these operations haven't completed yet, resulting in an empty or uninitialized list.
How can I solve this and ensure that the button's text is set only after both the connection to Google Play Billing has been established and the product details have been successfully retrieved?
","You can create an interface BillingStatusListener
interface BillingStatusListener {
   void onProductListFetched(List<YourType> productList);
   void onBillingSetupFinishedSuccessfully();
}

Make your FragmentDialog implement this interface
class FragmentDialog implements BillingStatusListener {

   @Override
   void onProductListFetched(List<YourType> productList) {
      // Setup your button text here
      // You are sure to have data here, and no Null Pointer exceptions

   }
}

Lastly, make your BillingManager accept an implementation of the interface above and use it as required
public class BillingManager {

    private List<ProductDetails> productDetailsList;

    private PurchasesUpdatedListener purchasesUpdatedListener = (billingResult, purchases) -> {
        // To be implemented in a later section.
    };

   private BillingStatusListener billingStatusListener; // Initialise this somewhere with the FragmentDialog class as the implementation

    private BillingClient billingClient = BillingClient.newBuilder(MyApplication.getAppContext())
            .setListener(purchasesUpdatedListener)
            .enablePendingPurchases(PendingPurchasesParams.newBuilder().enableOneTimeProducts().build())
            .build();

    public void establishConnection() {
        billingClient.startConnection(new BillingClientStateListener() {
            @Override
            public void onBillingSetupFinished(BillingResult billingResult) {
                if (billingResult.getResponseCode() == BillingClient.BillingResponseCode.OK) {
                    Log.i(""BillingManager"", ""Connected to Google Play."");
                    getProductDetails();
                }
            }
            @Override
            public void onBillingServiceDisconnected() {
                Log.i(""BillingManager"", ""Disconnected from Google Play."");
                establishConnection();
            }
        });
    }

    private void getProductDetails() {
        QueryProductDetailsParams queryProductDetailsParams =
                QueryProductDetailsParams.newBuilder()
                        .setProductList(List.of(
                                QueryProductDetailsParams.Product.newBuilder()
                                        .setProductId(""full_version"")
                                        .setProductType(BillingClient.ProductType.INAPP)
                                        .build()))
                        .build();

        billingClient.queryProductDetailsAsync(
                queryProductDetailsParams,
                (billingResult, productDetailsList) -> {
                    this.productDetailsList = productDetailsList;
                    // you will surely have data here
                    // so let's notify the interface
                    billingStatusListener.onProductListFetched(productDetailList);
                }
        );

    }

    public List<ProductDetails> getProductDetailsList() {
        return productDetailsList;
    }

    public void launchPurchaseFlow(Activity activity, ProductDetails productDetails) {
        List<BillingFlowParams.ProductDetailsParams> productDetailsParamsList =
                List.of(
                        BillingFlowParams.ProductDetailsParams.newBuilder()
                                // retrieve a value for ""productDetails"" by calling queryProductDetailsAsync()
                                .setProductDetails(productDetails)
                                .build()
                );

        BillingFlowParams billingFlowParams = BillingFlowParams.newBuilder()
                .setProductDetailsParamsList(productDetailsParamsList)
                .build();

    // Launch the billing flow
        BillingResult billingResult = billingClient.launchBillingFlow(activity, billingFlowParams);
    }
}

",java
jupyterlab vs jupyternotebook,"Hello everyone on Stack Overflow. Today, I would like to ask something very different question. 
I am currently working as a data scientist, and I work alot on JupyterLab/Notebook. Couple of my co-workers use Notebook instead of JupyterLab. It seems like there are not much difference between those two (I really like how JupyterLab presents codes in different colors). I searched on the internet, and it says
""JupyterLab is the next generation of the Jupyter Notebook""
However, some featuers like plotly figures do not work well on JupyterLab but works well on Jupyter Notebook. I do not know why this is happening.
Can anyone who work on these two tell me the actual differences?

Thank you for your replies!
","I used to work with Jupyter Notebook for about 4 years and last year I switched to Jupyter Lab. I think it is a nice improvement, I believe the biggest advantage is the improved user interface: it is much easier to switch between notebooks and everything feels more organized with tabs.
Concerning functionalities, the JupyterLab improvement is being modular: you can easily write your own plugins if you need it. 
I have no experience using plotly, but I have similar issues with other packages. Everything was fixed in a short time, so I wouldn't worry too much about it.
",data-science
how to correctly evaluate model performance of a regressor in gridsearchcv sklearn,"I'm trying to found a set of best hyperparameters for my Gradient Boosting Regressor with Grid Search CV. But I have difficulties getting the performance of the best model.
My code is as follows, this function is expected to return an optimized model.
def parameter_tuning_Gradient_Boost(X,
                                    y,
                                    ):

    model = GradientBoostingRegressor()

    param_grid = {  'learning_rate': [0.005, 0.01, 0.02, 0.05, 0.1],
                    'subsample'    : [1.0, 0.8, 0.6],
                    'n_estimators' : [100, 200, 500, 1000],
                    'max_depth'    : [2, 4, 6, 8, 10]
                    }

    grid_search = GridSearchCV( model,
                                param_grid,
                                cv = 5,
                                n_jobs = 8,
                                verbose = 0)

    grid_search.fit(X = X,
                    y = y,)

    print('Best Parameters by Searching: %s' % grid_search.best_params_)

    best_parameters = grid_search.best_estimator_.get_params()

    model = GradientBoostingRegressor(  learning_rate = best_parameters['learning_rate'],
                                        subsample = best_parameters['subsample'],
                                        n_estimators = best_parameters['n_estimators'],
                                        max_depth = best_parameters['max_depth'],
                                        )
    
    return model


In general, I have the following questions：

Do I have to use train_test_split function to split X and y, and then feed X_train and y_train to grid_search.fit function? Some said GridSearchCV will automatically split data into train and test if you set cv = 5. But I saw some online tutorial will do something like this:

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)
grid_search.fit(X_train, y_train)


What is the metrics score for a regressor in GridSearchCV? after fitting GridSearchCV, I run the follwing commands and get way different score. I am wondering what is the correct way to get model cv perforamce for a regressor in GridSearchCV sklearn.

print(""Best Score:"", grid_search.score(X, y))
print(""Best Score: %.3f"" % grid_search.best_score_)

I tried to apply GridSearchCV method to perform parameter tuning for a regressor and get its cross-validation performance, I want to know what is the default evaluation metrics here, and I want to know do I have to split the data into train and test set when I set cv parameter in GridSearchCV.
","
Yes GridSearchCV will split the data into 5 train/test splits. It will then use these splits to find the optimal hyperparameters. However, it's also good practice to set aside a completely unseen split of the data. That you score the model(s) on when you are completely done with training. Take a look at this article to read more on this. Remember, after evaluating the model on the unseen data set. You are not ""allowed"" to improve your model.

The scoring metric for GridSearchCV can be user defined. But as a standard it will use the scoring parameter from the estimator. In this case squared_error which is the default scoring for GradientBoostingRegressor, the parameter is named loss=squared_error.


To answer the second part of the question, we need to understand what happens in GridSearchCV when fitting. When the optimal hyperparameters are found, a model is refitted on all the data with the optimal hyperparameters, if refit=True, which it is by default.
grid_search.score(X,y) scores the refitted model on all of the data while
grid_search._best_score returns the average score of the models with the optimal hyperparameters on the five splits.
",cross-validation
how to get rid of quotunnamed 0quot column in a pandas dataframe read in from csv file,"I have a situation wherein sometimes when I read a csv from df I get an unwanted index-like column named unnamed:0. 
file.csv

,A,B,C
0,1,2,3
1,4,5,6
2,7,8,9

The CSV is read with this:
pd.read_csv('file.csv')

   Unnamed: 0  A  B  C
0           0  1  2  3
1           1  4  5  6
2           2  7  8  9

This is very annoying! Does anyone have an idea on how to get rid of this?
","It's the index column, pass pd.to_csv(..., index=False) to not write out an unnamed index column in the first place, see the to_csv() docs.
Example:
In [37]:
df = pd.DataFrame(np.random.randn(5,3), columns=list('abc'))
pd.read_csv(io.StringIO(df.to_csv()))

Out[37]:
   Unnamed: 0         a         b         c
0           0  0.109066 -1.112704 -0.545209
1           1  0.447114  1.525341  0.317252
2           2  0.507495  0.137863  0.886283
3           3  1.452867  1.888363  1.168101
4           4  0.901371 -0.704805  0.088335

compare with:
In [38]:
pd.read_csv(io.StringIO(df.to_csv(index=False)))

Out[38]:
          a         b         c
0  0.109066 -1.112704 -0.545209
1  0.447114  1.525341  0.317252
2  0.507495  0.137863  0.886283
3  1.452867  1.888363  1.168101
4  0.901371 -0.704805  0.088335

You could also optionally tell read_csv that the first column is the index column by passing index_col=0:
In [40]:
pd.read_csv(io.StringIO(df.to_csv()), index_col=0)

Out[40]:
          a         b         c
0  0.109066 -1.112704 -0.545209
1  0.447114  1.525341  0.317252
2  0.507495  0.137863  0.886283
3  1.452867  1.888363  1.168101
4  0.901371 -0.704805  0.088335

",pandas
can39t suppress warning from transformerssrctransformersmodeling_utilspy,"My implementation for the AutoModel AutoTokenizer classes are fairly simple:
from transformers import AutoModel, AutoTokenizer
import numpy as np
from rank_bm25 import BM25Okapi
from sklearn.neighbors import NearestNeighbors

class EmbeddingModels:

    def bert(self, model_name, text):
        tokenizer = AutoTokenizer.from_pretrained(model_name)
        model = AutoModel.from_pretrained(model_name)
        inputs = tokenizer(text, return_tensors=""pt"", truncation=True, padding=True)
        outputs = model(**inputs)
        embeddings = outputs.last_hidden_state.mean(dim=1).detach().numpy()
        return embeddings
    
    def create_chunks(self, text, chunk_size):
        return [text[i:i + chunk_size] for i in range(0, len(text), chunk_size)]

But I can't get this warning to go away:
A parameter name that contains 'beta' will be renamed internally to 'bias'. 
Please use a different name to suppress this warning.
A parameter name that contains 'gamma' will be renamed internally to 'weight'. 
Please use a different name to suppress this warning.

There is no reference to the word beta or gamma anywhere in my repo.
Updating the package, suppressing the warnings with import warnings
","Before loading from pretrained model set transformers logger level to error as shown below. It sure is really frustrating not being able to leverage the warnings library filter
    loggers = [logging.getLogger(name) for name in logging.root.manager.loggerDict]
    for logger in loggers:
        if ""transformers"" in logger.name.lower():
            logger.setLevel(logging.ERROR)

    # now you can load state dict from pretrained
    model = transformers.BertModel.from_pretrained(
        ""bert-base-uncased"",
        use_safetensors=True,
        return_dict=False,
        attn_implementation=""sdpa"",
    )

",huggingface-transformers
valueerror unrecognized model in mrpc should have a model_type key in its configjson or contain one of the following strings in its name,"Goal: Amend this Notebook to work with Albert and Distilbert models
Kernel: conda_pytorch_p36. I did Restart & Run All, and refreshed file view in working directory.
Error occurs in Section 1.2, only for these 2 new models.
For filenames etc., I've created a variable used everywhere:
MODEL_NAME = 'albert-base-v2'  # 'distilbert-base-uncased', 'bert-base-uncased'

I replaced imports with:
from transformers import (AutoConfig, AutoModel, AutoTokenizer)
#from transformers import (BertConfig, BertForSequenceClassification, BertTokenizer,)

As suggested in Transformers Documentation - Auto Classes.

Instantiating one of AutoConfig, AutoModel, and AutoTokenizer will directly create a class of the relevant architecture.


Section 1.2:
# load model
model = AutoModel.from_pretrained(configs.output_dir)  # BertForSequenceClassification
model.to(configs.device)


# quantize model
quantized_model = torch.quantization.quantize_dynamic(
    model, {torch.nn.Linear}, dtype=torch.qint8
)

#print(quantized_model)

def print_size_of_model(model):
    torch.save(model.state_dict(), ""temp.p"")
    print('Size (MB):', os.path.getsize(""temp.p"")/(1024*1024))
    os.remove('temp.p')

print_size_of_model(model)
print_size_of_model(quantized_model)

Traceback:
ValueError: Unrecognized model in ./MRPC/. Should have a `model_type` key in its config.json, or contain one of the following strings in its name: imagegpt, qdqbert, vision-encoder-decoder, trocr, fnet, segformer, vision-text-dual-encoder, perceiver, gptj, layoutlmv2, beit, rembert, visual_bert, canine, roformer, clip, bigbird_pegasus, deit, luke, detr, gpt_neo, big_bird, speech_to_text_2, speech_to_text, vit, wav2vec2, m2m_100, convbert, led, blenderbot-small, retribert, ibert, mt5, t5, mobilebert, distilbert, albert, bert-generation, camembert, xlm-roberta, pegasus, marian, mbart, megatron-bert, mpnet, bart, blenderbot, reformer, longformer, roberta, deberta-v2, deberta, flaubert, fsmt, squeezebert, hubert, bert, openai-gpt, gpt2, transfo-xl, xlnet, xlm-prophetnet, prophetnet, xlm, ctrl, electra, speech-encoder-decoder, encoder-decoder, funnel, lxmert, dpr, layoutlm, rag, tapas, splinter, sew-d, sew, unispeech-sat, unispeech, wavlm

Please let me know if there's anything else I can add to post.
","Explanation:
When instantiating AutoModel, you must specify a model_type parameter in ./MRPC/config.json file (downloaded during Notebook runtime).
List of model_types can be found here.

Solution:
Code that appends model_type to config.json, in the same format:
import json

json_filename = './MRPC/config.json'

with open(json_filename) as json_file:
    json_decoded = json.load(json_file)

json_decoded['model_type'] = # !!

with open(json_filename, 'w') as json_file:
    json.dump(json_decoded, json_file, indent=2, separators=(',', ': '))

config.json:
{
  ""attention_probs_dropout_prob"": 0.1,
  ""finetuning_task"": ""mrpc"",
  ""hidden_act"": ""gelu"",
  ""hidden_dropout_prob"": 0.1,
  ""hidden_size"": 768,
  ""initializer_range"": 0.02,
  ""intermediate_size"": 3072,
  ""layer_norm_eps"": 1e-12,
  ""max_position_embeddings"": 512,
  ""num_attention_heads"": 12,
  ""num_hidden_layers"": 12,
  ""num_labels"": 2,
  ""output_attentions"": false,
  ""output_hidden_states"": false,
  ""pruned_heads"": {},
  ""torchscript"": false,
  ""type_vocab_size"": 2,
  ""vocab_size"": 30522,
  ""model_type"": ""albert""
}


",huggingface-transformers
seaborn boxplot add annotation labels for max values,"I would like to show some values in a Seaborn box plot such as Total# observations, Mean, Mix/Max value for each box plot series. Is there a way to show these in the plot?
import seaborn as sns
import matplotlib.pyplot as plt

sns.set_theme(style=""ticks"")

# Initialize the figure with a logarithmic x axis
f, ax = plt.subplots(figsize=(7, 6))
ax.set_xscale(""log"")

# Load the example planets dataset
planets = sns.load_dataset(""planets"")

# Plot the orbital period with horizontal boxes
sns.boxplot(
    planets, x=""distance"", y=""method"", hue=""method"",
    whis=[0, 100], width=.6, palette=""vlag""
)

# Add in points to show each observation
sns.stripplot(planets, x=""distance"", y=""method"", size=4, color="".3"")

# Tweak the visual presentation
ax.xaxis.grid(True)
ax.set(ylabel="""")
sns.despine(trim=True, left=True)

Example output:

","Just use matplotlib text function with the ""data coordinates"" for x and y:
mytext = 'Total: XX \n Mean: YY \n Min Value: AA \n Max Value: BB'
ax.text(x=15000, y='Radial Velocity', 
        s=mytext, style='italic', 
        color='white', bbox = {'facecolor': 'black'}, 
        fontsize=8, verticalalignment='center')
plt.show()


",matplotlib
select dataframe column and replace values by indices if true,"Hard to find the right title...here is what I want:
I have a dataframe and a column col1 with values : val1, val2, val3
I want to select the rows with val2 or val3 values for this specific column and replace them with val4 value but not for all of them, just for a ""slice"" between idx x and y :
import pandas as pd
data = {'col1':[""val1"",""val3"",""val3"",""val2"",""val1"",""val2"",""val3"",""val1""],'col2':[""val3"",""val1"",""val2"",""val1"",""val2"",""val3"",""val2"",""val2""]}
df = pd.DataFrame(data)
df
   col1  col2
0  val1  val3
1  val3  val1
2  val3  val2
3  val2  val1
4  val1  val2
5  val2  val3
6  val3  val2
7  val1  val2

Select rows from col1 with val2 or val3 values :
(df['col1']==""val2"") | (df['col1']==""val3"")
0    False
1     True
2     True
3     True
4    False
5     True
6     True
7    False

Now I want to replace the first 4 True rows for col1 (rows with index 1 2 3 5) with val4 in order to obtain :
   col1  col2
0  val1  val3
1  val4  val1
2  val4  val2
3  val4  val1
4  val1  val2
5  val4  val3
6  val3  val2
7  val1  val2

I thought something like :
df[((df['col1']==""val2"") | (df['col1']==""val3""))==True][0:4] = ""val4""
but it doesn't work (not surprise...)
Thought I need to use something like .loc
Thanx for any clue
","You can get the rows based on the condition
condition = (df['col1'] == ""val2"") | (df['col1'] == ""val3"")

And then get indices of rows that match the condition
indices = df[condition].index[:4]

Finally use loc to replace the selected rows with val4
df.loc[indices, 'col1'] = 'val4'

Output
   col1  col2
0  val1  val3
1  val4  val1
2  val4  val2
3  val4  val1
4  val1  val2
5  val4  val3
6  val3  val2
7  val1  val2

",pandas
model wrapper for sklearn cross_val_score,"This is an minimal example using XGBClassifier, but am interested how this would work in general. I am trying to wrap the model class in order to use it in cross validation. In this case I am only weighing the imbalanced classes, but my ultimate goal is a bit broader change in the pipeline.
My first try was to simply override the fit function:
from sklearn import metrics
from sklearn.utils.class_weight import compute_sample_weight
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import RepeatedStratifiedKFold
from sklearn.model_selection import train_test_split
from xgboost import XGBClassifier
from sklearn.base import BaseEstimator, ClassifierMixin

class WeightedXGBClassifier(XGBClassifier, BaseEstimator, ClassifierMixin):
    
    @staticmethod
    def get_weights(y):
        sample_weights = compute_sample_weight(class_weight='balanced', y=y)
        return sample_weights
    
    def fit(self, X, y, **kwargs):
        weights = self.get_weights(y)
        super(XGBClassifier, self).fit(X, y, sample_weight=weights, **kwargs)

which works fine, when I'm trying to fit the model, use predictions etc.. But using this in sklearn cross_val_score
xgb_model_cv = WeightedXGBClassifier(n_estimators=100, max_depth=4, alpha=100, use_label_encoder=False)

cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)
auc_scorer = metrics.make_scorer(metrics.roc_auc_score, needs_proba=True)
scores = cross_val_score(xgb_model_cv, X, y, scoring=auc_scorer, cv=cv, n_jobs=-1, verbose=1)

throws an error
File ""/home/ubuntu/anaconda3/envs/pyTF/lib/python3.9/site-packages/sklearn/model_selection/_validation.py"", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File ""/home/ubuntu/anaconda3/envs/pyTF/lib/python3.9/site-packages/sklearn/metrics/_scorer.py"", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File ""/home/ubuntu/anaconda3/envs/pyTF/lib/python3.9/site-packages/sklearn/metrics/_scorer.py"", line 306, in _score
    y_pred = self._select_proba_binary(y_pred, clf.classes_)
AttributeError: 'WeightedXGBClassifier' object has no attribute 'classes_'

Now, it is my understanding the classes_ attribute is created, when the model is fitted, but I am not sure how to then properly wrap the model to capture this. Note that running
model = XGBClassifier(use_label_encoder=False, scale_pos_weight=(~y).sum()/y.sum())
cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)
scores = cross_val_score(model, X, y, scoring='roc_auc', cv=cv, n_jobs=-1)

works fine. My second try was:
class XGBClassifierWrapper(BaseEstimator, ClassifierMixin):
    def __init__(self, **kwargs):
#         super(BaseEstimator).__init__()
#         super(ClassifierMixin).__init__()
        self.xgb_classifier_obj = XGBClassifier(**kwargs)
    
    @staticmethod
    def get_weights(y):
        sample_weights = compute_sample_weight(class_weight='balanced', y=y)
        return sample_weights
    
    def fit(self, X, y, **kwargs):
        weights = self.get_weights(y)
        self.xgb_classifier_obj.fit(X, y, sample_weight=weights, **kwargs)
        return self
    
    def predict(self, X, **kwargs):
        return self.xgb_classifier_obj.predict(X, **kwargs)
    
    def predict_proba(self, X, **kwargs):
        return self.xgb_classifier_obj.predict_proba(X, **kwargs)

which again resulted in the same error as in the case above, i.e., missing classes_ attribute.
","(I don't actually get an error when I run any of your code; however, I do get a scores consisting only of nan, and adding error_score='raise' I get your error message.)
In the first approach, I believe the only real problem is in your initialization. super(XGBClassifier, self): that's looking for a parent class of XGBClassifier, and not XGBClassifier itself, as I assume you want.  Replacing with just the vanilla super() and everything works.
You should also add return self to the end of fit in your first attempt, but it's not important here.  You can probably safely drop BaseEstimator and ClassifierMixin from the inheritance, since XGBClassifier already inherits from them.
Your second, wrapper, approach just fails because the wrapped xgb_classifier_obj has all the fitted attributes, including classes_, but your wrapper doesn't expose that directly.  You can just set self.classes_ = self.xgb_classifier_obj.classes_ in fit, or perhaps define a @property delegation.
You should also consider that your __init__ this time doesn't meet the sklearn API, so cloning won't work correctly.  I'd advise using the first approach for this reason (fixing it requires rather more tedious work, in my opinion).
",cross-validation
importerror using the trainer with pytorch requires accelerategt0260 but i have version already installed 101,"I'm trying to run traning this way but face import error, how to fix it:
args = TrainingArguments(output_dir=""finetuned"",
                          num_train_epochs=10,
                          per_device_train_batch_size=16,
                          save_steps=10000,
                          gradient_accumulation_steps = 2,
                          warmup_steps=500,
                         lr_scheduler_type=""polynomial"",
                         fp16=True,
                         )

trainer = Trainer(
    model = model,
    args = args,
    train_dataset = train_dataset,
    eval_dataset = test_dataset,
    tokenizer = tokenizer,

)


trainer.train()

However it gives me such error:
File ~\mambaforge\lib\site-packages\transformers\training_args.py:1750, in TrainingArguments.__post_init__(self)
   1748 # Initialize device before we proceed
   1749 if self.framework == ""pt"" and is_torch_available():
-> 1750     self.device
   1752 if self.torchdynamo is not None:
   1753     warnings.warn(
   1754         ""`torchdynamo` is deprecated and will be removed in version 5 of 🤗 Transformers. Use""
   1755         "" `torch_compile_backend` instead"",
   1756         FutureWarning,
   1757     )

File ~\mambaforge\lib\site-packages\transformers\training_args.py:2250, in TrainingArguments.device(self)
   2246 """"""
   2247 The device used by this process.
   2248 """"""
   2249 requires_backends(self, [""torch""])
-> 2250 return self._setup_devices

File ~\mambaforge\lib\site-packages\transformers\utils\generic.py:60, in cached_property.__get__(self, obj, objtype)
     58 cached = getattr(obj, attr, None)
     59 if cached is None:
---> 60     cached = self.fget(obj)
     61     setattr(obj, attr, cached)
     62 return cached

File ~\mambaforge\lib\site-packages\transformers\training_args.py:2123, in TrainingArguments._setup_devices(self)
   2121 if not is_sagemaker_mp_enabled():
   2122     if not is_accelerate_available():
-> 2123         raise ImportError(
   2124             f""Using the `Trainer` with `PyTorch` requires `accelerate>={ACCELERATE_MIN_VERSION}`: ""
   2125             ""Please run `pip install transformers[torch]` or `pip install 'accelerate>={ACCELERATE_MIN_VERSION}'`""
   2126         )
   2127 # We delay the init of `PartialState` to the end for clarity
   2128 accelerator_state_kwargs = {""enabled"": True, ""use_configured_state"": False}

ImportError: Using the `Trainer` with `PyTorch` requires `accelerate>=0.26.0`: Please run `pip install transformers[torch]` or `pip install 'accelerate>={ACCELERATE_MIN_VERSION}'`

but I have right version:
import accelerate    
accelerate.__version__

> '1.0.1'


transformers.__version__ 
> transformers

","Just create a new conda environment and install everything from scratch. As to the cause of your issue, it could be that you were using the wrong python environment by accident, it could also be a failed pip upgrade. A lot of mistakes can lead to this outcome. Creating a new environment step by step from scratch can fix anything.
",huggingface-transformers
trying to make word search program in c,"Given an input dictionary read in from a file to use for all test cases, and several word search grids, i want to identify all words from the dictionary that appear in each word search grid.
I have read in the dictionary.txt file and I believe it can read in any grid of letters, I am having trouble trying to find the words in the grid that appear in the dictionary.txt file. I decided to use a recursive binary search on the string of characters going in all directions but it is a bit complicated for me. 
My program runs until it gets to the word searching function where it tries to find the words in the grid and it'll crash but I dont know why and not sure if i am on the right track.
here is the piece i think is wrong,
int binsearch(char** dictionary, char** puzzle, int low, int high){

int mid;

if(low == 0 && high == 0){
    return 0;
}

mid = (low+high)/2 ;

if(strcmp(*puzzle,dictionary[mid]) == 0){
        //found a match
    return 1;
}

else if(strcmp(*puzzle,dictionary[mid]) > 0){
        //check upper half
    return binsearch(dictionary,puzzle,mid+1,high);
}

else if(strcmp(*puzzle,dictionary[mid]) < 0){
    //check lower half
    return binsearch(dictionary,puzzle,low,mid-1);
}
else return 0;

}

char wordSearch(char** dictionary, char** puzzle, int row, int col){

int i, X, Y, dir = 0;
char* wordsfound[20]= {'\0'};
for (X=0;X<row+1;X++){
    for(Y=0;Y<col;Y++){
        for(dir=0;dir<DX_SIZE;dir++) //check every direction
            for(i=0;i<19;i++){
                //will continue in direction DX,DY starting at x,y
                int nextX = X + DX[dir] * i;
                int nextY = Y + DY[dir] * i;
                if(nextX < 0 || nextX >= row) break; //keep in bounds
                if(nextY < 0 || nextY >= col) break;
                //store the string of letters
                *wordsfound[i] = (puzzle[nextX][nextY]);
                if(i>2){ //minimum word is 3
                //if the string of letters is actually a word, print
                    int bin = binsearch(dictionary,wordsfound,1,listlength);
                    if(bin){
                        printf(""%s\n"",wordsfound);
                    }
                }
            }
    }
}

}

but here is my entire code
#include <stdio.h>
#include <stdlib.h>
#include <string.h>
#define listlength 149256
#define maxWordLen 19


char** getWords(int rows, int cols);
void freeArray(char** array, int rows);
char** makeGridArray(int rows, int cols);
int binsearch(char** dictionary, char** puzzle, int low, int high);
char wordSearch(char** dictionary, char** puzzle, int row, int col);
const int DX_SIZE = 8;
const int DX[] = {-1,-1,-1,0,0,1,1,1};
const int DY[] = {-1,0,1,-1,1,-1,0,1};

int main(){

    //read in dictionary
    int i,j,x=0, numCases, gridRow, gridCol;
    char** words = getWords(listlength, maxWordLen);

    //Get number of cases.

    printf(""enter number of cases:\n"");
    scanf(""%d"", &numCases);

    //process each case.

    while(x < numCases){

        scanf(""%d%d"",&gridRow,&gridCol);

        //make word search grid
        char** grid = makeGridArray(gridRow+1, gridCol);

        /* for testing if grid is storing properly

        for(i=0; i<gridRow+1;i++){
            printf(""%s\n"",grid[i]);
        }

        */
        printf(""Words Found Grid #%d:"",x+1);
        wordSearch(words, grid, gridRow+1, gridCol);
        x++;
        freeArray(grid,gridRow+1);
    }
    freeArray(words, listlength);

}


char** getWords(int rows, int cols){

    int i;

    //allocate top level of pointers.
    char** words = malloc(sizeof(char*)*rows);

    //allocate each individual array
    for(i=0; i<rows; i++){
        words[i] = malloc(sizeof(char)*cols+1);
    }

    //read dictionary.txt
    FILE *dictionary = fopen(""dictionary.txt"", ""r"");
    for(i=0; i<rows; i++){
        fgets(words[i], cols+1,dictionary);
    }

    fclose(dictionary);
    return words;
}

char** makeGridArray(int rows, int cols){

    //allocate top level of pointers.
    char** grid = malloc(sizeof(char*)*rows);
    int i,j;

    //allocate each individual array
    for(i=0; i<rows;i++){
        grid[i] = malloc(sizeof(char)*cols+1);
    }
    //read in user input grid
    for(i=0;i<rows;i++){
        gets(grid[i]);
    }
    return grid;
}

int binsearch(char** dictionary, char** puzzle, int low, int high){

    int mid;

    if(low == 0 && high == 0){
        return 0;
    }

    mid = (low+high)/2 ;

    if(strcmp(*puzzle,dictionary[mid]) == 0){
            //found a match
        return 1;
    }

    else if(strcmp(*puzzle,dictionary[mid]) > 0){
            //check upper half
        return binsearch(dictionary,puzzle,mid+1,high);
    }

    else if(strcmp(*puzzle,dictionary[mid]) < 0){
        //check lower half
        return binsearch(dictionary,puzzle,low,mid-1);
    }
    else return 0;

}

char wordSearch(char** dictionary, char** puzzle, int row, int col){

    int i, X, Y, dir = 0;
    char* wordsfound[20]= {'\0'};
    for (X=0;X<row+1;X++){
        for(Y=0;Y<col;Y++){
            for(dir=0;dir<DX_SIZE;dir++) //check every direction
                for(i=0;i<19;i++){
                    //will continue in direction DX,DY starting at x,y
                    int nextX = X + DX[dir] * i;
                    int nextY = Y + DY[dir] * i;
                    if(nextX < 0 || nextX >= row) break; //keep in bounds
                    if(nextY < 0 || nextY >= col) break;
                    //store the string of letters
                    *wordsfound[i] = (puzzle[nextX][nextY]);
                    if(i>2){ //minimum word is 3
                    //if the string of letters is actually a word, print
                        int bin = binsearch(dictionary,wordsfound,1,listlength);
                        if(bin){
                            printf(""%s\n"",wordsfound);
                        }
                    }
                }
        }
    }

}


void freeArray(char** array, int rows){
    //free arrays
    int i;
    for(i=0; i<rows; i++){
        free(array[i]);
    }
    free(array);
}

","Here:
    char* wordsfound[20]= {'\0'};
    for (X=0;X<row+1;X++){
        for(Y=0;Y<col;Y++){
            for(dir=0;dir<DX_SIZE;dir++) //check every direction
                for(i=0;i<19;i++){
                    ...
                    *wordsfound[i] = (puzzle[nextX][nextY]);

Should be:
    char wordsfound[20]= {'\0'};                    /* not 20 pointers! */
    for (X=0;X<row+1;X++){
        for(Y=0;Y<col;Y++){
            for(dir=0;dir<DX_SIZE;dir++) //check every direction
                for(i=0;i<19;i++){
                    ...
                    wordsfound[i] = (puzzle[nextX][nextY]);   /* no '*' */
                    wordsfound[i+1] = '\0';           /* end the string */

Also, in your binsearch function when you go to check the upper half or the lower half, you need to make sure that mid+1 and mid-1 respectively are still valid indexes into the array.  If you access elements that are outside the array bounds, bad things are waiting to bite you.
For example, if low = 0 and high = 1:
mid = (low+high)/2 ;       /* this is zero */

so you compare with the array element 0 and decide you need to look in the lower half, now you call with low as mid-1 which is -1.  Oh noes!
",c
gridsearchcv not choosing the best hyperparameters for xgboost,"I am developing a regression model with xgboost. Since xgboost has multiple hyperparameters, I have added the cross validation logic with GridSearchCV(). As a trial, I set max_depth: [2,3]. My python code is as below.
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import make_scorer
from sklearn.metrics import mean_squared_error
​
xgb_reg = xgb.XGBRegressor()
​
# Obtain the best hyper parameter
scorer=make_scorer(mean_squared_error, False)
params = {'max_depth': [2,3], 
          'eta': [0.1], 
          'colsample_bytree': [1.0],
          'colsample_bylevel': [0.3],
          'subsample': [0.9],
          'gamma': [0],
          'lambda': [1],
          'alpha':[0],
          'min_child_weight':[1]
         }
grid_xgb_reg=GridSearchCV(xgb_reg,
                          param_grid=params,
                          scoring=scorer,
                          cv=5,
                          n_jobs=-1)
​
grid_xgb_reg.fit(X_train, y_train)
y_pred = grid_xgb_reg.predict(X_test)
y_train_pred = grid_xgb_reg.predict(X_train)

## Evaluate model
from sklearn.metrics import mean_squared_error
from sklearn.metrics import r2_score
​
print('RMSE  train: %.3f,  test: %.3f' %(np.sqrt(mean_squared_error(y_train, y_train_pred)),np.sqrt(mean_squared_error(y_test, y_pred))))
print('R^2   train: %.3f,  test: %.3f' %(r2_score(y_train, y_train_pred),r2_score(y_test, y_pred)))

The problem is the GridSearchCV does not seem to choose the best hyperparameters. In my case, when I set max_depth as [2,3], The result is as follows. In the following case, GridSearchCV chose max_depth:2 as the best hyper params.
#  The result when max_depth is 2
RMSE  train: 11.861,  test: 15.113
R^2   train: 0.817,  test: 0.601

However, if I updated max_depth to [3](by getting rid of 2), the test score is better than the previous value as follows.
#  The result when max_depth is 3
RMSE  train: 9.951,  test: 14.752
R^2   train: 0.871,  test: 0.620

Question
My understanding is that even if I set max_depth as [2,3], the GridSearchCV method SHOULD choose the max_depth:3 as the best hyperparameters since max_depth:3 can return the better score in terms of RSME or R^2 than max_depth:2. Could anyone tell me why my code cannot choose the best hyperparameters when I set max_depth as [2,3]?
","If you run a second experiment with max_depth:2, then the results are not comparable to the first experiment with max_depth:[2,3] even for the run with max_depth:2, since there are sources of randomness in your code which you do not explicitly control, i.e. your code is not reproducible.
The first source of randomness is the CV folds; in order to ensure that the experiments will be run on identical splits of the data, you should define your GridSearchCV as follows:
from sklearn.model_selection import KFold

seed_cv = 123 # any random value here

kf = KFold(n_splits=5, random_state=seed_cv)

grid_xgb_reg=GridSearchCV(xgb_reg,
                          param_grid=params,
                          scoring=scorer,
                          cv=kf,   # <- change here
                          n_jobs=-1)

The second source of randomness is the XGBRegressor itself, which also includes a random_state argument (see the docs); you should change it to:
seed_xgb = 456 # any random value here (can even be the same with seed_cv)
xgb_reg = xgb.XGBRegressor(random_state=seed_xgb)

But even with these arrangements, while your data splits will now be identical, the regression models built will not be necessarily so in the general case; here, if you keep the experiments like that, i.e. first with  max_depth:[2,3] and then with max_depth:2, the results will be identical indeed; but if you change it to, say,  first with  max_depth:[2,3] and then with max_depth:3, they will not, since in the first experiment, the run with max_depth:3 will start with a different state of the random number generator (i.e. the one after the run with max_depth:2 has finished).
There are limits to how identical you can make different runs under such conditions; for an example of a very subtle difference that nevertheless destroys the exact reproducibility between two experiments, see my answer in Why does the importance parameter influence performance of Random Forest in R?
",scikit-learn
how to create a scaler applying log transformation and minmaxscaler in sklearn,"I want to apply log() to my DataFrame and MinMaxScaler() together.
I want the output to be a pandas DataFrame() with indexes and columns from the original data.
I want to use the parameters used to fit_transform() to inverse_transform() resulting in a new data frame. So, it needs to be constructed inside the FunctionTransformer.
What I tried:
from sklearn.preprocessing import FunctionTransformer
from sklearn.preprocessing import MinMaxScaler
import pandas as pd
import numpy as np

# Initialize MinMaxScaler with range (0, 1)
scaler_logMinMax = MinMaxScaler(feature_range=(0, 1))

# Log transformation function
def log_and_scale(X, scaler=scaler_logMinMax, shift=1e-9):
    X_log = np.log(X + shift)  # Apply log transformation with a small shift
    return pd.DataFrame(scaler.fit_transform(X_log))  # Scale the log-transformed data


# Inverse transformation: first unscale, then inverse log transform
def inv_log_and_scale(X, scaler=scaler_logMinMax, shift=1e-9):
    X_unscaled = scaler.inverse_transform(X)  # Inverse scaling
    return np.exp(X_unscaled) - shift  # Inverse of log transformation

# Create FunctionTransformer for the log and scale transformation
log_and_scale_transformer = FunctionTransformer(func=log_and_scale, inverse_func=inv_log_and_scale, validate=True)
df_subset = pd.DataFrame(
    {
        1: [135.2342984, 83.17136704, 23.41329775, 3.574450787],
        2: [59.31328422, 18.15285711, 11.1736562, 4.788951527],
        3: [45.0087282, 4.094515245, 106.536704, 527.0962651],
    }
)
df_subset.columns = [1, 2, 3]
df_subset.index = [""201001"", ""201002"", ""201003"", ""201004""]
df_subset.index.name = ""Date""
df_subset.columns.name = ""id""
cols_to_apply_scaler = [1, 2]
df_subset

id  1           2           3
Date            
201001  135.234298  59.313284   45.008728
201002  83.171367   18.152857   4.094515
201003  23.413298   11.173656   106.536704
201004  3.574451    4.788952    527.096265

# Transforming
df_subset[cols_to_apply_scaler] = pd.DataFrame(log_and_scale_transformer.fit_transform(df_subset[cols_to_apply_scaler]))
df_subset

id  1   2   3
Date            
201001  NaN NaN 45.008728
201002  NaN NaN 4.094515
201003  NaN NaN 106.536704
201004  NaN NaN 527.096265
# The way that I expect to apply the inverse transformer.
# df_subset[cols_to_apply_scaler] = log_and_scale_transformer.inverse_transform(df_subset[cols_to_apply_scaler])

Questions:

The pd.DataFrame(log_and_scale_transformer.fit_transform(df_subset[cols_to_apply_scaler])) works, but it can't assign to the original DataFrame because the name of columns change. How to fix it?
How the values of scaler_logMinMax from fit_transform() were carried through the inverse_transform?

I also tried log_and_scale_transformer = log_and_scale_transformer.set_output(transform=""pandas"") after creating the dataframe, but it did not work.
I need to filter the columns before applying the function.
I also want to stick with FunctionTransformer because I use other transformers with the same structure. For ex:
# Define the inverse transformation function with a shift
def inv_y(X, shift=0.5):
    return 1 / (X + shift)

# Define the inverse inverse transformation to revert to original values
def inv_inv_y(X, shift=0.5):
    return (1 - X * shift) / X

# Create the FunctionTransformer
inverse_transformer = FunctionTransformer(func=inv_y, inverse_func=inv_inv_y, validate=False, check_inverse=True)

In summary, I cannot apply a function and a scaler together.

With a different simple example, it works:
# DataFrame Example
X = np.array([[0, 1, 2], [2, 3, 4], [5, 7, 9]])
cols = [""A"", ""B"", ""C""]
cols_to_apply_scaler = cols[:-1]
X = pd.DataFrame(X, columns=cols, index=[0,1,2])
X

    A   B   C
0   0   1   2
1   2   3   4
2   5   7   9

# Transforming
X[cols_to_apply_scaler] = pd.DataFrame(log_and_scale_transformer.fit_transform(X[cols_to_apply_scaler]))

    A           B           C
0   0.000000    0.000000    2
1   0.958971    0.564575    4
2   1.000000    1.000000    9

/home/guilherme/anaconda3/envs/time_series/lib/python3.11/site-packages/sklearn/base.py:493: UserWarning: X does not have valid feature names, but FunctionTransformer was fitted with feature names
  warnings.warn(

# Inverse
X[cols_to_apply_scaler] = log_and_scale_transformer.inverse_transform(X[cols_to_apply_scaler])
X

    A               B   C
0   6.203855e-25    1.0 2
1   2.000000e+00    3.0 4
2   5.000000e+00    7.0 9

But I did not understand the warning. Can I fix it?
","Concerning your first question, to preserve the index and columns work with Dataframes.
Concerning your second question, the values used in fit_transform() are carried through to inverse_transform() because the state of the scaler is stored internally within the object instance.
Full example based on you OP:
from sklearn.preprocessing import FunctionTransformer, MinMaxScaler
import pandas as pd, numpy as np


scaler_logMinMax = MinMaxScaler(feature_range=(0, 1))

# Log transformation function
def log_and_scale(X, scaler=scaler_logMinMax, shift=1e-9):
    X_log = np.log(X + shift)
    scaled = scaler.fit_transform(X_log)
    return pd.DataFrame(scaled, index=X.index, columns=X.columns)


def inv_log_and_scale(X, scaler=scaler_logMinMax, shift=1e-9):
    unscaled = scaler.inverse_transform(X)
    return pd.DataFrame(np.exp(unscaled) - shift, index=X.index, columns=X.columns)


log_and_scale_transformer = FunctionTransformer(
    func=log_and_scale, 
    inverse_func=inv_log_and_scale, 
    validate=False  # Allow pandas
)


df_subset = pd.DataFrame(
    {
        1: [135.2342984, 83.17136704, 23.41329775, 3.574450787],
        2: [59.31328422, 18.15285711, 11.1736562, 4.788951527],
        3: [45.0087282, 4.094515245, 106.536704, 527.0962651],
    },
    index=[""201001"", ""201002"", ""201003"", ""201004""]
)
df_subset.columns = [1, 2, 3]
df_subset.index.name = ""Date""
df_subset.columns.name = ""id""
cols_to_apply_scaler = [1, 2]

# fit and transform
df_subset[cols_to_apply_scaler] = log_and_scale_transformer.fit_transform(df_subset[cols_to_apply_scaler])


print(""Transformed DataFrame:"")
print(df_subset)

# inverse transform the same columns
df_subset[cols_to_apply_scaler] = log_and_scale_transformer.inverse_transform(df_subset[cols_to_apply_scaler])


print(""\nInverse Transformed DataFrame:"")
print(df_subset)

This prints
id           1         2           3
Date                                
201001  1.000000  1.000000   45.008728
201002  0.894048  0.789684    4.094515
201003  0.574869  0.649927  106.536704
201004  0.000000  0.000000  527.096265

and
id            1          2           3
Date                                 
201001  135.234298  59.313284   45.008728
201002   83.171367  18.152857    4.094515
201003   23.413298  11.173656  106.536704
201004    3.574451   4.788952  527.096265

",scikit-learn
matplotlib pltshow isn39t showing graph,"My plotting code doesn't seem to be showing the graph (lines 12 to 59 are probably not breaking it, but I included them just in case - I had data that took a while to put into a sorted list).
I've messed around with it and tried different things but I think the main problem is that I don't understand what figure(), plt.show(), import matplotlib.pyplot as plt,  from pylab import * and some other lines actually mean. I don't know when I need to use them or why I need to use them.  
Could someone help me and explain how to draw an .svg of two lists and have it show at the end with details about why each line is included and when I put plt. in front and when I put ax. in front and when I don't put anything in front, etc? Sorry this will take so long to answer, but I really don't understand matplotlib or any of the examples on their website.
import matplotlib
matplotlib.use('SVG')
import matplotlib.pyplot as plt
import string, math
from pylab import *
from decimal import *

name = raw_input(""Enter the filename:\n"")

myfile = open(""datafiles/""+name+"".data"", 'r')

xData = []
yData = []
plots = [name]
mydata = myfile.readlines()

i = 0
N = len(mydata)
while (i < N):
    string = mydata[i]
    data = [str(x) for x in string.split("" "")]
    data=filter(lambda x: len(x)>0, data)
    xData.append(data[1])
    yData.append(data[2])
    i = i + 1

i = 0
while (i < N):
    if (float(xData[i]) <= 0):
        xData[i] = """"
        yData[i] = """"
    if (xData[i] == ""nan"" or xData[i] == ""-nan""):
        xData[i] = """"
        yData[i] = """"
    i = i + 1

xData=filter(lambda x: len(x)>0, xData)
yData=filter(lambda x: len(x)>0, yData)

N = len(xData)
i = 0

while (i < N):
    xData[i] = float(xData[i])
    yData[i] = float(yData[i])
    i = i + 1

j = 0
while (j < N):
    i = 0 
    while (i < (N-j-1)):
        if (xData[i]>xData[i+1]):
            a, b = xData[i], xData[i+1]
            xData[i+1], xData[i] = a, b

            a, b = yData[i], yData[i+1]
            yData[i+1], yData[i] = a, b
        i = i + 1
    j = j + 1

plot = plt.figure(1)
plt.plot(xData, yData)
plt.show(1)
plt.savefig(name)

","You are attempting to use a backend that will not produce graphics with plt.show(). Instead you need to use another backend such as WXAgg or QT4agg, the selection of which will depend on your system. See this information on Matplotlib's backends. Instead, you should use only plt.savefig('filename.svg') if you desire to have a file in the svg format. The resulting file will be in your working directory, you only need to open it after your script has finished.
To elaborate a bit to answer some of your other questions about not understanding what individual lines mean: 
plt.show() will produce an interactive plot on your screen, assuming you are using a backend (renderer) that supports plotting to your user interface.
import matplotlib.pyplot as plt simply imports the functions and classes from the pyplot library from the matplotlib package, and the as plt part is sort of like making a nickname to access those functions easier. For example, plt.show(), plt.figure, etc. instead of having to type out matplotlib.pyplot.show() every time. On the other hand, from pylab import * imports all of the functions without the prefix. In general, I would avoid using import * because it can be confusing to read back your code later. Also, pylab probably isn't something you need for the code you've shown here.
plt.figure() is the command that initializes your figure. In this case, since you used plot = plt.figure, you can then type plot.plot(xData, yData), because your variable plot now is part of the figure class. You would use ax for example if you had some additional axes, subplots or color bars on which you needed to perform some action.
I would really recommend going through the pyplot tutorial on the matplotlib website to give you a more thorough, but still relatively brief and simple introduction to using matplotlib.
",matplotlib
how to implement linear interpolation,"Say I am given data as follows:
x = [1, 2.5, 3.4, 5.8, 6]
y = [2, 4, 5.8, 4.3, 4]

I want to design a function that will interpolate linearly between 1 and 2.5, 2.5 to 3.4, and so on using Python.
I have tried looking through this Python tutorial, but I am still unable to get my head around it.
","As I understand your question, you want to write some function y = interpolate(x_values, y_values, x), which will give you the y value at some x? The basic idea then follows these steps:

Find the indices of the values in x_values which define an interval containing x. For instance, for x=3 with your example lists, the containing interval would be [x1,x2]=[2.5,3.4], and the indices would be i1=1, i2=2 
Calculate the slope on this interval by (y_values[i2]-y_values[i1])/(x_values[i2]-x_values[i1]) (ie dy/dx).
The value at x is now the value at x1 plus the slope multiplied by the distance from x1.

You will additionally need to decide what happens if x is outside the interval of x_values, either it's an error, or you could interpolate ""backwards"", assuming the slope is the same as the first/last interval.
Did this help, or did you need more specific advice?
",python
how to make predictions with scikit39s surprise,"I'm having some trouble understanding the Surprise workflow. I have a file for training (which I seek to split into training and validation), and a file for testing data. I'm having trouble understanding the difference between a Surprise Dataset and Trainset
# Import data
data_dir = 'DIRECTORY_NAME'
reader = Reader(rating_scale=(1, 5))

# Create pandas dataframes
train_valid_df = pd.read_csv(os.path.join(data_dir, 'TRAINING_FILENAME.csv'))
train_df, valid_df = train_test_split(train_valid_df, test_size=0.2)
test_df = pd.read_csv(os.path.join(data_dir, 'TEST_FILENAME.csv'))

# Create surprise Dataset objects
train_valid_Dataset = Dataset.load_from_df(train_valid_df[['user_id', 'item_id', 'rating']], reader)
train_Dataset = Dataset.load_from_df(train_df[['user_id', 'item_id', 'rating']], reader)
valid_Dataset = Dataset.load_from_df(valid_df[['user_id', 'item_id', 'rating']], reader)
test_Dataset = Dataset.load_from_df(test_df[['user_id', 'item_id', 'rating']], reader)

# Create surprise Trainset object (and testset object?)
train_Trainset = train_data.build_full_trainset()
valid_Testset = trainset.build_anti_testset()

Then, I create my predictor:
algo = KNNBaseline(k=60, min_k=2, sim_options={'name': 'msd', 'user_based': True})

Now, if I want to cross validate I would do
cross_v = cross_validate(algo, all_data, measures=['mae'], cv=10, verbose=True)

Which trains the model (?), but if I wanted to use my fixed validation set, what would I do? This:?
algo.fit(train_Trainset)

After doing this, I tried to get some predictions:
predictions = algo.test(valid_Testset)
print(predictions[0])

With this being the result

But when I try to predict using item and user id numbers, it says such a prediction is impossible:
print(algo.predict('13', '194'))
print(algo.predict('260', '338'))
print(algo.predict('924', '559'))

Yielding:

The first user/item pair is from the training antiset, the second from the validation set, and the third from the training set. I don't know why this is behaving like this, and I've found the documentation confusing at times. Similarly, many tutorials online seem to be training on pandas dataframes which I get errors thrown for. Can anybody clarify what the workflow of surprise actually looks like? How do I train and then make predictions on a testing set?
","Since you have separate train and test, we create something similar to your data:
from surprise import Dataset, KNNBaseline, Reader
import pandas as pd
import numpy as np
from surprise.model_selection import cross_validate
reader = Reader(rating_scale=(1, 5))

train_df = pd.DataFrame({'user_id':np.random.choice(['1','2','3','4'],100),
                         'item_id':np.random.choice(['101','102','103','104'],100),
                         'rating':np.random.uniform(1,5,100)})

valid_df = pd.DataFrame({'user_id':np.random.choice(['1','2','3','4'],100),
                         'item_id':np.random.choice(['101','102','103','104'],100),
                         'rating':np.random.uniform(1,5,100)})

Then we need to convert the training data to a surprise.trainset , similar to what you have done:
train_Dataset = Dataset.load_from_df(train_df[['user_id', 'item_id', 'rating']], reader)
valid_Dataset = Dataset.load_from_df(valid_df[['user_id', 'item_id', 'rating']], reader)

train_Dataset = train_Dataset.build_full_trainset()

For fitting, you only need the train_Dataset, for the cross-validation, I am not sure what you are trying to do and i see it's out of the scope of question for prediction, so we fit:
algo = KNNBaseline(k=60, min_k=2, sim_options={'name': 'msd', 'user_based': True})
algo.fit(train_Dataset)

To predict, you need to provide the input as a list or array, which has the same shape as your input, so for example, if we want to provide the test Dataset, it will be:
testset = [valid_Dataset.df.loc[i].to_list() for i in range(len(valid_Dataset.df))]
algo.test(testset)[:2] 

[Prediction(uid='2', iid='103', r_ui=3.0224818872683845, est=2.8486558674146125, details={'actual_k': 25, 'was_impossible': False}),
 Prediction(uid='2', iid='103', r_ui=4.609064535195377, est=2.8486558674146125, details={'actual_k': 25, 'was_impossible': False})]

If you wanna test one or two values, it will be:
algo.test([['1','101',None]])

",scikit-learn
implementing bag of words in scikitlearn,"from sklearn.feature_extraction.text import CountVectorizer
import pandas as pd
headers = ['label', 'sms_message']
df = pd.read_csv ('spam.csv', names = headers)
df ['label'] = df['label'].map({'ham': 0, 'spam': 1})
print (df.head(7))
print (df.shape)
count_vector = CountVectorizer()
#count_vector.fit(df)
y = count_vector.fit_transform(df)
count_vector.get_feature_names()
doc_array = y.toarray()
print (doc_array)
frequency_matrix = pd.DataFrame(doc_array, columns = count_vector.get_feature_names())
frequency_matrix

Sample data and output:
   label                                        sms_message
0      0  Go until jurong point, crazy.. Available only ...
1      0                      Ok lar... Joking wif u oni...
2      1  Free entry in 2 a wkly comp to win FA Cup fina...
3      0  U dun say so early hor... U c already then say...

(5573, 2)
[[1 0]
 [0 1]]

label   sms_message
0   1   0
1   0   1

My Question:
My csv file is basically many rows of sms messages.
I cannot understand why I am getting only output for the column labels and not for the entire rows of sms texts.
Thank you for any help.
","Pass only the sms_message column to count vectorizer as shown below.
import numpy as np
import pandas as pd
from sklearn.feature_extraction.text import CountVectorizer

docs = ['Tea is an aromatic beverage..',
        'After water, it is the most widely consumed drink in the world',
        'There are many different types of tea.',
        'Tea has a stimulating effect in humans.',
        'Tea originated in Southwest China during the Shang dynasty'] 

df = pd.DataFrame({'sms_message': docs, 'label': np.random.choice([0, 1], size=5)})

cv = CountVectorizer()
counts = cv.fit_transform(df['sms_message'])

df_counts = pd.DataFrame(counts.toarray(), columns=cv.get_feature_names_out())
df_counts['label'] = df['label']

Output:
df_counts

Out[26]: 
   after  an  are  aromatic  beverage  ...  types  water  widely  world  label
0      0   1    0         1         1  ...      0      0       0      0      1
1      1   0    0         0         0  ...      0      1       1      1      0
2      0   0    1         0         0  ...      1      0       0      0      1
3      0   0    0         0         0  ...      0      0       0      0      1
4      0   0    0         0         0  ...      0      0       0      0      0

[5 rows x 32 columns]

",scikit-learn
finetuning model39s classifier layer with new label,"I would like to fine-tune already fine-tuned BertForSequenceClassification model with new dataset containing just 1 additional label which hasn't been seen by model before.
By that, I would like to add 1 new label to the set of labels that model is currently able of classifying properly.
Moreover, I don't want classifier weights to be randomly initialized, I'd like to keep them intact and just update them accordingly to the dataset examples while increasing the size of classifier layer by 1.
The dataset used for further fine-tuning could look like this:
sentece,label
intent example 1,new_label
intent example 2,new_label
...
intent example 10,new_label

My model's current classifier layer looks like this:
Linear(in_features=768, out_features=135, bias=True)

How could I achieve it?
Is it even a good approach?
","You can just extend the weights and bias of your model with new values. Please have a look at the commented example below:
#This is the section that loads your model
#I will just use an pretrained model for this example
import torch
from torch import nn
from transformers import AutoModelForSequenceClassification, AutoTokenizer
tokenizer = AutoTokenizer.from_pretrained(""jpcorb20/toxic-detector-distilroberta"")
model = AutoModelForSequenceClassification.from_pretrained(""jpcorb20/toxic-detector-distilroberta"")
#we check the output of one sample to compare it later with the extended layer
#to verify that we kept the previous learnt ""knowledge""
f = tokenizer.encode_plus(""This is an example"", return_tensors='pt')
print(model(**f).logits)

#Now we need to find out the name of the linear layer you want to extend
#The layers on top of distilroberta are wrapped inside a classifier section
#This name can differ for you because it can be chosen randomly
#use model.parameters instead find the classification layer
print(model.classifier)

#The output shows us that the classification layer is called `out_proj`
#We can now extend the weights by creating a new tensor that consists of the
#old weights and a randomly initialized tensor for the new label 
model.classifier.out_proj.weight = nn.Parameter(torch.cat((model.classifier.out_proj.weight, torch.randn(1,768)),0))

#We do the same for the bias:
model.classifier.out_proj.bias = nn.Parameter(torch.cat((model.classifier.out_proj.bias, torch.randn(1)),0))

#and be happy when we compare the output with our expectation 
print(model(**f).logits)

Output:
tensor([[-7.3604, -9.4899, -8.4170, -9.7688, -8.4067, -9.3895]],
       grad_fn=<AddmmBackward>)
RobertaClassificationHead(
  (dense): Linear(in_features=768, out_features=768, bias=True)
  (dropout): Dropout(p=0.1, inplace=False)
  (out_proj): Linear(in_features=768, out_features=6, bias=True)
)
tensor([[-7.3604, -9.4899, -8.4170, -9.7688, -8.4067, -9.3895,  2.2124]],
       grad_fn=<AddmmBackward>)

Please note, that you should fine-tune your model. The new weights are randomly initialized and will therefore negatively impact the performance.
",huggingface-transformers
getting quottypeerror fit missing 1 required positional argument 39y39quot for linear regression in python,"What I'm trying to do:

Fit a linear regression model on data from PCA transformation
Use that linear regression model to perform cross-validation

ks = [1,2,3,4,5,6,8,10,12,15,20]
mean_val_mse = []
# loop through all k values
for k in ks:
    # instantiate pca and set n_components = k
    pca = PCA(n_components=k)
    # fit and transform
    # fit learns the number of components, transform actually reduces the dimensions
    x_train_pca = pca.fit_transform(X_train_scaled)
    
    # create linreg and fit it on pca data
    lr = LinearRegression().fit(x_train_pca, y_train)
    cv = cross_validate(lr, X=x_train_pca, cv=10, return_train_score=True, scoring='neg_mean_squared_error', error_score='raise')

When I do so, my cross-validation is throwing the error:
TypeError: fit() missing 1 required positional argument: 'y'
from line: cv = cross_validate(lr, X=x_train_pca, cv=10, return_train_score=True, scoring='neg_mean_squared_error', error_score='raise')

I've checked that my y_train is the correct shape and run through the syntax but I'm not seeing a reason why it would give this error.  Am I instantiating the LinearRegression wrong or is the error somewhere else?
","I have never dealt with Machine Learning, but the error message says you didn't supply y into that last line, so maybe try
cv = cross_validate(lr, X=x_train_pca, y=y_train, cv=10, return_train_score=True, scoring='neg_mean_squared_error', error_score='raise')

source:
https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_validate.html#sklearn.model_selection.cross_validate
",cross-validation
how to create nonalphabetically ordered categorical column in polars dataframe,"In Pandas, you can create an ""ordered"" Categorical column from existing string column as follows:
column_values_with_custom_order = [""B"", ""A"", ""C""]

df[""Column""] = pd.Categorical(df.Column, categories=column_values_with_custom_order, ordered=True)

In Polars documentation, I couldn't find such way to create ordered columns. However, I could reproduce this by using pl.from_pandas(df) so I suspect that this is possible with Polars as well.
What would be the recommended way to this?
I tried to create new column with:
polars_df.with_columns(col(""Column"").cast(pl.Categorical))

but I don't know how to include the custom ordering to this.
I also checked In polars, can I create a categorical type with levels myself?, but I would prefer not to add another column to my Dataframe only for ordering.
","Say you have
df = pl.DataFrame(
     {""cats"": [""z"", ""z"", ""k"", ""a"", ""b""], ""vals"": [3, 1, 2, 2, 3]}
     )

and you want to make cats a categorical but you want the categorical ordered as
myorder=[""k"", ""z"", ""b"", ""a""]

There are two ways to do this.  One way is with pl.StringCache() as in the question you reference and the other is more messy.  The former does not require you add any columns to your df.  It's actually very succinct.
with pl.StringCache():
    pl.Series(myorder).cast(pl.Categorical)
    df=df.with_columns(pl.col('cats').cast(pl.Categorical))

What happens is that everything in the StringCache gets the same key values so when the myorder list is casted that saves what keys get allocated to each string value.  When your df gets casted under the same cache it gets the same key/string values which are in the order you wanted.
The other way to do this is as follows:
You have to sort your df by the ordering then you can do set_ordering('physical').  If you want to maintain your original order then you just have to use with_row_count at the beginning so you can restore that order.
Putting it all together, it looks like this:
df=df.with_row_index('i').join(
        pl.from_dicts([{'order':x, 'cats':y} for x,y in enumerate(myorder)]), on='cats') \
    .sort('order').drop('order') \
    .with_columns(pl.col('cats').cast(pl.Categorical('physical'))) \
    .sort('i').drop('i')

You can verify by doing:
df.select('cats',pl.col('cats').to_physical().alias('phys'))

shape: (5, 2)
┌──────┬──────┐
│ cats ┆ phys │
│ ---  ┆ ---  │
│ cat  ┆ u32  │
╞══════╪══════╡
│ z    ┆ 1    │
│ z    ┆ 1    │
│ k    ┆ 0    │
│ a    ┆ 3    │
│ b    ┆ 2    │
└──────┴──────┘

",data-science
building ml classifier with imbalanced data,"I have a dataset with 1400 obs and 19 columns. The Target variable has values 1 (value that I am most interested in) and 0. The distribution of classes shows imbalance (70:30).
Using the code below I am getting weird values (all 1s). I am not figuring out if this is due to a problem of overfitting/imbalance data or to feature selection (I used Pearson correlation since all values are numeric/boolean).
I am thinking that the steps followed are wrong.
import numpy as np
import math
import sklearn.metrics as metrics
from sklearn.metrics import f1_score

y = df['Label']
X = df.drop('Label',axis=1)

def create_cv(X,y):
    if type(X)!=np.ndarray:
        X=X.values
        y=y.values
 
    test_size=1/5
    proportion_of_true=y[y==1].shape[0]/y.shape[0]
    num_test_samples=math.ceil(y.shape[0]*test_size)
    num_test_true_labels=math.floor(num_test_samples*proportion_of_true)
    num_test_false_labels=math.floor(num_test_samples-num_test_true_labels)
    
    y_test=np.concatenate([y[y==0][:num_test_false_labels],y[y==1][:num_test_true_labels]])
    y_train=np.concatenate([y[y==0][num_test_false_labels:],y[y==1][num_test_true_labels:]])

    X_test=np.concatenate([X[y==0][:num_test_false_labels] ,X[y==1][:num_test_true_labels]],axis=0)
    X_train=np.concatenate([X[y==0][num_test_false_labels:],X[y==1][num_test_true_labels:]],axis=0)
    return X_train,X_test,y_train,y_test

X_train,X_test,y_train,y_test=create_cv(X,y)
X_train,X_crossv,y_train,y_crossv=create_cv(X_train,y_train)
    
tree = DecisionTreeClassifier(max_depth = 5)
tree.fit(X_train, y_train)       

y_predict_test = tree.predict(X_test)

print(classification_report(y_test, y_predict_test))
f1_score(y_test, y_predict_test)

Output:
     precision    recall  f1-score   support

           0       1.00      1.00      1.00        24
           1       1.00      1.00      1.00        70

    accuracy                           1.00        94
   macro avg       1.00      1.00      1.00        94
weighted avg       1.00      1.00      1.00        94

Has anyone experienced similar issues in building a classifier when data has imbalance, using CV and/or under sampling? Happy to share the whole dataset, in case you might want to replicate the output.
What I would like to ask you for some clear answer to follow that can show me the steps and what I am doing wrong.
I know that, to reduce overfitting and work with balance data, there are some methods such as random sampling (over/under), SMOTE, CV. My idea is

Split the data on train/test taking into account imbalance
Perform CV on trains set
Apply undersampling only on a test fold
After the model has been chosen with the help of CV, undersample the train set and train the classifier
Estimate the performance on the untouched test set
(f1-score)

as also outlined in this question: CV and under sampling on a test fold .
I think the steps above should make sense, but happy to receive any feedback that you might have on this.
","When you have imbalanced data you have to perform stratification. The usual way is to oversample the class that has less values.
Another option is to train your algorithm with less data. If you have a good dataset that should not be a problem. In this case you grab first the samples from the less represented class use the size of the set to compute how many samples to get from the other class:
This code may help you split your dataset that way:
def split_dataset(dataset: pd.DataFrame, train_share=0.8):
    """"""Splits the dataset into training and test sets""""""
    all_idx = range(len(dataset))
    train_count = int(len(all_idx) * train_share)

    train_idx = random.sample(all_idx, train_count)
    test_idx = list(set(all_idx).difference(set(train_idx)))

    train = dataset.iloc[train_idx]
    test = dataset.iloc[test_idx]

    return train, test

def split_dataset_stratified(dataset, target_attr, positive_class, train_share=0.8):
    """"""Splits the dataset as in `split_dataset` but with stratification""""""

    data_pos = dataset[dataset[target_attr] == positive_class]
    data_neg = dataset[dataset[target_attr] != positive_class]

    if len(data_pos) < len(data_neg):
        train_pos, test_pos = split_dataset(data_pos, train_share)
        train_neg, test_neg = split_dataset(data_neg, len(train_pos)/len(data_neg))
        # set.difference makes the test set larger
        test_neg = test_neg.iloc[0:len(test_pos)]
    else:
        train_neg, test_neg = split_dataset(data_neg, train_share)
        train_pos, test_pos = split_dataset(data_pos, len(train_neg)/len(data_pos))
        # set.difference makes the test set larger
        test_pos = test_pos.iloc[0:len(test_neg)]

    return train_pos.append(train_neg).sample(frac = 1).reset_index(drop = True), \
           test_pos.append(test_neg).sample(frac = 1).reset_index(drop = True)

Usage:
train_ds, test_ds = split_dataset_stratified(data, target_attr, positive_class)

You can now perform cross validation on train_ds and evaluate your model in test_ds.
",cross-validation
how do i persist faiss indexes,"In the langchain wiki of FAISS, https://python.langchain.com/v0.2/docs/integrations/vectorstores/faiss/, it only talks about saving indexes to files.
db.save_local(""faiss_index"")

new_db = FAISS.load_local(""faiss_index"", embeddings)

docs = new_db.similarity_search(query)

How can I save the indexes to databases, such that we can organize and concurrently access multiple indexes?
Searched online but could not get much info on this.
Can FAISS be used with any kind of distributed databases?
","In fact, FAISS is considered as an in-memory database itself in order to vector search based on similarity that you can serialize and deserialize the indexes using functions like write_index and read_index within the FAISS interface directly or using save_local and load_local within the LangChain integration which typically uses the pickle for serialization.
If you need to store serialized files, you could manually save them in a NoSQL database like MongoDB as binary data, and then deserialize and retrieve them when needed, however, it is not the best practice!
If you are looking for a vector database that is not in-memory and capable in a scalable system, you might want to consider using Milvus which is designed for this purpose.
",langchain
disable scientific notation and offset in pandas plot function,"I have a dataframe df with 2 columns I want to plot together and days as index:
           | col1  | col2   | col3 | ...
2020-01-01 | 1     | 300000 | ...
2020-01-02 | 1000  | 600000 | ...
2020-01-03 | 3000  | 50000  | ...

Plotting col1 + col2 via
df[[""col1"", ""col2""]].plot()

shows values from 0 till 1.0 and at the top ""1e6"" like in this example: https://i.sstatic.net/tJjgX.png
I want the full value range on the y-axis and no scientific notation. How can I do this via pandas .plot() or matplotlib?
","You have several options:
Option One: With Matplotlib
axes=fig.add_axes([0,0,1,1])
axes.set_xticks() # with list or range() inside
axes.set_yticks() # with list or range() inside

#You can also label the ticks with your desired values
axes.set_xticklabels() # with list or range() inside
axes.set_yticklabels() # with list or range() inside

Option Two: Change Pandas settings
pd.set_option('display.float_format', lambda x: '%.3f' % x)

or
pd.options.display.float_format = '{:.2f}'.format

I believe option one is better since you need it only for the graph and don't necessarily want to modify your dataframe columns.
Cheers!
",pandas
why is this power calculating function crashing,"I tried following function (as suggested on these forums) to calculate power. However, it is causing program to hang up.
static long ipow(int b, int e) {
    long r = 1;
    while (e--) r *= b;
    return r;
}

double cfilefn(int a, int b, int c) {
    return (ipow(a, b) / (double)c); 
}

cfilefn(2,3,4);

The function looks all right. Where is the error and how can it be solved?
","The ipow function will misbehave if the second argument is a negative number: it will run for a while and have implementation defined behavior when e reaches INT_MIN. You should modify the test while (e--) r *= b; as:
static long ipow(int b, int e) {
    long r = 1;
    while (e-- > 0)
        r *= b;
    return r;
}

Note however that ipow will cause arithmetic overflow for moderately large values of e and since you want a double result from cfilefn, you should use double arithmetics for the power function:
#include <math.h>

double cfilefn(int a, int b, int c) {
    return pow(a, b) / c; 
}

",c
received server error 500 while deploying huggingface model on sgaemaker,"I've successfully fine tuned a sentence-transformers model all-MiniLM-L12-v2 on our data in SageMaker Studio and the model was saved in S3 as a model.tar.gaz.
I want to deploy this model for inference (all code snippets included below). According to HuggingFace doc these types of model required a Custom Inference module. So I've downloaded and unpacked the model.tar.gz created, then followed the tutorial to add the code/inference.py and pushed it back to S3 as new model.tar.gz
The endpoint is created successfully, but as soon as I call the predictor.predict() it crashes with the following error:
ModelError: An error occurred (ModelError) when calling the InvokeEndpoint operation: Received server error (500) from primary with message ""{
  ""code"": 500,
  ""type"": ""InternalServerException"",
  ""message"": ""Worker died.""
}

looking in CloudWatch I got a lot of info messages, where the instance seems to be setting up successfully then I get this warning message:
2024-07-30T13:19:09,702 [WARN ] W-9000-model_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: model, error: Worker died.

Here are the relevant code snippets:
End point creation:
from sagemaker.huggingface.model import HuggingFaceModel
from sagemaker import get_execution_role, image_uris

role            = get_execution_role()
estimator_image = image_uris.retrieve(framework='pytorch',region='eu-west-1',version='2.0.0',py_version='py310',image_scope='inference', instance_type='ml.g5.4xlarge')
sm_model_ref    = model_path

# create Hugging Face Model Class
huggingface_model = HuggingFaceModel(
    model_data    = sm_model_ref,
    role          = role,                                                     
    image_uri     = estimator_image,
)

The custom inference.py file and its location in the model.tar.gz:
%%writefile models/model/code/inference.py

import torch

# Create a custom inference to overwrite the default method
def predict_fn(data, model):

    # create sentences pair
    sentences1 = data[""premise""]
    sentences2 = data[""hypothesis""]
 
    # Compute token embeddings
    with torch.no_grad():
        embeddings1 = model.encode(sentences1)
        embeddings2 = model.encode(sentences2)
        
        # Compute cosine similarities        
        similarities = model.similarity(embeddings1, embeddings2)
 
    return similarities

And its location:
model.tar.gz
 |_ _ 1_Pooling
 |_ _ 2_Normalize
 |_ _ checkpoint-8300
 |_ _ checkpoint-8334
 |_ _ code
   |_ _ inference.py
 |_ _ config_sentence_transformers.json
 |_ _ config.json
 |_ _ model.safetensors
 |_ _ module.json
 |_ _ README.md
 |_ _ sentence_bert_config.json
 |_ _ special_token_map.json
 |_ _ tokenizer_config.json
 |_ _ tokenizer.json
 |_ _ vocab.txt

","It seems most of the doc on the topic, including HuggingFace doc was out of date. You no longer need the repackage the model.tar.gz with code/inference.py
All I had to do was pass the S3 path to my initial model.tar.gz after training to the estimator, and pass the location of inference.py and requirements.txt in the source_dir and entry_point.
huggingface_model = HuggingFaceModel(
    entry_point   = 'inference.py',
    source_dir    = 'code',
    model_data    = sm_model_ref,
    role          = role,                                                     # IAM role with permissions to create an endpoint
    image_uri     = estimator_image,
)

",huggingface-transformers
bash prompt line setting issue,"I am using xshell to connect a cloud service of centos, and I set the $PS1 value in /etc/bashrc as \e[0;34m[\u@\h \W]$ \e[m which makes my promt a blue color so that I can tell which is my command input and which is the output.
However, this prompt could not automatically add a new line if my command is more than one line. If one line is full, it just starts padding from the left of the same line. You can see the screenshots as follows:


What I want is that the command can automatically add a new line when one line is full.
I tried \n but that just add a new line before command which is not the effect I want.
Now I tried PS1='[\e[0;34m[\u@\h \W]$ \e[m]', the effect is like:


","bash can't tell how much space your prompt actually occupies on screen, because the ANSI escape sequences that set colors don't take any space. You need to enclose them (and only them) inside \[...\] to tell bash as much.
PS1='\[\e[0;34m\][\u@\h \W]$ \[\e[m\]'

bash already knows how to handle its own escape sequences \u, \h, and \W. The ANSI escape sequences only have special meaning to the terminal.
That is, \u et al are expanded before bash tries to determine how many characters are in the prompt. For all it knows, \e, [, 0, ;, 3, 4, and m will all be displayed literally as single characters. The terminal sees them, and instead of displaying them, changes the color used to print the following characters.
",prompt
typeerror expected a runnable callable or dictinstead got an unsupported type ltclass 39list39gt,"I am experimenting with LLM development.
Here is my code:
import langchain, pydantic, transformers
from langchain import HuggingFacePipeline
from langchain_core.prompts import PromptTemplate
from langchain_core.runnables.base import RunnableSequence, RunnableMap, RunnableLambda

from langchain.callbacks import get_openai_callback
from pydantic import BaseModel, Field
from langchain.output_parsers import PydanticOutputParser
from transformers import pipeline

class MedicalSpecialty(BaseModel):
    medical_specialty: str = Field(description=""medical specialty the patient should go to"")
    urgent: bool = Field(description=""the patient should go to the hospital immediately"")

parser = PydanticOutputParser(pydantic_object=MedicalSpecialty)

queries = [""i have ache in my chest and in my left arm. Which medical specialty should I go to?""]

template = """"""
Question: {question}
""""""
prompt = PromptTemplate(template=template, input_variables=[""question""])


llm = HuggingFacePipeline.from_model_id(
    model_id=""bigscience/bloom-1b7"",
    task=""text-generation"",
    model_kwargs={""max_length"": 1024},
    device=-1  # Ensure it runs on CPU for macOS M1
)

# Wrap the prompt in a RunnableLambda to make it a Runnable
prompt_runnable = RunnableLambda(lambda x: prompt.format(**x))

# Define the sequence that includes the prompt and LLM
sequence = RunnableSequence([
    prompt_runnable,
    llm
])

with get_openai_callback() as CB:
    for query in queries:
        result = sequence.invoke({""question"": query})
        print(query)
        print(result)
        print(""===================================="")
    
    # Print the costs of the requests
    print(cb)

Unfortunately, after several iterations, I keep getting this error:
TypeError                                 Traceback (most recent call last)
Cell In[6], line 19
     16 prompt_runnable = RunnableLambda(lambda x: prompt.format(**x))
     18 # Define the sequence that includes the prompt and LLM
---> 19 sequence = RunnableSequence([
     20     prompt_runnable,
     21     llm
     22 ])
     24 with get_openai_callback() as CB:
     25     for query in queries:

File /opt/anaconda3/envs/LLM/lib/python3.11/site-    packages/langchain_core/runnables/base.py:2632, in RunnableSequence.__init__(self,     name, first, middle, last, *steps)
   2630         steps_flat.extend(step.steps)
   2631     else:
-> 2632         steps_flat.append(coerce_to_runnable(step))
   2633 if len(steps_flat) < 2:
   2634     raise ValueError(
   2635         f""RunnableSequence must have at least 2 steps, got     {len(steps_flat)}""
   2636     )

File /opt/anaconda3/envs/LLM/lib/python3.11/site-    packages/langchain_core/runnables/base.py:5554, in coerce_to_runnable(thing)
   5552     return cast(Runnable[Input, Output], RunnableParallel(thing))
   5553 else:
-> 5554     raise TypeError(
   5555         f""Expected a Runnable, callable or dict.""
   5556         f""Instead got an unsupported type: {type(thing)}""
   5557     )

TypeError: Expected a Runnable, callable or dict.Instead got an unsupported type:     <class 'list'>

Please, someone help!
","No need to use lambda for this simple prompt with one input.
Try to use below and report back here on what happens
prompt_runnable = PromptRunnable(prompt)

",langchain
creating new string with sorted letters from a string word in java,"How do I create a String with alphabetical order letters taken from another String?
Let's say I have something like this
String theWord = ""Hello World"";

How do I compute the new String to make it look like""

dehllloorw

Which is theWord but sorted character by character in alphabetical order.
Thanks in advance
","char[] chars = theWord.toCharArray();
Arrays.sort(chars);
String newWord = new String(chars);

",java
how to improve code performance  using google translate api ,"import time
start = time.time()
import pandas as pd
from deep_translator import GoogleTranslator
    
data = pd.read_excel(r""latestdata.xlsx"")
translatedata = data['column']. fillna('novalue')
    
list = []
for i in translatedata:
      finaldata = GoogleTranslator(source='auto', target='english').translate(i)
      print(finaldata)
      list.append(finaldata)
    
df = pd.DataFrame(list, columns=['Translated_values'])
df.to_csv(r""jobdone.csv"", sep= ';')
    
end = time.time()

print(f""Runtime of the program is {end - start}"")

I have data of 220k points and trying to translate a column data At first I tried to use pool method parallel program but got an error that I can not access API several time at once. My question is if there is other way to improve performance of code that I have right now.
# 4066.826668739319     with just 10000 data all together.
# 3809.4675991535187    computation time when I run in 2 batch's of 5000

","
Q :"" ... is ( there ) other way to improve performance of code ...? ""

A :Yes, there are a few ways,yet do not expect anything magical, as you have already reported the API-provider's throttling/blocking somewhat higher levels of concurrent API-call from being served
There still might be some positive effects from latency-masking tricks from a just-[CONCURRENT] orchestration of several API-calls, as the End-to-End latencies are principally ""long"" as going many-times across the over-the-""network""-horizons and having also some remarkable server-side TAT-latency on translation-matching engines.
Details matter, a lot...
A performance boosting code-template to start with( avoiding 220k+ repeated local-side overheads' add-on costs ) :
import time
import pandas as pd
from   deep_translator import GoogleTranslator as gXLTe
    
xltDF = pd.read_excel( r""latestdata.xlsx"" )['column'].fillna( 'novalue' )
resDF = xltDF.copy( deep = True )

PROC_ns_START = time.perf_counter_ns()
#________________________________________________________ CRITICAL SECTION: start
for                  i in range( len( xltDF ) ):
         resDF.iloc( i ) = gXLTe( source = 'auto',
                                  target = 'english'
                                  ).translate( xltDF.iloc( i ) )

#________________________________________________________ CRITICAL SECTION: end
PROC_ns_END = time.perf_counter_ns()

resDF.to_csv( r""jobdone.csv"",
              sep = ';'
              )

print( f""Runtime was {0:} [ns]"".format( PROC_ns_END - PROC_ns_START ) )

Tips for performance boosting :

if Google API-policy permits, we may increase thread-count, that participate on CRITICAL SECTION,
as the Python-interpreter threads are ""inside"" the same address-space and still are GIL-lock MUTEX-blocked, we may operate all just-[CONCURRENT] accesses to the same DataFrame-objects, best using non-overlapping, separate (thread-private) block-iterators over disjunct halves ( for a pair of threads ) over disjunct thirds ( for 3 threads ) etc...
as the Google API-policy is limiting attempts to overly concurrent access to the API-service, you shall build-in some, even naive-robustness

def thread_hosted_blockCRAWLer( i_start, i_end ):
    for i in range( i_start, i_end ):
        while True:
              try:
                  resDF.iloc( i ) = gXLTe( source = 'auto',
                                           target = 'english'
                                           ).translate( xltDF.iloc( i ) )
                  # SUCCEDED
                  break
              except:
                  # FAILED
                  print( ""EXC: _blockCRAWLer() on index "", i )
                  time.sleep( ... )
                  # be careful here, not to get on API-provider's BLACK-LIST
                  continue


if more time-related details per thread, may reuse this

Do not hesitate to go tuning & tweaking - and anyway, keep us posted how fast you managed to get, that's fair, isn't it?
",pandas
how do i pass sklearns train_test_split actual dataseries and not single values as input argument,"I want to train an LSTM-based RNN model for binary classification and for that I wanted to use tensorflow keras model with LSTM layers. In order to do so, I need testing input and output as well as validation input and output, which I wanted to generate with sklearns train_test_split.
def prepare_data(self, satellites):
        """"""
        Prepare time-series data for RNN.
        """"""
        feature_sequences = []
        labels = []
        
        for sat in satellites:
            if sat.manoeuvrability is not None:
                # Stack the orbital parameters as time-series features (epochs will be the time dimension)
                features = np.column_stack((
                    sat.apoapses,
                    sat.periapses,
                    sat.inclinations,
                    sat.mean_motions,
                    sat.eccentricities,
                    sat.semimajor_axes,
                    sat.orbital_energy
                ))
                feature_sequences.append(features)
                labels.append(sat.manoeuvrability)
        
        X = np.array(feature_sequences, dtype=object)
        y = np.array(labels)
        
        return train_test_split(X, y, test_size=0.2, random_state=42)

train_test_split returns me a None-Value. Removing the dtype=object cast in the argument leads me to an

ValueError: setting an array element with a sequence. The requested
array has an inhomogeneous shape after 1 dimensions. The detected
shape was (73,) + inhomogeneous part.

How do I properly form my features vector for sklearns train_test_split if I want to pass literal timeseries as arguments? The time-dependence is important in my case, so I really cant work around with manually breaking down time series to the average or something
","I simplified your code to this:
def prepare_data():
        feature_sequences = []
        labels = []
        
        for i in range(10):

            features = np.column_stack((2*i*5, ""hello""))  # wrong?
            # features = (2*i*5, ""hello"")  # correct

            feature_sequences.append(features)
            labels.append(i)

        X = np.array(feature_sequences, dtype=object)
        y = np.array(labels)
        
        return train_test_split(X, y, test_size=0.2, random_state=42)

The returned split for the features in a 3D array which it shouldn't be. Just replace the line with column_stack and the resulting split looks better.
",scikit-learn
error using llmfactory with quottheblokeopenhermes25mistral7bggufquot huggingface,"I tried replicating a simple Python code to create a small LLM model.
I have macOS M1 machine.
I created a separate environment where I installed Pytorch and llama-cpp-python. The code:
from llmflex import LlmFactory

# Load the model from Huggingface
try:
    # Instantiate the model with the correct identifier
    model = LlmFactory(""TheBloke/OpenHermes-2.5-Mistral-7B-GGUF"")

    # Configure parameters directly if the object itself is callable
    #llm = model(temperature=0.7, max_new_tokens=512)

    # Disable Metal and run on CPU
    llm = model(temperature=0.7, max_new_tokens=512, use_metal=False)

    # Generate a response
    response = llm.generate(""Hello, how are you?"")
    print(response)

except AttributeError as e:
    print(f""Attribute error: {e}"")
except AssertionError as e:
    print(f""Assertion error: {e}"")
except Exception as e:
    print(f""An error occurred: {e}"")

As you can see, I tried with and without Metal, but I received the same error (the last portion of the output):
llm_load_vocab: special tokens definition check successful ( 261/32002 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = SPM
llm_load_print_meta: n_vocab          = 32002
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: n_ctx_train      = 32768
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 8
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_gqa            = 4
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: n_ff             = 14336
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 32768
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q2_K
llm_load_print_meta: model params     = 7.24 B
llm_load_print_meta: model size       = 2.87 GiB (3.41 BPW) 
llm_load_print_meta: general.name     = teknium_openhermes-2.5-mistral-7b
llm_load_print_meta: BOS token        = 1 '<s>'
llm_load_print_meta: EOS token        = 32000 '<|im_end|>'
llm_load_print_meta: UNK token        = 0 '<unk>'
llm_load_print_meta: PAD token        = 0 '<unk>'
llm_load_print_meta: LF token         = 13 '<0x0A>'
llm_load_tensors: ggml ctx size =    0.11 MiB
llm_load_tensors: mem required  = 2939.69 MiB

llama_new_context_with_model: n_ctx      = 4096
llama_new_context_with_model: freq_base  = 10000.0
llama_new_context_with_model: freq_scale = 1
llama_new_context_with_model: KV self size  =  512.00 MiB, K (f16):  256.00 MiB, V     (f16):  256.00 MiB
llama_build_graph: non-view tensors processed: 676/676
ggml_metal_init: allocating
ggml_metal_init: found discrete device: Apple M1
ggml_metal_init: picking device: Apple M1
ggml_metal_init: default.metallib not found, loading from source
ggml_metal_init: GGML_METAL_PATH_RESOURCES = nil
ggml_metal_init: error: could not use bundle path to find ggml-metal.metal, falling     back to trying cwd
ggml_metal_init: loading 'ggml-metal.metal'
ggml_metal_init: error: Error Domain=NSCocoaErrorDomain Code=260 ""The file “ggml-    metal.metal” couldn’t be opened because there is no such file."" UserInfo=.   {NSFilePath=ggml-metal.metal, NSUnderlyingError=0x600002eeb2a0 {Error     Domain=NSPOSIXErrorDomain Code=2 ""No such file or directory""}}
llama_new_context_with_model: ggml_metal_init() failed
AVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 |     NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 =     0 | SSSE3 = 0 | VSX = 0 | 

Assertion error: 

Obviously, something is wrong, but I cannot pinpoint the error because I am new to this.
I do not want to use CUDA; I want to use the CPU.
Please, help
Here is some additional information: https://huggingface.co/TheBloke/OpenHermes-2.5-Mistral-7B-GGUF
","I guess it is an issue when using it with MacOS M1. Metal has some problems that have not been fully resolved. I am closing this, but if there is an answer, please DM me
",large-language-model
numpy float64 vs python float,"I'm battling some floating point problems in Pandas read_csv function.  In my investigation, I found this:
In [15]: a = 5.9975

In [16]: a
Out[16]: 5.9975

In [17]: np.float64(a)
Out[17]: 5.9974999999999996

Why is builtin float of Python and the np.float64 type from Python giving different results?  I thought they were both C++ doubles?
",">>> numpy.float64(5.9975).hex()
'0x1.7fd70a3d70a3dp+2'
>>> (5.9975).hex()
'0x1.7fd70a3d70a3dp+2'

They are the same number. What differs is their representation; the Python native type uses a ""sane"" representation, and the NumPy type uses an accurate representation.
",numpy
what is the default accuracy scoring in cross_val_score in sklearn,"I have a regression model made using random-Forest. I made pipelines using scikit-learn to process data and now have used RandomForestRegressor to predict.
I want to get the accuracy of model. because of the problem of over-fitting I decided to use the cross_val_score function to get rid of that.
from sklearn.ensemble import RandomForestRegressor
forest_reg = make_pipeline(preprocessing,
RandomForestRegressor(random_state=1))
acc = cross_val_score(forest_reg, data, labels,cv=10)

then, I used this to get the accuracy:
print(acc.mean(),acc.std())

It gives me around 0.84 and 0.06.
I understand the standard deviation part but how is the first one calculated? Is 0.84 good? Is there a better scoring way to get accuracy?
","Firt, keep in mind that accuracy is typically used for classification tasks. Not for regression.
The documentation says:

scores:  ndarray of float of shape=(len(list(cv)),)
Array of scores of the estimator for each run of the cross validation.

You have set the cv parameter to 10. It means that acc is an array of 10 scores.
But here you don't have the accuracy of each run. Instead you have the coefficient of determination of the random forest prediction:
Again the cross_val_score documentation says:

scoring: str or callable, default=None
A str (see model evaluation
documentation) or a scorer callable object / function with signature
scorer(estimator, X, y) which should return only a single value.
Similar to cross_validate but only a single metric is permitted.
If None, the estimator’s default scorer (if available) is used.

And the default scorer of RandomForestRegressor is R²:

score(X, y, sample_weight=None)
Return the coefficient of determination of the prediction.
The coefficient of determination   is defined as    , where   is the
residual sum of squares ((y_true - y_pred)** 2).sum() and   is the
total sum of squares ((y_true - y_true.mean()) ** 2).sum(). The best
possible score is 1.0 and it can be negative (because the model can be
arbitrarily worse). A constant model that always predicts the expected
value of y, disregarding the input features, would get a   score of
0.0.

Based on your question, you have an R² value with a mean of 0.86 and a standard deviation of 0.06 over 10 runs. I cannot determine if this is good or not; only you can decide if it is acceptable.
",scikit-learn
using groupshufflesplit with gridsearchcv and cross_val_score for nested crossvalidation,"I am trying to perform nested cross-validation while also incorporating group-based splitting using the GroupShuffleSplit class. However, I'm encountering a ""TypeError: cannot pickle 'generator' object"" when trying to use a custom cross-validation object with GridSearchCV. As fas as i know this Error occurs because group_split.split(...) returns an generator which cant be used in the cross_val_score function. Therefore i want to ask if there is a way to easily use GroupShuffleSplit for nested cross-validation.
Regarding my simplified sample code:
I have a dataset with features X, labels y, and group labels groups. The goal is to perform nested cross-validation, where both the inner and outer loops split the data based on the group labels. I would like to use GridSearchCV for hyperparameter tuning and cross_val_score for evaluating the performance.
import numpy as np
from sklearn.model_selection import GroupShuffleSplit, GridSearchCV, cross_val_score
from sklearn.ensemble import RandomForestClassifier

X = np.random.rand(100, 10)
y = np.random.randint(2, size=100)
groups = np.random.randint(4, size=100)  # Example group labels

rf_classifier = RandomForestClassifier()
param_grid = {'n_estimators': [50, 100, 200]}

inner_cv = GroupShuffleSplit(n_splits=5, test_size=0.2)
outer_cv = GroupShuffleSplit(n_splits=5, test_size=0.2)

grid_search = GridSearchCV(estimator=rf_classifier, param_grid=param_grid, cv=inner_cv.split(X, y, groups=groups))
nested_scores = cross_val_score(estimator=grid_search, X=X, y=y, cv=outer_cv.split(X, y, groups=groups))

Resulting in the following Stacktrace Error:
---------------------------------------------------------------------------
Empty                                     Traceback (most recent call last)
File c:\Anaconda3_x64\lib\site-packages\joblib\parallel.py:825, in Parallel.dispatch_one_batch(self, iterator)
    824 try:
--> 825     tasks = self._ready_batches.get(block=False)
    826 except queue.Empty:
    827     # slice the iterator n_jobs * batchsize items at a time. If the
    828     # slice returns less than that, then the current batchsize puts
   (...)
    831     # accordingly to distribute evenly the last items between all
    832     # workers.

File c:\Anaconda3_x64\lib\queue.py:168, in Queue.get(self, block, timeout)
    167     if not self._qsize():
--> 168         raise Empty
    169 elif timeout is None:

Empty: 

During handling of the above exception, another exception occurred:

TypeError                                 Traceback (most recent call last)
Cell In[29], line 16
     13 outer_cv = GroupShuffleSplit(n_splits=5, test_size=0.2)
     15 grid_search = GridSearchCV(estimator=rf_classifier, param_grid=param_grid, cv=inner_cv.split(X, y, groups=groups))
---> 16 nested_scores = cross_val_score(estimator=grid_search, X=X, y=y, cv=outer_cv.split(X, y, groups=groups))
     18 print(nested_scores)

File c:\Anaconda3_x64\lib\site-packages\sklearn\model_selection\_validation.py:515, in cross_val_score(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, error_score)
    512 # To ensure multimetric format is not supported
    513 scorer = check_scoring(estimator, scoring=scoring)
--> 515 cv_results = cross_validate(
    516     estimator=estimator,
    517     X=X,
    518     y=y,
    519     groups=groups,
    520     scoring={""score"": scorer},
    521     cv=cv,
    522     n_jobs=n_jobs,
    523     verbose=verbose,
    524     fit_params=fit_params,
    525     pre_dispatch=pre_dispatch,
    526     error_score=error_score,
    527 )
    528 return cv_results[""test_score""]

File c:\Anaconda3_x64\lib\site-packages\sklearn\model_selection\_validation.py:266, in cross_validate(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, return_train_score, return_estimator, error_score)
    263 # We clone the estimator to make sure that all the folds are
    264 # independent, and that it is pickle-able.
    265 parallel = Parallel(n_jobs=n_jobs, verbose=verbose, pre_dispatch=pre_dispatch)
--> 266 results = parallel(
    267     delayed(_fit_and_score)(
    268         clone(estimator),
    269         X,
    270         y,
    271         scorers,
    272         train,
    273         test,
    274         verbose,
    275         None,
    276         fit_params,
    277         return_train_score=return_train_score,
    278         return_times=True,
    279         return_estimator=return_estimator,
    280         error_score=error_score,
    281     )
    282     for train, test in cv.split(X, y, groups)
    283 )
    285 _warn_or_raise_about_fit_failures(results, error_score)
    287 # For callabe scoring, the return type is only know after calling. If the
    288 # return type is a dictionary, the error scores can now be inserted with
    289 # the correct key.

File c:\Anaconda3_x64\lib\site-packages\sklearn\utils\parallel.py:63, in Parallel.__call__(self, iterable)
     58 config = get_config()
     59 iterable_with_config = (
     60     (_with_config(delayed_func, config), args, kwargs)
     61     for delayed_func, args, kwargs in iterable
     62 )
---> 63 return super().__call__(iterable_with_config)

File c:\Anaconda3_x64\lib\site-packages\joblib\parallel.py:1048, in Parallel.__call__(self, iterable)
   1039 try:
   1040     # Only set self._iterating to True if at least a batch
   1041     # was dispatched. In particular this covers the edge
   (...)
   1045     # was very quick and its callback already dispatched all the
   1046     # remaining jobs.
   1047     self._iterating = False
-> 1048     if self.dispatch_one_batch(iterator):
   1049         self._iterating = self._original_iterator is not None
   1051     while self.dispatch_one_batch(iterator):

File c:\Anaconda3_x64\lib\site-packages\joblib\parallel.py:836, in Parallel.dispatch_one_batch(self, iterator)
    833 n_jobs = self._cached_effective_n_jobs
    834 big_batch_size = batch_size * n_jobs
--> 836 islice = list(itertools.islice(iterator, big_batch_size))
    837 if len(islice) == 0:
    838     return False

File c:\Anaconda3_x64\lib\site-packages\sklearn\utils\parallel.py:59, in <genexpr>(.0)
     54 # Capture the thread-local scikit-learn configuration at the time
     55 # Parallel.__call__ is issued since the tasks can be dispatched
     56 # in a different thread depending on the backend and on the value of
     57 # pre_dispatch and n_jobs.
     58 config = get_config()
---> 59 iterable_with_config = (
     60     (_with_config(delayed_func, config), args, kwargs)
     61     for delayed_func, args, kwargs in iterable
     62 )
     63 return super().__call__(iterable_with_config)

File c:\Anaconda3_x64\lib\site-packages\sklearn\model_selection\_validation.py:268, in <genexpr>(.0)
    263 # We clone the estimator to make sure that all the folds are
    264 # independent, and that it is pickle-able.
    265 parallel = Parallel(n_jobs=n_jobs, verbose=verbose, pre_dispatch=pre_dispatch)
    266 results = parallel(
    267     delayed(_fit_and_score)(
--> 268         clone(estimator),
    269         X,
    270         y,
    271         scorers,
    272         train,
    273         test,
    274         verbose,
    275         None,
    276         fit_params,
    277         return_train_score=return_train_score,
    278         return_times=True,
    279         return_estimator=return_estimator,
    280         error_score=error_score,
    281     )
    282     for train, test in cv.split(X, y, groups)
    283 )
    285 _warn_or_raise_about_fit_failures(results, error_score)
    287 # For callabe scoring, the return type is only know after calling. If the
    288 # return type is a dictionary, the error scores can now be inserted with
    289 # the correct key.

File c:\Anaconda3_x64\lib\site-packages\sklearn\base.py:89, in clone(estimator, safe)
     87 new_object_params = estimator.get_params(deep=False)
     88 for name, param in new_object_params.items():
---> 89     new_object_params[name] = clone(param, safe=False)
     90 new_object = klass(**new_object_params)
     91 params_set = new_object.get_params(deep=False)

File c:\Anaconda3_x64\lib\site-packages\sklearn\base.py:70, in clone(estimator, safe)
     68 elif not hasattr(estimator, ""get_params"") or isinstance(estimator, type):
     69     if not safe:
---> 70         return copy.deepcopy(estimator)
     71     else:
     72         if isinstance(estimator, type):

File c:\Anaconda3_x64\lib\copy.py:161, in deepcopy(x, memo, _nil)
    159 reductor = getattr(x, ""__reduce_ex__"", None)
    160 if reductor is not None:
--> 161     rv = reductor(4)
    162 else:
    163     reductor = getattr(x, ""__reduce__"", None)

TypeError: cannot pickle 'generator' object

","I'm not sure that this is possible before version 1.3 without writing a manual loop to replace cross_val_score.  Besides the generator issue, you're trying to tell the grid search object that it should split all of X, but it won't see all of X (it having already been split by the outer splitter).
In 1.3, we get metadata routing which automatically routes groups to group splitters.  Then we can do e.g.
from sklearn import set_config
set_config(enable_metadata_routing=True)

grid_search = GridSearchCV(estimator=rf_classifier, param_grid=param_grid, cv=inner_cv)
nested_scores = cross_val_score(estimator=grid_search, X=X, y=y, cv=outer_cv, params={'groups': groups})

Just to check that this really routes to both splitters, here's a modified version of your script:
import numpy as np
import pandas as pd
from sklearn.model_selection import GroupShuffleSplit, GridSearchCV, cross_val_score
from sklearn.base import BaseEstimator, ClassifierMixin
from sklearn import set_config

set_config(enable_metadata_routing=True)

X = np.random.rand(100, 10)
y = np.random.randint(2, size=100)
groups = np.random.randint(4, size=100)  # Example group labels

X = pd.DataFrame(X)


class MyClassifier(ClassifierMixin, BaseEstimator):
    def __init__(self, n_estimators=1):
        self.n_estimators = n_estimators

    def fit(self, X, y):
        print(""train: "", groups[X.index])
        return self
    
    def predict(self, X):
        print(""test: "", groups[X.index])
        return np.random.randint(2, size=len(X))


rf_classifier = MyClassifier()
param_grid = {'n_estimators': [50, 100]}

inner_cv = GroupShuffleSplit(n_splits=2, test_size=0.33)
outer_cv = GroupShuffleSplit(n_splits=2, test_size=0.25)

grid_search = GridSearchCV(estimator=rf_classifier, param_grid=param_grid, cv=inner_cv, verbose=10)
nested_scores = cross_val_score(estimator=grid_search, X=X, y=y, cv=outer_cv, params={'groups': groups}, verbose=10)

print(nested_score)

The outer splits put a single group in the test set, then the inner splits pick one of the remaining three as test and the last two in train.  Here's my output:
[CV] START .....................................................................
Fitting 2 folds for each of 2 candidates, totalling 4 fits
[CV 1/2; 1/2] START n_estimators=50.............................................
train:  [1 3 3 3 1 3 3 1 1 1 1 1 3 1 1 1 3 3 1 3 3 3 3 1 1 1 3 3 3 3 3 3 3 3 3 1 3
 3 3 3 1 3 1 1 1 3 3 1 1 3 1 1 1 1 1 1]
test:  [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]
[CV 1/2; 1/2] END ..............n_estimators=50;, score=0.353 total time=   0.0s
[CV 2/2; 1/2] START n_estimators=50.............................................
train:  [3 3 3 3 3 0 0 3 3 3 3 3 3 0 3 0 0 0 3 3 0 0 0 3 3 0 0 3 3 3 3 3 3 3 3 3 0
 0 0 0 3 3 3 0 0 3]
test:  [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]
[CV 2/2; 1/2] END ..............n_estimators=50;, score=0.407 total time=   0.0s
[CV 1/2; 2/2] START n_estimators=100............................................
train:  [1 3 3 3 1 3 3 1 1 1 1 1 3 1 1 1 3 3 1 3 3 3 3 1 1 1 3 3 3 3 3 3 3 3 3 1 3
 3 3 3 1 3 1 1 1 3 3 1 1 3 1 1 1 1 1 1]
test:  [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]
[CV 1/2; 2/2] END .............n_estimators=100;, score=0.412 total time=   0.0s
[CV 2/2; 2/2] START n_estimators=100............................................
train:  [3 3 3 3 3 0 0 3 3 3 3 3 3 0 3 0 0 0 3 3 0 0 0 3 3 0 0 3 3 3 3 3 3 3 3 3 0
 0 0 0 3 3 3 0 0 3]
test:  [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]
[CV 2/2; 2/2] END .............n_estimators=100;, score=0.333 total time=   0.0s
train:  [1 3 3 3 1 3 3 1 1 1 0 1 1 0 3 1 1 1 3 3 1 3 3 3 0 3 1 1 0 1 0 0 3 3 0 0 0
 3 3 0 0 3 3 3 3 3 1 3 3 3 3 0 0 1 0 0 3 1 1 1 3 3 1 1 0 0 3 1 1 1 1 1 1]
test:  [2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2]
[CV] END ................................ score: (test=0.481) total time=   0.0s
[CV] START .....................................................................
Fitting 2 folds for each of 2 candidates, totalling 4 fits
[CV 1/2; 1/2] START n_estimators=50.............................................
train:  [3 3 3 3 2 3 2 2 2 3 2 3 2 3 2 2 2 3 3 2 3 3 2 2 2 3 3 2 2 3 3 2 2 2 3 3 3
 3 3 3 3 2 3 3 2 2 3 2 2 2 2 3 3 2 3 2]
test:  [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]
[CV 1/2; 1/2] END ..............n_estimators=50;, score=0.588 total time=   0.0s
[CV 2/2; 1/2] START n_estimators=50.............................................
train:  [3 3 3 3 2 3 2 2 2 3 2 3 2 3 2 2 2 3 3 2 3 3 2 2 2 3 3 2 2 3 3 2 2 2 3 3 3
 3 3 3 3 2 3 3 2 2 3 2 2 2 2 3 3 2 3 2]
test:  [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]
[CV 2/2; 1/2] END ..............n_estimators=50;, score=0.588 total time=   0.0s
[CV 1/2; 2/2] START n_estimators=100............................................
train:  [3 3 3 3 2 3 2 2 2 3 2 3 2 3 2 2 2 3 3 2 3 3 2 2 2 3 3 2 2 3 3 2 2 2 3 3 3
 3 3 3 3 2 3 3 2 2 3 2 2 2 2 3 3 2 3 2]
test:  [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]
[CV 1/2; 2/2] END .............n_estimators=100;, score=0.647 total time=   0.0s
[CV 2/2; 2/2] START n_estimators=100............................................
train:  [3 3 3 3 2 3 2 2 2 3 2 3 2 3 2 2 2 3 3 2 3 3 2 2 2 3 3 2 2 3 3 2 2 2 3 3 3
 3 3 3 3 2 3 3 2 2 3 2 2 2 2 3 3 2 3 2]
test:  [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]
[CV 2/2; 2/2] END .............n_estimators=100;, score=0.471 total time=   0.0s
train:  [3 3 3 3 2 3 2 2 0 2 0 3 2 3 2 3 2 2 2 3 3 2 3 0 3 2 2 2 0 0 0 3 3 2 0 0 2
 0 3 3 0 0 2 2 2 3 3 3 3 3 3 3 2 3 3 2 2 0 0 0 0 3 2 2 2 2 3 3 2 0 0 3 2]
test:  [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]
[CV] END ................................ score: (test=0.593) total time=   0.0s
[0.48148148 0.59259259]
[Parallel(n_jobs=1)]: Done   1 tasks      | elapsed:    0.0s

",cross-validation
cross validation with coco data format json files,"I am a newbie ML learner and trying semantic image segmentation on google colab with COCO data format json and lots of images on google drive.
update
I borrowed this code as a starting point. So my code on colab is pretty much like this.
https://github.com/akTwelve/tutorials/blob/master/mask_rcnn/MaskRCNN_TrainAndInference.ipynb
/update
I am splitting an exported json file into 2 jsons (train/validate with 80/20 ratio) every time I receive new annotation data. But this is getting tiring since I have more than 1000 annotations in a file and I do it manually with replace function of VS code.
Is there a better way to do this programatically on google colab?
So what I like to do is rotating annotation data without spitting a json file manually.
Say, I have 1000 annotations in ONE json file on my google drive, I would like to use the 1-800 annotations for training and the 801-1000 annotations for validating for the 1st train session, then for the next train session I would like to use the 210-1000 annotations for training and 1-200 annotations for validating. Like selecting a part of data in json from code on colab.
Or if I can rotate the data during one train session (K-Fold Cross Validation?), that is even better but I have no clue to do this.
Here is parts of my code on the colab.
Loading json files
dataset_train = CocoLikeDataset()
dataset_train.load_data('PATH_TO_TRAIN_JSON', 'PATH_TO_IMAGES')
dataset_train.prepare()

dataset_val = CocoLikeDataset()
dataset_val.load_data('PATH_TO_VALIDATE_JSON', 'PATH_TO_IMAGES')
dataset_val.prepare()

Initializing model
model = modellib.MaskRCNN(mode=""training"", config=config, model_dir=MODEL_DIR)

init_with = ""coco""

if init_with == ""imagenet"":
    model.load_weights(model.get_imagenet_weights(), by_name=True)
elif init_with == ""coco"":
    model.load_weights(COCO_MODEL_PATH, by_name=True,
                       exclude=[""mrcnn_class_logits"", ""mrcnn_bbox_fc"", 
                                ""mrcnn_bbox"", ""mrcnn_mask""])
elif init_with == ""last"":
    model.load_weights(model.find_last(), by_name=True)

train
start_train = time.time()
model.train(dataset_train, dataset_val, 
            learning_rate=config.LEARNING_RATE, 
            epochs=30, 
            layers='heads')
end_train = time.time()
minutes = round((end_train - start_train) / 60, 2)
print(f'Training took {minutes} minutes')

validate 
start_train = time.time()
model.train(dataset_train, dataset_val, 
            learning_rate=config.LEARNING_RATE / 10,
            epochs=10, 
            layers=""all"")
end_train = time.time()
minutes = round((end_train - start_train) / 60, 2)
print(f'Training took {minutes} minutes')

json
{
  ""info"": {
    ""year"": 2020,
    ""version"": ""1"",
    ""description"": ""Exported using VGG Image Annotator (http://www.robots.ox.ac.uk/~vgg/software/via/)"",
    ""contributor"": """",
    ""url"": ""http://www.robots.ox.ac.uk/~vgg/software/via/"",
    ""date_created"": ""Tue Jan 21 2020 16:18:14""
  },
  ""images"": [
    {
      ""id"": 0,
      ""width"": 2880,
      ""height"": 2160,
      ""file_name"": ""sample01.jpg"",
      ""license"": 1,
      ""flickr_url"": ""sample01.jpg"",
      ""coco_url"": ""sample01.jpg"",
      ""date_captured"": """"
    }
  ],
   ""annotations"": [
    {
      ""id"": 0,
      ""image_id"": ""0"",
      ""category_id"": 1,
      ""segmentation"": [
        588,
        783,
        595,
        844,
        607,
        687,
        620,
        703,
        595,
        722,
        582,
        761
      ],
      ""area"": 108199,
      ""bbox"": [
        582,
        687,
        287,
        377
      ],
      ""iscrowd"": 0
    }
  ],
  ""licenses"": [
    {
      ""id"": 1,
      ""name"": ""Unknown"",
      ""url"": """"
    }
  ],
  ""categories"": [
    {
      ""id"": 1,
      ""name"": ""nail"",
      ""supercategory"": ""type""
    }
  ]
}

FYI, My workflow is like

Label images with VIA annotation tool
Export annotations in coco format json
Modify the json and save to my google drive
Load the json on colab and start training

","There's a very good utility function in the sklearn library for doing exactly what you want here. It's called train_test_split.
Now, it's hard to understand what your data structures are, but I am assuming that this code:
dataset_train = CocoLikeDataset()
dataset_train.load_data('PATH_TO_TRAIN_JSON', 'PATH_TO_IMAGES')
dataset_train.prepare()

populates dataset_train with some kind of array of images, or else an array of the paths to the images. sklearn's train_test_split function is able to accept pandas DataFrames as well as numpy arrays.
I am usually very comfortable with pandas DataFrames, so I would suggest you combine the training and validation data into one DataFrame using the pandas function concat, then create a random split using the sklearn function train_test_split at the beginning of every training epoch. It would look something like the following:
import pandas as pd
from sklearn.model_selection import train_test_split

# Convert the data into a DataFrame
master_df = pd.concat([pd.DataFrame(dataset_train), pd.DataFrame(dataset_val)], ignore_index=True)

# Separate out the data and targets DataFrames' (required by train_test_split)
data_df = master_df[['image_data_col_1','image_data_col_2','image_data_col_3']]
targets_df = master_df[['class_label']]

# Split the data into a random train/test (or train/val) split
data_train, data_val, targets_train, targets_val = train_test_split(data_df, targets_df, test_size=0.2)

# Training loop
# If the training function requires the targets to be present in the same DataFrame, you can do this before beginning training:
dataset_train_df = pd.concat([data_train, targets_train], axis=1)
dataset_val_df = pd.concat([data_val, targets_val], axis=1)
##################################
# Continue with training loop...
##################################


Just one last note: ideally, you should have three sets - train, test, and validation. So separate out a testing set beforehand, and then do the train_test_split at the beginning of every iteration of the training loop to obtain your train-validation split from the remaining data.
",cross-validation
python osenviron osputenv usrbinenv,"I want to ensure os.system('env') not contain some specific variable myname
which is export in ~/.bashrc as export myname=csj
Therefore, I wrote below python code:
import os

def print_all():
    print ""os.environ['myname']=%s"" % os.environ.get('myname')
    print ""os.getenv('myname')=%s"" % os.getenv('myname')
    os.system('env | grep myname')
    print

def delete_myname():
    if 'myname' in os.environ: os.environ.pop('myname')
    if os.getenv('myname'): os.unsetenv('myname')

print_all()

os.putenv('myname', 'csj2')
print ""---------------------""
delete_myname()
print_all()

os.putenv('myname', 'csj3')
print ""---------------------""
delete_myname()
print_all()

I think examine both os.environ['myname'] and os.getenv('myname') 
and then delete them if exist, 
can ensure os.system('env | grep myname') get nothing.
However, the result is:
os.environ['myname']=csj
os.getenv('myname')=csj
myname=csj

---------------------
os.environ['myname']=None
os.getenv('myname')=None

---------------------
os.environ['myname']=None
os.getenv('myname')=None
myname=csj3

I don't understand why I still got csj3 on os.system('env | grep myname')?
","From the docs:

Note: Calling putenv() directly does not change os.environ, so it’s better to modify os.environ.

For unsetenv there is a similar warning:

however, calls to unsetenv() don’t update os.environ, so it is actually preferable to delete items of os.environ.

getenv just returns the value from os.environ as its implementation shows, so by using it you get into a state where it seems the value isn't set when you look it up from python, while it actually is in the real environment. The only way to get it now I can think of would be to call the C getenv function using C types...
If i modify your code to use os.environ instead of calling putenv/unsetenv everything works as expected:
import os

def print_all():
    print ""os.environ['myname']=%s"" % (os.environ['myname'] if 'myname' in os.environ else ""None"")
    os.system('env | grep myname')
    print

def delete_myname():
    if 'myname' in os.environ: os.environ.pop('myname')

print_all()

os.environ['myname'] = 'csj2'
print ""---------------------""
print_all()
delete_myname()
print_all()

os.environ['myname'] = 'csj3'
print ""---------------------""
print_all()
delete_myname()
print_all()

output:
$ myname=somevalue python2 test.py 
os.environ['myname']=somevalue
myname=somevalue

---------------------
os.environ['myname']=csj2
myname=csj2

os.environ['myname']=None

---------------------
os.environ['myname']=csj3
myname=csj3

os.environ['myname']=None

",python
remove backslash continuation character,"I'm trying to parse some code with AST, but I'm having an issue because of backslash continuation character.
When I have a continuation character \, textwrap will not manage to dedent the code, I would like to know how to get rid of it.
code = """"""
    def foo():
        message = ""This is a very long message that will probably need to wrap at the end of the line!\n \
And it actually did!""
""""""

import textwrap
print textwrap.dedent(code)

import ast
ast.parse(textwrap.dedent(code))

I'm adding more details to clarify the question:
I have a module nemo.py with the following content:
class Foo(object):

    def bar(self):
        message = ""This is a very long message that will probably need to wrap at the end of the line!\n \
And it actually did!""

and the main module trying to parse the code:
import ast
import nemo
import inspect
import textwrap

code = str().join(inspect.getsourcelines(nemo.Foo.bar)[0])
ast.parse(textwrap.dedent(code))

And the traceback:
Traceback (most recent call last):
  File ""/Users/kelsolaar/Documents/Development/Research/_BI.py"", line 7, in <module>
    ast.parse(textwrap.dedent(code))
  File ""/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/ast.py"", line 37, in parse
    return compile(source, filename, mode, PyCF_ONLY_AST)
  File ""<unknown>"", line 1
    def bar(self):
    ^
IndentationError: unexpected indent

","This is because you misunderstood what textwrap.dedent() does. 
It only remove any common leading white spaces. In your case there's no common leading white space, therefore nothing is removed. 
Moreover, what you want is actually \\ instead of \n \ in this case. This is because you actually want what is printed to be parsed. \\ will print only one \ and it's what you want. \n \ will print a new line within ""..."" clause which is invalid.
Now consider this code:
>>> code = """"""
    def foo():
        message = ""This is a very long message that will probably need to wrap at the end of the line! \\
    And it actually did!""
""""""

>>> print textwrap.dedent(code)

def foo():
    message = ""This is a very long message that will probably need to wrap at the e
nd of the line! \
And it actually did!""

>>> ast.parse(textwrap.dedent(code))
<_ast.Module object at 0x10e9e5bd0>

In this case there is common leading white spaces, and hence they are removed.

Edit:
If you want to get rid of the \ all together, you can consider using """"""My sentence"""""" for message in def bar. 
",python
openai api throws bad request when uploading a file,"I am using the Python version of the OpenAI API and trying to upload a file with the following code:
client.files.create(file=myFile, purpose=""fine-tune"")
The client has been initialized with a valid API key, and other calls to the API (to send text prompts) work correctly, but when I try to post a file I get a ""HTTP/1.1 400 Bad Request"" error.
I have verified that the file exists and is a readable text file (the code logs the first line of the file before posting it).
The code used to initialize myFile is:
myFile = open(file_path, ""rb"");
What could the problem be?
","As stated by @codenanda, I was using the wrong file format. The current version of the OpenAI API only supports JSON files.
",chatgpt
why doesn39t permuting positional encodings in gpt2 affect the output as expected,"I'm trying to understand the role of positional encoding in the GPT-2 Transformer model. From what I understand, positional encodings are crucial because they give the model a sense of the order of tokens.
However, I'm confused about the behavior I'm observing:

Permuting Positional Encodings: When I permute the positional encodings while keeping the input tokens the same, the generated output barely changes. I expected significant changes since the positional information should alter the model’s understanding of token order.

Permuting Input Tokens: When I permute the input tokens (while permuting positional encodings in the same manner), the output changes significantly, but it doesn't revert to what it was with the original order.


This behavior is confusing because I expected the output to revert when both the positional encodings and tokens are permuted in the same way.
Could someone help clarify why this is happening? Is there something about how GPT-2 handles positional encoding that I'm missing? How can I modify my code to get the behavior I expect, where permuting both the positional encoding and input tokens in the same way results in the original output?
Thanks in advance!
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer

def permute_columns(matrix, permutation=None):
    n = len(permutation)
    first_n_columns = matrix[:, :n]
    permuted_columns = first_n_columns[:, permutation]
    remaining_columns = matrix[:, n:]
    new_matrix = torch.hstack((permuted_columns, remaining_columns))
    return new_matrix

model_name = ""gpt2""
model = AutoModelForCausalLM.from_pretrained(model_name)
tokenizer = AutoTokenizer.from_pretrained(model_name)

permutation = [0, 4, 2, 3, 1]
# permute positional encoding
model.transformer.wpe.weight.data = permute_columns(model.transformer.wpe.weight.data.T, permutation).T

input_text = ""The man ate the cow""
input_ids = tokenizer(input_text, return_tensors=""pt"").input_ids

# permute input
input_ids = permute_columns(input_ids, permutation)

outputs = model.generate(input_ids, max_length=50)
generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)

I attempted to permute the positional encodings in a GPT-2 model, expecting this to change the generated output. Additionally, I tried permuting the input tokens along with the positional encodings in the same manner, anticipating that the output would revert to the original.
What I Expected:

When permuting only the positional encodings, I expected the output to change significantly because the model should interpret the token order differently.
When permuting both the input tokens and positional encodings in the same way, I expected the output to revert to what it was with the original order.

What Actually Happened:

Permuting only the positional encodings resulted in minimal changes to the generated output, which was unexpected.

Permuting both the input tokens and positional encodings in the same manner led to a different output, but it did not revert to the original as I anticipated.


","You have to consider the effect of the causal attention mask.
CLMs like GPT-2 use an attention mask to prevent tokens from attending to tokens that come later in the sequence. This is important because if this wasn't the case, the model could ""look ahead"" and cheat at the next token prediction task. The attention mask restricts the model such that token i can only attend to tokens j <= i.
Say we have tokens [0, 1, 2, 3, 4]. Token 0 attends to itself. Token 1 attends to [0, 1] and so on.
Now consider your permutations.
If we permute the positional embeddings, we give the model slightly different signal, but the token order and attention order is still the same. Token 0 still attends to token 0. Token 1 still attends to [0, 1], and so on. As a result, the output is mostly similar to the base case.
Now we permute the token order, say to [3, 2, 0, 4, 1]. Token 3, which used to attend to [0, 1, 2, 3], can now only attend to itself. Token 2, which used to attend to [0, 1, 2] can now only attend to [3, 2]. Token permutation substantially changes what information is routed to what tokens, resulting in a substantial difference in model output.
If you want to look at the effect of token order and positional embeddings in isolation, you should use a BERT-style masked language model that does not use a causal attention mask.
",huggingface-transformers
how to replace values in numpy array at the same time,"I have numpy array with random numbers. For example like this
[7 1 2 0 2 3 4 0 5]
and I want to replace every number at the same time if number from this array = 7, I want to replace it with 2 , also if number = 2, replace it with 3. So it will be like [2 1 3 0 3 3 4 0 5] . I have tried it with np.where but can't change any of them.
","It's better to use np.select if you've multiple conditions:
a = np.array([7, 1, 2, 0, 2, 3, 4, 0, 5])
a = np.select([a == 7, a == 2], [2, 3], a)

OUTPUT:
[2 1 3 0 3 3 4 0 5]

",numpy
how to create major and minor gridlines with different linestyles,"I am currently using matplotlib.pyplot to create graphs and would like to have the major gridlines solid and black and the minor ones either greyed or dashed.
In the grid properties, which=both/major/mine, and then color and linestyle are defined simply by linestyle. Is there a way to specify minor linestyle only?
The appropriate code I have so far is
plt.plot(current, counts, 'rd', markersize=8)
plt.yscale('log')
plt.grid(b=True, which='both', color='0.65', linestyle='-')

","Actually, it is as simple as setting major and minor separately:
In [9]: plot([23, 456, 676, 89, 906, 34, 2345])
Out[9]: [<matplotlib.lines.Line2D at 0x6112f90>]

In [10]: yscale('log')

In [11]: grid(visible=True, which='major', color='b', linestyle='-')

In [12]: grid(visible=True, which='minor', color='r', linestyle='--')

The gotcha with minor grids is that you have to have minor tick marks turned on too.  In the above code this is done by yscale('log'), but it can also be done with plt.minorticks_on().

Note: before matplotlib 3.5, visible parameter was named b
",matplotlib
how can i split a dataset from a csv file for training and testing,"I'm using Python and I need to split my .csv imported data in two parts, a training and test set, E.G 70% training and 30% test. 
I keep getting various errors, such as 'list' object is not callable and so on. 
Is there any easy way of doing this?
Thanks
EDIT:
The code is basic, I'm just looking to split the dataset.
from csv import reader
with open('C:/Dataset.csv', 'r') as f:
    data = list(reader(f)) #Imports the CSV
    data[0:1] ( data )

TypeError: 'list' object is not callable
","You can use pandas:
import pandas as pd
import numpy as np

df = pd.read_csv('C:/Dataset.csv')
df['split'] = np.random.randn(df.shape[0], 1)

msk = np.random.rand(len(df)) <= 0.7

train = df[msk]
test = df[~msk]

",data-science
is there a difference between scipypi numpypi or mathpi,"In a project using SciPy and NumPy, when should one use scipy.pi vs numpy.pi vs just math.pi? Is there a difference between these values?
",">>> import math
>>> import numpy as np
>>> import scipy
>>> math.pi == np.pi == scipy.pi
True

So it doesn't matter, they are all the same value.
The only reason all three modules provide a pi value is so if you are using just one of the three modules, you can conveniently have access to pi without having to import another module. They're not providing different values for pi.
",numpy
i have a set of words i would like to exclude and i want to check if each new word entered is contained wholly within another how would i do this,"So I am currently making a word game, and I want to have it so that the user cannot enter a word that is wholly contained within another word (e.g. 'set' in 'subset'), however my current code:
noWord = False
for _word in used_words:
    if set(word).issubset(set(_word)):
        noWord = True
if noWord == False:
    # Calculate score for the word

makes it so that it doesn't matter what order the letters are in (e.g. playing 'pitch' would prevent you from playing 'chip') which is a feature I do not want. How would I go about doing this efficiently.
I tried searching the _word variable as a list and checking if each set of letters from that list matches the word variable, but that was incredibly inefficient.
e.g.
wordAsAList == list(_word)
# All words have to be 3 letters or more in my game
for v in range(len(wordAsAList) - 2):
    for i in range(len(wordAsAList)):
        if len(wordAsAList) - v - 3 >= i:
            if ("""".join(wordAsAList[i:(len(wordAsAList) - v)])) == word:
                noWord = True

Beyond that I have little to no clue how to solve the problem / make the code above more efficient. (Also I know I shouldn't use range(len(list)), but I still do anyway)
Thanks in advance :)
","That is what set does. set create a set (that is a collection without any order, whose purpose is to verify what belongs to it, what is included in it. Well, sets. Like in math).
To build a set, you call set with another collection of things. Like S=set([3, 1, 2]) is the set containing numbers 1, 2 and 3. Then set([1,3]).issubset(S) is True, since set {1,3} is included in set {1,2,3}
And when you call set with something that is not a collection, you get an error. set(1) raises an error.
Now, here, you called set with a string. That you consider to be a ""atomic"" value. So, you may think that it should have raised an error. But a string is a collection: a collection of chars. So S=set(""hello"") is really the set containing {'h', 'e', 'l', 'o'} (in any order). And T=set(""lol"") is also a set, {'o','l'} (I show them in random orders, since, like in maths, there is no order).
So T.issubset(S) is True here. Because all elements of T are also elements of S.
Now, that was ""why"". As for ""how"" it is very simple:
""set"" in ""subset""

is True.
And this time, in the sense you expect it to be: because string ""set"" is included in string ""subset"".
To check if a word is part of any word of a list, you can
L=[""subset"", ""hello"", ""world""] # example list of words
any(""set"" in x for x in L) # True, because ""set"" is substring of one of the words of L
any(""lol"" in x for x in L) # False, because ""lol"" is substring of no word x of L

",python
interpolation over an irregular grid,"So, I have three numpy arrays which store latitude, longitude, and some property value on a grid -- that is, I have LAT(y,x), LON(y,x), and, say temperature T(y,x), for some limits of x and y.  The grid isn't necessarily regular -- in fact, it's tripolar.
I then want to interpolate these property (temperature) values onto a bunch of different lat/lon points (stored as lat1(t), lon1(t), for about 10,000 t...) which do not fall on the actual grid points.  I've tried matplotlib.mlab.griddata, but that takes far too long (it's not really designed for what I'm doing, after all).  I've also tried scipy.interpolate.interp2d, but I get a MemoryError (my grids are about 400x400).
Is there any sort of slick, preferably fast way of doing this?  I can't help but think the answer is something obvious...  Thanks!!
","Try the combination of inverse-distance weighting and
scipy.spatial.KDTree
described in SO
inverse-distance-weighted-idw-interpolation-with-python.
Kd-trees
work nicely in 2d 3d ..., inverse-distance weighting is smooth and local,
and the k= number of nearest neighbours can be varied to tradeoff speed / accuracy.
",numpy
adjust threshold cros_val_score sklearn,"There is a way to set the threshold cross_val_score sklearn?
I've trained a model, then I adjust the threshold to 0.22. The model in the following below :
# Try with Threshold
pred_proba = LGBM_Model.predict_proba(X_test)


# Adjust threshold for predictions proba
prediction_with_threshold = []
for item in pred_proba[:,0]:
    if item > 0.22 :
        prediction_with_threshold.append(0)
    else:
        prediction_with_threshold.append(1)

print(classification_report(y_test,prediction_with_threshold))

then I want to validate this model using cross_val_score. I've searched but can't find the method to set threshold for cross_val_score. The cross_val_score that I've used like the following below :
F1Scores = cross_val_score(LGBMClassifier(random_state=101,learning_rate=0.01,max_depth=-1,min_data_in_leaf=60,num_iterations=200,num_leaves=70),X,y,cv=5,scoring='f1')
F1Scores

### how to adjust threshold to 0.22 ??

Or there is other method to validate this model using threshold?
","Assuming that you are working with a two-class classification problem you could override the predict method of LGBMClassifier object with your thresholding approach as shown below:
import numpy as np
from lightgbm import LGBMClassifier
from sklearn.datasets import make_classification

X, y = make_classification(n_features=10, random_state=0, n_classes=2, n_samples=1000, n_informative=8)

class MyLGBClassifier(LGBMClassifier):
    def predict(self,X, threshold=0.22,raw_score=False, num_iteration=None,
                pred_leaf=False, pred_contrib=False, **kwargs):
        result = super(MyLGBClassifier, self).predict_proba(X, raw_score, num_iteration,
                                    pred_leaf, pred_contrib, **kwargs)
        predictions = [1 if p>threshold else 0 for p in result[:,0]]
        return predictions

clf = MyLGBClassifier()
clf.fit(X,y)
clf.predict(X,threshold=2)  # just testing the implementation
# [0,0,0,0,..,0,0,0]        # we get all zeros since we have set threshold as 2

F1Scores = cross_val_score(MyLGBClassifier(random_state=101,learning_rate=0.01,max_depth=-1,min_data_in_leaf=60,num_iterations=2,num_leaves=5),X,y,cv=5,scoring='f1')
F1Scores
#array([0.84263959, 0.83333333, 0.8       , 0.78787879, 0.87684729])

",cross-validation
how to reorder columns if the columns have the same part name,"I want to reorder columns name if the columns have the same part name. Sample as below:
import pandas as pd

df = pd.DataFrame({
    'Branch': ['Hanoi'],
    '20241201_Candy': [3], '20241202_Candy': [4], '20241203_Candy': [5],
    '20241201_Candle': [3], '20241202_Candle': [4], '20241203_Candle': [5],
    '20241201_Biscuit': [3], '20241202_Biscuit': [4], '20241203_Biscuit': [5]})

Below is my Expected Ouput:
df2 = pd.DataFrame({
    'Branch': ['Hanoi'],
    '20241201_Biscuit': [3], '20241201_Candle': [3], '20241201_Candy': [3],
    '20241202_Biscuit': [4], '20241202_Candle': [4], '20241202_Candy': [4],
    '20241203_Biscuit': [5], '20241203_Candle': [5], '20241203_Candy': [5]})

So I want to auto reorder dataframe if it has same date.
","You can use df.reindex, single out column 'Branch' and apply sorted to the remainder, df.columns[1:]:
out = df.reindex(['Branch'] + sorted(df.columns[1:]), axis=1)

out.equals(df2)
# True

Or directly:
out2 = df[['Branch'] + sorted(df.columns[1:])]

out2.equals(df2)
# True

",pandas
creating reusable and composable filters for pandas dataframes,"I am working with multiple Pandas DataFrames with a similar structure and would like to create reusable filters that I can define once and then apply or combine as needed.
The only working solution I came up with so far feels clunky to me and makes it hard to combine filters with OR:
import pandas as pd
df = pd.DataFrame({""A"":[1,1,2],""B"":[1,2,3]})

def filter_A(df):
    return df.loc[df[""A""]==1]

def filter_B(df):
    return df.loc[df[""B""]==2]

print(filter_A(filter_B(df)).head())

I am hoping for something along the lines of
filter_A = (df[""A""]==1)
filter_B = (df[""B""]==2)

print(df.loc[(filter_A) & (filter_B)])

but reusable after changing the df and also applicable to other DataFrames with the same columns. Is there any cleaner or more readable way to do this?
","You can use the .eval() method, which allows for the evaluation of a string describing operations on dataframe columns:

Evaluate these string expressions on the dataframe df.

Combine the results of these evaluations using the bitwise AND operator (&), which performs element-wise logical AND operation.

Use the .loc accessor to filter the dataframe based on the combined condition.


filter_A = 'A == 1'
filter_B = 'B == 2'
df.loc[df.eval(filter_A) & df.eval(filter_B)]

Output:
   A  B
1  1  2

",pandas
in python pandas data frame shift values from category to rating rating to reviews  current ver to android ver,"This is my main data:

I need to change this line of data:

This is my output:

In python pandas data frame shift values from Category to Rating, Rating to Reviews, etc. Current Ver to Android Ver. App column should be constant.
I tried a lot. But its not giving me the proper output. Please help me
","You .loc:
df.loc[df[""Price""]==""Everyone"", ""Category"":""Android Ver""] = df.loc[df[""Price""]==""Everyone"", ""Category"":""Android Ver""].shift(axis=1)

",data-science
google play developer api  400 invalid value  inapppurchases,"My question is similar to this one. However, I am using the API Java Client Library with a service account, making calls to the API from my server.
My code is following this guide, which is very simple. However, I can't seem to get an appropriate error for my request. This is how I build my AndroidPublisher:
val credential = GoogleCredential.fromStream(FileInputStream(
        ""/path/to/json""
)).createScoped(Collections.singleton(AndroidPublisherScopes.ANDROIDPUBLISHER))

androidPublisher = AndroidPublisher.Builder(httpTransport, JSON_FACTORY, credential)
        .setApplicationName(packageName)
        .build()

Where the JSON is generated from the Developer Console, under Service Accounts. This is how I make my request:
androidPublisher.purchases().subscriptions().get(packageName, ""valid-sku"", ""invalid-token"").execute()

My subscription ID is valid but my token is invalid. I expect an error such as ""invalid token"" in the response. However, what I get is:
com.google.api.client.googleapis.json.GoogleJsonResponseException: 400 Bad Request
{
  ""code"" : 400,
  ""errors"" : [ {
    ""domain"" : ""global"",
    ""message"" : ""Invalid Value"",
    ""reason"" : ""invalid""
  } ],
  ""message"" : ""Invalid Value""
}

Is that a generic error because of the invalid token or is it an authentication issue? If it an authentication issue, how do I solve it? If it is an invalid token issue, how am I supposed to know?
Some more information: 

I get the same error when trying to make that call from the API Explorer as well (this time using a Client ID and API Key instead of Service Account).
I have not delegated domain-wide access to the service account. Do I have to for some reason?
I can successfully make other calls to the API such as inappproducts.list

","So, the reason that happened was just because the purchaseToken I was using was wrong.
I did not expect that to be the reason as I thought that in the case of an invalid token, I would receive a ""token invalid"" error (or something similar). As it turns out, the responses given by Google are pretty inconsistent (a 404 could also be given for an invalid token).
",java
how to handle nan values in rental price prediction project,"I am working on a rental price prediction project where I web scraped data from Facebook Marketplace. When extracting the areas of the properties, I am encountering many NaN values.
I am web scraping from a small city and it is unlikely that I will be able to find more data. How can I effectively handle the NaN values in my data? Are there any machine learning algorithms or external sources of information that can be used to impute missing values in this situation?
Any suggestions or advice would be greatly appreciated. Thank you in advance!
I have considered using the mean or median based on property type, number of bedrooms, and bathrooms, but I am not sure if this is the best approach.
","There are many methods that you can use when it comes to missing values in your data. As you mentioned general approach is to fill with mean-median. I recommend grouping them first then filling with mean or median.
df['a'].fillna(df.groupby('b')['a'].transform('mean'))

I recon you can use zipcode or something similar to group them.
Another thing you can do is before filling empty places, create another column that indicates if the values are missing. this may help your model to treat those values differently and don't overfit on those values.
For further info link
",data-science
how to define kronecker product layer of 2 keras layers of shape none 4096 is performed,"Let's say there are 2 different/separate keras layers,
encoder_1 & encoder_2 with both having output shape of (None, 4096).
Now how to define keras multiply layer which gives (None, 4096, 4096) as it's output shape.
Is this same as Kronecker product?
If not the same please show how to implement Kronecker product of 2 layers named, encoder_1 & encoder_2?
","So you should be able to achieve this simply using either the Dot layer or dot method of Keras, after inserting dimensions of length 1:
import tensorflow as tf
from tensorflow.keras.layers import dot

encoder_1 = tf.expand_dims(encoder_1, axis=2)
encoder_2 = tf.expand_dims(encoder_2, axis=1)
outer = dot([encoder_1, encoder_2], axes=(2, 1))

outer should be a tensor of shape (None, 4096, 4096).
",multimodal
gridspec with shared axes in python,"This solution to another thread suggests using gridspec.GridSpec instead of plt.subplots. However, when I share axes between subplots, I usually use a syntax like the following
  fig, axes = plt.subplots(N, 1, sharex='col', sharey=True, figsize=(3,18))

How can I specify sharex and sharey when I use GridSpec ?
","First off, there's an easier workaround for your original problem, as long as you're okay with being slightly imprecise. Just reset the top extent of the subplots to the default after calling tight_layout:
fig, axes = plt.subplots(ncols=2, sharey=True)
plt.setp(axes, title='Test')
fig.suptitle('An overall title', size=20)

fig.tight_layout()
fig.subplots_adjust(top=0.9) 

plt.show()



However, to answer your question, you'll need to create the subplots at a slightly lower level to use gridspec.  If you want to replicate the hiding of shared axes like subplots does, you'll need to do that manually, by using the sharey argument to Figure.add_subplot and hiding the duplicated ticks with plt.setp(ax.get_yticklabels(), visible=False).
As an example:
import matplotlib.pyplot as plt
from matplotlib import gridspec

fig = plt.figure()
gs = gridspec.GridSpec(1,2)
ax1 = fig.add_subplot(gs[0])
ax2 = fig.add_subplot(gs[1], sharey=ax1)
plt.setp(ax2.get_yticklabels(), visible=False)

plt.setp([ax1, ax2], title='Test')
fig.suptitle('An overall title', size=20)
gs.tight_layout(fig, rect=[0, 0, 1, 0.97])

plt.show()


",matplotlib
langchain error  notimplementederror need to determine which default deprecation schedule to use within  minor releases,"I ran the bellow code. However, currently, it shows an error massage.
from langchain.llms import GooglePalm

api_key = 'my_API'


llm = GooglePalm(google_api_key=api_key,
                 temperature=0.1)

This is the error I got.
NotImplementedError                       Traceback (most recent call last)
<ipython-input-2-a3e32679669c> in <cell line: 7>()
      5 
      6 # Create llm variable here
----> 7 llm = GooglePalm(google_api_key=api_key,
      8                  temperature=0.1)

2 frames
/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py in warn_deprecated(since, message, name, alternative, pending, obj_type, addendum, removal)
    293         if not removal:
    294             removal = f""in {removal}"" if removal else ""within ?? minor releases""
--> 295             raise NotImplementedError(
    296                 f""Need to determine which default deprecation schedule to use. ""
    297                 f""{removal}""

NotImplementedError: Need to determine which default deprecation schedule to use. within ?? minor releases

Can someone please help me to solve this?
I need to create the large language model variable.
","I also had a similar error, I'm using langchain==0.1.4 in which its google_palm file suggests deprecation of GooglePalm and it is replaced with
langchain_google_genai.GoogleGenerativeAI
Make use of GoogleGenerativeAI instead;
for its installation follow these steps:

pip install --upgrade --quiet langchain-google-genai
pip install -q -U google-generativeai

Then use this code:
from langchain_google_genai import GoogleGenerativeAI

llm = GoogleGenerativeAI(model=""models/text-bison-001"", google_api_key=SECRET_KEY, temperature=0.1)

Use Google Palm API key.
Article Links:

Langchain GoogleGenerativeAI
GoogleGenerativeAI Quick Start guide

I hope you find this useful.
",langchain
named argument passed as a dict syntaxerror syntax error at or near quotquot or syntaxerror type modifiers must be simple constants or identifiers,"I'm facing some troubles when trying to execute a PostgreSQL CREATE TABLE query with a dictionary of parameters using psycopg:
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
""""""
Created on Fri Nov  1 09:32:32 2024

@author: me
""""""

import psycopg

pg_uri = ""postgres://postgres:******@localhost:5432/mydatabase""
conn_dict = psycopg.conninfo.conninfo_to_dict(pg_uri)
conn = psycopg.connect(**conn_dict)
curs = conn.cursor()

d = {""CRS"": 4326}
raw = ""CREATE TABLE foo (id INT, geom geometry('Point',%(CRS)s));""

query0 = psycopg.sql.SQL(raw).format(**d)
query1 = psycopg.sql.SQL(raw.format(**d))
query2 = psycopg.sql.SQL(raw)

curs.execute(query0) # SyntaxError: syntax error at or near ""%""
curs.execute(query1) # SyntaxError: syntax error at or near ""%""
curs.execute(query2,d) # SyntaxError: type modifiers must be simple constants or identifiers


The curs.execute() calls either raise:

SyntaxError: syntax error at or near ""%"" with the two firsts queries or:
SyntaxError: type modifiers must be simple constants or identifiers with the last one

Relevant pieces of documentation
https://www.psycopg.org/psycopg3/docs/basic/params.html
Version Info
Python: 3.10
psycopg: 3.2.3
PostgreSQL: 16
","As the documentation for SQL.format states, SQL.format uses the same placeholder syntax as str.format, i.e. {CRS} and not %(CRS)s.
d = {""CRS"": 4326}
raw = ""CREATE TABLE foo (id INT, geom geometry('Point',{CRS}));""

query0 = psycopg.sql.SQL(raw).format(**d)

",python
numerically integrating signals with absolute value,"Suppose I have a numpy s array of acceleration values representing some signal sampled at a fixed rate dt. I want to compute the cumulative absolute velocity, i.e. np.trapz(np.abs(s), dx=dt).
This is great except if dt is ""large"" (e.g. 0.01) and the signal s is both long and crossing between positive and negative values frequently (an unfortunately common occurrence), an error is accumulated from the fact that taking |s| drops the information about the original sign of s. See the picture for a better idea of what this error actually looks like.
I have some custom code that can correctly account for this error by creating a modified trapezium rule with numba, but there are other very similar functions that I need to implement doing things like np.trapz(np.square(s), dx=dt). Is there an off the shelf solution for this sort of numerical integral that:

Can be reused for both integrals np.trapz(np.square(s), dx=dt) and np.trapz(np.abs(s), dx=dt), etc...
Is ideally vectorised so that integration can be done for tens of thousands of signals at once in a reasonable time?

For the record, the following parallel numba code is what I am using to integrate the signals
@numba.njit(parallel=True)
def cav_integrate(
    waveform: npt.NDArray[np.float32], dt: float
) -> npt.NDArray[np.float32]:
    """"""Compute the Cumulative Absolute Velocity (CAV) of a waveform.""""""
    cav = np.zeros((waveform.shape[0],), dtype=np.float32)
    for i in range(waveform.shape[0]):
        for j in range(waveform.shape[1] - 1):
            if np.sign(waveform[i, j]) * np.sign(waveform[i, j + 1]) >= 0:
                cav[i] += dt / 2 * (np.abs(waveform[i, j]) + np.abs(waveform[i, j + 1]))
            else:
                slope = (waveform[i, j + 1] - waveform[i, j]) / dt
                x0 = -waveform[i, j] / slope
                cav[i] += x0 / 2 * np.abs(waveform[i, j]) + (dt - x0) / 2 * np.abs(
                    waveform[i, j + 1]
                )
    return cav

Example Data
I have uploaded a small broadband ground motion simulation to a  dropbox link (approx. 91MiB) for testing. This data comes from a finite difference simulation of a recent earthquake near Wellington, New Zealand plus some empirically derived high-frequency noise.  The file is an HDF5 containing some station data (irrelevant for our purposes), and simulation waveforms in the ""waveforms"" key. The array has shape (number of stations, timesteps, components) = (1433, 5876, 3). The 1d numpy array waveform[i, :, j] is the simulated acceleration for the ith station in the jth component. We need to compute the cumulative absolute velocity (CAV) for each component and each station independently. The benchmark code to do this can be found below:
import time

import h5py
import numba
import numpy as np
import numpy.typing as npt

broadband_input_file = h5py.File(""broadband.h5"", ""r"")
# Load the entire dataset into memory so that the first method is not arbitrarily slowed down by file I/O
waveforms = np.array(broadband_input_file[""waveforms""])
dt = 0.01

start = time.process_time()
cav_naive = np.trapz(np.abs(waveforms), dx=dt, axis=1)
print(f""CAV naive time: {time.process_time() - start}"")


@numba.njit
def cav_integrate(
    waveform: npt.NDArray[np.float32], dt: float
) -> npt.NDArray[np.float32]:
    """"""Compute the Cumulative Absolute Velocity (CAV) of a waveform.""""""
    cav = np.zeros((waveform.shape[0], waveform.shape[-1]), dtype=np.float32)
    for c in range(waveform.shape[-1]):
        for i in range(waveform.shape[0]):
            for j in range(waveform.shape[1] - 1):
                if np.sign(waveform[i, j, c]) * np.sign(waveform[i, j + 1, c]) >= 0:
                    cav[i, c] += (
                        dt
                        / 2
                        * (np.abs(waveform[i, j, c]) + np.abs(waveform[i, j + 1, c]))
                    )
                else:
                    slope = (waveform[i, j + 1, c] - waveform[i, j, c]) / dt
                    x0 = -waveform[i, j, c] / slope
                    cav[i, c] += x0 / 2 * np.abs(waveform[i, j, c]) + (
                        dt - x0
                    ) / 2 * np.abs(waveform[i, j + 1, c])
    return cav


# Warm up the numba compilation cache
_ = cav_integrate(waveforms, dt)

start = time.process_time()
cav_bespoke = cav_integrate(waveforms, dt)
print(f""Custom CAV time: {time.process_time() - start}"")


print(
    f""Ratio naive CAV / custom CAV (0, 25, 50, 75, 100% quartiles): {np.percentile(cav_naive / cav_bespoke, [0, 25, 50, 75, 100])}""
)

Which gives the following output
CAV naive time: 0.14353649699999993
Custom CAV time: 0.11182449700000019
Ratio naive CAV / custom CAV (0, 25, 50, 75, 100% quartiles): [1.00607312 1.00999796 1.01163089 1.01318455 1.02221394]

These differences are reasonably small, better examples of larger differences are shown in the comments. Some of the observed waveforms have 20-40% differences between the methods. Even 2% differences might be important for some of the researchers I support. Note also that the CAV calculation is done on a single thread for comparison, but I would parallelise both methods in reality for the largest waveform arrays (having 6 or 7x the stations and 10-20x the timesteps depending on the temporal resolution of the simulation). Funnily enough the parallel overhead for this small file makes cav_integrate slower than the naive approach if enabled.
We actually do the CAV calculation for all linear combinations cos(theta) * waveform[i, :, 0] + sin(theta) * waveform[i, :, 1] where theta = 0, 1,...180° to obtain orientation independent measurements of CAV. This is part of the reason it needs to be fast.
","This answer focus more on the performance/vectorization aspects than numerical integration.

Faster implementation

Is ideally vectorised so that integration can be done for tens of thousands of signals at once in a reasonable time?

Technically, Numba code can run with njit (and without errors) are always vectorized based on the definition on Numpy (a vectorized function is basically a natively compiled function). However, it can be made faster. The first thing to do is to use multiple threads so the code can benefit from multiple CPU cores.

Funnily enough the parallel overhead for this small file makes cav_integrate slower than the naive approach if enabled.

This is because there is 2 issues:

process_time returns the sum of the system and user CPU time (i.e. amount of parallel work) of the current process. Its does not measures the wall clock time. Thus, the benchmark is biased. You should use time() to measure the wall clock time instead.
Numba don't automatically parallelize loops. It only parallelize some basic array operation but to parallelize loops, you need to use prange instead of range or otherwise the loop will be sequential (so the code of the question is not actually parallel).

To efficiently parallelize the code, we should swap the i-based and c-based loops.
Moreover, there are other things to consider when it comes to performance:

you should avoid updating cav[i, c] and accumulate values in a local variable instead (be careful to use it only within the parallel loop).
you should also be careful to avoid implicit 64-bit FP numbers conversions because they are often more expensive and certainly not needed here since you operate on 32-bit FP numbers. For example, dt is a 64-bit number so any operation involving it will results in a 64-bit number.
I think you can use a min-max check instead of a sign-product-based one (the former is more efficient and SIMD friendly)
you should use math tricks so to avoid some expensive mathematical computations like divisions (and avoiding repeated computation like multiplication by a constant for all terms of a sum)
adapt the code so to benefit from SIMD units and improve memory accesses (this can typically improve the scalability of the code): this is typically done by physically swapping the c and j axis (though this requires a different input layout).

Here is the modified code considering all points except the last one (about SIMD and the memory layout):
@numba.njit(parallel=True)
def cav_integrate_opt(
    waveform: npt.NDArray[np.float32], dt: float
) -> npt.NDArray[np.float32]:
    """"""Compute the Cumulative Absolute Velocity (CAV) of a waveform.""""""
    cav = np.zeros((waveform.shape[0], waveform.shape[-1]), dtype=np.float32)
    dtf = np.float32(dt)
    half = np.float32(0.5)
    for i in numba.prange(waveform.shape[0]):
        for c in range(waveform.shape[-1]):
            tmp = np.float32(0)
            for j in range(waveform.shape[1] - 1):
                v1 = waveform[i, j, c]
                v2 = waveform[i, j + 1, c]
                if min(v1, v2) >= 0 or max(v1, v2) <= 0:
                    tmp += dtf * (np.abs(v1) + np.abs(v2))
                else:
                    inv_slope = dtf / (v2 - v1)
                    x0 = -v1 * inv_slope
                    tmp += x0 * np.abs(v1) + (dtf - x0) * np.abs(v2)
            cav[i, c] = tmp * half
    return cav

Here are performance results on my AMD Ryzen 5700U CPU (with 8 cores):
naive trapz (seq):              315 ms
initial cav_integrate (seq):    244 ms
optimized cav_integrate (par):   10 ms   <-----

Th optimized implementation is 25 times faster than cav_integrate and 31 times faster than the naive approach.
For better performance, please consider the last optimization point (more precisely about SIMD). That being said, this can be a bit complex to perform here. It might requires the else branch to be rarely executed (i.e. <5%) so to be pretty efficient.

More generic integration

Can be reused for both integrals np.trapz(np.square(s), dx=dt) and np.trapz(np.abs(s), dx=dt), etc...

Here are some thoughts:
For np.trapz(np.abs(s), dx=dt), a solution consists in computing the minimum value of the signal, then subtract the minimum to the signal, compute np.trapz of the resulting adapted signal so to finally correct the result. This solution is more efficient than you current one because it can benefit from SIMD instructions. However, it does not work for np.square.
A generic solution is to add new points close to the problematic area (thanks to an interpolation function). This solution is not optimal because to increase the computational time and is not numerically exact either (though using a lot of point should give a pretty accurate solution). You do not need to interpolate all points nor to generate new array for the whole array : you can do that line by line or even on the fly (a bit more complicated). This can save a lot of RAM and computation time.
Another generic solution is to pass a generic function in parameter to the numba function for computing differently the case where the sign change. However, this solution should be significantly slower than your specialized solution because it does not benefit from SIMD instructions and add an expensive function call that can hardly be inlined.
You can mix the two last solution so to build a generic solution which should be still faster once running with multiple threads and optimized like above. The idea is to add just one point where the curve cross the line y=0 and and split the integration in two parts. A linear interpolation should give results similar to cav_integrate_opt (if not even equal). Here is an example:
@numba.njit(parallel=True)
def cav_integrate_opt_generic(waveform, dt, fun):
    cav = np.zeros((waveform.shape[0], waveform.shape[-1]), dtype=np.float32)
    dtf = np.float32(dt)
    half = np.float32(0.5)
    for i in numba.prange(waveform.shape[0]):
        for c in range(waveform.shape[-1]):
            tmp = np.float32(0)
            for j in range(waveform.shape[1] - 1):
                v1 = waveform[i, j, c]
                v2 = waveform[i, j + 1, c]
                if min(v1, v2) < 0 and max(v1, v2) > 0:
                    # Basic linear interp
                    # Consider passing another generic function in 
                    # parameter to find roots if needed (more expensive).
                    inv_slope = dtf / (v2 - v1)
                    x0 = -v1 * inv_slope
                    tmp += x0 * fun(v1) + (dtf - x0) * fun(v2)
                else:
                    tmp += dtf * (fun(v1) + fun(v2))
            cav[i, c] = tmp * half
    return cav

# Needs to be wrapped in a Numba function for sake of performance
# (so Numba can call it directly like a native C function)
@numba.njit
def numba_abs(y):
    return np.abs(y)

# Note `cav_integrate_opt_generic` is recompiled for each different provided function.
cav_bespoke = cav_integrate_opt_generic(waveforms, dt, numba_abs)

If you want to do that using a higher-order interpolation and integration then you certainly need to consider more points and a generic function to find roots (which is certainly much more expensive when it is even possible to find analytical solutions).
It turns out this more generic function is only 5~10% slower for np.abs on my machine. Result are the same for np.abs.
",numpy
cross validation on train validation amp test set,"In the scenario of having three sets

A train set of e.g. 80% (for model training)
A validation set of e.g. 10% (for model training)
A test set of e.g. 10% (for final model testing)

let's say I perform k-fold cross validation (CV) on the example dataset of [1,2,3,4,5,6,7,8,9,10]. Let's also say

10 is the test set in this example
the remaining [1,2,3,4,5,6,7,8,9] will be used for training and validation

leave-one-out CV would than look something like this
# Fold 1
[2, 3, 4, 5, 6, 7, 8, 9] # train
[1]                      # validation
# Fold 2
[1, 3, 4, 5, 6, 7, 8, 9] # train
[2]                      # validation
# Fold 3
[1, 2, 4, 5, 6, 7, 8, 9] # train
[3]                      # validation
# Fold 4
[1, 2, 3, 5, 6, 7, 8, 9] # train
[4]                      # validation
# Fold 5
[1, 2, 3, 4, 6, 7, 8, 9] # train
[5]                      # validation
# Fold 6
[1, 2, 3, 4, 5, 7, 8, 9] # train
[6]                      # validation
# Fold 7
[1, 2, 3, 4, 5, 6, 8, 9] # train
[7]                      # validation
# Fold 8
[1, 2, 3, 4, 5, 6, 7, 9] # train
[8]                      # validation
# Fold 9
[1, 2, 3, 4, 5, 6, 7, 8] # train
[9]                      # validation

Great, now the model has been built and validation using each data point of the combined train and validation set once.
Next, I would test my model on the test set (10) and get some performance.
What I was wondering now is why we not also perform CV using the test set and average the result to see the impact of different test sets? Meaning why we don't do the above process 10 times such that we have each data point also in the test set?
It would be obviously computationally extremely expensive but I was thinking about that cause it seemed difficult to choose an appropriate test set. For example, it could be that my model from above would have performed much differently when I would have chosen 1 as the test set and trained and validated on the remaining points.
I wondered about this in scenarios where I have groups in my data. For example

[1,2,3,4] comes from group A,
[5,6,7,8] comes from group B and
[9,10] comes from group C.

In this case when choosing 10 as the test set, it could perform much differently than choosing 1 right, or am I missing something here?
","All your train-validation-test splits should be randomly sampled and sufficiently big. Hence if your data comes from different groups you should have roughly the same distribution of groups across train, validation and test pools. If your test performance varies based on the sampling seed you're definitely doing something wrong.
As to why not use test set for cross-validation, this would result in overfitting. Usually you would run your cross-validation many times with different hyperparameters and use cv score to select best models. If you don't have a separate test set to evaluate your model at the end of model selection you would never know if you overfitted to the training pool during model selection iterations.
",cross-validation
user selects the number of prompt boxes to display,"Trying to write a javascript code
That will enable user to select the number of prompt boxes.
Like,The prompt box will say, how many prompts do you want. Then if the user inputs 2. Two prompt box will show and if user inputs 3 or 4. That number of prompt boxes will show.
    let num = Number(prompt('Enter the number of prompts:'));

if (num=1){
  let name1 = prompt('Enter first user name:');
  let age1 = prompt('Enter first user age');
  
  
  console.log(name1)
  console.log(age1);
}
  
else if (num=2){
  let name1 = prompt('Enter first user name:');
  let age1 = prompt('Enter first user age');
  
  let name2 = prompt('Enter second user name:');
  let age2 = prompt('Enter second user age');
  
  console.log(name1);
  console.log(age1);
  
  console.log(name2)
  console.log(age2);
}
else{
  let name1 = prompt('Enter first user name:');
  let age1 = prompt('Enter first user age');
  
  let name2 = prompt('Enter second user name:');
  let age2 = prompt('Enter second user age');
  
  let name3 = prompt('Enter third user name:');
  let age3 = prompt('Enter third user age');
  
  
  console.log(name1)
  console.log(age1);
  
  console.log(name2)
  console.log(age2);
  
  console.log(name3)
  console.log(age3);
  
}

","I would just use a for loop.
let num = Number(prompt('Enter the number of prompts:'));

const data = [];

for (let i = 0; i < num; i++) {
  let name = prompt(`Enter user ${i}'s name:`);
  let age = prompt(`Enter user ${i}'s age:`);

  data.push({ name, age });
}

data.forEach(d => {
  console.log(d.name);
  console.log(d.age);
});

Simplify it by rewording the prompts so you can use the index of the current position in the loop.
If you want everything to be logged at the end, keep all the answers in an array, and iterate through that array at the end to log the answers.
",prompt
disable internal scikit input validation checks,"Under the hood Scikit-Learn does a lot of input validation checks defined here such as checking for (X,y) shapes, expected scalars/arrays, ensuring estimator is already fitted, etc. When designing models these checks are extremely useful to catch early bugs, but these become runtime overheads in large mature pipelines once in production.
Is there currently any way, maybe via a global setting, passing in keyword arguments, or similar to disable these internal checks within pipelines, estimators & transformers?
","From this scikit discussion, turns out we can set global settings via:
import sklearn
sklearn.set_config(
   assume_finite=True,  # disable validation
)

",scikit-learn
how to draw scaleindependent horizontal bars with tips in matplotlib,"I want to create a plot that shows genomic coding regions as arrows that may contain colorfully highlighted domain regions.
In principle it is something like this:
import numpy as np
import matplotlib.patches as patches
import matplotlib.pyplot as plt

def test(bar_height=0.8, figsize=(10, 6), arrow_headlen=0.2, dpi=600):
    X0 = np.arange(0, 10, 1)
    X1 = X0 + 2
    Y = np.arange(0, 10, 1)

    fig = plt.figure(figsize=figsize)
    ax = fig.add_subplot(111)

    data_2_px = ax.transData.transform  # data domain to figure
    px_2_data = ax.transData.inverted().transform  # figure to data domain

    # get arrow head_length as fraction of arrow width
    # so that it doesnt grow longer with longer x-axis
    dy = bar_height * arrow_headlen
    dpx = data_2_px([(0, dy)]) - data_2_px([(0, 0)])
    arrowlen = (px_2_data([(dpx[0, 1], dpx[0, 0])]) - px_2_data([(0, 0)]))[0, 0]

    ax.barh(y=Y, left=X0, width=X1 - X0, height=bar_height, color=""0.5"")
    for y, x1 in zip(Y, X1):
        yl = y - 0.49 * bar_height  # low arrow corner (avoid being draw 1 px too low)
        yh = y + 0.49 * bar_height  # high arrow corner (avoid being draw 1 px too high)
        arrow = patches.Polygon([(x1, yl), (x1, yh), (x1 + arrowlen, y)], color=""0.5"")
        ax.add_patch(arrow)

    # highlight parts of arrows
    ax.barh(y=Y, left=X0 + 0.5, width=(X1 - X0) / 2, height=bar_height, color=""blue"")

    fig.savefig(""./test_from_savefig.png"", dpi=dpi)
    plt.show()

This draws 10 transcript ""arrows"" in gray and each of these transcripts contains a region highlighted in blue.
When plt.show() opens this in a viewer and I save it from there I get this image (A):

The picture that is saved by fig.savefig() with higher DPI however gives this image (B):

As you can see the arrow heads are suddenly not flush with the arrow base anymore.
It seems that they were scaled differently than the bars.
But both of them are defined in the data domain.
So shouldn't they still be flush.
Image A is what I want to create:

gray arrows of a defined width whose head width is the same as the arrow base width
arrow heads that do not grow longer or shorter depending on the x-axis
possibility to highlight a part of the arrow base

However, I also want to be able to save this plot as raster graphic in a higher resolution.
Why don't I use FancyArrow?
FancyArrow would be a more straight-forward way of defining arrows.
However, they are not drawn in a very reproducible way.
Sometimes a FancyArrow is drawn 1 pixel higher or lower.
This means If I draw a gray FancyArrow and then a blue rectangle over it, there will sometimes be some visible misalignment (e.g. a 1 pixel gray line visible behind the blue area).
I have found that only barh is able to draw a bar of different colors that actually looks like it belongs together.
","To have the arrow heads flush with the arrow bodies even when modifying the dpi value, set linewidth=0 in the patches.Polygon initialization.
Your code then gives this image (saved as PNG with 600 DPI)

Please note that for some reason, now a thin white line appears at the conjunction of some body-head pairs (see the second one starting from top, for example).
To go deeper into this latter issue, I have refactored the code using the rectangle corners to define the triangle vertex (which I paste below), but the result looks just the same.
import numpy as np
import matplotlib.patches as patches
import matplotlib.pyplot as plt

def test(arrow_height=0.8, arrow_bodylen=2.0, arrow_headlen=0.2, 
         figsize=(10, 6), dpi=600):
    # Define points corresponding to arrow tails
    X_tail = np.arange(0, 10, 1)
    Y = np.arange(0, 10, 1)

    # Create the figure
    fig = plt.figure(figsize=figsize)    
    ax = fig.add_subplot(111)

    # Draw arrow bodies (i.e. rectangles)
    h_bars = ax.barh(y=Y,
                     left=X_tail, 
                     width=arrow_bodylen, 
                     height=arrow_height, 
                     color=""0.5"",
                     linewidth = 0  # though it does not seem to have any effect
                    )
    # Loop through rectangles to add heads (i.e. triangles) to arrow bodies
    for bar in h_bars.patches:
        # `get_corners()` return the rectangle corners, starting from `bar.xy`
        # (here it is lower-left corner) in counter-clockwise order. 
        bar_LL, bar_LR, bar_TR, bar_TL = bar.get_corners()
        assert bar_LR[0] == bar_TR[0] # probably could be removed...
        # Define the point corresponding to the arrow tip
        arrow_tip = ( bar_LR[0] + arrow_height * arrow_headlen,
                     (bar_LR[1] + bar_TR[1]) / 2
                     )
        # Define a triangle between the right end of the bar and the arrow tip
        arrow = patches.Polygon([bar_LR, 
                                 bar_TR, 
                                 arrow_tip
                                 ], 
                                color=""0.5"",
                                linewidth = 0
                                )
        ax.add_patch(arrow) # draw the triangle
    
    # Highlight parts of the arrow bodies
    ax.barh(y=Y, 
            left=X_tail + 0.25 * arrow_bodylen, 
            width=arrow_bodylen / 2, 
            height=arrow_height, 
            color=""blue"",
            linewidth = 0
            )
    
    fig.savefig(""./test_from_savefig.png"", dpi=dpi)
    plt.show()

test()

Hope this helps at least a bit!
",matplotlib
how do i suppress yesno prompt when saving digitally signed mail in a format that is not secure,"I wrote VBA to save an email as a text file. 
For digitally signed emails it prompts me if I want to save the file in a format that is not secure (yes or no to continue). I want to suppress this warning. 
I tried the following:
With Application
     .EnableEvents = False
     .ScreenUpdating = False
End With
Application.DisplayAlerts = False
m.SaveAs savePath, olTXT 
With Application
     .EnableEvents = True
     .ScreenUpdating = ture
End With

And
Application.DisplayAlerts = False
m.SaveAs savePath, olTXT 
Application.DisplayAlerts = True

They both do not work (I get a compile error). How do I turn off these alerts in Outlook?
","Those properties are for Word or Excel.  Outlook does not have the equivalent functionality.  The best you can do is use the SendKeys method to simulate clicking the Yes or No buttons: https://msdn.microsoft.com/en-us/library/office/gg278655.aspx
",prompt
