instruction,input,output,Categories
how do i force clustering of data in a specific evident pattern,"I have a large set of 'Vehicle speed vs Engine RPM' values for a vehicle. I'm try to predict the time spent by the vehicle on each gear.
I ran K-Means clustering on the dataset and got the following result:

Clearly, my algorithm has failed to capture the evident pattern. I want to force K-Means (or any other clustering algorithm, for that matter) to cluster data along the six sloped lines. Snippet of relevant code:
import numpy as np
import pandas as pd
from matplotlib import pyplot as plt
from sklearn.cluster import KMeans

plt.rcParams['figure.figsize'] = (16, 9)
plt.style.use('ggplot')

# Importing the dataset
data = pd.read_csv('speedRpm.csv')
print(data.shape)
data.head()

# Getting the data points
f1 = data['rpm'].values
f2 = data['speed'].values
X = np.array(list(zip(f1, f2)))

# Number of clusters
k = 5

kmeans = KMeans(n_clusters=k)
# Fitting the input data
kmeans = kmeans.fit(X)
# Getting the cluster labels
labels = kmeans.predict(X)
# Centroid values
centroids = kmeans.cluster_centers_

labeled_array = {i: X[np.where(kmeans.labels_ == i)] for i in range(kmeans.n_clusters)}

colors = ['r', 'g', 'b', 'y', 'c']
fig, ax = plt.subplots()
for i in range(k):
        points = np.array([X[j] for j in range(len(X)) if kmeans.labels_[j] == i])
        ax.scatter(points[:, 0], points[:, 1], s=7, c=colors[i])
ax.scatter(centroids[:, 0], centroids[:, 1], marker='*', s=200, c='#050505')

plt.show()

How do I make sure the clustering algorithm captures the right pattern, even though it possibly isn't the most efficient?
Thanks!
EDIT:
Ran the same set of points using DBSCAN this time. After playing around with the eps and min_samples values for sometime, got the following result:
 
Although, still not perfect and way too many outliers, the algorithm is beginning to capture the linear trend.
Code:
import numpy as np
import pandas as pd
from matplotlib import pyplot as plt
from sklearn.cluster import KMeans
from sklearn.cluster import DBSCAN

plt.rcParams['figure.figsize'] = (16, 9)
plt.style.use('ggplot')

# Importing the dataset
data = pd.read_csv('speedRpm.csv')
print(data.shape)
data.head()

# Getting the values and plotting it
f1 = data['rpm'].values
f2 = data['speed'].values
X = np.array(list(zip(f1, f2)))

# DBSCAN

# Compute DBSCAN
db = DBSCAN(eps=1.1, min_samples=3).fit(X)
core_samples_mask = np.zeros_like(db.labels_, dtype=bool)
core_samples_mask[db.core_sample_indices_] = True
labels = db.labels_

# Number of clusters in labels, ignoring noise if present.
n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)
print ""Estimated Number of Clusters"", n_clusters_

# Black removed and is used for noise instead.
unique_labels = set(labels)
colors = [plt.cm.Spectral(each)
          for each in np.linspace(0, 1, len(unique_labels))]
for k, col in zip(unique_labels, colors):
    if k == -1:
        # Black used for noise.
        col = [0, 0, 0, 1]

    class_member_mask = (labels == k)

    xy = X[class_member_mask & core_samples_mask]
    plt.plot(xy[:, 0], xy[:, 1], 'o', markerfacecolor=tuple(col),
             markeredgecolor='k', markersize=14)

    xy = X[class_member_mask & ~core_samples_mask]
    plt.plot(xy[:, 0], xy[:, 1], 'o', markerfacecolor=tuple(col),
             markeredgecolor='k', markersize=6)

plt.title('Estimated number of clusters: %d' % n_clusters_)
plt.show()

"," High Level 
There are two major options here: 

Transform your data so that k-means-style clustering algorithms succeed
Pick a different algorithm

Minor option:

Tweak kmeans by forcing the initialization to be smarter


 Option 2 
Python has a good description of several clustering algorithms here . From the link, a (crudely cropped) helpful graphic:

This row looks similar to your dataset; have you tried a Gaussian mixture model? A GMM has few well known theoretical properties, but it works by assigning probabilities that points belong to each cluster center based on a posterior calculated from the data. You can often initialize it with kmeans, which Sklearn does for you.
Similarly, desnity-based clustering algorithms (DBSCAN, e.g.), seem like a logical choice. Your data has a nice segmentation of dense clusters, and this seems like a good topological property to be filtering for. In the image on the linked wikipedia page:

they offer the caption:

DBSCAN can find non-linearly separable clusters. This dataset cannot
  be adequately clustered with k-means

which seems to speak to your troubles.

 More on your troubles 
Kmeans is an extremely versatile algorithm, but it is not globally optimal and suffers from a lot of weak-points. Here is dense reading
In addition to problems like the mickey mouse problem, kmeans is often trying to minimize simple euclidean distance to the centroids. While this makes a lot of sense for a lot of problems, it doesn't make sense in yours, where the skew of the clusters means that isn't quite the correct measure. Notice that other algorithms like agglomerative/hierarchical clustering, shown above, that use similar measures, have similar trappings.
I haven't covered transforming your data or tweaking kmeans because the latter requires actually hacking into (or writing your own) clustering algorithm (I don't recommend for a simple exploratory problem given the coverage of sklearn and similar packages), where the former seems like a local solution sensitive to your exact data. ICA might be a decent start, but there's a lot of options for that task
",data-science
getting straight line while creating arima model,"I have a Fan Speed (RPM) dataset of 192.405 Values (train+test values). I am training the ARIMA model and trying to predict the rest of the future values of our dataset and comparing the results.
While fitting the model in test data I am getting straight line for predictions
from sklearn.model_selection import train_test_split 
from statsmodels.tsa.arima_model import ARIMA

dfx = df[(df['Tarih']>'2020-07-23') & (df['Tarih']<'2020-10-23')]

X_train = dfx[:int(dfx.shape[0]*0.8)] #2 months
X_test = dfx[int(dfx.shape[0]*0.8):] # rest, 1 months

model = ARIMA(X_train.Value, order=(4,1,4))
model_fit = model.fit(disp=0)
print(model_fit.summary())

test = X_test
train = X_train

What could i do now ?

","Your ARIMA model uses the last 4 observations to make a prediction. The first prediction will be based on the four last known data points. The second prediction will be based on the first prediction and the last three known data points. The third prediction will be based on the first and second prediction and the last two known data points and so on. Your fifth prediction will be based entirely on predicted values. The hundredth prediction will be based on predicted values based on predicted values based on predicted values … Each prediction will have a slight deviation from the actual values. These prediction errors accumulate over time. This often leads to ARIMA simply prediction a straight line when you try to predict such large horizons.
If your model uses the MA component, represented by the q parameter, then you can only predict q steps into the future. That means your model is only able to predict the next four data points, after that the prediction will converge into a straight line.
",data-science
aggregate function as an argument,"I need to write a function:
def group_and_aggregate_data(df:pd.DataFrame, group_by_column:str, agg_func) -> pd.DataFrame

that groups my Excel data by city name and applies the agg_func passed as an argument. I tried this:
def group_and_aggregate_data(dataframe, cond, func):
    df_bycity = dataframe.groupby(cond).func()
    return df_bycity

but it doesn't work in Python.
","Can you try this with df.agg():
def group_and_aggregate_data(dataframe, cond, func):
    df_bycity = dataframe.groupby(cond).agg(func)
    return df_bycity

",data-science
color prediction system with opencv python,"
Hi I am an initial stage developer trying to make a color prediction system, but I came up with some error that I can not understand. I think you could help me with this .... I will share with you my proper code
This is the error: IndexError: index 1 is out of bounds for axis 0 with size 1
The error showing in RGB2HEX(color): function but I am unable to solve it
Here is the Code:
def RGB2HEX(color):
return ""#{:02x}{:02x}{:02x}"".format(int(color[0]), int(color[1]), int(color[2]))

def get_image(image_path):
image = cv2.imread(image_path)
image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
return image

IMAGE_DIRECTORY = 'C:/Users/Dell/Desktop/CPS 02'
COLORS = {
    'GREEN': [0, 128, 0],
    'BLUE': [0, 0, 128],
    'YELLOW': [255, 255, 0]
}
images = []

for file in os.listdir(IMAGE_DIRECTORY):
    if not file.startswith('.'):
        images.append(get_image(os.path.join(IMAGE_DIRECTORY, file)))

# extracting colors from image 
def get_colors(images, number_of_colors, show_char = True):
for j in range(len(images)):
    modified_image = cv2.resize(images[j], (600, 400), interpolation = cv2.INTER_AREA)
    modified_image = modified_image.reshape(modified_image.shape[0]*modified_image.shape[1],1)

    clf = KMeans(n_clusters = number_of_colors)
    labels = clf.fit_predict(modified_image)

    counts = Counter(labels)

    center_colors = clf.cluster_centers_
    # We get ordered colors by iterating through the keys
    ordered_colors = [center_colors[i] for i in counts.keys()]
    hex_colors = [RGB2HEX(ordered_colors[i]) for i in counts.keys()]
    rgb_colors = [ordered_colors[i] for i in counts.keys()]

# matching an image by its color
def match_image_by_color(image, color, threshold = 60, number_of_colors = 10): 

image_colors = get_colors(image, number_of_colors, False)
selected_color = rgb2lab(np.uint8(np.asarray([[color]])))

select_image = False
for i in range(number_of_colors):
    curr_color = rgb2lab(np.uint8(np.asarray([[image_colors[i]]])))
    diff = deltaE_cie76(selected_color, curr_color)
    if (diff < threshold):
        select_image = True

return select_image

# Selecting an image
def show_selected_images(images, color, threshold, colors_to_match):
index = 1

for i in range(len(images)):
    selected = match_image_by_color(images[i], color, threshold, colors_to_match)
    if (selected):
        plt.subplot(1, 5, index)
        plt.imshow(images[i])
        index += 1

# printing the result 
plt.figure(figsize = (20, 10))
show_selected_images(images, COLORS['BLUE'], 60, 5)

","Using this page as a guide for imports and such, I was able to successfully implement your code on some stock images I have AND recreate your error. I think I know what is going on.
I believe the images you are reading in are single channel (grayscale), not RGB. When you reshape the images, you turn each image into an Nx1 vector -- only a single color channel there. Hence your RGB2HEX() function errors when it references the second dimension of your 1D image vector.
When I instead read in a three channel (RGB) image, and reshape to an Nx3 matrix, your function successfully executes. I have included full code below. You'll note the two lines (commented out) that can be used with grayscale images to reproduce your error. Obviously comment out the RGB equivalent lines if you want the grayscale version.
from sklearn.cluster import KMeans
import matplotlib.pyplot as plt
import numpy as np
import cv2
from collections import Counter
import os
import argparse

def RGB2HEX(color):
    return ""#{:02x}{:02x}{:02x}"".format(int(color[0]), int(color[1]), int(color[2]))

def get_image(image_path):
    image = cv2.imread(image_path)
    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
    # example to read as grayscale image
    # image = cv2.imread(image_path, 0)
    return image

IMAGE_DIRECTORY = './path/to/images/'
COLORS = {
    'GREEN': [0, 128, 0],
    'BLUE': [0, 0, 128],
    'YELLOW': [255, 255, 0]
}
images = []

for file in os.listdir(IMAGE_DIRECTORY):
    if not file.startswith('.'):
        images.append(get_image(os.path.join(IMAGE_DIRECTORY, file)))

# extracting colors from image 
def get_colors(images, number_of_colors, show_char = True):
    for j in range(len(images)):
        modified_image = cv2.resize(images[j], (600, 400), interpolation = cv2.INTER_AREA)
        modified_image = modified_image.reshape(modified_image.shape[0]*modified_image.shape[1], 3)
        # example to use with grayscale images        
        # modified_image = modified_image.reshape(modified_image.shape[0]*modified_image.shape[1], 1)

        clf = KMeans(n_clusters = number_of_colors)
        labels = clf.fit_predict(modified_image)

        counts = Counter(labels)

        center_colors = clf.cluster_centers_
        # We get ordered colors by iterating through the keys
        ordered_colors = [center_colors[i] for i in counts.keys()]
        hex_colors = [RGB2HEX(ordered_colors[i]) for i in counts.keys()]
        rgb_colors = [ordered_colors[i] for i in counts.keys()]

",data-science
error in odbcconnectaccess2007quotbdembalsesmdbquot r could not find the function quotodbcconnectaccess2007quot after installing and loading rodbc,"I'm a student of Data Science, and I'm working in a project to visualize a database of Spanish reservoirs; but I'm getting this error:
# package installations if required:
if(!require(""tidyverse"")) install.packages(""tidyverse"")
if(!require(""janitor"")) install.packages(""janitor"")
if(!require(""patchwork"")) install.packages(""patchwork"")
if(!require(""RODBC"")) install.packages(""RODBC"")

# load the packages
library(tidyverse)
library(readxl)
library(janitor)
library(patchwork)
library(RODBC)

# URL of the database
url <- ""https://www.miteco.gob.es/content/dam/miteco/es/agua/temas/evaluacion-de-los-recursos-hidricos/boletin-hidrologico/Historico-de-embalses/BD-Embalses.zip""

# download
tempf <- tempfile() # temp file
download.file(url, tempf) 
unzip(tempf)

# open the connection with mdb
conn <- odbcConnectAccess2007(""BD-Embalses.mdb"")

I installed everything on my system (Arch linux), but I can't make the connection to the database for some reason (it says that the function could not be found). Everything is in Spanish, I'm just translating it into English.
","As can be found in the win.R file of the RODBC package, odbcConnectAccess2007 is a Windows-exclusive function. It depends on an ODBC driver for MS Access being available, and these are only available for Windows.
This means you need to find an alternative way, e.g. using mdbtools with the ODBC driver and either DBI + odbc (recommended) or RODBC, mdbtools + Hmisc::mdb.get, or RJDBC + UCanAccess.
Side note, always download.file(url, tempf, mode = ""wb"") if you're downloading a non-text file. This would lead to errors further down the line.
",data-science
searching for maybe spurious correlation in google trends,"This is a bit of an odd question but I was thinking about this for some time now and I would be very interested in best practice of doing this:
Let's assume I have a dataset with a time series variable X. Now I want to find ALL google search terms Y that have a (maybe spurious) correlation within the same time span with X. The problem is that you don't know which search terms were entered in google and therefore can't check for ALL correlations. So basically I want to do something similar to spurious correlations.
How would you start doing this?
I got this idea while using
Google Trends
","This was available as Google Correlate, but was recently deprecated.
You can read more about it in the whitepaper
",data-science
gridsearchcv  xgboost  early stopping,"i am trying to do hyperparemeter search with using scikit-learn's GridSearchCV on XGBoost. During gridsearch i'd like it to early stop, since it reduce search time drastically and (expecting to) have better results on my prediction/regression task. I am using XGBoost via its Scikit-Learn API.
    model = xgb.XGBRegressor()
    GridSearchCV(model, paramGrid, verbose=verbose ,fit_params={'early_stopping_rounds':42}, cv=TimeSeriesSplit(n_splits=cv).get_n_splits([trainX, trainY]), n_jobs=n_jobs, iid=iid).fit(trainX,trainY)

I tried to give early stopping parameters with using fit_params, but then it throws this error which is basically because of lack of validation set which is required for early stopping:
/opt/anaconda/anaconda3/lib/python3.5/site-packages/xgboost/callback.py in callback(env=XGBoostCallbackEnv(model=<xgboost.core.Booster o...teration=4000, rank=0, evaluation_result_list=[]))
    187         else:
    188             assert env.cvfolds is not None
    189 
    190     def callback(env):
    191         """"""internal function""""""
--> 192         score = env.evaluation_result_list[-1][1]
        score = undefined
        env.evaluation_result_list = []
    193         if len(state) == 0:
    194             init(env)
    195         best_score = state['best_score']
    196         best_iteration = state['best_iteration']

How can i apply GridSearch on XGBoost with using early_stopping_rounds?
note: model is working without gridsearch, also GridSearch works without 'fit_params={'early_stopping_rounds':42}
","An update to @glao's answer and a response to @Vasim's comment/question, as of sklearn 0.21.3 (note that fit_params has been moved out of the instantiation of GridSearchCV and been moved into the fit() method; also, the import specifically pulls in the sklearn wrapper module from xgboost):
import xgboost.sklearn as xgb
from sklearn.model_selection import GridSearchCV
from sklearn.model_selection import TimeSeriesSplit

cv = 2

trainX= [[1], [2], [3], [4], [5]]
trainY = [1, 2, 3, 4, 5]

# these are the evaluation sets
testX = trainX 
testY = trainY

paramGrid = {""subsample"" : [0.5, 0.8]}

fit_params={""early_stopping_rounds"":42, 
            ""eval_metric"" : ""mae"", 
            ""eval_set"" : [[testX, testY]]}

model = xgb.XGBRegressor()

gridsearch = GridSearchCV(model, paramGrid, verbose=1,             
         cv=TimeSeriesSplit(n_splits=cv).get_n_splits([trainX, trainY]))

gridsearch.fit(trainX, trainY, **fit_params)

",data-science
creating team data science process in ado,"Our organization is currently utilizing Azure DevOps (ADO), and we're interested in integrating the ""Team Data Science Process"" (TDSP) into our data science and machine learning project workflows within ADO.
Upon accessing ADO Board --> Process, I notice that there are four available processes:

Basic
Agile
Scrum
CMMI

What I am seeking guidance on is how we can effectively incorporate TDSP using one of these processes. Specifically, I would like each new project to encompass the following stages, each of which can be labeled as either 'To Do', 'Doing', or 'Done':

Business Understanding
Data Acquisition and Understanding
Modeling
Deployment

Any assistance or advice on how we can achieve this would be greatly appreciated.
","
I would like each new project to encompass the following stages, each of which can be labeled as either 'To Do', 'Doing', or 'Done'

All processes support the above states, 'To Do', 'Doing', or 'Done'. It is recommended that you read Default processes to have a basic understanding of each process and choose the one that suits you.
Among the available processes, Agile or Scrum would be more suitable for integrating TDSP due to their flexibility and iterative nature.
You can use different work item types to represent your different stages.

You can create inheritance process based on Agile or Scrum .

Add custom work item types to reflect the stages of TDSP. For example, for Business Understanding, create a work item type called ""Business Understanding"", and so on.

For each new work item type, define the states as 'To Do', 'Doing', and 'Done'. This will help you track the progress of each stage.


You can also use custom states to represent the stages without create new work item type.

Create an inheritance process and select a default work item type for your work. For example, Epic.
For the target work item type, add new states for ""Business Understanding"", ""Data Acquisition and Understanding"", ""Modeling"" and ""Deployment"".
Add a new field in the work item with picklist 'To Do', 'Doing', or 'Done'.

The above suggestions are just to provide you with some ideas, they may not fully meet your situation. Adjust them according to your actual needs.
",data-science
how to perform standardization and normalization on features from different feature engineering process,"I'm working with a dataset where each sample contains both numeric and text data. Therefore multiple methods are employed to build the training feature matrix from the dataset. For each sample in the dataset, I construct a vector representation from 3 parts.

Doc2Vec vector representation for paragraph text: I use the gensim implemetation of paragraph vector to encode the text into a 100-D vetors of floats between [-5, 5]

One-hot encoded vector for text label: Each sample in the dataset has zero or more text label, I aggregate out all of the unique labels used in the dataset and encode it into a binary array containing only 0 and 1. For example, if the complete set of labels is [Python, Java, JavaScript, C++] and a sample contains labels Python and Java, the resulted vector will be [1, 1, 0, 0].

Numeric data & categorical data:

Numeric data fields are built into the feature vector as is
Categorical data are mapped to integers and built into the feature vector



The resulted feature matrix looks something like below
[
  [-1.02, 1.33, 2.35, -0.48, ... -4.11, 1, 0, 1, 1, 0, 0, ..., 1, 0, 235, 11.5, 333],
  [-0.22, 3.03, 1.95, -0.48, ... -4.11, 0, 1, 1, 1, 0, 0, ..., 0, 0, 233, 22, 333],
  [-2.07, -1.33, -2.35, -0.48, ... -4.11, 1, 1, 0, 1, 1, 0, ..., 1, 1, 102, 13, 333],
  [-4.32, 4.33, 1.75, -0.48, ... -4.11, 0, 0, 0, 1, 0, 1, ..., 1, 0, 98, 8, 333],
]

Should I apply any standardization or normalization on the dataset? If so, should I do it before or after concatenating different parts of feature?
I'm using scikit-learn and the major algorithm I using will be Gradient Boosting.
","Yes, you need to process features separately: you should apply standardization or normalization only on the original numerical features, you shouldn't do it for doc2vec, OHE or encoded categorical features.
",data-science
parallel querying indices for a list of filter expressions in polars dataframe,"I want to get the indices for a list of filters in polars and get a sparse matrix from it, how can I parallel the process? This is what I have right now, a pretty naive and brute force way for achieving what I need, but this is having some serious performance issue
def get_sparse_matrix(exprs: list[pl.Expr]) -> scipy.sparse.csc_matrix:
    df = df.with_row_index('_index')
    rows: list[int] = []
    cols: list[int] = []
    for col, expr in enumerate(exprs):
        r = self.df.filter(expr)['_index']
        rows.extend(r)
        cols.extend([col] * len(r))

    X = csc_matrix((np.ones(len(rows)), (rows, cols)), shape= 
   (len(self.df), len(rules)))

    return X

Example Input:
# df is a polars dataframe with size 8 * 3
df = pl.DataFrame(
[[1,2,3,4,5,6,7,8], 
[3,4,5,6,7,8,9,10], 
[5,6,7,8,9,10,11,12],
[5,6,41,8,21,10,51,12],
])

# three polars expressions
exprs = [pl.col('column_0') > 3, pl.col('column_1') < 6, pl.col('column_4') > 11]

Example output:
X is a sparse matrix of size 8 (number of records) X 3 (number of expressions), where the element at i,j equals to 1 if ith record matches the jth expression
","So I am not completely sure what exactly you want, but I hope that satisfies your needs
import polars as pl
from scipy.sparse import csc_matrix
import numpy as np

df = pl.DataFrame(
    [[1,2,3,4,5,6,7,8], 
    [3,4,5,6,7,8,9,10], 
    [5,6,7,8,9,10,11,12],
    [5,6,41,8,21,10,51,12],
])


exprs = [(pl.col('column_0') > 3).cast(pl.Int8), 
         (pl.col('column_1') < 6).cast(pl.Int8), 
         (pl.col('column_3') > 11).cast(pl.Int8)]

X = df.select(exprs)
csc_matrix(X.to_numpy())

",data-science
fill nan values in polars using a customdefined function for a specific column,"I have this code in pandas:
df[col] = (
            df[col]
            .fillna(method=""ffill"", limit=1)
            .apply(lambda x: my_function(x))
        )

I want to re-write this in Polars.
I have tried this:
df = df.with_columns(
            pl.col(col)
            .fill_null(strategy=""forward"", limit=1)
            .map_elements(lambda x: my_function(x))
        )

It does not work properly. It fills with forward strategy but ignores filling missing values with my defined function. What should I change in my code to get what I want?
try this code:
df_polars = pl.DataFrame(
    {""A"": [1, 2, None, None, None, None, 4, None], ""B"": [5, None, None, None, None, 7, None, 9]}
)

df_pandas = pd.DataFrame(
    {""A"": [1, 2, None, None, None, None, 4, None], ""B"": [5, None, None, None, None, 7, None, 9]}
)

last_valid_data: int


def my_function(x):
    global last_valid_data
    if x == None or np.isnan(x):
        result = last_valid_data * 10
    else:
        last_valid_data = x
        result = x
    return result


col = ""A""

last_valid_data = df_pandas[col][0]
df_pandas[col] = df_pandas[col].fillna(method=""ffill"", limit=1).apply(lambda x: my_function(x))

last_valid_data = df_polars[col][0]
df_polars = df_polars.with_columns(
    pl.col(col).fill_null(strategy=""forward"", limit=1).map_elements(lambda x: my_function(x))
)

Desired output in pandas is:
      A    B
0   1.0  5.0
1   2.0  NaN
2   2.0  NaN
3  20.0  NaN
4  20.0  NaN
5  20.0  7.0
6   4.0  NaN
7   4.0  9.0

What I get in Polars is:
┌──────┬──────┐
│ A    ┆ B    │
│ ---  ┆ ---  │
│ i64  ┆ i64  │
╞══════╪══════╡
│ 1    ┆ 5    │
│ 2    ┆ null │
│ 2    ┆ null │
│ null ┆ null │
│ null ┆ null │
│ null ┆ 7    │
│ 4    ┆ null │
│ 4    ┆ 9    │
└──────┴──────┘

","The issue here is that in Polars .map_elements defaults to skip_nulls=True
df_polars.with_columns(
   pl.col('A').map_elements(lambda me: print(f'{me=}'))
)

me=1
me=2
me=4

As your example specifically needs to target the nulls, you need to change this to False
df_polars.with_columns(
   pl.col('A').map_elements(lambda me: print(f'{me=}'), skip_nulls=False)
)

me=1
me=2
me=None
me=None
me=None
me=None
me=4
me=None

",data-science
when should you convert age column in float or int,"I had this doubt, often datasets have the Age column values in either int or float datatype (Eg Titanic).
So suppose the column has all float values, should you convert them all to int or let it be just like that while feeding it to ML Model,
Does it have any harm or adverse effects in prediction results and what's the right way?
","age is a continuous variable: every moment that passes you age, you don't age incrementally once a year, so the data type which most closely reflects reality is a float and not an integer.
However using a float or an integer depends on the use case, eg:

Are you using age as a feature describing how old people are? Better use float (eg a person who is 59.9 is older than a person who is 59.1 and may be more likely to develop certain medical conditions, or maybe less physically fit and less likely to survive in an event of a sinking ship)
Are you reporting on age groups? Might be better off rounding to nearest integer (eg 39.9 -> 40, 34.2 -> 34) and potentially binning (eg 25-34, 35-45)
Are you looking at age from a legal standpoint? (e.g. analysis of underage drinking) then you should use the rounded down int value (eg if legal age is 16 and a person is 15.9, legally they are 15 and therefore underage drinking)
etc...

As a general remark you'll often find that there is no single ""right way"" of dealing with data, it all depends on the use case.
",data-science
inner_kws having no effect on seaborn violin plot,"I generated a bunch of violin plots, here is an example of one and the code that generates it:
plt.figure(figsize=(8, 4))
ax = sns.violinplot(
    x=data, # `data` is a few thousand float values between 0 and 1
    orient='h',
    color=get_color(ff), # `get_color` returns a color based on the dataset, #FFBE0B in this case
    cut=0
)


I want to make the black box in the middle quite a bit bigger. According to the documentation from Seaborn at https://seaborn.pydata.org/generated/seaborn.violinplot.html, I should be able to do this with the inner_kws parameter. I added this argument to the above code:
plt.figure(figsize=(8, 4))
ax = sns.violinplot(
    x=data, # `data` is a few thousand float values between 0 and 1
    orient='h',
    color=get_color(ff), # `get_color` returns a color based on the dataset, #FFBE0B in this case
    inner_kws=dict(box_width=150, whis_width=20),
    cut=0
)

Above, the box and whisker width are 150 and 20 respectively. I've also tried 15 and 2, and 1500 and 200. No matter what values I enter here, the figure does not change at all. What am I doing wrong?
","The inner_kws argument was introduced in version 0.13.0; if you have an older version of seaborn installed it has no effect. I had seaborn v0.12.2 (installed via conda) and your example printed with normal boxplot dimensions until I upgraded seaborn to v0.13.2,
E.g.
#!/usr/bin/env python

import random
import seaborn as sns
import matplotlib.pyplot as plt

data = []
for i in range(0, 1000):
    x = round(random.uniform(0, 1), 4)
    data.append(x)

ax = sns.violinplot(
    x=data, # `data` is a few thousand float values between 0 and 1
    orient='h',
    color='#FFBE0B',
    inner_kws=dict(box_width=100, whis_width=20),
    cut=0
)

plt.savefig(""example.png"")


Does that solve your problem? Or is there something else causing an issue?
",data-science
impute missing values with mean of column in machine learning,"I know that imputing missing values is exactly what it sounds, i'm talking about imputing it with mean of the column. I usually impute missing values before i split the data into train and test but then i saw this QnA that said 

CAUTION: if you want to use this for Machine Learning / Data Science: from a Data Science perspective it is wrong to first replace NA and then split into train and test... You MUST first split into train and test, then replace NA by mean on train and then apply this stateful preprocessing model to test, see the answer involving sklearn below! – Fabian Werner Aug 28 '19 at 9:18

What does it mean by that? can we do it? and how do we do it? is there any different between doing it before or after splitting the data? if yes, why? Please help me to understand because i'm quite confused over this thing.
","Yes, this is a correct statement. You should at first split the data into train and valid/test data, calculate the mean on the train data and apply it to valid/test data.
In fact this is relevant to any processing which is based on the data itself.
If you calculate and transform on the whole dataset, you leak information into the data. But we want to have a correct validation, so valid/test dataset should be processed exactly like train
",data-science
check if any value in a polars dataframe is true,"This is quite a simple ask but I can't seem to find any clear simplistic solution to this, feels like I'm missing something.
Let's say I have a DataFrame of type
df = pl.from_repr(""""""
┌───────┬───────┬───────┐
│ a     ┆ b     ┆ c     │
│ ---   ┆ ---   ┆ ---   │
│ bool  ┆ bool  ┆ bool  │
╞═══════╪═══════╪═══════╡
│ false ┆ true  ┆ false │
│ false ┆ false ┆ false │
│ false ┆ false ┆ false │
└───────┴───────┴───────┘
"""""")

How do I do a simple check if any of the values in the DataFrame is True?
Some solutions I have found is
selection = df.select(pl.all().any(ignore_nulls=True))

or
selection = df.filter(pl.any_horizontal())

and then check in that row
any(selection.row(0))

Is just seems like so many steps for a single check
","These two options are a bit shorter and stay in pure Polars.
# Unpivot all the booleans into a single ""value"" column
# Pull the ""value column out as a Series any do the any
df.unpivot()[""value""].any()

# pl.all().any() checks for any True values per column
# pl.any_horizontal() checks horizontally per row, reducing to a single value
df.select(pl.any_horizontal(pl.all().any())).item()

To your question

This is quite a simple ask but I can't seem to find any clear simplistic solution to this, feels like I'm missing something. It just seems like so many steps for a single check

You are not missing anything. The reason it feels like a bit more work is because a DataFrame can be thought of more like a (database) table. Generally you have different columns of potentially different types, and you want to different calculations with different columns. So reducing both dimensions into a single value in a single step is just not something typically offered by DataFrame libraries.
Numpy is much better suited if you have matrices and does offer this in a single step.
arr = df.to_numpy()
arr.any() # True

",data-science
how do i install sklearn module properly,"I'm trying to install sklearn module using pip command but after the installation is completed , all I can see is this folder
C:\Users\Aditi\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\LocalCache\local-packages\Python38\site-packages\sklearn-0.0-py3.8.egg-info 

In my directory and even the error says module name sklearn not found.
I've tried reinstalling it many times but still I'm not able to see the main sklearn folder in the above directory.Only 1 folder is installed i.e sklearn-0.0-py3.8.egg-info .Can anyone please help?
","Try to install using command pip install scikit-learn.
If it still not work for you, you can update the numpy or reinstall the numpy.
You can check here all the help related to installation and verifying the installation of scikit-learn
",data-science
how to create nonalphabetically ordered categorical column in polars dataframe,"In Pandas, you can create an ""ordered"" Categorical column from existing string column as follows:
column_values_with_custom_order = [""B"", ""A"", ""C""]

df[""Column""] = pd.Categorical(df.Column, categories=column_values_with_custom_order, ordered=True)

In Polars documentation, I couldn't find such way to create ordered columns. However, I could reproduce this by using pl.from_pandas(df) so I suspect that this is possible with Polars as well.
What would be the recommended way to this?
I tried to create new column with:
polars_df.with_columns(col(""Column"").cast(pl.Categorical))

but I don't know how to include the custom ordering to this.
I also checked In polars, can I create a categorical type with levels myself?, but I would prefer not to add another column to my Dataframe only for ordering.
","Say you have
df = pl.DataFrame(
     {""cats"": [""z"", ""z"", ""k"", ""a"", ""b""], ""vals"": [3, 1, 2, 2, 3]}
     )

and you want to make cats a categorical but you want the categorical ordered as
myorder=[""k"", ""z"", ""b"", ""a""]

There are two ways to do this.  One way is with pl.StringCache() as in the question you reference and the other is more messy.  The former does not require you add any columns to your df.  It's actually very succinct.
with pl.StringCache():
    pl.Series(myorder).cast(pl.Categorical)
    df=df.with_columns(pl.col('cats').cast(pl.Categorical))

What happens is that everything in the StringCache gets the same key values so when the myorder list is casted that saves what keys get allocated to each string value.  When your df gets casted under the same cache it gets the same key/string values which are in the order you wanted.
The other way to do this is as follows:
You have to sort your df by the ordering then you can do set_ordering('physical').  If you want to maintain your original order then you just have to use with_row_count at the beginning so you can restore that order.
Putting it all together, it looks like this:
df=df.with_row_index('i').join(
        pl.from_dicts([{'order':x, 'cats':y} for x,y in enumerate(myorder)]), on='cats') \
    .sort('order').drop('order') \
    .with_columns(pl.col('cats').cast(pl.Categorical('physical'))) \
    .sort('i').drop('i')

You can verify by doing:
df.select('cats',pl.col('cats').to_physical().alias('phys'))

shape: (5, 2)
┌──────┬──────┐
│ cats ┆ phys │
│ ---  ┆ ---  │
│ cat  ┆ u32  │
╞══════╪══════╡
│ z    ┆ 1    │
│ z    ┆ 1    │
│ k    ┆ 0    │
│ a    ┆ 3    │
│ b    ┆ 2    │
└──────┴──────┘

",data-science
conditioning previous values within groups in r,"I'm trying to write a code that will allow me to create a TRUE or FALSE variable within the groups name depending on the value of the earliest record of the column poped of the following data.frame:
 library(tidyverse)   
  name<-c(""AAA"",""AAA"",""AAA"",""AAA"",""AAA"",""AAA"",""AAA"")
  poped<-c(NA,1,NA,NA,1,NA,NA)
  order<-c(1:7)
  tag<-c(""X"",""Y"",""X"",""X"",""Y"",""X"",""X"")

>   df
  name order tag poped
1  AAA     1   X    NA
2  AAA     2   Y     1
3  AAA     3   X    NA
4  AAA     4   X    NA
5  AAA     5   Y     1
6  AAA     6   X    NA
7  AAA     7   X    NA

I want to mutate a two new variable named CHECK and  POS
CHECK will take on the values
    1= If the closest (above) value where the tag column is Y and poped is 1
    0= If the closest (above) value where the tag column is Y and poped is 0
    2 = If the current row has tag = Y
    NA = Otherwise

POS will take on the value of the closest (above) row number where the tag column is Y and poped is 1, and NA otherwise.
My desired output will be:
>   df
  name order tag poped CHECK POS                                                            why
1  AAA     1   X    NA    NA  NA                                      There is no previous data
2  AAA     2   Y     1    NA  NA                                                current tag = Y
3  AAA     3   X    NA     1   2 the closest value above where tag=Y is in row 2 and poped is 1
4  AAA     4   X    NA     1   2 the closest value above where tag=Y is in row 2 and poped is 1
5  AAA     5   Y     1    NA  NA                                                current tag = Y
6  AAA     6   X    NA     1   5 the closest value above where tag=Y is in row 5 and poped is 1
7  AAA     7   X    NA     1   5 the closest value above where tag=Y is in row 5 and poped is 1

How can I create a solution, ideally using Tidyverse?
","df %>%
  mutate(ctag=if_else(tag==""Y"",tag,as.character(NA)),
         cpop=if_else(tag==""Y"",poped,as.double(NA)),
         maxr=if_else(tag==""Y"" & poped==1,order,as.integer(NA))) %>%
  fill(ctag,cpop,maxr) %>% 
  mutate(
    CHECK = case_when(
      tag == ""Y""~2,
      lag(ctag) == ""Y"" & lag(cpop)==1 ~1,
      lag(ctag) == ""Y"" & lag(cpop)==0 ~0,
      TRUE~as.double(NA)),
    POS = if_else(tag==""Y"", as.integer(NA), maxr)
  ) %>% 
  select(!ctag:maxr)

Output:
  name  order tag   poped CHECK   POS
  <chr> <int> <chr> <dbl> <dbl> <int>
1 AAA       1 X        NA    NA    NA
2 AAA       2 Y         1     2    NA
3 AAA       3 X        NA     1     2
4 AAA       4 X        NA     1     2
5 AAA       5 Y         1     2    NA
6 AAA       6 X        NA     1     5
7 AAA       7 X        NA     1     5

",data-science
how to imitate pandas39 indexbased querying in polars,"Any idea what I can do to imitate the below pandas code using polars? Polars doesn't have indexes like pandas so I couldn't figure out what I can do .
df = pd.DataFrame(data = ([21,123], [132,412], [23, 43]), columns = ['c1', 'c2']).set_index(""c1"")

out = df.loc[[23, 132]] # polars equivalent of this?

print(pl.from_pandas(out.reset_index()))

and it prints
shape: (2, 2)
┌─────┬─────┐
│ c1  ┆ c2  │
│ --- ┆ --- │
│ i64 ┆ i64 │
╞═════╪═════╡
│ 23  ┆ 43  │
│ 132 ┆ 412 │
└─────┴─────┘

the only polars conversion I could figure out to do is
df = pl.DataFrame(data = ([21,123], [132,412], [23, 43]), schema = ['c1', 'c2'], orient = 'row')
print(df.filter(pl.col(""c1"").is_in([23, 132])))

but it prints
shape: (2, 2)
┌─────┬─────┐
│ c1  ┆ c2  │
│ --- ┆ --- │
│ i64 ┆ i64 │
╞═════╪═════╡
│ 132 ┆ 412 │
│ 23  ┆ 43  │
└─────┴─────┘

which is okay but the rows are not in the order I gave. I gave [23, 132] and want the output rows to be in the same order, like how pandas' output has.
I can use a sort() later yes, but the original data I use this on has like 30Million rows so I'm looking for something that's as fast as possible.
","I suggest using a left join to accomplish this.  This will maintain the order corresponding to your list of index values.  (And it is quite performant.)
For example, let's start with this shuffled DataFrame.
nbr_rows = 30_000_000

df = pl.DataFrame({
    'c1': pl.int_range(0, nbr_rows, eager=True).shuffle(2),
    'c2': pl.int_range(0, nbr_rows, eager=True).shuffle(3),
})
df

shape: (30000000, 2)
┌──────────┬──────────┐
│ c1       ┆ c2       │
│ ---      ┆ ---      │
│ i64      ┆ i64      │
╞══════════╪══════════╡
│ 4052015  ┆ 20642741 │
│ 7787054  ┆ 17007051 │
│ 20246150 ┆ 19445431 │
│ 1309992  ┆ 6495751  │
│ ...      ┆ ...      │
│ 10371090 ┆ 4791782  │
│ 26281644 ┆ 12350777 │
│ 6740626  ┆ 24888572 │
│ 22573405 ┆ 14885989 │
└──────────┴──────────┘

And these index values:
nbr_index_values = 10_000
s1 = pl.Series(name='c1', values=pl.int_range(0, nbr_index_values, eager=True).shuffle())
s1

shape: (10000,)
Series: 'c1' [i64]
[
        1754
        6716
        3485
        7058
        7216
        1040
        1832
        3921
        1639
        6734
        5560
        7596
        ...
        4243
        4455
        894
        7806
        9291
        1883
        9947
        3309
        2030
        7731
        4706
        8528
        8426
]

We now perform a left join to obtain the rows corresponding to the index values.  (Note that the list of index values is the left DataFrame in this join.)
start = time.perf_counter()
df2 = (
    s1.to_frame()
    .join(
        df,
        on='c1',
        how='left'
    )
)
print(time.perf_counter() - start)

df2

>>> print(time.perf_counter() - start)
0.8427023889998964

shape: (10000, 2)
┌──────┬──────────┐
│ c1   ┆ c2       │
│ ---  ┆ ---      │
│ i64  ┆ i64      │
╞══════╪══════════╡
│ 1754 ┆ 15734441 │
│ 6716 ┆ 20631535 │
│ 3485 ┆ 20199121 │
│ 7058 ┆ 15881128 │
│ ...  ┆ ...      │
│ 7731 ┆ 19420197 │
│ 4706 ┆ 16918008 │
│ 8528 ┆ 5278904  │
│ 8426 ┆ 18927935 │
└──────┴──────────┘

Notice how the rows are in the same order as the index values.  We can verify this:
s1.equals(df2.get_column('c1'), check_dtypes=True)

>>> s1.equals(df2.get_column('c1'), check_dtypes=True)
True

And the performance is quite good.  On my 32-core system, this takes less than a second.
",data-science
how can i shift a column in a dataframe x times into an array apply a function and create a new column,"I am having a hard time understanding how I can better improve this code:
from datetime import datetime

import math
import pandas as pd
import numpy as np

def trending(row, label, amt, prepend = False, postpend = False, reverse = True):
    arr = []

    if prepend:
        arr.append(row[label])

    for i in range(amt):
        arr.append(row[f'{label}_{i}'])

    if postpend:
        arr.append(row[label])

    if reverse:
        arr.reverse()

    if any(x == None or math.isnan(x) for x in arr):
        return 0

    return slope(arr)

def slope(arr):
    if len(arr) <= 1:
        return 0
    coeffs = np.polyfit(range(len(arr)), arr, 1)
    slope = coeffs[-2]
    return float(slope)

def trends(df):
    start = datetime.now()

    look_back = 5

    for i in range(look_back):
        j = i + 1
        df2 = df[['close', 'pvi', 'nvi', 'smi', 'roc', 'macd', 'histogram', 'percent_b', 'height']]
        df2 = df2.add_suffix(f'_{i}')
        df2 = df2.shift(j)
        df = pd.concat([df, df2], axis=1)

    def close_trend(row):
        return trending(row, 'close', look_back, True, False)

    def pvi_trend(row):
        return trending(row, 'pvi', look_back, True, False)

    def nvi_trend(row):
        return trending(row, 'nvi', look_back, True, False)

    def smi_trend(row):
        return trending(row, 'smi', look_back, True, False)

    def macd_trend(row):
        return trending(row, 'macd', look_back, True, False)

    def roc_trend(row):
        return trending(row, 'roc', look_back, True, False)

    def histogram_trend(row):
        return trending(row, 'histogram', look_back, True, False)

    def percent_b_trend(row):
        return trending(row, 'percent_b', look_back, True, False)

    def height_trend(row):
        return trending(row, 'height', look_back, True, False)
    
    df['close_trend'] = df.apply(close_trend, axis=1)
    df['pvi_trend'] = df.apply(pvi_trend, axis=1)
    df['nvi_trend'] = df.apply(nvi_trend, axis=1)
    df['smi_trend'] = df.apply(smi_trend, axis=1)
    df['macd_trend'] = df.apply(macd_trend, axis=1)
    df['roc_trend'] = df.apply(roc_trend, axis=1)
    df['histogram_trend'] = df.apply(histogram_trend, axis=1)
    df['percent_b_trend'] = df.apply(percent_b_trend, axis=1)
    df['height_trend'] = df.apply(height_trend, axis=1)

    print(f'Trends function took {datetime.now() - start}')

    return df

data = []

for i in range(10000):
    data.append([545.9, 0.3333398862,   0.01673619117,  0.2111060119,   55.95725508,    1.100447539,    0.8652411735,   0.8219623901,   1.808441041,    46.79554862])

columns = ['close', 'macd', 'histogram', 'roc', 'rsi', 'pvi', 'nvi', 'percent_b', 'height', 'smi']

df = pd.DataFrame(columns=columns, data=data)

df = trends(df)

print(df)

It works, but it is awfully slow. I have a dataframe without about 10k rows and it takes about 30s. Basically, what I am trying to accomplish is this:

Shift the ""trend columns"" x number of times so each row has the x number of previous values
Take those values + the current rows value and find the slope
Set the slope to a ""trending"" column

","You can achieve huge speed-up (200-300x) by applying polyfit to whole dataframe instead of doing it row by row.
I added some code to check correctness, check yourself and remove  afterwards.
def trending_vectorized(df, label, amt, prepend=False, postpend=False, reverse=True):
    arr = []

    if prepend:
        arr.append(df[label])

    for i in range(amt):
        arr.append(df[f'{label}_{i}'])

    if postpend:
        arr.append(df[label])

    if reverse:
        arr.reverse()

    arr = np.vstack(arr)
    poly_coeffs = np.polyfit(np.arange(len(arr)), np.vstack(arr), 1)
    poly_coeffs[np.isnan(poly_coeffs)] = 0  # possible speed-up: insert zeros where needed
    return poly_coeffs[0, :]  # slope only

def trends(df):
    start = datetime.now()

    look_back = 5

    for i in range(look_back):
        j = i + 1
        df2 = df[['close', 'pvi', 'nvi', 'smi', 'roc', 'macd', 'histogram', 'percent_b', 'height']]
        df2 = df2.add_suffix(f'_{i}')
        df2 = df2.shift(j)
        df = pd.concat([df, df2], axis=1)

    def close_trend(row):
        return trending(row, 'close', look_back, True, False)

    def pvi_trend(row):
        return trending(row, 'pvi', look_back, True, False)

    def nvi_trend(row):
        return trending(row, 'nvi', look_back, True, False)

    def smi_trend(row):
        return trending(row, 'smi', look_back, True, False)

    def macd_trend(row):
        return trending(row, 'macd', look_back, True, False)

    def roc_trend(row):
        return trending(row, 'roc', look_back, True, False)

    def histogram_trend(row):
        return trending(row, 'histogram', look_back, True, False)

    def percent_b_trend(row):
        return trending(row, 'percent_b', look_back, True, False)

    def height_trend(row):
        return trending(row, 'height', look_back, True, False)

    df['close_trend'] = df.apply(close_trend, axis=1)
    df['pvi_trend'] = df.apply(pvi_trend, axis=1)
    df['nvi_trend'] = df.apply(nvi_trend, axis=1)
    df['smi_trend'] = df.apply(smi_trend, axis=1)
    df['macd_trend'] = df.apply(macd_trend, axis=1)
    df['roc_trend'] = df.apply(roc_trend, axis=1)
    df['histogram_trend'] = df.apply(histogram_trend, axis=1)
    df['percent_b_trend'] = df.apply(percent_b_trend, axis=1)
    df['height_trend'] = df.apply(height_trend, axis=1)

    df['close_trend2'] = trending_vectorized(df, 'close', look_back, True, False)
    df['pvi_trend2'] = trending_vectorized(df, 'pvi', look_back, True, False)
    df['nvi_trend2'] = trending_vectorized(df, 'nvi', look_back, True, False)
    df['smi_trend2'] = trending_vectorized(df, 'smi', look_back, True, False)
    df['macd_trend2'] = trending_vectorized(df, 'macd', look_back, True, False)
    df['roc_trend2'] = trending_vectorized(df, 'roc', look_back, True, False)
    df['histogram_trend2'] = trending_vectorized(df, 'histogram', look_back, True, False)
    df['percent_b_trend2'] = trending_vectorized(df, 'percent_b', look_back, True, False)
    df['height_trend2'] = trending_vectorized(df, 'height', look_back, True, False)

    assert np.allclose(df['close_trend'], df['close_trend2'])
    assert np.allclose(df['pvi_trend'], df['pvi_trend2'])
    assert np.allclose(df['nvi_trend'], df['nvi_trend2'])
    assert np.allclose(df['smi_trend'], df['smi_trend2'])
    assert np.allclose(df['macd_trend'], df['macd_trend2'])
    assert np.allclose(df['roc_trend'], df['roc_trend2'])
    assert np.allclose(df['histogram_trend'], df['histogram_trend2'])
    assert np.allclose(df['percent_b_trend'], df['percent_b_trend2'])
    assert np.allclose(df['height_trend'], df['height_trend2'])
    print(f'Trends function took {datetime.now() - start}')

    return df

Noted you liked clean code, so bonus - cleaned function:
def trends2(df):
    col_names = ['close', 'pvi', 'nvi', 'smi', 'macd', 'roc', 'histogram', 'percent_b', 'height']
    look_back = 5
    for i in range(look_back):
        j = i + 1
        df2 = df[col_names]
        df2 = df2.add_suffix(f'_{i}')
        df2 = df2.shift(j)
        df = pd.concat([df, df2], axis=1)

    for col_name in col_names:
        df[f'{col_name}_trend'] = trending_vectorized(df, col_name, look_back, True, False)

    return df

",data-science
note you may need to restart the kernel to use updated packages jupyter,"I was working with Jupiter notebook but I entered a difficulty. Could you help me?
I have to use  from scipy.special import j. Even though I installed scipy lib, It could not run properly. After I searched, I used%pip install scipy --upgrade.
Then I got this message like:
""Requirement already satisfied"". But at the end of the MSG, it said:
""Note: you may need to restart the kernel to use updated packages.""
I reseat kernel from toolbar thousand times, even I tried this code:
HTML(""<script>Jupyter.notebook.kernel.restart()</script>"")

Still, it said:
""Note: you may need to restart the kernel to use updated packages.""
Because I already reset the kernel many times and I do not know what else to do, I ran my import sentence again:
from scipy.special import j
but I see:
""ImportError: cannot import name 'j' from 'scipy.special'""
please help me if you can. Now I'm stuck!
","I strongly recommend you to create a New Environment (new_env), Then try this, sometimes the same name with multiple folders may cause this problem.
Step1: Create a New Environment.
Step2: Install the packages.
Step3: Definetely it will work.
Still, the Problem continues means, use it google collab!
",data-science
is there a way to cumulatively and distinctively expand list in polars,"For distance, I want to accomplish conversion like below.
df = pl.DataFrame({
    ""col"": [[""a""],  [""a"", ""b""],  [""c""]]
})

┌────────────┐
│ col        │
│ ---        │
│ list[str]  │
╞════════════╡
│ [""a""]      │
│ [""a"", ""b""] │
│ [""c""]      │
└────────────┘
↓
↓
↓
┌────────────┬─────────────────┐
│ col        ┆ col_cum         │
│ ---        ┆ ---             │
│ list[str]  ┆ list[str]       │
╞════════════╪═════════════════╡
│ [""a""]      ┆ [""a""]           │
│ [""a"", ""b""] ┆ [""a"", ""b""]      │
│ [""c""]      ┆ [""a"", ""b"", ""c""] │
└────────────┴─────────────────┘

I've tried polars.Expr.cumulative_eval(), but could not get it to work.
I can access the first element and last element in every iteration. But I want here is the result of the previous iteration i think.
Could I get some help?
","We can use the cumulative_eval expression.
But first, let's expand your data so that we can include some other things that may be of interest.

We'll include a group variable to show how the algorithm can be used with grouping variables.
We'll also include an empty list to show how the algorithm will handle these.

import polars as pl

df = pl.DataFrame(
    {
        ""group"": [1, 1, 1, 2, 2, 2, 2],
        ""var"": [[""a""], [""a"", ""b""], [""c""], [""p""], [""q"", ""p""], [], [""s""]],
    }
)
df

shape: (7, 2)
┌───────┬────────────┐
│ group ┆ var        │
│ ---   ┆ ---        │
│ i64   ┆ list[str]  │
╞═══════╪════════════╡
│ 1     ┆ [""a""]      │
│ 1     ┆ [""a"", ""b""] │
│ 1     ┆ [""c""]      │
│ 2     ┆ [""p""]      │
│ 2     ┆ [""q"", ""p""] │
│ 2     ┆ []         │
│ 2     ┆ [""s""]      │
└───────┴────────────┘

The Algorithm
Here's the heart of the algorithm:
(
    df
    .with_columns(
        pl.col('var')
        .cumulative_eval(
            pl.element()
            .explode()
            .unique()
            .sort()
            .implode()
        )
        .list.drop_nulls()
        .over('group')
        .alias('cumulative')
    )
)

shape: (7, 3)
┌───────┬────────────┬─────────────────┐
│ group ┆ var        ┆ cumulative      │
│ ---   ┆ ---        ┆ ---             │
│ i64   ┆ list[str]  ┆ list[str]       │
╞═══════╪════════════╪═════════════════╡
│ 1     ┆ [""a""]      ┆ [""a""]           │
│ 1     ┆ [""a"", ""b""] ┆ [""a"", ""b""]      │
│ 1     ┆ [""c""]      ┆ [""a"", ""b"", ""c""] │
│ 2     ┆ [""p""]      ┆ [""p""]           │
│ 2     ┆ [""q"", ""p""] ┆ [""p"", ""q""]      │
│ 2     ┆ []         ┆ [""p"", ""q""]      │
│ 2     ┆ [""s""]      ┆ [""p"", ""q"", ""s""] │
└───────┴────────────┴─────────────────┘

How it works
cumulative_eval allows us to treat a subset of rows for a column as if it was a Series itself (with the exception that we access the elements of the underlying Series using polars.element.)
So, let's simulate what the cumulative_eval expression is doing by working work with the Series itself directly.  We'll simulate what the algorithm does when cumulative_eval reaches the last row where group ==  1 (the third row).
The first major step of the algorithm is to explode the lists.  explode will put each element of each list on its own row:
(
    df
    .select(
        pl.col('var')
        .filter(pl.col('group') == 1)
        .explode()
    )
)

shape: (4, 1)
┌─────┐
│ var │
│ --- │
│ str │
╞═════╡
│ a   │
│ a   │
│ b   │
│ c   │
└─────┘

In the next step, we will use unique and sort to eliminate duplicates and keep the order consistent.
(
    df
    .select(
        pl.col('var')
        .filter(pl.col('group') == 1)
        .explode()
        .unique()
        .sort()
    )
)

shape: (3, 1)
┌─────┐
│ var │
│ --- │
│ str │
╞═════╡
│ a   │
│ b   │
│ c   │
└─────┘

At this point, we need only to roll up all the values into a list.
(
    df
    .select(
        pl.col('var')
        .filter(pl.col('group') == 1)
        .explode()
        .unique()
        .sort()
        .implode()
    )
)

shape: (1, 1)
┌─────────────────┐
│ var             │
│ ---             │
│ list[str]       │
╞═════════════════╡
│ [""a"", ""b"", ""c""] │
└─────────────────┘

And that is the value that cumulative_eval returns for the third row.
Performance
The documentation for cumulative_eval comes with a strong warning about performance.

Warning: This can be really slow as it can have O(n^2) complexity. Don’t use this for operations that visit all elements.

Let's simulate some data.  The code below generates about 9.5 million records, 10,000 groups, so that there are about 950 observations per group.
import numpy as np
from string import ascii_lowercase

rng = np.random.default_rng(1)

nbr_rows = 10_000_000
df = (
    pl.DataFrame({
        'group': rng.integers(1, 10_000, size=nbr_rows),
        'sub_list': rng.integers(1, 10_000, size=nbr_rows),
        'var': rng.choice(list(ascii_lowercase), nbr_rows)
    })
    .group_by('group', 'sub_list')
    .agg(
        pl.col('var')
    )
    .drop('sub_list')
    .sort('group')
)
df

shape: (9515737, 2)
┌───────┬────────────┐
│ group ┆ var        │
│ ---   ┆ ---        │
│ i64   ┆ list[str]  │
╞═══════╪════════════╡
│ 1     ┆ [""q"", ""r""] │
│ 1     ┆ [""z""]      │
│ 1     ┆ [""b""]      │
│ 1     ┆ [""j""]      │
│ ...   ┆ ...        │
│ 9999  ┆ [""z""]      │
│ 9999  ┆ [""e""]      │
│ 9999  ┆ [""s""]      │
│ 9999  ┆ [""s""]      │
└───────┴────────────┘

One my 32-core system, here's the wall-clock time:
import time
start = time.perf_counter()
(
    df
    .with_columns(
        pl.col('var')
        .cumulative_eval(
            pl.element()
            .explode()
            .unique()
            .sort()
            .implode()
        )
        .list.drop_nulls()
        .over('group')
        .alias('cumulative')
    )
)
print(time.perf_counter() - start)

shape: (9515737, 3)
┌───────┬────────────┬─────────────────────┐
│ group ┆ var        ┆ cumulative          │
│ ---   ┆ ---        ┆ ---                 │
│ i64   ┆ list[str]  ┆ list[str]           │
╞═══════╪════════════╪═════════════════════╡
│ 1     ┆ [""q"", ""r""] ┆ [""q"", ""r""]          │
│ 1     ┆ [""z""]      ┆ [""q"", ""r"", ""z""]     │
│ 1     ┆ [""b""]      ┆ [""b"", ""q"", ... ""z""] │
│ 1     ┆ [""j""]      ┆ [""b"", ""j"", ... ""z""] │
│ ...   ┆ ...        ┆ ...                 │
│ 9999  ┆ [""z""]      ┆ [""a"", ""b"", ... ""z""] │
│ 9999  ┆ [""e""]      ┆ [""a"", ""b"", ... ""z""] │
│ 9999  ┆ [""s""]      ┆ [""a"", ""b"", ... ""z""] │
│ 9999  ┆ [""s""]      ┆ [""a"", ""b"", ... ""z""] │
└───────┴────────────┴─────────────────────┘
>>> print(time.perf_counter() - start)
118.46121257600134

Roughly 2 minutes on my system for 9.5 million records.  Depending on your system, you may get better or worse performance.  But the point is that is didn't take hours to complete.
If you need better performance, we can come up with a better-performing algorithm (or perhaps put in a feature request for a cumlist feature in Polars, which might have a complexity better than the O(n^2) complexity of cumulative_eval.)
",data-science
what is the recommended way for retrieving row numbers index for polars,"I know polars does not support index by design, so df.filter(expr).index isn't an option, another way I can think of is by adding a new column before applying any filters, not sure if this is an optimal way for doing so in polars
df.with_columns(pl.Series('index', range(len(df))).filter(expr).index

","Use with_row_index():
df = pl.DataFrame([pl.Series(""a"", [5, 9, 6]), pl.Series(""b"", [8, 3, 4])])

In [20]: df.with_row_index()
Out[20]: 
shape: (3, 3)
┌────────┬─────┬─────┐
│ index  ┆ a   ┆ b   │
│ ---    ┆ --- ┆ --- │
│ u32    ┆ i64 ┆ i64 │
╞════════╪═════╪═════╡
│ 0      ┆ 5   ┆ 8   │
│ 1      ┆ 9   ┆ 3   │
│ 2      ┆ 6   ┆ 4   │
└────────┴─────┴─────┘

# Start from 1 instead of 0.
In [21]: df.with_row_index(offset=1)
Out[21]: 
shape: (3, 3)
┌────────┬─────┬─────┐
│ index  ┆ a   ┆ b   │
│ ---    ┆ --- ┆ --- │
│ u32    ┆ i64 ┆ i64 │
╞════════╪═════╪═════╡
│ 1      ┆ 5   ┆ 8   │
│ 2      ┆ 9   ┆ 3   │
│ 3      ┆ 6   ┆ 4   │
└────────┴─────┴─────┘

# Start from 1 and call column ""my_index"".
In [22]: df.with_row_index(name=""my_index"", offset=1)
Out[22]: 
shape: (3, 3)
┌──────────┬─────┬─────┐
│ my_index ┆ a   ┆ b   │
│ ---      ┆ --- ┆ --- │
│ u32      ┆ i64 ┆ i64 │
╞══════════╪═════╪═════╡
│ 1        ┆ 5   ┆ 8   │
│ 2        ┆ 9   ┆ 3   │
│ 3        ┆ 6   ┆ 4   │
└──────────┴─────┴─────┘

",data-science
count and groupby a specfic value,"I have a dataframe where i want to count a specific value that occurs in a row.
This code below gives the right answer and now i want to add a new coluumn to my dataframe
occur = df.groupby(['Code_5elaag','Essentieel_Optioneel']).size()
occur

**Code_5elaag  Essentieel_Optioneel**
1101         essentieel               8
             optioneel                8
1102         essentieel               8
             optioneel               51
1103         essentieel               8
                                     ..
96231        optioneel                6
96232        essentieel               1
             optioneel                2
96290        essentieel               9
             optioneel               17

When i assign a new colum to the frame this is the output:
uniq['ess'] = df.groupby(['Code_5elaag'])['Essentieel_Optioneel'].transform(np.size)

    Code_5elaag Omschrijving_5elaag Soort_Skill Aantal_skills   ess
0   1101    Officieren landmacht    taken   16  16              15
16  1102    Officieren luchtmacht   taken   59  59              59
75  1103    Officieren marechaussee taken   16  16              16


But that is not what i want i want to divide the amount of Aantal_skills to how much is essentieel and optioneel fo for the first row it should be 8 essentieel and 8 optional
","You are close, need grouping by both columns:
df['ess'] = df.groupby(['Code_5elaag','Essentieel_Optioneel'])['Essentieel_Optioneel'].transform('size')

If need 2 new columns use crosstab with DataFrame.join:
out = df.join(pd.crosstab(df['Code_5elaag'], df['Essentieel_Optioneel']), on='Code_5elaag')

",data-science
nifty50 data science project in python error occuring keyerror 39date39,"I am working on Nifty50 dataset as my Data science project but this error occurs when I'm trying to implement datetime. Please help me?
This is my code to change Date which is an object into datetime:
nfty50_data['Date'] = pd.to_datetime(nifty50_data['Date'])

but that error occurs:

KeyError: 'Date'

","too long to comment ....
all of the field names have a trailing space (excepting the last)'
#output from nifty50.columns:
Index(['Date ', 'Open ', 'High ', 'Low ', 'Close ', 'Shares  Traded ','Turnover (₹ Cr)'],dtype='object')

# so using either 
# (the first throws a warning about format but works fine)

nifty['Date '] = pd.to_datetime(nifty['Date ']) 

#or 

nifty['Date '] = pd.to_datetime(nifty['Date '], format='%d-%b-%Y')


hope this helps.
",data-science
shape mismatch xgboost regressor,"I have trained an XGBoost Regressor model on data that has a different shape to the test data I intend to predict on. Is there a way to go around this or a model that can tolerate feature mismatches?
The input training data and test data got mismatched during One Hot Encoding of categorical features.
best_xgb = xgb.XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,
             colsample_bynode=1, colsample_bytree=1, enable_categorical=False,
             gamma=0, gpu_id=-1, importance_type=None,
             interaction_constraints='', learning_rate=0.05, max_delta_step=0,
             max_depth=6, min_child_weight=10,monotone_constraints='()', n_estimators=400, n_jobs=4,
             num_parallel_tree=1, predictor='auto', random_state=0, reg_alpha=0,
             reg_lambda=1, scale_pos_weight=1, subsample=1, tree_method='exact',
             validate_parameters=1, verbosity=None)

best_xgb.fit(X, y)

best_xgb.predict(test_data)


I get the following error:
Shape Mismatch Error
","Please check where 249-235=14 features are in test data.
Or fit on same data
best_xgb.fit(X[test_data.columns], y)

",data-science
pytorch transformtotensor changes image,"I want to convert images to tensor using torchvision.transforms.ToTensor(). After processing, I printed the image but the image was not right. Here is my code:
trans = transforms.Compose([
    transforms.ToTensor()])

demo = Image.open(img) 
demo_img = trans(demo)
demo_array = demo_img.numpy()*255
print(Image.fromarray(demo_array.astype(np.uint8)))

The original image is:

After processing, it looks like:

","It seems that the problem is with the channel axis.
If you look at torchvision.transforms docs, especially on ToTensor()

Converts a PIL Image or numpy.ndarray (H x W x C) in the range
[0, 255] to a torch.FloatTensor of shape (C x H x W) in the range [0.0, 1.0]

So once you perform the transformation and return to numpy.array your shape is: (C, H, W) and you should change the positions, you can do the following:
demo_array = np.moveaxis(demo_img.numpy()*255, 0, -1)

This will transform the array to shape (H, W, C) and then when you return to PIL and show it will be the same image.
So in total:
import numpy as np
from PIL import Image
from torchvision import transforms

trans = transforms.Compose([transforms.ToTensor()])

demo = Image.open(img) 
demo_img = trans(demo)
demo_array = np.moveaxis(demo_img.numpy()*255, 0, -1)
print(Image.fromarray(demo_array.astype(np.uint8)))

",data-science
taskrun failed to finish due to an error for coretex bioinformatics workflow,"After starting bioinformatics workflow in Coretex, I am getting the following message even though data seems to be in order:
""Failed to determine which column contains sampleIDs/names..."" and then the list of available names, but I am using one from the list.
I am trying to run a microbiome sequencing task in Coretex, and I have used standard microbiome sequencing data in .fastq.gz format. Run should have been successful but it is failing every time.
I've worked with this R code for uploading metadata:
loadMetadata <- function(metadataSample) {
metadata_csv_path <- builtins$str(
    metadataSample$joinPath(""metadata.csv"")
)

if (file.exists(metadata_csv_path)) {
    # Default SampleSheet.csv format
    metadata <- read.table(
        metadata_csv_path,
        sep = "","",
        header = TRUE,
        check.names = TRUE
    )
} else {
    # Format accepted by qiime2
    metadata_tsv_path <- builtins$str(
        metadataSample$joinPath(""metadata.tsv"")
    )

    if (!file.exists(metadata_tsv_path)) {
        stop(""Metadata file not found"")
    }

    metadata <- read.table(
        metadata_tsv_path,
        sep = ""\t"",
        header = TRUE,
        check.names = TRUE
    )

    # qiime has 1 extra row after header which contains types
    metadata <- metadata[-1,]
}

# Remove leading and trailing whitespace
colnames(metadata) <- lapply(colnames(metadata), trimws)

stringColumns <- names(metadata)[vapply(metadata, is.character, logical(1))]
metadata[, stringColumns] <- lapply(metadata[, stringColumns], trimws)

sampleIdColumn <- getSampleIdColumnName(metadata)
print(paste(""Matched metadata sample ID/name column to"", sampleIdColumn))

print(""Renaming metadata sample ID/name column to \""sampleId\"""")
names(metadata)[names(metadata) == sampleIdColumn] <- ""sampleId""

print(""Metadata"")
print(colnames(metadata))
print(head(metadata))

print(metadata$sampleId)

# assign the names of samples (01Sat1...) to metadata rows instead of 1,2,3...
row.names(metadata) <- metadata$sampleId
metadata$sampleId <- as.factor(metadata$sampleId)

return(metadata)

}
","Judging by the logs of your Coretex Workflow it looks like your Dataset contains metadata.csv file which uses ; as a separator, but the Coretex Task for loading BioInformatics data tries to load it with a , as a separator. This was changed in the latest version of the Task and you can see the full changelog here.
Instead of always forcing the separator to be , (old version):
    # Default SampleSheet.csv format
    metadata <- read.table(
        metadata_csv_path,
        sep = "","",
        header = TRUE,
        check.names = TRUE
    )

It will now try to automatically determine what the separator is using fread function (new version):
    metadata <- fread(metadata_csv_path, data.table=FALSE)

",data-science
getting valueerror all arrays must be of the same length,"I have been trying to convert a dictionary into a dataframe but everytime i keep getting ValueError: All arrays must be of the same length. i Have checkde the length of each array and confirmed them to be the same but i am still getting the same error
def metrics_from_pipes(pipes_dict):
     for name, pipeline in pipes_dict.items():
        
        pipeline.fit(X_train, y_train)
        y_pred_val = pipeline.predict(X_val)
        y_pred_train = pipeline.predict(X_train)


train_metrics = {
            'model':list(pipes_dict.keys()),
            'MAE':train_mae,
            'MAPE':train_mape,
            'RMSE':train_rmse,
            'RSquared':train_rsquared
        }
        
        train_metrics_data = pd.DataFrame(train_metrics)
        val_metrics = {
            'model':list(pipes_dict.keys()),
            'MAE':val_mae,
            'MAPE':val_mape,
            'RMSE':val_rmse,
            'RSquared':val_rsquared            
        }
        
        val_metrics_data = pd.DataFrame(val_metrics,)

        #Merging metrics from train and test set
        train_val_metrics = train_metrics_data.merge(val_metrics_data,
                                               on = 'Model',
                                               how = 'left',
                                               suffixes = ('_train', '_val'))
        
        # sorting columns 
        train_val_metrics = train_val_metrics.reindex(columns = ['Model',
                                                               'MAE_train',
                                                                'MAPE_train',
                                                                'RMSE_train',
                                                                'RSquared_train',
                                                                'MAE_val',
                                                                'MAPE_val',
                                                                'RMSE_val',
                                                                'RSquared_val'])
        
    
    
    return train_val_metrics.set_index('Model').transpose()

# get the metrics table
metrics_table = metrics_from_pipes(pipelines)

running this code gives this error
ValueError                                Traceback (most recent call last)
Cell In[45], line 82
     80     return train_val_metrics.set_index('Model').transpose()
     81 # get the metrics table
---> 82 metrics_table = metrics_from_pipes(pipelines)
     83 #print('Table 1: Base Models Metrics')
     84 #metrics_table.style.background_gradient(cmap = Blues)
     85 metrics_table

Cell In[45], line 50, in metrics_from_pipes(pipes_dict)
     41 # aggregate the performance metric lists into seperate dataframes
     42 train_metrics = {
     43     'model':list(pipes_dict.keys()),
     44     'MAE':train_mae,
   (...)
     47     'RSquared':train_rsquared
     48 }
---> 50 train_metrics_data = pd.DataFrame(train_metrics)
     51 val_metrics = {
     52     'model':list(pipes_dict.keys()),
     53     'MAE':val_mae,
   (...)
     56     'RSquared':val_rsquared            
     57 }
     59 val_metrics_data = pd.DataFrame(val_metrics,)

ValueError: All arrays must be of the same length

when i checked for the result of the dictionary for both train_metrics and val metrics, i got this
({'model': ['Linear Regression',
   'Random Forest Regressor',
   'Gradient Boost Regression',
   'Extra Tree Regressor'],
  'MAE': [829.1023412412194,
   288.33455697065233,
   712.9637267872279,
   0.0010629575741748962],
  'MAPE': [1.0302372135902111,
   0.20937541440883897,
   0.538244903316323,
   6.306697580961048e-07],
  'RMSE': [1120.5542708017374,
   416.48933196590013,
   1012.399201767692,
   0.05804079289490426],
  'RSquared': [0.5598288286601083,
   0.9391916010838417,
   0.6406981997919169,
   0.9999999988190745]},
 {'model': ['Linear Regression',
   'Random Forest Regressor',
   'Gradient Boost Regression',
   'Extra Tree Regressor'],
  'MAE': [855.9254413559535,
   802.5902302175274,
   772.3140648475379,
   839.9018341377154],
  'MAPE': [1.0395487579496652,
   0.5607987708065988,
   0.5438627253681279,
   0.5852285872937784],
  'RMSE': [1148.6549900167981,
   1158.8411708570625,
   1109.6145558003204,
   1223.23337689915],
  'RSquared': [0.5876710102285392,
   0.5803255834810521,
   0.6152231339508221,
   0.5323905190373128]})

","Print the contents of train_metrics, just before line 50;
train_metrics_data = pd.DataFrame(train_metrics)

Then you will see what the dict looked like just before it crashes. I ran a part of the faulty code, and it seems to work just fine.
Using python 3.10.14 :
import pandas as pd

train_metrics = {'model': ['Linear Regression',
   'Random Forest Regressor',
   'Gradient Boost Regression',
   'Extra Tree Regressor'],
  'MAE': [829.1023412412194,
   288.33455697065233,
   712.9637267872279,
   0.0010629575741748962],
  'MAPE': [1.0302372135902111,
   0.20937541440883897,
   0.538244903316323,
   6.306697580961048e-07],
  'RMSE': [1120.5542708017374,
   416.48933196590013,
   1012.399201767692,
   0.05804079289490426],
  'RSquared': [0.5598288286601083,
   0.9391916010838417,
   0.6406981997919169,
   0.9999999988190745]}

train_metrics_data = pd.DataFrame(train_metrics)

print(train_metrics_data)

prints:
                       model         MAE          MAPE         RMSE  RSquared
0          Linear Regression  829.102341  1.030237e+00  1120.554271  0.559829
1    Random Forest Regressor  288.334557  2.093754e-01   416.489332  0.939192
2  Gradient Boost Regression  712.963727  5.382449e-01  1012.399202  0.640698
3       Extra Tree Regressor    0.001063  6.306698e-07     0.058041  1.000000

",data-science
why is it that calling standard sum on a numpy array produces a different result than numpysum,"Observe in the following code, creating an numpy array and calling the builtin python sum function produces different results than numpy.sum
How is numpy's sum function implemented? And why is the result different?
test = [.1]*10
test = [np.float64(x) for x in test]
test[5]= np.float64(-.9)

d = [np.asarray(test) for x in range(0,60000)]
sum(sum(d))

outputs
np.float64(-1.7473212210461497e-08)

but
np.sum(d)

outputs
np.float64(9.987344284922983e-12)

","Numpy uses pairwise summation:https://github.com/numpy/numpy/pull/3685 but python uses reduce summation.
The answer is only partially related to FP inaccuracy because if I have an array of FP numbers and use the same algorithm to sum them, I should expect the same result if I sum them in the same order.
",data-science
r monte carlo binomial gambling simulation,"I am trying to figure out how to simulate a biased gambling problem using monte carlo simulations.
The problem is:
Simulate two players tossing a coin; A and B.
Player A has a 0.55 chance to win. Player B has a 0.45 chance.
Each player starts with $3.
If one player wins they take $1 from the other.
The game ends when either one player has all the money OR 25 iterations have been played.
I then want to plot the relative frequencies of players winning, then run this many time in order to get the estimates for player A and player B winning all the money.
What I am stuck on is getting the montecarlo simulations going and the calculating the probability of one player accumulating all the other player's money.
So far I can generate the data frame for one game and plot it.
Game <- c('Bethany', 'Algernon')   #outcomes in the game

#initialise an empty df
Games_data <- data.frame(Game = numeric(),
                        winner = character(),
                        Bethany_bank = numeric(),
                        Algernon_bank = numeric(),
                        Bethany_Freq = numeric(),
                        Algernon_Freq =  numeric()
)

#intialise variables
count <- 26
i <- 1
temp_Bethany_bank <- 3
temp_Algernon_bank <- 3

#populate the data frame until 25 games or someone wins
while(i < count) {
  temp_game <- i
  temp_winner <- sample(Game, prob =c(0.55, 0.45), size = 1)
  
  if(temp_winner == 'Bethany') {
    temp_Bethany_bank <- temp_Bethany_bank + 1
    temp_Algernon_bank <- temp_Algernon_bank - 1
  } else {
      temp_Bethany_bank <- temp_Bethany_bank - 1
      temp_Algernon_bank <- temp_Algernon_bank + 1}
  
  temp_Bethany_freq = 0.0
  temp_Algernon_freq = 0.0
  
  temp <- data.frame(Game = temp_game,
                     winner = temp_winner,
                     Bethany_bank = temp_Bethany_bank,
                     Algernon_bank = temp_Algernon_bank,
                     Bethany_Freq = temp_Bethany_freq,
                     Algernon_Freq = temp_Algernon_freq
                     )
  
  Games_data <- rbind(Games_data, temp)
  
  Games_data$Bethany_Freq <- cumsum(Games_data$winner == 'Bethany') / 1:nrow(Games_data)
  Games_data$Algernon_Freq <- cumsum(Games_data$winner == 'Algernon') / 1:nrow(Games_data)
  
  if(Games_data$Bethany_bank[i] <= 0 || Games_data$Algernon_bank[i] <= 0) {break} else {i <- i + 1}
}

#show the dataframe and the plot:
Games_data

ggplot(data = Games_data) +
  geom_point(aes(x = Game, y =  Bethany_Freq), color = 'coral', alpha = 0.8) + #Bethany's wins
  geom_point(aes(x = Game, y =  Algernon_Freq), color = 'steelblue', alpha = 0.8) + #Bethany's wins
  geom_line(aes(x = Game, y =  Bethany_Freq), color = 'coral') +
  geom_line(aes(x = Game, y =  Algernon_Freq), color = 'steelblue') +
  theme_classic() + 
  labs(title = ""Relative frequency plots for Bethany vs Algernon over 25 games"") 


How can I run this many times, like 100 or 1000, store the outputs in an object, plot all the trials and then calculate the probability of one player getting all the money?
Cheers in advance!
","Wrap your code in a function, call it using replicate, then plot them all using, say, marrangeGrob from gridExtra.
mc_sim <- function() {
... your code

# Additional code to account for a ""no result"" game.
  if(i==count)
    Games_data <- rbind(Games_data, 
                        data.frame(Game = i,
                                   winner = ""no result"",
                                   Bethany_bank = temp_Bethany_bank,
                                   Algernon_bank = temp_Algernon_bank,
                                   Bethany_Freq = temp_Bethany_freq,
                                   Algernon_Freq = temp_Algernon_freq))

    Games_data
}

Run 9 simulations and save the plots to a list.
sim <- replicate(9, mc_sim(), simplify=FALSE)

sim_plots <- lapply(sim, \(x) ggplot(data = x) +
  geom_point(aes(x = Game, y =  Bethany_Freq), color = 'coral', alpha = 0.8) + #Bethany's wins
  geom_point(aes(x = Game, y =  Algernon_Freq), color = 'steelblue', alpha = 0.8) + #Bethany's wins
  geom_line(aes(x = Game, y =  Bethany_Freq), color = 'coral') +
  geom_line(aes(x = Game, y =  Algernon_Freq), color = 'steelblue') +
  theme_classic() 
)

Plot them
library(gridExtra)
marrangeGrob(sim_plots, nrow=3, ncol=3,
             top=""Relative frequency plots for Bethany vs Algernon over 25 games"")



The probabilities of each player winning can be calcualted using the relative frequencies.
prop.table(table(sapply(sim, \(x) x$winner[nrow(x)])))

 Algernon   Bethany 
0.4444444 0.5555556 

With more simulations, the probabilities converge to about 35% and 62% with 3% being ""no result"".
sim <- replicate(1000, mc_sim(), simplify=FALSE)
prop.table(table(sapply(sim, \(x) x$winner[nrow(x)])))
 #Algernon   Bethany no result 
 #   0.346     0.623     0.031

",data-science
how can i replace null values in polars with a prefix with ascending numbers,"I am trying to replace null values in my dataframe column by a prefix and ascending numbers(to make each unique).ie
df = pl.from_repr(""""""
┌──────────────┬──────────────┐
│ name         ┆ asset_number │
│ ---          ┆ ---          │
│ str          ┆ str          │
╞══════════════╪══════════════╡
│ Office Chair ┆ null         │
│ Office Chair ┆ null         │
│ Office Chair ┆ null         │
│ Office Chair ┆ CMP - 001    │
│ Office Chair ┆ CMP - 005    │
│ Office Chair ┆ null         │
│ Table        ┆ null         │
│ Table        ┆ CMP - 007    │
└──────────────┴──────────────┘
"""""")

the null values should be replaced to something like PREFIX - 001,PREFIX - 002,...
","df = df.with_columns(
    pl.col(""asset_number"").fill_null(
        ""PREFIX - "" + pl.int_range(pl.len()).cast(pl.String)
    )
)

",data-science
how to access a part of an element from a list,"import cv2
import os
import glob
import pandas as pd
from pylibdmtx import pylibdmtx
import xlsxwriter


# co de for scanning

img_dir = ""C:\\images"" # Enter Directory of all images
data_path = os.path.join(img_dir,'*g')
files = glob.glob(data_path)
data = []
result=[]

for f1 in files:
    img = cv2.imread(f1,cv2.IMREAD_UNCHANGED)
    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
    ret, thresh = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY | cv2.THRESH_OTSU)
    msg = pylibdmtx.decode(thresh)
    print(msg)
    data.append(img)
    result.append(msg)

print(type(result[0]))

I have a lsit of 4 list inside a lists names result .The output of above code is result . The code is intended to read the barcode , but it also provides location which is not required by me .
SO after the above code , I have a output named result , whch gives me ::
[[Decoded(data=b'AZ:HP7CXNGSUFEPZCO4GS5RQPY6XY', rect=Rect(left=37, top=152, width=94, height=97))], [Decoded(data=b'AZ:RCHKBW5WGZE98J7E9853OW4ZHE', rect=Rect(left=40, top=125, width=91, height=95))], [Decoded(data=b'AZ:5Z7HME1FRNAZFINDPTDAOTB9GQ', rect=Rect(left=27, top=112, width=88, height=88))]

so NOW i want ot jsut extract or find The az aprt from the all the single lists and export it to excel.
AZ:HP7CXNGSUFEPZCO4GS5RQPY6XY
AZ:RCHKBW5WGZE98J7E9853OW4ZHE
AZ:5Z7HME1FRNAZFINDPTDAOTB9GQ

I want only the above output and omit all the location details .
I have tried with indexing , but IT's saying lists out of range.
Please helpme.
","You need to iterate on the list and retrieve the good properties on each.
values = [[Decoded(data=b'AZ:HP7CXNGSUFEPZCO4GS5RQPY6XY', rect=Rect(left=37, top=152, width=94, height=97))],
          [Decoded(data=b'AZ:9475EFWZCNARPEJEZEMXDFHIBI', rect=Rect(left=32, top=191, width=90, height=88))],
          [Decoded(data=b'AZ:6ECWZUQGEJCR5EZXDH9URCN53M', rect=Rect(left=48, top=183, width=88, height=89))],
          [Decoded(data=b'AZ:XZ9P6KTDGREM5KIXUO9IHCTKAQ', rect=Rect(left=73, top=121, width=91, height=94))]]

datas = [value[0].data for value in values]          # list of encoded string (b'')
datas = [value[0].data.decode() for value in values] # list of strings

",data-science
understanding sklearn39s knnimputer,"I was going through its documentation and it says

Each sample’s missing values are imputed using the mean value from
n_neighbors nearest neighbors found in the training set. Two samples
are close if the features that neither are missing are close.

Now, playing around with a toy dataset, i.e.
>>>X = [[1, 2, nan], [3, 4, 3], [nan, 6, 5], [8, 8, 7]]
>>>X

   [[ 1.,  2., nan],
    [ 3.,  4.,  3.],
    [nan,  6.,  5.],
    [ 8.,  8.,  7.]]

And we make a KNNImputer as follows:
imputer = KNNImputer(n_neighbors=2)

The question is, how does it fill the nans while having nans in 2 of the columns? For example, if it is to fill the nan in the 3rd column of the 1st row, how will it choose which features are the closest since one of the rows has nan in the first column as well? When I do imputer.fit_transform(X) it gives me
array([[1. , 2. , 4. ],
       [3. , 4. , 3. ],
       [5.5, 6. , 5. ],
       [8. , 8. , 7. ]])

which means for filling out the nan in row 1, the nearest neighbors were the second and the third row. How did it calculate the euclidean distance between the first and the third row?
","
How does it fill the NaNs using rows that also have NaNs?

This doesn't seem to be mentioned in the docs. But by digging a bit into the source code, it appears that for each column being imputed, all donors at a smaller distance are considered, even if they have missing values. The way this is handled is by setting to 0 the missing values in a weight matrix, which is obtained according to the used distance, see _get_weights.
The relevant code is in _calc_impute, where after finding a distance matrix for all potential donors, and then the above mentioned matrix of weights, it is imputed as:
# fill nans with zeros
if weight_matrix is not None:
    weight_matrix[np.isnan(weight_matrix)] = 0.0

Where all potential donors are considered if they have at least one non-nan distance with the reciever
dist_pot_donors : ndarray of shape (n_receivers, n_potential_donors)
    Distance matrix between the receivers and potential donors from
    training set. There must be at least one non-nan distance between
    a receiver and a potential donor.

We could check this with a toy example; in the following matrix, when inputting the missing value in [nan,  7.,  4.,  5.], the last row (which also contains two NaNs) is chosen (note that I've set n_neighbors=1). This is because the distance wrt the last row is 0, as the distance corresponding to the NaN values has been set to 0. So by just having a minimal difference with rows 2 and 3, the last row is chosen since it is seen as being equal:
X = np.array([[np.nan,7,4,5],[2,8,4,5],[3,7,4,6],[1,np.nan,np.nan,5]])

print(X)
array([[nan,  7.,  4.,  5.],
       [ 2.,  8.,  4.,  5.],
       [ 3.,  7.,  4.,  6.],
       [ 1., nan, nan,  5.]])

from sklearn.impute import KNNImputer
imputer = KNNImputer(n_neighbors=1)

imputer.fit_transform(X)
array([[1., 7., 4., 5.],
       [2., 8., 4., 5.],
       [3., 7., 4., 6.],
       [1., 7., 4., 5.]])

",data-science
sklearn standardscaler doesn39t allow direct transform we need to fit_transform,"What is the difference between fit_transform and transform?
Why doesn't transform directly works?
from sklearn.preprocessing import StandardScaler

 X_scaler = StandardScaler()
 X_train = X_scaler.fit_transform(X_train)
 X_test = X_scaler.transform(X_test)

If directly transformed it gives the below error

NotFittedError: This StandardScaler instance is not fitted yet. Call
  'fit' with appropriate arguments before using this method.

","StandardScaler, as per documentation:

Standardize features by removing the mean and scaling to unit variance

So it needs to somehow first know about the mean and variance of your data.
So fit() or fit_transform() is needed so that StandardScaler can go through all of your data to find the mean and variance. Those can be accessed 
by attributes:

mean_ : The mean value for each feature (column) in the training set.
var_ : The variance for each feature in the training set. 

Note that those will be calculated separately for each column in the data. 
In transform(), it will just use those mean and variance values to scale the data.
Now you might say that why just it don't calculate those attributes during transform(). This is done so that the test data is scaled in the same way as a training data is scaled (from fit_transform()). If you calculate mean and variance of data in each call to transform(), then all passed data will have different scale, which is not what you want.
This is true for all scikit transformers. 
1) fit() - Will only go through the data and save all needed attributes of data
2) transform() -  Use the saved attributes from fit() to change the data
3) fit_transform() - Utility function to fit() and then transform() the same data.
Usually you would call fit_transform() on training data, and only transform() on test data.
",data-science
sklearn pipelines  gridsearchcv  xgboost  learning curve,"I am new to sklearn & XGBoost.
I would like to use GridSearchCV to tune a XGBoost classifier. One of the checks that I would like to do is the graphical analysis of the loss from train and test. So far I have created the following code:
# Create a new instance of the classifier
xgbr =  xgb.XGBClassifier()
# Create a new pipeline with preprocessing steps and model (imballanced learn)
pipeline  = imb_pipeline([
                          ('preprocess', preprocess), # Encode and transform categorical variables
                          ('re-sample', samplers[0]), # re-samples data to ballanced state
                          ('scale', scalers[0]), # scales the data
                          ('model', xgbr), # models
                          ])

# Create parameter values for gridsearch - carefull, ""model__"" prepended defined in pipeline
params = { 
    'model__max_depth': [3, 4, 5, 6, 8, 10, 12, 15],
    'model__learning_rate': [0.001, 0.01, 0.1, 0.20, 0.25, 0.30],
    ""model__gamma"":[0, 0.25, 0.5, 0.75,1],
    'model__n_estimators': [100, 500, 1000],
    ""model__subsample"":[0.9],
    ""model__colsample_bytree"":[0.5],
    ""model__early_stopping_rounds"": [10], 
    ""model__random_state"": [random_state], 
    ""model__eval_metric"" : [""error""], 
    ""model__eval_set"" : [[(X_train, Y_train), (X_test,Y_test)]]
}

# Use GridSearchCV for all combinations
grid = GridSearchCV(
    estimator = pipeline,
    param_grid = params,
    scoring = 'roc_auc',
    n_jobs = -1,
    cv = 5,
    verbose = 3,
)

# Model fitting
grid.fit(X_train, Y_train)

I have create in params a key-value pair for eval_metric and eval_set:
My question is now, how to access those values and plot a curve of train and test loss (sorry I cannot post a figure here).
Another question: Are the values hand-over by eval_set also piped by the pipeline or do I have to create a separate pipeline for those?
I am using xgb.__version == 0.90, sklearn.__version__ == 1.0.2, python == 3.7.13 @ (google colab)
","I think you are misunderstanding how the grid search is coupled to the cross-validation. Here your training set will be partitioned in 5 (cv = 5) almost even chunks, for each value of the hyperparameter grid it will train on 4 and predict (+ eval) on on the last one, iterating on all possible splits. This will give a cv estimate of the error. But this is all happening inside your training data. After picking the hyperparameters giving the best (=smallest) error, you want to evaluate this model (trained on the entire training set) on the evaluation data (held out and totally new data to the model and hyperparameters. This will give you the a reliable estimate of the generalization error of your model.
Also if you use
    ""model__eval_set"" : [[(X_train, Y_train), (X_test,Y_test)]]

the grid will assume that this is a binary parameter to optimize on, and you do not want that.
All in all you are looking for something of the like:

params = { 
    'model__max_depth': [3, 4, 5, 6, 8, 10, 12, 15],
    'model__learning_rate': [0.001, 0.01, 0.1, 0.20, 0.25, 0.30],
    ""model__gamma"":[0, 0.25, 0.5, 0.75,1],
    'model__n_estimators': [100, 500, 1000],
    ""model__subsample"":[0.9],
    ""model__colsample_bytree"":[0.5],
    ""model__early_stopping_rounds"": [10], 
    ""model__random_state"": [random_state], 
}

grid = GridSearchCV(
    estimator = pipeline,
    param_grid = params,
    scoring = 'roc_auc',
    n_jobs = -1,
    cv = 5,
    verbose = 3,
)

# Model fitting
grid = grid.fit(X_train, Y_train, eval_set=[(X_test, Y_test)])

eval_auc = sklearn.metrics.roc_auc_score( Y_test, grid.best_estimator_.predict_proba(X_test)[:,1])

",data-science
pandas get business monthend dates correctly even for dates already on monthend,"let's say i have and dataframe as below
date,ent_id,val
2021-03-23,101,61
2021-03-12,103,64
2021-03-15,101,32
2021-04-01,103,39
2021-04-02,101,71
2021-04-02,103,79
2021-04-30,101,51
2021-04-30,103,53
2021-05-31,101,28
2021-05-31,103,26
2021-05-31,101,47
2021-05-31,103,61
2021-06-06,101,45
2021-06-06,103,78
2021-06-07,101,23
2021-06-07,103,31
2021-07-31,101,14
2021-07-31,103,02
2021-07-31,101,82
2021-07-31,103,15

i want to create an addition column in dataframe which contain month end date
based of following condition
case 
when DAYNAME('date')='Sunday' then days_add(date,-2)
when DAYNAME('date')='Saturday' then days_add(date,-1)
else date 

so the output would be like this
date,ent_id,val,month_end
2021-03-23,101,61,2021-03-31
2021-03-12,103,64,2021-03-31
2021-03-15,101,32,2021-03-31
2021-04-01,103,39,2021-04-30
2021-04-02,101,71,2021-04-30
2021-04-02,103,79,2021-04-30
2021-04-30,101,51,2021-04-30
2021-04-30,103,53,2021-04-30
2021-05-31,101,28,2021-05-31
2021-05-31,103,26,2021-05-31
2021-05-31,101,47,2021-05-31
2021-05-31,103,61,2021-05-31
2021-06-06,101,45,2021-06-30
2021-06-06,103,78,2021-06-30
2021-06-07,101,23,2021-06-30
2021-06-07,103,31,2021-06-30
2021-07-31,101,14,2021-07-31
2021-07-31,103,02,2021-07-31
2021-07-31,101,82,2021-07-31
2021-07-31,103,15,2021-07-31

my effort
import pandas as pd
from datetime import timedelta
from pandas.tseries.offsets import MonthEnd
import numpy as np

df.loc[(df['date']+MonthEnd(0)).dt.day_name()=='Sunday','month_end'] =[df.loc[(df['date']+MonthEnd(0)).dt.day_name()=='Sunday']['date']+timedelta(days=-2)]
df.loc[(df['date']+MonthEnd(0)).dt.day_name()=='Saturday','month_end'] =[df.loc[(df['date']+MonthEnd(0)).dt.day_name()=='Saturday']['date']+timedelta(days=-1)]

but getting this error
ValueError: Must have equal len keys and value when setting with an ndarray

any other better solution is most welcome
","You can use pd.offsets.MonthEnd(n=0) and then adjust for Sunday and Saturday.  Note that n=0 is needed or else 2021-07-31 will be rolled forward to 2021-08-31.
df['month_end'] = df['date'] + pd.offsets.MonthEnd(n=0)
df.loc[df['month_end'].dt.day_name() == 'Sunday', 'month_end'] -= pd.DateOffset(2)
df.loc[df['month_end'].dt.day_name() == 'Saturday', 'month_end'] -= pd.DateOffset(1)


print(df)

         date  ent_id  val  month_end
0  2021-03-23     101   61 2021-03-31
1  2021-03-12     103   64 2021-03-31
2  2021-03-15     101   32 2021-03-31
3  2021-04-01     103   39 2021-04-30
4  2021-04-02     101   71 2021-04-30
5  2021-04-02     103   79 2021-04-30
6  2021-04-30     101   51 2021-04-30
7  2021-04-30     103   53 2021-04-30
8  2021-05-31     101   28 2021-05-31
9  2021-05-31     103   26 2021-05-31
10 2021-05-31     101   47 2021-05-31
11 2021-05-31     103   61 2021-05-31
12 2021-06-06     101   45 2021-06-30
13 2021-06-06     103   78 2021-06-30
14 2021-06-07     101   23 2021-06-30
15 2021-06-07     103   31 2021-06-30
16 2021-07-31     101   14 2021-07-30
17 2021-07-31     103    2 2021-07-30
18 2021-07-31     101   82 2021-07-30
19 2021-07-31     103   15 2021-07-30

Note that although you seems to want to get the last business date of a month, we cannot simply use pd.offsets.BMonthEnd(n=0) since 2021-07-31 will still be rolled forward to 2021-08-31.
Edit: Solution using BMonthEnd
When I first looked at the question, what immediately popped up to my mind is that probably we can make use of pd.offsets.BMonthEnd to solve the question.
As I also know that for pd.offsets.MonthEnd, we can use parameter n=0 (or simply 0) to ensure the date is not rolled if on an anchor point (on an anchor point if the date is already on month-end date in this case).  Hence, my initial trial is to use pd.offsets.BMonthEnd(n=0).  To my surprise, it doesn't behave similarly as the MonthEnd counterpart.  Date of 2021-07-31 is still rolled forward to 2021-08-31.
As this kind of problem is very common, I would like to provide a workaround to enable us to still use BMonthEnd instead of providing codes to check and modify for Sunday and Saturday.
Here's the workaround code to enable BMonthEnd to behave similarly as MonthEnd(n=0):
df['business_month_end'] = df['date'] + pd.offsets.MonthEnd(0) - pd.offsets.MonthBegin() + pd.offsets.BMonthEnd()

Here, MonthEnd(0) is the same as MonthEnd(n=0) while MonthEnd() and BMonthEnd() without passing parameter is the same as passing n=1 (the default).
The mechanism is that we borrow the feature of MonthEnd(n=0) to keep on anchor even on anchor point and get the month-begin of that date (should be the first date within the same month) and then apply the BMonthEnd function to let it gets us the last business date of the same month (with adjustments if dates fall on Sunday & Saturday).
Edit: Updated Solution using BMonthEnd
Pandas provides a solution using BMonthEnd in the docs.
If you want to get the end of the current business month:
>>> ts = pd.Timestamp(2022, 11, 30)
>>> pd.offsets.BMonthEnd().rollforward(ts)
Timestamp('2022-11-30 00:00:00')

",data-science
getting error 524 while running jupyter lab in google cloud platform,"I am not able to access jupyter lab created on google cloud

I created one notebook using Google AI platform. I was able to start it and work but suddenly it stopped and I am not able to start it now. I tried building and restarting the jupyterlab, but of no use. I have checked my disk usages as well, which is only 12%.
I tried the diagnostic tool, which gave the following result:

but didn't fix it.
Thanks in advance.
","The error might be caused by a corrupted disk partition “/dev/sdb”. After connecting to the notebook instance through SSH, run sudo fsck /dev/sdb to perform a disk check and repair, and then perform a reboot.
In case this does not help, you can download your data after zipping the required content from the “/home/jupyter/” folder and upload it to a new notebook instance.
Below is the zip utility command for your reference.
cd /home/
sudo zip -r test-1.zip jupyter/
",data-science
how can i download a kaggle dataset on my colab notebook,"Can someone help me with downloading Kaggle dataset on my Colab Notebook? I am new to Colab.
","You can use the following method to download any kaggle dataset to your Colab notebook.

First, you have to create an API key in Kaggle.
Go to kaggle.com and open your user settings page.
From the API access section download an API key.
A file called kaggle.json will be downloaded to your computer.
Now open your Colab notebook and copy the following snippet to a cell.
from google.colab import files
files.upload()
After executing this upload the kaggle.json file.
Install the kaggle API using !pip install -q kaggle and move kaggle.json file to ~/.kaggle
!mkdir -p ~/.kaggle
!cp kaggle.json ~/.kaggle/
Now you can download the dataset to your Colab notebook by copying the API command of the dataset that you want to download.
For example:
!kaggle competitions download -c titanic

The dataset should get downloaded to your notebook after this. Happy Coding.
",data-science
what uml diagrams can be used for a data science project,"I am working on a data science project for my 3-2 mini project. My project analyzes the performance of a country in the Olympics based on some attributes. But I am confused about the UML diagrams I should be using in my project.
","There are some 15 UML diagram types out there. A sensible sequence of diagrams to be created depends on your approach. 
If you'd like to create an analysis model that is a conceptual model of your problem domain then a sensible sequence of diagrams might be:

Usecase diagrams
Activity diagrams
Class diagrams

and if your project gets bigger you might need package diagrams.
If you'd like to create a design model that is a conceptual model of your solution domain then a sensible sequence of diagrams might be:
 1. Component diagrams
 2. Class diagrams
 3. Sequence diagrams
 4. Statecharts
In both cases a starting point is having a diagram for your system context. Some people like to mix component and usecase diagram features to denote a system context. 
The aspects you might want to take into concideration of your diagram choices are:

syntax - how strictly would you like to follow the UML standard and what use does adhering to the standard have for you
semantics - what is your need - what do you want to document - and who needs to understand it 
pragmatics - what is the best way to achieve your projects goal e.g. being efficient and effective
tool - what tools do you have at hand and are used and known to your peers - what can you afford to invest in keeping the tool infrastructure up

",data-science
how to optimise hyperparameterss for randomforestclassifier in python for large datasets,"
I'm just working on this problem where I thought RandomForestClassifier from scikit-learn would be better solution for a large dataset. Only after trying with it for this, I found it to be not accurate. The model is either overfitting or underperforming, and sometimes the training time goes on forever.

500000 samples and 50 features. My goal si to classify data into 3 categories.
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.metrics import accuracy_score

# Load dataset
X = ...  # Features
y = ...  # Labels

# Split the dataset
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Initialize the classifier
rf = RandomForestClassifier(random_state=42)

# Define the parameter grid
param_grid = {
    'n_estimators': [100, 200, 300],
    'max_depth': [10, 20, 30],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4],
    'bootstrap': [True, False]
}

# Grid search
grid_search = GridSearchCV(estimator=rf, param_grid=param_grid, cv=3, n_jobs=-1, verbose=2)
grid_search.fit(X_train, y_train)

# Best parameters and model
best_params = grid_search.best_params_
best_rf = grid_search.best_estimator_

# Predictions and accuracy
y_pred = best_rf.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)

print(f""Best Parameters: {best_params}"")
print(f""Accuracy: {accuracy}"")

I tried many methods like manual hyperparameter tuning, grid search for a systematic approach and randomised search giving me inconsistent results.
I want help in improving all these areas. Thanks in advance.
","To optimise for large datasets, you can use techniques like Bayesian optimization or some evolutionary algorithms.
This can help you explore parameter space without exhaustive grid search.
Use just the best frameworks like Dask or Spark to handle such large datasets and accelerate training times efficiently.
",data-science
pandas  typeerror float argument must be a string or a number,"I have a dataframe that contains
user_id    date       browser  conversion  test  sex  age  country
   1    2015-12-03       IE        1         0    M   32.0   US

Here is my code:
from sklearn import tree
data['date'] = pd.to_datetime(data.date)
columns = [c for c in data.columns.tolist() if c not in [""test""]]
clf = tree.DecisionTreeClassifier()
clf = clf.fit(data[columns], data[""test""])

I am getting this error:
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
<ipython-input-560-95a8a54aa939> in <module>()
      4 from sklearn import tree
      5 clf = tree.DecisionTreeClassifier(max_depth=2, min_samples_leaf = (len(data)/100) )
----> 6 clf = clf.fit(data[columns],data[""test""])

C:\Users\SnehaPriya\Anaconda2\lib\site-packages\sklearn\tree\tree.pyc in fit(self, X, y, sample_weight, check_input, X_idx_sorted)
    152         random_state = check_random_state(self.random_state)
    153         if check_input:
--> 154             X = check_array(X, dtype=DTYPE, accept_sparse=""csc"")
    155             if issparse(X):
    156                 X.sort_indices()

C:\Users\SnehaPriya\Anaconda2\lib\site-packages\sklearn\utils\validation.pyc in check_array(array, accept_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)
    371                                       force_all_finite)
    372     else:
--> 373         array = np.array(array, dtype=dtype, order=order, copy=copy)
    374 
    375         if ensure_2d:

TypeError: float() argument must be a string or a number

How do I overcome this error?
","IIUC you need exclude column date also:
columns = [c for c in columns if c not in [""test"", 'date']]

because error:

TypeError: float() argument must be a string or a number, not 'Timestamp'

",data-science
training linear models with mae using sklearn in python,"I'm currently trying to train a linear model using sklearn in python but not with mean squared error (MSE) as error measure - but with mean absolute error (MAE). I specificially need a linear model with MAE as requirement from my professor at university.
I've looked into sklearn.linear_model.LinearRegression which since it is an OLS regressor does not provide alternative error measures. 
Hence, I checked the other available regressors and stumbled upon sklearn.linear_model.HuberRegressor and sklearn.linear_model.SGDRegressor. They both mention MAE as part of their error measures - but do not seem to provide simple MAE. Is there a way to choose the parameters for one of those regressors so that the resulting error measure is a simple MAE? Or is there another regressor in sklearn which I've overlooked? 
Alternatively, is there another (easy to use) python 3.X package which provides what I need?
Thanks for your help!
","In SGD, if you use 'epsilon_insensitive' with epsilon=0 it should work as if you used MAE.
You could also take a look at statsmodels quantile regression (using MAE is also called median regression, and median is a quantile).
",data-science
calculate tstudent test for different standard statistical tables in python,"I need calculate t-value using (alpha)/2 and df (degrees of freedom) for the next table:
https://www.fisterra.com/mbe/investiga/t_student/images/t_stud4.gif
test : t(n-1,alpha/2) and (gl = df)
I want to compute the value for (alpha/2, df) = (0.05/2, 9). Using referenced table I got:
t(0.025,9) = 2.262

What is the right way?
I just found the following code,
import scipy.stats as sts
sts.t.ppf(0.95,9) = 1.8331129

which is for alternating table:
http://image.slidesharecdn.com/tablat-student-120318105418-phpapp02/95/tabla-t-student-1-728.jpg?cb=1332086091
","Try the following
import scipy.stats

def t(alpha, gl):
    return scipy.stats.t.ppf(1-(alpha/2), gl)

You can test it with
print(t(0.05,9))

The result will be 2.262
",data-science
how to plot multiple pandas columns,"I have dataframe total_year, which contains three columns (year, action, comedy).

How can I plot two columns (action and comedy) on y-axis?
My code plots only one:
total_year[-15:].plot(x='year', y='action', figsize=(10,5), grid=True)


","Pandas.DataFrame.plot() per default uses index for plotting X axis, all other numeric columns will be used as Y values.
So setting year column as index will do the trick:
total_year.set_index('year').plot(figsize=(10,5), grid=True)

",data-science
using networkx to create node labels from an array,"I have some nodes that are being generated from a numpy adjacency matrix. I also have an array of labels for said nodes already where node 0 in the matrix corresponds to label 0 in the array. What is the easiest way to apply those labels to the nodes in the graph?
","You can specify the labels while drawing but you must first convert the array into a dictionary where the key value is the node number and the value is the label. After whuch you can specify labels with the label argument like so:
nx.draw(G, labels=labelDict)

",data-science
xarray combine datasets with different dimensions,"I'm trying to merge two Xarray datasets. The resolutions of the datasets are different (one has more points than the other). Ultimately, I need to multiply the values together into one dataset.
I need it to be pretty fast, so nested ""for"" loops through x and y coordinates won't be optimal (I'm working with big datasets). Is there any clean way to do this that I'm not yet aware of? Thanks so much.
","For xarray Datasets named xrds1 and xrds2 covering the same area, but with different resolutions, you can use xr.Dataset.interp_like. Something like:
xrds_merged = xrds2.merge(xrds1.interp_like(xrds2))

If the two datasets have variables with the same name, you may have to rename one first with xr.Dataset.rename_vars() if you want to retain both.
Note that this (interp_like) requires scipy to be installed.
",data-science
sql  expand column of arrays into column of elements,"I have a SQL table, and one column of the table has type text[]. I want to create write a query that will create a new table, which consists of all arrays flattened and concatenated. Ex: If there are 3 items in the table, and the array entry for each of those items is [1, 2, 3], NULL, [1, 4, 5], I want the result set to be [1, 2, 3, 1, 4, 5].
UNNEST seems like it could be useful here. But, I can't find a way to apply the function to every array in the table. Essentially, I want to ""map"" this function over every row in the table. Can anyone point me in a good direction?
CREATE TABLE arrs (
  col1 int,
  col2 text[]
);


INSERT INTO arrs (col1, col2) VALUES (1, '{""a"", ""b"", ""c""}');
INSERT INTO arrs (col1, col2) VALUES (2, '{""d"", ""e""}');

I want the query to return a table with 5 rows with text values ""a"", ""b"", ""c"", ""d"", ""e"" for the above table.
Useful REPL for testing: https://replit.com/languages/sqlite
","Just expand all the arrays in the table (with UNNEST) and put into one common array (with ARRAY_AGG):
with t as (select unnest(col2) as elems
from arrs)
select array_agg(t.elems)
from t;

Here's dbfiddle also
",data-science
svc classifier taking too much time for training,"I am using SVC classifier with Linear kernel to train my model.
Train data: 42000 records
    model = SVC(probability=True)
    model.fit(self.features_train, self.labels_train)
    y_pred = model.predict(self.features_test)
    train_accuracy = model.score(self.features_train,self.labels_train)
    test_accuracy = model.score(self.features_test, self.labels_test)

It takes more than 2 hours to train my model.
Am I doing something wrong? 
Also, what can be done to improve the time
Thanks in advance
","There are several possibilities to speed up your SVM training. Let n be the number of records, and d the embedding dimensionality. I assume you use scikit-learn.

Reducing training set size. Quoting the docs:

The fit time complexity is more than quadratic with the number of samples which makes it hard to scale to dataset with more than a couple of 10000 samples.

O(n^2) complexity will most likely dominate other factors. Sampling fewer records for training will thus have the largest impact on time. Besides random sampling, you could also try instance selection methods. For example, principal sample analysis has been proposed recently.
Reducing dimensionality. As others have hinted at in their comments, embedding dimension also impacts runtime. Computing inner products for the linear kernel is in O(d). Dimensionality reduction can, therefore, also reduce runtime. In another question, latent semantic indexing was suggested specifically for TF-IDF representations.
Parameters. Use SVC(probability=False) unless you need the probabilities, because they ""will slow down that method."" (from the docs).
Implementation. To the best of my knowledge, scikit-learn just wraps around LIBSVM and LIBLINEAR. I am speculating here, but you may be able to speed this up by using efficient BLAS libraries, such as in Intel's MKL.
Different classifier. You may try sklearn.svm.LinearSVC, which is...

[s]imilar to SVC with parameter kernel=’linear’, but implemented in terms of liblinear rather than libsvm, so it has more flexibility in the choice of penalties and loss functions and should scale better to large numbers of samples.

Moreover, a scikit-learn dev suggested the kernel_approximation module in a similar question.

",data-science
creating data frame from text file,"I have a dataset of over 1000 txt files which contains information of books
The Project Gutenberg EBook of Apocolocyntosis, by Lucius Seneca

This eBook is for the use of anyone anywhere at no cost and with
almost no restrictions whatsoever.  You may copy it, give it away or
re-use it under the terms of the Project Gutenberg License included
with this eBook or online at www.gutenberg.org


Title: Apocolocyntosis

Author: Lucius Seneca

Release Date: November 10, 2003 [EBook #10001]
[Date last updated: April 9, 2005]

Language: English

Character set encoding: ASCII

*** START OF THIS PROJECT GUTENBERG EBOOK APOCOLOCYNTOSIS ***


I'm trying to use pandas to read these files and create a data frame from it getting Title, Author, Release Date, and Language as columns and its values but so far I have been having errors
Reading from a single file
df = pd.read_csv('dataset/10001.txt')

Error
ParserError                               Traceback (most recent call last)
Input In [30], in <cell line: 1>()
----> 1 df = pd.read_csv('dataset/10001.txt')

File ~\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\LocalCache\local-packages\Python39\site-packages\pandas\util\_decorators.py:311, in deprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper(*args, **kwargs)
    305 if len(args) > num_allow_args:
    306     warnings.warn(
    307         msg.format(arguments=arguments),
    308         FutureWarning,
    309         stacklevel=stacklevel,
    310     )
--> 311 return func(*args, **kwargs)

File ~\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\LocalCache\local-packages\Python39\site-packages\pandas\io\parsers\readers.py:680, in read_csv(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)
    665 kwds_defaults = _refine_defaults_read(
    666     dialect,
    667     delimiter,
   (...)
    676     defaults={""delimiter"": "",""},
    677 )
    678 kwds.update(kwds_defaults)
--> 680 return _read(filepath_or_buffer, kwds)

File ~\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\LocalCache\local-packages\Python39\site-packages\pandas\io\parsers\readers.py:581, in _read(filepath_or_buffer, kwds)
    578     return parser
    580 with parser:
--> 581     return parser.read(nrows)

File ~\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\LocalCache\local-packages\Python39\site-packages\pandas\io\parsers\readers.py:1254, in TextFileReader.read(self, nrows)
   1252 nrows = validate_integer(""nrows"", nrows)
   1253 try:
-> 1254     index, columns, col_dict = self._engine.read(nrows)
   1255 except Exception:
   1256     self.close()

File ~\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\LocalCache\local-packages\Python39\site-packages\pandas\io\parsers\c_parser_wrapper.py:225, in CParserWrapper.read(self, nrows)
    223 try:
    224     if self.low_memory:
--> 225         chunks = self._reader.read_low_memory(nrows)
    226         # destructive to chunks
    227         data = _concatenate_chunks(chunks)

File ~\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\LocalCache\local-packages\Python39\site-packages\pandas\_libs\parsers.pyx:805, in pandas._libs.parsers.TextReader.read_low_memory()

File ~\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\LocalCache\local-packages\Python39\site-packages\pandas\_libs\parsers.pyx:861, in pandas._libs.parsers.TextReader._read_rows()

File ~\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\LocalCache\local-packages\Python39\site-packages\pandas\_libs\parsers.pyx:847, in pandas._libs.parsers.TextReader._tokenize_rows()

File ~\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\LocalCache\local-packages\Python39\site-packages\pandas\_libs\parsers.pyx:1960, in pandas._libs.parsers.raise_parser_error()

ParserError: Error tokenizing data. C error: Expected 2 fields in line 60, saw 3

","The following code shows how you can tackle the data extraction for one file. Providing they are all in the same format, then this should be pretty efficient.

re.compile : provides the regex to use to find the item of interest
had to do some extra manipulation with release_date because of extra text on that line.
you could add a for-loop to navigate through the 1000s of books.

Code:
import re
import pandas as pd

with open('dataset/10001.txt', 'r') as text_file:
    text = text_file.read()

# These can be reused for each book    
title = re.compile(r'Title: (.*)\n')
author = re.compile(r'Author: (.*)\n')
release_date = re.compile(r'Release Date: (.*)\s')

book_title = title.search(text).group(1)
book_author = author.search(text).group(1)
book_release = release_date.search(text).group(1).split(' [')[0]

df = pd.DataFrame({""Title"": [book_title], ""Author"": [book_author], ""Release_Date"": [book_release]})
print(df)


Output:



data.txt
The Project Gutenberg EBook of Apocolocyntosis, by Lucius Seneca

This eBook is for the use of anyone anywhere at no cost and with
almost no restrictions whatsoever.  You may copy it, give it away or
re-use it under the terms of the Project Gutenberg License included
with this eBook or online at www.gutenberg.org


Title: Apocolocyntosis

Author: Lucius Seneca

Release Date: November 10, 2003 [EBook #10001]
[Date last updated: April 9, 2005]

Language: English

Character set encoding: ASCII

*** START OF THIS PROJECT GUTENBERG EBOOK APOCOLOCYNTOSIS ***

",data-science
r39s browser equivalent in python,"The title says it all.  When you are working R and using RStudio, its really easy and simple to debug something by dropping a browser() call anywhere in your code and seeing what goes wrong.  Is there a way to do that with Python? I'm slowly getting very sick of print statement debugging.
","It looks like you are looking for ipdb
The basic usage is to set:
import ipdb
ipdb.set_trace()

in your code to explore; this will take you right to that part of code, so you can explore all the variables at that point.
For your specific use case: ""Would it be a setting in my Console so that it Opens pdb right before something crashes"" (a comment to another answer), you can use context manager: launch_ipdb_on_exception
For example:
from ipdb import launch_ipdb_on_exception

def silly():
    my_list = [1,2,3]
    for i in xrange(4):
        print my_list[i]

if __name__ == ""__main__"":
    with launch_ipdb_on_exception():
        silly()

Will take you to ipdb session:
      5         for i in xrange(4):
----> 6             print my_list[i]
      7

ipdb> i
3

",data-science
how can i write some code that ask to user to load a file,"I'm building a tool for the company where I work and I've built a program that returns different statistics from any csv file, of course with an specific data structure. Now, my issue is that I don't know how to do is to request the user to upload a file. 
In order to create this program I've been using as practice mode, 
this: df = pd.read_csv('',delimiter=';', encoding='ISO-8859-1')

Any ideas?
","If you need some basic ui that the user can interact so they can select a file, you may want to consider the easygui module.
Or if you don't want to install a new module, just use tkinter built in.
Try this:
from tkinter import *
from tkinter.filedialog import askopenfilename
import pandas as pd

Tk().withdraw()
print(""Please select a csv file to load"") 
file = askopenfilename()
df = pd.read_csv(file, header = 0)

Hope this helps :)) . Check out the easygui module too. The documentation is brief so you can get a hang of it after a few tries.
",data-science
how can i split a dataset from a csv file for training and testing,"I'm using Python and I need to split my .csv imported data in two parts, a training and test set, E.G 70% training and 30% test. 
I keep getting various errors, such as 'list' object is not callable and so on. 
Is there any easy way of doing this?
Thanks
EDIT:
The code is basic, I'm just looking to split the dataset.
from csv import reader
with open('C:/Dataset.csv', 'r') as f:
    data = list(reader(f)) #Imports the CSV
    data[0:1] ( data )

TypeError: 'list' object is not callable
","You can use pandas:
import pandas as pd
import numpy as np

df = pd.read_csv('C:/Dataset.csv')
df['split'] = np.random.randn(df.shape[0], 1)

msk = np.random.rand(len(df)) <= 0.7

train = df[msk]
test = df[~msk]

",data-science
is there any way to work with netcdf files in nim,"I've discovered Nim some days ago and am really interested in it.
However, I need to know if I'd able to work with NetCDF files (or at least GRB files) to see if it is suitable for my every-day life.
I've searched the web for some library or code, but couldn't find any, so if anybody knows a way to do it, please let me know.
","Possible options (as of September 2020) to work with netcdf files using nim:

since netcdf4 is based on hdf5, maybe nimhdf5 could be useful
using nimpy bridge to access python libraries that access netcdf, such as netcdf4-python (your local python environment will be a dependency)
wrap netcdf C api using tools like c2nim or nimterop

",data-science
python file naming convention for data science projects,"What is the correct naming convention for files in a data science and machine learning project?
I believe the file name of the Python classes should be a noun. However, I want to make it clear that whether to name the class as a subject noun or object noun.
Which of these should I use?
1) The class that outputs plots.
visualization.py, visualizer.py, vis.py, or ... 
2) The class that analyses the dataset and outputs files that contains results.
analysis.py, analyzer.py, or ... 
3) The class that coverts the dataset to pickle files.
preprocessor.py, preprocessing.py, prepare.py, or ...
(I had checked PEP8 but couldn't find the clearly naming conversion for the file names)
","here in PEP-8 naming convention section, YOU will find the correct way. 
it's is also discuss in pep-8 that naming convention is ambiguous.
so if you want a correct way ( which another organization follows) then go to GitHub ( tensorflow for example ) see how they maintain there naming convention for maintained project. 
you can follow there structure and start doing the project.
Nothing is fixed. it's all depends on how you want to structure it. Better is it should be, easy to read and maintain. 
",data-science
deciding on how to scale data and which scaler to use,"I am trying to train an MLP model with two Dense layers in keras to do prediction for a small data set of around 100 uni-variate time series. This model should get values of 6 days and predict the 7th day value. As an input to the model, I first concatenate these time series after each other in a dataframe like follow: 
 ts1    val1 
 ts1    val2 
   ...     
 ts1    varN 
 ts2    val1 
 ts2    val2 
   ...     
 ts3    varN 
 ts3    val1 
 ts3    val2 
   ...     
 ts3    varN 
   ...
 ts100  val1 
 ts100  val2 
   ...     
 ts100  varN 

I wonder what is the best way to scale this data? First of all, Should I scale each time series (ts_n) independently, so there will be 100 scalers at the end? Or should I better scale them all together (one acaler at the end) so that I won't lose the correlation between them? Or since all of these time series are considered as the same feature, then there is no point in having correlation?! 
My second question is about which scaling method I should choose? min-max or StandardScaler (from sklearn)? Some time series behave quite different from the others, and they have big variations in their values. If I use min-max scaler, it will ignore these differences, right? So isn't it better to use StandardScaler that (hopefully) considers the differences in score between each time series?
P.S. I should mention that 'after' the scaling is done, I will create timesteps and will have the final results like this:
        timestep1 | timestep2 | timestep3 | timestep4 | timestep5 | timestep6 | timestep7
 ts1      var1    |   var2    |   var3    |   var4    |   var5    |   var6    |   var7    
 ts1      var2    |   var3    |   var4    |   var5    |   var6    |   var7    |   var8    
 ts1      var3    |   var4    |   var5    |   var6    |   var7    |   var8    |   var9
 ...
 ts2      var1    |   var2    |   var3    |   var4    |   var5    |   var6    |   var7    
 ts2      var2    |   var3    |   var4    |   var5    |   var6    |   var7    |   var8    
 ts2      var3    |   var4    |   var5    |   var6    |   var7    |   var8    |   var9
 ...
 ts100      var1    |   var2    |   var3    |   var4    |   var5    |   var6    |   var7    
 ts100      var2    |   var3    |   var4    |   var5    |   var6    |   var7    |   var8    
 ts100      var3    |   var4    |   var5    |   var6    |   var7    |   var8    |   var9
 ...

","In general, I've found very little difference in performance between MinMaxScaler and StandardScaler.  Of course, since (it appears) you'll be scaling your target variable, as well, you should make sure you use a scaler that is consistent with your output activation function.  For example, if you're output activation function is a ReLU, you wont be able to predict any negative values.  In that case, I would lean towards the MinMaxScaler since all of your targets will end up in the interval [0,1]. 
In terms of whether to scale the time series together or independently, it may depend on the specific setting.  If the scales tend to have different time-dependent behaviors, it's likely good to scale them together so the difference is preserved.  If they all behave with a similar pattern, scaling them independently will likely work best.  
It's also worth considering other network architectures for time-series forecasting, e.g. RNNs.
",data-science
line chart with custom confidence interval in altair,"Suppose i have the data frame below:

I checked the documentation but it's only based on a single column. 
Reproducible code:
x = np.random.normal(100,5,100)
data = pd.DataFrame(x)
epsilon = 10
data.columns = ['x']
data['lower'] = x - epsilon
data['upper'] = x + epsilon
data


I'd actually like to use altair, since i like it's interactivity.
","You can layer a line and an area chart, usng the y and y2 encodings to specify the range:
import altair as alt
import pandas as pd
import numpy as np

x = np.random.normal(100,5,100)
epsilon = 10
data = pd.DataFrame({
    'x': x,
    'lower': x - epsilon,
    'upper': x + epsilon
}).reset_index()

line = alt.Chart(data).mark_line().encode(
    x='index',
    y='x'
)

band = alt.Chart(data).mark_area(
    opacity=0.5
).encode(
    x='index',
    y='lower',
    y2='upper'
)

band + line


",data-science
pytorch geometric having issues with tensor sizes,"This is the first time I'm using Pytorch and Pytorch geometric. I'm trying to create a simple Graph Neural Network with Pytorch Geometric. I'm creating a custom dataset by following the Pytorch Geometric documentations and extending the InMemoryDataset. After that I split the dataset into training, validation and test dataset which sizes (3496, 437, 439) respectively. These are the number of graphs in each dataset. Here is my simple Neural Network
class Net(torch.nn.Module):
def __init__(self):
    super(Net, self).__init__()
    self.conv1 = GCNConv(dataset.num_node_features, 10)
    self.conv2 = GCNConv(10, dataset.num_classes)

def forward(self, data):
    x, edge_index, batch = data.x, data.edge_index, data.batch
    x = self.conv1(x, edge_index)
    x = F.relu(x)
    x = F.dropout(x, training=self.training)
    x = self.conv2(x, edge_index)

    return F.log_softmax(x, dim=1)

I get this error while training my model, which suggest that there's some issue with my input dimensions. Maybe the reason is behind my batch sizes?
RuntimeError: The following operation failed in the TorchScript interpreter.
Traceback of TorchScript (most recent call last):
File ""E:\Users\abc\Anaconda3\lib\site-packages\torch_scatter\scatter.py"", line 22, in scatter_add
        size[dim] = int(index.max()) + 1
    out = torch.zeros(size, dtype=src.dtype, device=src.device)
    return out.scatter_add_(dim, index, src)
           ~~~~~~~~~~~~~~~~ <--- HERE
else:
    return out.scatter_add_(dim, index, src)
RuntimeError: index 13654 is out of bounds for dimension 0 with size 678

The error happens specifically on this line of code in the Neural Network,
x = self.conv1(x, edge_index)

EDIT: Added more information about edge_index and explained in more detail about the data that I'm using.
Here are the shapes of the variables that I'm trying to pass
x: torch.Size([678, 43])
edge_index: torch.Size([2, 668])
torch.max(edge_index): tensor(541690)
torch.min(edge_index): tensor(1920)

I'm using a datalist which contains Data(x=node_features, edge_index=edge_index, y=labels) objects. When I'm splitting the dataset into training, validation and test datasets, I get (3496, 437, 439) graphs in each dataset respectively. Originally I tried to create one single graph from my dataset, but I'm not sure how it would work with Dataloader and minibatches.
train_loader = DataLoader(train_dataset, batch_size=batch_size)
val_loader = DataLoader(val_dataset, batch_size=batch_size)
test_loader = DataLoader(test_dataset, batch_size=batch_size)

Here's the code that generates the graph from dataframe. I've tried to create an simple graph where there are just some amount of vertices with some amount of edges connecting them. I've probably overlooked something and that's why I have this issue. I've tried to follow the Pytorch geometric documentation when creating this graph (Pytorch Geometric: Creating your own dataset)
def process(self):
        data_list = []

        grouped = df.groupby('EntityId')
        for id, group in grouped:
            node_features = torch.tensor(group.drop(['Labels'], axis=1).values)
            source_nodes = group.index[1:].values
            target_nodes = group.index[:-1].values
            labels = torch.tensor(group.Labels.values)
            edge_index = torch.tensor([source_nodes, target_nodes])

            data = Data(x=node_features, edge_index=edge_index, y=labels)
            data_list.append(data)

        if self.pre_filter is not None:
            data_list = [data for data in data_list if self.pre_filter(data)]

        if self.pre_transform is not None:
            data_list = [self.pre_transform(data) for data in data_list]

        data, slices = self.collate(data_list)
        torch.save((data, slices), self.processed_paths[0])

If someone could help me with the process of creating a graph on any kind of data and using it with GCNConv, I would appreciate it.
","I agree with @trialNerror -- it is a data problem. Your edge_index should refer to the data nodes and its max should not be that high. Since you don't want to show us the data and ask for ""creating a graph on any kind of data "", here it is.
I mostly left your Net unchanged. You can play around with the constants stated to match with your data.
import torch
import torch.nn as nn
import torch.nn.functional as F

from torch_geometric.nn import GCNConv
from torch_geometric.data import Data

num_node_features = 100
num_classes = 2
num_nodes = 678
num_edges = 1500
num_hidden_nodes = 128

x = torch.randn((num_nodes, num_node_features), dtype=torch.float32)
edge_index = torch.randint(low=0, high=num_nodes, size=(2, num_edges), dtype=torch.long)
y = torch.randint(low=0, high=num_classes, size=(num_nodes,), dtype=torch.long)

class Net(torch.nn.Module):
    def __init__(self):
        super(Net, self).__init__()
        self.conv1 = GCNConv(num_node_features, num_hidden_nodes)
        self.conv2 = GCNConv(num_hidden_nodes, num_classes)

    def forward(self, data):
        x, edge_index = data.x, data.edge_index
        x = self.conv1(x, edge_index)
        x = F.relu(x)
        x = F.dropout(x, training=self.training)
        x = self.conv2(x, edge_index)

        return F.log_softmax(x, dim=1)

data = Data(x=x, edge_index=edge_index, y=y)

net = Net()

optimizer = torch.optim.Adam(net.parameters(), lr=1e-2)
for i in range(1000):
    output = net(data)
    loss = F.cross_entropy(output, data.y)
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()

    if i % 100 == 0:
        print('Accuracy: ', (torch.argmax(output, dim=1)==data.y).float().mean())

Output
Accuracy:  tensor(0.5059)
Accuracy:  tensor(0.8702)
Accuracy:  tensor(0.9159)
Accuracy:  tensor(0.9233)
Accuracy:  tensor(0.9336)
Accuracy:  tensor(0.9484)
Accuracy:  tensor(0.9602)
Accuracy:  tensor(0.9676)
Accuracy:  tensor(0.9705)
Accuracy:  tensor(0.9749)

(yes we can overfit to random data)
",data-science
kaggle dataset  letter and numbers meaning,"I have come across some data on Kaggle. https://www.itl.nist.gov/div898/strd/nls/data/LINKS/DATA/MGH09.dat
I am trying to insert this to excel by copy and pasting it. What do the letters and numbers mean? How would this data look once it was converted?
","The number 0.00256 is expressed as 2.56E-03 in scientific notation .
What this means is that the number before E (2.56 in this case) is multiplied by 10 raised to the power of -3 , which is 2.56*10^(-3) .
Similarly the number 744.67 is expressed as 7.4467E+02.
For example , your first y value (1.975000E-01) would be 0.1975 since it is 1.975 * 10^(-1) .
Your first x value (4.0E+00) would simply be 4 since it is 4.0*10^(0).
",data-science
plot scikitlearn sklearn svm decision boundary  surface,"I am currently performing multi class SVM with linear kernel using python's scikit library. 
The sample training data and testing data are as given below:
Model data:
x = [[20,32,45,33,32,44,0],[23,32,45,12,32,66,11],[16,32,45,12,32,44,23],[120,2,55,62,82,14,81],[30,222,115,12,42,64,91],[220,12,55,222,82,14,181],[30,222,315,12,222,64,111]]
y = [0,0,0,1,1,2,2]

I want to plot the decision boundary and visualize the datasets. Can someone please help to plot this type of data.
The data given above is just mock data so feel free to change the values.
It would be helpful if at least if you could suggest the steps that are to followed. 
Thanks in advance
","You have to choose only 2 features to do this. The reason is that you cannot plot a 7D plot. After selecting the 2 features use only these for the visualization of the decision surface.
(I have also written an article about this here: https://towardsdatascience.com/support-vector-machines-svm-clearly-explained-a-python-tutorial-for-classification-problems-29c539f3ad8?source=friends_link&sk=80f72ab272550d76a0cc3730d7c8af35)

Now, the next question that you would ask: How can I choose these 2 features?. Well, there are a lot of ways. You could do a univariate F-value (feature ranking) test and see what features/variables are the most important. Then you could use these for the plot. Also, we could reduce the dimensionality from 7 to 2 using PCA for example.

2D plot for 2 features and using the iris dataset
from sklearn.svm import SVC
import numpy as np
import matplotlib.pyplot as plt
from sklearn import svm, datasets

iris = datasets.load_iris()
# Select 2 features / variable for the 2D plot that we are going to create.
X = iris.data[:, :2]  # we only take the first two features.
y = iris.target

def make_meshgrid(x, y, h=.02):
    x_min, x_max = x.min() - 1, x.max() + 1
    y_min, y_max = y.min() - 1, y.max() + 1
    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))
    return xx, yy

def plot_contours(ax, clf, xx, yy, **params):
    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])
    Z = Z.reshape(xx.shape)
    out = ax.contourf(xx, yy, Z, **params)
    return out

model = svm.SVC(kernel='linear')
clf = model.fit(X, y)

fig, ax = plt.subplots()
# title for the plots
title = ('Decision surface of linear SVC ')
# Set-up grid for plotting.
X0, X1 = X[:, 0], X[:, 1]
xx, yy = make_meshgrid(X0, X1)

plot_contours(ax, clf, xx, yy, cmap=plt.cm.coolwarm, alpha=0.8)
ax.scatter(X0, X1, c=y, cmap=plt.cm.coolwarm, s=20, edgecolors='k')
ax.set_ylabel('y label here')
ax.set_xlabel('x label here')
ax.set_xticks(())
ax.set_yticks(())
ax.set_title(title)
ax.legend()
plt.show()



EDIT: Apply PCA to reduce dimensionality.
from sklearn.svm import SVC
import numpy as np
import matplotlib.pyplot as plt
from sklearn import svm, datasets
from sklearn.decomposition import PCA

iris = datasets.load_iris()

X = iris.data  
y = iris.target

pca = PCA(n_components=2)
Xreduced = pca.fit_transform(X)

def make_meshgrid(x, y, h=.02):
    x_min, x_max = x.min() - 1, x.max() + 1
    y_min, y_max = y.min() - 1, y.max() + 1
    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))
    return xx, yy

def plot_contours(ax, clf, xx, yy, **params):
    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])
    Z = Z.reshape(xx.shape)
    out = ax.contourf(xx, yy, Z, **params)
    return out

model = svm.SVC(kernel='linear')
clf = model.fit(Xreduced, y)

fig, ax = plt.subplots()
# title for the plots
title = ('Decision surface of linear SVC ')
# Set-up grid for plotting.
X0, X1 = Xreduced[:, 0], Xreduced[:, 1]
xx, yy = make_meshgrid(X0, X1)

plot_contours(ax, clf, xx, yy, cmap=plt.cm.coolwarm, alpha=0.8)
ax.scatter(X0, X1, c=y, cmap=plt.cm.coolwarm, s=20, edgecolors='k')
ax.set_ylabel('PC2')
ax.set_xlabel('PC1')
ax.set_xticks(())
ax.set_yticks(())
ax.set_title('Decison surface using the PCA transformed/projected features')
ax.legend()
plt.show()



EDIT 1 (April 15th, 2020):
Case: 3D plot for 3 features and using the iris dataset
from sklearn.svm import SVC
import numpy as np
import matplotlib.pyplot as plt
from sklearn import svm, datasets
from mpl_toolkits.mplot3d import Axes3D

iris = datasets.load_iris()
X = iris.data[:, :3]  # we only take the first three features.
Y = iris.target

#make it binary classification problem
X = X[np.logical_or(Y==0,Y==1)]
Y = Y[np.logical_or(Y==0,Y==1)]

model = svm.SVC(kernel='linear')
clf = model.fit(X, Y)

# The equation of the separating plane is given by all x so that np.dot(svc.coef_[0], x) + b = 0.
# Solve for w3 (z)
z = lambda x,y: (-clf.intercept_[0]-clf.coef_[0][0]*x -clf.coef_[0][1]*y) / clf.coef_[0][2]

tmp = np.linspace(-5,5,30)
x,y = np.meshgrid(tmp,tmp)

fig = plt.figure()
ax  = fig.add_subplot(111, projection='3d')
ax.plot3D(X[Y==0,0], X[Y==0,1], X[Y==0,2],'ob')
ax.plot3D(X[Y==1,0], X[Y==1,1], X[Y==1,2],'sr')
ax.plot_surface(x, y, z(x,y))
ax.view_init(30, 60)
plt.show()


",data-science
mat1 and mat2 must have the same dtype,"I'm trying to build a neural network to predict per-capita-income for counties in US based on the education level of their citizens.
X and y have the same dtype (I have checked this) but I'm getting an error.
Here is my data:
   county_FIPS state          county  per_capita_personal_income_2019  \
0        51013    VA   Arlington, VA                            97629   

   per_capita_personal_income_2020  per_capita_personal_income_2021  \
0                           100687                           107603    

   associate_degree_numbers_2016_2020  bachelor_degree_numbers_2016_2020  \
0                               19573                             132394   
 

And here is my network
import torch
import pandas as pd
df = pd.read_csv(""./input/US counties - education vs per capita personal income - results-20221227-213216.csv"")
X = torch.tensor(df[[""bachelor_degree_numbers_2016_2020"", ""associate_degree_numbers_2016_2020""]].values)
y = torch.tensor(df[""per_capita_personal_income_2020""].values)

X.dtype
torch.int64

y.dtype
torch.int64

import torch.nn as nn
class BaseNet(nn.Module):
    def __init__(self, in_dim, hidden_dim, out_dim):
        super(BaseNet, self).__init__()
        self.classifier = nn.Sequential(
        nn.Linear(in_dim, hidden_dim, bias=True), 
        nn.ReLU(), 
        nn.Linear(feature_dim, out_dim, bias=True))
        
    def forward(self, x): 
        return self.classifier(x)

from torch import optim
import matplotlib.pyplot as plt
in_dim, hidden_dim, out_dim = 2, 20, 1
lr = 1e-3
epochs = 40
loss_fn = nn.CrossEntropyLoss()
classifier = BaseNet(in_dim, hidden_dim, out_dim)
optimizer = optim.SGD(classifier.parameters(), lr=lr)

def train(classifier, optimizer, epochs, loss_fn):
    classifier.train()
    losses = []
    for epoch in range(epochs):
        out = classifier(X)
        loss = loss_fn(out, y)
        loss.backward()
        optimizer.step()
        optimizer.zero_grad()
        losses.append(loss/len(X))
        print(""Epoch {} train loss: {}"".format(epoch+1, loss/len(X)))
    
    plt.plot([i for i in range(1, epochs + 1)])
    plt.xlabel(""Epoch"")
    plt.ylabel(""Training Loss"")
    plt.show()

train(classifier, optimizer, epochs, loss_fn)

Here is the full stack trace of the error that I am getting when I try to train the network:
---------------------------------------------------------------------------
RuntimeError                              Traceback (most recent call last)
Input In [77], in <cell line: 39>()
     36     plt.ylabel(""Training Loss"")
     37     plt.show()
---> 39 train(classifier, optimizer, epochs, loss_fn)

Input In [77], in train(classifier, optimizer, epochs, loss_fn)
     24 losses = []
     25 for epoch in range(epochs):
---> 26     out = classifier(X)
     27     loss = loss_fn(out, y)
     28     loss.backward()

File ~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:1194, in Module._call_impl(self, *input, **kwargs)
   1190 # If we don't have any hooks, we want to skip the rest of the logic in
   1191 # this function, and just call forward.
   1192 if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks
   1193         or _global_forward_hooks or _global_forward_pre_hooks):
-> 1194     return forward_call(*input, **kwargs)
   1195 # Do not call functions when jit is used
   1196 full_backward_hooks, non_full_backward_hooks = [], []

Input In [77], in BaseNet.forward(self, x)
     10 def forward(self, x): 
---> 11     return self.classifier(x)

File ~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:1194, in Module._call_impl(self, *input, **kwargs)
   1190 # If we don't have any hooks, we want to skip the rest of the logic in
   1191 # this function, and just call forward.
   1192 if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks
   1193         or _global_forward_hooks or _global_forward_pre_hooks):
-> 1194     return forward_call(*input, **kwargs)
   1195 # Do not call functions when jit is used
   1196 full_backward_hooks, non_full_backward_hooks = [], []

File ~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/container.py:204, in Sequential.forward(self, input)
    202 def forward(self, input):
    203     for module in self:
--> 204         input = module(input)
    205     return input

File ~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:1194, in Module._call_impl(self, *input, **kwargs)
   1190 # If we don't have any hooks, we want to skip the rest of the logic in
   1191 # this function, and just call forward.
   1192 if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks
   1193         or _global_forward_hooks or _global_forward_pre_hooks):
-> 1194     return forward_call(*input, **kwargs)
   1195 # Do not call functions when jit is used
   1196 full_backward_hooks, non_full_backward_hooks = [], []

File ~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/linear.py:114, in Linear.forward(self, input)
    113 def forward(self, input: Tensor) -> Tensor:
--> 114     return F.linear(input, self.weight, self.bias)

RuntimeError: mat1 and mat2 must have the same dtype

Updates
I have tried casting X and y to float tensors but this comes up with the following error: expected scalar type Long but found Float. If someone who knows PyTorch could try running this notebook for themselves that would be a great help. I'm struggling to get off the ground with Kaggle and ML.
","The reason for this is because the parameters dtype of nn.Linear doesn't match your input's dtype; the default dtype for nn.Linear is torch.float32 which is in your case different from your input data - float64.
The solution to this question solves your problem and explains why @Anonymous answer works.
In short, add self.double() at the end of your constructor and things should run.
",data-science
inverse of numpygradient function,"I need to create a function which would be the inverse of the np.gradient function.
Where the Vx,Vy arrays (Velocity component vectors) are the input and the output would be an array of anti-derivatives (Arrival Time) at the datapoints x,y.
I have data on a (x,y) grid with scalar values (time) at each point.
I have used the numpy gradient function and linear interpolation to determine the gradient vector Velocity (Vx,Vy) at each point (See below).
I have achieved this by:
 #LinearTriInterpolator applied to a delaunay triangular mesh
 LTI= LinearTriInterpolator(masked_triang, time_array)

 #Gradient requested at the mesh nodes:
 (Vx, Vy) = LTI.gradient(triang.x, triang.y)

The first image below shows the velocity vectors at each point, and the point labels represent the time value which formed the derivatives (Vx,Vy)

The next image shows the resultant scalar value of the derivatives (Vx,Vy) plotted as a colored contour graph with associated node labels.

So my challenge is:
I need to reverse the process!
Using the gradient vectors (Vx,Vy) or the resultant scalar value to determine the original Time-Value at that point.
Is this possible?
Knowing that the numpy.gradient function is computed using second order accurate central differences in the interior points and either first or second order accurate one-sides (forward or backwards) differences at the boundaries, I am sure there is a function which would reverse this process.
I was thinking that taking a line derivative between the original point (t=0 at x1,y1) to any point (xi,yi) over the Vx,Vy plane would give me the sum of the velocity components. I could then divide this value by the distance between the two points to get the time taken..
Would this approach work? And if so, which numpy integrate function would be best applied?

An example of my data can be found here [http://www.filedropper.com/calculatearrivaltimefromgradientvalues060820]
Your help would be greatly appreciated
EDIT:
Maybe this simplified drawing might help understand where I'm trying to get to..

EDIT:
Thanks to @Aguy who has contibuted to this code.. I Have tried to get a more accurate representation using a meshgrid of spacing 0.5 x 0.5m and calculating the gradient at each meshpoint, however I am not able to integrate it properly. I also have some edge affects which are affecting the results that I don't know how to correct.
import numpy as np
from scipy import interpolate
from matplotlib import pyplot
from mpl_toolkits.mplot3d import Axes3D

#Createmesh grid with a spacing of 0.5 x 0.5
stepx = 0.5
stepy = 0.5
xx = np.arange(min(x), max(x), stepx)
yy = np.arange(min(y), max(y), stepy)
xgrid, ygrid = np.meshgrid(xx, yy)
grid_z1 = interpolate.griddata((x,y), Arrival_Time, (xgrid, ygrid), method='linear') #Interpolating the Time values

#Formatdata
X = np.ravel(xgrid)
Y= np.ravel(ygrid)
zs = np.ravel(grid_z1)
Z = zs.reshape(X.shape)

#Calculate Gradient
(dx,dy) = np.gradient(grid_z1) #Find gradient for points on meshgrid

Velocity_dx= dx/stepx #velocity ms/m
Velocity_dy= dy/stepx #velocity ms/m

Resultant = (Velocity_dx**2 + Velocity_dy**2)**0.5 #Resultant scalar value ms/m

Resultant = np.ravel(Resultant)

#Plot Original Data F(X,Y) on the meshgrid
fig = pyplot.figure()
ax = fig.add_subplot(projection='3d')
ax.scatter(x,y,Arrival_Time,color='r')
ax.plot_trisurf(X, Y, Z)
ax.set_xlabel('X-Coordinates')
ax.set_ylabel('Y-Coordinates')
ax.set_zlabel('Time (ms)')
pyplot.show()

#Plot the Derivative of f'(X,Y) on the meshgrid
fig = pyplot.figure()
ax = fig.add_subplot(projection='3d')
ax.scatter(X,Y,Resultant,color='r',s=0.2)
ax.plot_trisurf(X, Y, Resultant)
ax.set_xlabel('X-Coordinates')
ax.set_ylabel('Y-Coordinates')
ax.set_zlabel('Velocity (ms/m)')
pyplot.show()

#Integrate to compare the original data input
dxintegral = np.nancumsum(Velocity_dx, axis=1)*stepx
dyintegral = np.nancumsum(Velocity_dy, axis=0)*stepy

valintegral = np.ma.zeros(dxintegral.shape)
for i in range(len(yy)):
    for j in range(len(xx)):
        valintegral[i, j] = np.ma.sum([dxintegral[0, len(xx) // 2], 
    dyintegral[i, len(yy)  // 2], dxintegral[i, j], - dxintegral[i, len(xx) // 2]])
valintegral = valintegral * np.isfinite(dxintegral)


Now the np.gradient is applied at every meshnode (dx,dy) = np.gradient(grid_z1)

Now in my process I would analyse the gradient values above and make some adjustments (There is some unsual edge effects that are being create which I need to rectify) and would then integrate the values to get back to a surface which would be very similar to f(x,y) shown above.
I need some help adjusting the integration function:
#Integrate to compare the original data input
dxintegral = np.nancumsum(Velocity_dx, axis=1)*stepx
dyintegral = np.nancumsum(Velocity_dy, axis=0)*stepy

valintegral = np.ma.zeros(dxintegral.shape)
for i in range(len(yy)):
    for j in range(len(xx)):
        valintegral[i, j] = np.ma.sum([dxintegral[0, len(xx) // 2], 
    dyintegral[i, len(yy)  // 2], dxintegral[i, j], - dxintegral[i, len(xx) // 2]])
valintegral = valintegral * np.isfinite(dxintegral)

And now I need to calculate the new 'Time' values at the original (x,y) point locations.
UPDATE (08-09-20) : I am getting some promising results using the help from @Aguy. The results can be seen below (with the blue contours representing the original data, and the red contours representing the integrated values).
I am still working on an integration approach which can remove the inaccuarcies at the areas of min(y) and max(y)

from matplotlib.tri import (Triangulation, UniformTriRefiner, 
CubicTriInterpolator,LinearTriInterpolator,TriInterpolator,TriAnalyzer)
import pandas as pd
from scipy.interpolate import griddata
import matplotlib.pyplot as plt
import numpy as np
from scipy import interpolate

#-------------------------------------------------------------------------
# STEP 1: Import data from Excel file, and set variables
#-------------------------------------------------------------------------
df_initial = pd.read_excel(
r'C:\Users\morga\PycharmProjects\venv\Development\Trial'
r'.xlsx')

Inputdata can be found here link
df_initial = df_initial .sort_values(by='Delay', ascending=True) #Update dataframe and sort by Delay
x = df_initial ['X'].to_numpy() 
y = df_initial ['Y'].to_numpy() 
Arrival_Time = df_initial ['Delay'].to_numpy() 

# Createmesh grid with a spacing of 0.5 x 0.5
stepx = 0.5
stepy = 0.5
xx = np.arange(min(x), max(x), stepx)
yy = np.arange(min(y), max(y), stepy)
xgrid, ygrid = np.meshgrid(xx, yy)
grid_z1 = interpolate.griddata((x, y), Arrival_Time, (xgrid, ygrid), method='linear')  # Interpolating the Time values

# Calculate Gradient (velocity ms/m)
(dy, dx) = np.gradient(grid_z1)  # Find gradient for points on meshgrid


Velocity_dx = dx / stepx  # x velocity component ms/m
Velocity_dy = dy / stepx  # y velocity component ms/m

# Integrate to compare the original data input
dxintegral = np.nancumsum(Velocity_dx, axis=1) * stepx
dyintegral = np.nancumsum(Velocity_dy, axis=0) * stepy

valintegral = np.ma.zeros(dxintegral.shape)  # Makes an array filled with 0's the same shape as dx integral
for i in range(len(yy)):
    for j in range(len(xx)):
        valintegral[i, j] = np.ma.sum(
        [dxintegral[0, len(xx) // 2], dyintegral[i, len(xx) // 2], dxintegral[i, j], - dxintegral[i, len(xx) // 2]])
valintegral[np.isnan(dx)] = np.nan
min_value = np.nanmin(valintegral)

valintegral = valintegral + (min_value * -1)

##Plot Results

fig = plt.figure()
ax = fig.add_subplot()
ax.scatter(x, y, color='black', s=7, zorder=3)
ax.set_xlabel('X-Coordinates')
ax.set_ylabel('Y-Coordinates')
ax.contour(xgrid, ygrid, valintegral, levels=50, colors='red', zorder=2)
ax.contour(xgrid, ygrid, grid_z1, levels=50, colors='blue', zorder=1)
ax.set_aspect('equal')
plt.show()

","TL;DR;
You have multiple challenges to address in this issue, mainly:

Potential reconstruction (scalar field) from its gradient (vector field)

But also:

Observation in a concave hull with non rectangular grid;
Numerical 2D line integration and numerical inaccuracy;

It seems it can be solved by choosing an adhoc interpolant and a smart way to integrate (as pointed out by @Aguy).
MCVE
In a first time, let's build a MCVE to highlight above mentioned key points.
Dataset
We recreate a scalar field and its gradient.
import numpy as np
from scipy import interpolate
import matplotlib.pyplot as plt

def f(x, y):
    return x**2 + x*y + 2*y + 1

Nx, Ny = 21, 17
xl = np.linspace(-3, 3, Nx)
yl = np.linspace(-2, 2, Ny)

X, Y = np.meshgrid(xl, yl)
Z = f(X, Y)
zl = np.arange(np.floor(Z.min()), np.ceil(Z.max())+1, 2)

dZdy, dZdx = np.gradient(Z, yl, xl, edge_order=1)
V = np.hypot(dZdx, dZdy)

The scalar field looks like:
axe = plt.axes(projection='3d')
axe.plot_surface(X, Y, Z, cmap='jet', alpha=0.5)
axe.view_init(elev=25, azim=-45)


And, the vector field looks like:
axe = plt.contour(X, Y, Z, zl, cmap='jet')
axe.axes.quiver(X, Y, dZdx, dZdy, V, units='x', pivot='tip', cmap='jet')
axe.axes.set_aspect('equal')
axe.axes.grid()


Indeed gradient is normal to potential levels. We also plot the gradient magnitude:
axe = plt.contour(X, Y, V, 10, cmap='jet')
axe.axes.set_aspect('equal')
axe.axes.grid()


Raw field reconstruction
If we naively reconstruct the scalar field from the gradient:
SdZx = np.cumsum(dZdx, axis=1)*np.diff(xl)[0]
SdZy = np.cumsum(dZdy, axis=0)*np.diff(yl)[0]

Zhat = np.zeros(SdZx.shape)
for i in range(Zhat.shape[0]):
    for j in range(Zhat.shape[1]):
        Zhat[i,j] += np.sum([SdZy[i,0], -SdZy[0,0], SdZx[i,j], -SdZx[i,0]])
        
Zhat += Z[0,0] - Zhat[0,0]

We can see the global result is roughly correct, but levels are less accurate where the gradient magnitude is low:

Interpolated field reconstruction
If we increase the grid resolution and pick a specific interpolant (usual when dealing with mesh grid), we can get a finer field reconstruction:
r = np.stack([X.ravel(), Y.ravel()]).T
Sx = interpolate.CloughTocher2DInterpolator(r, dZdx.ravel())
Sy = interpolate.CloughTocher2DInterpolator(r, dZdy.ravel())

Nx, Ny = 200, 200
xli = np.linspace(xl.min(), xl.max(), Nx)
yli = np.linspace(yl.min(), yl.max(), Nx)
Xi, Yi = np.meshgrid(xli, yli)
ri = np.stack([Xi.ravel(), Yi.ravel()]).T

dZdxi = Sx(ri).reshape(Xi.shape)
dZdyi = Sy(ri).reshape(Xi.shape)

SdZxi = np.cumsum(dZdxi, axis=1)*np.diff(xli)[0]
SdZyi = np.cumsum(dZdyi, axis=0)*np.diff(yli)[0]

Zhati = np.zeros(SdZxi.shape)
for i in range(Zhati.shape[0]):
    for j in range(Zhati.shape[1]):
        Zhati[i,j] += np.sum([SdZyi[i,0], -SdZyi[0,0], SdZxi[i,j], -SdZxi[i,0]])
        
Zhati += Z[0,0] - Zhati[0,0]

Which definitely performs way better:

So basically, increasing the grid resolution with an adhoc interpolant may help you to get more accurate result. The interpolant also solve the need to get a regular rectangular grid from a triangular mesh to perform integration.
Concave and convex hull
You also have pointed out inaccuracy on the edges. Those are the result of the combination of the interpolant choice and the integration methodology. The integration methodology fails to properly compute the scalar field when it reach concave region with few interpolated points. The problem disappear when choosing a mesh-free interpolant able to extrapolate.
To illustrate it, let's remove some data from our MCVE:
q = np.full(dZdx.shape, False)
q[0:6,5:11] = True
q[-6:,-6:] = True
dZdx[q] = np.nan
dZdy[q] = np.nan

Then the interpolant can be constructed as follow:
q2 = ~np.isnan(dZdx.ravel())
r = np.stack([X.ravel(), Y.ravel()]).T[q2,:]
Sx = interpolate.CloughTocher2DInterpolator(r, dZdx.ravel()[q2])
Sy = interpolate.CloughTocher2DInterpolator(r, dZdy.ravel()[q2])

Performing the integration we see that in addition of classical edge effect we do have less accurate value in concave regions (swingy dot-dash lines where the hull is concave) and we have no data outside the convex hull as Clough Tocher is a mesh-based interpolant:
Vl = np.arange(0, 11, 1)
axe = plt.contour(X, Y, np.hypot(dZdx, dZdy), Vl, cmap='jet')
axe.axes.contour(Xi, Yi, np.hypot(dZdxi, dZdyi), Vl, cmap='jet', linestyles='-.')
axe.axes.set_aspect('equal')
axe.axes.grid()


So basically the error we are seeing on the corner are most likely due to integration issue combined with interpolation limited to the convex hull.
To overcome this we can choose a different interpolant such as RBF (Radial Basis Function Kernel) which is able to create data outside the convex hull:
Sx = interpolate.Rbf(r[:,0], r[:,1], dZdx.ravel()[q2], function='thin_plate')
Sy = interpolate.Rbf(r[:,0], r[:,1], dZdy.ravel()[q2], function='thin_plate')

dZdxi = Sx(ri[:,0], ri[:,1]).reshape(Xi.shape)
dZdyi = Sy(ri[:,0], ri[:,1]).reshape(Xi.shape)

Notice the slightly different interface of this interpolator (mind how parmaters are passed).
The result is the following:

We can see the region outside the convex hull can be extrapolated (RBF are mesh free). So choosing the adhoc interpolant is definitely a key point to solve your problem. But we still need to be aware that extrapolation may perform well but is somehow meaningless and dangerous.
Solving your problem
The answer provided by @Aguy is perfectly fine as it setups a clever way to integrate that is not disturbed by missing points outside the convex hull. But as you mentioned there is inaccuracy in concave region inside the convex hull.
If you wish to remove the edge effect you detected, you will have to resort to an interpolant able to extrapolate as well, or find another way to integrate.
Interpolant change
Using RBF interpolant seems to solve your problem. Here is the complete code:
df = pd.read_excel('./Trial-Wireup 2.xlsx')
x = df['X'].to_numpy()
y = df['Y'].to_numpy()
z = df['Delay'].to_numpy()

r = np.stack([x, y]).T

#S = interpolate.CloughTocher2DInterpolator(r, z)
#S = interpolate.LinearNDInterpolator(r, z)
S = interpolate.Rbf(x, y, z, epsilon=0.1, function='thin_plate')

N = 200
xl = np.linspace(x.min(), x.max(), N)
yl = np.linspace(y.min(), y.max(), N)
X, Y = np.meshgrid(xl, yl)

#Zp = S(np.stack([X.ravel(), Y.ravel()]).T)
Zp = S(X.ravel(), Y.ravel())
Z = Zp.reshape(X.shape)

dZdy, dZdx = np.gradient(Z, yl, xl, edge_order=1)

SdZx = np.nancumsum(dZdx, axis=1)*np.diff(xl)[0]
SdZy = np.nancumsum(dZdy, axis=0)*np.diff(yl)[0]

Zhat = np.zeros(SdZx.shape)
for i in range(Zhat.shape[0]):
    for j in range(Zhat.shape[1]):
        #Zhat[i,j] += np.nansum([SdZy[i,0], -SdZy[0,0], SdZx[i,j], -SdZx[i,0]])
        Zhat[i,j] += np.nansum([SdZx[0,N//2], SdZy[i,N//2], SdZx[i,j], -SdZx[i,N//2]])
        
Zhat += Z[100,100] - Zhat[100,100]

lz = np.linspace(0, 5000, 20)
axe = plt.contour(X, Y, Z, lz, cmap='jet')
axe = plt.contour(X, Y, Zhat, lz, cmap='jet', linestyles=':')
axe.axes.plot(x, y, '.', markersize=1)
axe.axes.set_aspect('equal')
axe.axes.grid()

Which graphically renders as follow:


The edge effect is gone because of the RBF interpolant can extrapolate over the whole grid. You can confirm it by comparing the result of mesh-based interpolants.
Linear


Clough Tocher


Integration variable order change
We can also try to find a better way to integrate and mitigate the edge effect, eg. let's change the integration variable order:
Zhat[i,j] += np.nansum([SdZy[N//2,0], SdZx[N//2,j], SdZy[i,j], -SdZy[N//2,j]])

With a classic linear interpolant. The result is quite correct, but we still have an edge effect on the bottom left corner:

As you noticed the problem occurs at the middle of the axis in region where the integration starts and lacks a reference point.
",data-science
data scraping from vivinocom,"I am trying to collect data from vivino.com and the DataFrame comes out empty, I can see that my soup is collecting the website info, but can't see where my error is.
My code:
def get_data():  

    headers = {""User-Agent"":""Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:66.0) Gecko/20100101 Firefox/66.0"", ""Accept-Encoding"":""gzip, deflate"", ""Accept"":""text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8"", ""DNT"":""1"",""Connection"":""close"", ""Upgrade-Insecure-Requests"":""1""}

    r = requests.get(""https://www.vivino.com/explore?e=eJzLLbI1VMvNzLM1UMtNrLA1NTBQS660DQhRS7Z1DQ1SKwDKpqfZliUWZaaWJOao5SfZFhRlJqeq5dsmFierlZdExwJVJFcWA-mCEgC1YxlZ"", headers=headers)#, proxies=proxies)
    content = r.content
    soup = BeautifulSoup(content, ""html.parser"")

and as I need the wine maker, wine name and ratings this is how I've tried this:
alls = []
    for d in soup.findAll('div', attrs={'class':'explorerCard__titleColumn--28kWX'}):
        
        Winery = d.find_all(""a"", attrs={""class"":""VintageTitle_winery--2YoIr""})
        Wine = d.find_all(""a"", attrs={""class"":""VintageTitle_wine--U7t9G""})
        Rating = d.find_all(""div"", attrs={""class"":""VivinoRatingWide_averageValue--1zL_5""})
        num_Reviews = d.find_all(""div"", attrs={""class"":""VivinoRatingWide__basedOn--s6y0t""})
        Stars = d.find_all(""div"", attrs={""aria-label"":""rating__rating--ZZb_x rating__vivino--1vGCy""})

        alll=[]

        if Winery is not None:
            #print(n[0][""alt""])
            alll.append(Winery.text)

        else:
            alll.append(""unknown-winery"")

        if Wine is not None:
            #print(wine.text)
            alll.append(wine.text)
        else:
            alll.append(""0"")

        if Rating is not None:
            #print(rating.text)
            alll.append(rating.text)

        else:
            alll.append(""0"")
...

and then getting the data into a DataFrame:
results = []
for i in range(1, no_pages+1):
    results.append(get_data())
flatten = lambda l: [item for sublist in l for item in sublist]
df = pd.DataFrame(flatten(results),columns=['Winery','Wine','Rating','num_review', 'Stars'])
df.to_csv('redwines.csv', index=False, encoding='utf-8')

","The previous answer is correct but it needs the user-agent header set: 
import requests
import pandas as pd

r = requests.get(
    ""https://www.vivino.com/api/explore/explore"",
    params = {
        ""country_code"": ""FR"",
        ""country_codes[]"":""pt"",
        ""currency_code"":""EUR"",
        ""grape_filter"":""varietal"",
        ""min_rating"":""1"",
        ""order_by"":""price"",
        ""order"":""asc"",
        ""page"": 1,
        ""price_range_max"":""500"",
        ""price_range_min"":""0"",
        ""wine_type_ids[]"":""1""
    },
    headers= {
        ""User-Agent"": ""Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:66.0) Gecko/20100101 Firefox/66.0""
    }
)
results = [
    (
        t[""vintage""][""wine""][""winery""][""name""], 
        f'{t[""vintage""][""wine""][""name""]} {t[""vintage""][""year""]}',
        t[""vintage""][""statistics""][""ratings_average""],
        t[""vintage""][""statistics""][""ratings_count""]
    )
    for t in r.json()[""explore_vintage""][""matches""]
]
dataframe = pd.DataFrame(results,columns=['Winery','Wine','Rating','num_review'])

print(dataframe)

You will need to increment the page field to iterate over the next results
",data-science
import langchain gt error  typeerror issubclass arg 1 must be a class,"I want to use langchain for my project.
so I installed it using following command : pip install langchain
but While importing ""langchain"" I am facing following Error:
File /usr/lib/python3.8/typing.py:774, in _GenericAlias.__subclasscheck__(self, cls)
    772 if self._special:
    773     if not isinstance(cls, _GenericAlias):
--> 774         return issubclass(cls, self.__origin__)
    775     if cls._special:
    776         return issubclass(cls.__origin__, self.__origin__)

TypeError: issubclass() arg 1 must be a class

Any one who can solve this error ?
","typing-inspect==0.8.0
typing_extensions==4.5.0

",data-science
implementing aic score for multiple linear regression manually,"I have implemented a multiple linear regression class by hand and right now I am working on the metrics methods. I have tried to calculate the AIC and BIC scores manually, but the results weren't correct. The reason is that I didn't use the Log Likelihood function but went with the SSE approach. Could you please suggest me how to change the implementation to compute the full AIC and BIC scores?
Here is how my method looks like right now:
  def AIC_BIC(self, actual = None, pred = None):
    if actual is None:
      actual = self.response
    if pred is None:
      pred = self.response_pred

    n = len(actual)
    k = self.num_features

    residual = np.subtract(pred, actual)
    RSS = np.sum(np.power(residual, 2))

    AIC = n * np.log(RSS / n) + 2 * k
    BIC = n * np.log(RSS / n) + k * np.log(n)

    return (AIC, BIC)

And please try to give me a manual approach and not a library call. Thanks!
","I managed to solve the issue with the help of @James.
Here is how my new implementation looks like:
 def LogLikelihood(self):
     n = self.num_obs
     k = self.num_features
     residuals = self.residuals

     ll = -(n * 1/2) * (1 + np.log(2 * np.pi)) - (n / 2) * np.log(residuals.dot(residuals) / n)

     return ll

  def AIC_BIC(self):
    ll = self.LogLikelihood()
    n = self.num_obs
    k = self.num_features + 1

    AIC = (-2 * ll) + (2 * k)
    BIC = (-2 * ll) + (k * np.log(n))

    return AIC, BIC

I implemented a log-likelihood calculation and used it in the formula which can be found on Wikipedia.
",data-science
is seq2seq the right model for my data any examples,"I'm trying to train a model to predict design patterns from web pages. I'm using coordinates of bounding rects given a bunch of element groupings. Patterns look like this:
 [[elementId, width, height, x, y]]

so my target would be the [[x,y]] given [[elementId, width, height]].
Concretely:
 [[5, 1.0, 1.0], [4, 1.0, 1.0], [2, 175.0, 65.0], [2, 1.0, 1.0], [4, 1.0, 1.0]]
 ->
 [[0.0, 0.0], [0.0, 10.0], [3.0, 0.0], [0.0, 68.0], [0.0, 10.0]]


 [[2, 14.0, 14.0], [2, 14.0, 14.0], [2, 14.0, 14.0]]  
 ->
 [[0.0, 3.0], [0.0, 3.0], [0.0, 3.0]]

Patterns vary in size so I've padded them with [[0,0,0]]. I currently have about 15k of them, but can get more.
I was told that seq2seq with attention is the right model for this job. I've started with https://machinelearningmastery.com/develop-encoder-decoder-model-sequence-sequence-prediction-keras/ and achieved horrendous results.
Every seq2seq example i can find (searching for keras or pytorch) is used for translation which is categorical and I'm struggling to find a good regression based example.
So my questions are:

Is this the right model (encoder/decoder LSTM) for what i'm trying to do?

Is there any examples if so?


","Seq2Seq/LSTM are used when input/output are variable lengths.  
Your input is of size 3 and output is of size 2 (at least for the given examples).  So you can use a simple one/two-hidden layer feed-forward model with the L2/L1 loss (for regression). Any opt (SGD/Adam) should be fine, however, Adam works well in practice. 
Also, I think you should not use coordinates as it is, you can scale it so that the highest coordinate is 1 and hence the input/output range would be between 0 and 1. An added advantage, this would help you to generalize to different screen sizes intuitively.
",data-science
attributeerror 39floatprogress39 object has no attribute 39style39,"import numpy as np
import pandas as pd

import torch
from torch.utils.data import Dataset

import stanza
stanza.download('en')
nlp = stanza.Pipeline(lang='en')



above code used for Creating a Pipeline
Stanza provides a plethora of pre-trained NLP models for 66 human languages that we can make use of. Downloading a pre-trained model and creating a pipeline is as easy as:
this code showing this error below shown
Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.5.0.json

---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
Cell In[21], line 8
     
----> 8 stanza.download('en')

--> 547     download_resources_json

--> 441 request_file(
 
--> 152     download_file(url, temppath, proxies, raise_for_status)

--> 119 with tqdm(total=file_size, unit='B', unit_scale=True, \
   
--> 245 self.colour = colour

--> 204         self.container.children[-2].style.bar_color = bar_color


i have also upgraded jupyter notebook , tqdm and stanza
pip install --upgrade stanza
pip install --upgrade tqdm


but problem still persist.    this is project regarding
","I had the same problem with another library. In the terminal, it works and shows a progress bar, in a notebook-cell it raised the error above.
I worked around the issue by capturing the output of the command that tries to show the progress bar:
from contextlib import redirect_stdout

f = io.StringIO()
with redirect_stdout(f):
    problematic_function()

(The problematic function in my case was SentenceTransformerEmbeddings from langchain)
",data-science
how to update a previous run into mlflow,"I would like to update previous runs done with MLFlow, ie. changing/updating a parameter value to accommodate a change in the implementation. Typical uses cases:

Log runs using a parameter A, and much later, log parameters A and B. It would be useful to update the value of parameter B of previous runs using its default value.
""Specialize"" a parameter. Implement a model using a boolean flag as a parameter. Update the implementation to take a string instead. Now we need to update the values of the parameter for the previous runs so that it stays consistent with the new behavior.
Correct a wrong parameter value loggued in the previous runs.

It is not always easy to trash the whole experiment as I need to keep the previous runs for statistical purpose. I would like also not to generate new experiments just for a single new parameter, to keep a single database of runs.
What is the best way to do this?
","To add or correct a parameter, metric or artifact of an existing run, pass run_id instead of experiment_id to mlflow.start_run function
with mlflow.start_run(run_id=""your_run_id"") as run:
    mlflow.log_param(""p1"",""your_corrected_value"")
    mlflow.log_metric(""m1"",42.0) # your corrected metrics
    mlflow.log_artifact(""data_sample.html"") # your corrected artifact file

You can correct, add to, or delete any MLflow run any time after it is complete. Get the run_id either from the UI or by using mlflow.search_runs.
Source: https://towardsdatascience.com/5-tips-for-mlflow-experiment-tracking-c70ae117b03f
",data-science
how to pass different set of data to train and test without splitting a dataframe python,"I have gone through multiple questions that help divide your dataframe into train and test, with scikit, without etc.
But my question is I have 2 different csvs ( 2 different dataframes from different years). I want to use one as train and other as test?
How to do so for LinearRegression / any model? 
","
Load the datasets individually.
Make sure they are in the same format of rows and columns (features).
Use the train set to fit the model.
Use the test set to predict the output after training.

# Load the data
train = pd.read_csv('train.csv')
test = pd.read_csv('test.csv')

# Split features and value
# when trying to predict column ""target"" 
X_train, y_train = train.drop(""target""), train[""target""]
X_test, y_test = test.drop(""target""), test[""target""]

# Fit (i.e. train) model
reg = LinearRegression()
reg.fit(X_train, y_train)

# Predict
pred = reg.predict(X_test)

# Score
accuracy = reg.score(X_test, y_test)

",data-science
quotattributeerror 39index39 object has no attribute 39tz_localize39quot while using the function history to extract stock information,"I'm trying to extract information of the Tesla stock but, I always get the error while coding it. Here's the code so far:
import plotly.graph_objects as go
import yfinance as yf
import pandas as pd
import requests

from bs4 import BeautifulSoup
from plotly.subplots import make_subplots

def make_graph(stock_data, revenue_data, stock):

    fig = make_subplots(rows=2, cols=1, shared_xaxes=True, subplot_titles=(""Historical Share Price"", ""Historical Revenue""), vertical_spacing = .3)
    stock_data_specific = stock_data[stock_data.Date <= '2021--06-14']
    revenue_data_specific = revenue_data[revenue_data.Date <= '2021-04-30']
    fig.add_trace(go.Scatter(x=pd.to_datetime(stock_data_specific.Date, infer_datetime_format=True), y=stock_data_specific.Close.astype(""float""), name=""Share Price""), row=1, col=1)
    fig.add_trace(go.Scatter(x=pd.to_datetime(revenue_data_specific.Date, infer_datetime_format=True), y=revenue_data_specific.Revenue.astype(""float""), name=""Revenue""), row=2, col=1)
    fig.update_xaxes(title_text=""Date"", row=1, col=1)
    fig.update_xaxes(title_text=""Date"", row=2, col=1)
    fig.update_yaxes(title_text=""Price ($US)"", row=1, col=1)
    fig.update_yaxes(title_text=""Revenue ($US Millions)"", row=2, col=1)
    fig.update_layout(showlegend=False,
    height= 900,
    title= stock,
    xaxis_rangeslider_visible=True)
    fig.show()

Tesla= yf.Ticker('TSLA')
tesla_database= Tesla.history(period= ""max"")

In the tesla_database part is where the following error comes:
Traceback (most recent call last):
  File ""<pyshell#23>"", line 1, in <module>
    tesla_database= Tesla.history(period= ""max"")
  File ""/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/yfinance/base.py"", line 295, in history
    df.index = df.index.tz_localize(""UTC"").tz_convert(
AttributeError: 'Index' object has no attribute 'tz_localize'

How can I fix this? According to the IBM Watson Course in Coursera, this is the correct answer but I get this error all the time. Can someone correct it, please?
","I had the same issue with pandas 1.4.1 For me reverting back to a previous pandas version solved the problem:
Try this on command line:
pip install pandas==1.2.2

Try this from whithin Jupyter Board:
!pip install pandas==1.2.2

",data-science
valueerror continuous format is not supported,"I have written a simple function where I am using the average_precision_score from scikit-learn to compute average precision.
My Code:
def compute_average_precision(predictions, gold):
    gold_predictions = np.zeros(predictions.size, dtype=np.int)
    for idx in range(gold):
        gold_predictions[idx] = 1
    return average_precision_score(predictions, gold_predictions)

When the function is executed, it produces the following error.
Traceback (most recent call last):
  File ""test.py"", line 91, in <module>
    total_avg_precision += compute_average_precision(np.asarray(probs), len(gold_candidates))
  File ""test.py"", line 29, in compute_average_precision
    return average_precision_score(predictions, gold_predictions)
  File ""/if5/wua4nw/anaconda3/lib/python3.5/site-packages/sklearn/metrics/ranking.py"", line 184, in average_precision_score
    average, sample_weight=sample_weight)
  File ""/if5/wua4nw/anaconda3/lib/python3.5/site-packages/sklearn/metrics/base.py"", line 81, in _average_binary_score
    raise ValueError(""{0} format is not supported"".format(y_type))
ValueError: continuous format is not supported

If I print the two numpy arrays predictions and gold_predictions, say for one example, it looks alright. [One example is provided below.]
[ 0.40865014  0.26047812  0.07588802  0.26604077  0.10586583  0.17118802
  0.26797949  0.34618672  0.33659923  0.22075308  0.42288553  0.24908153
  0.26506338  0.28224747  0.32942101  0.19986877  0.39831917  0.23635269
  0.34715138  0.39831917  0.23635269  0.35822859  0.12110706]
[1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]

What I am doing wrong here? What is the meaning of the error?
","Just taking a look at the sklearn docs 

Parameters:   
y_true : array, shape = [n_samples] or [n_samples, n_classes] True
  binary labels in binary label indicators.
y_score : array, shape = [n_samples] or [n_samples, n_classes] Target
  scores, can either be probability estimates of the positive class,
  confidence values, or non-thresholded measure of decisions (as
  returned by “decision_function” on some classifiers).

So your first argument has to be an array of binary labels, but you are passing some sort of float array as the first argument. So I believe you need to reverse the order of the arguments you are passing.
",data-science
finding final regression equation in python,"How to find final regression model equation including coefficients with all variables? is there any method? 
","Show you an Example
I show you an example with OLS using boston house price data set.
Code:
# load a dataset and regression function
from sklearn import linear_model,datasets
import pandas as pd
# I use boston dataset to show you 
full_data = datasets.load_boston()

# get a regressor, fit intercept
reg = linear_model.LinearRegression(fit_intercept=True)
# data is our explanatory, target is our response
reg.fit(full_data['data'],full_data['target'])

# we have 1 intercept and  11 variables' coef
reg.intercept_,reg.coef_

# get the name of features
full_data.feature_names
# append to get a new list
coef = np.append(reg.intercept_,reg.coef_)
feature_names = np.append(['Intercept'], full_data.feature_names)
# output a dataframe contains coefficients you want
pd.DataFrame({""feature_names"":feature_names,""coef"":coef})

Output:
   feature_names       coef
0      Intercept  36.459488
1           CRIM  -0.108011
2             ZN   0.046420
3          INDUS   0.020559
4           CHAS   2.686734
5            NOX -17.766611
6             RM   3.809865
7            AGE   0.000692
8            DIS  -1.475567
9            RAD   0.306049
10           TAX  -0.012335
11       PTRATIO  -0.952747
12             B   0.009312
13         LSTAT  -0.524758

Some Advice
You can use dir(object) to see what's in your fitted model, like using dir(full_data) and dir(reg) to see atrributes and methods of an instance.
As for sklearn, here is an official guide about it. You can find functions and datasets on the guide.
",data-science
how to fix overflowerror overflow in int64 addition,"I'm trying to subtract column df['date_of_admission'] from the column df['DOB'] to find the difference between then and store the age value in df['age'] column, however, I'm getting this error:

OverflowError: Overflow in int64 addition

 DOB          date_of_admission      age
 2000-05-07   2019-01-19 12:26:00        
 1965-01-30   2019-03-21 02:23:12        
 NaT          2018-11-02 18:30:10        
 1981-05-01   2019-05-08 12:26:00       
 1957-01-10   2018-12-31 04:01:15         
 1968-07-14   2019-01-28 15:05:09            
 NaT          2018-04-13 06:20:01 
 NaT          2019-02-15 01:01:57 
 2001-02-10   2019-03-21 08:22:00       
 1990-03-29   2018-11-29 03:05:03
.....         ......
.....         .....
.....         .....

I've tried it with the following:
import numpy as np
import pandas as pd
from datetime import dt

df['age'] = (df['date_of_admission'] - df['DOB']).dt.days // 365

Expected to get the following age column after finding the difference between: 
age
26
69
NaN
58
.
.
.

","Convert both columns into date then subtract it 
import pandas as pd


df['date_of_admission'] = pd.to_datetime(df['date_of_admission']).dt.date

df['DOB'] = pd.to_datetime(df['DOB']).dt.date

df['age'] = ((df['date_of_admission']-df['DOB']).dt.days) //365


SECOND TEST
#Now I have use DOB AND date_of_admission data from the question and it is working fine

df = pd.DataFrame(data={""DOB"":['2000-05-07','1965-01-30','NaT'],
                   ""date_of_admission"":[""2019-01-19 12:26:00"",""2019-03-21 02:23:12"", ""2018-11-02 18:30:10""]})

df['DOB'] = pd.to_datetime(df['DOB']).dt.date
df['date_of_admission'] = pd.to_datetime(df['date_of_admission']).dt.date
df['age'] = ((df['date_of_admission']-df['DOB']).dt.days) //365

RESULT:
DOB       date_of_admission   age
2000-05-07  2019-01-19       18.0
1965-01-30  2019-03-21       54.0
NaT         2018-11-02       NaN

",data-science
i got syntax error by using jupyter notebook,"[Here the image]
1I dont know why i get this error.
Here is the code:
import pandas as pd
import matplotlib.pyplot as plt 
import numpy as np 

data = pd.read_csv(""files/ch04/ACCIDENTS_GU_BCN_2013.csv"" , encoding= 'unicode_escape')
data['Date'] = data[u'Dia de mes']. apply(lambda x: str(x))
+ '-' + data[u'Mes de any']. apply(lambda x: str(x))
data['Date'] = pd.to_datetime(data['Date'])
accidents = data.groupby (['Date']).size()
print accidents.mean()

","the problem here is that you're using Python3 which requires brackets after the print statement.
So, do this : print(accidents.mean())
",data-science
sklearn utils compute_class_weight function for large dataset,"I am training a tensorflow keras sequential model on around 20+ GB text based categorical data in a postgres db and i need to give class weights to the model.
Here is what i am doing.
class_weights = sklearn.utils.class_weight.compute_class_weight('balanced', classes, y)

model.fit(x, y, epochs=100, batch_size=32, class_weight=class_weights, validation_split=0.2, callbacks=[early_stopping])

Since i can't load the whole thing in memory i figured i can use fit_generator method in keras model. 
However how can i calculate the class weights on this data? sklearn does not provide any special function for this, is it the right tool for this ?
I thought of doing it on multiple random samples but is there a better approach where whole data can be used ?
","You can use the generators and also you can compute the class weights. 
Let's say you have your generator like this
train_generator = train_datagen.flow_from_directory(
        'train_directory',
        target_size=(224, 224),
        batch_size=32,
        class_mode = ""categorical""
        )

and the class weights for the training set can be computed like this
class_weights = class_weight.compute_class_weight(
           'balanced',
            np.unique(train_generator.classes), 
            train_generator.classes)

[EDIT 1]
Since you mentioned about postgres sql in the comments, I am adding the prototype answer here. 
first fetch the count for each classes using a separate query from postgres sql and use it to compute the class weights. you can compute it manually. The basic logic is the count of least weighed class gets the value 1, and the rest of the classes get <1 based on the relative count to the least weighed class. 
for example you have 3 classes A,B,C with 100,200,150 then class weights becomes {A:1,B:0.5,C:0.66}
let compute it manually after fetching the values from postgres sql. 
[Query]
cur.execute(""SELECT class, count(*) FROM table group by classes order by 1"")
rows = cur.fetchall()

The above query will return rows with tuples (class name, count for each class) ordered from least to highest. 
Then the below line will code will create the class weights dictionary
class_weights = {}
for row in rows:
    class_weights[row[0]]=rows[0][1]/row[1] 
    #dividing the least value the current value to get the weight, 
    # so that the least value becomes 1, 
    # and other values becomes < 1

",data-science
gradient descent is not converging for very large values in a small dataset,"I am trying to write a program to calculate the slope and the intercept of a linear regression model but when I am running more than 10 iterations, the gradient descent function gives the np.nan value for both intercept as well as slope.
Below is my implementation
def get_gradient_at_b(x, y, b, m):
  N = len(x)
  diff = 0
  for i in range(N):
    x_val = x[i]
    y_val = y[i]
    diff += (y_val - ((m * x_val) + b))
  b_gradient = -(2/N) * diff  
  return b_gradient

def get_gradient_at_m(x, y, b, m):
  N = len(x)
  diff = 0
  for i in range(N):
      x_val = x[i]
      y_val = y[i]
      diff += x_val * (y_val - ((m * x_val) + b))
  m_gradient = -(2/N) * diff  
  return m_gradient

def step_gradient(b_current, m_current, x, y, learning_rate):
    b_gradient = get_gradient_at_b(x, y, b_current, m_current)
    m_gradient = get_gradient_at_m(x, y, b_current, m_current)
    b = b_current - (learning_rate * b_gradient)
    m = m_current - (learning_rate * m_gradient)
    return [b, m]

def gradient_descent(x, y, learning_rate, num_iterations):
  b = 0
  m = 0
  for i in range(num_iterations):
    b, m = step_gradient(b, m, x, y, learning_rate)
  return [b,m]  

I am running it on the following data:
a=[3.87656018e+11, 4.10320300e+11, 4.15730874e+11, 4.52699998e+11,
       4.62146799e+11, 4.78965491e+11, 5.08068952e+11, 5.99592902e+11,
       6.99688853e+11, 8.08901077e+11, 9.20316530e+11, 1.20111177e+12,
       1.18695276e+12, 1.32394030e+12, 1.65661707e+12, 1.82304993e+12,
       1.82763786e+12, 1.85672212e+12, 2.03912745e+12, 2.10239081e+12,
       2.27422971e+12, 2.60081824e+12]
b=[3.3469950e+10, 3.4784980e+10, 3.3218720e+10, 3.6822490e+10,
       4.4560290e+10, 4.3826720e+10, 5.2719430e+10, 6.3842550e+10,
       8.3535940e+10, 1.0309053e+11, 1.2641405e+11, 1.6313218e+11,
       1.8529536e+11, 1.7875143e+11, 2.4981555e+11, 3.0596392e+11,
       3.0040058e+11, 3.1440530e+11, 3.1033848e+11, 2.6229109e+11,
       2.7585243e+11, 3.0352616e+11]

print(gradient_descent(a, b, 0.01, 100))
#result --> [nan, nan]

When I run the gradient_descent function on a dataset with smaller values, it gives the correct answers. Also I was able to obtain the intercept and slope for the above data with from sklearn.linear_model import LinearRegression
Any help will be appreciated in figuring out why the result is [nan, nan] instead of giving me the correct intercept and slope.
","You need to reduce the learning rate. Since the values in a and b are so large (>= 1e11), the learning rate needs be approximately 1e-25 for this to even do the gradient descent, else it will randomly overshoot because of large gradients of a and b.
b, m = gradient_descent(a, b, 5e-25, 100)
print(b, m)
Out: -3.7387067636195266e-13 0.13854551291084335

",data-science
python matplotlib  color code positive and negative values in plot,"I have a bunch of samples with shape (1, 104). All samples are integers(positive, negative and 0) which are being used in the imshow function of matplotlib. Below is the function I've created to display them as images.
def show_as_image(sample):
    bitmap = sample.reshape((13, 8))
    plt.figure()
    # this line needs changes.
    plt.imshow(bitmap, cmap='gray', interpolation='nearest')
    plt.colorbar()
    plt.show()

I need to color code the positive and negative values from the sample. PS: Take 0 as positive.
How do I change my code?
","You could set the normalization of the colorcoding such that it is equally spread between the negative absolute value and positive absolute value of the data. Using a colormap with a light color in the middle can help visualizing how far away from zero the values are.
import numpy as np
import matplotlib.pyplot as plt

def show_as_image(sample):
    bitmap = sample.reshape((13, 8))
    maxval = np.max(np.abs([bitmap.min(),bitmap.max()]))
    plt.figure()
    plt.imshow(bitmap, cmap='RdYlGn', interpolation='nearest',
               vmin=-maxval, vmax=maxval)
    plt.colorbar()
    plt.show()

sample=np.random.randn(1,104)
show_as_image(sample)


If instead a binary map is required, you may map positive values to e.g. 1 and negative ones to 0.
import numpy as np
import matplotlib.pyplot as plt

def show_as_image(sample):
    bitmap = sample.reshape((13, 8))
    bitmap[bitmap >= 0] = 1
    bitmap[bitmap < 0] = 0
    plt.figure()
    plt.imshow(bitmap, cmap='RdYlGn', interpolation='nearest',
               vmin=-.1, vmax=1.1)
    plt.show()

sample=np.random.randn(1,104)
show_as_image(sample)


In such case the use of a colorbar is probably useless.
",data-science
is there a way to read line by line from a asc file and checking it after a specific substring,"I'm having a file with many lines, which I want to transform in a data frame to do some data science. 
Reading line by line I founding a little code snippet, which not seems to be working well. But that is not the main problem. First of all, i want only save the lines which are having the string ""CANFD"" inside. And I know that is not working with a standard if construct, because of the vector. The substring is working and yes the numbers are correct.
fileName <- ""Data2F001new.ASC""
conn <- file(fileName,open=""r"")
linn <-readLines(conn)
for (i in 1:length(linn)){
  {
    tmp <- substring(linn, 12, 16)
    if(tmp==""CANFD""){
    system <- substring(linn, 12, 16)
    timestamp <- substring(linn, 0, 10)  
    bytes <- substring(linn, 54, 56)
  channel <- substring(linn, 19,20)
  }
}
close(conn)     

R says me following: The condition has length > 1 and only the first element will be used.
The expected Output are the lines with CANFD.
","Here is an example how you could do it:
## Create a temp file as dummy example
tmp <- tempfile(fileext = "".txt"")
con <- file(tmp, open = ""rw"")
## Write some lines into the file
writeLines(sample(LETTERS, 1000, replace = TRUE), con)

## read the lines
all_lines <- readLines(con) ## each element represents one line

## filter lines which contain an 'A'
## in your case you want something like grep(""CANFD"", all_lines)
## see ?grep for further info
all_lines[grep(""A"", all_lines)]
close(con)

",data-science
creating a dictionary from a dataframe,"dataframe img
I want to map the retail_store_id with the medicine name, by creating a dictionary where 53 i.e the retail_store_id is the key and the values will be the corresponding medicine name.
dict = { '53': ['PAN 40MG TAB', 'MOXIKIND CV 625MG TAB'.....]}      

","I have considered the below dataset as example

you can use the below code.
import os
import  pandas as pd

# create a empty dictionary
dict = {}

# read the data from the dataframe
df = pd.read_excel(r'C:\*********\Output.xlsx')

# iterate over the DF
for index in df.index:

    # initialize key vairiable with ID columns
    key = df['ID'][index]

    # if key alreadt exists the append the value which is in list format for else starting 
    # adding values into a list
    if key in dict:
        dict[key].append(df['Medicine Name'][index])
    else:    
        dict[key] = [df['Medicine Name'][index]]


print(dict)

Output Dictionary
{53: ['A', 'B', 'C', 'D', 'E', 'F'], 
54: ['G', 'H', 'I', 'J', 'K'], 
55: ['L', 'M']}

",data-science
ways to resample nonstandard cftimeindex calendars 360day noleapyear with xarray for pandas usage,"#60198708 brought me to open this question as I've not found the nice solution yet.
The issue
I have downloaded several climate models from the EURO-CORDEX ensemble for daily precipitaion flux. While some models work with standard calendar, compatible with Pandas datetime, others, particularly MOHC HadGem2 ES, use 360-day CFTimeIndex.
The principal question is, how to effectively resample to monthly data with these calendars to be able to harmonize it and produce ensemble statistics later.
The data for precipitation flux (2011-2015 excerpt) can look like this
You can download it here.
<xarray.Dataset>
Dimensions:       (bnds: 2, rlat: 412, rlon: 424, time: 1800)
Coordinates:
    lat           (rlat, rlon) float64 ...
    lon           (rlat, rlon) float64 ...
  * rlat          (rlat) float64 -23.38 -23.26 -23.16 ... 21.61 21.73 21.83
  * rlon          (rlon) float64 -28.38 -28.26 -28.16 ... 17.93 18.05 18.16
  * time          (time) object 2011-01-01 12:00:00 ... 2015-12-30 12:00:00
Dimensions without coordinates: bnds
Data variables:
    pr            (time, rlat, rlon) float32 ...
    rotated_pole  |S1 ...
    time_bnds     (time, bnds) object ...
Attributes:
    CDI:                            Climate Data Interface version 1.3.2
    Conventions:                    CF-1.6
    NCO:                            4.4.2
    CDO:                            Climate Data Operators version 1.3.2 (htt...
    contact:                        Fredrik Boberg, Danish Meteorological Ins...
    creation_date:                  2019-11-16 14:39:25
    experiment:                     Scenario experiment using HadGEM as drivi...
    experiment_id:                  rcp45
    driving_experiment:             MOHC-HadGEM2-ES,rcp45,r1i1p1
    driving_model_id:               MOHC-HadGEM2-ES
    driving_model_ensemble_member:  r1i1p1
    driving_experiment_name:        rcp45
    frequency:                      day
    institution:                    Danish Meteorological Institute
    institute_id:                   DMI
    model_id:                       DMI-HIRHAM5
    rcm_version_id:                 v2
    project_id:                     CORDEX
    CORDEX_domain:                  EUR-11
    product:                        output
    tracking_id:                    hdl:21.14103/158e462e-499c-4d6e-8462-ac3e...
    c3s_disclaimer:                 This data has been produced in the contex...


As you can see, dataset's time dimension is cftime.Datetime360Day. All months are 30-days which is sometimes good for climate projections, not for pandas though.
<xarray.DataArray 'time' (time: 1800)>
array([cftime.Datetime360Day(2011-01-01 12:00:00),
       cftime.Datetime360Day(2011-01-02 12:00:00),
       cftime.Datetime360Day(2011-01-03 12:00:00), ...,
       cftime.Datetime360Day(2015-12-28 12:00:00),
       cftime.Datetime360Day(2015-12-29 12:00:00),
       cftime.Datetime360Day(2015-12-30 12:00:00)], dtype=object)
Coordinates:
  * time     (time) object 2011-01-01 12:00:00 ... 2015-12-30 12:00:00
Attributes:
    standard_name:  time
    long_name:      time
    bounds:         time_bnds

What I tried so far
I went the dirty way by converting CFTimeIndex to string, putting to the pandas.DataFrame and converting the time with pd.to_datetime and errors=coerce
ds = xarray.open_dataset('data/mohc_hadgem2_es.nc')

def cft_to_string(cfttime_obj):
        month = str(cfttime_obj.month)
        day = str(cfttime_obj.day)

        # This is awful but there were no two-digit months/days by default
        month = '0'+month if len(month)==1 else month
        day = '0'+day if len(day)==1 else day

        return f'{cfttime_obj.year}-{month}-{day}'

# Apply above function
ds_time_strings = list(map(cft_to_string, ds['time']))

# Get precipitation values only (to use in pandas dataframe)
# Suppose the data are from multiple pixels (for whole of Europe)
# - that's why the mean(axis=(1,2))

precipitation = ds['pr'].values.mean(axis=(1,2))

# To dataframe
df = pd.DataFrame(index=ds_time_strings, data={'precipitation': precipitation})

# Coerce erroneous dates
df.index = pd.to_datetime(df.index, errors='coerce') # Now, dates such as 2011-02-30 are omitted

This gives a dataframe with non-standard dates as NaT and some dates (31st days) are missing. I don't mind since I create projections for 90 years span.
            precipitation
2011-01-01  0.000049
2011-01-02  0.000042
2011-01-03  0.000031
2011-01-04  0.000030
2011-01-05  0.000038
... ...
2011-02-28  0.000041
NaT         0.000055
NaT         0.000046
2011-03-01  0.000031
... ...
2015-12-26  0.000028
2015-12-27  0.000034
2015-12-28  0.000028
2015-12-29  0.000025
2015-12-30  0.000024
1800 rows × 1 columns

Now I can resample to monthly data using pandas easily.
While this seems to work, is there a cleaner way with xarray/pandas only? Possibly non-string based?

ds.indexes['time'].to_datetimeindex() fails on a non-standard calendar
ds.resample(time='M') would do the resampling, however, it yields non-standard month ends. I did not find the way to floor to correct month ends since ds['time'].dt.floor('M') fails on ValueError: <MonthEnd: n=1> is a non-fixed frequency
xarray.groupby(time='time.month') can handle non-standard calendars, however, its use case is to group along different axes, which is undesired

I certainly must have missed something as this is a complex issue. Any help appreciated.
","Thanks for the detailed example!  If a time series of monthly means is acceptable for your analysis, I think the cleanest approach would be to resample to ""month-start"" frequency and then harmonize the date types, e.g. for the datasets indexed by a CFTimeIndex, something like:
resampled = ds.resample(time=""MS"").mean()
resampled[""time""] = resampled.indexes[""time""].to_datetimeindex()

This is basically your second bullet point, but with a minor change.  Resampling to month-start frequency gets around the issue that a 360-day calendar contains month ends that do not exist in a standard calendar, e.g. February 30th.
",data-science
have two dictionary columns with different lengths want to use series explode but it mismatch,"I have tow columns in my data frame that have multiple dictionarise in it and I want to expand it into multiple columns but the problem when I use explode series it mismatch
example:




Column A
Column B
Columns C




Cell 1
{""a"":0.5} {""b"":0.5}
{""d1"":0.25}{""d2"":0.25}{""d3"":0.25}{""d4"":0.25}


Cell 2
{""c"":1.0}  
{""t1"":0.5} {""t2"":0.5} 









desirable output:




Column A
Column B1
Columns B2
Columns C1
Columns C2




Cell 1
""a""
0.5
""d1""
0.25


Cell 1
""a""
0.5
""d2""
0.25


Cell 1
""b""
0.5
""d3""
0.25


Cell 1
""b""
0.5
""d4""
0.25


Cell 2
""c""
1.0
""t1""
0.5


Cell 2
""c""
1.0
""t2""
0.5




i used to use
df1 = ( 
       df.assign(Columns B2 =lambda df: df.Columns B.apply(lambda x: x.values()))    
     
       .reset_index()
       .apply(pd.Series.explode)
     
            )
output = ( 
       df1.assign(Columns C2 =lambda df: df.Columns C.apply(lambda x: x.values()))    
     
       .reset_index()
       .apply(pd.Series.explode)
     
            )

the problem is that it mismatches the rows something in column C1 supposed to be in row 2 go to 3
","Assuming this input and that Column B has the shortest dictionaries:
df = pd.DataFrame({'Column A': ['Cell 1', 'Cell 2'],
                   'Column B': [[{'a': 0.5}, {'b': 0.5}], [{'c': 1.0}]],
                   'Column C': [[{'d1': 0.25}, {'d2': 0.25}, {'d3': 0.25}, {'d4': 0.25}], [{'t1': 0.5}, {'t2': 0.5}]]})

You can try to repeat the shortest list, then convert your key/value pairs as new DataFrames:
tmp = pd.DataFrame([[np.repeat(a, int(np.ceil(len(b)/len(a))))[:len(b)], b]
                    for a, b in zip(df['Column B'], df['Column C'])],
                   columns=['Column B', 'Column C']
                   ).explode(['Column B', 'Column C'])

def convert(col):
    s = col.apply(lambda x: list(x.items())[0])
    return (pd.DataFrame(s.tolist(), index=s.index)
              .rename(columns=lambda x: f'{col.name}{x+1}')
            )

out = (df.drop(columns=['Column B', 'Column C'])
         .join(pd.concat([convert(tmp[c]) for c in list(tmp)], axis=1))
      )

Output:
  Column A Column B1  Column B2 Column C1  Column C2
0   Cell 1         a        0.5        d1       0.25
0   Cell 1         a        0.5        d2       0.25
0   Cell 1         b        0.5        d3       0.25
0   Cell 1         b        0.5        d4       0.25
1   Cell 2         c        1.0        t1       0.50
1   Cell 2         c        1.0        t2       0.50

",data-science
how do i show a scatter plot in python after doing pca,"I made a random data of my own, that comprises of a text file with 18 rows and 5 columns with all integer entries.
I successfully managed to do PCA but now stuck. I am unable to do a scatter plot. Here is my code:
f=open(r'<path>mydata.txt')
print(f.read()) #reading from a file


with open(r'<path>mydata.txt') as f:
emp= []
for line in f:
    line = line.split() 
    if line:            
        line = [int(i) for i in line]
        emp.append(line)


from sklearn.decomposition import PCA
import pylab as pl
from itertools import cycle
X = emp
pca = PCA(n_components=3, whiten=True).fit(X)
X_pca = pca.transform(X) #regular PCA

Now, with PCA done and my variances known, how do I plot?
Here is how a sample data in my data set looks:
2    1    2    3    0
2    3    2    3    0
1    3    1    1    0
1    5    2    1    0
2    3    1    1    0
3    3    0    1    0
7    1    1    1    1
7    2    2    1    1
1    1    1    4    1
3    2    3    2    1
2    2    2    2    1
1    3    2    3    1
2    3    2    1    2
2    2    1    1    2
7    5    3    2    2
3    4    2    4    2
2    1    1    1    2
7    1    3    3    2

","Is this what you are asking for?
import numpy as np
from matplotlib import pyplot as plt


data1 = [np.random.normal(0,0.1, 10), np.random.normal(0,0.1,10)]
data2 = [np.random.normal(1,0.2, 10), np.random.normal(2,0.3,10)]
data3 = [np.random.normal(-2,0.1, 10), np.random.normal(1,0.5,10)]


plt.scatter(data1[0],data1[1])
plt.scatter(data2[0],data2[1])
plt.scatter(data3[0],data3[1])

plt.show()

the result for the three different data sets would look something like this: 
EDIT:
Hopefully I now understand your question better. Here the new code:
import numpy as np
from matplotlib import pyplot as plt    

with open(r'mydata.txt') as f:
    emp= []
    for line in f:
        line = line.split() 
        if line:            
            line = [int(i) for i in line]
            emp.append(line)


from sklearn.decomposition import PCA
import pylab as pl
from itertools import cycle
X = emp
pca = PCA(n_components=3, whiten=True).fit(X)
X_pca = pca.transform(X) #regular PCA

jobs = ['A', 'B', 'C']
job_id = np.array([e[4] for e in emp])

fig, axes = plt.subplots(3,3, figsize=(5,5))

for row in range(axes.shape[0]):
    for col in range(axes.shape[1]):
        ax = axes[row,col]
        if row == col:
            ax.tick_params(
                axis='both',which='both',
                bottom='off',top='off',
                labelbottom='off',
                left='off',right='off',
                labelleft='off'
            )
            ax.text(0.5,0.5,jobs[row],horizontalalignment='center')
        else:
            ax.scatter(X_pca[:,row][job_id==0],X_pca[:,col][job_id==0],c='r')
            ax.scatter(X_pca[:,row][job_id==1],X_pca[:,col][job_id==1],c='g')
            ax.scatter(X_pca[:,row][job_id==2],X_pca[:,col][job_id==2],c='b')
fig.tight_layout()
plt.show()

I named the jobs 'A', 'B', and 'C' with the ids 0, 1, and 2, respectively. From the last row of emp, I create a numpy array that holds these indices. In the crucial plotting commands, I mask the data by the job ids. Hope this helps.
The resulting plot looks like this:

EDIT 2:
If you want only one plot where you correlate, say, the first and the second column of X_pca with each other, the code becomes much more simple:
import numpy as np
from matplotlib import pyplot as plt

with open(r'mydata.txt') as f:
    emp= []
    for line in f:
        line = line.split() 
        if line:            
            line = [int(i) for i in line]
            emp.append(line)


from sklearn.decomposition import PCA
import pylab as pl
from itertools import cycle
X = emp
pca = PCA(n_components=3, whiten=True).fit(X)
X_pca = pca.transform(X) #regular PCA

jobs = ['A', 'B', 'C']
job_id = np.array([e[4] for e in emp])

row = 0
col = 1

plt.scatter(X_pca[:,row][job_id==0],X_pca[:,col][job_id==0],c='r')
plt.scatter(X_pca[:,row][job_id==1],X_pca[:,col][job_id==1],c='g')
plt.scatter(X_pca[:,row][job_id==2],X_pca[:,col][job_id==2],c='b')

plt.show()

The result looks like this:
I strongly suggest that you read the documentation of the functions used in these examples.
",data-science
typeerror 39module39 object is not callable  while using umap,"import umap as UMAP

import umap


retarget = {df_train['target'].value_counts().reset_index()['index'][i]: i for i in range(len(df_train['target'].value_counts()))}
retarget2 = {i: k for k, i in retarget.items()}
df_train['target'] = df_train['target'].map(retarget)

umap = umap(n_components = 2, n_neighbors = 10, min_dist = 0.99).fit_transform(df_train.drop('target', axis = 1).sample(15000, random_state = 228), df_train['target'].sample(15000, random_state = 228))


I am trying to use UMAP for visualization but it is keep on giving me error:
TypeError                                 Traceback (most recent call last)
<ipython-input-15-bb51a04f463b> in <module>
      8 df_train['target'] = df_train['target'].map(retarget)
      9 
---> 10 umap = umap(n_components = 2, n_neighbors = 10, min_dist = 0.99).fit_transform(df_train.drop('target', axis = 1).sample(15000, random_state = 228), df_train['target'].sample(15000, random_state = 228))
     11 
     12 plt.figure(figsize=(15, 12))

TypeError: 'module' object is not callable

I have umap installed in my system:
(base) C:\Users\bakumari\Anaconda3\Lib\site-packages>pip install umap
Collecting umap
  Using cached umap-0.1.1-py3-none-any.whl
Installing collected packages: umap
Successfully installed umap-0.1.1

I am trying to use umap for visualization purpose.
","You need to install umap-learn
pip uninstall umap
pip install umap-learn

and then
import umap
umap = umap.UMAP(n_components = 2, n_neighbors = 10, min_dist = 0.99).fit_transform(df_train.drop('target', axis = 1).sample(15000, random_state = 228), df_train['target'].sample(15000, random_state = 228))

",data-science
approximating the sine function with polynomials using gaussian elimination,"I'm trying to approximate the sine function over an interval with gaussian elimination using python. Using this code.
from copy import deepcopy
def convert_to_row_eschelon(A_, B_):
    A = deepcopy(A_)
    B = deepcopy(B_)
    dim = len(A)
    for cc in range(dim):
        # pivot_row = A[cc]
        for r in range(cc + 1, dim):
            leading_term = A[r][cc]
            for c in range(cc, dim):
                # print(A[r][c], A[r][cc])
                A[r][c] = A[r][c] - A[cc][c] * leading_term / A[cc][cc]
            B[r] = B[r] - B[cc] * leading_term / A[cc][cc]
    return A, B

def back_sub(matrix_pair):
    A = matrix_pair[0]
    B = matrix_pair[1]
    res = [None] * len(B)
    
    for i in range(len(B) - 1, -1, -1):
        def f(j):
            return A[i][j] * res[j]
        res[i] = (B[i] - sum([f(k) for k in range(i + 1, len(B))])) / A[i][i]
    return res


def gaussian_elimination(A, B):
    return back_sub(convert_to_row_eschelon(A, B))
A = [
      [1, 2, 3],
      [4, 5, 7],
      [23, 12, 12]
]
B = [4, 6, 7]
fig = 10
# print(convert_to_row_eschelon(A, B))
def make_polynomial(x_points, y_points):
    # A[x_point index used][degree]
    degree = len(x_points)
    A = []
    for i in range(degree):
        A.append([])
        for j in range(degree):
            A[i].append(x_points[i] ** j) # This is line 45 
    coeff = gaussian_elimination(A, y_points)
    def f(x):
        coeff_f = coeff
        res = 0
        for i in range(len(coeff_f)):
            res += x ** i * coeff_f[i]
        return res
    return f

def generate_x(start, finish, increment):
    x_points = []
    curr = start
    while curr < finish:
        x_points.append(curr)
        curr += increment
    return x_points
from math import sin, pi
start = 0 # These are the intervals
finish = 2 * pi
increment = 0.01
def test_func(x):
    return  sin(x)
# Creating the polynomial
x_val_f = generate_x(start, finish, increment)
x_val_test = generate_x(start, finish, 0.01)

f = make_polynomial(x_val_f, [test_func(i) for i in x_val_f])
print(f(3))
y_val_f = [f(i) for i in x_val_f]
y_val_test = [test_func(i) for i in x_val_test]
error = sum([abs(y_val_f[i] - y_val_test[i]) for i in range(len(y_val_f))]) / len(y_val_f)
print('average error : {}'.format(error))
# plotting f
import matplotlib.pyplot as plt


plt.plot(x_val_test, y_val_test, label = ""test_func"")
plt.scatter(x_val_f, y_val_f, label = ""f(x)"", s = 10)
plt.xlabel('x-axis')
plt.ylabel('y-axis')
plt.ylim(-1,1) 
plt.title('Graph')
plt.legend()
plt.show()

But whenever I try making the increment smaller (to make the approximation function more accurately supposedly) python keeps giving me this error.
File ""c:/Users/username/Desktop/Curve fitting.py"", line 45, in make_polynomial
A[i].append(x_points[i] ** j)
        OverflowError: (34, 'Result too large')

Is this just because I have too many points so x_points[i] ** j becomes too big? or have I made a mistake somewhere? and even if I do make it work by making the increment larger some of the points don't match up with the sin function.
0.1 increment plot. Test_func is the sine function and f is the approximation function.
Does anyone know why this happens?
Here is one more screenshot over an increment of 0.07 within the same interval as the one in the code. 0.07 Increment plot. If there are other things that might help with this please let me know.
","In such cases, it helps to use a debugger to find out what's going wrong. Or at the very least, use try-except blocks to catch the exception and print your variables to see where your code explodes.
try:
    for i in range(degree):
        A.append([])
        for j in range(degree):
            A[i].append(x_points[i] ** j) # This is line 45 
except OverflowError:
    print(f""i = {i}; j = {j}"")

### output: 
i = 310; j = 628

In this case, your code breaks when i = 310 and j = 628. This is because x_points[i] ** j = 3.10 ** 628, which is too big to store in a double.
(Note that I specifically catch the OverflowError exception because the error message tells me that's the error that was raised. It is not a good idea to have an unfiltered except clause that catches all exceptions, because that can hide actual problems in your code and prevent other functionality such as exiting with Ctrl+C)
",data-science
nltkdownload39punkt39 giving output as false,"Here is my code:
df['num_words'] = df['text'].apply(lambda x:len(nltk.word_tokenize(x)))

which gives me the following error:
Resource punkt not found.
Please use the NLTK Downloader to obtain the resource:
   
>>> import nltk
>>> nltk.download('punkt')
  
For more information see: https://www.nltk.org/data.html

Attempted to load tokenizers/punkt/english.pickle

Then I tried to install nltk and download the file 'punkt' using nltk.download('punkt').
But I am getting this error.
I tried some alternative codes like:
import nltk
import ssl

try:
    _create_unverified_https_context = ssl._create_unverified_context
except AttributeError:
    pass
else:
    ssl._create_default_https_context = _create_unverified_https_context

nltk.download()

Also tried changing the networks as at some places I found it is saying server issue.
","Try to launch the jupyter notebooks session as administrator (open the command or anaconda prompt as administrator).
The last option would be to download the corpus manually. You may find this, helpful in your case.
",data-science
filter out rows of a dataframe containing a specific string,"I have a massive dataframe. The dataframe has column patient.drug. This column contains list of dictionaries as its elements.
I want to filter out all the rows that contain 'NIFEDIPINE' word in patient.drug column.
The dataframe is very large. Here is a sample of it.
                                                         patient.drug
0                       [{'drugcharacterization': '1', 'medicinalproduct': 'PANDOL'}]
1                       [{'drugcharacterization': '2', 'medicinalproduct': 'NIFEDIPINE'}]      
2                       [{'drugcharacterization': '3', 'medicinalproduct': 'SIMVASTATIN'}]
3                       [{'drugcharacterization': '4', 'medicinalproduct': 'NIFEDIPINE'}]      

so far, I have tried
df[df['patient.drug'].str.contains('NIFEDIPINE')]

but it is giving me an error.
 raise KeyError(f""None of [{key}] are in the [{axis_name}]"")

KeyError: ""None of [Float64Index([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n              ...\n              nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n             dtype='float64', length=12000)] are in the [columns]""

I have also tried using in operator and iterating over rows.
lst=[]
for i in range(len(df)):
    if 'NIFEDIPINE' in df.loc[i, ""patirnt.drug""]:
        lst.append(i)
print(lst)

Which is also causing an error. What should I do to get it right?
","After reproducing your Data,
>>> df
                                                         patient.drug
0  [{'drugcharacterization': '1', 'medicinalproduct': 'PANDOL'}]
1  [{'drugcharacterization': '2', 'medicinalproduct': 'NIFEDIPINE'}]
2  [{'drugcharacterization': '3', 'medicinalproduct': 'SIMVASTATIN'}]
3  [{'drugcharacterization': '3', 'medicinalproduct': 'SIMVASTATIN'}]
4  [{'drugcharacterization': '4', 'medicinalproduct': 'NIFEDIPINE'}]

While using Your code:
>>> df[df['patient.drug'].str.contains('NIFEDIPINE')]

Error:
    raise KeyError(f""None of [{key}] are in the [{axis_name}]"")
KeyError: ""None of [Float64Index([nan, nan, nan, nan, nan], dtype='float64')] are in the [columns]""

Solution:
    >>> df[df['patient.drug'].astype('str').str.contains('NIFEDIPINE')]
                                                        patient.drug
1  [{'drugcharacterization': '2', 'medicinalproduct': 'NIFEDIPINE'}]
4  [{'drugcharacterization': '4', 'medicinalproduct': 'NIFEDIPINE'}]

Note:
This is raising issue due to indexer check in the pandas indexer.py section, which is as follows:
--> pandas/core/indexing.py
# Count missing values:
missing_mask = indexer < 0
missing = (missing_mask).sum()

if missing:
    if missing == len(indexer):
        axis_name = self.obj._get_axis_name(axis)
        raise KeyError(f""None of [{key}] are in the [{axis_name}]"")

    # We (temporarily) allow for some missing keys with .loc, except in
    # some cases (e.g. setting) in which ""raise_missing"" will be False

",data-science
can ttest be calculated on large samples with nonnormal distribution,"Can t-test be calculated on large samples with non-normal distribution?
For example, the number of users in group A is 100K, the number of users in group B is 100K. I want to test whether the average session duration of these two groups is statistically significant.
1st method) We calculated the average session duration of these users on the day after the AB test (DAY1) as

31.2 min for group A
30.2 min for group B.

We know that users in groups A and B have a non-normal distribution of DAY1 session values.
In such a case, would it be correct to use two samples t-test to test the DAY1 avg session durations of two groups? (We will accept n=100K)
(Some sources say that calculating t-scores for large samples will give accurate results even with non-normal distribution.)
2nd method) Would it be a correct method to calculate the t-score over the daily average session duration during the day the AB test is open?
E.g; In the scenario below, the average daily session duration of 100K users in groups A and B are calculated. We will accept the number of days here as the number of observations and get n=30.
We will also calculate the two-sample t-test calculation over n=30.




Group
day0 avg duration
day1 avg duration
day2 avg duration
...
day30 av gduration




A
30.2
31.2
32.4
...
33.2


B
29.1
30.2
30.4
...
30.1




Do these methods give correct results or is it necessary to apply another method in such scenarios?
Would it make sense to calculate t-test on large samples in AB test?
","The t-test assumes that the means of different samples taken from a population are normally distributed. It doesn't assume that the population itself is normally distributed.
For a population with finite variance, the central limit theorem suggests that the means of samples from the population are normally distributed. However, the sample size needed for the distribution of means to be approximately normal depends on the degree of non-normalness of the population. The t-test is invalid for small samples from non-normal population distributions, but is valid for large samples from non-normal distributions.
Method 1 works because of this reason (large sample size ~100K) and you are correct that calculating t-scores for large samples will give accurate results even with non-normal distribution. [You may also consider using a z-test for the sample sizes you're working with (100K). T-tests are more appropriate for smaller sample sizes, such as n < 30]
Method 2 works because the daily averages should be normally distributed given enough samples per the central limit theorem. Time-spent datasets may be skewed but generally work well.
",data-science
converting data frame column values to float type,"My dataset has two columns, 'A' and 'B,' both of which have percentage values but are of the object datatype. For example,




A%
B%




1.x%
3.x%


2.x%
4.x%




Goal: I'm mostly interested in using this for machine learning clustering, hence my goal is to convert it to decimal form. For example, convert the '1.2%' object value to a float value of 0.012.
I tried two methods: the first was successful, but it took a long time.

I removed or stripped the object % from say '34%' using pandas.Series.str.strip to '34' obj and then converted this value to float using .to_numeric() --> 34. Now I divided this value with 100 and got the result 0.34.

However, in the second way I was attempting the below,


The function:
def Tab_to_float(z):
    return float(z.strip('%'))/100

Now when I pass the column (which is an object) as below:
Tab_to_float(df['A']) 

I get error:

AttributeError: 'Series' object has no attribute 'strip'

I tried feeding this function an int, float, numpy array, and even a dataframe, but I got the same error: 'that' object has no attribute'strip'. I'm not sure where I'm going wrong. Is there a better way to deal with such requirements? Any help is much appreciated!
","df['A'] = df.apply(lambda row : Tab_to_float(row['A']), axis = 1)

You can do this for these two columns and then you can apply this function.
We are applying a function along an axis of the DataFrame. (Here we are changing each element of a column). We are not changing anything for the Tab_to_float function in this solution.
data = {
        'A':['34.3%', '24%'],
        'B':['32%','33%'] }
 
df = pd.DataFrame(data)

 
df['A'] = df.apply(lambda row : Tab_to_float(row['A']), axis = 1)
df['B'] = df.apply(lambda row : Tab_to_float(row['B']), axis = 1)

print(df)

Outputs:
       A     B
0  0.343  0.32
1  0.240  0.33

",data-science
training machine learning in production,"Is there a way to train your machine learning model in the cloud? Or does it really have to be batch training? i.e. (Pull some data on SQL, then feed that to the model)
What i was thinking is implementing my own model from scratch, use Stochastic Gradient Descent to update the parameters for every row from the database.
","I think you are looking for something like GCP AI platform

You can use BigQuery to store your data and do some analytics and perform inbuilt ML models.
AI Platform Notebooks for manage your notebooks
Check this list for built in algorithms in GCP
Or if you have a your model, you can use cloud resources to run your model.check this link how to use GCP resources for your model

",data-science
assigning final scores to identified technologies considering users39 stackoverflow reputation badge counts post scores noof posts and post date,"I am trying to determine the importance of various factors in assigning a score for identified technologies using the user's StackOverflow post tags and content.
The considering factors are users' reputation scores, badge counts(gold, silver, bronze), post scores, number of posts, and post date. I want to give more weight to technologies identified from recent posts and give higher weight to users who have earned gold badges.
I have already identified technologies using post tags and titles, but I am having trouble assigning appropriate weights to each factor for calculating the final score for the technology expertise score. Can you suggest a way for me to assign a final score to identified technologies without assigning specific weights as I like to each factor?
","So, there's this thing called the Analytical Hierarchy Process (AHP) created by Saaty in the 70s. It's like a fancy decision-making method that helps deal with tricky problems. You know, those decisions with lots of factors... AHP helps you sort them out.
Here's how it works: First, you break down the problem into smaller parts, like categories and subcategories. This helps keep things organized. Then comes the cool part: you compare these parts to figure out which ones are more important than others. You give them scores like ""equal importance"" or ""way more important.""
To gather these scores, we made a Google Form where people say what's more important. We put all their answers in a Google Sheet, and whichever score gets the most votes wins. We do this for every part, so we know what matters most.
Then, we use math to figure out the real weight of each part. It's like a balancing act. Imagine a seesaw with weights on it. The heavier side is the one that matters more. We do this for categories and subcategories, using equations and stuff.
After all that math, we have these weights that show what really counts. We even break down subcategories like ""Badge Count"" into smaller pieces like gold, silver, and bronze badges.
So, in the end, AHP gives us a clear way to see what's important and what's not.  This helps us make smart choices and pick the most crucial factors in any decision.
",data-science
recalculate the values of the binary classification probabilities based on the threshold,"I have highly imbalanced data so for binary classification I convert probabilities for 1-class with threshold = 0.06.
I want to show probabilities to management so I need to adjust then on condition that 0.06 is my new 50% boundary.
So I want my low probability, like 0.045, 0.067, 0.01 values to be recalculated to be higher percentage.
I guess I should multuply it, but I don't know how to find the value.
data for reference
  id     probability
_____________________
168835    0.529622
168836    0.870282
168837    0.988074
180922    0.457827
78352     0.272279
            ...   
320739    0.003046
329237    0.692332
329238    0.926343
329239    0.994264
320741    0.002714

","Not sure if it is any useful after a year, but what you have to do is apply inverse function, to get back the x values, move everything left and reapply your probability function to get back the probabilities. Multiplying won't work, unless you are using linear function, which I'm guessing is not the case.
Assuming you use a standard logistic regression your code for recalculating probabilities should look something like this
import numpy as np
import pandas as pd

df = pd.DataFrame({""probability_old"":[0.529622,0.870282,0.988074,0.457827,0.272279,0.003046,0.692332,0.926343,0.994264,0.002714,0.06,0.5]})

def sig(z):
    return 1/(1+np.exp(-z))
def inv_sig(z):
    return np.log(z/(1-z))

y_0 = 0.06
# inv_sig(y_0) ≈ -2.75
df[""probability_new""] = sig(inv_sig(df[""probability_old""]) - inv_sig(y_0))

Results:




id
probability_old
probability_new




0
0.529622
0.946352


1
0.870282
0.990576


2
0.988074
0.999230


3
0.457827
0.929723


4
0.272279
0.854264


5
0.003046
0.045680


6
0.692332
0.972417


7
0.926343
0.994950


8
0.994264
0.999632


9
0.002714
0.040892


10
0.060000
0.500000


11
0.500000
0.940000




Hopefully this image will clarify the logic behind the code

",data-science
how can we map catagorical codes in a dataframe back to the original data points in the original dataframe,"I have a simple dataframe that looks like this.
import pandas as pd
 
# Intitialise data of lists
data = [{'Year': 2020, 'Airport':2000, 'Casino':5000, 'Stadium':9000, 'Size':'Small'}, 
       {'Year': 2019, 'Airport':3000, 'Casino':4000, 'Stadium':12000, 'Size':'Medium'},
       {'Year': 2018, 'Airport':5000, 'Casino':9000, 'Stadium':10000, 'Size':'Medium'},
       {'Year': 2017, 'Airport':5000, 'Casino':10000, 'Stadium':15000, 'Size':'Large'}]
df = pd.DataFrame(data)


df = df.set_index(['Year'])
df

df_fin = pd.DataFrame({col: df[col].astype('category').cat.codes for col in df}, index=df.index)
print(df_fin.columns)
df_fin


Then, I convert everything to categorical codes, like this.
df_fin = pd.DataFrame({col: df[col].astype('category').cat.codes for col in df}, index=df.index)
print(df_fin.columns)
df_fin


Then, I am doing a basic basic classification experiment, like this.
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

from sklearn.datasets import make_moons
from sklearn.metrics import accuracy_score
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier,AdaBoostClassifier,GradientBoostingClassifier

X = df_fin[['Airport', 'Casino', 'Stadium', ]]
y = df_fin['Size']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

clf = AdaBoostClassifier(n_estimators=100)
clf.fit(X_train, y_train)
y_pred = clf.predict(X_test)
accuracy_score(y_test, y_pred)

Finally, if I want to make a prediction, I can do this.
print(clf.predict([[2, 3, 3]]))

The result is '0' for size, which is what I would expect for 2017. However, I don't want to use the categorical codes, I want to use the original records from the original 'df'.
How can I make a prediction like this?
print(clf.predict([[5000,10000,15000]]))

So I can get a prediction of 'Large'. Somehow I need to map the categorical codes back to the records in the original 'df'. How can I do this?
","The best approach would be to only encode the target variable, as the features you posted are perfectly fine and can be used in a classifier without any preprocessing.
However if you want to go down the path of converting everything to categorical codes I would recommend using OrdinalEncoder for converting features and LabelEncoder for converting target, as that way it makes it easy to convert back and forth between original values and categorical codes.
from sklearn.preprocessing import LabelEncoder, OrdinalEncoder

feature_encoder = OrdinalEncoder()
target_encoder = LabelEncoder()

X = df.loc[:, ['Airport', 'Casino', 'Stadium']]
y = df.loc[:, ""Size""]

X = feature_encoder.fit_transform(X)
y = target_encoder.fit_transform(y)

# training model

Having both encoders we can make predictions for original values
ex_x = [[5000,10000,15000]]
ex_x_encoded = feature_encoder.transform(ex_x)
ex_y_encoded = clf.predict(ex_x_encoded)
ex_y = target_encoder.inverse_transform(ex_y_encoded)
print(ex_y)
# ['Large']

Or even better, wrap it all in a function
def predict_og(X):
    # 1. Encode Features
    X_encoded = feature_encoder.transform(X)

    # 2. Predict 
    y_pred_encoded = clf.predict(X_encoded)

    # 3. Decode target
    y_pred = target_encoder.inverse_transform(y_pred_encoded)

    return y_pred

",data-science
how to change a string value with comma 343543 to int in a dataframe with python,"I have a CVS file IMDb list. The number of votes is written like 345,545, and Python sees this like a string. I want to change this to a number value for using operations like <,+,% and I want to add those values in a new column.
def change_int(x):
    y = x.split("","")
    z = int(y[0] + y[1])
    return z

df[""imdbVotes2""] = df.imdbVotes.apply(change_int(df[""imdbVotes""]))

I tried to use a function like this.
I expect:
0    343,564    343564
1    676,565    676565

","Your function has a small problem. The split function returns a list of strings, but you can’t directly concatenate those strings. You need to join the string first and then convert the result to an integer.
def change_int(x):
  y = x.split("","")
  z = int("""".join(y))
  return z

This function will now take an input string like '345, 545' and return a corresponding integer 345545.
",data-science
in python pandas data frame shift values from category to rating rating to reviews  current ver to android ver,"This is my main data:

I need to change this line of data:

This is my output:

In python pandas data frame shift values from Category to Rating, Rating to Reviews, etc. Current Ver to Android Ver. App column should be constant.
I tried a lot. But its not giving me the proper output. Please help me
","You .loc:
df.loc[df[""Price""]==""Everyone"", ""Category"":""Android Ver""] = df.loc[df[""Price""]==""Everyone"", ""Category"":""Android Ver""].shift(axis=1)

",data-science
how to download xlsx file from doi link,"I want to download two files automatically from Python for a reproducible statistical analysis.
These links

https://doi.org/10.1371/journal.pone.0282068.s001
https://doi.org/10.1371/journal.pone.0282068.s002

I tried
import requests

url = 'https://doi.org/10.1371/journal.pone.0282068.s001'

response = requests.get(url)

I suspect that the file is actually the content of response.content, which appears to be a bunch of encoded information (e.g. \xe2\x81a\xe4\x1dq\xbe9~3\x94\x885\xba\xc8\x9bz\'~\x1c)X>\xaaXyg\x929\xf84\xc2\x06\t\n x5\).
How do I download these files and save them as XLSX files?
","you need to save the content into the file.
import requests

url = 'https://doi.org/10.1371/journal.pone.0282068.s001'

response = requests.get(url)
data = response.content

with open('filename.xlsx', 'wb') as f:
    f.write(data)

your content will be saved in file filename.xlsx
",data-science
distill y_true and y_pred from a confusion matrix,"I have a confusion matrix that I generated via Scikit-Learn. I was wondering if it were possible to reverse engineer it and get the y_true and y_pred given only the confusion matrix in Python. If there is nothing built in (like in Scikit-Learn), would someone mind providing a code snippet?
","Its not possible to reverse-engineer y_true and y_pred from a given confusion matrix, as the confusion matrix is invariant to permutations of the elements of  y_pred and y_true.
An easy example would be the following: Consider y_true=[0,1] and y_pred=[1, 0]. The confusion matrix is exactly the same than for y_true=[1, 0] and y_pred=[0, 1].
",data-science
what is the difference between metric and nonmetric mds for a beginner,"I am fairly new to data science and would like to know in simple words (like teaching your grandmother) what the difference between metric and non-metric Multidimensional scaling is.
I have been googling for 2 days and watching different videos and wasn't able to quite understand some of the terms people are using to describe the difference, maybe I am lacking some basic knowledge but I don't know in which area so if you have an idea of what I should have a firm understanding of before tackling this subject, I would appreciate the advice. Here is what I know:
Multidimensional scaling is a way of reducing dimensions to be able to visualize or represent data in a more friendly manner. I know that there are several ways for MDS like metric and non metric, PCA and FA (maybe FA is a part of PCA, I'm not sure).
The example I am trying to apply this on is a set of data showing different cities and attributes related to these cities. For example, on a score from 1-7 (1 lowest - 7 highest), this is the score of each city and the corresponding attribute.
          **Clean**      **Friendly**     **Expensive**     **Beautiful**          

Berlin----------- 4  --------------------- 2-----------------------5------------------------6
Geneva---------6  --------------------- 3-----------------------7------------------------7         
Paris------------ 3  --------------------- 4-----------------------6------------------------7
Barcelona----- 2  --------------------- 6-----------------------3------------------------4
How do I know if I should be using metric or non-metric MDS. Are there general rules of thumb or simple logic that I can use to decide without going deep into the technical process.
Thank you
","Well, I might not be able to give you a specific answer but a simple answer would be that metric MDS already has the input matrix in the form of distances (i.e. actual distances between cities) and therefore the distances have meaning in the input matrix and create a map of actual physical locations from those distances.
In non-metric MDS, the distances are just a representation of the rankings (i.e. high as in 7 or low as in 1) and they do not have any meaning on their own but they are needed to create the map using euclidean geometry and the map then just shows the similarity in rankings represented by distances between coordinates on the map.
",data-science
using cross_val_predict against test data set,"I'm confused about using cross_val_predict in a test data set.
I created a simple Random Forest model and used cross_val_predict to make predictions:
from sklearn.ensemble import RandomForestClassifier
from sklearn.cross_validation import cross_val_predict, KFold

lr = RandomForestClassifier(random_state=1, class_weight=""balanced"", n_estimators=25, max_depth=6)
kf = KFold(train_df.shape[0], random_state=1)
predictions = cross_val_predict(lr,train_df[features_columns], train_df[""target""], cv=kf)
predictions = pd.Series(predictions)

I'm confused on the next step here. How do I use what is learnt above to make predictions on the test data set?
","As @DmitryPolonskiy commented, the model has to be trained (with the fit method) before it can be used to predict. 
# Train the model (a.k.a. `fit` training data to it).
lr.fit(train_df[features_columns], train_df[""target""])
# Use the model to make predictions based on testing data.
y_pred = lr.predict(test_df[feature_columns])
# Compare the predicted y values to actual y values.
accuracy = (y_pred == test_df[""target""]).mean()

cross_val_predict is a method of cross validation, which lets you determine the accuracy of your model. Take a look at sklearn's cross-validation page.
",data-science
how to create multiple plots,"I'm to Python and learning it by doing. I want to make two plots with matplotlib in Python. The second plot keeps the limits of first one. Wonder how I can change the limits of each next plot from previous. Any help, please. What is the recommended method? 
X1 = [80, 100, 120, 140, 160, 180, 200, 220, 240, 260]
Y1 = [70, 65, 90, 95, 110, 115, 120, 140, 155, 150]

from matplotlib import pyplot as plt
plt.plot(
    X1
  , Y1
  , color = ""green""
  , marker = ""o""
  , linestyle = ""solid""
)
plt.show()


X2 = [80, 100, 120, 140, 160, 180, 200]
Y2 = [70, 65, 90, 95, 110, 115, 120]

plt.plot(
    X2
  , Y2
  , color = ""green""
  , marker = ""o""
  , linestyle = ""solid""
)
plt.show()

","There are two ways:
The quick and easy way; set the x and y limits in each plot to what you want.
plt.xlim(60,200)
plt.ylim(60,200)

(for example). Just paste those two lines just before both plt.show() and they'll be the same.
The harder, but better way and this is using subplots.
# create a figure object    
fig = plt.figure()
# create two axes within the figure and arrange them on the grid 1x2
ax1 = fig.add_Subplot(121)
# ax2 is the second set of axes so it is 1x2, 2nd plot (hence 122)
# they won't have the same limits this way because they are set up as separate objects, whereas in your example they are the same object that is being re-purposed each time!
ax2 = fig.add_Subplot(122)

ax1.plot(X1,Y1)
ax2.plot(X2,Y2)

",data-science
matplotlib plotting all columns on the xaxis and values on the yaxis grouped by a third variable,"I have data that looks like this:
group    var1    var2    var3
0        0.6    0.001    0.11  
1       -0.36  -0.007   -0.066481 

I want to create a dot graph where the 3 different variables form the x-axis and the y-axis represent the values in the data frame, with different symbols/colors for the dots depending on the group on the leftmost column.
I tried melting the data into the format
group var  value
0     var1 0.6
1     var1 -0.36
0     var2 0.001
1     var2 -0.007
0     var3 0.11
1     var3 -0.066481

But I was only able to make separate plots by variable instead of one big plot. Any help would be appreciated, thank you!
","Here is the code that plots based on two values and each group has specific color:
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

# Sample data
data = {
    'group': [0, 1],
    'var1': [0.6, -0.36],
    'var2': [0.001, -0.007],
    'var3': [0.11, -0.066481]
}

df = pd.DataFrame(data)

# Reshape the data using melt
df_melted = df.melt(id_vars='group', var_name='variable', value_name='value')

# Create the dot plot using seaborn
sns.set(style=""ticks"")
sns.stripplot(x='variable', y='value', hue='group', data=df_melted, dodge=True, size=10, palette=""Set2"")

# Set labels and title
plt.xlabel('Variable')
plt.ylabel('Value')
plt.title('Dot Graph')

# Display the plot
plt.show()

",data-science
why cant i use dfloc within the drop method,"I’m just getting started on pandas and I was trying to delete a sequence of rows.
In my data frame, the index is the province names.
Since df.loc[‘NL’:’QC’] returns a list of rows, I thought I would feed that as an input to my drop function to specify which rows I want deleted, but it doesn’t work.
Can someone help me understand why? Thank you.
This is what I wrote
df.drop(df.loc[‘NL’:’QC’])

It threw an error that said could not find ‘province’ , ‘province_name’ etc on axis.
These are the names of my columns.
I know that by default the drop() will have axis =0 , so I’m not sure why it’s parsing through columns? Or why it “can’t” find it.
If I change axis = 1, it works and all my columns disappear.
Just my indices remain from NL to QC and nothing else.
Apologies if I’m not explaining it the best, I’m just starting out and may not be using the best words to describe my problem.
Appreciate the help, thank you.
","pandas drop function gets an input

""Index or column labels to drop. A tuple will be used as a single label and not treated as a list-like.""

Therefore you should pass the indices and not the rows -
df.drop(df.loc['NL':'QC'].index)
See more here
",data-science
python  pandas shift entities of a row to the right end,"I have the following data frame (number of ""Date"" columns can vary):
Customer  Date1  Date2  Date3  Date4
0        A     10   40.0    NaN   60.0
1        B     20   50.0    NaN    NaN
2        C     30    NaN    NaN    NaN
If there is a ""NaN"" in the last column (as said, number of columns can vary), I want to right shift all the columns to the end of the data frame such that it then looks like this:
Customer  Date1  Date2  Date3  Date4
0        A     10   40.0    NaN   60.0
1        B    NaN    NaN     20   50.0
2        C    NaN    NaN    NaN     30
All the values which remain empty can be set to NaN.
How can I do that in Python?
I tried this code but didn't work:
import numpy as np
import pandas as pd

data = {
    'Customer': ['A', 'B', 'C'],
    'Date1': [10, 20, 30],
    'Date2': [40, 50, np.nan],
    'Date3': [np.nan, np.nan, np.nan],
    'Date4': [60, np.nan, np.nan]
}

df = pd.DataFrame(data)


for i in range(1, len(df.columns)):
    df.iloc[:, i] = df.iloc[:, i-1].shift(fill_value=np.nan)

print(df)

","You can temporarily set the non-target columns as index (or drop them), then push the non-NaNs to the right with sorting, and only update the rows that are matching a specific mask (here NaN in the last column):
out = (df
   .set_index('Customer', append=True)
   .pipe(lambda d: d.mask(d.iloc[:, -1].isna(),
                          d.transform(lambda x : sorted(x, key=pd.notnull), axis=1)
                         )
        )
   .reset_index('Customer')
)

Alternative:
other_cols = ['Customer']
out = df.drop(columns=other_cols)
m = out.iloc[:, -1].isna()
out.loc[m, :] = out.loc[m, :].transform(lambda x : sorted(x, key=pd.notnull), axis=1)
out = df[other_cols].join(out)[df.columns]

NB. there are several methods to shift non-NaNs, here is one, but non-sorting based methods are possible if this is a bottleneck.
Output:
  Customer  Date1  Date2  Date3  Date4
0        A   10.0   40.0    NaN   60.0
1        B    NaN    NaN   20.0   50.0
2        C    NaN    NaN    NaN   30.0

",data-science
how to calculate time differences without a date and only with times,"import pandas as pd

stoptimes_df = pd.DataFrame({
    'trip_id': ['1', '1', '1', '2', '2', '2'], 
    'arrival_time': [""12:10:00"", ""12:20:00"", ""12:30:00"", ""27:32:00"", ""27:39:00"", ""27:45:00""],
    'departure_time': [""12:10:00"", ""12:20:00"", ""12:30:00"", ""27:32:00"", ""27:39:00"", ""27:45:00""],
    'stop_id': ['de:08437:48835:0:2', 'de:08426:6306', 'de:08426:6307', 'de:08116:6703', 'de:08116:3821', 'de:08415:28256:0:1']})

I have this dataframe given, which shows different bus lines (trip_id) and the different stops, and I would like to insert a new column which contains the difference between the arrival time of the following line and the departure time of the line before. Unfortunately I am not able to do this because when I change the datatype to datetime.time() I can not calculate which the times. This is only possible if I use the datatype datetime.datetime(), but then I have also a date in the columns ""arrival_time"" and ""departure time"" written, like ""1900-01-01 12:10:00"", which I do not want. I have a similar problem when I use timedelta. So the point is I want to keep only the times without a date in the two given columns and in the new column there should be the time difference in minutes or seconds. For example in the last line it should say in the new column 6 (min) or 300 (sec). Does someone know how to do this?
What I did so far in code:
def convert_to_datetime(time):
    hours, minutes, seconds = map(int, time.split(':'))
    hours = hours % 24  # change time format to 0-24 hours
    time = str(hours) + ':' + str(minutes) + ':' + str(seconds)
    time = datetime.strptime(time,""%H:%M:%S"").time()
    # time_delta = timedelta(hours=hours, minutes=minutes, seconds=seconds)
    return time

stoptimes_df['arrival_time'] = stoptimes_df['arrival_time'].apply(convert_to_datetime)
stoptimes_df['departure_time'] = stoptimes_df['departure_time'].apply(convert_to_datetime)
stoptimes_df

# tried first with only one column to calculate
stoptimes_df['time_btw_stops'] = stoptimes_df.groupby('trip_id')['arrival_time'].diff()
stoptimes_df

This leads to the following error:

TypeError: unsupported operand type(s) for -: 'datetime.time' and
'datetime.time'

","Since you are missing date and your time go over 24-periods which I cannot use pd.Timestamp, I would consider using a simple equation.
df = pd.DataFrame([['12:10:00'], 
                   ['12:20:00'], 
                   ['12:23:00'], 
                   ['12:29:00'], 
                   ['27:11:00']
                  ], columns=['arrival_time'])

# Convert the time string into seconds.
df['sec'] = df.arrival_time.str[0:2].astype('int') * 60 * 60  # hour
            + df.arrival_time.str[3:5].astype('int') * 60  # minute
            + df.arrival_time.str[6:8].astype('int'). # second

# Then you can do diff to calculate time diff.

# seconds diff
df.sec.diff()

# minutes diff
df.sec.diff() // 60 

",data-science
scikitlearn applying an arbitary function as part of a pipeline,"I've just discovered the Pipeline feature of scikit-learn, and I find it very useful for testing different combinations of preprocessing steps before training my model. 
A pipeline is a chain of objects that implement the fit and transform methods. Now, if I wanted to add a new preprocessing step, I used to write a  class that inherits from sklearn.base.estimator. However, I'm thinking that there must be a simpler method. Do I really need to wrap every function I want to apply in an estimator class? 
Example:
class Categorizer(sklearn.base.BaseEstimator):
    """"""
    Converts given columns into pandas dtype 'category'.
    """"""

    def __init__(self, columns):
        self.columns = columns

    def fit(self, X, y):
        return self


    def transform(self, X):
        for column in self.columns:
            X[column] = X[column].astype(""category"")
        return X

","For a general solution (working for many other use cases, not just transformers, but also simple models etc.), you can write your own decorator if you have state-free functions (which do not implement fit), for example by doing:
class TransformerWrapper(sklearn.base.BaseEstimator):

    def __init__(self, func):
        self._func = func

    def fit(self, *args, **kwargs):
        return self

    def transform(self, X, *args, **kwargs):
        return self._func(X, *args, **kwargs)

and now you can do
@TransformerWrapper
def foo(x):
  return x*2

which is the equivalent of doing
def foo(x):
  return x*2

foo = TransformerWrapper(foo)

which is what sklearn.preprocessing.FunctionTransformer is doing under the hood.
Personally I find decorating simpler, since you have a nice separation of your preprocessors from the rest of the code, but it is up to you which path to follow.
In fact you should be able to decorate with sklearn function by
from sklearn.preprocessing import FunctionTransformer

@FunctionTransformer
def foo(x):
  return x*2

too.
",data-science
how to simplify quotget mean ages for women of every classquot,"I have the famous titanic dataset, called fr1.
The class column contains the following: 1, 2, 3
The gender column: male, female
I want to obtain the mean ages for females for each class, I think It should be a simpler way to code the following, imagine for example that there are 20 classes instead of 3:
c3 = fr1[fr1[""class""] ==3]
c3w = fr1[fr1[""gender""] =='female']
print(round(c3w[""Age""].mean()))

","You can filter the females, then pandas groupby method and apply aggregation function to each group (=for each class):
fr1 = pd.DataFrame({
    ""class"": [""1"", ""2"", ""2""],
    ""gender"": [""female"", ""female"", ""female""],
    ""age"": [30, 25, 18]
})

fr1[fr1[""gender""] == ""female""] \
    .groupby(""class"", as_index=False)[""age""] \
    .agg(lambda x: round(np.mean(x)))

Output:
  class  age
0     1   30
1     2   22

It could be even simpler, if you skipped the rounding part.
fr1[fr1[""gender""] == ""female""] \
    .groupby(""class"", as_index=False)[""age""] \
    .mean()

",data-science
extractingremovingskipping free text from json files in azure data factory,"I am trying to do some source transformation in ADF where I have alot server logs in the following format:
#PartnerName    QA Server
#ApplicationName    T_GSPClient
#AccountName    DoNotModifyDMS
#SDK    desktop
#ClientVersion  5.1.1894.3
#InputChannel   DesktopMic
#User   JohnDoe
#NmsLogin   JohnDoe
#SessionId  7ba732d6-3445-4b16-b7e8-345fgd4f5g4
#ClientIP   209.122.69.109
#SRTechnology   S2
#SROptions  NoTextBefore
#GeneralLogLevel    Trace
#ModuleLogLevels    
#ServerDateTimeUTC  2023-07-06 15:28:33.105
{""date"":""2023-07-06\t15:28:09.653"",""level"":""TRACE"",""msg"":""DMVAServerMessage-Initialize:{&quot;dmhMessage&quot;:{&quot;messageHeader&quot;:{&quot;messageType&quot;:&quot;unsubscribe&quot;,&quot;sessionId&quot;:&quot;00000000-0000-0000-0000-000000000000&quot;,&quot;messageId&quot;:&quot;aeb65e55-a7c0-4e96-a960-dc2a252d6b2c&quot;,&quot;transactionType&quot;:&quot;acknowledgement&quot;,&quot;clientType&quot;:&quot;ttsChannel&quot;,&quot;version&quot;:&quot;1.0&quot;,&quot;application&quot;:&quot;DesktopSDK&quot;},&quot;messageResponse&quot;:{&quot;resultCode&quot;:&quot;SERVER_ERROR&quot;,&quot;errorMessage&quot;:&quot;The subscription id does not exist, unsubscribe did not remove an entry&quot;}}} "",""traceId"":""409e0d44-ad50-4f17-84c7-0521e01e11fc"",""spanId"":""e0750e44-ad50-4f17-90a9-3b6940e0294b"",""resource"":{""module"":"".NET"",""class"":""Nuance.SpeechAnywhere.Internal.DMVA.DMVAServerMessage"",""function"":""Initialize"",""line"":70,""pid"":26612,""thread"":""[28-27280]""}}
{""date"":""2023-07-06\t15:28:09.653"",""level"":""TRACE"",""msg"":""DMVAServerMessage-Initialize:{&quot;dmhMessage&quot;:{&quot;messageHeader&quot;:{&quot;messageType&quot;:&quot;unsubscribe&quot;,&quot;sessionId&quot;:&quot;00000000-0000-0000-0000-000000000000&quot;,&quot;messageId&quot;:&quot;aeb65e55-a7c0-4e96-a960-dc2a252d6b2c&quot;,&quot;transactionType&quot;:&quot;acknowledgement&quot;,&quot;clientType&quot;:&quot;ttsChannel&quot;,&quot;version&quot;:&quot;1.0&quot;,&quot;application&quot;:&quot;DesktopSDK&quot;},&quot;messageResponse&quot;:{&quot;resultCode&quot;:&quot;SERVER_ERROR&quot;,&quot;errorMessage&quot;:&quot;The subscription id does not exist, unsubscribe did not remove an entry&quot;}}} "",""traceId"":""409e0d44-ad50-4f17-84c7-0521e01e11fc"",""spanId"":""e0750e44-ad50-4f17-90a9-3b6940e0294b"",""resource"":{""module"":"".NET"",""class"":""Nuance.SpeechAnywhere.Internal.DMVA.DMVAServerMessage"",""function"":""Initialize"",""line"":70,""pid"":26612,""thread"":""[28-27280]""}}
{""date"":""2023-07-06\t15:28:09.653"",""level"":""TRACE"",""msg"":""DMVAServerMessage-Initialize:{&quot;dmhMessage&quot;:{&quot;messageHeader&quot;:{&quot;messageType&quot;:&quot;unsubscribe&quot;,&quot;sessionId&quot;:&quot;00000000-0000-0000-0000-000000000000&quot;,&quot;messageId&quot;:&quot;aeb65e55-a7c0-4e96-a960-dc2a252d6b2c&quot;,&quot;transactionType&quot;:&quot;acknowledgement&quot;,&quot;clientType&quot;:&quot;ttsChannel&quot;,&quot;version&quot;:&quot;1.0&quot;,&quot;application&quot;:&quot;DesktopSDK&quot;},&quot;messageResponse&quot;:{&quot;resultCode&quot;:&quot;SERVER_ERROR&quot;,&quot;errorMessage&quot;:&quot;The subscription id does not exist, unsubscribe did not remove an entry&quot;}}} "",""traceId"":""409e0d44-ad50-4f17-84c7-0521e01e11fc"",""spanId"":""e0750e44-ad50-4f17-90a9-3b6940e0294b"",""resource"":{""module"":"".NET"",""class"":""Nuance.SpeechAnywhere.Internal.DMVA.DMVAServerMessage"",""function"":""Initialize"",""line"":70,""pid"":26612,""thread"":""[28-27280]""}}
{""date"":""2023-07-06\t15:28:09.653"",""level"":""TRACE"",""msg"":""DMVAServerMessage-Initialize:{&quot;dmhMessage&quot;:{&quot;messageHeader&quot;:{&quot;messageType&quot;:&quot;unsubscribe&quot;,&quot;sessionId&quot;:&quot;00000000-0000-0000-0000-000000000000&quot;,&quot;messageId&quot;:&quot;aeb65e55-a7c0-4e96-a960-dc2a252d6b2c&quot;,&quot;transactionType&quot;:&quot;acknowledgement&quot;,&quot;clientType&quot;:&quot;ttsChannel&quot;,&quot;version&quot;:&quot;1.0&quot;,&quot;application&quot;:&quot;DesktopSDK&quot;},&quot;messageResponse&quot;:{&quot;resultCode&quot;:&quot;SERVER_ERROR&quot;,&quot;errorMessage&quot;:&quot;The subscription id does not exist, unsubscribe did not remove an entry&quot;}}} "",""traceId"":""409e0d44-ad50-4f17-84c7-0521e01e11fc"",""spanId"":""e0750e44-ad50-4f17-90a9-3b6940e0294b"",""resource"":{""module"":"".NET"",""class"":""Nuance.SpeechAnywhere.Internal.DMVA.DMVAServerMessage"",""function"":""Initialize"",""line"":70,""pid"":26612,""thread"":""[28-27280]""}}

My objective is remove/skip the first free text lines and keep the other json data and then move it to another blob for further transformation. I tried source transformation with derived column but my Data Flow is still showing json format error. I also used Copy Data activity via ForEach with ""enableSkipIncompatibleRow"": true, it doesn't work. It only works if I just work with single file, not when I try to iterate over many files and skip/remove those lines.
","In order to remove the first few lines which are not in Json format using ADF dataflow, follow the below approach.

Take the source transformation with the delimitted source dataset . Take any column delimitter which is not part of the text.



Here, I took the tilde ~ as column delimitter. So, the entire row is in the same column.



Then take the filter transformation and give the filter on condition as, substring(Column_name,1,1)=='{'. This removes all the rows which do not begin with {.



Then take the sink transformation and set the filename option to Output to single file and give the filename. In optimize, select single partition.


Here, I took sink file as delimited dataset with column delimiter as No delimitter and Quote character as No quote character.

Data in Output file

",data-science
difference between predict and predict_proba functions in scikit learn,"Greetings data science community! How's going? So, I'm studying classification Tree and scikit-learning and during my studyings i come across this ""issue"":
After training a tree (clf = DecisionTreeClassifier()) and training it (clf.fit(Xtrain, ytrain)) i have decided to test its performance on the training data itself (just to compare, later, with the test data, in terms of Sensitivity Specificity and ROC-AUC).
But instead to only apply the predict() I also applied the predict_proba() on the X_train data.
As you can se by the image, the observation 4 has 50 % of probability to give zero and 50%  to give one (according to predict_proba() function) however the predict() function classified it as zero
Image with the dataframe where the first column is the result from predict_proba() function and the second column is the result from predict() column
Did the predict() function sort as ZERO by ""chance"" or since it's zero or one, does it sort as zero because it comes first (as if order matters)?
I could not solve my doubts when analyzing the documentation of the functions (source: https://github.com/scikit-learn/scikit-learn/blob/7f9bad99d/sklearn/tree/\_classes.py#L476)
Thanks in advance!
","In case of binary classification the returned class is computed as follows
proba = self.tree_.predict(X)
...
return self.classes_.take(np.argmax(proba, axis=1), axis=0)

Reference: sklearn code
So basicaly the choice falls on numpy.argmax.
Let's look in the numpy documentation and read the following:

Notes:
In case of multiple occurrences of the maximum values, the indices corresponding to the first occurrence are returned.

So the final answer - in case of equal probabilities the first class is chosen always, which in case of binary classification corresponds to the negative label.
",data-science
how to get lab streaming layer to work in c,"I have a device that records live data like eeg measurements. I want to send these data with a lab streaming layer (lsl). Unfortunately I'm unable to configure the lsl and get it to run. I'm not sure if I even copied the dll to the right places.
What I did so far:

downloaded recent lsl dll from latest release page

copied the liblsl64.lib and liblsl64.dll to my project folder and the Debug folder of my project

copied LSL.cs (from github as described in C# Bindings) into my project: renamed const string libname = ""lsl""; to const string libname = ""liblsl64"";

when I run the code:
liblsl.StreamInfo info = new liblsl.StreamInfo(""TestCSharp"",""EEG"");


I get a System.BadImageFormatException: HRESULT: 0x8007000B
Can anyone help me with this?
","I get the System.BadImageFormatException, because I have ""Any CPU"" instead of ""x64"" in my configurations manager. It also works when using a .Net Core application for accessing the library.
",data-science
how to merge columns with same value and add values together from different columns using python,"enter image description here
Hi, I'm taking a data science course and not familiar with python.
I was wondering if it is possible to merge the same years together and adding the global sales for each year togehter?
after googling, I tried using .groupby but that didn't work, and I also tried .aggregate and it also failed.
maybe I'm using the commands wrong, or maybe I'm using the wrong commands for the task, because I'm very new to this I am not sure
","Just use .groupby and .sum.
import pandas as pd
df = pd.DataFrame({'Year': [2006, 2006, 2007],
  'Global_Sales': [0.7,0.8,0.9],
  'Other data': [1, 2, 3]})

So a test DataFrame like:
    Year    Global_Sales    Other data
0   2006    0.7             1
1   2006    0.8             2
2   2007    0.9             3

We can then .groupby and .sum and extract just the Global_Sales column:
merged = df.groupby('Year').sum()['Global_Sales']
merged.reset_index()

this gives:
    Year    Global_Sales
0   2006    1.5
1   2007    0.9

The .reset_index takes the ""grouped column"" which is 'Year' here, and upgrades it from an index in the resulting DataFrame returned from .sum() back to a column.
Update: to remove the warning about numeric_only, just pre-select the relevant columns so that .sum() does not try other columns:
I.e.
df[['Year', 'Global_Sales']].groupby('Year').sum().reset_index()

will give the same:
    Year    Global_Sales
0   2006    1.5
1   2007    0.9

",data-science
how to convert pandas dataframe to the shape of a correlation matrix,"I have a pandas dataframe which looks vaguely like this:
Out[130]: 
     xvar            yvar                   meanRsquared
0    filled_water    precip                 0.119730
1    filled_water    snow                   0.113214
2    filled_water    filled_wetland         0.119529
3    filled_wetland  precip                 0.104826
4    filled_wetland  snow                   0.121540
5    filled_wetland  filled_water           0.121540
[676 rows x 3 columns]


I would like to transform it's shape into a more traditional correlation matrix, where the columns and the index are the variables, and the values are the meanRsquared.
Is there any easy way to do this? I've been playing around for an hour and can't figure out how I could do this.
DISCLAIMER: Yes, I know pandas has a built in function for creating a correlation matrix. However my current df is the average of hundreds of correlation matrices over many watersheds, so I cannot use that.
This is my best attempt, but obviously the logic failed towards the end.
listOfdicts = []
for xvar in df['xvar'].unique():
    for yvar in df['yvar'].unique():
        adict = {}
        adict['index'] = xvar 
        adict[yvar] = yvar
        adict['r'] = df['insert r value here']
        listOfdicts.append(adict)
answer = pd.Dataframe.from_dict(listOfdicts)

I don't expect this to work, but this was my best shot.
","You need to look at pivot method (https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.pivot.html).
import pandas as pd
df =pd.DataFrame(
    data={
        'xvar': ['filled_water', 'filled_water', 'filled_water',
                 'filled_wetland', 'filled_wetland', 'filled_wetland'],   

        'yvar':['precip','snow','filled_wetland',                  
                'precip','snow','filled_water' ], 
        'meanRsquared':[1,2,3,4,5,6] 
    }, index=range(6)
)

df.pivot(index='xvar', columns='yvar', values='meanRsquared')

Output:
    yvar            filled_water  filled_wetland  precip  snow
xvar                                                      
filled_water             NaN             3.0     1.0   2.0
filled_wetland           6.0             NaN     4.0   5.0

",data-science
how to avoid nan values when i use frame39colum39mapdict,"I have the following dataset frame1




Color
Item




Red
Shirt


White
Shoes


Yellow
Shirt


Green
Shoes




I want to set all the colors for Shoes item to be ""Blue"", I use map


x = {""Shoes"": ""Blue""}
fr1[""Color""] = fr1[""Item""].map(x)

I expected the following result




Color
Item




Red
Shirt


Blue
Shoes


Yellow
Shirt


Blue
Shoes




Instead I got this




Color
Item




NaN
Shirt


Blue
Shoes


NaN
Shirt


Blue
Shoes



","Either only use the rows with a valid key with boolean indexing:
x = {""Shoes"": ""Blue""}
m = fr1[""Item""].isin(list(x))
fr1.loc[m, ""Color""] = fr1.loc[m, ""Item""].map(x)

Or fillna:
fr1[""Color""] = fr1[""Item""].map(x).fillna(fr1[""Color""])

",data-science
combine 2 df with a for loop to make projections,"I have 2 data frames, both with strings, dates, and numbers. df1 is data for 2020 and df2 is data for 2021-2025. I will use df2 (column H) as a growth rate on df1. I need to multiply all numbers of df1 (columns D, E, F) by df2 (H[i,]) for each year in df2, from 2021 to 2025.
I have structured a function, but I am still thinking about how to address it.
Could you please check my code and provide me with some ideas to complete the function?
I really appreciate your help.
df1 <- read.csv(""df1.csv"", check.names=FALSE)
df2 <- read.csv(""df2.csv"", check.names=FALSE)

df1:
A   B   year    D   E   F
abc ab  2020    0   1   2
def cd  2020    3   4   0
ghi ef  2020    0   5   6
jkl gh  2020    7   8   0
mno ij  2020    0   9   10

df2: 
year    H
2021    1.1
2022    1.2
2023    1.3
2024    1.4
2025    1.5

df3 <- data.frame()
for (i in 1:length(df2)){
  df3 = rbind(df1, df2 %>% 
        mutate(df1$all_columns_with_numbers = all_columns_with_numbers[i,] * df2$H[i,] ))
}
df3

A    B     C    D    E      F
abc ab  2021    0    1.1    2.2
abc ab  2022    3.6  4.8    0
abc ab  2023    0    6.5    7.8
abc ab  2024    9.8  11.2   0
abc ab  2025    0    13.5   15
def cd  2021    …    …      …

","It sounds like what you're wanting to do is a kind of cross join, followed by mutiplying the columns D, E, and F by H. That is shown below:
library(tidyverse)

df1 %>%
    select(-year) %>%
    cross_join(df2) %>%
    # multiply columns D, E, F by H
    mutate(across(c(D, E, F), ~ . * H)) %>%
    select(-H)

# A tibble: 25 × 6
   A     B         D     E     F  year
   <chr> <chr> <dbl> <dbl> <dbl> <int>
 1 abc   ab      0     1.1   2.2  2021
 2 abc   ab      0     1.2   2.4  2022
 3 abc   ab      0     1.3   2.6  2023
 4 abc   ab      0     1.4   2.8  2024
 5 abc   ab      0     1.5   3    2025
 6 def   cd      3.3   4.4   0    2021
 7 def   cd      3.6   4.8   0    2022
 8 def   cd      3.9   5.2   0    2023
 9 def   cd      4.2   5.6   0    2024
10 def   cd      4.5   6     0    2025
# ℹ 15 more rows

Though, after speaking with Onyambu, and reading through your question again, I'm now unsure! Maybe what you want is some way of getting D, E, and F multiplied by H for each year, something like this:
grow_year <- function(df) {
    df %>%
    mutate(year = year + 1) %>%
    left_join(df2, by = ""year"") %>%
    mutate(across(c(D, E, F), ~ . * H)) %>%
    select(-H)
}
y21 <- grow_year(df1)
y22 <- grow_year(y21)
y23 <- grow_year(y22)
y24 <- grow_year(y23)
y25 <- grow_year(y24)

out <- bind_rows(df1, y21, y22, y23, y24, y25)

# A tibble: 30 × 6
   A     B      year     D     E     F
   <chr> <chr> <dbl> <dbl> <dbl> <dbl>
 1 abc   ab     2020   0     1     2  
 2 def   cd     2020   3     4     0  
 3 ghi   ef     2020   0     5     6  
 4 jkl   gh     2020   7     8     0  
 5 mno   ij     2020   0     9    10  
 6 abc   ab     2021   0     1.1   2.2
 7 def   cd     2021   3.3   4.4   0  
 8 ghi   ef     2021   0     5.5   6.6
 9 jkl   gh     2021   7.7   8.8   0  
10 mno   ij     2021   0     9.9  11  
# ℹ 20 more rows

",data-science
selecting row with highest value based on two different columns,"I have a dataframe with 3 columns:
I want to make a rule that if for a same city and same id, pick the maximum value and drop the row with lower value.
eg:




City
ID
Value




London
1
12.45


Amsterdam
1
14.56


Paris
1
16.89


New York
1
23.86


Chicago
1
14.56


Chicago
1
20.76




Expected Output : Select highest value for same city and ID.
Here Chicago has 2 entries with same ID, I want to select the row with highest value.
Expected Output




City
ID
Value




London
1
12.45


Amsterdam
1
14.56


Paris
1
16.89


New York
1
23.86


Chicago
1
20.76



","Sort the dataframe by Value in descending order and drop the duplicates:
df = df.sort_values(by=['Value'], ascending=False).drop_duplicates(['City', 'ID'], keep='first')
print(df)

Prints:
        City  ID  Value
3   New York   1  23.86
5    Chicago   1  20.76
2      Paris   1  16.89
1  Amsterdam   1  14.56
0     London   1  12.45

",data-science
getting an error from hdbscan while importing bertopic,"I'm trying to import bertopic but it gives the following error. I tried different versions and re create a new environment. But it's still same. I'm using Apple M2 Pro processor




lib
version




BERTopic
0.15.0


HDBSCAN
0.8.29


umap-learn
0.5.3 




TypeError                                 Traceback (most recent call last)
Cell In[3], line 4
      2 import pandas as pd
      3 # import matplotlib.pyplot as plt
----> 4 from bertopic import BERTopic
      5 import gensim
      6 import gensim.corpora as corpora

File ~/miniforge3/envs/bertopic/lib/python3.8/site-packages/bertopic/__init__.py:1
----> 1 from bertopic._bertopic import BERTopic
      3 __version__ = ""0.15.0""
      5 __all__ = [
      6     ""BERTopic"",
      7 ]

File ~/miniforge3/envs/bertopic/lib/python3.8/site-packages/bertopic/_bertopic.py:37
     34 from typing import List, Tuple, Union, Mapping, Any, Callable, Iterable
     36 # Models
---> 37 import hdbscan
     38 from umap import UMAP
     39 from sklearn.preprocessing import normalize

File ~/miniforge3/envs/bertopic/lib/python3.8/site-packages/hdbscan/__init__.py:1
----> 1 from .hdbscan_ import HDBSCAN, hdbscan
      2 from .robust_single_linkage_ import RobustSingleLinkage, robust_single_linkage
      3 from .validity import validity_index

File ~/miniforge3/envs/bertopic/lib/python3.8/site-packages/hdbscan/hdbscan_.py:40
     37 from .plots import CondensedTree, SingleLinkageTree, MinimumSpanningTree
     38 from .prediction import PredictionData
---> 40 FAST_METRICS = KDTree.valid_metrics + BallTree.valid_metrics + [""cosine"", ""arccos""]
     42 # Author: Leland McInnes <leland.mcinnes@gmail.com>
     43 #         Steve Astels <sastels@gmail.com>
     44 #         John Healy <jchealy@gmail.com>
     45 #
     46 # License: BSD 3 clause
     47 from numpy import isclose

TypeError: unsupported operand type(s) for +: 'builtin_function_or_method' and 'builtin_function_or_method'

","This is tracked in a Github issue and is due to a change in scikit-learn.
To avoid this bug, you could install scikit-learn < 1.3.0. (pip install -U scikit-learn==1.2.2) The creator of HDBScan has a fix in mind, so likely at some point this will work with later scikit-learn versions.
",data-science
how to get company earning announcements data api,"I want to get real-time earning announcements data API.
I tried yfinance but it doesn't work currently.
So is there any other replacement API?
Most of API need to pay money, But I just want to try for my personal projects so it's little pressured
Does Yahoo Finance no longer offer API? I heard that yfinance is not yahoo finance API.
I tried yfinance, yahooquery, other API's
I want real-time earning announcements data api
","Alphavantage: https://www.alphavantage.co/#page-top
Alphavantage has an awesome API with finance information and best part is it is completely free to get an API key.
import requests

api_key = ''#your api key

symbol = 'AAPL'# symbol of stock/company

url = f'https://www.alphavantage.co/query?function=EARNINGS&symbol={symbol}&apikey={api_key}'

response = requests.get(url)

if response.status_code == 200:
    data = response.json()

    earnings_announcements = data['quarterlyEarnings']

    for announcement in earnings_announcements:
        report_date = announcement['reportedDate']
        eps_estimate = announcement['estimatedEPS']
        eps_actual = announcement['reportedEPS']
        fiscal_period = announcement['fiscalDateEnding']

        print(f""Report Date: {report_date}"")
        print(f""EPS Estimate: {eps_estimate}"")
        print(f""EPS Actual: {eps_actual}"")
        print(f""Fiscal Period: {fiscal_period}"")
        print()
else:
    print(""Error occurred while fetching earnings announcements."")

",data-science
every movie is from 1970 what did i do wrong,"getDecade <- function(year) {
  case_when(
    year < 1900 ~ ""1890s"",
    year >= 1900 && year < 1910 ~ ""1900s"",
    year >= 1910 && year < 1920 ~ ""1910s"",
    year >= 1920 && year < 1930 ~ ""1920s"",
    year >= 1930 && year < 1940 ~ ""1930s"",
    year >= 1940 && year < 1950 ~ ""1940s"",
    year >= 1950 && year < 1960 ~ ""1950s"",
    year >= 1960 && year < 1970 ~ ""1960s"",
    year >= 1970 && year < 1980 ~ ""1970s"",
    year >= 1980 && year < 1990 ~ ""1980s"",
    year >= 1990 && year < 2000 ~ ""1990s"",
    year >= 2000 && year < 2010 ~ ""2000s"",
    TRUE ~ ""other""
  )
}

# (b) Check that your decade function works as intended.
movies <- ggplot2movies::movies %>% 
  mutate (decade = getDecade(year))
print(unique(movies$decade))

The output is:
[1] ""1970s"" ""1890s""
Why is almost every movie from 1970s??? What is happening???
","As Ric said, the problem was &&. See here for more info
Good code:
getDecade <- function(year) {
  case_when(
    year < 1900 ~ ""1890s"",
    year >= 1900 & year < 1910 ~ ""1900s"",
    year >= 1910 & year < 1920 ~ ""1910s"",
    year >= 1920 & year < 1930 ~ ""1920s"",
    year >= 1930 & year < 1940 ~ ""1930s"",
    year >= 1940 & year < 1950 ~ ""1940s"",
    year >= 1950 & year < 1960 ~ ""1950s"",
    year >= 1960 & year < 1970 ~ ""1960s"",
    year >= 1970 & year < 1980 ~ ""1970s"",
    year >= 1980 & year < 1990 ~ ""1980s"",
    year >= 1990 & year < 2000 ~ ""1990s"",
    year >= 2000 & year < 2010 ~ ""2000s"",
    TRUE ~ ""other""
  )
}

Better code:
movies %>% 
  mutate(decade = cut(year, breaks = seq(1890, 2010, 10), include.lowest = TRUE, labels = paste0(seq(1890, 2000, 10), ""s"")))

Credit to Konrad for the cut idea
",data-science
suggesting multivariate ratios with bounds in optuna,"I'm currently working with Optuna and I'm trying to suggest ratios that are bounded by multiple variables. Specifically, I'm dealing with ratios X_1, X_2, ..., X_k that are bounded by ∑X_i = 1 and 0 <= X_i <= 1 for all i.
Unfortunately, Optuna does not provide a Dirichlet distribution, which would be the ideal solution for this problem.
I've tried the following approach, where I iteratively suggest each ratio and subtract it from the remaining total. However, this does not work as expected:
def objective(trial):
    k = 10
    ratios = np.zeros(k)
    
    residual = 1
    for i in range(k - 1):
        ratios[i] = trial.suggest_float(f'ratio_{i}', 0, residual)
        residual -= ratios[i]
        
#     ratios[k - 1] = trial.suggest_float(f'ratio_{k - 1}', residual, residual)
    ratios[k - 1] = residual
    return np.log(ratios).sum()

study = optuna.create_study(direction='maximize')
study.optimize(objective, n_trials=20)

I also tried another approach where I suggest all ratios independently and then normalize them. This approach does not produce any errors, but it suggests k times, even though the degree of freedom is k - 1, which is inconsistent:
def objective(trial):
    k = 10
    ratios = np.zeros(k)
    
    for i in range(k):
        ratios[i] = trial.suggest_float(f'ratio_{i}', 0, 1)
    
    ratios /= ratios.sum()
    return np.log(ratios).sum()

study = optuna.create_study(direction='maximize')
study.optimize(objective, n_trials=20)

I'm looking for a way to suggest a ratio that is bounded by multiple variables. While the above examples are simple and differentiable, I need to deal with a more complex objective function that requires variables.
","The issue was resolved by creating an Objective class that maintains a maximum value (self.max) for the ratio suggestions. This maximum value is updated in each trial to be the average of the current maximum and the maximum ratio suggested in the current trial. This approach ensures that the suggested ratios are gradually adjusted towards the optimal values.
Here's the corrected code:
class Objective:
    def __init__(self):
        self.max = 1
    def __call__(self, trial):
        k = 10
        ratios = np.zeros(k)

        for i in range(k):
            ratios[i] = trial.suggest_float(f'ratio_{i}', 0, self.max)

        ratios /= ratios.sum()
        self.max = (self.max + ratios.max()) / 2
        return np.log(ratios).sum()

study = optuna.create_study(direction='maximize')
study.optimize(Objective(), n_trials=100)

In this code, the Objective class is instantiated and passed to the study.optimize method. The __call__ method of the Objective class is used as the objective function for the optimization. This method suggests the ratios, normalizes them, updates the maximum value for the suggestions, and returns the sum of the logarithms of the ratios.
",data-science
do clustering on dataset but don39t divide items with the same label,"I have a dataset with 10k+ points. Some of the data is labeled into 100+ labels. The rest is to classify.
I cannot classify the new data directly as the number of labels is too high and the result doesn't look good enough.
So I want to learn clustering algorithm on the labeled dataset and divide roughly into 3-8 bins. I don't care which labels go into which group, but I want them to be concise. So the new data will be assigned into the correct cluster. And later on classified within model trained on a single bin.
Question: How to cluster dataset but force the algorithm to keep the different points in the same label in the same cluster?
Data:




Id
Label
Feature 1
Feature 2
[...]




1
A
1
3
...


2
A
7
7
...


3
B
10
9
...


4
B
50
11
...


5
C
91
15
...


6
C
31
17
...


7
D
0
19
...


...
...
...
...
...




Expected outcome:




Id
Label
Cluster




1
A
1


2
A
1


3
B
2


4
B
2


5
C
1


6
C
1


7
D
2


...
...
...




I was trying to find a way to include the division in the loss function
","You can use KMeans from scikit-learn to achieve this, let us say your dataframe is df something like this:
from sklearn.cluster import KMeans

X = df.drop(columns=['Label']).values

kmeans = KMeans(n_clusters=3, random_state=0, n_init=""auto"").fit(X)
df['cluster'] = kmeans.labels_
print(df) # df should contain label and cluster for all the datapoints

Mixing of classes will happen no matter what clustering algorithm you use on the whole dataset. The other Idea is that you somehow embed all the datapoints of one class to a single vector by doing that you will have only 100+ vectors each representing one class after that cluster these 100+ vectors to any number of clusters you desire, then you will have no mixing.
",data-science
how to convert the nonmonday column to monday in pandas dataframe,"I want to write a function that if a date is Tue.Wed.Tur. then convert it as this week's Monday, if a date is  Fri. Sat. Sun. then convert it as next week's Monday. If its a Monday, just leave as it is.
The example input Dataframe is like this:




Model#
Order Category
2022/4/18
2022/5/10
2022/5/18
2022/5/26
2022/6/24
2022/7/16
2022/7/24




A
Open
0
0
0
0
1
1
2


B
Close
1
1
0
0
1
2
2


C
Open
0
0
1
1
0
1
2




The ideal output is like this:
All the date headers are converted to Monday based on the rules I specified.




Model#
Order Category
2022/4/18
2022/5/9
2022/5/16
2022/5/23
2022/6/27
2022/7/18
2022/7/25




A
Open
0
0
0
0
1
1
2


B
Close
1
1
0
0
1
2
2


C
Open
0
0
1
1
0
1
2




Really appreciate your help!
","You can use:
import numpy as np

# Convert as datetime and extract day of week
dates = pd.to_datetime(df.columns[2:])
dow = dates.day_of_week

# Create timedelta Mon, Thu, Wed, Thu are negative, Fri, Sat, Sun are positive
offsets = pd.to_timedelta(np.where(dow < 4, -dow, 7-dow), unit='D')

# Adjust the dates then reindex your columns
dates = (dates + offsets).strftime('%Y/%-m/%-d')
df.columns = df.columns[:2].append(dates)

Output:




Model#
Order Category
2022/4/18
2022/5/9
2022/5/16
2022/5/23
2022/6/27
2022/7/18
2022/7/25




A
Open
0
0
0
0
1
1
2


B
Close
1
1
0
0
1
2
2


C
Open
0
0
1
1
0
1
2




Intermediates:




Before
Offset
After




2022-04-18
0 days
2022-04-18


2022-05-10
-1 days
2022-05-09


2022-05-18
-2 days
2022-05-16


2022-05-26
-3 days
2022-05-23


2022-06-24
3 days
2022-06-27


2022-07-16
2 days
2022-07-18


2022-07-24
1 days
2022-07-25



",data-science
how do i combine my dataframes while iterating through rows and adding a new column everytime,"I have a raw dataframe, similar to below:
Original DF I have




index
text




0
i am happy today ...


1
i am confused because ...


2
i would love to do ...


...
...


1000000
i am exhausted about ...




So I have to run all these texts through different models which each produce a score. Thereafter, I need to combine them into one dataframe as below:
Processed DF i want




index
text
score_1
score_2
score 3




0
i am happy today ...
0.2
0.4
0.238


1
i am confused because ...
0.8
0.3
0.64


2
i would love to do ...
0.67
0.546
0.35


...
...
...
...
...


1000000
i am exhausted about ...
0.21
0.41
0.8




So i have to load individual models for each one (which isn't instant) and because there are so many rows, I have to split it up into batches (of 100 for example). After that I have to combine my dataframes. My code is something like this:
full_df = pd.read_csv('fulldf.csv')
batch_size = 100
num_batches = len(full_df)/100 # assume it's a round number
df_list = []
new_df = []

for i in range(num_batches): 
    # Breaking up the main dataframe
    df_list.append(full_df.iloc[i*batch_size:(i+1)*batch_size]

for model in list_of_models:
    model.load() # Time consuming step so I only do it once per model
    for df in df_list:
        df = df.reset_index()
        # Some code to generate scores for each row of df subset
        df['score_' + model_number] = score
        df.reset_index(drop = True, inplace = True)
        new_df.append(df)

total_df = pd.concat(new_df)

However, the results appear somewhat incorrectly.
DF I am getting with above code




index
text
score_1
score_2
score 3




0
i am happy today ...
0.2
NA
NA


1
i am confused because ...
0.8
NA
NA


...
...
...
...
...


1000000
i am exhausted about ...
0.21
NA
NA


0
i am happy today ...
NA
0.4
NA


1
i am confused because ...
NA
0.3
NA


...
...
...
...
...


1000000
i am exhausted about ...
NA
0.41
NA


0
i am happy today ...
NA
NA
0.238


1
i am confused because ...
NA
NA
0.64


...
...
...
...
...


1000000
i am exhausted about ...
NA
NA
0.8




As you can see, the numbers are correctly aligned to the index but the rows basically repeat 3 times (or more times if there are more 'scores').
I have the constraints that cannot load all the rows into memory at once so I have to do them in batches. Moreover, I cannot load a model, do 100 rows, then load another model and do the same 100 rows again as this takes too long due to model loading time).
I Have tried several solutions, such as adding `total_df = pd.concat(new_df, axis = 1) to the concat, but that doesn't work as it just appends sideways.
Is there any way to fix this and get the desired result?
","Ok so I managed to derive a solution which works for me and gives me the desired output.
The basic logic behind it is that I create multiple temporary dataframes to hold different data portions.

I create 2 empty lists, df_list and midway_df
I split up the testset dataframe into batches and place them into df_list
I iterate through each model i want to run the data through
I create another empty list, predicted_dataframes
I further iterate through the whole test dataframe (in batches as per df_list) and derive the scores for each row of the test dataframe subset.
I append this scored dataframe to the predicted_dataframes list.
After finishing inference for the whole model, I combine into one model-specific dataframe i.e. a df which contains scores for a model for the whole test set.
Append the combined model-specific dataframe to the midway_df list
Repeat for every model
Once done, concat midway_df's dataframes into one. This will result in a messy dataframe with many repeating column names. This needs to be filtered down.
Get the columns of the scores
Get the columns except the scores (only from the first dataframe to avoid repeats)
Filter big dataframe to only contain these columns.

full_df = pd.read_csv('fulldf.csv')
batch_size = 100
num_batches = len(full_df)/100 # assume it's a round number

df_list = []
midway_df = []

for i in range(num_batches): 
    # Breaking up the main dataframe
    df_list.append(full_df.iloc[i*per_batch:(i+1)*per_batch])

for model in list_of_models:
    model.load() # Time consuming step so I only do it once per model
    predicted_dataframes = [] # Captures the predictions for each model
    for df in df_list:
        df = df.reset_index()
        # Some code to generate scores for each row of df subset
        df['score_' + str(model.name)] = scores
        df.reset_index(drop = True, inplace = True)

        # After model has completed on one subset of full test dataset
        predicted_dataframes.append(df)

    # After one model has completed for all rows of the test dataset
    temp_df = pd.concat(predicted_dataframes)
    midway_df.append(temp_df) # Question: Should I be doing deepcopy somewhere here?

# After all models have completed and all scores have been saved in midway_df
total_df = pd.concat(midway_df, axis = 1)
score_cols = [f'score_{model.name}' for model.name in list_of_models]
standard_cols = [col for col in midway_df[0].columns if col not in score_cols]
total_df = pd.concat([midway_df[0][standard_cols], total_df[score_cols]], axis = 1)
total_df = reset_index(drop = True, inplace = True)

",data-science
how does one create a pandas column binary based on whether or not another column contains dates,"I am analyzing customer churn. My dataset contains years of customers that have stayed or left. I need to create a ['CHURN_FLAG'] column based on whether or not there is a date in the ['CHURN_DATE'] column. If there is a date in the ['CHURN_DATE'] column then the customer churned.
Current data frame:




CHURNdate




2023-1-1


NaT




Desired:




CHURNdate
CHURNflag




2023-1-1
1


NaT
0




I created a column ['TODAY_DATE'] and have attempted to solve by assessing if the ['CHURNdate'] < ['TODAY_DATE'] then the binary 1 would populate, else 0. Here is the code:
df2['CHURNflag'] = np.where(df2['CHURNdate']<df2['TODAY_DATE'], 0, 1)

Naturally, it didn't work. :( The datatypes are datetime64
","
churn['CHURNflag'] = np.where(churn['CHURNdate'].isna(), 0, 1)

Out[24]: 
    CHURNdate  CHURNflag
0  2023-01-01          1
1         NaN          0

",data-science
why does the kstest give a pvalue of 1 if the distribution is different,"Let's take two sets:
a = [5,5,5,5,5,4,4,4,4,3,3,3,2,2,1]
b = [5,4,3,2,1]

We perform the KS-Test using Python:
from scipy import stats
stats.ks_2samp(b,a)
KstestResult(statistic=0.2, pvalue=0.9979360165118678, statistic_location=2, statistic_sign=1)

Why is the result a p-value of 0.9979? This means that the distribution of the values in the two sets is almost identical. But it's not! What do I missunderstand?
Kind regards.
","The observed value of the KS test statistic, namely 0.2, is actually relatively small, considering the distribution of the test statistic for a reasonable null hypothesis; I think this is where the surprise is coming from.
As mentioned, the usual KS test assumes there are no ties, so we'll have to compute the p-value ourselves. We can make progress by assuming the null hypothesis is that both samples come from a uniform distribution and estimating the p-value by random sampling. (This null hypothesis is more restrictive than the conventional one which just assume the same distribution, not necessarily uniform.)
Here are a few lines of R code to estimate the p-value. In the interest of brevity, it's specific to the problem as stated: a sample of size 5 and a sample of size 15, each one from a uniform distribution on the set { 1, 2, 3, 4, 5 }, and the observed KS test statistic is 0.2.
my.ecdf <- function (x) cumsum (sapply (1:5, function (k) sum (x == k))/length(x))

R <- function (n) sample.int (5, size = n, replace = T)

generate.ks.test.statistic <- function (n) sapply (1:n, function (k) max (abs (my.ecdf (R (15)) - my.ecdf (R (5)))))

ks <- generate.ks.test.statistic (10000)

sum (ks >= 0.2)/10000

For this last input, I get 0.8243. That's not as extreme as the value you reported (more than 0.99), but still enough to show that 0.2 is actually relatively small. You can look at hist(ks) to see what the distribution looks like.
",data-science
pythonmatplotlib plot vertical line for specific dates in line chart,"I used pandas to read a CSV file of the stock data of Tesla and plotted a line chart of the price in 2021. Also, I have  dates = [] with a collection of dates, which I want to plot in the same line chart with vertical lines.
My code:
import pandas as pd
import matplotlib.pyplot as plt


""""""
Stock Data
""""""
path_to_stock_file = ""/path/to/file/data/TSLA-USD.csv""
stock_df = pd.read_csv(path_to_stock_file)
in_year = stock_df[stock_df[""Date""].str.contains(""2021"")]

dates = [] # this list consists of a collection of dates 

""""""
Plot
""""""
in_year[""Close""].plot()
plt.figure(figsize=(100, 10))
in_year.index = in_year[""Date""]
plt.plot(in_year.index, in_year[""Close""])

for d in dates:
    plt.axvline(x=f""{d}"", color=""green"")


plt.xlabel(""date"")
plt.ylabel(""$ price"")
plt.xticks(rotation=45)
plt.title(""Tesla Stock Price 2021"")
plt.show()

Data used for plot:
# stock date/price for line chart
{
    ""2021-01-04"": 729.77002,
    ""2021-01-05"": 735.109985,
    ""2021-01-06"": 755.97998,
    ""2021-01-07"": 816.039978,
    ""2021-01-08"": 880.02002,
    ""2021-01-11"": 811.190002,
    ""2021-01-12"": 849.440002,
    ""2021-01-13"": 854.409973,
    ""2021-01-14"": 845.0,
    ""2021-01-15"": 826.159973,
    ""2021-01-19"": 844.549988,
    ""2021-01-20"": 850.450012,
    ""2021-01-21"": 844.98999,
    ""2021-01-22"": 846.640015,
    ""2021-01-25"": 880.799988,
    ""2021-01-26"": 883.090027,
    ""2021-01-27"": 864.159973,
    ""2021-01-28"": 835.429993,
    ""2021-01-29"": 793.530029,
    ""2021-02-01"": 839.809998,
    ""2021-02-02"": 872.789978,
    ""2021-02-03"": 854.690002,
    ""2021-02-04"": 849.98999,
    ""2021-02-05"": 852.22998,
    ""2021-02-08"": 863.419983,
    ""2021-02-09"": 849.460022,
    ""2021-02-10"": 804.820007,
    ""2021-02-11"": 811.659973,
    ""2021-02-12"": 816.119995,
    ""2021-02-16"": 796.219971,
    ""2021-02-17"": 798.150024,
    ""2021-02-18"": 787.380005,
    ""2021-02-19"": 781.299988,
    ""2021-02-22"": 714.5,
    ""2021-02-23"": 698.840027,
    ""2021-02-24"": 742.02002,
    ""2021-02-25"": 682.219971,
    ""2021-02-26"": 675.5,
    ""2021-03-01"": 718.429993,
    ""2021-03-02"": 686.440002,
    ""2021-03-03"": 653.200012,
    ""2021-03-04"": 621.440002,
    ""2021-03-05"": 597.950012,
    ""2021-03-08"": 563.0,
    ""2021-03-09"": 673.580017,
    ""2021-03-10"": 668.059998,
    ""2021-03-11"": 699.599976,
    ""2021-03-12"": 693.72998,
    ""2021-03-15"": 707.940002,
    ""2021-03-16"": 676.880005,
    ""2021-03-17"": 701.809998,
    ""2021-03-18"": 653.159973,
    ""2021-03-19"": 654.869995,
    ""2021-03-22"": 670.0,
    ""2021-03-23"": 662.159973,
    ""2021-03-24"": 630.27002,
    ""2021-03-25"": 640.390015,
    ""2021-03-26"": 618.710022,
    ""2021-03-29"": 611.289978,
    ""2021-03-30"": 635.619995,
    ""2021-03-31"": 667.929993,
    ""2021-04-01"": 661.75,
    ""2021-04-05"": 691.049988,
    ""2021-04-06"": 691.619995,
    ""2021-04-07"": 670.969971,
    ""2021-04-08"": 683.799988,
    ""2021-04-09"": 677.02002,
    ""2021-04-12"": 701.97998,
    ""2021-04-13"": 762.320007,
    ""2021-04-14"": 732.22998,
    ""2021-04-15"": 738.849976,
    ""2021-04-16"": 739.780029,
    ""2021-04-19"": 714.630005,
    ""2021-04-20"": 718.98999,
    ""2021-04-21"": 744.119995,
    ""2021-04-22"": 719.690002,
    ""2021-04-23"": 729.400024,
    ""2021-04-26"": 738.200012,
    ""2021-04-27"": 704.73999,
    ""2021-04-28"": 694.400024,
    ""2021-04-29"": 677.0,
    ""2021-04-30"": 709.440002,
    ""2021-05-03"": 684.900024,
    ""2021-05-04"": 673.599976,
    ""2021-05-05"": 670.940002,
    ""2021-05-06"": 663.539978,
    ""2021-05-07"": 672.369995,
    ""2021-05-10"": 629.039978,
    ""2021-05-11"": 617.200012,
    ""2021-05-12"": 589.890015,
    ""2021-05-13"": 571.690002,
    ""2021-05-14"": 589.73999,
    ""2021-05-17"": 576.830017,
    ""2021-05-18"": 577.869995,
    ""2021-05-19"": 563.460022,
    ""2021-05-20"": 586.780029,
    ""2021-05-21"": 580.880005,
    ""2021-05-24"": 606.440002,
    ""2021-05-25"": 604.690002,
    ""2021-05-26"": 619.130005,
    ""2021-05-27"": 630.849976,
    ""2021-05-28"": 625.219971,
    ""2021-06-01"": 623.900024,
    ""2021-06-02"": 605.119995,
    ""2021-06-03"": 572.840027,
    ""2021-06-04"": 599.049988,
    ""2021-06-07"": 605.130005,
    ""2021-06-08"": 603.590027,
    ""2021-06-09"": 598.780029,
    ""2021-06-10"": 610.119995,
    ""2021-06-11"": 609.890015,
    ""2021-06-14"": 617.690002,
    ""2021-06-15"": 599.359985,
    ""2021-06-16"": 604.869995,
    ""2021-06-17"": 616.599976,
    ""2021-06-18"": 623.309998,
    ""2021-06-21"": 620.830017,
    ""2021-06-22"": 623.710022,
    ""2021-06-23"": 656.570007,
    ""2021-06-24"": 679.820007,
    ""2021-06-25"": 671.869995,
    ""2021-06-28"": 688.719971,
    ""2021-06-29"": 680.76001,
    ""2021-06-30"": 679.700012,
    ""2021-07-01"": 677.919983,
    ""2021-07-02"": 678.900024,
    ""2021-07-06"": 659.580017,
    ""2021-07-07"": 644.650024,
    ""2021-07-08"": 652.809998,
    ""2021-07-09"": 656.950012,
    ""2021-07-12"": 685.700012,
    ""2021-07-13"": 668.539978,
    ""2021-07-14"": 653.380005,
    ""2021-07-15"": 650.599976,
    ""2021-07-16"": 644.219971,
    ""2021-07-19"": 646.219971,
    ""2021-07-20"": 660.5,
    ""2021-07-21"": 655.289978,
    ""2021-07-22"": 649.26001,
    ""2021-07-23"": 643.380005,
    ""2021-07-26"": 657.619995,
    ""2021-07-27"": 644.780029,
    ""2021-07-28"": 646.97998,
    ""2021-07-29"": 677.349976,
    ""2021-07-30"": 687.200012,
    ""2021-08-02"": 709.669983,
    ""2021-08-03"": 709.73999,
    ""2021-08-04"": 710.919983,
    ""2021-08-05"": 714.630005,
    ""2021-08-06"": 699.099976,
    ""2021-08-09"": 713.76001,
    ""2021-08-10"": 709.98999,
    ""2021-08-11"": 707.820007,
    ""2021-08-12"": 722.25,
    ""2021-08-13"": 717.169983,
    ""2021-08-16"": 686.169983,
    ""2021-08-17"": 665.710022,
    ""2021-08-18"": 688.98999,
    ""2021-08-19"": 673.469971,
    ""2021-08-20"": 680.26001,
    ""2021-08-23"": 706.299988,
    ""2021-08-24"": 708.48999,
    ""2021-08-25"": 711.200012,
    ""2021-08-26"": 701.159973,
    ""2021-08-27"": 711.919983,
    ""2021-08-30"": 730.909973,
    ""2021-08-31"": 735.719971,
    ""2021-09-01"": 734.090027,
    ""2021-09-02"": 732.390015,
    ""2021-09-03"": 733.570007,
    ""2021-09-07"": 752.919983,
    ""2021-09-08"": 753.869995,
    ""2021-09-09"": 754.859985,
    ""2021-09-10"": 736.27002,
    ""2021-09-13"": 743.0,
    ""2021-09-14"": 744.48999,
    ""2021-09-15"": 755.830017,
    ""2021-09-16"": 756.98999,
    ""2021-09-17"": 759.48999,
    ""2021-09-20"": 730.169983,
    ""2021-09-21"": 739.380005,
    ""2021-09-22"": 751.940002,
    ""2021-09-23"": 753.640015,
    ""2021-09-24"": 774.390015,
    ""2021-09-27"": 791.359985,
    ""2021-09-28"": 777.559998,
    ""2021-09-29"": 781.309998,
    ""2021-09-30"": 775.47998,
    ""2021-10-01"": 775.219971,
    ""2021-10-04"": 781.530029,
    ""2021-10-05"": 780.590027,
    ""2021-10-06"": 782.75,
    ""2021-10-07"": 793.609985,
    ""2021-10-08"": 785.48999,
    ""2021-10-11"": 791.940002,
    ""2021-10-12"": 805.719971,
    ""2021-10-13"": 811.080017,
    ""2021-10-14"": 818.320007,
    ""2021-10-15"": 843.030029,
    ""2021-10-18"": 870.109985,
    ""2021-10-19"": 864.27002,
    ""2021-10-20"": 865.799988,
    ""2021-10-21"": 894.0,
    ""2021-10-22"": 909.679993,
    ""2021-10-25"": 1024.859985,
    ""2021-10-26"": 1018.429993,
    ""2021-10-27"": 1037.859985,
    ""2021-10-28"": 1077.040039,
    ""2021-10-29"": 1114.0,
    ""2021-11-01"": 1208.589966,
    ""2021-11-02"": 1172.0,
    ""2021-11-03"": 1213.859985,
    ""2021-11-04"": 1229.910034,
    ""2021-11-05"": 1222.089966,
    ""2021-11-08"": 1162.939941,
    ""2021-11-09"": 1023.5,
    ""2021-11-10"": 1067.949951,
    ""2021-11-11"": 1063.51001,
    ""2021-11-12"": 1033.420044,
    ""2021-11-15"": 1013.390015,
    ""2021-11-16"": 1054.72998,
    ""2021-11-17"": 1089.01001,
    ""2021-11-18"": 1096.380005,
    ""2021-11-19"": 1137.060059,
    ""2021-11-22"": 1156.869995,
    ""2021-11-23"": 1109.030029,
    ""2021-11-24"": 1116.0,
    ""2021-11-26"": 1081.920044,
    ""2021-11-29"": 1136.98999,
    ""2021-11-30"": 1144.76001,
    ""2021-12-01"": 1095.0,
    ""2021-12-02"": 1084.599976,
    ""2021-12-03"": 1014.969971,
    ""2021-12-06"": 1009.01001,
    ""2021-12-07"": 1051.75,
    ""2021-12-08"": 1068.959961,
    ""2021-12-09"": 1003.799988,
    ""2021-12-10"": 1017.030029,
    ""2021-12-13"": 966.409973,
    ""2021-12-14"": 958.51001,
    ""2021-12-15"": 975.98999,
    ""2021-12-16"": 926.919983,
    ""2021-12-17"": 932.570007,
    ""2021-12-20"": 899.940002,
    ""2021-12-21"": 938.530029,
    ""2021-12-22"": 1008.869995,
    ""2021-12-23"": 1067.0,
    ""2021-12-27"": 1093.939941,
    ""2021-12-28"": 1088.469971,
    ""2021-12-29"": 1086.189941,
    ""2021-12-30"": 1070.339966,
    ""2021-12-31"": 1056.780029,
}

# specific dates for vertical lines
[
    ""2021-12-24"",
    ""2021-12-23"",
    ""2021-12-23"",
    ""2021-12-23"",
    ""2021-12-18"",
    ""2021-12-14"",
    ""2021-12-06"",
    ""2021-12-01"",
    ""2021-11-06"",
    ""2021-11-01"",
    ""2021-10-08"",
    ""2021-10-08"",
    ""2021-10-02"",
    ""2021-09-10"",
    ""2021-09-10"",
    ""2021-08-05"",
    ""2021-07-29"",
    ""2021-07-10"",
    ""2021-07-02"",
    ""2021-06-24"",
    ""2021-06-21"",
    ""2021-06-11"",
    ""2021-05-20"",
    ""2021-05-19"",
    ""2021-05-13"",
    ""2021-05-11"",
    ""2021-04-22"",
    ""2021-04-22"",
    ""2021-04-22"",
    ""2021-04-18"",
    ""2021-04-15"",
    ""2021-04-08"",
    ""2021-03-24"",
    ""2021-03-24"",
    ""2021-03-14"",
    ""2021-03-07"",
    ""2021-03-06"",
    ""2021-02-07"",
    ""2021-01-02"",
]

Resulting chart:

As you can see, only the dates from dates = [], which also have an associated date in the stock data have been plotted properly. All dates from dates = [], which doesn't have an associated date on the x-axis (stock price dates) have been plotted at the end of the chart.
Since the stock market has trading hours the data will always have some gaps with no date/price available.
Does anyone have an idea how I can solve the problem otherwise?
","Create a DataFrame with a DatetimeIndex (not just strings), but you can safely leave the dates corresponding to vertical lines as strings of format yyyy-mm-dd. Your plotting code can be cleaned up somewhat, which may have been part of the issue.
Setup
Based on your sample data, where plot_dict is your dictionary of plot values and dates is your list of date strings:
in_year = pd.DataFrame.from_dict(plot_dict, orient='index')
in_year.index = pd.to_datetime(in_year.index)

Modified Plotting Code
import pandas as pd
import matplotlib.pyplot as plt

in_year.plot(legend=None)

for d in dates:
    plt.axvline(x=f""{d}"", color=""green"")

plt.xlabel(""date"")
plt.ylabel(""$ price"")
plt.xticks(rotation=45)
plt.title(""Tesla Stock Price 2021"")
plt.show()

Result

",data-science
how to iterate through a website that shows no index in the url,"I'm trying to get data from ""https://chainplay.gg/"" but I'm stuck and I can't get data other than that on page one.
This is the code I wrote and it returns the .csv I'm looking for but, without getting me the other 44 pages of data I can't do much with it:
from bs4 import BeautifulSoup as bs 
import json
import csv

r = requests.get('https://chainplay.gg/') 

soup = bs(r.content)

#print(soup.prettify()) 

data = json.loads(soup.find('script', id=""__NEXT_DATA__"", type=""application/json"").text) 
#print(data)
data = data[""props""][""pageProps""][""game""]
#print(data)

json_object = json.dumps(data, indent=4)
 
with open(""games.json"", ""w"") as outfile:
    outfile.write(json_object)

with open('games.json') as json_file:
    data = json.load(json_file)
 
games = data['games']
 
data_file = open('data_file.csv', 'w')
 
csv_writer = csv.writer(data_file)

count = 0
 
for game in games:
    if count == 0:
 
        # Writing headers of CSV file
        header = game.keys()
        csv_writer.writerow(header)
        count += 1
 
    # Writing data of CSV file
    csv_writer.writerow(game.values())

data_file.close()

The one above is only the code that I know for sure is working, every other attempt resulted in empty dataframes/continuous error generation.
I tried using Selenium and looking or looking for pages indexes but to no avail. Is there someone that could please help me?
","If you examine the network traffic when you access the site you can extract the POST requests and find the pageNumber parameter that you can use to iterate over the pages.
Try this:
import requests

json_data = {
    'pageNumber': 1, # <- change this to retrieve other pages
    'pageSize': 50,
}

response = requests.post('https://api.chainplay.gg/api/app/project/get-games', 
                         json=json_data)

df = pd.DataFrame(response.json()[""payload""][""games""])
df.head()

Output
code    name    rank    imageUrl    symbol  blockChains genres  platforms   price   price24hChangePercent   priceChangePercentage7dInCurrency   priceChangePercentage1hInCurrency   volume24h   marketCap   sparkLine7dUrl  dateAdded   sparkLine7d isWatched   isInMainWatchList   mainWatchListId
0   the-sandbox The Sandbox 1   https://tk-storage.s3.ap-southeast-1.amazonaws...   sand    [{'name': 'Ethereum', 'code': 'ethereum', 'ext...   [{'name': 'Metaverse', 'code': 'metaverse'}]    [{'name': 'Windows', 'code': 'windows'}]    0.381557    0.51693 -33.477796  -0.601724   176221778   706710369   https://tk-storage.s3.ap-southeast-1.amazonaws...   2021-11-01T07:43:44Z    None    False   False   None
1   decentraland    Decentraland    2   https://tk-storage.s3.ap-southeast-1.amazonaws...   mana    [{'name': 'Ethereum', 'code': 'ethereum', 'ext...   [{'name': 'Metaverse', 'code': 'metaverse'}]    [{'name': 'Browser', 'code': 'browser'}]    0.341267    1.70391 -30.828218  -0.409165   79353426    621542744   https://tk-storage.s3.ap-southeast-1.amazonaws...   2021-11-01T07:43:44Z    None    False   False   None
2   axie-infinity   Axie Infinity

",data-science
getting a modulenotfounderror with librosa,"I am trying to load the audio files into the NumPy array using this code
#%%
import librosa
import matplotlib.pyplot as plt
import IPython.display as ipd
import os, os.path
import time
import joblib
import numpy as np

#%%
fname = 'archive\\Actor_01\\03-01-01-01-01-01-01.wav'

data, sampling_rate = librosa.load(fname)
plt.figure(figsize=(15, 5))
librosa.display.waveshow(data, sr=sampling_rate)

ipd.Audio(fname)

# %%
lst = []

for subdir, dirs, files in os.walk('archive'):
    for file in files:
        try:
            X, sample_rate = librosa.load(os.path.join(subdir, file), res_type='kaiser_fast')
            mfccs = np.mean(librosa.feature.mfcc(y=X, sr=sample_rate, n_mfcc=40).T, axis=0)
            file_class = int(file[7:8]) - 1
            arr = mfccs, file_class
            lst.append(arr)
        except ValueError as err:
            print(err)
            continue

Everything runs fine except I am getting an error at line 25
ModuleNotFoundError                       Traceback (most recent call last)
c:\Users\powellt1\Documents\COMP5500\NonGitSED\algorithm.py in line 7
      23 for file in files:
      24     try:
----> 25         X, sample_rate = librosa.load(os.path.join(subdir, file), res_type='kaiser_fast')
      26         mfccs = np.mean(librosa.feature.mfcc(y=X, sr=sample_rate, n_mfcc=40).T, axis=0)
      27         file_class = int(file[7:8]) - 1
File c:\Users\powellt1\AppData\Local\Programs\Python\Python311\Lib\site-packages\librosa\core\audio.py:193, in load(path, sr, mono, offset, duration, dtype, res_type)
    190     y = to_mono(y)
    192 if sr is not None:
--> 193     y = resample(y, orig_sr=sr_native, target_sr=sr, res_type=res_type)
    195 else:
    196     sr = sr_native

File c:\Users\powellt1\AppData\Local\Programs\Python\Python311\Lib\site-packages\librosa\core\audio.py:684, in resample(y, orig_sr, target_sr, res_type, fix, scale, axis, **kwargs)
    675     y_hat = np.apply_along_axis(
    676         soxr.resample,
    677         axis=axis,
   (...)
    681         quality=res_type,
    682     )
    683 else:
--> 684     y_hat = resampy.resample(y, orig_sr, target_sr, filter=res_type, axis=axis)

I know I have librosa installed properly and it works in the previous cells.
I've tried to reinstall and verify the installation of librosa and I've tried using
from librosa.core import load

but nothing seems to be working to fix the error.
","I figured out after coming through the documentation that you have to install resampy package separately from the librosa package itself. After installing the package I was able to get it to work.
",data-science
why does the algorithm sometimes not behave as intended,"we are currently working on a college project. We have been tasked to optimize the maintenance schedule for repairs on bikes from a bike sharing service. The bikes can only be rented from and returned to bike docking stations. We need to calculate the idle time, which is defined as follows:

the time period from when bike with bike id = x was dropped off at station y until any other bike at station y is booked

We implemented our solution as follows:
import pandas as pd
csv_file = '../Data_Cleanup/outCSV/Clean_Metro_Set.csv'
metro = pd.read_csv(csv_file)
metro['start_time'] = pd.to_datetime(metro['start_time'])
metro ['end_time'] = pd.to_datetime(metro['end_time'])
metro = metro.sort_values(by='start_time')
metro['idle_time'] = None

BigDict = {
    # station_id: {
    #     bike_id: (transaction_id ,end_time)
    # }
}

for i, row in metro.iterrows():
    current_start_time = row[""start_time""]
    current_end_time = row[""end_time""]
    current_end_station_id = row[""end_station_id""]
    current_start_station_id = row[""start_station_id""]
    current_bike_id = row[""bike_id""]
    current_index = i

    if current_start_station_id in BigDict:
        for bike in list(BigDict[current_start_station_id]):  # Create a copy of the keys
            idle_time = current_start_time - BigDict[current_start_station_id][bike][1]
            metro.at[BigDict[current_start_station_id][bike][0], ""idle_time""] = idle_time
            if idle_time.total_seconds() >= 0:
                del BigDict[current_start_station_id][bike]

    if current_end_station_id not in BigDict:
        BigDict[current_end_station_id] = {current_bike_id: (current_index, current_end_time)}

    BigDict[current_end_station_id][current_bike_id] = (current_index, current_end_time)

metro.to_csv('../Data_Cleanup/outCSV/Metro_Set_with_IdleTime.csv')

The Input data looks like this:
input data
Expected output:
expected output
Although some of the values don't get calculated correctly.
E.g.
error 1
error 2
As you can see, in the first picture there is a row with a negative idle time. Because we sorted the dataframe by end time, we sometimes run into the issue that a transaction at a later row has an earlier start time than the end time of the previous transaction(c.f error 1). In this case the idle time should be updated whenever transaction meets the following two conditions:

the end_station_id of the transaction, for which the idle time is being calculated, is the same as the start station id of the transaction, over which the for loop is currently iterating.
the transaction, over which the for loop is currently iterating, has a later start time than the end_time of the transaction, for which we calulate the idle time

In the error above(c.f. screen snippets) this does not occur and we cannot figure out the reason. Any help would be appreciated
","We managed to fix it ourselves with the following teaks:
BigDict = {
# station_id: {
#     transaction_id: end_time
# }
}
 
for i, row in metro.iterrows():
    current_start_time = row[""start_time""]
    current_end_time = row[""end_time""]
    current_end_station_id = row[""end_station_id""]
    current_start_station_id = row[""start_station_id""]
    current_bike_id = row[""bike_id""]
    current_transaction_id = i

    if current_start_station_id in BigDict:
        for transaction in list(BigDict[current_start_station_id]):  # Create a copy of the keys
            if current_start_time < BigDict[current_start_station_id][transaction]:
                continue
            if metro.at[transaction, ""idle_time""] is not None:
                continue
            idle_time = current_start_time - BigDict[current_start_station_id][transaction]
            metro.at[transaction, ""idle_time""] = idle_time
            #if idle_time.total_seconds() >= 0:
            del BigDict[current_start_station_id][transaction]

    if current_end_station_id not in BigDict:
        BigDict[current_end_station_id] = {current_transaction_id: current_end_time}

    BigDict[current_end_station_id][current_transaction_id] = current_end_time

",data-science
when using scikitlearn kmeans clustering how can you extract the centroids in original data domain,"I am using the sklearn KMeans k-means clustering algorithm. Before clustering, I normalize my data from [0,1] using
scaler = MinMaxScaler()
scaled_features = scaler.fit_transform(data)

Now, I can run the K-means algorithm.
kmeans = KMeans(
        init=""random"",
        n_clusters=3,
        n_init=10,
        max_iter=3000,
    )
    kmeans.fit(scaled_features)

Then, I can extract the 3 cluster centroids using kmeans.cluster_centers_. However, these centroids are in the normalized domain [0,1]. How can I re-transform these to the original data domain?
","Get the corrdinates in [0,1] scale , then use scaler.inverse_tranform to convert them to the original coordinates.
",data-science
sum of column that resets to zero through out a process,"Is there an easy way to go about a total for a column that increments but can reset back to zero through out the dataset? I have started to go down the path of a for loop and keeping track of previous value if it isn't a zero and using multiple variables, but wanted to check if there is a better way to go about it. Here is an example of the data I'm trying to work with:




id
Time
Extruder1
LineSpeed




157
5/22/2023 10:14:09.229 PM
1560.0
0.0


158
5/22/2023 10:16:28.582 PM
1563.0
0.0


159
5/23/2023 7:17:37.831 AM
1563.0
0.0


160
5/23/2023 7:19:57.184 AM
0.0
0.0


161
5/23/2023 7:33:53.302 AM
2.0
0.0


162
5/23/2023 7:36:12.655 AM
4.0
0.0


163
5/23/2023 7:38:32.008 AM
6.0
0.0


334
5/23/2023 2:15:41.371 PM
789.0
0.0


335
5/23/2023 2:18:00.724 PM
792.0
0.0


336
5/23/2023 2:20:20.077 PM
794.0
0.0


349
5/23/2023 2:50:31.666 PM
2.0
0.0




I would need to get a total of 3 from the first three rows and then get 6 from the next four, and so on. Is there a method in Pandas that will work for this type of column?
","Following our conversation, if there is a gap greater than 10, we can consider we have a new group:
THRESHOLD = 10
df['Group'] = df['Extruder1'].diff().abs().gt(THRESHOLD).cumsum()
total = df.groupby('Group')['Extruder1'].agg(lambda x: x.iloc[-1] - x.iloc[0]).sum()

Output:
>>> total
14.0

>>> df
     id                       Time  Extruder1  LineSpeed  Group
0   157  5/22/2023 10:14:09.229 PM     1560.0        0.0      0
1   158  5/22/2023 10:16:28.582 PM     1563.0        0.0      0
2   159   5/23/2023 7:17:37.831 AM     1563.0        0.0      0  # 1563 - 1560 = 3

3   160   5/23/2023 7:19:57.184 AM        0.0        0.0      1
4   161   5/23/2023 7:33:53.302 AM        2.0        0.0      1
5   162   5/23/2023 7:36:12.655 AM        4.0        0.0      1
6   163   5/23/2023 7:38:32.008 AM        6.0        0.0      1  # 6 - 0 = 6

7   334   5/23/2023 2:15:41.371 PM      789.0        0.0      2
8   335   5/23/2023 2:18:00.724 PM      792.0        0.0      2
9   336   5/23/2023 2:20:20.077 PM      794.0        0.0      2  # 794 - 789 = 5

10  349   5/23/2023 2:50:31.666 PM        2.0        0.0      3  # 2 - 2 = 0


IIUC, you can use:
df['Group'] = df['Extruder1'].lt(df['Extruder1'].shift()).cumsum()
# OR
df['Group'] = df['Extruder1'].diff().lt(0).cumsum()

print(df)

# Output
     id                       Time  Extruder1  LineSpeed  Group
0   157  5/22/2023 10:14:09.229 PM     1560.0        0.0      0
1   158  5/22/2023 10:16:28.582 PM     1563.0        0.0      0
2   159   5/23/2023 7:17:37.831 AM     1563.0        0.0      0
3   160   5/23/2023 7:19:57.184 AM        0.0        0.0      1
4   161   5/23/2023 7:33:53.302 AM        2.0        0.0      1
5   162   5/23/2023 7:36:12.655 AM        4.0        0.0      1
6   163   5/23/2023 7:38:32.008 AM        6.0        0.0      1
7   334   5/23/2023 2:15:41.371 PM      789.0        0.0      1
8   335   5/23/2023 2:18:00.724 PM      792.0        0.0      1
9   336   5/23/2023 2:20:20.077 PM      794.0        0.0      1
10  349   5/23/2023 2:50:31.666 PM        2.0        0.0      2

Step by step:
>>> out = df['Extruder1'].diff()
0        NaN
1        3.0
2        0.0
3    -1563.0
4        2.0
5        2.0
6        2.0
7      783.0
8        3.0
9        2.0
10    -792.0
Name: Extruder1, dtype: float64

>>> out = out.lt(0)
0     False
1     False
2     False
3      True
4     False
5     False
6     False
7     False
8     False
9     False
10     True
Name: Extruder1, dtype: bool

>>> out = out.cumsum()
0     0
1     0
2     0
3     1
4     1
5     1
6     1
7     1
8     1
9     1
10    2
Name: Extruder1, dtype: int64

Another representation (Python>=3.8):
>>> pd.concat([out := df['Extruder1'],
               out := out.diff(),
               out := out.lt(0),
               out.cumsum()], axis=1,
              keys=['Extruder1', 'diff', 'lt', 'cumsum'])

    Extruder1    diff     lt  cumsum
0      1560.0     NaN  False       0
1      1563.0     3.0  False       0
2      1563.0     0.0  False       0
3         0.0 -1563.0   True       1
4         2.0     2.0  False       1
5         4.0     2.0  False       1
6         6.0     2.0  False       1
7       789.0   783.0  False       1
8       792.0     3.0  False       1
9       794.0     2.0  False       1
10        2.0  -792.0   True       2

",data-science
pandas series mark all the rows between two values,"I have a series ( a single col in a df) with 3 possible values:
Stable, Increase, Decresae

, and I want to mark all the areas between a Increase to the subsequent Decrease. So for the values:
Stable
Stable
Stable
Increase
Increase
Stable
Stable
Decrease
Stable
Increase
Stable
Decrease

I will get: -,-,-,+,+,+,+,-,-,+,+,-
What is the best way to do so?
","map True on Increase and False on Decrease, then ffill. Finally map the +/- with numpy.where:
s = df['col'].map({'Increase': True, 'Decrease': False}).ffill().fillna(False)
df['indicator'] = np.where(s, '+', '-')

As a one-liner:
df['indicator'] = np.where(df['col'].map({'Increase': True, 'Decrease': False})
                                    .ffill().fillna(False),
                           '+', '-')

Output:
         col indicator
0     Stable         -
1     Stable         -
2     Stable         -
3   Increase         +
4   Increase         +
5     Stable         +
6     Stable         +
7   Decrease         -
8     Stable         -
9   Increase         +
10    Stable         +
11  Decrease         -

Intermediates:
         col    map  ffill+fillna indicator
0     Stable    NaN         False         -
1     Stable    NaN         False         -
2     Stable    NaN         False         -
3   Increase   True          True         +
4   Increase   True          True         +
5     Stable    NaN          True         +
6     Stable    NaN          True         +
7   Decrease  False         False         -
8     Stable    NaN         False         -
9   Increase   True          True         +
10    Stable    NaN          True         +
11  Decrease  False         False         -

",data-science
in the rouge metrics what do the low mid and high values mean,"The ROUGE metrics were introduced to ""automatically determine the quality of a summary by comparing it to other (ideal) summaries created by humans"" [1].
When calculating any ROUGE metric you get an AggregateScore object with 3 parameters: low, mid, high.
How are these aggregate values calculated?
For example, from the huggingface implementation [2]:
>>> rouge = evaluate.load('rouge')
>>> predictions = [""hello there"", ""general kenobi""]
>>> references = [""hello there"", ""general kenobi""]
>>> results = rouge.compute(predictions=predictions,
...                         references=references)
>>> print(list(results.keys()))
['rouge1', 'rouge2', 'rougeL', 'rougeLsum']
>>> print(results[""rouge1""])
AggregateScore(low=Score(precision=1.0, recall=1.0, fmeasure=1.0), mid=Score(precision=1.0, recall=1.0, fmeasure=1.0), high=Score(precision=1.0, recall=1.0, fmeasure=1.0))
>>> print(results[""rouge1""].mid.fmeasure)
1.0


Edit: On July 7th, the huggingface  implementation was simplified to return a cleaner and easier to understand dict: https://github.com/huggingface/evaluate/issues/148
","Given a list of (summary, gold_summary) pairs, any ROUGE metric is calculated per each item in the list. In huggingface, you can opt-out of the aggregation part by adding  use_aggregator=False and get these values returned.
For the aggregation, a bootstrap resampling is used [1, 2]. Bootstrap resampling is a technique used to extract confidence intervals [3, 4]. The idea is that for n samples, you draw x times a sample with replacement of size n, and then calculate some statistic for each resample. Now you get a new distribution called the empirical bootstrap distribution, which can be used to extract confidence intervals.
In the ROUGE implementation by google [4], they used:

n for the number of resamples to run
mean for the resample statistic
2.5th, 50th and 97.5th percentiles to calculate the values for low, mid and high, respectively (can be controlled with the confidence_interval param)

Randomness in ROUGE
Note that due to the bootstrapping technique used in ROUGE, it is non-deterministic, and can return different results for each run (see [5]). If you don't want to opt out from using the bootstrapping technique, you can set the seed in the load function, as such: evaluate.load('rouge', seed=42).
",data-science
auto arima in python results in poor fitting prediction of trend,"New to ARIMA and attempting to model a dataset in Python using auto ARIMA.
I'm using auto-ARIMA as I believe it will be better at defining the values of p, d and q however the results are poor and I need some guidance.
Please see my reproducible attempts below
Attempt as follows:
    # DEPENDENCIES
    import pandas as pd 
    import numpy as np 
    import matplotlib.pyplot as plt
    import pmdarima as pm 
    from pmdarima.model_selection import train_test_split 
    from statsmodels.tsa.stattools import adfuller
    from pmdarima.arima import ADFTest
    from pmdarima import auto_arima
    from sklearn.metrics import r2_score 

# CREATE DATA
data_plot = pd.DataFrame(data removed)

# SET INDEX
data_plot['date_index'] = pd.to_datetime(data_plot['date']
data_plot.set_index('date_index', inplace=True)

# CREATE ARIMA DATASET
arima_data = data_plot[['value']]
arima_data

# PLOT DATA
arima_data['value'].plot(figsize=(7,4))

The above steps result in a dataset that should look like this.

# Dicky Fuller test for stationarity 
adf_test = ADFTest(alpha = 0.05)
adf_test.should_diff(arima_data)

Result = 0.9867 indicating non-stationary data which should be handled by appropriate over of differencing later in auto arima process.
# Assign training and test subsets - 80:20 split 

print('Dataset dimensions;', arima_data.shape)
train_data = arima_data[:-24]
test_data = arima_data[-24:]
print('Training data dimension:', train_data.shape, round((len(train_data)/len(arima_data)*100),2),'% of dataset')
print('Test data dimension:', test_data.shape, round((len(train_data)/len(arima_data)*100),2),'% of dataset')

# Plot training & test data
plt.plot(train_data)
plt.plot(test_data)


 # Run auto arima
    arima_model = auto_arima(train_data, start_p=0, d=1, start_q=0,
    max_p=5, max_d=5, max_q=5,
    start_P=0, D=1, start_Q=0, max_P=5, max_D=5,
    max_Q=5, m=12, seasonal=True,
    stationary=False,
    error_action='warn', trace=True,
    suppress_warnings=True, stepwise=True,
    random_state=20, n_fits=50)
        
    print(arima_model.aic())

Output suggests best model is 'ARIMA(1,1,1)(0,1,0)[12]' with AIC 1725.35484
#Store predicted values and view resultant df

prediction = pd.DataFrame(arima_model.predict(n_periods=25), index=test_data.index)
prediction.columns = ['predicted_value']
prediction

# Plot prediction against test and training trends 

plt.figure(figsize=(7,4))
plt.plot(train_data, label=""Training"")
plt.plot(test_data, label=""Test"")
plt.plot(prediction, label=""Predicted"")
plt.legend(loc='upper right')
plt.show()


# Finding r2 model score
    test_data['predicted_value'] = prediction 
    r2_score(test_data['value'], test_data['predicted_value'])

Result: -6.985
","Is auto_arima a method done by you? It depends how you differentiate and what you do there. Did you check the autocorrelation and partial autocorrelation to know which repeating time lags you have there?
Also, it seems you have some seasonality patterns every year, you could try a SARIMA model if you are not doing it already.
To try a SARIMA model you have to:

Stationarized the data, in this case by differentiation you can convert the moving mean a stationary one.

data_stationarized = train_data.diff()[1:]


Check the autocorrelation and partial autocorrelation to check the seasonality.
You can use the library statsmodels for this.

import statsmodels.api as sm
sm.graphics.tsa.plot_acf(data_stationarized);


You can see that the most prominent flag is the twelfth flag, so as the granularity of the data is by month, that means there is prominent seasonality pattern every 12 months.
We can check the partial autocorrelation to confirm it too:
sm.graphics.tsa.plot_pacf(data_stationarized);


Again the most prominent flag is the twelfth one.

Fit the model with a seasonality order of 12. There are more parameters to explain which can be adjusted to have better results, but then this post will be very long.

model = sm.tsa.SARIMAX(endog=train_data, order=(2,0,0), seasonal_order=(2,0,0,12))
model_fit = model.fit()


Evaluate the results

from sklearn.metrics import mean_squared_error

y_pred = model_fit.forecast(steps=24)

# when squared=False then is equals to RMSE
mean_squared_error(y_true=test_data.values, y_pred=y_pred, squared=False)

This outputs 12063.88, which you can use to compare different results more rigorously.
For a graphical check:
prediction = pd.DataFrame(model_fit.forecast(steps=25), index=test_data.index)
prediction.columns = ['predicted_value']
prediction

# Plot prediction against test and training trends

plt.figure(figsize=(7,4))
plt.plot(train_data, label=""Training"")
plt.plot(test_data, label=""Test"")
plt.plot(prediction, label=""Predicted"")
plt.legend(loc='upper right')
plt.xticks([])
plt.yticks([])
plt.show();


Now you can see that the predictions get closer to the expected values.
You could continue fine tuning the order and seasonal order to get even better results, I will advice to check the docs of statsmodel.
Another advice it's to analyze the autocorrelation and partial autocorrelation of the residuals to check if your model is capturing all of the patterns. You have them in the model_fit object.
",data-science
what does 39 page 39 do in rrstudio,"What does ::Page{} do in R/RStudio ?
I'm studying Data Science through IBM certification course in coursera and the notes contain this line of code in all the code blocks and no explanation to what the ""Page"" function is doing
#load ggplot package

library(ggplot2)

::page{title=""create a scatterplot of displacement (disp) and miles per gallon (mpg)""}

ggplot(aes(x=disp,y=mpg,),data=mtcars)+geom_point()


I tried googling but found no help, there was something similar ""page()"" instead of ""Page{}"" as in my notes, but that was something completely different.
","Ig it was just a code mistake on the author side, cuz if I run without those colons it runs fine as intended.
",data-science
what does dataframeselect_dtypesexclude39object39 actually do,"I'm learning data science with the help of an online course. While learning about filtering specific datatype from a DataFrame object in Pandas, I came across this line of code:
df = df.select_dtypes(exclude=['object'])

The purpose of the module was to show how to get only numeric datatype. 
I'm not able to figure out what all things or data-types are included in the object type.
I tried to understand this from documentation given on Scipy Oficial
https://docs.scipy.org/doc/numpy-1.13.0/reference/arrays.dtypes.html
","So basically, it will select all the columns except the columns with data type object.
References:

pandas.DataFrame.select_dtypes
Text data types

",data-science
how to rewrite lambda function via pandas apply method to beat famous quotvalueerror the truth value of a series is ambiguousquot,"Edit: Solutions posted in this notebook. Special thanks to Étienne Célèry and ifly6!

I am trying to figure out how to beat the feared error:

ValueError: The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().

d = {
    'nickname': ['bobg89', 'coolkid34','livelaughlove38'], 
    'state': ['NY', 'CA','TN'],
    'score': [100, 200,300]
}
df = pd.DataFrame(data=d)
df_2 = df.copy() #for use in the second non-lambda part
print(df)

And this outputs:
          nickname state  score
0           bobg89    NY    100
1        coolkid34    CA    200
2  livelaughlove38    TN    300

Then the goal is to add 50 to the score if they are from NY.
def add_some_love(state_value,score_value,name):
     if state_value == name:
          return score_value + 50
     else:
          return score_value

Then we can apply that function with a lambda function.
df['love_added'] = df.apply(lambda x: add_some_love(x.state, x.score, 'NY'), axis=1)
print(df)

And that gets us:
          nickname state  score  love_added
0           bobg89    NY    100         150
1        coolkid34    CA    200         200
2  livelaughlove38    TN    300         300

And here is where I tried writing it, without the lambda, and that's where I get the error.
It seems like @MSeifert's answer here explains why this happens (that the function is looking at a whole column instead of a row in a column, but I also thought passing axis = 1 into the .apply() method would apply the function row-wise, and fix the problem).
So I then do this:
df2['love_added'] = df2.apply(add_some_love(df2.state, df2.score, 'NY'), axis=1)
print(df2)

And then you get the error:

ValueError: The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().

So I've tried these solutions, but I can't seem to figure out how to rewrite the add_some_love() function so that it can run properly without the lambda function.
Does anyone have any advice?
Thanks so much for your time and consideration.
","Your add_some_love function need a string input in order to execute the comparison if state_value == name. When you apply a lambda function over a DataFrame, you can pass every cell to the function, instead of the whole Series.
You can't use that exact add_some_love function without a lambda function.
If you still want to use apply(), try this function:
def add_some_love(row, name):
   if row.state == name:
       row.score = row.score + 50
   return row

df_2 = df_2.apply(add_some_love, axis=1, args=('NY',))

However, this should be the fastest and most efficient:
FILTER = df_2['state'] == 'NY'
df_2.loc[FILTER, 'score'] = df_2.loc[FILTER, 'score'] + 50

",data-science
how can i use pandas or openpyxl to classify a database in excel based on supplier names,"I want to classify a database in excel using Python (Pandas or Openpyxl), where I have the name of the supplier in the 1st column and I want to fill in the type in the 2nd column. But I need to identify the supplier as a string (because the cells of the suppliers may vary). How can I do it?
I thought that this could work, but it's adding ""SHOPEE"" (example) for all the lines:
x = [table.loc[:,""supplier""].str.contains(""SHOPEE"")]
for x in table:
    table[""TYPE""] = ""SHOPEE""
display(table)

","try to use this as a template, make changes wherever you need
import pandas as pd

# Load the Excel file into a DataFrame
df = pd.read_excel('your_file.xlsx')

# Define a dictionary to map suppliers to their corresponding types
supplier_types = {
    'SHOPEE': 'Type 1',
    'SUPPLIER2': 'Type 2',
    'SUPPLIER3': 'Type 3',
    # Add more mappings as needed
}

# Iterate over the rows of the DataFrame
for index, row in df.iterrows():
    supplier = str(row['supplier'])  # Convert supplier to a string
    for key, value in supplier_types.items():
        if key in supplier:
            df.at[index, 'TYPE'] = value  # Assign the corresponding type

# Display the updated DataFrame
print(df)

",data-science
jupyterlab vs jupyternotebook,"Hello everyone on Stack Overflow. Today, I would like to ask something very different question. 
I am currently working as a data scientist, and I work alot on JupyterLab/Notebook. Couple of my co-workers use Notebook instead of JupyterLab. It seems like there are not much difference between those two (I really like how JupyterLab presents codes in different colors). I searched on the internet, and it says
""JupyterLab is the next generation of the Jupyter Notebook""
However, some featuers like plotly figures do not work well on JupyterLab but works well on Jupyter Notebook. I do not know why this is happening.
Can anyone who work on these two tell me the actual differences?

Thank you for your replies!
","I used to work with Jupyter Notebook for about 4 years and last year I switched to Jupyter Lab. I think it is a nice improvement, I believe the biggest advantage is the improved user interface: it is much easier to switch between notebooks and everything feels more organized with tabs.
Concerning functionalities, the JupyterLab improvement is being modular: you can easily write your own plugins if you need it. 
I have no experience using plotly, but I have similar issues with other packages. Everything was fixed in a short time, so I wouldn't worry too much about it.
",data-science
combining strings which have been altered,"I have the following three strings:
""A randomized, prospective study of [intervention]endometrial resection[intervention] to prevent recurrent endometrial polyps in women with breast cancer receiving tamoxifen. To assess the role of endometrial resection in preventing recurrence of tamoxifen-associated endometrial polyps in women with breast cancer.
""A randomized, prospective study of endometrial resection to prevent [condition]recurrent endometrial polyps[condition] in women with breast cancer receiving tamoxifen. To assess the role of endometrial resection in preventing recurrence of tamoxifen-associated endometrial polyps in women with breast cancer.
""A randomized, prospective study of endometrial resection to prevent recurrent endometrial polyps in [eligibility]women with breast cancer receiving tamoxifen[eligibility]. To assess the role of endometrial resection in preventing recurrence of tamoxifen-associated endometrial polyps in women with breast cancer.

Is there a way to efficiently combine the three strings into one, where you can see all the annotations (between brackets) that I have made? I cannot come up with anything efficient by myself. The result should look like:
""A randomized, prospective study of [intervention]endometrial resection[intervention] to prevent [condition]recurrent endometrial polyps[condition] in [eligibility]women with breast cancer receiving tamoxifen[eligibility]. To assess the role of endometrial resection in preventing recurrence of tamoxifen-associated endometrial polyps in women with breast cancer.

Thanks in advance!
","Assuming you are only adding those words+brackets immediately next to the existing words (i.e. splitting the string on space won't change the alignment, which is the case in the example). A simple solution might be to zip the split strings and keep the longest variant using max, then join back into a single string:
strings = [""A randomized, prospective study of [intervention]endometrial resection[intervention] to prevent recurrent endometrial polyps in women with breast cancer receiving tamoxifen. To assess the role of endometrial resection in preventing recurrence of tamoxifen-associated endometrial polyps in women with breast cancer."",
           ""A randomized, prospective study of endometrial resection to prevent [condition]recurrent endometrial polyps[condition] in women with breast cancer receiving tamoxifen. To assess the role of endometrial resection in preventing recurrence of tamoxifen-associated endometrial polyps in women with breast cancer."",
           ""A randomized, prospective study of endometrial resection to prevent recurrent endometrial polyps in [eligibility]women with breast cancer receiving tamoxifen[eligibility]. To assess the role of endometrial resection in preventing recurrence of tamoxifen-associated endometrial polyps in women with breast cancer."",
          ]

out = ' '.join([max(x, key=len) for x in zip(*map(lambda s: s.split(), strings))])

Output:
'A randomized, prospective study of [intervention]endometrial resection[intervention] to prevent [condition]recurrent endometrial polyps[condition] in [eligibility]women with breast cancer receiving tamoxifen[eligibility]. To assess the role of endometrial resection in preventing recurrence of tamoxifen-associated endometrial polyps in women with breast cancer.'

If you need something more robust, a good starting point might be to use the difflib module to compute the successive differences, keeping the longest variant in each comparison.
",data-science
how do i drop and change dtype in a pipeline with sklearn,"I have some scraped data that needs some cleaning. After the cleaning, I want to create a ""numerical and categorical pipelines"" inside a ColumnTransformer such as:
categorical_cols = df.select_dtypes(include='object').columns
numerical_cols = df.select_dtypes(exclude='object').columns

num_pipeline = Pipeline(
    steps=[
    ('scaler', StandardScaler())
    ]
)

cat_pipeline = Pipeline(
    steps=[
        ('onehotencoder', OneHotEncoder(handle_unknown='ignore'))
    ]
)

preprocessor = ColumnTransformer([
    ('num_pipeline', num_pipeline, numerical_cols),
    ('cat_pipeline', cat_pipeline, categorical_cols)
])

My idea was to create a transformer class Transformer(BaseEstimator, TransformerMixin): and create a pipeline with it. That transformer would include all the cleaning steps. My problem is that some of the steps change the dtype from object to integer mostly so I'm thinking that instead of defining the categorical_cols and numerical_cols with dtypes, instead, do it with column names.
Would that be the correct approach? The idea would be to automate the process so I can train the model with new data every time.
","Instead of making a list of columns beforehand you can use scikit-learn's make_column_selector to dynamically specify the columns that each transformer will be applied to.
In your example:
from sklearn.compose import make_column_selector as selector

preprocessor = ColumnTransformer([
    ('num_pipeline', num_pipeline, selector(dtype_exclude=object)),
    ('cat_pipeline', cat_pipeline, selector(dtype_include=object))
])


Under the hood it uses pandas' select_dtypes for the type selection. You can pass a regex and select based on column name as well.
I also recommend you checking out make_column_transformer for more control over the pipeline.
",data-science
groupby mean not working on titanic dataset in python,"I am using titanic dataset and tring to run the groupby command but its not working as shown on countless tutorials online. I have named my dataframe as ks_cl. Here is the command I executed in VScode:
ks_cl.groupby(['sex']).mean()
This is the output:
NotImplementedError                       Traceback (most recent call last)
File d:\Program Files\Python\Lib\site-packages\pandas\core\groupby\groupby.py:1490, in GroupBy._cython_agg_general..array_func(values)
   1489 try:
-> 1490     result = self.grouper._cython_operation(
   1491         ""aggregate"",
   1492         values,
   1493         how,
   1494         axis=data.ndim - 1,
   1495         min_count=min_count,
   1496         **kwargs,
   1497     )
   1498 except NotImplementedError:
   1499     # generally if we have numeric_only=False
   1500     # and non-applicable functions
   1501     # try to python agg
   1502     # TODO: shouldn't min_count matter?

File d:\Program Files\Python\Lib\site-packages\pandas\core\groupby\ops.py:959, in BaseGrouper._cython_operation(self, kind, values, how, axis, min_count, **kwargs)
    958 ngroups = self.ngroups
--> 959 return cy_op.cython_operation(
    960     values=values,
    961     axis=axis,
    962     min_count=min_count,
    963     comp_ids=ids,
...
   1698             # e.g. ""foo""
-> 1699             raise TypeError(f""Could not convert {x} to numeric"") from err
   1700 return x

TypeError: Could not convert CSSSCSSSSSQSSSCSSCQSCSSSSSSSSSSSSCSCSSSSSSSSSQSSSCSSSCCSSQSCSCSSSSSSSCSSSSSSSQSCSSCCCSSSSCQSCSSCCSSSSCCSSCSSCCSSSSSQSSSSSSSSSSSSSCSCSCSSSCSQSSSCSSSCSSSSCCSSSSSCSSSSSSSCSCSCSSSSSSSSSCSCSSQQSSSCCSSCSSSSSSSSSSSQSSSCSSSSSSSSSSSSCCCCSSSSCSSCSCCCSSQS to numeric
Output is truncated. View as a scrollable element or open in a text editor. Adjust cell output settings...

I was expecting this output:
enter image description here
","You need to turn on numeric_only in GroupBy.mean :

numeric_only : (bool), default None
Include only float, int, boolean
columns. If None, will attempt to use everything, then use only
numeric data. Not implemented for Series.
Deprecated since version 1.5.0: Specifying numeric_only=None is
deprecated. The default value will be False in a future version of
pandas.
Source : [docs]

And as per pandas 2.0.0 :

Changed default of numeric_only in various DataFrameGroupBy methods;
all methods now default to numeric_only=False (GH46072)

link = ""https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv""

ks_cl = pd.read_csv(link)
​
out = ks_cl.groupby(""Sex"").mean(numeric_only=True)

​
Output :
print(out)

        PassengerId  Survived   Pclass       Age    SibSp    Parch      Fare
Sex                                                                         
female   431.028662  0.742038 2.159236 27.915709 0.694268 0.649682 44.479818
male     454.147314  0.188908 2.389948 30.726645 0.429809 0.235702 25.523893

",data-science
what is the best way of under sampling in r,"I have a dataset with attribute A,B, C. C is a factor with 2 labels zz and z. number of (z) > number of (zz), I want to under sample my data set so that in the new data has same no of zz and z values. Can't use any external package for this. Best if can be done using sample function
--------------------------------------------------
| Attribute A   |   Attribute B . | Attribute c  |
--------------------------------------------------
|  xx           | y1              | zz           |
--------------------------------------------------
|  mm           | r1              |  z           |
--------------------------------------------------
|  ab           | 1r              |  z           |
--------------------------------------------------
|  ry           | cm              |  zz          |
--------------------------------------------------
|  ca           | rx              |  z           |
--------------------------------------------------
|  mm           | zr              |  z           |
--------------------------------------------------

And the result should be-

| Attribute A   |   Attribute B . | Attribute c  |
--------------------------------------------------
|  xx           | y1              | zz           |
--------------------------------------------------
|  mm           | r1              |  z           |
--------------------------------------------------
|  ab           | 1r              |  z           |
--------------------------------------------------
|  ry           | cm              |  zz          |
--------------------------------------------------

Here probability of zz = probability  of z = 0.5
","Say your data is in a data frame called data with columns A, B, and C, you can do something like:
## rows that have ""z"" and ""zz"" entries
z_ind <- which(data$C == ""z"")
zz_ind <- which(data$C == ""zz"")

nsamp <- 10   #number of elements to sample
## if you want all elements of the smaller class, could be:
## nsamp <- min(length(z_ind), length(zz_ind))

## select `nsamp` entries with ""z"" and `nsamp` entries with ""zz""
pick_z <- sample(z_ind, nsamp)
pick_zz <- sample(zz_ind, nsamp)

new_data <- data[c(pick_z, pick_zz), ]

",data-science
complete cryptocurrencies data,"I am working on cryptocurrencies blockchain data and I want data from the beginning of the time of that particular cryptocurrency. Is there any way to download complete block data in the Postgresql file?
https://blockchair.com/dumps is although offering this but they limit the download speed and number of downloading files. Moreover, I am also waiting for their reply. Meanwhile, I am finding some other ways or websites to download complete data of multiple cryptocurrencies in SQL format. I cannot download the .csv or .tsv file because it takes a lot of space on my laptop. Therefore, I want to use any other format (preferably .sql format)
","This is depends on cryptocurrency, you have. I can suggest you, how to fetch data from Bitcoin-compatible crypto, i.e. Bitcoin, Emercoin, etc.
The cryptocurrency node (wallet) has JSON RPC API interface.Usinf this API, you retrieve all your data with following commands from this command list:

Get total block counter with command getblockcount.
Iterate block number from 0 to result of getblockcount. For each number, call getblockhash.
For result of getblockhash call getblock. This function provide transactions list, enclosed in this block.
For ech transaction (nested loop), call getrawtransaction. Hint: if you call getrawtransaction with 3rd argument ""1"", node automatically decodes transaction, and return you decoded transaction in JSON.

You can extract from a transaction vectors vin and vout, and upload all data into your SQL database.
",data-science
what is different between jupyterhub and simple jupyter,"What is differents between Jupyterhub and simple Jupyter?
I want you describe in more detail
","By ""simple Jupyter"", I assume you mean a local installation/environment in which you are running Jupyter Notebook. JupyterHub is simply ""a multi-user version of the notebook designed for companies, classrooms and research labs""—essentially just a piece of server-side infrastructure provided by the Jupyter project to help deploy a Jupyter environment for easy connection and use by multiple distributed clients.
Once connected, up and running, the interface and user experience is quite similar, except with JupyterHub it is running on a server somewhere that you usually connect to by pointing your browser to an external URL, instead of localhost or a specific address/port like you would use for a local Jupyter Notebook.
",data-science
python script iterates over whole folder but skips files in the folder,"I tried running the following code. The code should read hdf5 files from a directory and create for every hdf5 file a png and a txt file with the same name (btw. I need it as input for the CNN YOLO).
The code does what I described but only for 20 images! I added print(i) to see if the for-loop is working proper... and it is. It prints every file in the directory (over 200 files). But it just creates 20 .png and 20 .txt files.
def process_fpath(path1, path2):
    sensor_dim = (101, 101)
    onlyfiles = [f for f in os.listdir(path1) if isfile(join(path1, f))]

    for i in onlyfiles:
        if i.endswith("".hdf""):
            print(i)
            #cut "".hdf""
            name = str(i[0:-5]) 
            # create png
            im = h5py.File(path1 + str(i), 'r')
            labels_im = im['labels']
            image = im['image']
            plt.imsave(path2 + name + '.png', image)
            
            # create txt
            exp = np.column_stack((np.zeros(np.size(labels_im,0)) , labels_im[:,0]/sensor_dim[0], labels_im[:,1]/sensor_dim[1], labels_im[:,3]/sensor_dim[0], labels_im[:,3]/sensor_dim[0]))
            np.savetxt(path2 + name + '.txt', exp, delimiter = '  ', fmt=['%d', '%8f', '%8f', '%8f', '%8f'])


This is my first post so if something isn't proper please let me know.
","
Maybe it's because of the name variable? You remove 5 characters but you want to remove only 4: name = str(i[0:-4])
Not related to your question, the last 3 lines are useless. you can remove them.

            continue
        else:
            continue


Try to run on a given file that is not working to understand what the problem is instead of looping on each of them.

",data-science
how can i see all parameters of a function in pandas,"dir(pd) shows only some of pandas functions not all, but in documentation there are a lot of them that are quite handy. Is there any way to see list of all functions and particularly their parameters with some code? Thanks
","You can use the help() method to get the signature and details of any function. e.g. help(pd.DataFrame.apply)
",data-science
index a different range of indicies from each row of numpy array,"I have two arrays of incidies with shape m. I need to take the mean of the values inbetween the indicies from an array with shape m x n. Can this be done without iterating through each row? What is the fastest way to do this?
idx0 = np.array([1, 3, 2, 5])
idx1 = np.array([5, 8, 6, 7])
d = np.array([[1,2,3,4,5,6,7,8,9], [1,2,3,4,5,6,7,8,9], [1,2,3,4,5,6,7,8,9], [1,2,3,4,5,6,7,8,9]])

Desired result:
<< d[np.arange(d.shape[0]), idx0:idx1]
>> np.array([3.5, 6, 4.5, 6.5])

The best ways I've found to do this are list comprehension or using a for loop with numba, but this seems like it should be a common problem? Thanks.
","In your case you'd still need to get slices of pairwised ranges of indices from idx0 and idx1.zip + slice is a straigtforward way:
[d[:, slice(*a)].mean() for a in zip(idx0, idx1)]


[3.5, 6.0, 4.5, 6.5]

",data-science
right way to use simple imputer,"I have a test- and a train-dataset with nan-values. I would like to use simple imputer in order to deal with them using the following code.
#numerical values
imp= SImp(strategy= 'median') #or 'mean', 'median', 'most_frequent' 
my_df.loc[:, num_cols_nan]= imp.fit_transform(def_df.loc[:, num_cols_nan])

#categorical values
imp= SImp(strategy= 'most_frequent')
my_df.loc[:, cat_cols_nan]= imp.fit_transform(def_df.loc[:, cat_cols_nan])`

My idea was to use median and most_frequent to fill in the NaNs.
However, browsing the web I find to different information on how to use the imputer:

use fit_transform for the train data-set and transform for the test-set in order to perform the replacement based on the distribution on the train set.
use fit_transform for train and test-set in order to perform the replacement based on the distribution of each set separately.

Just to clarify version 1. seems to be the most frequent case but since I'm a total beginner I'm kind of confused here.
So my question is which one is the ""right"" version to use?
","Both approaches that you mentioned are valid and can be used for imputing missing values in both train and test sets. The difference lies in the way the imputer is trained and applied to the data.
Approach 1: fit_transform on train set and transform on test set
In this approach, you fit the imputer on the train set using fit_transform, which learns the parameters (e.g. mean, median, most frequent) of the imputation model based on the distribution of the training data. Then, you use transform on the test set to impute missing values in the test set using the same learned parameters. This approach ensures that the test set is imputed based on the distribution of the training data, which is the goal in most cases.
Approach 2: fit_transform on train set and test set separately
In this approach, you fit the imputer separately on both the train and test sets using fit_transform. This means that the imputation parameters will be learned separately based on the distribution of each set, which may lead to different imputation results between the two sets. This approach is less common and is generally not recommended because it may lead to overfitting to the test set.
In general, it is recommended to use Approach 1 for imputing missing values in both the train and test sets. This ensures that the imputation is done based on the distribution of the training data, which is what we want to learn from. Additionally, it ensures that the same imputation model is used for both the train and test sets, which is important for ensuring consistency in the modeling process.
",data-science
how to resampling o multidimensional event without losing information,"I have a dataset representing a time series
The time series has 165 events
Each unique event has 15 rows, FX1, FX2, … FX15
The time between each row is 1 minute so one event takes 15 minutes
Each row has 6 features F1,F2,F3,T1,T2,T3
All values are numeric and integers values
Here is an example of an event :
This event started at 2000/01/01 00:00:00 and ended at 2000/01/01 00:15:00
The first row in the event has F1=3,F2=-4,…T3=45

                   time  F1   F2   F3  T1  T2   T3
0   2000/01/01 00:01:00  94  -76    0  47   9  -20
1   2000/01/01 00:02:00  -2   85   14  79  92  -95
2   2000/01/01 00:03:00  -3   13 -100  33  74  -43
3   2000/01/01 00:04:00  39   64  -29  32 -73  -44
4   2000/01/01 00:05:00  80   44    3  73  56 -100
5   2000/01/01 00:06:00 -19  -51  -77  32  72   24
6   2000/01/01 00:07:00  79  -69  -87   4  20   19
7   2000/01/01 00:08:00  68    6   95 -76  34   58
8   2000/01/01 00:09:00  26  -59   24  79 -43   48
9   2000/01/01 00:10:00  71    8  -85 -15 -45  -56
10  2000/01/01 00:11:00  51   98    6 -53 -39    5
11  2000/01/01 00:12:00  99   73  -48  -1  64   56
12  2000/01/01 00:13:00 -12   13  -63  51  36   95
13  2000/01/01 00:14:00   8 -100   54  91 -56  -32


My question is this
Can I convert one event to just one row without losing information?
Let's say I use Panda data frame resample() method for downsampling
Think of it like resampling a dataset in this case resampling to 15 minutes will create a unique row I think but will the resulting unidimensional data lack any information?
So the desired output for the above data frame will be something like this
                 time  F1   F2   F3   T1   T2     T3
 2000/01/01 00:01:00  XF1  XF2  XF3  xT1  xT2    xT3

Where XF1 is the value that best represents the array of 14 values that F1 took from 0 to 14 ( 94, -2, -3, 39, 80, -19, 79, ....8).
","With the following toy dataframe:
import random
import numpy as np
import pandas as pd

df = pd.DataFrame(
    {""time"": [f""2000/01/01 00:{i:02}:00"" for i in range(1, 60)]}
    | {
        key: [random.randrange(-100, 100) for _ in range(1, 60)]
        for key in (""F1"", ""F2"", ""F3"", ""T1"", ""T2"", ""T3"")
    }
)

print(df.head(30))

# Output
                   time  F1  F2  F3  T1  T2   T3
0   2000/01/01 00:01:00  44 -91   5 -23  97   14
1   2000/01/01 00:02:00 -38  46 -14  -5 -66   39
2   2000/01/01 00:03:00  70 -63 -28 -53  53   77
3   2000/01/01 00:04:00  33 -16  82  98  54   95
4   2000/01/01 00:05:00 -51 -89 -52 -88 -68  -61
5   2000/01/01 00:06:00 -64  69  25 -98  21   63
6   2000/01/01 00:07:00 -52  51 -34  35 -47   83
7   2000/01/01 00:08:00 -10  10 -87 -49  75    7
8   2000/01/01 00:09:00 -51 -95  25 -49 -43  -13
9   2000/01/01 00:10:00 -16  88 -23  -3 -17   71
10  2000/01/01 00:11:00   4 -97   3  53 -35  -83
11  2000/01/01 00:12:00 -94 -17 -88  -5  41   60
12  2000/01/01 00:13:00  91 -14  43  79  -8   14
13  2000/01/01 00:14:00  94  -1 -57   7 -21   91
14  2000/01/01 00:15:00 -60  -2  39 -56 -61   24
15  2000/01/01 00:16:00 -20 -83  30  68 -97  -87
16  2000/01/01 00:17:00   7  70 -65  49  13  -66
17  2000/01/01 00:18:00  29 -70  78  84 -80   -5
18  2000/01/01 00:19:00  57 -57 -78 -75  29  -12
19  2000/01/01 00:20:00  -1 -48 -91  89  25   88
20  2000/01/01 00:21:00 -60 -90   6  34 -77   34
21  2000/01/01 00:22:00 -28   7 -33 -64  42   56
22  2000/01/01 00:23:00 -29  85  45  29 -20  -38
23  2000/01/01 00:24:00  40 -26  17  18  50 -100
24  2000/01/01 00:25:00 -74  60 -50  -3  81  -91
25  2000/01/01 00:26:00  35  47 -90  19  48  -47
26  2000/01/01 00:27:00 -32  34 -43  33  26   26
27  2000/01/01 00:28:00  74  12 -11 -97 -20  -29
28  2000/01/01 00:29:00  58 -90  -7 -88  29  -89
29  2000/01/01 00:30:00  39 -51 -88 -94 -26  -27

Here is one way to do it:
df[""time""] = pd.to_datetime(df[""time""], format=""%Y/%m/%d %H:%M:%S"")
new_df = (
    df.set_index(""time"").resample(""15T"").agg(lambda x: int(np.mean(x)))
)  # 15 min. resampling

Then:
print(new_df)
# Output

                     F1  F2  F3  T1  T2  T3
time
2000-01-01 00:00:00  -2 -15 -14  -7   2  32
2000-01-01 00:15:00   0 -10 -16   2   0 -22
2000-01-01 00:30:00   7  -3 -13 -12   0  24
2000-01-01 00:45:00  -1   0  -5 -23 -12 -23

Without more context, it's impossible to determine what would be the best representation of the resampled values, so I chose the mean value, but you can replace the lambda function with anything more suitable.
Also, I don't think you will be able to avoid losing some information, as resampling/aggregating necessarily comes at a cost.
",data-science
is there a python function that allows me to sum the last goals of a team in a dataframe,"I'm solo learning football data science, and want to analyse some things in the turkish league. Now I'm trying to make some analyses from the past 5 games of both teams from the upcoming games. First I've already made a dataframe with the games of the season. Now I want to make a indicator of points of goals from each team, but since of the start from the season. The ideia is: I find how many goals the home team scored in the past 5 matches, goals scored at home count as 1 points and goals scored away count as 2 points. I make the same for the away team and for the conceded goals. At the moment, when I filter the last 5 home team games, I only get his last 5 at home, and not the last five overall.
This is my code so far
import pandas as pd

webs = pd.read_csv('https://www.football-data.co.uk/mmz4281/2223/T1.csv')
tk = webs[['Date','HomeTeam','AwayTeam','FTHG','FTAG','FTR']]
tk.columns = ['Data','Home','Away','FT_Gols_H','FT_Gols_A','FT_Result']

tk['Goal_Points'] = tk.groupby('Home')['FT_Gols_H'].rolling(window=5, min_periods=1).sum().reset_index(0,drop=True).shift(1)

","I'd suggest reformatting your data a little. Make the dataframe into one with 3 columns: the date, the team, and the goals they got on that date, no matter whether they were at home or away.
Example:
home_team_games = tk[['Data', 'Home', 'FT_Gols_H']].rename(columns={
    'Home': 'Team',
    'FT_Gols_H': 'Goals',
})
away_team_games = tk[['Data', 'Away', 'FT_Gols_A']].rename(columns={
    'Away': 'Team',
    'FT_Gols_A': 'Goals',
})
tk = pd.concat([home_team_games, away_team_games])
tk = tk.sort_values('Data')

Then you have a dataframe like this:
           Data          Team  Goals
193  01/02/2023   Antalyaspor      0
192  01/02/2023  Umraniyespor      2
191  01/02/2023   Giresunspor      1
190  01/02/2023     Hatayspor      1
193  01/02/2023   Trabzonspor      2
..          ...           ...    ...
189  31/01/2023    Karagumruk      1
107  31/10/2022   Antalyaspor      2
107  31/10/2022     Sivasspor      0
106  31/10/2022   Giresunspor      1
106  31/10/2022    Buyuksehyr      3

I had to modify your code a little bit to get it to work on this data frame. I changed the name of the columns, and I also had to drop both levels of the index and not just the first level.
tk['Goals_Prev_5_Games'] = tk.groupby('Team')['Goals'].rolling(window=5, min_periods=1).sum().reset_index(drop=True).shift(1)

",data-science
scaling a dataset in matplotlib on x and y axis relative to another dataset,"I am trying to scale two different sets of data to be visually equivalent.
Green data set has extreme Y values and significantly more data points. Hence Orange data set falls flat and short.
What functions exist that allow me to scale them equivalently with one another?
*Future viewers: 'MinMax normalization' is one method as mentioned by the responses.

","You can do this by squeezing the values between 0 and 1.
import numpy as np
import matplotlib.pyplot as plt

# Define the green and orange data sets
green_data = np.random.normal(50, 10, 100)
orange_data = np.random.normal(25, 5, 10)

# Normalize the data sets using min-max scaling
green_data_normalized = (green_data - np.min(green_data)) / (np.max(green_data) - np.min(green_data))
orange_data_normalized = (orange_data - np.min(orange_data)) / (np.max(orange_data) - np.min(orange_data))

# Plot the normalized data sets
plt.plot(green_data_normalized, label='Green Data')
plt.plot(orange_data_normalized, label='Orange Data')
plt.legend()
plt.show()


Edit: If you want to be able to get the orange values to have the same x-width as your green values, you can draw a straight line between each point, and use the midpoint to figure out what data point should go in between. This will widen the line by creating more data points, numpy has this built in with np.interp (short for interpolate).
import numpy as np
import matplotlib.pyplot as plt

# Define the green and orange data sets
green_data = np.random.normal(50, 10, 100)
orange_data = np.random.normal(25, 5, 10)

# Define the x-values for the original and extended orange data
x_orange_original = np.linspace(0, 1, len(orange_data))
x_orange_extended = np.linspace(0, 1, len(green_data))

# Interpolate the orange data to extend it
orange_data_extended = np.interp(x_orange_extended, x_orange_original, orange_data)

# Normalize the data sets using min-max scaling
green_data_normalized = (green_data - np.min(green_data)) / (np.max(green_data) - np.min(green_data))
orange_data_normalized = (orange_data_extended - np.min(orange_data_extended)) / (np.max(orange_data_extended) - np.min(orange_data_extended))

# Plot the normalized data sets
plt.plot(green_data_normalized, label='Green Data')
plt.plot(orange_data_normalized, label='Orange Data')
plt.legend()
plt.show()


",data-science
how to cherry pick tuned parameters after workflow_map,"I've tuned succesfully several models using {tidymodels} and {workflow_set}. However, when testing the validation dataset with tune::last_fit(), the parameters obtained by tune::select_best don't behave well. This makes me want to manually test other sets of parameters on the validation set. I find tune::show_best() and tune::select_best() very limited for doing so, since they only consider one metric when choosing the right parameters. I've managed to filter the tibbles with a more complex logic involving several metrics using pure {dplyr} but this is not optimal and is time consuming, involving manually finalizing each model every time I want to test one of the models.
Is there a way to cherry pick a set of parameters based on the workflow_id and .config data of the tibble?
It would be really helpful that tune::select_best() could take more conditions to pick a model.
This is the classical process to get the ""best"" set of parameters (which unfortunately is not in my case since I get a model with a very high roc_auc but very bad spec for example).
models_tuned <- models %>% 
  workflow_map(""tune_bayes"",
               resamples = cv_folds,
               initial = 20,
               iter= 10,
               metrics = mm_metrics,
               verbose = TRUE)

best_results <- models_tuned %>% 
  extract_workflow_set_result(id = ""norm_nnet"") %>% 
  select_best(metric = ""accuracy"")

fitted_workflow <- models_tuned %>%
  extract_workflow(id = ""norm_nnet"") %>%
  finalize_workflow(best_results) %>% 
  last_fit(split=split_df,
           metrics=mm_metrics)

","As answered here:
You can pass any parameters you want to finalize_workflow(). The parameters argument takes any tibble that includes values for the tuning parameters.
There are also other select_*() functions to use as well as the unreleased desirability2 package.

I get a model with a very high roc_auc but very bad spec for example

That sounds like a class imbalance. You might look into using a cost-sensitive learning method or rebalance (perhaps with themis).
",data-science
how do i encode location data for linear regression in python,"I am doing a beginner project to predict house prices. I have one category called 'city' with values such as Boston, Detroit, NY, etc.
If I use One Hot Encoder I will end up with a very huge dataset because there are around 100 unique values. But I am unsure about using Label Encoding because the regression may treat the values as numerical.
I thought that would be plenty explained on the internet as it is a typical exercise but I am unable to find a solution. How do I encode location data?
","If you have a categorical feature with many unique values, try using target encoding
The idea is to calculate average price for each city and then replace city name with that average for each row. This average price will tell your model how attractive this city in general.
One possible implementation is this:
mean_by_city = df.groupby(""city"").agg({""price"": ""mean""}).squeeze().to_dict()
df[""city_mean_encoding""] = df[""city""].map(mean_by_city)

One advice: don't forget to check for rare categories. If your dataset has only few examples of houses in some small town, the mean might not be accurate. In this case I can recommend to add a dummy ""Rare"" category.
",data-science
avoid x axis labels repetition in altair chart,"So I got this chart which displays a time series in X.Axis using yearmonth('date'):T. The thing is, it duplicates the labels to fill the spaces, but I don't want that behaviour. I'd like a single label each time. How can I do this?

example df:
ID      date        total
3425    2022-01-04  30
15161   2022-01-12  1730
18192   2022-01-13  1526
21014   2022-01-21  99
24692   2022-02-07  28
30163   2022-02-21  175
31531   2022-02-22  446
34467   2022-03-22  57
34880   2022-03-23  500
35144   2022-03-30  41

","Try to change the type of the date column to O (Ordinal) if you are using T(Time).
chart = (
alt.Chart(source).mark_line().encode(
    x=alt.X(""Fecha:O"", timeUnit=""yearmonth"", title=""Fecha""),
    y=alt.Y(""Monto:Q"", stack=None)
)

",data-science
converting multi column header df from wide to long format in python,"I would like to convert an excel file from wide format into long format.
I'm reading an excel file which has not only two rows of headers, it also includes merged cells in the header.
Input Example:
| Task | Name | May,2022  | Jun,2022  | Jul,2022  |
|      |      |status|type|status|type|status|type|
| ---- | ---- |------|----|------|----|------|----|
| 1    | Tom  |1     |AB  |1     |DT  |0     |AB  | 
| 2    | John |0     |DT  |1     |CC  |0     |GH  |

Desired Output:




Task
Name
Date
status
type




1
Tom
May,2022
1
AB


1
Tom
Jun,2022
1
DT


1
Tom
Jul,2022
0
AB


2
John
May,2022
0
DT


2
John
Jun,2022
1
CC


2
John
Jul,2022
0
GH




What i have:
import pandas as pd
df = pd.read_excel(path)
df = df.melt(id_vars=[""Task"",""Name""],
     var_name=""Date"",
     value_name=""Value"")

I don't know how to deal with the multi header.
I tried combining the two headers.
import pandas as pd
df = pd.read_excel(path, header[1,2])
df.columns = df.columns.to_series().ffill().values
df.columns.value[0]='Task'
df.columns.value[1]='Name'
df = df.melt(id_vars=[""Task"",""Name""],
     var_name=""Date"",
     value_name=""Value"")

This results in having something like: (I'm fine with the date transformation)




Task
Name
(2022-05-01 00:00:00, status)
(2022-05-01 00:00:00, type)




1
Tom
1
AB


2
John
0
DT




The melt function returns a KeyError 'Task'.
Even if this would work i would somehow need to split the date from the status and type into seperate columns.
Someone knows how to deal with such data?
","You can use set_index to preserve Task and Name columns then stack Date column and finally reset_index to get expected output:
>>> (df.set_index(['Task', 'Name']).rename_axis(columns=['Date', None])
       .stack(level=0).reset_index())

   Task  Name      Date  status type
0     1   Tom  Jul,2022       0   AB
1     1   Tom  Jun,2022       1   DT
2     1   Tom  May,2022       1   AB
3     2  John  Jul,2022       0   GH
4     2  John  Jun,2022       1   CC
5     2  John  May,2022       0   DT

Minimal Reproducible Example:
data = {'index': [0, 1],
        'columns': [('Task', ''),
         ('Name', ''),
         ('May,2022', 'status'),
         ('May,2022', 'type'),
         ('Jun,2022', 'status'),
         ('Jun,2022', 'type'),
         ('Jul,2022', 'status'),
         ('Jul,2022', 'type')],
        'data': [[1, 'Tom', 1, 'AB', 1, 'DT', 0, 'AB'],
                 [2, 'John', 0, 'DT', 1, 'CC', 0, 'GH']],
        'index_names': [None],
        'column_names': [None, None]}
df = pd.DataFrame.from_dict(data, orient='tight')

",data-science
quotvalueerror dimensions of labels and x must be compatiblequot using jupyter notebook,"data = {'continent': ['Asia', 'Africa', 'Europe', 'North America', 'South America', 'Australia'],
       'population_density': [150, 44, 72, 22, 25, 3]}
df = pd.DataFrame(data)
plt.boxplot(df['population_density'], labels=df['continent'])
plt.title('Population Densities by Continent')
plt.ylabel('Population Density')    

my code is above. when I run it, I get the error ""ValueError: Dimensions of labels and X must be compatible"", but idk why, there are 6 labels for 6 data points.
The condition of my question is ""There must be six columns in a single boxplot area, as there are six continents"".
","Population densities are usually graphed in a bar graph. Here is a quote that supports the statement:

Bar graphs probably the most versatile way of displaying population
data. They are often used to show population growth over time but they
can also show things such as the relative proportions of different
groups in the society or they may be used to compare populations of
different places.

To make the bar graph, you need to change this line in your code:
plt.boxplot(df['population_density'], labels=df['continent'])

To this:
plt.bar(df['continent'], df['population_density'])

This is the resulting graph after running plt.show():

If you really want to use a box plot, you can replace this:
plt.boxplot(df['population_density'], labels=df['continent'])

With this:
df.boxplot(by='continent')

This is the resulting graph after running plt.show():

As you can see, it does not look quite right.
",data-science
replace a part of string in pandas data column replace doesn39t work,"I have been trying to clean my data column by taking a part of the text out. Unfortunately cannot get my head around it.
I tried using the .replace method in pandas series, but that did not seem to have worked
df['Salary Estimate'].str.replace(' (Glassdoor est.)', '',regex=True)


0       $53K-$91K (Glassdoor est.)
1      $63K-$112K (Glassdoor est.)
2       $80K-$90K (Glassdoor est.)
3       $56K-$97K (Glassdoor est.)
4      $86K-$143K (Glassdoor est.)
                  ...             
922                             -1
925                             -1
928    $59K-$125K (Glassdoor est.)
945    $80K-$142K (Glassdoor est.)
948    $62K-$113K (Glassdoor est.)
Name: Salary Estimate, Length: 600, dtype: object


What I expected was


0       $53K-$91K
1      $63K-$112K
2       $80K-$90K
3       $56K-$97K
4      $86K-$143K
                  ...             
922                             -1
925                             -1
928    $59K-$125K
945    $80K-$142K
948    $62K-$113K
Name: Salary Estimate, Length: 600, dtype: object`


","If you enable regex, you have to escape regex symbol like (, ) or .:
import re

>>> df['Salary Estimate'].str.replace(re.escape(r' (Glassdoor est.)'), '',regex=True)
0     $53K-$91K
1    $63K-$112K
2     $80K-$90K
3     $56K-$97K
4    $86K-$143K
Name: Salary Estimate, dtype: object

# Or without import re module
>>> df['Salary Estimate'].str.replace(r' \(Glassdoor est\.\)', '',regex=True)
0     $53K-$91K
1    $63K-$112K
2     $80K-$90K
3     $56K-$97K
4    $86K-$143K
Name: Salary Estimate, dtype: object

You can also extract numbers:
>>> df['Salary Estimate'].str.extract(r'\$(?P<min>\d+)K-\$(?P<max>\d+)K')
  min  max
0  53   91
1  63  112
2  80   90
3  56   97
4  86  143

",data-science
eli5 permuterfeature_importances_ returning all zeros,"I'm trying to get permutation importances for a RandomForestClassifier on a small sample of data, but while I can get simple feature importances, my permutation importances are coming back as all zeros. 
This is the code:
Input1:
X_train_encoded = encoder.fit_transform(X_train1)
X_val_encoded = encoder.transform(X_val1)
model = RandomForestClassifier(n_estimators=300, random_state=25, 
                               n_jobs=-1,max_depth=2)
model.fit(X_train_encoded, y_train1)

Output1:
RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=2, max_features='auto',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_impurity_split=None,
                       min_samples_leaf=1, min_samples_split=2,
                       min_weight_fraction_leaf=0.0, n_estimators=300,
                       n_jobs=-1, oob_score=False, random_state=25, verbose=0,
                       warm_start=False)

Input2:
permuter = PermutationImportance(
    model,
    scoring='accuracy',
    n_iter=3,
    random_state=25
)
permuter.fit(X_val_encoded, y_val1)

Output2:
PermutationImportance(cv='prefit',
                      estimator=RandomForestClassifier(bootstrap=True,
                                                       ccp_alpha=0.0,
                                                       class_weight=None,
                                                       criterion='gini',
                                                       max_depth=2,
                                                       max_features='auto',
                                                       max_leaf_nodes=None,
                                                       max_samples=None,
                                                       min_impurity_decrease=0.0,
                                                       min_impurity_split=None,
                                                       min_samples_leaf=1,
                                                       min_samples_split=2,
                                                       min_weight_fraction_leaf=0.0,
                                                       n_estimators=300,
                                                       n_jobs=-1,
                                                       oob_score=False,
                                                       random_state=25,
                                                       verbose=0,
                                                       warm_start=False),
                      n_iter=3, random_state=25, refit=True,
                      scoring='accuracy')

(PROBLEM) Input3:
feature_names = X_val_encoded.columns.tolist()
pd.Series(permuter.feature_importances_, feature_names).sort_values()

(PROBLEM) Output3:
Player     0.0
POS        0.0
ATT        0.0
YDS        0.0
TDS        0.0
REC        0.0
YDS.1      0.0
TDS.1      0.0
FL         0.0
FPTS       0.0
Overall    0.0
pos_adp    0.0
dtype: float64

I expect to get values here, but instead I get zeros - am I doing something wrong or is that
a possible result?
In: permuter.feature_importances_
Out:array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])

","Turns out the issue was with the data I was passing in, rather than the code itself. 
The data had fewer than 70 observations, so after I was able to add more observations to it (just under 400), I was able to get permutation importances as expected.
",data-science
remove numbers in tuples and enter them in new rows in csv,"so I have an ugly CSV file with one column and only 2 rows, but it has many tuples in it, that looks like:




Column A




(1, 2, 3)(4, 5, 6)(7, 8, 9)


(3, 2, 1)(5, 3, 6)(9, 8, 7)




and I want to have it looks like




Column A
Column B
Column C




1
2
3


4
5
6


7
8
9


3
2
1


5
3
6


9
8
7




any suggestions?
","Since you have a data-science tag, why not use pandas ?
#pip install pandas
import pandas as pd

df = (pd.read_csv(""input.csv"", sep=""|"").squeeze()
           .str.strip(""()"").str.split(""\)\("", expand=True)
           .melt(ignore_index=False)[""value""].str.split("",\s*"", expand=True)
           .sort_index().set_axis([""Column A"", ""Column B"", ""Column C""], axis=1)
      )

Output :
print(df)

     Column A Column B Column C
0        1        2        3
0        4        5        6
0        7        8        9
1        3        2        1
1        5        3        6
1        9        8        7

",data-science
python version and performance in ml,"I couldn't find any answer on that question, but does Python version really matter when you implement different ML models (like catboost or xgboost) or NN (like pytorch). I do know that these models were written on C++ and other programming languages and my question may sound quite trivial, but does it really matter to upgrade Python version to increase performance and use less Memory
","If you think about the question at a lower level - you basically have 2 components.
The cpython interpreter executing statements, and the library running some optimised compute.
Generally for a model the interpreter will take very little time (maybe microseconds). This will probably be affected by the python version, but should not be considered for performance tuning.
You're better off checking different versions of the module, BLAS, Intel MKL, number of threads, onnx, torch jit, and similar things that will have much greater performance impacts.
Using cprofile might help you figure out what can be improved.
",data-science
how to handle nan values in rental price prediction project,"I am working on a rental price prediction project where I web scraped data from Facebook Marketplace. When extracting the areas of the properties, I am encountering many NaN values.
I am web scraping from a small city and it is unlikely that I will be able to find more data. How can I effectively handle the NaN values in my data? Are there any machine learning algorithms or external sources of information that can be used to impute missing values in this situation?
Any suggestions or advice would be greatly appreciated. Thank you in advance!
I have considered using the mean or median based on property type, number of bedrooms, and bathrooms, but I am not sure if this is the best approach.
","There are many methods that you can use when it comes to missing values in your data. As you mentioned general approach is to fill with mean-median. I recommend grouping them first then filling with mean or median.
df['a'].fillna(df.groupby('b')['a'].transform('mean'))

I recon you can use zipcode or something similar to group them.
Another thing you can do is before filling empty places, create another column that indicates if the values are missing. this may help your model to treat those values differently and don't overfit on those values.
For further info link
",data-science
how to display greyscaled image properly using pilow,"I'm trying to open an Image using PIL in python. The code I'm running works fine with other images (and greyscale images I produced myself). But this one image file doesn't display properly. It makes the Image pixels either true White or true Black instead of a regular greyscaled pixels.
I received this image, with other 3 very similar ones. But weirdly, only this one is giving me this unusual show()
I've already tried converting the Image to rgb. with the method convert, but the output is still a pure black and white image.
I've opened the image in aseprite (image editor) and saved again with another name and the copy doesn't reproduce the error.
Noticed as well that this image's bit density is 16 (different from all other images I've tested). And I couldn't find any other image with this density.
The image that gave me this trouble follows:

(I believe is downloadable)
For some reason, the display function in python notebooks can reveal the image correctly.
I can reproduce this malfunction with no other images.
To get the same malfunction I'm facing just download the image and open it with:
from PIL import Image

image = Image.open('image.png')
image.show()

","This should really be a comment, not an answer, but it’s too large... As I wrote in a comment, it seems that Pillow has some issues with handling “deep” images. Here’s an extremely ugly (and somewhat ham-fisted, as I am not good with numpy nor with Pillow) work-around, mostly to demonstrate the issue. I am sure there are more elegant ways to write this:
import numpy as np
from PIL import Image

image = Image.open('lCNSM.png')

# The mode is 'I' = 32-bit signed
print(f'{image.mode=}')

# Convert to numpy array so we can do math on it    
a32 = np.array(image)

# min=0, max=65535, indicating that only part of the
# 32-bit range is actually used
print(a32.dtype, a32.min(), a32.max())

# Scale down do 8 bits (via float...). Can numpy do bit shifts?
# I’m too lazy to find out. =)
a8 = (a32.astype(np.float64) * (1/256)).astype(np.uint8)

# Convert to image and show!
im8 = Image.fromarray(a8)
im8.show()

",data-science
how could i get pandas groupby not to take indices in account but values of my dataframe,"I have a dataFrame with dates and prices, for example :




date
price




2006
500


2007
2000


2007
3400


2006
5000




and i want to group my data by year so that i obtain :




2007
2006




2000
500


3400
5000




This is the code i tried :
df = my_old_df.groupby(['date'])
my_desried_df = pd.DataFrame(data=df.groups)
but i obtain what i desire but with the indices of the values not the value (the price inmy case) i expected.
","With pivot :
out = (
        df.assign(idx= df.groupby(""date"").cumcount().add(1))
            .pivot(index=""idx"", columns=""date"", values=""price"")
            .rename_axis(None, axis=1).reset_index(drop=True)
)

Output :
​
print(out)

   2006  2007
0   500  2000
1  5000  3400

",data-science
finding all possible sums of three integers within a list,"I need to use a list of perfect cubes to find all possible sums of three different numbers within that list. The sum must also be a perfect cube. My original solution was to nest 3 for loops to cover each number, but this tends to become quite inefficient, as the aforementioned list can be very long. For instance, the cubes list could contain all cubes lower than 6'000'000.
cubes = [1, 8, 27, 64, 125]

for item in cubes:
    for item2 in cubes: 
        for item3 in cubes:
            sum = item + item2 + item3
            if numpy.cbrt(sum) == int(numpy.cbrt(sum)):
                print(sum)

Does anyone have a more efficient way to do this?
","(Ugh... I just realized d doesn't have to be in the list, can be larger. So I'll need to extend this. Later...)
Solutions are cubes a < b < c < d with a + b + c == d. So d - c == a + b. Go through all pairs of c and d and check whether d - c occurs as a sum of some a and b. Takes about 0.01 seconds to solve the list of all cubes lower than 6'000'000.
from time import time

cubes = [i**3 for i in range(1, int(6e6**(1/3)) + 1)]

start = time()

a_plus_b = set()
sums = set()
for i, c in enumerate(cubes[2:], 2):
    b = cubes[i - 1]
    for a in cubes[:i-1]:
        a_plus_b.add(a + b)
    for d in cubes[i+1:]:
        if d - c in a_plus_b:
            sums.add(d)

print(time() - start)
print(sorted(sums))

Output (Attempt This Online!):
0.011432647705078125
[216, 729, 1728, 5832, 6859, 8000, 13824, 15625, 19683, 21952, 24389, 27000, 46656, 54872, 64000, 68921, 74088, 85184, 91125, 97336, 110592, 125000, 148877, 157464, 175616, 185193, 195112, 216000, 250047, 287496, 300763, 328509, 343000, 357911, 373248, 421875, 438976, 474552, 512000, 531441, 551368, 592704, 614125, 658503, 681472, 704969, 729000, 778688, 804357, 857375, 884736, 912673, 970299, 1000000, 1061208, 1092727, 1157625, 1191016, 1259712, 1331000, 1367631, 1404928, 1442897, 1481544, 1520875, 1560896, 1601613, 1728000, 1771561, 1815848, 1860867, 1953125, 2000376, 2048383, 2146689, 2299968, 2352637, 2406104, 2460375, 2571353, 2628072, 2685619, 2744000, 2803221, 2863288, 2985984, 3048625, 3176523, 3375000, 3442951, 3511808, 3581577, 3796416, 4019679, 4096000, 4251528, 4410944, 4657463, 4741632, 4913000, 5000211, 5088448, 5268024, 5359375, 5451776, 5545233, 5639752, 5735339, 5832000, 5929741]

Version showing all sums verbosely:
from collections import defaultdict

cubes = [i**3 for i in range(1, int(6e6**(1/3)) + 1)]

a_plus_b = defaultdict(list)
for i, c in enumerate(cubes[2:], 2):
    b = cubes[i - 1]
    for a in cubes[:i-1]:
        a_plus_b[a + b].append((a, b))
    for d in cubes[i+1:]:
        for a, b in a_plus_b[d - c]:
            print(f'{a} + {b} + {c} == {d}')

Output (Attempt This Online!):
27 + 64 + 125 == 216
1 + 216 + 512 == 729
216 + 512 + 1000 == 1728
729 + 1728 + 3375 == 5832
8 + 1728 + 4096 == 5832
343 + 2744 + 4913 == 8000
27 + 1000 + 5832 == 6859
1728 + 4096 + 8000 == 13824
5832 + 6859 + 9261 == 21952
64 + 4913 + 10648 == 15625
27 + 5832 + 13824 == 19683
3375 + 8000 + 15625 == 27000
1331 + 3375 + 19683 == 24389
5832 + 13824 + 27000 == 46656
64 + 13824 + 32768 == 46656
216 + 32768 + 35937 == 68921
2744 + 21952 + 39304 == 64000
9261 + 21952 + 42875 == 74088
216 + 8000 + 46656 == 54872
19683 + 27000 + 50653 == 97336
27 + 46656 + 50653 == 97336
8 + 4913 + 64000 == 68921
125 + 27000 + 64000 == 91125
13824 + 32768 + 64000 == 110592
4096 + 12167 + 68921 == 85184
46656 + 54872 + 74088 == 175616
512 + 39304 + 85184 == 125000
24389 + 39304 + 85184 == 148877
19683 + 46656 + 91125 == 157464
216 + 46656 + 110592 == 157464
3375 + 74088 + 117649 == 195112
27000 + 64000 + 125000 == 216000
9261 + 74088 + 132651 == 216000
1728 + 6859 + 148877 == 157464
729 + 27000 + 157464 == 185193
10648 + 27000 + 157464 == 195112
10648 + 132651 + 157464 == 300763
35937 + 85184 + 166375 == 287496
343 + 74088 + 175616 == 250047
343 + 157464 + 185193 == 343000
46656 + 110592 + 216000 == 373248
46656 + 54872 + 226981 == 328509
157464 + 185193 + 250047 == 592704
512 + 110592 + 262144 == 373248
125000 + 226981 + 262144 == 614125
39304 + 59319 + 274625 == 373248
59319 + 140608 + 274625 == 474552
54872 + 79507 + 287496 == 421875
1728 + 132651 + 287496 == 421875
1728 + 262144 + 287496 == 551368
21952 + 175616 + 314432 == 512000
6859 + 216000 + 328509 == 551368
195112 + 205379 + 328509 == 729000
2744 + 12167 + 343000 == 357911
74088 + 175616 + 343000 == 592704
29791 + 35937 + 373248 == 438976
1728 + 64000 + 373248 == 438976
729 + 157464 + 373248 == 531441
15625 + 110592 + 405224 == 531441
157464 + 216000 + 405224 == 778688
216 + 373248 + 405224 == 778688
21952 + 148877 + 421875 == 592704
91125 + 216000 + 421875 == 729000
17576 + 166375 + 474552 == 658503
54872 + 110592 + 493039 == 658503
8000 + 157464 + 493039 == 658503
91125 + 328509 + 493039 == 912673
64 + 39304 + 512000 == 551368
1000 + 216000 + 512000 == 729000
110592 + 262144 + 512000 == 884736
35937 + 91125 + 531441 == 658503
32768 + 97336 + 551368 == 681472
9261 + 79507 + 592704 == 681472
373248 + 438976 + 592704 == 1404928
32768 + 157464 + 614125 == 804357
42875 + 343000 + 614125 == 1000000
132651 + 314432 + 614125 == 1061208
15625 + 29791 + 636056 == 681472
4913 + 64000 + 636056 == 704969
15625 + 54872 + 658503 == 729000
1331 + 287496 + 681472 == 970299
4096 + 314432 + 681472 == 1000000
195112 + 314432 + 681472 == 1191016
3375 + 551368 + 704969 == 1259712
3375 + 125000 + 729000 == 857375
6859 + 148877 + 729000 == 884736
157464 + 373248 + 729000 == 1259712
35937 + 343000 + 778688 == 1157625
185193 + 438976 + 857375 == 1481544
1728 + 373248 + 884736 == 1259712
24389 + 421875 + 884736 == 1331000
125000 + 405224 + 912673 == 1442897
12167 + 636056 + 912673 == 1560896
636056 + 857375 + 912673 == 2406104
27000 + 592704 + 941192 == 1560896
5832 + 884736 + 970299 == 1860867
830584 + 884736 + 970299 == 2685619
216000 + 512000 + 1000000 == 1728000
6859 + 778688 + 1030301 == 1815848
1728 + 29791 + 1061208 == 1092727
74088 + 592704 + 1061208 == 1728000
117649 + 592704 + 1061208 == 1771561
2197 + 132651 + 1124864 == 1259712
2197 + 474552 + 1124864 == 1601613
250047 + 592704 + 1157625 == 2000376
12167 + 830584 + 1157625 == 2000376
729000 + 857375 + 1157625 == 2744000
13824 + 54872 + 1191016 == 1259712
4096 + 103823 + 1259712 == 1367631
5832 + 216000 + 1259712 == 1481544
85184 + 216000 + 1259712 == 1560896
85184 + 1061208 + 1259712 == 2406104
8000 + 614125 + 1331000 == 1953125
287496 + 681472 + 1331000 == 2299968
531441 + 729000 + 1367631 == 2628072
729 + 1259712 + 1367631 == 2628072
2744 + 592704 + 1404928 == 2000376
27 + 39304 + 1481544 == 1520875
2744 + 1259712 + 1481544 == 2744000
328509 + 778688 + 1520875 == 2628072
729 + 166375 + 1560896 == 1728000
85184 + 132651 + 1643032 == 1860867
117649 + 941192 + 1685159 == 2744000
216 + 132651 + 1728000 == 1860867
3375 + 729000 + 1728000 == 2460375
373248 + 884736 + 1728000 == 2985984
2197 + 274625 + 1771561 == 2048383
373248 + 438976 + 1815848 == 2628072
373248 + 614125 + 1815848 == 2803221
110592 + 328509 + 1860867 == 2299968
125 + 438976 + 1860867 == 2299968
54872 + 185193 + 1906624 == 2146689
328509 + 1860867 + 1906624 == 4096000
421875 + 1000000 + 1953125 == 3375000
9261 + 343000 + 2000376 == 2352637
1259712 + 1481544 + 2000376 == 4741632
85184 + 389017 + 2097152 == 2571353
4096 + 884736 + 2097152 == 2985984
1000000 + 1815848 + 2097152 == 4913000
314432 + 474552 + 2197000 == 2985984
474552 + 1124864 + 2197000 == 3796416
27 + 1771561 + 2248091 == 4019679
438976 + 636056 + 2299968 == 3375000
13824 + 1061208 + 2299968 == 3375000
658503 + 1061208 + 2299968 == 4019679
13824 + 2097152 + 2299968 == 4410944
166375 + 421875 + 2460375 == 3048625
531441 + 1259712 + 2460375 == 4251528
1728 + 531441 + 2515456 == 3048625
4913 + 1061208 + 2515456 == 3581577
175616 + 1404928 + 2515456 == 4096000
1225043 + 1259712 + 2515456 == 5000211
29791 + 262144 + 2571353 == 2863288
1 + 357911 + 2628072 == 2985984
357911 + 389017 + 2628072 == 3375000
54872 + 1728000 + 2628072 == 4410944
1 + 2460375 + 2628072 == 5088448
1560896 + 1643032 + 2628072 == 5832000
21952 + 97336 + 2744000 == 2863288
10648 + 421875 + 2744000 == 3176523
592704 + 1404928 + 2744000 == 4741632
884736 + 1225043 + 2803221 == 4913000
274625 + 658503 + 2863288 == 3796416
110592 + 2571353 + 2863288 == 5545233
238328 + 287496 + 2985984 == 3511808
13824 + 512000 + 2985984 == 3511808
5832 + 1259712 + 2985984 == 4251528
658503 + 1560896 + 3048625 == 5268024
328509 + 970299 + 3112136 == 4410944
91125 + 2000376 + 3176523 == 5268024
110592 + 2352637 + 3176523 == 5639752
97336 + 103823 + 3241792 == 3442951
205379 + 804357 + 3241792 == 4251528
125000 + 884736 + 3241792 == 4251528
175616 + 1191016 + 3375000 == 4741632
729000 + 1728000 + 3375000 == 5832000
1259712 + 1295029 + 3375000 == 5929741
6859 + 1481544 + 3511808 == 5000211
250047 + 2000376 + 3581577 == 5832000
21952 + 1685159 + 3652264 == 5359375
140608 + 1331000 + 3796416 == 5268024
438976 + 884736 + 3944312 == 5268024
64000 + 1259712 + 3944312 == 5268024
46656 + 185193 + 4019679 == 4251528
1728 + 636056 + 4019679 == 4657463
512 + 314432 + 4096000 == 4410944
8000 + 1728000 + 4096000 == 5832000
19683 + 729000 + 4251528 == 5000211
287496 + 729000 + 4251528 == 5268024
103823 + 912673 + 4251528 == 5268024
157464 + 512000 + 4330747 == 5000211
262144 + 778688 + 4410944 == 5451776
15625 + 778688 + 4657463 == 5451776
74088 + 636056 + 4741632 == 5451776
125000 + 238328 + 5088448 == 5451776
39304 + 512000 + 5088448 == 5639752
125000 + 438976 + 5268024 == 5832000
4913 + 185193 + 5545233 == 5735339

",data-science
pandas  irregular time series data  compare row to next 8 hours of rows,"Right now I am using pandas to analyze call center data. The data is structured as followed:

call_time = pd.to_datetime([
         '2020-01-01 01:00:00',
         '2020-01-01 09:00:00',
         '2020-01-01 01:00:00',
         '2020-01-01 03:00:00',
         '2020-01-01 04:00:00',
         '2020-01-01 06:00:00',
         '2020-01-01 01:00:00',
         '2020-01-01 10:00:00',
])
df = pd.DataFrame({'phone_number': ['a', 'a', 'b', 'b', 'b', 'b', 'c', 'c'],
                  'was_answered': [False, True, False, False, True, False, False, True],},
                  index=call_time)

>>> df
                    phone_number  was_answered
2020-01-01 01:00:00            a         False
2020-01-01 09:00:00            a          True
2020-01-01 01:00:00            b         False
2020-01-01 03:00:00            b         False
2020-01-01 04:00:00            b          True
2020-01-01 06:00:00            b         False
2020-01-01 01:00:00            c         False
2020-01-01 10:00:00            c          True

Each row represents a call at the call center.
The index (call_time) is the time the call started. phone_number shows the hashed phone numbers. was_answered denotes if the call was answered.
The data was sorted by phone_number first and then the call_time. Notice that the call_time is therefore not chronological.
What I would like to do is identify the return callers. Return callers are callers that were not answered the first time, but called back within 8 hours.
On top of this I need the time from the first call until the next call, regardless of wether the call was answered.
So it would look something like this:
                    phone_number  was_answered is_return_call time_until_return
2020-01-01 01:00:00            a         False          False              Null
2020-01-01 09:00:00            a          True           True          08:00:00
2020-01-01 01:00:00            b         False          False              Null
2020-01-01 03:00:00            b         False           True          02:00:00
2020-01-01 04:00:00            b          True           True          01:00:00
2020-01-01 06:00:00            b         False          False              Null
2020-01-01 01:00:00            c         False          False              Null
2020-01-01 10:00:00            c          True          False              Null

I have tried many thing. Does anyone know how to do this? If my goal is unclear, please let me know!
","You can solve this with rolling windows:
was_answered = df.groupby(""phone_number"", group_keys=True)[""was_answered""]

# When the call has never been answered in the previous 8
# hours, it's a return call. Since we use closed=""left"", if
# it's the first call in 8 hours, the window is empty, its
# sum is NaN and hence not considered a return call.
is_return_call = was_answered.rolling(""8H"", closed=""left"").sum().eq(0)

# Time difference since previous call
time_since_last_call = was_answered.apply(lambda w: w.index.to_series().diff())

result = pd.concat(
    [
        was_answered.rolling(1).sum().astype(""bool""),
        is_return_call.rename(""is_return_call""),
        # If it's not a return call, time_until_return = NA
        time_since_last_call.where(is_return_call, None).rename(""time_until_return""),
    ],
    axis=1,
)

# Alternatively, you can extract `was_answered` from the
# original frame but with modified index so that it can line
# up with the other 2 series
result = pd.concat(
    [   
        df.set_index(""phone_number"", append=True).swaplevel()[""was_answered""],
        is_return_call.rename(""is_return_call""),
        time_since_last_call.where(is_return_call, None).rename(""time_until_return""),
    ],
    axis=1,
)

Result:
                                  was_answered  is_return_call time_until_return
phone_number                                                                    
a            2020-01-01 01:00:00         False           False               NaT
             2020-01-01 09:00:00          True            True   0 days 08:00:00
b            2020-01-01 01:00:00         False           False               NaT
             2020-01-01 03:00:00         False            True   0 days 02:00:00
             2020-01-01 04:00:00          True            True   0 days 01:00:00
             2020-01-01 06:00:00         False           False               NaT
c            2020-01-01 01:00:00         False           False               NaT
             2020-01-01 10:00:00          True           False               NaT

",data-science
selenium returns empty values,"Can you help me to solve the follow issue?
I'm creating a function with Selenium to scrape some jobs from Glassdor, but even though I'm using the correct XPath to get the ""salary estimate"" it just returns the except results.
xpath = './/span[@class=""css-18034rf e1wijj242""]' 

try:
    salary_estimate.append(driver.find_element(By.XPATH, xpath).text)

except:
    salary_estimate.append(""#N/A"")
    pass

Here is the salary estimate on the job card and the search link:
salary estimate position
","That because the locator you are using is dynamically generated and changes on every reload and will not return anything
Try using this
//span[@data-test='detailSalary']

This will return you all ranges and when you do a .text you will get output as $74T - $1L (Glassdoor Est.)
Then to further Filter out the Salary ranges if you want , you have to extract just the parent text element
 parent_element = driver.find_element(By.XPATH,""//span[@data-test='detailSalary']"");
 print(driver.execute_script('return arguments[0].childNodes[0].textContent;', parent_element).strip())

This will give output as $55.00 - $85.00 Per hour
Similarly you can extract all by iterating through the list of elements
parent_list = driver.find_elements(By.XPATH,""//span[@data-test='detailSalary']"");


for el in parent_list:
  
   print(driver.execute_script('return arguments[0].childNodes[0].textContent;', el).strip());

",data-science
optimizing over two loss functions in difrent ranges,"I am optimizing over two loss functions which take very different values. To give an example: 
loss1 = 1534
loss2 = 0.723

and I want to optimize over loss1+loss2. Would rescaling loss1 to values closer to loss2 be a good idea? I tried the naive way of just multiplying loss2 by 1000, within the overall loss term (sum), but the problem is, as loss1 goes down (say around 600, 500) , loss2 becomes too large. 
My idea is to find a way to keep both loss terms in the same range, during the whole optimization process. What is the best way of doing this? 
","Perhaps you could use a min-max scaler to scale both losses between 0 and 1.
so by doing:
loss1 = mse(predicted, target)
loss2 = passion(predicted, target)

loss1_scaled = (loss1 - loss1.min())/(loss1.max() - loss1.min()) 
loss2_scaled = (loss2 - loss2.min())/(loss2.max() - loss2.min()) 


total_loss = loss1_scaled + loss2_scaled

",data-science
what is the issue here  i have already installed geopandas and folium from pip install command from powershell,"I was getting ready to perform a EDA on Tsunami Data as I was importing the following module geopandas module gave the error of not found so i installed it from the powershell by using pip but then too the error persisted and the same is happening with the folium too.
import os, math
import numpy as np
import pandas as pd
import geopandas as gpd

import matplotlib.pyplot as plt
import seaborn as sns
sns.set_style(""darkgrid"")

import folium
from folium import Choropleth, Circle, Marker
from folium.plugins import HeatMap, MarkerCluster

here is the error 
ModuleNotFoundError                       Traceback (most recent call last)
~\AppData\Local\Temp\ipykernel_9832\21392064.py in <module>
      2 import numpy as np
      3 import pandas as pd
----> 4 import geopandas as gpd
      5 
      6 import matplotlib.pyplot as plt

ModuleNotFoundError: No module named 'geopandas'
enter image description here

I was simply expecting it to work totally fine  i previously worked on this same system with several modules there was no error of such types.
","Solution:
enter the virtual env and then do the installation of the packages
source /env/script/activate
pip install %NAMEHERE%

",data-science
prince quotfamd object has no attribute 39plot_row_coordinates39quot,"As far as I can tell, this is the same implementation as every example of this function that I could find.
Calling famd.plot_row_coordinates() produces: 'FAMD' object has no attribute 'plot_row_coordinates'.
The prior lines run fine.
#FAMD
famd = FAMD(n_components = 10, n_iter = 3, random_state = 101)
famd.fit(df.drop('Target', axis=1)) #leave out response variable 'Target'
famd.eigenvalues_summary

#plot components
famd.transform(df)
famd.plot_row_coordinates(
    df.head(200),
    figsize = (15,10),
    color_labels=['Completion status: {}'.format(t) for t in df['Target']]
)

","The 'plot_row_coordinates' function got removed from the package on 22nd of February. You should now either use 'famd.plot' function or plot coordinates yourself, with code looking something like this:
coordinates = famd.row_coordinates(X)
x = coordinates[0].astype(np.float)
y = coordinates[1].astype(np.float)
plt.plot(x,y)

",data-science
calculating summation over months of pandas dataframe,"I have a pandas dataframe given below:
ID       Year       R1  R1_f
KAR1    20201001    1   5
KAR1    20201101    2   6
KAR1    20201201    3   7
KAR1    20210101    4   8
KAR1    20210201    5   9
KAR1    20210301    6   10
KAR1    20210401    7   11
KAR1    20210501    8   12
KAR1    20210601    9   13
KAR1    20210701    10  14
KAR1    20210801    11  15
KAR1    20210901    12  16
KAR2    20201001    4   9
KAR2    20201101    3   8
KAR2    20201201    2   7
KAR2    20210101    1   6
KAR2    20210201    9   5
KAR2    20210301    2   4
KAR2    20210401    6   3
KAR2    20210501    5   2
KAR2    20210601    3   1
KAR2    20210701    30  2
KAR2    20210801    34  3
KAR2    20210901    20  4

I need to transform above dataframe as given below:
    ID Year      R1_sum 3m_R1 6m_R1 9m_R1 12m_R1 R1_f 3m_R1_f 6m_R1_f 9m_R1_f 12m_R1_f 
   KAR1 20210901   12      33    57    72    78    16    45     81      108      126 
   KAR2 20210901   20      84    98    110   119    4     9      15      30        54

In above output dataframe:
R1_sum is having value equal to value in year 20210901 for both Id's.
3m_R1 is the summation of values of 3 months 20210901 to 20210701 for column R1
6m_R1 is the summation of values of 6 months from 20210901 to 20210401 for column R1
9m_R1 is the summation of values of 9 months from 20210901 to 20210101 for column R1
12m_R1 is the summation of values of 12 months from 20210901 to 20201001 for column R1
R1_f is having value equal to value in year 20210901 for both Id's.
3m_R1_f is the summation of values of 3 months 20210901 to 20210701 for column R1_f
6m_R1_f is the summation of values of 6 months from 20210901 to 20210401 for column R1_f
9m_R1_f is the summation of values of 9 months from 20210901 to 20210101 for column R1_f
12m_R1_f is the summation of values of 12 months from 20210901 to 20201001 for column R1_f
Please help
","For count from last months to first months per groups ID first crate helper groups by GroupBy.cumcount and aggregate sum, then aggregate sum with GroupBy.cumsum, reshape by DataFrame.unstack, flatten MultiIndex in columns and add to DataFrame created by GroupBy.last:
Data:
df = pd.DataFrame({'ID': ['KAR1', 'KAR1', 'KAR1', 'KAR1', 'KAR1', 'KAR1', 'KAR1', 
                          'KAR1', 'KAR1', 'KAR1', 'KAR1', 'KAR1', 'KAR2', 'KAR2', 'KAR2', 'KAR2', 'KAR2', 'KAR2', 'KAR2', 'KAR2', 'KAR2', 'KAR2', 'KAR2', 'KAR2'], 
                   'Year': [20201001, 20201101, 20201201, 20210101, 20210201, 20210301,
                            20210401, 20210501, 20210601, 20210701, 20210801, 20210901, 
                            20201001, 20201101, 20201201, 20210101, 20210201, 20210301,
                            20210401, 20210501, 20210601, 20210701, 20210801, 20210901], 
                   'R1': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 4, 3, 2, 1, 9, 2, 6, 5, 3, 30, 34, 20], 
                   'R1_f': [5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 9, 8, 7, 6, 5, 4, 3, 2, 1, 2, 3, 4]})
    
print (df)


cols = ['R1','R1_f']
g = df.groupby('ID').cumcount(ascending=False) // 3
df1 = df.groupby(['ID',g])[cols].sum().groupby(level=0).cumsum().unstack()
print (df1)
      R1               R1_f              
       0   1    2    3    0   1    2    3
ID                                       
KAR1  33  57   72   78   45  81  108  126
KAR2  84  98  110  119    9  15   30   54

df2 = df.groupby('ID')[['Year'] + cols].last()
df2.columns = pd.MultiIndex.from_product([df2.columns, [-1]])
print (df2)
            -1  -1   -1
ID                     
KAR1  20210901  12   16
KAR2  20210901  20    4

df = df2.join(df1).sort_index(axis=1)
df.columns = [f'{(b + 1) * 3}m_{a}' if b!=-1 else f'{a}_sum' for a, b in df.columns]
df = df.reset_index()
df.insert(1, 'Year', df.pop('Year_sum'))
print (df)
     ID      Year  R1_sum  3m_R1  6m_R1  9m_R1  12m_R1  R1_f_sum  3m_R1_f  \
0  KAR1  20210901      12     33     57     72      78        16       45   
1  KAR2  20210901      20     84     98    110     119         4        9   

   6m_R1_f  9m_R1_f  12m_R1_f  
0       81      108       126  
1       15       30        54  

",data-science
how to add a dataframe to another on python,"So I have 3 columns. ETA (eta/km*100) (a number), Climate, and month.
My purpose is to drop the values higher that 0.95 quartile and lower than 0.05 (the extreme cases on this dataset) for each subset of 3 months and Climate, and the reagroup the dataset on a single dataset.
The issue I'm having here is that even tho inside the ""for"" statement it does the job, when I print the resulting data frame, it only has the last subset (Hurricane, last 3 months) without dropping the extreme data.
I've tried concat, add and append. Not sure what I'm doing wrong here.
Climate = ['Sunny', 'Cloudy', 'Foggy', 'Rain', 'Storm', 'Hurricane']

newDf = df_Cl
newDf.iloc[0:0]


for cl in Climate:
    print (cl)
    for num in range(4, 14, 3):
        print (num)
        df_Temp = df.loc[(df['Climate'] == cl) & (df['month'] < num)]
        bajo = df_Temp['eta/km*100'].quantile(0.05)
        alto = df_Temp['eta/km*100'].quantile(0.95)
        df_Temp = df_Temp[df_Temp['eta/km*100'] > bajo]
        df_Temp = df_Temp[df_Temp['eta/km*100'] < alto]
        newDf.add(df_Temp)

I've also tried:
newDf += df_Temp

But all the values become ""NaN""
","Use:
Climate = ['Sunny', 'Cloudy', 'Foggy', 'Rain', 'Storm', 'Hurricane']

#filter only rows by Climate list
df1 = df[df['Climate'].isin(Climate)]

#create groups per Climate and each 3 months
g = df1.groupby(['Climate', df['month'].sub(1).floordiv(3)])['eta/km*100']

#filter between 0.05 and 0.95 quantile
out = df1[df1['eta/km*100'].between(g.quantile(0.05),
                                    g.quantile.quantile(0.95), inclusive='neither')]

Your solution working if append final df_Temp to lsit of DataFrames and last use concat for join together:
L = []
for cl in Climate:
    print (cl)
    for num in range(4, 14, 3):
        print (num)
        df_Temp = df.loc[(df['Climate'] == cl) & (df['month'] < num)]
        bajo = df_Temp['eta/km*100'].quantile(0.05)
        alto = df_Temp['eta/km*100'].quantile(0.95)
        df_Temp = df_Temp[df_Temp['eta/km*100'] > bajo]
        df_Temp = df_Temp[df_Temp['eta/km*100'] < alto]
        L.append(df_Temp)
        
out = pd.concat(L)

",data-science
apply a multiplication to random elements on my dataframe,"So what I want is to randomly chose a given amount of elements on my dataframe, and to those elements, apply an operation (which will be a multiplication by a number which will also be randomly chosen between a range) to an 'eta' column. I'm stuck on achieving this.
atm I've randomly gotten a list of index within my dataframe but what I dont know is how to only apply the multiplication to the elements with those index and not the rest.
Some help would be trully appreciated!
numExtreme = 50 #Amount of registres to apply the random modification.
mod_indices = np.random.choice(df.index, numExtreme, replace=False) #set of ""numExtreme"" indices randomly chosen.

","Use DataFrame.loc for select random indices and only column eta and multiple by random array between 1 and 50:
np.random.seed(123)
df = pd.DataFrame({'eta':np.random.randint(10, size=10),
                   'another':np.random.randint(10, size=10)})
print (df)
   eta  another
0    2        9
1    2        0
2    6        0
3    1        9
4    3        3
5    9        4
6    6        0
7    1        0
8    0        4
9    1        1

numExtreme = 5 #Amount of registres to apply the random modification.
mod_indices = np.random.choice(df.index, numExtreme, replace=False)

print (mod_indices)
[5 1 0 8 6]

arr = np.random.randint(1, 50, len(mod_indices))
print (arr)
[49  8 42 36 29]


df.loc[mod_indices, 'eta'] *= arr
print (df)
   eta  another
0   84        9
1   16        0
2    6        0
3    1        9
4    3        3
5  441        4
6  174        0
7    1        0
8    0        4
9    1        1

",data-science
is it possible to create your own character symbols,"So I have tried using private character editor for creating character symbols, but it is impossible to export them to be shared with other users. However I was wondering is there a way to create your own character set? Is there a programming language for it or different OS? I know characters have 64x64 grid, which is available in private character editor.
You may ask why I'm doing this. Well I'm learning how character symbols are created, so I want to create and export. I know that unicode or ascii dominates, however if anyone knows anything, please do let me know :)
","Where do I start? You're mixing together several different layers/systems, so it's hard to know what exactly you need/want, but I'll try to unwind some of them to at least give you directions on where to continue looking.
A ""character"" is a very vague term that can be used to refer to and/or be composed of any or all of the following:

an abstract description of the character, like ""the latin captial letter A"" to describe A.
some ID of some kind to identify that character. In Unicode the ID of the latin capital letter A is U+0041 (that's hex, the equivalent decimal value is 65). In Unicode that's called the ""unicode codepoint"".
a description of how to encode these #1 and/or #2 into bytes to store in files/transfer/... ASCII is one such encoding (a very limited, old one that can only use a very small set of characters). ISO-8859-1 or Latin1 are slightly newer ones but still very limited. UTF-8 and UTF-16 are modern ones that can encode every character that Unicode describes.
One (or more) glyph (i.e. physical shape description) that tells the computer how that character is actually drawn to the screen (or printed).

Now if you want to invent your own new character, you need to do something for all 4 of those.
#1 is fairly straightforward: think of it and write it down.
#2 is already harder: pretty much the standard to give away those IDs is the Unicode specification these days. While that universality is great and it solved a ton of problems, you can't really just ""add something"" to Unicode on your own (that's also one of its majors strengths, by the way).
There is an escape hatch, however: Unicode provides so-called Private Use Areas which are ranges of codepoints that Unicode explicitly promises to never officially assign a meaning so that they can be used by some software internally.
You could simply pick one of those IDs and say that this is now your new character. Of course no one else will agree with you (that's the point of a Private Use Area), but we'll care about that later.
We can solve #3 by simply using one of the universal encodings (preferably UTF-8), if we decide to use the PUA. If we don't use the PUA, then ... well, you're basically out of luck, because you'll have to define an entirely new encoding that you still have to tell everyone else about (and convince them to support/use it).
And #4 is where we actually start defining what the character actually looks like. You say that ""characters have 64x64 grid"" which is really just one possible way. Most charaters on modern computers are drawn with vector fonts, mostly TrueType and OpenType. What you describe is a so called Bitmap Font.
Fonts basically define the shape of characters (usually identified by the Unicode codepoint) by providing an image (either bitmap or vector) that the computer ought to use to draw that character (in reality it's a bit more complex than that, because some glyphs might actually represent multiple unicode codepoints or a single codepoints might be represented by multiple different glyphs, depending on what's around it).
Now if you want to define your own font that has a glyph for your character, you simply have to assign the shape you want to the PUA codepoint you picked earlier.
Now: no one else will know what you mean when you use that PUA codepoint, but if you tell them ""oh, and make sure to render that with the font I provided"", then it will at least look like you want it to.
So at a high level, what you need to do to define your own character that you can share with your friends:

pick a codepoint from one of the Private Use Areas. Conflicts with what others picked are unavoidable.
create a font that has a glyph for that codepoint (using a tool like FontForge, for example)
send some text with that codepoint to your friends and tell them to use the font you also sent them to display it.

I glanced over some fairly involved details, but articles like this one should help fill in some of the gaps.
",data-science
pandas groupby only same id and when column value is false,"I have the following problem I am trying to solve currently. A dataframe with a lot of numerical columns consisting out of a ""serial_number"" column that act as ID. A column ""update"" that is either True or False. And also some numerical columns that I need to sum/divide up with the following: Sum/divide all numerical columns where the rows are ""update""=False to the next rows where ""update"" is True (including the row where ""update"" = True.
To give you an additional context, the entries are used for training a machine learning model, but for the rows where ""update""=false I don't have the target variable. Therefore, I need to sum or mean the values to the next ""update""=true row.
Thanks in advance!
As an example, this would be the input table:




serial_number
model
numerical_mean
numerical_1_sum
numerical_2_sum
update




a
2023-01-01
5
10
20
False


a
2023-01-02
10
15
10
False


a
2023-01-03
15
15
10
True


b
2023-01-01
10
15
10
False


b
2023-01-02
15
15
10
True


b
2023-01-03
15
15
10
False


b
2023-01-04
15
15
10
True


b
2023-01-05
15
15
10
False


c
2023-01-04
15
15
10
True




The resulting output should look like this:




serial_number
date
numerical_mean
numerical_1_sum
numerical_2_sum
update




a
2023-01-03
10
40
40
True


b
2023-01-02
12.5
30
20
True


b
2023-01-04
15
30
20
True


c
2023-01-04
15
15
10
True




The output table has the same amount of rows as the input table has rows where ""update""=True. So basically, I try to take all rows within the same serial_number between the first ""update""=false and first ""update""=true rows and either sum them up or take the average.
","Code
# filter the columns that you would like to aggregate
c1 = df.filter(like='_sum')
c2 = df.filter(like='_mean')

# create a agg dictionary which maps column names
# to the corresponding aggregation functions
agg_dict = {
    'model': 'last', 
    'update': 'any',
    **dict.fromkeys(c1, 'sum'), 
    **dict.fromkeys(c2, 'mean'),
}

# grouper to identify different blocks of rows followed by True
b = df[::-1]['update'].cumsum()

# group the dataframe by serial_number and blocks and aggregate
result = df.groupby(['serial_number', b]).agg(agg_dict)

# Query the results to remove the rows that do 
# not have any subsequent rows with 'update=true',
# for example, (b, 2023-01-05).
result = result.droplevel(1).query('update').reset_index()

Result
  serial_number       model  update  numerical_1_sum  numerical_2_sum  numerical_mean
0             a  2023-01-03    True               40               40            10.0
1             b  2023-01-04    True               30               20            15.0
2             b  2023-01-02    True               30               20            12.5
3             c  2023-01-04    True               15               10            15.0

",data-science
is it possible to have dask return a default value if a delayed object uses too much memory,"Is it possible to have Dask return a default value if a delayed object uses too much memory?
I want to evaluate a list of machine learning pipelines on large datasets in parallel. I am doing this in a loop where I generate models/pipelines then evaluate them.
This is how I am executing the parallelization:
for i in range(10):
   pipeline_list = generate_next_pipelines()
   scores = dask.compute(*[dask.delayed(fit_and_score)(pipeline, X, y) for pipeline in pipeline_list])
   # save/print scores

I also get errors about high unmanaged memory usage. Is there a step I am missing that would either reduce memory usage and/or free the unreleased memory more often?
I increased the memory limit by setting a LocalCluster to the Max memory of the system. This allows the code to run, but if a task requests more memory than available, the entire script crashes. I would like for Dasks to instead return a default value such as ""Not enough RAM"" if a given worker goes over the memory limit.
cluster = LocalCluster(n_workers=n_jobs, 
                       threads_per_worker=1,
                       memory_limit='64GB')
client = Client(cluster)

Thanks for the help
","If you function raises a MemoryError, then you can use standard python ways to detect it and return something different
@dask.delayed
def func_or(pipeline, X, y):
    try:
         return fit_and_score(pipeline, X, y)
    except MemoryError:
         return alternate_thing

However, if your worker is being killed completely due to memory use, your function will return with an exception (KilledWorker typically) and you need to detect this in the return value. You may with to use the client.submit API instead of delayed/dask.compute to get realtime information on the state of each element.
",data-science
create new column that counts each string match,"I aim to create a new column based on multiple string matches. This new column will return a 1 number for every match on the event column.
        Date        Event
0 2022-11-01        Breakfast
1 2022-11-01        Breakfast
2 2022-11-01        Lunch
3 2022-11-02        Breakfast
4 2022-11-02        Lunch

After
        Date        Event        Breakfast        Lunch
0 2022-11-01        Breakfast        1            null
1 2022-11-01        Breakfast        1            null
2 2022-11-01        Lunch            null         1
3 2022-11-02        Breakfast        1            null
4 2022-11-02        Lunch            null         1

","You can use pd.get_dummies for the task:
out = pd.concat([df, pd.get_dummies(df['Event'])], axis=1)
print(out)

Prints:
         Date      Event  Breakfast  Lunch
0  2022-11-01  Breakfast          1      0
1  2022-11-01  Breakfast          1      0
2  2022-11-01      Lunch          0      1
3  2022-11-02  Breakfast          1      0
4  2022-11-02      Lunch          0      1


If you want None instead 0:
out = out.replace({0: None})
print(out)

Prints:
         Date      Event Breakfast Lunch
0  2022-11-01  Breakfast         1  None
1  2022-11-01  Breakfast         1  None
2  2022-11-01      Lunch      None     1
3  2022-11-02  Breakfast         1  None
4  2022-11-02      Lunch      None     1

",data-science
matplotlib multiple axes mixups,"I have a problem with a multi axis matplotlib plot. The code is close to what I want but somehow axes are getting mixed up. The ticks are missing on ax4 aka the green y-axis but only show up on ax2 (the red one) and the labels are duplicated and appear on both axes, ax2 and ax4.
import numpy as np
import matplotlib.pyplot as plt

# Generate fake data
distance = np.logspace(2, 4, num=50)
a_detector = 10**4 / distance**1.5
b_detector = 10**5 / distance**1.6
c_detector = 10**3 / distance**1.4
d_detector = 10**2 / distance**1.2

# Create figure and axes
fig, ax1 = plt.subplots(figsize=(20, 10))

ax1.plot(distance, a_detector, 'bo-', label='A')
ax1.set_xlabel('Shower Plane Distance [meter]')
ax1.set_ylabel('signal type I', color='blue')
ax1.set_xscale('log')
ax1.set_yscale('log')
ax1.tick_params(axis='y', labelcolor='blue')

ax2 = ax1.twinx()
ax2.plot(distance, b_detector, 'ro-', label='B')
ax2.set_ylabel('signal type II', color='red')
ax2.set_yscale('log')
ax2.tick_params(axis='y', labelcolor='red')

ax3 = ax1.twinx()
ax3.spines['right'].set_position(('outward', 90))
ax3.plot(distance, c_detector, 'ks-', label='C')
ax3.set_ylabel('signal type III', color='black')
ax3.set_yscale('log')
ax3.tick_params(axis='y', labelcolor='black')

ax4 = ax1.twinx()
ax4.spines['left'].set_position(('outward', 90))
ax4.plot(distance, d_detector, 'g^-', label='D')
ax4.set_ylabel('signal type IV', color='green')
ax4.set_yscale('log')
ax4.tick_params(axis='y', labelcolor='green')
ax4.yaxis.set_label_position('left')
ax4.yaxis.set_tick_params(labelleft=True)

fig.legend(loc='upper right', bbox_to_anchor=(0.89, 0.86))
plt.show()

","You just need to replace:
ax4.yaxis.set_tick_params(labelleft=True)

by:
ax4.yaxis.set_tick_params(which='major', left=True, right=False, labelleft=True, labelright=False)  
ax4.yaxis.set_tick_params(which='minor', left=True, right=False, labelleft=True, labelright=False)

You put the yaxis major tick labels to the left but you also need to remove them from their original location with labelright=False.
This has to be done also for the tick labels (previously it was the tick marks) and also for the minor ticks (default is major ticks).

",matplotlib
python finding outliers from a trend of data,"Notice this post is not duplicated to any of the following relevant pieces on SO:

Find The Parity Outlier Python

Python: Find outliers inside a list

Find outlier using z score


I was given data in an experiment:

    import matplotlib.pyplot as plt
    
    x = [22, 24, 26, 28, 30, 32, 34, 36, 38, 40, 42, 44, 46, 48, 50]
    y_NaOH = [94.2, 146.2, 222.2, 276.2, 336.2, 372.2, 428.2, 542.2, 576.2, 684.2, 766.2, 848.2, 904.2, 1042.2, 1136.2]
    y_NaHCO3 = [232.0, 308.0, 322.0, 374.0, 436.0, 494.0, 592.0, 660.0, 704.0, 824.0, 900.0, 958.0, 1048.0, 1138.0, 1232.0]
    y_BaOH2 = [493.1, 533.1, 549.1, 607.1, 665.1, 731.1, 797.1, 867.1, 971.1, 1007.1, 1091.1, 1221.1, 1311.1, 1371.1, 1497.1, ]
    
    plt.plot(x, y_NaOH)
    plt.plot(x, y_NaHCO3)
    plt.plot(x, y_BaOH2)
    plt.show()



However, I had trouble marking the outliers, here's what I have tried:

    import matplotlib.pyplot as plt
    import statistics
    
    x = [22, 24, 26, 28, 30, 32, 34, 36, 38, 40, 42, 44, 46, 48, 50]
    y_NaOH = [94.2, 146.2, 222.2, 276.2, 336.2, 372.2, 428.2, 542.2, 576.2, 684.2, 766.2, 848.2, 904.2, 1042.2, 1136.2]
    y_NaHCO3 = [232.0, 308.0, 322.0, 374.0, 436.0, 494.0, 592.0, 660.0, 704.0, 824.0, 900.0, 958.0, 1048.0, 1138.0, 1232.0]
    y_BaOH2 = [493.1, 533.1, 549.1, 607.1, 665.1, 731.1, 797.1, 867.1, 971.1, 1007.1, 1091.1, 1221.1, 1311.1, 1371.1, 1497.1, ]
    
    # plt.plot(x, y_NaOH)
    # plt.plot(x, y_NaHCO3)
    # plt.plot(x, y_BaOH2)
    # plt.show()
    
    
    def detect_outlier(data_1):
        threshold = 1
        mean_1 = statistics.mean(data_1)
        std_1 = statistics.stdev(data_1)
        result_dataset = [y  for y in data_1 if abs((y - mean_1)/std_1)<=threshold ]
    
        return result_dataset
    
    
    if __name__==""__main__"":
        dataset = y_NaHCO3
        result_dataset = detect_outlier(dataset)
        print(result_dataset)
        # [374.0, 436.0, 494.0, 592.0, 660.0, 704.0, 824.0, 900.0, 958.0]


Incorrectly, this method always filter out the edge values of my data, actually I seek to remove the dots that doesn't fit the curve.

Plus, I can observe the shape of the curve and mark the outliers manually, but it really costs a lot of time. I will be very grateful for your help.

Expected output
I want to sketch the data in line and mark the outliers as dots, for example:

    from matplotlib import pyplot as plt
    
    x = [22, 24, 26, 28, 30, 32, 34, 36, 38, 40, 42, 44, 46, 48, 50]
    y_NaOH = [94.2, 146.2, 222.2, 276.2, 336.2, 372.2, 428.2, 542.2, 576.2, 684.2, 766.2, 848.2, 904.2, 1042.2, 1136.2]
    y_NaHCO3 = [232.0, 308.0, 322.0, 374.0, 436.0, 494.0, 592.0, 660.0, 704.0, 824.0, 900.0, 958.0, 1048.0, 1138.0, 1232.0]
    y_BaOH2 = [493.1, 533.1, 549.1, 607.1, 665.1, 731.1, 797.1, 867.1, 971.1, 1007.1, 1091.1, 1221.1, 1311.1, 1371.1, 1497.1, ]
    
    o_NaOH = [542.2]
    o_NaHCO3 = [308.0]
    o_BaOH2 = [493.1]
    
    
    def sketch_rejected(xv, yv, y_out):
        nx = []
        ny = []
        x_out = []
        for ii, dd in enumerate(yv):
            if dd not in y_out:
                nx.append(xv[ii])
                ny.append(dd)
            else:
                x_out.append(xv[ii])
        plt.plot(nx, ny)
        plt.scatter(x_out, y_out)
    
    
    sketch_rejected(x, y_NaOH, o_NaOH)
    sketch_rejected(x, y_NaHCO3, o_NaHCO3)
    sketch_rejected(x, y_BaOH2, o_BaOH2)
    
    plt.show()




the outliers are the spiky parts of the curve which the dot doesn't fit the gradient.

Instead of manually sketch each graph and identify the outliers, can I use a module to regress the data at first, then calculate the outliers.
In real life, I have tons of testing result and I don't know the general equation of each.
Appreciate for your help.
","There are quite a few GitHub repos for data science, all you have to do is complete your git installation
For using outliers.py

    from outliers.variance import graph
    
    x = [22, 24, 26, 28, 30, 32, 34, 36, 38, 40, 42, 44, 46, 48, 50]
    y_NaOH = [94.2, 146.2, 222.2, 276.2, 336.2, 372.2, 428.2, 542.2, 576.2, 684.2, 766.2, 848.2, 904.2, 1042.2, 1136.2]
    y_NaHCO3 = [232.0, 308.0, 322.0, 374.0, 436.0, 494.0, 592.0, 660.0, 704.0, 824.0, 900.0, 958.0, 1048.0, 1138.0, 1232.0]
    y_BaOH2 = [493.1, 533.1, 549.1, 607.1, 665.1, 731.1, 797.1, 867.1, 971.1, 1007.1, 1091.1, 1221.1, 1311.1, 1371.1, 1497.1, ]
    
    graph(
        xs=x,
        ys=[y_NaOH, y_NaHCO3, y_BaOH2],
        title='title',
        legends=[f'legend {i + 1}' for i in range(len(x))],
        xlabel='xlabel',
        ylabel='ylabel',
    )
    



",matplotlib
change number of ticks labeled using logformatter in matplotlib,"In Matplotlib's documentation for matplotlib.ticker.LogFormatter it is stated that with the parameter minor_thresholds one can control the labeling of minor ticks:

If labelOnlyBase is False, these two numbers control the labeling of ticks that are not at integer powers of base; normally these are the minor ticks. The controlling parameter is the log of the axis data range. In the typical case where base is 10 it is the number of decades spanned by the axis, so we can call it 'numdec'. If numdec <= all, all minor ticks will be labeled. If all < numdec <= subset, then only a subset of minor ticks will be labeled, so as to avoid crowding. If numdec > subset then no minor ticks will be labeled.

I would like to change the number of automatically labeled ticks when all < numdec <= subset, because sometimes it does not fully avoid crowding, for example this figure

was generated using LogFormatterSciNotation, which has LogFormatter as base:
import matplotlib.ticker as ticker

ax.set_minor_formatter(ticker.LogFormatterSciNotation(base=10, minor_thresholds=(1, 0.4)))

In this case, numdec = 0.7 and we are in the case all < numdec <= subset where only a subset of minor ticks are labeled, for instance three.
Is there a way to change this number of labeled ticks, without specifying their values?
","This appears to be hardwired, to reduce the number of ticks by a factor of 2, here.
Note that this method (.set_locs()) is essentially setting the ._sublabels attribute of the Formatter instance. This method is called automatically, so we can't just set ._sublabels ourselves to overwrite them, but we can rewrite the .set_locs method to modify its behaviour.
In this minimal example, I have assumed that you will only do this when all < numdec <= subset is True, so I haven't accounted for the other cases. For a more complete solution you will probably want to do this more thoroughly.
import matplotlib.pyplot as plt
import matplotlib.ticker as ticker
import numpy as np

# Set up a simple plot to demonstrate the behaviour
fig, ax = plt.subplots()
ax.set_xscale('log')
ax.set_xlim(.1, .8)
fmt = ticker.LogFormatterSciNotation(base=10, minor_thresholds=(1, 0.4))

def my_locs(self, locs=None):
    '''
    Redefine the set_locs method of the formatter
    Here I've used the ""locs"" parameter to control the number
    of tick labels; in the existing method, locs is not used. 
    '''
    b = self._base
    c = np.geomspace(1, b, int(b)//int(locs) + 1)
    self._sublabels = set(np.round(c))

## Tune the number of labels you want with the locs value.
## Higher numbers = fewer labels
locs = 3
fmt.set_locs = lambda x: my_locs(fmt, locs)

ax.xaxis.set_minor_formatter(fmt)

With locs=2 (equivalent to the default value)

With locs=3

With locs=4

",matplotlib
matplotlib legend not respecting content size with lualatex,"I need to generate my matplotlib plots using lualatex instead of pdflatex. Among other things, I am using fontspec to change the document fonts. Below I am using this as an example and set lmroman10-regular.otf as the font. This creates a few issues. One is that the handles in the legends are not fully centered and are followed by some whitespace before the right border:

The python code generating the intermediate .pgf file looks like this:
import matplotlib
import numpy

from matplotlib import pyplot

x = numpy.linspace(-1, 1)
y = x ** 2

matplotlib.rcParams[""figure.figsize""] = (3, 2.5)
matplotlib.rcParams[""font.family""] = ""serif""
matplotlib.rcParams[""font.size""] = 10
matplotlib.rcParams[""legend.fontsize""] = 8
matplotlib.rcParams[""pgf.texsystem""] = ""lualatex""

PREAMBLE = r""""""\usepackage{ifluatex}
\ifluatex
\usepackage{fontspec}
\setmainfont{lmroman10-regular.otf}
\fi
""""""

matplotlib.rcParams[""text.latex.preamble""] = PREAMBLE
matplotlib.rcParams[""pgf.preamble""] = PREAMBLE
matplotlib.rcParams[""text.usetex""] = True

pyplot.plot(x, y, label=""this is the data"")
pyplot.legend()
pyplot.xlabel(""xlabel"")
pyplot.tight_layout()
pyplot.savefig(""lualatex_test.pgf"")

The .pgf file is then embedded in a latex document. It seems it is not possible to directly compile to .pdf since then the document font will not be the selected font which can for example be seen by setting the font to someting more different like AntPoltExpd-Italic.otf. Also note note that the \ifluatex statement has to be added around the lualatex-only code since matplotlib uses pdflatex to determine the dimensions of text fragements, as will be seen below.
For the sake of this simple example, the .tex file to render the .pgf may be just:
\documentclass[11pt]{scrbook}
\usepackage{pgf}
\usepackage{fontspec}
\setmainfont{lmroman10-regular.otf}
\newcommand{\mathdefault}[1]{#1}
\begin{document}
\input{lualatex_test.pgf}
\end{document}

which can be typeset using lualatex <filename> and results in the figure shown above (without the red arrow).
I thought I had identified the reason for this but it seems I missed something. As mentioned above, matplotlib computes the dimensions of the text patches by actually placing them in a latex templated and compiling it using pdflatex. This happens in the matplotlib.texmanager.TexManager class in the corresponding file on the main branch for example. I thought I could fix it like this:

class TexManager:

    ...

    @classmethod
    def get_text_width_height_descent(cls, tex, fontsize, renderer=None):
        """"""Return width, height and descent of the text.""""""
        if tex.strip() == '':
            return 0, 0, 0
        dvifile = cls.make_dvi(tex, fontsize)
        dpi_fraction = renderer.points_to_pixels(1.) if renderer else 1
        with dviread.Dvi(dvifile, 72 * dpi_fraction) as dvi:
            page, = dvi
        # A total height (including the descent) needs to be returned.
        w = page.width
        # !!!
        if tex == ""this is the data"":
            w /= 1.14
            print(""fixed width"")
        # !!!
        return w, page.height + page.descent, page.descent

which, to my understanding, should trick matplotlib into thinking the text is shorter by a factor of 1.14 (just a guess, should be adapted once the solution works). The code definitely gets called, since ""fixed width"" gets printed. But the gap is not fixed:

How can I fix this issue? How is matplotlib computing the legend content's width and can I maybe patch this to account for the correct width? Let's assume I know that the width error factor for the font size 8 is approximately 1.14. I can easily determine this for other fonts and font sizes.
","It seems that when using custom font settings, you have to set the following rcParam:
matplotlib.rcParams[""pgf.rcfonts""] = False

Otherwise, if I understand it correctly, the font settings are applied from the rcParams.
This solves the spacing issue with your example:

For more details, see also the documentation of the .pgf backend where this parameter is exaplained.
",matplotlib
plot surfaces on a cube,"I would like to plot surfaces to a cube with matplotlib. I am trying to use ax.plot_surface(X, Y, Z), however I am a bit confused. What should the X, Y and Z represent as 2D arrays?
import numpy as np
from mpl_toolkits.mplot3d import Axes3D
import matplotlib.pyplot as plt

points = np.array([[-1, -1, -1],
                      [1, -1, -1 ],
                      [1, 1, -1],
                      [-1, 1, -1],
                      [-1, -1, 1],
                      [1, -1, 1 ],
                      [1, 1, 1],
                      [-1, 1, 1]])

fig = plt.figure()
ax = fig.add_subplot(111, projection='3d')
# ax.plot_surface(X, Y, Z)  # how?
ax.scatter3D(points[:, 0], points[:, 1], points[:, 2])
ax.set_xlabel('X')
ax.set_ylabel('Y')
ax.set_zlabel('Z')
plt.show()

","Each face of the cube is a surface for which you can either define each corner yourself, or use meshgrid:
import numpy as np
from mpl_toolkits.mplot3d import Axes3D
import matplotlib.pyplot as plt

points = np.array([[-1, -1, -1],
                      [1, -1, -1 ],
                      [1, 1, -1],
                      [-1, 1, -1],
                      [-1, -1, 1],
                      [1, -1, 1 ],
                      [1, 1, 1],
                      [-1, 1, 1]])

fig = plt.figure()
ax = fig.add_subplot(111, projection='3d')
r = [-1,1]
X, Y = np.meshgrid(r, r)
z = np.array([[1, 1]])
ax.plot_surface(X,Y, 1*z, alpha=0.5)
ax.plot_surface(X,Y,-1*z, alpha=0.5)
ax.plot_surface(X,-1*z,Y, alpha=0.5)
ax.plot_surface(X,1*z,Y, alpha=0.5)
ax.plot_surface(1*z,X,Y, alpha=0.5)
ax.plot_surface(-1*z,X,Y, alpha=0.5)
ax.scatter3D(points[:, 0], points[:, 1], points[:, 2])
ax.set_xlabel('X')
ax.set_ylabel('Y')
ax.set_zlabel('Z')
plt.show()


X,Y, and Z are (the same) list of 2D points:
>>> numpy.meshgrid([-1,1], [-1,1])
[array([[-1,  1],
       [-1,  1]]), array([[-1, -1],
       [ 1,  1]])]

",matplotlib
gridspec with shared axes in python,"This solution to another thread suggests using gridspec.GridSpec instead of plt.subplots. However, when I share axes between subplots, I usually use a syntax like the following
  fig, axes = plt.subplots(N, 1, sharex='col', sharey=True, figsize=(3,18))

How can I specify sharex and sharey when I use GridSpec ?
","First off, there's an easier workaround for your original problem, as long as you're okay with being slightly imprecise. Just reset the top extent of the subplots to the default after calling tight_layout:
fig, axes = plt.subplots(ncols=2, sharey=True)
plt.setp(axes, title='Test')
fig.suptitle('An overall title', size=20)

fig.tight_layout()
fig.subplots_adjust(top=0.9) 

plt.show()



However, to answer your question, you'll need to create the subplots at a slightly lower level to use gridspec.  If you want to replicate the hiding of shared axes like subplots does, you'll need to do that manually, by using the sharey argument to Figure.add_subplot and hiding the duplicated ticks with plt.setp(ax.get_yticklabels(), visible=False).
As an example:
import matplotlib.pyplot as plt
from matplotlib import gridspec

fig = plt.figure()
gs = gridspec.GridSpec(1,2)
ax1 = fig.add_subplot(gs[0])
ax2 = fig.add_subplot(gs[1], sharey=ax1)
plt.setp(ax2.get_yticklabels(), visible=False)

plt.setp([ax1, ax2], title='Test')
fig.suptitle('An overall title', size=20)
gs.tight_layout(fig, rect=[0, 0, 1, 0.97])

plt.show()


",matplotlib
how to read the data from log file and then calculate the results and plot a graph using python,"I have a log file looks like this:
# seconds.nanoseconds+offset(sequence)
1737119268.154152577+1.009023e-03(0)
1737119268.174213704+1.055868e-03(1)
1737119268.194189758+9.929340e-04(2)
1737119268.214228223+1.148472e-03(3)
1737119268.234281484+1.086814e-03(4)
1737119268.254280558+1.110649e-03(5)
1737119268.274280203+1.066759e-03(6)
1737119268.294279759+1.261670e-03(7)
1737119268.314287395+1.231778e-03(8)
1737119268.334195574+9.443450e-04(9)
1737119268.354297130+9.879940e-04(10)
1737119268.374272817+1.080131e-03(11)
1737119268.394278885+1.024396e-03(12)
1737119268.414284465+9.852860e-04(13)
1737119268.434281287+1.055045e-03(14)
1737119268.454279880+1.374439e-03(15)
1737119268.474283558+1.287552e-03(16)
1737119268.494280915+1.188272e-03(17)
1737119268.514209498+9.444750e-04(18)

I want to write a Python script to read the data from the log file and then calculate the results, for example: 1737119268.154152577+1.009023e-03 and split the numbers in '(0)'. Then I want this Python script to plot a graph of the calculated results.
So far I tried this
log_Name = 'test_rtt_25-01-17_13-07-41_values5_rate50.log'
log_Path = ""/home/ubuntu/results-25-01-09-docker/""
true_Path = log_Path + log_Name
with open(true_Path, ""r"") as f:
    lines = f.readlines()
    data = []
    for line in lines:
        print(line)
        data.append(line.split('('))

print(data)

But I don't really get what I want. How can I let the script calculate and plot a graph of the the results?
","You need to split each line twice: first on ( to separate the time from sequence, then split the time on + to calculate the total. Here's how I did it:
import matplotlib.pyplot as plt
from datetime import datetime, timezone

# Lists for timestamps and sequences
timestamps = []
sequences = []

# Read and process the log file
with open('log.txt', 'r') as f:
    for line in f:
        if not line.startswith('#'):  # Skip comments
            # Split by ( to separate timestamp+offset from sequence
            time_part, seq_part = line.strip().split('(')
            
            # Split time_part by + to get base time and offset
            base, offset = time_part.split('+')
            
            # Calculate total time by adding the base time and offset
            total_seconds = float(base) + float(offset)
            timestamps.append(total_seconds)
            
            # Extract sequence number by removing ')'
            seq = int(seq_part[:-1])
            sequences.append(seq)

# Convert timestamps to timezone-aware UTC datetime objects
utc_times = [datetime.fromtimestamp(ts, tz=timezone.utc) for ts in timestamps]

# Plot the graph with sequences on the x-axis and UTC times on the y-axis
plt.figure(figsize=(10, 8))
plt.plot(sequences, utc_times, 'b.-')  # Plot with blue dots and lines
plt.xlabel('Sequence')  # Label for the x-axis
plt.ylabel('Time (UTC)')  # Label for the y-axis
plt.title('Timestamps vs Sequence')  # Title of the plot
plt.grid(True)  # Add a grid for better readability
plt.xticks(sequences)  # Show sequence numbers on the x-axis
plt.gcf().autofmt_xdate()  # Format dates on the y-axis nicely
plt.show()

This generates this graph:

",matplotlib
increase the decimal places of the data labels in a shap waterfall plot,"Some of the data labels of the SHAP waterfall plot that I get are equal to zero. However, they are not and the plot itself rounds them. Therefore, I need to increase the decimal places of the labels.
The following is the part of the code for this purpose:
plt.figure()
fig, ax = plt.subplots()
fig.tight_layout(pad=7.0)
shap.plots.waterfall(val[26], show=False)
for text in ax.texts:
    text.set_color('k')
    value = float(text.get_text())
    text.set_text(f""{value:.{4}f}"")
fig, ax = plt.gcf(), plt.gca()
ax.grid(axis='y', color='grey', linestyle=':', linewidth=1, alpha=0.5)
for tick in ax.get_xticklabels():
    tick.set_fontname('Times New Roman')
    tick.set_fontsize(14)
    tick.set_color('k')
for tick in ax.get_yticklabels():
    tick.set_fontname('Times New Roman')
    tick.set_fontsize(14)
    tick.set_color('k')
ax.spines[""top""].set_visible(True)
ax.spines[""top""].set_color('k')
ax.spines[""left""].set_visible(True)
ax.spines[""left""].set_color('k')
ax.spines[""right""].set_visible(True)
ax.spines[""right""].set_color('k')
plt.show()

So far, I have tried the code below but it returns a value error. This is because despite being numbers, the labels are strings, and it does not affect them.
for text in ax.texts:
    value = float(text.get_text())
    text.set_text(f""{value:.{4}f}"")

I have also tried increasing the decimal places for the whole Python environment but it did not help due to the same reason as above. I used the method below:
from decimal import getcontext
getcontext().prec = 4

The plot that I get is as follows:

","That was subtle!
I checked your code on a dataset and I got this error:
ValueError: could not convert string to float: '−0.21'

It is a small detail but if you look closely you may notice that the minus sign almost touches 0 so I wondered if it was really a minus sign. If you take the unicode of the first character with ord() you get 8722, whereas it should be 45 for a normal minus sign.
So the solution is to check the strings if they have this code and if so replace it by the real minus sign to get the string of a negative number which can be converted by float():
for text in ax.texts:
    text.set_color('k')
    ctext = text.get_text()
    if ord(ctext[0]) == 8722:
       ctext = ""-"" + ctext[1:]
    value = float(ctext)
    text.set_text(f""{value:.{4}f}"")

",matplotlib
numerically obtaining response of a damped driven oscillator,"I am trying to plot the response of a periodically-driven damped oscillator whose dynamics is governed by,
x''+ 2Gx' + f0^2 x = F cos(ft)
where the constants denote the following.
G: Damping coefficient
f0: Natural frequency
f: Driving frequently
F: Strength of the drive
To do so, I solved the above differential equation for x(t). Next, I extracted the steady-state part from x(t), took its Fourier transform, and plotted its magnitude to visualize the response of the oscillator.
Here is the code that attempts to achieve it.
import numpy as np
import matplotlib.pyplot as plt
from scipy.fft import fft, fftfreq

G=1.0
f0=2
f1=5
F=1

N=500000
T=50
dt=T/N

t=np.linspace(0,T,N)
u=np.zeros(N,dtype=float)  # Position
v=np.zeros(N,dtype=float)  # Velocity

u[0]=0
v[0]=0.5

for i in range(N-1):
    u[i+1] = u[i] + v[i]*dt
    v[i+1] = v[i] - 2*G*v[i]*dt - (f0*f0)*u[i]*dt + F*np.cos(f1*t[i])*dt 

slice_index=int(20/dt)
U=u[slice_index:]

X_f = fft(U)
frequencies = fftfreq(len(U), dt)
psd = np.abs(X_f)
positive_freqs = frequencies[frequencies > 0]  

plt.plot(positive_freqs, psd[frequencies > 0], label=""Simulated PSD"")
plt.plot(frequencies, psd)


Since the oscillator is forced and reaches a steady state, I expect the response to peak around the driving frequency. However, the above code gives a peak located nowhere near f. What am I doing wrong?
","Your frequencies f0 and f1 are applied in the finite-difference model in rad/s. This may or may not have been your intention.
However, your frequencies from the FFT are in cycles/s.
Since you are using the symbol f, rather than omega, I would guess that you want them in cycles/s. In your finite-difference model then you would have to use 2.PI.f in both locations where you put an f before. Specifically in the line
v[i+1] = v[i] - 2*G*v[i]*dt - (2 * np.pi * f0 ) ** 2 * u[i]*dt + F*np.cos( 2 * np.pi * f1*t[i] ) * dt

Then you get peak energy at a frequency of 5 Hz. (Trim the x-axis scale.)
You are very heavily damped, BTW. Also, you aren't strictly plotting PSD.

import numpy as np
import matplotlib.pyplot as plt
from scipy.fft import fft, fftfreq

G=1.0
f0=2
f1=5
F=1

N=500000
T=50
dt=T/N

t=np.linspace(0,T,N)
u=np.zeros(N,dtype=float)  # Position
v=np.zeros(N,dtype=float)  # Velocity

u[0]=0
v[0]=0.5

for i in range(N-1):
    u[i+1] = u[i] + v[i]*dt
    v[i+1] = v[i] - 2*G*v[i]*dt - (2 * np.pi * f0 ) ** 2 * u[i]*dt + F*np.cos(2 * np.pi * f1*t[i] ) * dt

slice_index=int(20/dt)
U=u[slice_index:]

X_f = fft(U)
frequencies = fftfreq(len(U), dt)
psd = np.abs(X_f)
positive_freqs = frequencies[frequencies > 0]  

plt.plot(positive_freqs, psd[frequencies > 0], label=""Simulated PSD"")
plt.xlim(0,10)
plt.show()

",matplotlib
how to superimpose the rotated subplot on the another sublot,"I would like to release such configuration of plots as it presented on the picture:

The next parameters have to be variable:

Position of subplot origin point
Rotation angle of subplot
Size of subplot

I tried to find solution using mpl_toolkits.axes_grid1.inset_locator.inset_axes (see below), but I can't change values of parameters 1,2 from the list above. Please, help me to find solution.
import matplotlib.pyplot as plt
import numpy as np
from mpl_toolkits.axes_grid1.inset_locator import inset_axes

x = np.linspace(0,100,100)
y = np.linspace(0,100,100)

fig, ax = plt.subplots(1, 1, figsize=[8, 8])
axins = inset_axes(ax, width='40%', height='40%', loc='center')
axins.set_xlim(0,80)
axins.set_ylim(0,80)
axins.plot(x,y)

plt.show()


","Based on using a floating axis (as in this answer) and the example here, I have the following solution:
import numpy as np

import matplotlib.pyplot as plt
from matplotlib.transforms import Affine2D
import mpl_toolkits.axisartist.floating_axes as floating_axes
from mpl_toolkits.axisartist.grid_finder import MaxNLocator

# create a figure and original axis
fig, ax_orig = plt.subplots(figsize=(7, 7))

# data for plotting in subplot
subplot_xdata = np.linspace(0, 80, 100)
subplot_ydata = np.linspace(0, 80, 100)

# extents of the subplot (based on data)
plot_extents = (
    subplot_xdata[0],
    subplot_xdata[-1],
    subplot_ydata[0],
    subplot_ydata[-1],
)

# create the floating subplot
rotation = 145  # rotation of subplot (degrees)
transform = Affine2D().rotate_deg(rotation)  # transform with rotation applied

# set the subplot grid to allow ticks at multiples of 5 or 10
grid_locator = MaxNLocator(steps=[5, 10])

helper = floating_axes.GridHelperCurveLinear(
    transform,
    plot_extents,
    grid_locator1=grid_locator,
    grid_locator2=grid_locator,
)
ax = floating_axes.FloatingSubplot(fig, 111, grid_helper=helper)

# position and scale the subplot (play about with these)
width = 0.35  # width relative to original axis
height = 0.35  # height relative to original axis
xloc = 0.4  # x-location (in axis coordinates between 0-1) of bottom left corner of (unrotated) subplot
yloc = 0.5  # y-location of bottom left corner of (unrotated) subplot
ax.set_position((xloc, yloc, width, height))

# get auxilary axis to for actually plotting the subplot data
aux_ax = ax.get_aux_axes(transform)
aux_ax.plot(subplot_xdata, subplot_ydata)

# add subplot to the figure
fig.add_subplot(ax)

# plot something on the original axes
ax_orig.plot(np.linspace(0, 2, 100), np.linspace(0, 2, 100))


Note that this solution requires you to specify the x-y extent of your subplot.
",matplotlib
how to align two plots in matplotlib,"I have the following matplotlib code
import matplotlib.pyplot as plt
import numpy as np

# Time of day values
time_of_day = [
    ""00:00"", ""01:00"", ""02:00"", ""03:00"", ""04:00"", ""05:00"", ""06:00"", ""07:00"", ""08:00"",
    ""09:00"", ""10:00"", ""11:00"", ""12:00"", ""13:00"", ""14:00"", ""15:00"", ""16:00"", ""17:00"",
    ""18:00"", ""19:00"", ""20:00"", ""21:00"", ""22:00"", ""23:00""
]

# Electricity load values in W
electricity_load = np.array([
    5460, 177, 163, 745, 770, 1049, 1090, 868, 277, 3117, 3416, 1383, 1551, 4324, 969, 431, 703, 1201, 898, 4969,7839, 259, 410, 617
])

# Convert electricity load values to kW
electricity_load_kW = electricity_load / 1000

# Price values in Cent/kWh
price_values = [
    27.1,  26.5,  26.0, 25.1, 26.6, 27.5,  34.4, 51.3, 45.3,  44.3, 44.3,  41.3,  38.1,  35.5,
    33.9,  37, 41.4,  48.6,  53.4,  48.6,  43.4, 38.7, 37.8,  27.4,
]

# Create subplots within the same figure
fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 10), sharex=True)

# Plotting the bar diagram
ax1.bar(time_of_day, electricity_load_kW[:len(time_of_day)], color='#FFD700', edgecolor='black')

# Labeling the first plot
ax1.set_ylabel('Electricity Load (kW)', fontsize=18)
ax1.grid(axis='y', linestyle='--', alpha=0.7)

# Plotting the step line plot
ax2.step(time_of_day, price_values, where='post', color='limegreen', linewidth=6)

# Labeling the second plot
ax2.set_xlabel('Time of Day', fontsize=18)
ax2.set_ylabel('Price (Cent/kWh)', fontsize=18)
ax2.grid(axis='y', linestyle='--', alpha=0.7)

# Adjust x-axis limits to eliminate empty space
ax1.set_xlim(time_of_day[0], time_of_day[-1])

# Increase thickness and rotation of x-tick labels
for ax in [ax1, ax2]:
    ax.tick_params(axis='x', which='both', labelsize=14, width=2)

# Set x-tick labels with rotation and horizontal alignment
plt.sca(ax1)
plt.xticks(rotation=45, ha='right')

plt.sca(ax2)
plt.xticks(rotation=45, ha='right')

# Tighten the layout
plt.tight_layout()

# Save the figure and plot it
plt.savefig('C:/Users/wi9632/Desktop/temp_combined.png', dpi=100)
plt.show()

So I have a bar diagram on the upper part and on the lower part there is a step line plot. What I want is that the horizontal lines of the bar diagram and the step line plot are at the same x-position such that they are aligned. This means that the values for the x-label ticks (00:00 to 01:00, 01:00 to 02:00 etc.) should be at the same position which is currently not the case.
","I suggest you to center the bar plot on edge:
# Plotting the bar diagram
ax1.bar(time_of_day, electricity_load_kW[:len(time_of_day)], color='#FFD700', edgecolor='black', align='edge', width=1)

This gives the following plot (I hope this is what you were searching for):

",matplotlib
how to unjoin of different plots when plotting multiple scatterline plots on one figure,"I am plotting multiple line and scatter graphs on a single figure and axis. My code sets one variable called total_steel_area and then goes through a set of values of another variable called phi_x__h. It then calculates x and y values from these variables and places them in a list. It then plots the values. The code then moves to the next value of total_steel_area and repeats. The output graph is shown below.
The diagonal lines connect the last value of one set of x,y values to the first value of the next set. My question is how can I remove this connecting line?
My code is below:
phi_N_bh = []
phi_M_bh2 = []
fig, ax = plt.subplots(dpi=100, figsize=(8,4))

for total_steel_area in np.arange(0.01,0.06,0.01):

    for phi_x__h in np.arange(0.1,2,0.1):
        phi_N__bh, phi_M__bh2 = calculate_phi_N__bh_and_phi_M__bh2(phi_x__h, lamb, alpha_cc, eta, f_ck, E_s, varepsilon_cu2, phi_d__h, phi_d2__h, f_yk, total_steel_area/2, total_steel_area/2)
        phi_N_bh.append(phi_N__bh)
        phi_M_bh2.append(phi_M__bh2)

    ax.plot(phi_M_bh2, phi_N_bh, c='b')
    ax.scatter(phi_M_bh2, phi_N_bh, c='b', s=10)
        
ax.set_title('Column Design Chart for Rectangular Column with Symmetrical Compression and Tension Reinforcement')   
ax.set_xlabel('M/bh²')
ax.set_ylabel('N/bh')
ax.text(1-0.1, 1-0.1, f'f_ck = {f_ck}, f_yk = {f_yk}', horizontalalignment='center',
    verticalalignment='center', transform=ax.transAxes)
ax.text(1-0.1, 1-0.2, f'd/h = {phi_d__h}, d2/h = {phi_d2__h}', horizontalalignment='center',
    verticalalignment='center', transform=ax.transAxes) 
ax.set_ylim(0)
ax.set_xlim(0)
ax.spines['top'].set_visible(False)
ax.spines['right'].set_visible(False)
ax.legend()


","You should initialize/reset your lists at each step of the outer loop:
for total_steel_area in np.arange(0.01,0.06,0.01):
    phi_N_bh = []
    phi_M_bh2 = []

    for phi_x__h in np.arange(0.1,2,0.1):
        phi_N__bh, phi_M__bh2 = calculate_phi_N__bh_and_phi_M__bh2(phi_x__h, lamb, alpha_cc, eta, f_ck, E_s, varepsilon_cu2, phi_d__h, phi_d2__h, f_yk, total_steel_area/2, total_steel_area/2)
        phi_N_bh.append(phi_N__bh)
        phi_M_bh2.append(phi_M__bh2)

    ax.plot(phi_M_bh2, phi_N_bh, c='b')
    ax.scatter(phi_M_bh2, phi_N_bh, c='b', s=10)     

There might be a vectorial way to compute your values, but this is difficult to guess without a reproducible example of the functions.
Output:

",matplotlib
subplot fourpack under another subplot the size of the fourpack,"I want to make a matplotlib figure that has two components:

A 2x2 ""four pack"" of subplots in the lower half of the figure

A subplot above the four pack that is the size of the four pack.


I have seen this answer where subplots can have different dimensions. How can that approach be tweaked when there are multiple columns and rows, however? Or is a completely different approach warranted?
My final figure should be arranged something like this:

If there is a straightforward way to have a left-right oritentation, shown below, that would be helpful, too.

","You can use plt.subplot_mosaic or GridSpec. Someone else can write an answer using GridSpec, but here is how you'd do it using subplot_mosaic.
For the large plot on top and 4 below it:
import matplotlib.pyplot as plt

fig, axs_dict = plt.subplot_mosaic(""AA;AA;BC;DE"")

If you want to put the large plot on the left and the four smaller on the right then just using this string: ""AABC;AADE"".
",matplotlib
matplotlib image as labels on pie chart,"I don't find the way to move the labels' text to images. How to use PIL or imageio to retrieve flag images from internet and use them instead of the strings.
Here is the MWE:
import matplotlib.pyplot as plt

nationalities = {
    'Fr': 5,
    'De': 10,
    'It': 3,
    'Se': 5,
    'Pl':2,
    'Cz': 3,
}

flags = {
    'Fr':'https://www.touteleurope.eu/wp-content/uploads/2022/08/drapeau-France.png',
    'De':'https://www.touteleurope.eu/wp-content/uploads/2022/07/drapeau-allemagne.png',
    'It':'https://www.touteleurope.eu/wp-content/uploads/2022/08/drapeau-Italie.png',
    'Se':'https://www.touteleurope.eu/wp-content/uploads/2022/07/1280px-Flag_of_Sweden.svg.png',
    'Pl':'https://www.touteleurope.eu/wp-content/uploads/2022/08/drapeau-Pologne.png',
    'Cz':'https://www.touteleurope.eu/wp-content/uploads/2022/08/drapeau-Republique-tcheque.png',
}

fig, ax = plt.subplots(figsize=(18,6))
plt.xticks(fontsize=20)
plt.yticks([], fontsize=20)
Countries = []
Composers = []
Flags = []
for Cnty, Cps in nationalities.items():
    Countries.append(Cnty)
    Composers.append(Cps)
    Flags.append( flags[Cnty] ) 
    nb_of_composers = 100*max(Composers)/sum(Composers)
print (str(nb_of_composers)[0:2])
#plt.pie( Composers, labels=Flags, radius=1, textprops={'size': 20}, wedgeprops={'linewidth': 1.0, 'edgecolor': 'white'}, )
plt.pie( Composers, labels=Countries, radius=1, textprops={'size': 20}, wedgeprops={'linewidth': 1.0, 'edgecolor': 'white'}, )
plt.text(s=str(nb_of_composers)[0:2]+'%', x=-0.7, y=0.3, fontsize=30, color='white')
plt.tight_layout()
plt.show()

Thanks a lot for your help !
","You can download the images with request+PIL, then add the images with imshow:
from PIL import Image
import requests

# download the images
headers ={
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.114 Safari/537.36'
}

Flags = {k: Image.open(requests.get(url, headers=headers, stream=True).raw)
         for k, url in flags.items()}

# plot the original graph
fig, ax = plt.subplots(figsize=(9,3))
plt.xticks(fontsize=20)
plt.yticks([], fontsize=20)

Countries, Composers = zip(*nationalities.items())
nb_of_composers = max(Composers)/sum(Composers)

patches, texts = plt.pie(Composers, labels=Countries, radius=1, textprops={'size': 20},
                         wedgeprops={'linewidth': 1.0, 'edgecolor': 'white'}, )
plt.text(s=f'{nb_of_composers:.0%}', x=-0.7, y=0.3, fontsize=30, color='white')

# replace the string labels with images
for t in texts:
    x, y = t.get_position()
    ax.imshow(Flags[t._text], extent=[x-0.15, x+0.15, y-0.1, y+0.1], zorder=1)
    t.set_visible(False)

plt.tight_layout()

Output:

",matplotlib
use matplotlib_inline and torchd2l show error notimplementederror implement enable_gui in a subclass,"I learning the ""DIVE INTO DEEP LEARNING"" pytorch version, in https://d2l.ai/chapter_preliminaries/calculus.html, I used 'jupyter notebook' command, and ran the pytorch code in the jupyter ,everything run ok, I rewrote the pytorch code into 1.py, so I can debug the pytorch code convenient, here is my 1.py code

#%matplotlib inline
import os
import numpy as np
from matplotlib_inline import backend_inline
from d2l import torch as d2l


def f(x):
    return 3 * x ** 2 - 4 * x
    
    
def numerical_lim(f, x, h):
    return (f(x + h) - f(x)) / h

h = 0.1
for i in range(5):
    print(f'h={h:.5f}, numerical limit={numerical_lim(f, 1, h):.5f}')
    h *= 0.1

def use_svg_display():  #@save
    """"""使用svg格式在Jupyter中显示绘图""""""
    backend_inline.set_matplotlib_formats('svg')
    
def set_figsize(figsize=(3.5, 2.5)):  #@save
    """"""设置matplotlib的图表大小""""""
    use_svg_display()
    d2l.plt.rcParams['figure.figsize'] = figsize

#@save
def set_axes(axes, xlabel, ylabel, xlim, ylim, xscale, yscale, legend):
    """"""设置matplotlib的轴""""""
    axes.set_xlabel(xlabel)
    axes.set_ylabel(ylabel)
    axes.set_xscale(xscale)
    axes.set_yscale(yscale)
    axes.set_xlim(xlim)
    axes.set_ylim(ylim)
    if legend:
        axes.legend(legend)
    axes.grid()


#@save
def plot(X, Y=None, xlabel=None, ylabel=None, legend=None, xlim=None,
         ylim=None, xscale='linear', yscale='linear',
         fmts=('-', 'm--', 'g-.', 'r:'), figsize=(3.5, 2.5), axes=None):
    """"""绘制数据点""""""
    if legend is None:
        legend = []

    set_figsize(figsize)
    
        
    axes = axes if axes else d2l.plt.gca()

    # 如果X有一个轴，输出True
    def has_one_axis(X):
        return (hasattr(X, ""ndim"") and X.ndim == 1 or isinstance(X, list)
                and not hasattr(X[0], ""__len__""))

    if has_one_axis(X):
        X = [X]
    if Y is None:
        X, Y = [[]] * len(X), X
    elif has_one_axis(Y):
        Y = [Y]
    if len(X) != len(Y):
        X = X * len(Y)
    axes.cla()
    for x, y, fmt in zip(X, Y, fmts):
        if len(x):
            axes.plot(x, y, fmt)
        else:
            axes.plot(y, fmt)
    set_axes(axes, xlabel, ylabel, xlim, ylim, xscale, yscale, legend)


x = np.arange(0, 3, 0.1)
plot(x, [f(x), 2 * x - 3], 'x', 'f(x)', legend=['f(x)', 'Tangent line (x=1)'])



I run the command
""conda activate d2l""
then run
""python 1.py""
it shows error:

⇒  conda activate d2l
lee@Princekin-MacbookPro:~/Desktop/Artificial_Intelligence/DEEP_LEARNING/DIVE_INTO_DEEP_LEARNING_PyTorch/task/2.4|main⚡
⇒  python 1.py
h=0.10000, numerical limit=2.30000
h=0.01000, numerical limit=2.03000
h=0.00100, numerical limit=2.00300
h=0.00010, numerical limit=2.00030
h=0.00001, numerical limit=2.00003
Traceback (most recent call last):
  File ""/Users/lee/Desktop/Artificial_Intelligence/DEEP_LEARNING/DIVE_INTO_DEEP_LEARNING_PyTorch/task/2.4/1.py"", line 79, in <module>
    plot(x, [f(x), 2 * x - 3], 'x', 'f(x)', legend=['f(x)', 'Tangent line (x=1)'])
  File ""/Users/lee/Desktop/Artificial_Intelligence/DEEP_LEARNING/DIVE_INTO_DEEP_LEARNING_PyTorch/task/2.4/1.py"", line 54, in plot
    axes = axes if axes else d2l.plt.gca()
  File ""/Users/lee/miniconda3/envs/d2l/lib/python3.9/site-packages/matplotlib/pyplot.py"", line 2309, in gca
    return gcf().gca()
  File ""/Users/lee/miniconda3/envs/d2l/lib/python3.9/site-packages/matplotlib/pyplot.py"", line 906, in gcf
    return figure()
  File ""/Users/lee/miniconda3/envs/d2l/lib/python3.9/site-packages/matplotlib/_api/deprecation.py"", line 454, in wrapper
    return func(*args, **kwargs)
  File ""/Users/lee/miniconda3/envs/d2l/lib/python3.9/site-packages/matplotlib/pyplot.py"", line 840, in figure
    manager = new_figure_manager(
  File ""/Users/lee/miniconda3/envs/d2l/lib/python3.9/site-packages/matplotlib/pyplot.py"", line 383, in new_figure_manager
    _warn_if_gui_out_of_main_thread()
  File ""/Users/lee/miniconda3/envs/d2l/lib/python3.9/site-packages/matplotlib/pyplot.py"", line 361, in _warn_if_gui_out_of_main_thread
    if _get_required_interactive_framework(_get_backend_mod()):
  File ""/Users/lee/miniconda3/envs/d2l/lib/python3.9/site-packages/matplotlib/pyplot.py"", line 208, in _get_backend_mod
    switch_backend(rcParams._get(""backend""))
  File ""/Users/lee/miniconda3/envs/d2l/lib/python3.9/site-packages/matplotlib/pyplot.py"", line 356, in switch_backend
    install_repl_displayhook()
  File ""/Users/lee/miniconda3/envs/d2l/lib/python3.9/site-packages/matplotlib/pyplot.py"", line 157, in install_repl_displayhook
    ip.enable_gui(ipython_gui_name)
  File ""/Users/lee/miniconda3/envs/d2l/lib/python3.9/site-packages/IPython/core/interactiveshell.py"", line 3607, in enable_gui
    raise NotImplementedError('Implement enable_gui in a subclass')
NotImplementedError: Implement enable_gui in a subclass
lee@Princekin-MacbookPro:~/Desktop/Artificial_Intelligence/DEEP_LEARNING/DIVE_INTO_DEEP_LEARNING_PyTorch/task/2.4|main⚡
⇒


I have no idea how to solve this problem, if you have any ideas, it would be greatly appreciated,thanks
","after times trying, I found the solution,

change the 1.py


#%matplotlib inline
import os
import numpy as np
from matplotlib_inline import backend_inline
from d2l import torch as d2l
import matplotlib.pyplot as plt


def f(x):
    return 3 * x ** 2 - 4 * x
    
    
def numerical_lim(f, x, h):
    return (f(x + h) - f(x)) / h

h = 0.1
for i in range(5):
    print(f'h={h:.5f}, numerical limit={numerical_lim(f, 1, h):.5f}')
    h *= 0.1

def use_svg_display():  #@save
    """"""使用svg格式在Jupyter中显示绘图""""""
    backend_inline.set_matplotlib_formats('svg')
    
def set_figsize(figsize=(3.5, 2.5)):  #@save
    """"""设置matplotlib的图表大小""""""
    use_svg_display()
    d2l.plt.rcParams['figure.figsize'] = figsize

#@save
def set_axes(axes, xlabel, ylabel, xlim, ylim, xscale, yscale, legend):
    """"""设置matplotlib的轴""""""
    axes.set_xlabel(xlabel)
    axes.set_ylabel(ylabel)
    axes.set_xscale(xscale)
    axes.set_yscale(yscale)
    axes.set_xlim(xlim)
    axes.set_ylim(ylim)
    if legend:
        axes.legend(legend)
    axes.grid()


#@save
def plot(X, Y=None, xlabel=None, ylabel=None, legend=None, xlim=None,
         ylim=None, xscale='linear', yscale='linear',
         fmts=('-', 'm--', 'g-.', 'r:'), figsize=(3.5, 2.5), axes=None):
    """"""绘制数据点""""""
    if legend is None:
        legend = []

    set_figsize(figsize)
    
        
    axes = axes if axes else d2l.plt.gca()

    # 如果X有一个轴，输出True
    def has_one_axis(X):
        return (hasattr(X, ""ndim"") and X.ndim == 1 or isinstance(X, list)
                and not hasattr(X[0], ""__len__""))

    if has_one_axis(X):
        X = [X]
    if Y is None:
        X, Y = [[]] * len(X), X
    elif has_one_axis(Y):
        Y = [Y]
    if len(X) != len(Y):
        X = X * len(Y)
    axes.cla()
    for x, y, fmt in zip(X, Y, fmts):
        if len(x):
            axes.plot(x, y, fmt)
        else:
            axes.plot(y, fmt)
    set_axes(axes, xlabel, ylabel, xlim, ylim, xscale, yscale, legend)


x = np.arange(0, 3, 0.1)
plot(x, [f(x), 2 * x - 3], 'x', 'f(x)', legend=['f(x)', 'Tangent line (x=1)'])


plt.show()




run the command ""IPython 1.py"";

then it worked successfully, and showed the figure, hope it can help someone!
",matplotlib
reset color cycle in matplotlib,"Say I have data about 3 trading strategies, each with and without transaction costs.  I want to plot, on the same axes, the time series of each of the 6 variants (3 strategies * 2 trading costs).  I would like the ""with transaction cost"" lines to be plotted with alpha=1 and linewidth=1 while I want the ""no transaction costs"" to be plotted with alpha=0.25 and linewidth=5.  But I would like the color to be the same for both versions of each strategy. 
I would like something along the lines of:
fig, ax = plt.subplots(1, 1, figsize=(10, 10))

for c in with_transaction_frame.columns:
    ax.plot(with_transaction_frame[c], label=c, alpha=1, linewidth=1)

****SOME MAGIC GOES HERE TO RESET THE COLOR CYCLE

for c in no_transaction_frame.columns:
    ax.plot(no_transaction_frame[c], label=c, alpha=0.25, linewidth=5)

ax.legend()

What is the appropriate code to put on the indicated line to reset the color cycle so it is ""back to the start"" when the second loop is invoked?
","In Matplotlib <1.5.0, you can reset the colorcycle to the original with Axes.set_color_cycle. Looking at the code for this, there is a function to do the actual work:
def set_color_cycle(self, clist=None):
    if clist is None:
        clist = rcParams['axes.color_cycle']
    self.color_cycle = itertools.cycle(clist)

And a method on the Axes which uses it:
def set_color_cycle(self, clist):
    """"""
    Set the color cycle for any future plot commands on this Axes.

    *clist* is a list of mpl color specifiers.
    """"""
    self._get_lines.set_color_cycle(clist)
    self._get_patches_for_fill.set_color_cycle(clist)

This basically means you can call the set_color_cycle with None as the only argument, and it will be replaced with the default cycle found in rcParams['axes.color_cycle'].
I tried this with the following code and got the expected result:
import matplotlib.pyplot as plt
import numpy as np

for i in range(3):
    plt.plot(np.arange(10) + i)

# for Matplotlib version < 1.5
plt.gca().set_color_cycle(None)
# for Matplotlib version >= 1.5
plt.gca().set_prop_cycle(None)

for i in range(3):
    plt.plot(np.arange(10, 1, -1) + i)

plt.show()


",matplotlib
how to overlay grayscale mask on top of rgb image using numpy and matplotlib  opencv or scikit image in case not possible,"I have 2 images from Carvana Image Dataset where image is jpg and mask is gif. I have converted the mask as grayscale as 0 or 1 and now want to overlay it over the image to see these 3 original, mask, superimposed side by side using matplotlib. What is the right way to do this?
from PIL import Image

def get_pair(image_path, mask_path):
    image = np.array(Image.open(image_path).convert('RGB'))
    mask = np.array(Image.open(mask_path).convert('L'), dtype = np.float32) # Mask should be Grayscale so each value is either 0 or 255
    mask[mask == 255.0] = 1.0 # whereever there is 255, convert it to 1: (1 == 255 == White)
    return image, mask 

One way could be:

image, mask = data[0]
image = image / 255
mask = np.stack((mask,)*3, axis=-1)

blended = image * mask
plt.imshow(blended)

but it shows only the car and everything else as black
Below are the 2 images


and I want to plot these 3 as:

","There might be a misconception in what you expect.

... but it shows only the car and everything else as black

This is how a binary mask usually operates.
The following selfcontained code (with the images from above saved accordingly) might explain what happens. Note the comments near blended1 = ...

from PIL import Image
import numpy as np
from matplotlib import pyplot as plt

def get_pair(image_path, mask_path):
    image = np.array(Image.open(image_path).convert('RGB'))
    mask = np.array(Image.open(mask_path).convert('L'), dtype = np.float32) # Mask should be Grayscale so each value is either 0 or 255
    mask[mask == 255.0] = 1.0 # whereever there is 255, convert it to 1: (1 == 255 == White)
    return image, mask 

img, mask = get_pair(""img.jpg"", ""mask.gif"")
print(f""{img.shape=}"")  #      -> img.shape=(1280, 1918, 3)
print(f""{mask.shape=}"")  #     -> img.shape=(1280, 1918)

mask2 = np.stack((mask,)*3, axis=-1)
print(f""{mask2.shape=}"")  # -> img.shape=(1280, 1918, 3)

# rescale image
img = img /255

# set every pixel to (0, 0, 0) (black) where mask is 0 and
# keep every pixel unchanged where mask is 1
# this is how a mask is usually applied
blended1 = img*mask2

# set every pixel to (1, 1, 1) (white) where mask is 1 and
# keep every pixel unchanged where mask is 0


blended2 = np.clip(img+mask2, 0, 1)

fig, axx = plt.subplots(1, 4, figsize=(8, 18))

for ax, arr, title in zip(axx,
                        [img, mask, blended1, blended2],
                        [""original"", ""mask"", ""blended1"", ""blended2""]):
    ax.imshow(arr)
    ax.axis(""off"")
    ax.set_title(title)

plt.show()

The resulting image:

",matplotlib
matplotlib bar graph incoherent behavior when using bottom and height parameters,"I am trying to plot a bar chart where for each day, used as X axis, we see the activity between different periods of time as a bar going from the time of the start of the activity to the time of the end of the activity. So Y axis goes from 0 to 24, and for example I could have a bar from 1AM to 2AM and a second bar from 3PM and 5PM.
I have used matplotlib bar graph and used the bottom and height parameters to make this work, and to a certain extent, that does work. When I have very little data, everything is displayed correctly, but when I have dozens of activities, somehow the data turns wrong.
The code is the following (taken from Power BI and I've never used Pandas, so I just converted straight to Python array)
import os, uuid, matplotlib
matplotlib.use('Agg')
import matplotlib.pyplot
import pandas
import datetime
import matplotlib.ticker
import matplotlib.dates
import numpy

dataset = pandas.read_csv('input_df_3a3333a0-fd5d-4630-8707-5fc23cb0b326.csv')

matplotlib.pyplot.figure(figsize=(5.55555555555556,4.16666666666667), dpi=72)

test_date_time = dataset.to_numpy().tolist()
test_date_time = list(filter(lambda x: type(x[0]) is not float, test_date_time))
test_date_time = [(datetime.datetime.fromisoformat(x[0]), datetime.datetime.fromisoformat(x[1])) for x in test_date_time]
test_date_time = sorted(test_date_time, key=lambda x:x[0])

values = {""days"": [], ""bottom"": [], ""height"": []}


# ### KIND OF WORKING
for (test_start, test_end) in test_date_time:
    values[""days""].append(test_start.date())
    values[""bottom""].append(test_start.time().hour + (test_start.time().minute / 60) + (test_start.time().second / 3600))
    values[""height""].append((test_end - test_start).total_seconds() / 3600)
matplotlib.pyplot.bar(x=values[""days""], height=values[""height""], bottom=values[""bottom""])


matplotlib.pyplot.show()


I have checked the values dictionnary and that looks pretty good to me. But when I plot it, I often get values far exceeding 24, like this for example :

Problem is, if I check through the debugguer, the last height value for April 4th is 1.44 hours and the bottom value is 19.01 hours, so I should have a bar going from 19.01 to 20.45 and that's it, which is not at all what I get.
I have looked at Timeline bar graph using python and matplotlib but I'm just curious at why is this happening ? Example data can be found here https://filebin.net/rivcvi6d9v92sywk
","The reason is that you have an outlier on index 513 (lasts from 04/04 to 04/06):
2024-04-04T15:02:44.0000000,2024-04-06T14:57:45.0000000

",matplotlib
matplotlib import error quotimporterror dll load failed while importing _path the specific module could not be foundquot,"I installed Python3.10 (64x)(download installer from the official website) on the clear Windows 10 (x64).
Using pip install <packetname> I installed packages: numpy, scipy and matplotlib.
enter image description here
Numpy and scipy work properly, but matplotlib import falls with ImportError: DLL load failed while importing _path: The specific module could not be found
enter image description here
It falls in the transforms.py on the command:
from matplotlib._path import ( affine_transform, count_bboxes_overlapping_bbox, update_path_extents)
I've tried reinstall matplotlib and install older versions - no result. Reviewing the forums with similar questions also wasn't successful.
Please, help me to get by this issue.
","The issue was solved by installing pandas packet
",matplotlib
matplotlib will not import even though it is installed using visual studio code,"Here is a picture of my terminal.
I cannot figure out what is going on, I've spent hours trying to make this import work. Please help
","Aside from the obvious of uninstalling and reinstalling matplotlib, I would recommend clicking the bottom left corner of VSCode and make sure it is pointed at your virtual environment. Even though in your terminal you are in a venv, it looks as if VSCode may not be.
",matplotlib
plotting dates on the xaxis,"I am trying to plot information against dates. I have a list of dates in the format ""01/02/1991"".
I converted them by doing the following:
x = parser.parse(date).strftime('%Y%m%d'))

which gives 19910102
Then I tried to use num2date
import matplotlib.dates as dates
new_x = dates.num2date(x)

Plotting:
plt.plot_date(new_x, other_data, fmt=""bo"", tz=None, xdate=True)

But I get an error. It says ""ValueError: year is out of range"". Any solutions?
","As @KyssTao has been saying, help(dates.num2date) says that the x has to be a float giving the number of days since 0001-01-01 plus one. Hence, 19910102 is not 2/Jan/1991, because if you counted 19910101 days from 0001-01-01 you'd get something in the year 54513 or similar (divide by 365.25, number of days in a year).
Use datestr2num instead (see help(dates.datestr2num)):
new_x = dates.datestr2num(date) # where date is '01/02/1991'

",matplotlib
i want matplotlib notebook not matplotllib widget or matplotlib ipympl  javascript error ipython is not defined,"I start with the usual imports and go to start an interactive plotting session:
# for viewing figures interactively
%matplotlib notebook
 
# To start plotting in matplotlib
import IPython
import matplotlib.pyplot as plt
import seaborn as sns
sns.set_style(""whitegrid"")

# Other data libraries
import numpy as np
import pandas as pd

# 3 dimensional plotting
from mpl_toolkits import mplot3d

# enable three-dimensional axes
fig = plt.figure()
ax = plt.axes(projection='3d')

this results in the error:

Javascript Error: IPython is not defined

Then I read here to test some other things. I install IPython Widgets:
pip install ipywidgets 
jupyter nbextension enable --py widgetsnbextension

Now putting %matplotlib widgets instead of %matplotlib notebook at the beginning of the codes displays the interactive figure.
In another approach, I can go and install ipympl and enable that extension:
pip install ipympl
jupyter nbextension enable --py --sys-prefix ipympl

After that useing %matplotlib ipympl instead of %matplotlib notebook at the first line shows the figure interactively.
But, the error for notebook magic command still persists. The thing is, this gets around the error and makes the code work but it does not solve the problem. I still have no clue why JavaScript cannot recognize IPyhton.
Any ideas for really solving the issue?
","So, the answer is even though matplotlib.org did not officially abandon %matplotlib notebook, they recommend to use %matplotlib ipympl so it is clear that ipympl needs to be installed. ipympl respects %matplotlib widget for better backend compatibility and performance.
jupyter notebook version: 7.2.2
matplotlib version: 3.10.0
seabor version: 0.13.2
",matplotlib
how to convert float notation to power of 10 scientific notation in python,"In Python, I have values given by 0.000001,0.00001,0.0001,....,1.0 stored in my_Array (np.array). 
For each of these values I need to label a curve in a plot and store it in the legend as val = 10e-6 instead of val = 0.000001.
The second version is automatically stored if I use (for the i'th value):
matplolib.pyplot.plot(...., label = 'val = ' + str(my_Array[i]))

Is there a function converting the float notation to the scientific power of 10 notation?
","You may use a combination of a ScalarFormatter and a FuncFormatter to format your values as mathtext. I.e. instead of 0.01 or 1e-2 it would look like .
import matplotlib.pyplot as plt
import matplotlib.ticker as mticker

vals = [0.000001,0.00001,0.0001,0.01,1.0]

f = mticker.ScalarFormatter(useOffset=False, useMathText=True)
g = lambda x,pos : ""${}$"".format(f._formatSciNotation('%1.10e' % x))
fmt = mticker.FuncFormatter(g)

fig, ax = plt.subplots()
for i,val in enumerate(vals):
    ax.plot([0,1],[i,i], label=""val = {}"".format(fmt(val)))

ax.legend()    
plt.show()


This is essentially the same concept as in Can I show decimal places and scientific notation on the axis of a matplotlib plot?, just not for the axes, but the legend.
",matplotlib
how can i set a default zoom in a matplotlib figure,"I have the following code...
import numpy as np
import matplotlib.pyplot as plt

np.random.seed(seed=2021)

x = np.arange(0, 200)
y = np.random.randint(1, 10, 200)

plt.plot(x, y)

plt.show()

...that generates this chart:

What I need is when the code start, the chart zooms by default to the last 25 records.

I do not want to limit the data. I want the 200 records to continue graphing, so that later I can move through the chart (with the arrow in the lower left corner) in case I want to see the historical data.
Is there a way to set a default zoom when the chart start?
","If you do not want to hard code the left limit in xlim, you can read the shape of your x array and use this to plot only the last 25 samples:
plt.xlim(left=x.shape[0]-25)
",matplotlib
how to change font properties of a matplotlib colorbar label,"In matplotlib, I want to change the font properties for a colorbar label. For example I want the label to appear bold.
Here is some example code:
from matplotlib.pylab import *
pcolor(arange(20).reshape(4,5))
cb = colorbar(label='a label')

and the result, where I want ""a label"" to appear bold:

All other answers on this site only answer how to change ticklabels or change all fonts in general (via modification of the matplotlibrc file)
","This two-liner can be used with any Text property (http://matplotlib.org/api/text_api.html#matplotlib.text.Text)
cb = plt.colorbar()
cb.set_label(label='a label',weight='bold')

",matplotlib
how to avoid matplot subplot from overlapping,"Code:
import pandas as pd
import matplotlib.pyplot as plt

maxRow = 23

fig, axes = plt.subplots(maxRow, 1)

for x in range(maxRow):
    axes[x].plot(1,1)

plt.show()

Running it shows all the plot overlapping like this.

Can I dynamically space the graphs so they are not overlapping?
Edit 1: Something that looks like this but with 20 more below it.

","If you want that many plots organized that way on a normal sized screen your only choice is to just make them really small. You can accomplish that by decreasing the dpi and increasing the size so that matplotlib will try to jam it in smaller.
Here is some example code demonstrating my suggestion:
import matplotlib.pyplot as plt

maxRow = 23
fig, axes = plt.subplots(maxRow, 1, dpi=20, figsize=(50, 50))
for x in range(maxRow):
    axes[x].plot(1, 1)
plt.show()

Which yields the following plot which doesn't have overlapping plots (but does have very tiny text):

The parameters are tunable so you can adjust until it matches your expectations.

After a little toying with it I was able to get the most readable version I could using the following code (any bigger and it either wouldn't fit on my screen or would overlap):
import matplotlib.pyplot as plt

maxRow = 23
fig, axes = plt.subplots(maxRow, 1, dpi=50, figsize=(20, 30))
fig.tight_layout()
for x in range(maxRow):
    axes[x].plot(1, 1)
plt.show()

Which looks like the following:

",matplotlib
inverse fast fourier transform ifft2 of scipy not working for fourier optics,"I'm following a tutorial on youtube on Fourier Optics in python, to simulate diffraction of light through a slit.

The video in question

Source Code of video

Now, I'm trying to implement the get_U(z, k) function and then display the corresponding plot below it, as shown in the video (I've got barebones knowledge about this topic), however, i just can't seem to get the plot working (white plot is visible the entire time). Upon inspection, I've discovered that the U variable just consists of a bunch of (nan+nanj) values, which I think shouldn't be the case. I've crosschecked the formula and it looks perfect. I also realise that, sometimes the np.sqrt() has to deal with negative values, but adding neither a np.abs() nor a np.where()(to convert negatives to zero) gives me the intended output.

My code:
import numpy as np
import scipy as sp
from scipy.fft import fft2, ifft2, fftfreq, fftshift
import matplotlib.pyplot as plt
import pint

plt.style.use(['grayscale'])
u = pint.UnitRegistry()

D = 0.1 * u.mm
lam = 660 * u.mm

x = np.linspace(-2, 2, 1600) * u.mm
xv, yv = np.meshgrid(x, x)

U0 = (np.abs(xv) < D/2) * (np.abs(yv) < 0.5 * u.mm)
U0 = U0.astype(float)

A = fft2(U0)
kx = fftfreq(len(x), np.diff(x)[0]) * 2 * np.pi
kxv, kyv = np.meshgrid(kx, kx)

def get_U(z, k):
  return ifft2(A*np.exp(1j*z.magnitude*np.sqrt(k.magnitude**2 - kxv.magnitude**2 - kyv.magnitude**2)))
k = 2*np.pi/(lam)
d = 3 * u.cm
U = get_U(d, k)

plt.figure(figsize=(5, 5))
plt.pcolormesh(xv, yv, np.abs(U), cmap='inferno')
plt.xlabel('$x$ [mm]')
plt.ylabel('$y$ [mm]')
plt.title('Single slit diffraction')
plt.show()

","Your units of lam are wrong - if you intend to use pint (but I suggest that you don't) then they should be in nm, not mm.
When you have made that change I suggest that you remove all reference to pint and mixed units and work entirely in a single set of length units (here, m). This is because units appear to be stripped when creating some of the numpy arrays. You can use scientific notation (e.g. 1e-9) to imply the units. Then you get what I think you require.

import numpy as np
import scipy as sp
from scipy.fft import fft2, ifft2, fftfreq, fftshift
import matplotlib.pyplot as plt

plt.style.use(['grayscale'])

D = 0.1   * 1e-3
lam = 660 * 1e-9

x = np.linspace(-2, 2, 1600) * 1e-3
xv, yv = np.meshgrid(x, x)

U0 = (np.abs(xv) < D/2) * (np.abs(yv) < 0.5 * 1e-3)
U0 = U0.astype(float)

A = fft2(U0)
kx = fftfreq(len(x), np.diff(x)[0]) * 2 * np.pi
kxv, kyv = np.meshgrid(kx, kx)

def get_U(z, k):
  return ifft2(A*np.exp(1j*z*np.sqrt(k**2 - kxv**2 - kyv**2)))
k = 2*np.pi/(lam)
d = 3 * 1e-2
U = get_U(d, k)

plt.figure(figsize=(5, 5))
plt.pcolormesh(xv*1e3, yv*1e3, np.abs(U), cmap='inferno')
plt.xlabel('$x$ [mm]')
plt.ylabel('$y$ [mm]')
plt.title('Single slit diffraction')
plt.show()

",matplotlib
how to align toprotated ylabel on the righthand side on a plot with matplotlib,"In matplotlib, how can I get the y-label to align with the top of the plot, when the label is rotated, moved to the right and top?
import matplotlib.pyplot as plt
import numpy as np

#create plot
fig, ax = plt.subplots()
ax.plot(np.sin(np.linspace(-np.pi, np.pi, 1001)))

ax.tick_params(
                    which=""both"",
                    direction=""out"",
                    left=False,
                    right=True,
                    labelleft=False,
                    labelright=True,
                )

ax.set_ylabel(""y-label"", loc=""top"", rotation=-90, labelpad=10)
ax.yaxis.set_label_position(""right"")


","You could fix the horizontal alignment after defining the label (unfortunately, you can't define it directly in set_ylabel if loc is used):
ax.set_ylabel('y-label', loc='top', rotation=-90, labelpad=10)
ax.yaxis.set_label_position('right')
ax.yaxis.label.set_horizontalalignment('left')

Output:

",matplotlib
how to add legend on matplotlib animations,"I have a animation of an orbit written in python. I want a legend that the label is a time ""dt"". This time updates (increases) along the orbit. For example, in the first frame of the gif the legend should be ""dt"", in the second, dt + dt and so on. I tried something that didn't work. Maybe I'm not using the correct commands. The error that appearas: TypeError: __init__() got multiple values for argument 'frames' . Does someone know how to do this legend? The code is above.
fig = plt.figure()

plt.xlabel(""x (km)"")
plt.ylabel(""y (km)"")
plt.gca().set_aspect('equal')
ax = plt.axes()
ax.set_facecolor(""black"")
circle = Circle((0, 0), rs_sun, color='dimgrey')
plt.gca().add_patch(circle)
plt.axis([-(rs_sun / 2.0) / u1 , (rs_sun / 2.0) / u1 , -(rs_sun / 2.0) / u1 , (rs_sun / 2.0) / u1 ])

# Legend

dt = 0.01

leg = [ax.plot(loc = 2, prop={'size':6})]

def update(dt):
  for i in range(len(x)):
    lab = 'Time:'+str(dt+dt*i)
    leg[i].set_text(lab)
  return leg

# GIF

graph, = plt.plot([], [], color=""gold"", markersize=3)
    
plt.close() 

def animate(i):
    graph.set_data(x[:i], y[:i])
    return graph,

skipframes = int(len(x)/500)
if skipframes == 0:
    skipframes = 1

ani = FuncAnimation(fig, update, animate, frames=range(0,len(x),skipframes), interval=20)

","To update the labels in your legend you can use:
graph, = plt.plot([], [], color=""gold"",lw=5,markersize=3,label='Time: 0')
L=plt.legend(loc=1)
L.get_texts()[0].set_text(lab)

L.get_texts()[0].set_text(lab) should be called inside the update function to refresh the label at each iteration of the animation.
You can find a minimal example below to illustrate the process:
import numpy as np
import matplotlib.pyplot as plt
import matplotlib.animation as animation

fig,ax = plt.subplots()
dt = 0.01
N_frames=30
x=np.random.choice(100,size=N_frames,) #Create random trajectory
y=np.random.choice(100,size=N_frames,) #Create random trajectory
graph, = plt.plot([], [], color=""gold"",lw=5,markersize=3,label='Time: 0')
L=plt.legend(loc=1) #Define legend objects

def init():
    ax.set_xlim(0, 100)
    ax.set_ylim(0, 100)
    return graph,

def animate(i):
    lab = 'Time:'+str(round(dt+dt*i,2))
    graph.set_data(x[:i], y[:i])
    L.get_texts()[0].set_text(lab) #Update label each at frame

    return graph,

ani = animation.FuncAnimation(fig,animate,frames=np.arange(N_frames),init_func=init,interval=200)
plt.show()

And the output gives:

",matplotlib
how to add  to annotations using the fmt option of bar_label,"I'm trying to use the new bar_label option in Matplotlib but am unable to find a way to append text e.g. '%' after the label values. Previously, using ax.text I could use f-strings, but I can't find a way to use f-strings with the bar-label approach.
fig, ax = plt.subplots(1, 1, figsize=(12,8))
hbars = ax.barh(wash_needs.index, wash_needs.values, color='#2a87c8')
ax.tick_params(axis='x', rotation=0)

# previously I used this approach to add labels 
#for i, v in enumerate(wash_needs):
#    ax.text(v +3, i, str(f""{v/temp:.0%}""), color='black', ha='right', va='center')

ax.bar_label(hbars, fmt='%.2f', padding=3) # this adds a label but I can't find a way to append a '%' after the number
    
plt.show()

","I found a way to append '%' to the label figures - add an additional '%%'
ax.bar_label(hbars, fmt='%.2f%%', padding=3)

Working Example
import pandas as pd
import seaborn as sns  # for tips data

tips = sns.load_dataset('tips').loc[:15, ['total_bill', 'tip']]
tips.insert(2, 'tip_percent', tips.tip.div(tips.total_bill).mul(100).round(2))

   total_bill   tip  tip_percent
0       16.99  1.01         5.94
1       10.34  1.66        16.05
2       21.01  3.50        16.66
3       23.68  3.31        13.98
4       24.59  3.61        14.68

# plot
ax = tips.plot(kind='barh', y='tip_percent', legend=False, figsize=(12, 8))
labels = ax.set(xlabel='Tips: Percent of Bill (%)', ylabel='Record', title='Tips % Demo')
annotations = ax.bar_label(ax.containers[0], fmt='%.2f%%', padding=3)
ax.margins(x=0.1)


",matplotlib
how can i run a code where i can plot and save multiple hours of the latest gfs model run,"I want to plot precip type from the start of the GFS run (hour 0) through hour 240 at 6 hour intervals. (in this code I only try to go to hour 108) Also, at the end of the code when saving the the plots, how do I save them each as seperate png images?
Here is the code:
from datetime import datetime
from datetime import timedelta
import pandas as pd
import cartopy.crs as ccrs
import cartopy.feature as cfeature
import matplotlib.pyplot as plt
import matplotlib.colors as mcolors
import xarray as xr
import numpy as np
import metpy.calc as mpcalc
from metpy.plots import USCOUNTIES
import netCDF4
from netCDF4 import Dataset
from netCDF4 import num2date
from metpy.units import units
from scipy.ndimage import gaussian_filter
import scipy.ndimage as ndimage
from siphon.catalog import TDSCatalog

start_time = datetime(2025, 1, 7, 12, 0, 0)
time_deltas = [timedelta(hours=6), timedelta(hours=12), timedelta(hours=18), timedelta(hours=24), timedelta(hours=30), timedelta(hours=36),
                timedelta(hours=42), timedelta(hours=48), timedelta(hours=54), timedelta(hours=60), timedelta(hours=66), timedelta(hours=72),
               timedelta(hours=78), timedelta(hours=84), timedelta(hours=90), timedelta(hours=96), timedelta(hours=102), timedelta(hours=108)]
for time_delta in time_deltas:
        dt = start_time + time_delta
#dt = datetime(2025,1,4,12)
best_gfs = TDSCatalog('https://thredds.ucar.edu/thredds/catalog/grib/NCEP/GFS/Global_0p25deg/catalog.xml?dataset=grib/NCEP/GFS/Global_0p25deg/Best')
best_ds = best_gfs.datasets[0]
ncss = best_ds.subset()
query = ncss.query()
query.accept('netcdf')
query.lonlat_box(north=75, south=15, east=320, west=185)
query.time(dt)
query.variables('Geopotential_height_isobaric', 'Pressure_reduced_to_MSL_msl', 'Precipitation_rate_surface', 'Snow_depth_surface', 'Categorical_Snow_surface','Categorical_Freezing_Rain_surface', 'Categorical_Ice_Pellets_surface')

data = ncss.get_data(query)
print(list(data.variables))

plev = list(data.variables['isobaric'][:])

lat = data.variables['latitude'][:].squeeze()
lon = data.variables['longitude'][:].squeeze()
time1 = data['time']
vtime = num2date(time1[:].squeeze(), units=time1.units)
emsl_var = data.variables['Pressure_reduced_to_MSL_msl']
preciprate = data.variables['Precipitation_rate_surface'][:].squeeze()
snowdepth = data.variables['Snow_depth_surface'][:].squeeze()
catsnow = data.variables['Categorical_Snow_surface'][:].squeeze()
catice = data.variables['Categorical_Freezing_Rain_surface'][:].squeeze()
catsleet = data.variables['Categorical_Ice_Pellets_surface'][:].squeeze()
EMSL = units.Quantity(emsl_var[:], emsl_var.units).to('hPa')
mslp = gaussian_filter(EMSL[0], sigma=3.0)
hght_1000 = data.variables['Geopotential_height_isobaric'][0, plev.index(100000)]
hght_500 = data.variables['Geopotential_height_isobaric'][0, plev.index(50000)]
thickness_1000_500 = gaussian_filter((hght_500 - hght_1000)/10, sigma=3.0)
lon_2d, lat_2d = np.meshgrid(lon, lat)

precip_inch_hour = preciprate * 141.73228346457
precip2 = mpcalc.smooth_n_point(precip_inch_hour, 5, 1)

precip_colors = [
   ""#bde9bf"",  # 0.01 - 0.02 inches 1
   ""#adddb0"",  # 0.02 - 0.03 inches 2
   ""#9ed0a0"",  # 0.03 - 0.04 inches 3
   ""#8ec491"",  # 0.04 - 0.05 inches 4
   ""#7fb882"",  # 0.05 - 0.06 inches 5
   ""#70ac74"",  # 0.06 - 0.07 inches 6
   ""#60a065"",  # 0.07 - 0.08 inches 7
   ""#519457"",  # 0.08 - 0.09 inches 8
   ""#418849"",  # 0.09 - 0.10 inches 9
   ""#307c3c"",  # 0.10 - 0.12 inches 10
   ""#1c712e"",  # 0.12 - 0.14 inches 11
   ""#f7f370"",  # 0.14 - 0.16 inches 12
   ""#fbdf65"",  # 0.16 - 0.18 inches 13
   ""#fecb5a"",  # 0.18 - 0.2 inches 14
   ""#ffb650"",  # 0.2 - 0.3 inches 15
   ""#ffa146"",  # 0.3 - 0.4 inches 16
   ""#ff8b3c"",   # 0.4 - 0.5 inches 17
   ""#f94609"",   # 0.5 - 0.6 inches 18
]

precip_colormap = mcolors.ListedColormap(precip_colors)

clev_precip =  np.concatenate((np.arange(0.01, 0.1, .01), np.arange(.1, .2, .02), np.arange(.2, .61, .1)))
norm = mcolors.BoundaryNorm(clev_precip, 18)

datacrs = ccrs.PlateCarree()
plotcrs = ccrs.LambertConformal(central_latitude=35, central_longitude=-100,standard_parallels=(30, 60))
bounds = ([-105, -90, 30, 40])
fig = plt.figure(figsize=(14,12))
ax = fig.add_subplot(1,1,1, projection=plotcrs)
ax.set_extent(bounds, crs=ccrs.PlateCarree())
ax.add_feature(cfeature.COASTLINE.with_scale('50m'), linewidth = 0.75)
ax.add_feature(cfeature.STATES, linewidth = 1)
ax.add_feature(USCOUNTIES, edgecolor='grey', linewidth = .5)
clevs = (np.arange(0, 540, 6),
         np.array([540]),
         np.arange(546, 700, 6))
colors = ('tab:blue', 'b', 'tab:red')
kw_clabels = {'fontsize': 11, 'inline': True, 'inline_spacing': 5, 'fmt': '%i',
              'rightside_up': True, 'use_clabeltext': True}

# Plot MSLP
clevmslp = np.arange(800., 1120., 2)
cs2 = ax.contour(lon_2d, lat_2d, mslp, clevmslp, colors='k', linewidths=1.25,
                 linestyles='solid', transform=ccrs.PlateCarree())

cf = ax.contourf(lon_2d, lat_2d, precip2, clev_precip, cmap=precip_colormap, norm=norm, extend='max', transform=ccrs.PlateCarree())

ax.set_title('GFS Precip Type, Rate(in/hr), MSLP (hPa), & 1000-500mb Thickness (dam)', loc='left', fontsize=10, weight = 'bold')
ax.set_title('Valid Time: {}z'.format(vtime), loc = 'right', fontsize=8)

I tried using this: dt = start_time + time_delta but this only plots the last timedelta which is hour 108 and not all the other timedelta hours.
Plot: Plot
","You need to put all your code inside the loop, now u are only generating the variable ""dt"" inside the loop, so, you end the loop with the final dt,that is the timedelta 108.
I made a litle change that makes u able to set how many 6 hours intervals u want.
Try this:
from datetime import datetime
from datetime import timedelta
import pandas as pd
import cartopy.crs as ccrs
import cartopy.feature as cfeature
import matplotlib.pyplot as plt
import matplotlib.colors as mcolors
import xarray as xr
import numpy as np
import metpy.calc as mpcalc
from metpy.plots import USCOUNTIES
import netCDF4
from netCDF4 import Dataset
from netCDF4 import num2date
from metpy.units import units
from scipy.ndimage import gaussian_filter
import scipy.ndimage as ndimage
from siphon.catalog import TDSCatalog

start_time = datetime(2025, 1, 7, 12, 0, 0)

hours_intervals = 40

for k in range(0,hours_intervals):
        dt = start_time + timedelta(hours= 6*(k+1))

        #dt = datetime(2025,1,4,12)
        best_gfs = TDSCatalog('https://thredds.ucar.edu/thredds/catalog/grib/NCEP/GFS/Global_0p25deg/catalog.xml?dataset=grib/NCEP/GFS/Global_0p25deg/Best')
        best_ds = best_gfs.datasets[0]
        ncss = best_ds.subset()
        query = ncss.query()
        query.accept('netcdf')
        query.lonlat_box(north=75, south=15, east=320, west=185)
        query.time(dt)
        query.variables('Geopotential_height_isobaric', 'Pressure_reduced_to_MSL_msl', 'Precipitation_rate_surface', 'Snow_depth_surface', 'Categorical_Snow_surface','Categorical_Freezing_Rain_surface', 'Categorical_Ice_Pellets_surface')
        
        data = ncss.get_data(query)
        print(list(data.variables))
        
        plev = list(data.variables['isobaric'][:])
        
        lat = data.variables['latitude'][:].squeeze()
        lon = data.variables['longitude'][:].squeeze()
        time1 = data['time']
        vtime = num2date(time1[:].squeeze(), units=time1.units)
        emsl_var = data.variables['Pressure_reduced_to_MSL_msl']
        preciprate = data.variables['Precipitation_rate_surface'][:].squeeze()
        snowdepth = data.variables['Snow_depth_surface'][:].squeeze()
        catsnow = data.variables['Categorical_Snow_surface'][:].squeeze()
        catice = data.variables['Categorical_Freezing_Rain_surface'][:].squeeze()
        catsleet = data.variables['Categorical_Ice_Pellets_surface'][:].squeeze()
        EMSL = units.Quantity(emsl_var[:], emsl_var.units).to('hPa')
        mslp = gaussian_filter(EMSL[0], sigma=3.0)
        hght_1000 = data.variables['Geopotential_height_isobaric'][0, plev.index(100000)]
        hght_500 = data.variables['Geopotential_height_isobaric'][0, plev.index(50000)]
        thickness_1000_500 = gaussian_filter((hght_500 - hght_1000)/10, sigma=3.0)
        lon_2d, lat_2d = np.meshgrid(lon, lat)
        
        precip_inch_hour = preciprate * 141.73228346457
        precip2 = mpcalc.smooth_n_point(precip_inch_hour, 5, 1)
        
        precip_colors = [
           ""#bde9bf"",  # 0.01 - 0.02 inches 1
           ""#adddb0"",  # 0.02 - 0.03 inches 2
           ""#9ed0a0"",  # 0.03 - 0.04 inches 3
           ""#8ec491"",  # 0.04 - 0.05 inches 4
           ""#7fb882"",  # 0.05 - 0.06 inches 5
           ""#70ac74"",  # 0.06 - 0.07 inches 6
           ""#60a065"",  # 0.07 - 0.08 inches 7
           ""#519457"",  # 0.08 - 0.09 inches 8
           ""#418849"",  # 0.09 - 0.10 inches 9
           ""#307c3c"",  # 0.10 - 0.12 inches 10
           ""#1c712e"",  # 0.12 - 0.14 inches 11
           ""#f7f370"",  # 0.14 - 0.16 inches 12
           ""#fbdf65"",  # 0.16 - 0.18 inches 13
           ""#fecb5a"",  # 0.18 - 0.2 inches 14
           ""#ffb650"",  # 0.2 - 0.3 inches 15
           ""#ffa146"",  # 0.3 - 0.4 inches 16
           ""#ff8b3c"",   # 0.4 - 0.5 inches 17
           ""#f94609"",   # 0.5 - 0.6 inches 18
        ]
        
        precip_colormap = mcolors.ListedColormap(precip_colors)
        
        clev_precip =  np.concatenate((np.arange(0.01, 0.1, .01), np.arange(.1, .2, .02), np.arange(.2, .61, .1)))
        norm = mcolors.BoundaryNorm(clev_precip, 18)
        
        datacrs = ccrs.PlateCarree()
        plotcrs = ccrs.LambertConformal(central_latitude=35, central_longitude=-100,standard_parallels=(30, 60))
        bounds = ([-105, -90, 30, 40])
        fig = plt.figure(figsize=(14,12))
        ax = fig.add_subplot(1,1,1, projection=plotcrs)
        ax.set_extent(bounds, crs=ccrs.PlateCarree())
        ax.add_feature(cfeature.COASTLINE.with_scale('50m'), linewidth = 0.75)
        ax.add_feature(cfeature.STATES, linewidth = 1)
        ax.add_feature(USCOUNTIES, edgecolor='grey', linewidth = .5)
        clevs = (np.arange(0, 540, 6),
                 np.array([540]),
                 np.arange(546, 700, 6))
        colors = ('tab:blue', 'b', 'tab:red')
        kw_clabels = {'fontsize': 11, 'inline': True, 'inline_spacing': 5, 'fmt': '%i',
                      'rightside_up': True, 'use_clabeltext': True}
        
        # Plot MSLP
        clevmslp = np.arange(800., 1120., 2)
        cs2 = ax.contour(lon_2d, lat_2d, mslp, clevmslp, colors='k', linewidths=1.25,
                         linestyles='solid', transform=ccrs.PlateCarree())
        
        cf = ax.contourf(lon_2d, lat_2d, precip2, clev_precip, cmap=precip_colormap, norm=norm, extend='max', transform=ccrs.PlateCarree())
        
        ax.set_title('GFS Precip Type, Rate(in/hr), MSLP (hPa), & 1000-500mb Thickness (dam)', loc='left', fontsize=10, weight = 'bold')
        ax.set_title('Valid Time: {}z'.format(vtime), loc = 'right', fontsize=8)

        fig.savefig('path/to/save/image/to.png')

",matplotlib
pydroid  unexpected eof while reading a line,"import matplotlib.pyplot as plt
xaxis=[]
yaxis=[]
x=int(input('number of triangles:'))
while x>0:
       for i in range(0,0x3ff):
                yaxis.append(i)
       for i in range(0x3ff, 0,-1):yaxis.append(i)

       x=x-1
for j in range(len(yaxis)):xaxis.append(j)
plt.plot(xaxis,yaxis)
plt.ylabel('amplitude')
plt.show()

The above code throws the following error:
Traceback (most recent call last):
      File ""/data/user/0/ru.iiec.pydroid3/files/temp_iiec_codefile.py"", line 4, in <module>
        x=int(input('number of triangles:'))
    EOFError: EOF when reading a line

","According to this post, it is just impossible to use input and print with matplotlib for the simple reason that there is no terminal attached to the GUI on Pydroid.
What you can do if you really want to run it on mobile is either to look for another python app which works as you want, either to use repl.it (your code perfectly worked for me on this website)
",matplotlib
building gauge chart in python with matplotlib,"I am attempting to follow this tutorial to create a roughly semi-circle gauge chart with matplotlib. However, my figure contains several different charts - it is being created via a line like:
fig, ax = plt.subplots(2, 4, figsize=(11,6))
With each chart being created similar to this:
ax[0,0].bar(categories, values)
The issue I am running into is if I try to alter the tutorial code to something like
ax[0,0].bar(x=[0, 0.44, 0.88,1.32,1.76,2.2,2.64], width=0.5, height=0.5, bottom=2, linewidth=3, edgecolor=""white"", color=colors, align=""edge"");
It seems to not be contained within the grid of charts, and is much larger than anticipated. I assume this is an issue with the line ax = fig.add_subplot(projection=""polar"");, since none of the other charts I have created use anything like this, but I am not sure how to specify that the chart needs to use polar coordinates otherwise. Any suggestions are appreciated.
Edit: Here is a full reproducible example with random data
fig, ax = plt.subplots(2, 2, figsize=(11,6))

gauge_colors = [""#FF2C2C"", ""#FFF34F"", ""#39FF14"", ""#FFF34F"", ""#FF2C2C""]
ax[0, 0] = plt.subplot(projection=""polar"")
ax[0, 0].bar(x=[0, 0.44, 0.88, 1.32, 1.76, 2.2, 2.64], width=0.5, height=0.5, bottom=2,
             linewidth=3, edgecolor=""white"",
             color=gauge_colors, align=""edge"");
ax[0, 0].annotate(""50"", xytext=(0, 0), xy=(1.1, 2.0),
                  arrowprops=dict(arrowstyle=""wedge, tail_width=0.5"", color=""black"", shrinkA=0),
                  bbox=dict(boxstyle=""circle"", facecolor=""black"", linewidth=2.0, ),
                  fontsize=45, color=""white"", ha=""center""
                  );

for i in range(2):
    for j in range(2):
        if i > 0 or j > 0:
            ax[i, j].plot(np.random.rand(10))

plt.show()

","The issue is that you cannot change the projection of an existing Axes instance. And plt.subplots does not allow you to set the projection of each Axes instance individually.
@rehaqds's answer is one way to do what you want. However, if you want to continue using plt.subplots to create your grid of subplots, you can, with a small modification.
First, you create the grid of subplots. Then you remove the one you want to change to polar projection, and then add it again, using, for example, fig.add_subplot().
For your minimal example, that would look like:
import matplotlib.pyplot as plt

fig, ax = plt.subplots(2, 2, figsize=(11,6))

ax[0, 0].remove()
ax[0, 0] = fig.add_subplot(2, 2, 1, projection=""polar"")

gauge_colors = [""#FF2C2C"", ""#FFF34F"", ""#39FF14"", ""#FFF34F"", ""#FF2C2C""]

ax[0, 0].bar(x=[0, 0.44, 0.88, 1.32, 1.76, 2.2, 2.64], width=0.5, height=0.5, bottom=2,
             linewidth=3, edgecolor=""white"",
             color=gauge_colors, align=""edge"");
ax[0, 0].annotate(""50"", xytext=(0, 0), xy=(1.1, 2.0),
                  arrowprops=dict(arrowstyle=""wedge, tail_width=0.5"", color=""black"", shrinkA=0),
                  bbox=dict(boxstyle=""circle"", facecolor=""black"", linewidth=2.0, ),
                  fontsize=15, color=""white"", ha=""center""
                  );

for i in range(2):
    for j in range(2):
        if i > 0 or j > 0:
            ax[i, j].plot(np.random.rand(10))

plt.show()


",matplotlib
draw lines between several points with known coordinates,"My task: to connect points with lines using known coordinates.
What I have already done: using the doctr library, I find the coordinates of the words I am interested in on a pdf file. (My python code is not important for this question, below I will show what data I get from a specific file)
For example, there is a pdf file in which I am looking for the word ""INVOICE""

After doctr has run, I get the following data
((0.09370404411764705, 0.0439453125), (0.33140099789915967, 0.09765625));
((0.5925912552521009, 0.1796875), (0.6575433298319328, 0.1953125));
((0.5925912552521009, 0.2041015625), (0.6575433298319328, 0.21875));
My question: how can I draw lines between these three points? To get a figure at the output.
For a better understanding of the situation for you: the figure on this file will be like a template for me. I receive a large number of documents, and in order not to look through everything, I will check whether this figure is on the received document, and look through only the necessary document.
","You can use PyMuPDF Library. The following code snippet will help you.
import fitz  # PyMuPDF
doc = fitz.open(""Document_Name_Original.pdf"")
page = doc[pno]  # page number
p1 = fitz.Point(x_1, y_1)  # First point
p2 = fitz.Point(x_2, y_2)  # Second point
p3 = fitz.Point(x_3, y_3)  # Third point
page.draw_line(p1, p2, color=fitz.pdfcolor[""red""], width=2)
page.draw_line(p2, p3, color=fitz.pdfcolor[""red""], width=2)
page.draw_line(p3, p1, color=fitz.pdfcolor[""red""], width=2)
doc.save(""Document_Name_Modified.pdf"")

Through this library, you can modify, add graph and add any plotting. If you want to know detail, this link 1 and link 2 help you for guide drawing on PDF document.
",matplotlib
set matplotlib default figure window title,"The default window title of a figure is figure X, where X is increased each figure.
I know how to change the title of a figure:
fig = pylab.gcf()
fig.canvas.set_window_title('Test')

But how do I change the default window title (So that it will be ""Test 1"", ""Test 2"" etc.), so that I will not need to change the window title each time?
I did not find a key in the mpl.rcParams
","There is no key in mpl.rcParams since the default title is hardcoded in the backends. For example, have a look at the figure initialization code of the QT5 backend:
self.window.setWindowTitle(""Figure %d"" % num)

This means you cannot change the default window title unless you change the code of the matplotlib module itself.
",matplotlib
within a function add dynamic ytick labels to a linechart that are evenly distributed when data value range in df parameter varies,"I have a series of datasets that I have to create a line graph for. I created a function that takes a df containing the data, a date to indicate the vertical line for ""Inception date"", and the dir where to save the created graph.
The issue I am having is that for the line graph, I can't seem to properly add yticks to display evenly and beyond the last data point. I also want the ytick values to be rounded to the tens value.
This is what I have tried:

Use np.arange() but it didn't work well. The range of the data varies based on the dataset and I couldn't figure this one out.
Use a condition to check if the max value is above or below 1000 to help with the bins distribution. I am not sure if this is a good approach as I made the assumption of having 1000 as a threshold. Then, I set nbins=7 to see how it looks. I can't figure out how to make the nbins value dynamic so that the bins are evenly distributed over the y axis.

Here is my function:
def build_multiple_line_graphs(df, inception_date, line_graph_path):
  """"""Builds line graphs for market data.""""""
     # adjust this line to your local folder path
     os.makedirs(line_graph_path, exist_ok=True)
  
     inception_date = datetime.strptime(inception_date, '%Y-%m-%d')
  
     # Plot the time series
     fig, ax = plt.subplots()
 
     # setting the size of the line graph
     dpi=150
     desired_width_pixels = 563
     desired_height_pixels = 169
     fig.set_size_inches(desired_width_pixels / dpi, desired_height_pixels / dpi)
     fig.tight_layout()
     df.plot(kind = 'line', ax=ax, y=complex, color='#0092BC')
      
     # Add a horizontal line at y=0
     ax.axhline(y=0, color='#005871', linestyle='-', label='Horizontal Line')
 
     # Set the background color
     ax.set_facecolor('#F0F8FF')
     fig.patch.set_facecolor('#e4f2ff')
 
     # removing frame from the graph
     ax.spines['top'].set_visible(False)
     ax.spines['right'].set_visible(False)
     ax.spines['left'].set_visible(False)
 
     # set custom y-axis label
     max_y = max(df)
     if max_y < 1000:
        max_y = 100*math.ceil(round(max_y/100)) if max_y > 0 else 1
     else:
        max_y = round((1000*math.ceil(max_y/1000))) if max_y > 0 else 1
    
     new_max_y = max_y + (max_y//3)
     increment = round(max_y/5)
     ax.set_yticks(np.arange(0, max_y, increment))
     plt.locator_params(axis='y', nbins=7)
        
     # here is where I tried np.arange with no success
     #ax.yaxis.set_major_locator(ticker.MultipleLocator(increment))
     #ax.set_yticks(np.arange(0,df+1, increment))
     #plt.yticks(np.arange(0,max(df)+increment, increment))
     #start, end = ax.get_ylim()
     #stepsize = end/5
     #ax.yaxis.set_ticks(np.arange(0, end+1, 100))
     ax.set_ylim(ymin=0)
 
     # set custom x-axis label every 5 years
     ax.xaxis.set_major_locator(mdates.YearLocator(5))
     ax.xaxis.set_major_formatter(mdates.DateFormatter('%Y'))
     plt.xticks(rotation=0, ha='center')
     ax.grid(axis='y', color='gray')
      
     # Customize the plot
     ax.legend().remove()
     ax.tick_params(axis='both', labelsize=8, left = False, bottom = False)
     ax.set_xlabel(None)
     ax.fill_between(df.index, df, 0, color='#FFFFFF', alpha=0.8)
     ax.set_xlim(df.index[0], df.index[-1])
            
     # formating the inception line
     ax.axvline(inception_date, color = '#0092BC', linestyle='dotted', label='Inception Date')
     ax.text(inception_date, 25, 'Inception Date', rotation=90, rotation_mode='anchor', transform_rotates_text= True, fontsize=6)
     inception_date_text = inception_date.strftime(""%m/%d/%y"")
     ax.annotate(inception_date_text, xy=(inception_date, -1), xytext=(-15, -18), fontsize=8,
     textcoords='offset points', ha='center', va='bottom')
 
     # saving the plot
     plt.gcf() # get current figure
     fig_size = plt.gcf().get_size_inches()
     plt.gcf().set_size_inches(fig_size[0]*2, fig_size[1]*2)
     plt.savefig(os.path.join(line_graph_path, 'name_linegraph.png'), bbox_inches='tight',dpi=300)
        
      # no display of the plot needed
      plt.close()


Here is how it looks now:

The Inception date in this example is 2024-08-23.
Here are the libraries I used:
import matplotlib.pyplot as plt
import matplotlib.dates as mdates
import matplotlib.ticker as ticker
import os
from datetime import datetime, timedelta
from datetime import *
import math

","You were almost there. Just add +1 on this line:
ax.set_yticks(np.arange(0, max_y + 1, increment))

",matplotlib
how to line plot pandas dataframe as sub graphs,"I have a DataFrame that looks like this:
{""1578286800000"":71,""1578373200000"":72,""1578459600000"":72,""1578546000000"":74,""1578632400000"":7,""1578891600000"":7,""1578978000000"":6,""1579064400000"":7,""1579150800000"":6}
The format is:
Datetime:int
I want to create sub graph out of the data like, graph one would be for the first 5 data pairs and graph two would be for the rest.
I've tried to graph the entire dataframe but keeps getting this error:
ValueError: If using all scalar values, you must pass an index
As you can see the dataframe doesn't have an index, and I don't know how to specify Datetime as the x axis and int as the y axis.
Edit 1 (with code):
import pandas as pd
import matplotlib.pyplot as plt
df = pd.read_json(""somedata.json"")
df.plot.line()
plt.show()

somedata.json contains the same data as mentioned at the beginning of the question.
Edit 2:
with open('temp.json', 'r') as json_file:
data_pairs = json.load(json_file)

dataframe = pd.DataFrame.from_dict(data_pairs, orient='index')

fig, axes = plt.subplots(2, 1)
dataframe[0:5].plot(ax=axes[0], legend=False)
_ = plt.xticks(rotation=45)
dataframe[5:].plot(ax=axes[1], legend=False)
_ = plt.xticks(rotation=45)

","You can create a dataframe (with one column) having the datetime as index by using the method from_dict() with orient='index'. Then you can use the plot() method from Pandas for a quick drawing of the data divided in 2 parts:
df = pd.DataFrame.from_dict({""1578286800000"":71,""1578373200000"":72,""1578459600000"":72,""1578546000000"":74,
                             ""1578632400000"":7,""1578891600000"":7,""1578978000000"":6,""1579064400000"":7,""1579150800000"":6},
                            orient='index')
fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(7,4))
df[:5].plot(ax=ax1)
df[5:].plot(ax=ax2)
plt.show()


",matplotlib
custom logarithmic axis scaling in matplotlib,"I'm trying to scale the x axis of a plot with math.log(1+x) instead of the usual 'log' scale option, and I've looked over some of the custom scaling examples but I can't get mine to work! Here's my MWE:
import matplotlib.pyplot as plt
import numpy as np
import math
from matplotlib.ticker import FormatStrFormatter
from matplotlib import scale as mscale
from matplotlib import transforms as mtransforms

class CustomScale(mscale.ScaleBase):
    name = 'custom'

    def __init__(self, axis, **kwargs):
        mscale.ScaleBase.__init__(self)
        self.thresh = None #thresh

    def get_transform(self):
        return self.CustomTransform(self.thresh)

    def set_default_locators_and_formatters(self, axis):
        pass

    class CustomTransform(mtransforms.Transform):
        input_dims = 1
        output_dims = 1
        is_separable = True

        def __init__(self, thresh):
            mtransforms.Transform.__init__(self)
            self.thresh = thresh

        def transform_non_affine(self, a):
            return math.log(1+a)

        def inverted(self):
            return CustomScale.InvertedCustomTransform(self.thresh)

    class InvertedCustomTransform(mtransforms.Transform):
        input_dims = 1
        output_dims = 1
        is_separable = True

        def __init__(self, thresh):
            mtransforms.Transform.__init__(self)
            self.thresh = thresh

        def transform_non_affine(self, a):
            return math.log(1+a)

        def inverted(self):
            return CustomScale.CustomTransform(self.thresh)

# Now that the Scale class has been defined, it must be registered so
# that ``matplotlib`` can find it.
mscale.register_scale(CustomScale)

z = [0,0.1,0.3,0.9,1,2,5]
thick = [20,40,20,60,37,32,21]

fig = plt.figure(figsize=(8,5))
ax1 = fig.add_subplot(111)
ax1.plot(z, thick, marker='o', linewidth=2, c='k')

plt.xlabel(r'$\rm{redshift}$', size=16)
plt.ylabel(r'$\rm{thickness\ (kpc)}$', size=16)
plt.gca().set_xscale('custom')
plt.show()

","The scale consists of two Transform classes, each of which needs to provide a transform_non_affine method. One class needs to transform from data to display coordinates, which would be log(a+1), the other is the inverse and needs to transform from display to data coordinates, which would in this case be exp(a)-1. 
Those methods need to handle numpy arrays, so they should use the respective numpy functions instead of those from the math package.
class CustomTransform(mtransforms.Transform):
    ....

    def transform_non_affine(self, a):
        return np.log(1+a)

class InvertedCustomTransform(mtransforms.Transform):
    ....

    def transform_non_affine(self, a):
        return np.exp(a)-1


",matplotlib
how to plot a line in 3 dimensional space,"I have two 3D-points, for example a = (100, 100, 10) and b = (0, 100, 60), and would like to fit a line through those points.
I know, the 3D line equation can have different shapes:

Vector-form: 
(x,y,z)=(x0,y0,z0)+t(a,b,c)

Parameter-form: 
x=x0+ta
y=y0+tb
z=z0+tc

But I have a problem getting the data in the right shape for a numerical function.
","The following code should work
import matplotlib.pyplot as plt

fig = plt.figure()
ax = plt.axes(projection ='3d')
 
# defining coordinates for the 2 points.
x = np.array([100, 0])
y = np.array([100, 100])
z = np.array([10, 60])
 
# plotting
ax.plot3D(x, y, z)
plt.show()

Here the ax.plot3D() plots a curve that joins the points (x[i], y[i], z[i])  with straight lines.
",matplotlib
no module named 39matplotlibbackendsregistry39,"I started learning matplotlib with the Python Data Science Handbook by Jake VanderPlas that starts with this code for importing and setting style:
%matplotlib inline
import matplotlib.pyplot as plt
plt.style.use('seaborn-whitegrid')
import numpy as np

First, I got this error:

'seaborn-whitegrid' is not a valid package style, path of style file, URL of style file, or library style name (library styles are listed in style.available)

I tried:
print(plt.style.available)

But the 'seaborn-whitegrid' was not listed. Then I went for updating packages:
pip install --upgrade matplotlib seaborn

Still it did not work. Now I get this error:
---------------------------------------------------------------------------
ModuleNotFoundError                       Traceback (most recent call last)
Cell In[1], line 1
----> 1 get_ipython().run_line_magic('matplotlib', 'inline')
      2 import matplotlib.pyplot as plt
      3 plt.style.use('seaborn-whitegrid')

File ~\AppData\Local\Programs\Python\Python313\Lib\site-packages\IPython\core\interactiveshell.py:2480, in InteractiveShell.run_line_magic(self, magic_name, line, _stack_depth)
   2478     kwargs['local_ns'] = self.get_local_scope(stack_depth)
   2479 with self.builtin_trap:
-> 2480     result = fn(*args, **kwargs)
   2482 # The code below prevents the output from being displayed
   2483 # when using magics with decorator @output_can_be_silenced
   2484 # when the last Python token in the expression is a ';'.
   2485 if getattr(fn, magic.MAGIC_OUTPUT_CAN_BE_SILENCED, False):

File ~\AppData\Local\Programs\Python\Python313\Lib\site-packages\IPython\core\magics\pylab.py:103, in PylabMagics.matplotlib(self, line)
     98     print(
     99         ""Available matplotlib backends: %s""
    100         % _list_matplotlib_backends_and_gui_loops()
    101     )
    102 else:
--> 103     gui, backend = self.shell.enable_matplotlib(args.gui)
    104     self._show_matplotlib_backend(args.gui, backend)

File ~\AppData\Local\Programs\Python\Python313\Lib\site-packages\IPython\core\interactiveshell.py:3665, in InteractiveShell.enable_matplotlib(self, gui)
   3662     import matplotlib_inline.backend_inline
   3664 from IPython.core import pylabtools as pt
-> 3665 gui, backend = pt.find_gui_and_backend(gui, self.pylab_gui_select)
   3667 if gui != None:
   3668     # If we have our first gui selection, store it
   3669     if self.pylab_gui_select is None:

File ~\AppData\Local\Programs\Python\Python313\Lib\site-packages\IPython\core\pylabtools.py:338, in find_gui_and_backend(gui, gui_select)
    321 def find_gui_and_backend(gui=None, gui_select=None):
    322     """"""Given a gui string return the gui and mpl backend.
    323 
    324     Parameters
   (...)
    335     'WXAgg','Qt4Agg','module://matplotlib_inline.backend_inline','agg').
    336     """"""
--> 338     import matplotlib
    340     if _matplotlib_manages_backends():
    341         backend_registry = matplotlib.backends.registry.backend_registry

File ~\AppData\Local\Programs\Python\Python313\Lib\site-packages\matplotlib\__init__.py:159
    155 from packaging.version import parse as parse_version
    157 # cbook must import matplotlib only within function
    158 # definitions, so it is safe to import from it here.
--> 159 from . import _api, _version, cbook, _docstring, rcsetup
    160 from matplotlib.cbook import sanitize_sequence
    161 from matplotlib._api import MatplotlibDeprecationWarning

File ~\AppData\Local\Programs\Python\Python313\Lib\site-packages\matplotlib\rcsetup.py:26
     23 import numpy as np
     25 from matplotlib import _api, cbook
---> 26 from matplotlib.backends import BackendFilter, backend_registry
     27 from matplotlib.cbook import ls_mapper
     28 from matplotlib.colors import Colormap, is_color_like

File ~\AppData\Local\Programs\Python\Python313\Lib\site-packages\matplotlib\backends\__init__.py:1
----> 1 from .registry import BackendFilter, backend_registry  # noqa: F401
      3 # NOTE: plt.switch_backend() (called at import time) will add a ""backend""
      4 # attribute here for backcompat.
      5 _QT_FORCE_QT5_BINDING = False

ModuleNotFoundError: No module named 'matplotlib.backends.registry'

Is this way of setting the style not valid anymore? Do I have to use:
import seaborn as sns
sns.set(style='whitegrid') 

What else can I do?
Updating the progress
After trying the recommended uninstall and reinstall, I got another error telling me it cannot uninstall:
WARNING: Ignoring invalid distribution ~atplotlib (C:\...\Python\Python313\Lib\site-packages) 
Collecting matplotlib==3.10.0   
Using cached matplotlib-3.10.0-cp313-cp313-win_amd64.whl.metadata (11 kB) 
Using cached matplotlib-3.10.0-cp313-cp313-win_amd64.whl (8.0 MB) 
Installing collected packages: matplotlib   
Attempting uninstall: matplotlib     
Found existing installation: matplotlib 3.10.0 error: uninstall-no-record-file  
× Cannot uninstall matplotlib 3.10.0 
╰─> The package's contents are unknown: no RECORD file was found for matplotlib. – 
    hint: You might be able to recover from this via: pip install --force-reinstall --no-deps matplotlib==3.10.0

But trying the offered command returns the same result!
I still do not know what to do with this. So I start with this import now:
%matplotlib inline
import matplotlib.pyplot as plt
import seaborn as sns
sns.set_style(""whitegrid"")

Again, it results in this long traceback:
---------------------------------------------------------------------------
ModuleNotFoundError                       Traceback (most recent call last)
Cell In[1], line 1
----> 1 get_ipython().run_line_magic('matplotlib', 'inline')
      2 import matplotlib.pyplot as plt
      3 import seaborn as sns

File ~\AppData\Local\Programs\Python\Python313\Lib\site-packages\IPython\core\interactiveshell.py:2480, in InteractiveShell.run_line_magic(self, magic_name, line, _stack_depth)
   2478     kwargs['local_ns'] = self.get_local_scope(stack_depth)
   2479 with self.builtin_trap:
-> 2480     result = fn(*args, **kwargs)
   2482 # The code below prevents the output from being displayed
   2483 # when using magics with decorator @output_can_be_silenced
   2484 # when the last Python token in the expression is a ';'.
   2485 if getattr(fn, magic.MAGIC_OUTPUT_CAN_BE_SILENCED, False):

File ~\AppData\Local\Programs\Python\Python313\Lib\site-packages\IPython\core\magics\pylab.py:103, in PylabMagics.matplotlib(self, line)
     98     print(
     99         ""Available matplotlib backends: %s""
    100         % _list_matplotlib_backends_and_gui_loops()
    101     )
    102 else:
--> 103     gui, backend = self.shell.enable_matplotlib(args.gui)
    104     self._show_matplotlib_backend(args.gui, backend)

File ~\AppData\Local\Programs\Python\Python313\Lib\site-packages\IPython\core\interactiveshell.py:3665, in InteractiveShell.enable_matplotlib(self, gui)
   3662     import matplotlib_inline.backend_inline
   3664 from IPython.core import pylabtools as pt
-> 3665 gui, backend = pt.find_gui_and_backend(gui, self.pylab_gui_select)
   3667 if gui != None:
   3668     # If we have our first gui selection, store it
   3669     if self.pylab_gui_select is None:

File ~\AppData\Local\Programs\Python\Python313\Lib\site-packages\IPython\core\pylabtools.py:338, in find_gui_and_backend(gui, gui_select)
    321 def find_gui_and_backend(gui=None, gui_select=None):
    322     """"""Given a gui string return the gui and mpl backend.
    323 
    324     Parameters
   (...)
    335     'WXAgg','Qt4Agg','module://matplotlib_inline.backend_inline','agg').
    336     """"""
--> 338     import matplotlib
    340     if _matplotlib_manages_backends():
    341         backend_registry = matplotlib.backends.registry.backend_registry

File ~\AppData\Local\Programs\Python\Python313\Lib\site-packages\matplotlib\__init__.py:159
    155 from packaging.version import parse as parse_version
    157 # cbook must import matplotlib only within function
    158 # definitions, so it is safe to import from it here.
--> 159 from . import _api, _version, cbook, _docstring, rcsetup
    160 from matplotlib.cbook import sanitize_sequence
    161 from matplotlib._api import MatplotlibDeprecationWarning

File ~\AppData\Local\Programs\Python\Python313\Lib\site-packages\matplotlib\rcsetup.py:26
     23 import numpy as np
     25 from matplotlib import _api, cbook
---> 26 from matplotlib.backends import BackendFilter, backend_registry
     27 from matplotlib.cbook import ls_mapper
     28 from matplotlib.colors import Colormap, is_color_like

File ~\AppData\Local\Programs\Python\Python313\Lib\site-packages\matplotlib\backends\__init__.py:1
----> 1 from .registry import BackendFilter, backend_registry  # noqa: F401
      3 # NOTE: plt.switch_backend() (called at import time) will add a ""backend""
      4 # attribute here for backcompat.
      5 _QT_FORCE_QT5_BINDING = False

ModuleNotFoundError: No module named 'matplotlib.backends.registry'

Then, by calling plt like this, I get this other traceback telling me plt is not defined:
fig = plt.figure()
ax = plt.axes()

Obviously, the previous error didn't let it to be imported correctly, as the error implies:
---------------------------------------------------------------------------
NameError                                 Traceback (most recent call last)
Cell In[2], line 1
----> 1 fig = plt.figure()
      2 ax = plt.axes()

NameError: name 'plt' is not defined

","The last thing I did was manually deleting all the residual files and remnanats from the previous installations. All the files named matplotlib, mpl_toolkits, ~atplotlib, or similar in the C:\Users\asus\AppData\Local\Programs\Python\Python313\Lib\site-packages folder had to go to restart with a clean slate. Then reinstall packages with:
pip install --upgrade ipython jupyter matplotlib

Now these work seamlessly:
%matplotlib inline
import matplotlib.pyplot as plt
import seaborn as sns
sns.set_style(""whitegrid"")
import numpy as np
fig = plt.figure()
ax = plt.axes()

",matplotlib
output matplotlib figure to svg with text as text not curves,"When I use matplotlib.pyplot.savefig(""test.svg"", format=""svg"") to export the figure as SVG, then the resulting SVG file is huge.
This is caused by the fact that there are a lot of text annotations in my figure, and each text ends up as paths in the SVG.
I want my text to end up as text strings in SVG, and not paths. It gets too hard to interpret the output too, if the text strings are exported this way.
Is there a way to force matplotlib to output text as text, not curves?
Currently, I see these code fragments in my SVG file:
<path d="" M9.8125 72.9062 L55.9062 72.9062 L55.9062 64.5938 L19.6719
64.5938 L19.6719 43.0156 L54.3906 43.0156 L54.3906 34.7188 L19.6719
34.7188 L19.6719 8.29688 L56.7812 8.29688 L56.7812 0 L9.8125 0 z ""
id=""DejaVuSans-45"" />

","Matplotlibs SVG text rendering can be configured either in the matplotlibrc or in code.
From Customizing Matplotlib with style sheets and rcParams:
#svg.fonttype : 'path'         # How to handle SVG fonts:
#    'none': Assume fonts are installed on the machine where the SVG will be viewed.
#    'path': Embed characters as paths -- supported by most SVG renderers
#    'svgfont': Embed characters as SVG fonts -- supported only by Chrome,
#               Opera and Safari

This translates to the following code for neither embedding the font nor rendering the text as path:
import matplotlib.pyplot as plt
plt.rcParams['svg.fonttype'] = 'none'

",matplotlib
problem clearing errorbar artists from dynamic matplotlib figure,"My goal is to clear and redraw errorbar data on a matplotlib plot while reusing the same background and axes. This means I cannot just clear the figure. I can do this trivially with plot by storing and removing the artists, but once I start using errorbar, I run into problems.
As an example, the below script will generate a plot of random data, and provides a button that clears that data without clearing the figure and generates/displays new random data.
import mplcursors
import matplotlib.pyplot as plt
from matplotlib.widgets import Button

class RandomDataPlotter:
    def __init__(self):
        self.fig, self.ax = plt.subplots()
        self.fig.subplots_adjust(bottom=0.2)
        self.current_data = None
        self.current_errorbars = None
        self.cursor = None

        # Add a button to toggle data
        self.button_ax = self.fig.add_axes([0.7, 0.05, 0.1, 0.075])
        self.button = Button(self.button_ax, 'Toggle Data')
        self.button.on_clicked(self.toggle_data)

        self.plot_random_data()

    def plot_random_data(self):
        # Clear existing artists
        if self.current_data:
            self.current_data.remove()
        if self.current_errorbars:
            for artist in self.current_errorbars:
                artist.remove()
        if self.cursor:
            self.cursor.remove()

        # Generate random data
        x = np.linspace(0, 10, 100)
        y = np.random.rand(100)
        y_err = np.random.rand(100) * 0.1

        # Plot data with error bars
        self.current_data, _, self.current_errorbars = self.ax.errorbar(x, y, yerr=y_err, fmt='o', color='blue')

        # Attach hover cursor
        self.cursor = mplcursors.cursor(self.ax, hover=True)
        self.cursor.connect(""add"", lambda sel: sel.annotation.set_text(f""x: {sel.target[0]:.2f}\ny: {sel.target[1]:.2f}""))

        # Redraw the canvas
        self.ax.relim()
        self.ax.autoscale_view()
        self.fig.canvas.draw_idle()

    def toggle_data(self, event):
        self.plot_random_data()

if __name__ == ""__main__"":
    plotter = RandomDataPlotter()
    plt.show()

This works on the first iteration/button press, however, something happens after that which generates the following error:
AttributeError: 'NoneType' object has no attribute 'canvas'
Traceback (most recent call last):
  File ""/Users/atom/hemanpro/HeMan/.venv/lib/python3.13/site-packages/matplotlib/cbook.py"", line 361, in process
    func(*args, **kwargs)
    ~~~~^^^^^^^^^^^^^^^^^
  File ""/Users/atom/hemanpro/HeMan/.venv/lib/python3.13/site-packages/matplotlib/widgets.py"", line 244, in <lambda>
    return self._observers.connect('clicked', lambda event: func(event))
                                                            ~~~~^^^^^^^
  File ""/Users/atom/hemanpro/HeMan/test_files/test2.py"", line 49, in toggle_data
    self.plot_random_data()
    ~~~~~~~~~~~~~~~~~~~~~^^
  File ""/Users/atom/hemanpro/HeMan/test_files/test2.py"", line 40, in plot_random_data
    self.cursor = mplcursors.cursor(self.ax, hover=True)
                  ~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^
  File ""/Users/atom/hemanpro/HeMan/.venv/lib/python3.13/site-packages/mplcursors/_mplcursors.py"", line 744, in cursor
    return Cursor(artists, **kwargs)
  File ""/Users/atom/hemanpro/HeMan/.venv/lib/python3.13/site-packages/mplcursors/_mplcursors.py"", line 264, in __init__
    for canvas in {artist.figure.canvas for artist in artists}]
                   ^^^^^^^^^^^^^^^^^^^^
AttributeError: 'NoneType' object has no attribute 'canvas'

My best guess at what's happening here is that there are leftover artists that no longer have a figure/canvas attached to them, which are generating an error when we try to attach the hover cursor. I have no idea how to clear this, however. Any assistance would be greatly appreciated.
","The errorbar method returns an ErrorbarContainer instance which is itself an artist containing the lines, etc.  Currently you are only removing the child artists and not the ErrorbarContainer, and I think it is that leftover container causing the error.  Calling the remove method on the container should remove itself and all its children, so you can just keep track of that, which makes things simpler.  However, there is currently a bug in Matplotlib and the method is not working as it should.  We can work around that by directly removing it from the axes' containers list.
import mplcursors
import matplotlib.pyplot as plt
from matplotlib.widgets import Button
import numpy as np

class RandomDataPlotter:
    def __init__(self):
        self.fig, self.ax = plt.subplots()
        self.fig.subplots_adjust(bottom=0.2)
        self.current_errorbar_container = None

        # Add a button to toggle data
        self.button_ax = self.fig.add_axes([0.7, 0.05, 0.1, 0.075])
        self.button = Button(self.button_ax, 'Toggle Data')
        self.button.on_clicked(self.toggle_data)

        self.plot_random_data()

    def plot_random_data(self):
        # Clear existing artists
        if self.current_errorbar_container:
            self.current_errorbar_container.remove()
            
            # Explicitly remove errorbar from containers list to workaround bug
            # https://github.com/matplotlib/matplotlib/issues/25274
            self.ax.containers.remove(self.current_errorbar_container)

        # Generate random data
        x = np.linspace(0, 10, 100)
        y = np.random.rand(100)
        y_err = np.random.rand(100) * 0.1

        # Plot data with error bars
        self.current_errorbar_container = self.ax.errorbar(x, y, yerr=y_err, fmt='o', color='blue')

        # Attach hover cursor
        self.cursor = mplcursors.cursor(self.ax, hover=True)
        self.cursor.connect(""add"", lambda sel: sel.annotation.set_text(f""x: {sel.target[0]:.2f}\ny: {sel.target[1]:.2f}""))

        # Redraw the canvas
        self.ax.relim()
        self.ax.autoscale_view()
        self.fig.canvas.draw_idle()

    def toggle_data(self, event):
        self.plot_random_data()

if __name__ == ""__main__"":
    plotter = RandomDataPlotter()
    plt.show()


",matplotlib
how to make srollable candelstick plot in python,"I have following candlestick plot. I want to make it scrollable so that I can see more details. The current plot is too long to see details. 
I have found examples for making a line plot scrollable at here:
Matplotlib: scrolling plot
However, updating a candlestick seems way more complicated than updating a line chart. The candlestick plot returns lines and patches. Can you help?
from pandas.io.data import get_data_yahoo
import matplotlib.pyplot as plt
from matplotlib import dates as mdates
from matplotlib import ticker as mticker
from matplotlib.finance import candlestick_ohlc
import datetime as dt
symbol = ""GOOG""

data = get_data_yahoo(symbol, start = '2011-9-01', end = '2015-10-23')
data.reset_index(inplace=True)
data['Date']=mdates.date2num(data['Date'].astype(dt.date))
fig = plt.figure()
ax1 = plt.subplot2grid((1,1),(0,0))
plt.title('How to make it scrollable')
plt.ylabel('Price')
ax1.xaxis.set_major_locator(mticker.MaxNLocator(6))
ax1.xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d'))

candlestick_ohlc(ax1,data.values,width=0.2)

","You can plot the whole plot, and then use the slider widget to modify the axes area.
I couldn't reproduce your data because I don't have the pandas.io.data library, so I modified the candlestick example from here, and added the slider.
import matplotlib.pyplot as plt
import datetime
from matplotlib.widgets import Slider
from matplotlib.finance import quotes_historical_yahoo_ohlc, candlestick_ohlc
from matplotlib.dates import DateFormatter, WeekdayLocator,\
    DayLocator, MONDAY

# (Year, month, day) tuples suffice as args for quotes_historical_yahoo
date1 = (2004, 2, 1)
date2 = (2004, 4, 12)

mondays = WeekdayLocator(MONDAY)        # major ticks on the mondays
alldays = DayLocator()              # minor ticks on the days
weekFormatter = DateFormatter('%b %d')  # e.g., Jan 12
dayFormatter = DateFormatter('%d')      # e.g., 12

quotes = quotes_historical_yahoo_ohlc('INTC', date1, date2)
if len(quotes) == 0:
    raise SystemExit

fig, ax = plt.subplots()
fig.subplots_adjust(bottom=0.2)
ax.xaxis.set_major_locator(mondays)
ax.xaxis.set_minor_locator(alldays)
ax.xaxis.set_major_formatter(weekFormatter)
#ax.xaxis.set_minor_formatter(dayFormatter)

#plot_day_summary(ax, quotes, ticksize=3)
candlestick_ohlc(ax, quotes, width=0.6)

ax.xaxis_date()
ax.autoscale_view()
plt.axis([datetime.date(*date1).toordinal(), datetime.date(*date1).toordinal()+10, 18.5, 22.5])
plt.setp(plt.gca().get_xticklabels(), rotation=45, horizontalalignment='right')


axcolor = 'lightgoldenrodyellow'
axpos = plt.axes([0.2, 0.05, 0.65, 0.03], axisbg=axcolor)


spos = Slider(axpos, 'Position', datetime.date(*date1).toordinal(), datetime.date(*date2).toordinal())

def update(val):
    pos = spos.val
    ax.axis([pos,pos+10, 18.5, 22.5])
    fig.canvas.draw_idle()

spos.on_changed(update)

plt.show()

I hardcoded some values of the axes sizes and positions, please be careful when adapting to your code.
Also same idea can be implemented to add a vertical scroll if needed.
",matplotlib
how to draw a filled arc in matplotlib,"In matplotlib, I would like draw an filled arc which looks like this:

The following code results in an unfilled line arc:
import matplotlib.patches as mpatches
import matplotlib.pyplot as plt

fg, ax = plt.subplots(1, 1)

pac = mpatches.Arc([0, -2.5], 5, 5, angle=0, theta1=45, theta2=135)
ax.add_patch(pac)

ax.axis([-2, 2, -2, 2])
ax.set_aspect(""equal"")
fg.canvas.draw()

The documentation says that filled arcs are not possible.
What would be the best way to draw one?
","@jeanrjc's solution almost gets you there, but it adds a completely unnecessary white triangle, which will hide other objects as well (see figure below, version 1).
This is a simpler approach, which only adds a polygon of the arc:
Basically we create a series of points (points) along the edge of the circle (from theta1 to theta2). This is already enough, as we can set the close flag in the Polygon constructor which will add the line from the last to the first point (creating a closed arc).
import matplotlib.patches as mpatches
import matplotlib.pyplot as plt
import numpy as np

def arc_patch(center, radius, theta1, theta2, ax=None, resolution=50, **kwargs):
    # make sure ax is not empty
    if ax is None:
        ax = plt.gca()
    # generate the points
    theta = np.linspace(np.radians(theta1), np.radians(theta2), resolution)
    points = np.vstack((radius*np.cos(theta) + center[0], 
                        radius*np.sin(theta) + center[1]))
    # build the polygon and add it to the axes
    poly = mpatches.Polygon(points.T, closed=True, **kwargs)
    ax.add_patch(poly)
    return poly

And then we apply it:
fig, ax = plt.subplots(1,2)

# @jeanrjc solution, which might hide other objects in your plot
ax[0].plot([-1,1],[1,-1], 'r', zorder = -10)
filled_arc((0.,0.3), 1, 90, 180, ax[0], 'blue')
ax[0].set_title('version 1')

# simpler approach, which really is just the arc
ax[1].plot([-1,1],[1,-1], 'r', zorder = -10)
arc_patch((0.,0.3), 1, 90, 180, ax=ax[1], fill=True, color='blue')
ax[1].set_title('version 2')

# axis settings
for a in ax:
    a.set_aspect('equal')
    a.set_xlim(-1.5, 1.5)
    a.set_ylim(-1.5, 1.5)

plt.show()

Result (version 2):

",matplotlib
plot an histogram with yaxis as percentage using funcformatter,"I have a list of data in which the numbers are between 1000 and 20 000.
data = [1000, 1000, 5000, 3000, 4000, 16000, 2000]

When I plot a histogram using the hist() function, the y-axis represents the number of occurrences of the values within a bin. Instead of the number of occurrences, I would like to have the percentage of occurrences. 

Code for the above plot:
f, ax = plt.subplots(1, 1, figsize=(10,5))
ax.hist(data, bins = len(list(set(data))))

I've been looking at this post which describes an example using FuncFormatter but I can't figure out how to adapt it to my problem. Some help and guidance would be welcome :)
EDIT: Main issue with the to_percent(y, position) function used by the FuncFormatter. The y corresponds to one given value on the y-axis I guess. I need to divide this value by the total number of elements which I apparently can' t pass to the function...
EDIT 2: Current solution I dislike because of the use of a global variable:
def to_percent(y, position):
    # Ignore the passed in position. This has the effect of scaling the default
    # tick locations.
    global n

    s = str(round(100 * y / n, 3))
    print (y)

    # The percent symbol needs escaping in latex
    if matplotlib.rcParams['text.usetex'] is True:
        return s + r'$\%$'
    else:
        return s + '%'

def plotting_hist(folder, output):
    global n

    data = list()
    # Do stuff to create data from folder

    n = len(data)
    f, ax = plt.subplots(1, 1, figsize=(10,5))
    ax.hist(data, bins = len(list(set(data))), rwidth = 1)

    formatter = FuncFormatter(to_percent)
    plt.gca().yaxis.set_major_formatter(formatter)

    plt.savefig(""{}.png"".format(output), dpi=500)

EDIT 3: Method with density = True

Actual desired output (method with global variable):

","Other answers seem utterly complicated. A histogram which shows the proportion instead of the absolute amount can easily produced by weighting the data with 1/n, where n is the number of datapoints.
Then a PercentFormatter can be used to show the proportion (e.g. 0.45) as percentage (45%).
import numpy as np
import matplotlib.pyplot as plt
from matplotlib.ticker import PercentFormatter

data = [1000, 1000, 5000, 3000, 4000, 16000, 2000]

plt.hist(data, weights=np.ones(len(data)) / len(data))

plt.gca().yaxis.set_major_formatter(PercentFormatter(1))
plt.show()


Here we see that three of the 7 values are in the first bin, i.e. 3/7=43%.
",matplotlib
prevent axes from cutting off dots in matplotlib scatter plots,"import matplotlib.pyplot as plt


x = [1, 2, 3, 4]
y = [0, 1, 7, 2]

plt.scatter(x, y, color='red')
plt.title('number of iterations')
plt.xlim([1, 4])
plt.ylim([1, 8])

If one was to plot this data, the dots on the axes are partially cut off. Is there a way to prevent this (i.e. can the dots be plotted on top of the axes)?
","Setting the clip_on attribute to False allows you to go beyond the axes, but by default the axes will be on top. For example, the script
x = [1, 2, 3, 4]
y = [0, 1, 7, 2]

plt.scatter(x, y, color=""red"", clip_on=False)
plt.title('number of iterations')
plt.xlim([1, 4])
plt.ylim([1, 8])

Yields the following.

Note that the axes ""cut through"" the dots. If you want the dots to go on top of the axes/labels, you need to change the default zorder. For example, the script
x = [1, 2, 3, 4]
y = [0, 1, 7, 2]

plt.scatter(x, y, color=""red"", clip_on=False, zorder = 10)
plt.title('number of iterations')
plt.xlim([1, 4])
plt.ylim([1, 8])

yields

Note: any zorder value 3 or greater will work here.
",matplotlib
density map heatmaps in matplotlib,"I have a list of coordinates:
y,x
445.92,483.156
78.273,321.512
417.311,204.304
62.047,235.216
87.24,532.1
150.863,378.184
79.981,474.14
258.894,87.74
56.496,222.336
85.105,454.176
80.408,411.672
90.656,433.568
378.027,441.296
433.964,290.6
453.606,317.648
383.578,115.432
128.232,312.496
116.276,93.536
94.072,222.336
52.226,327.308
321.663,187.56
392.972,279.008

I would like to plot a density map (or heat map) based on these points, using matplotlib. I am using pcolormesh and contourf. 
My problem is that pcolormesh is not having same size of the pitch: 

This is the code:
x, y = np.genfromtxt('pogba_t1314.csv', delimiter=',', unpack=True)

#print(x[1], y[1])
y = y[np.logical_not(np.isnan(y))]
x = x[np.logical_not(np.isnan(x))]
k = gaussian_kde(np.vstack([x, y]))
xi, yi = np.mgrid[x.min():x.max():x.size**0.5*1j,y.min():y.max():y.size**0.5*1j]
zi = k(np.vstack([xi.flatten(), yi.flatten()]))

fig = plt.figure(figsize=(9,10))
ax1 = fig.add_subplot(211)


ax1.pcolormesh(xi, yi, zi.reshape(xi.shape), alpha=0.5)

ax1.plot(y,x, ""o"")
ax1.set_xlim(0, 740)
ax1.set_ylim(515, 0)

#overlay soccer field
im = plt.imread('statszone_football_pitch.png')
ax1.imshow(im, extent=[0, 740, 0, 515], aspect='auto')


fig.savefig('pogba1516.png')

Here it is a link for the csv file: https://dl.dropboxusercontent.com/u/12348226/pogba_t1314.csv
","This will hopefully get you started on the right track, but I would definitely recommend reading the docs for pcolor and pcolormesh.
You have commented # Plot the density map using nearest-neighbor interpolation, but since Z is a 1D array, you don't have any 2D density data for a density map. Density maps are most easily created through the use of np.histogram2d as I'll show below using your data.
Z, xedges, yedges = np.histogram2d(x, y)

Z is now a 2D array that has information about the distribution of your x, y coordinates. This distribution can be plotted with pcolormesh like so
plt.pcolormesh(xedges, yedges, Z.T)

Sort of a ways to go before you obtain an image like the one you posted, but it should explain your error and help get you on the right track.
Update: For nicer, smoother density maps
Assuming you have two 1D arrays, x and y you can use a kernel density estimate to obtain much nicer heatmaps in the following way [reference],
from scipy.stats import gaussian_kde

k = gaussian_kde(np.vstack([x, y]))
xi, yi = np.mgrid[x.min():x.max():x.size**0.5*1j,y.min():y.max():y.size**0.5*1j]
zi = k(np.vstack([xi.flatten(), yi.flatten()]))

Now, you can plot the Gaussian KDE with either pcolormesh or contourf, depending on what kind of effect/aesthetics you're after:
import matplotlib.pyplot as plt

fig = plt.figure(figsize=(7,8))
ax1 = fig.add_subplot(211)
ax2 = fig.add_subplot(212)

# alpha=0.5 will make the plots semitransparent
ax1.pcolormesh(xi, yi, zi.reshape(xi.shape), alpha=0.5)
ax2.contourf(xi, yi, zi.reshape(xi.shape), alpha=0.5)

ax1.set_xlim(x.min(), x.max())
ax1.set_ylim(y.min(), y.max())
ax2.set_xlim(x.min(), x.max())
ax2.set_ylim(y.min(), y.max())

# you can also overlay your soccer field
im = plt.imread('soccerPitch.jpg')
ax1.imshow(im, extent=[x.min(), x.max(), y.min(), y.max()], aspect='auto')
ax2.imshow(im, extent=[x.min(), x.max(), y.min(), y.max()], aspect='auto')

I get this image:

",matplotlib
how to add clipboard support to matplotlib figures,"In MATLAB, there is a very convenient option to copy the current figure to the clipboard. Although Python/numpy/scipy/matplotlib is a great alternative to MATLAB, such an option is unfortunately missing.
Can this option easily be added to Matplotlib figures? Preferably, all MPL figures should automatically benefit from this functionality.
I'm using MPL's Qt4Agg backend, with PySide.
","Yes, it can. The idea is to replace the default plt.figure with a custom one (a technique known as monkey patching) that injects a keyboard handler for copying to the clipboard. The following code will allow you to copy any MPL figure to the clipboard by pressing Ctrl+C:
import io
import matplotlib.pyplot as plt
from PySide.QtGui import QApplication, QImage

def add_clipboard_to_figures():
    # use monkey-patching to replace the original plt.figure() function with
    # our own, which supports clipboard-copying
    oldfig = plt.figure

    def newfig(*args, **kwargs):
        fig = oldfig(*args, **kwargs)
        def clipboard_handler(event):
            if event.key == 'ctrl+c':
                # store the image in a buffer using savefig(), this has the
                # advantage of applying all the default savefig parameters
                # such as background color; those would be ignored if you simply
                # grab the canvas using Qt
                buf = io.BytesIO()
                fig.savefig(buf)
                QApplication.clipboard().setImage(QImage.fromData(buf.getvalue()))
                buf.close()

        fig.canvas.mpl_connect('key_press_event', clipboard_handler)
        return fig

    plt.figure = newfig

add_clipboard_to_figures()

Note that if you want to use from matplotlib.pyplot import * (e.g. in an interactive session), you need to do so after you've executed the above code, otherwise the figure you import into the default namespace will be the unpatched version.
",matplotlib
why do plot tick labels overlap only for imported csv data,"When I create a simple matpltlib plot with numpy arrays, the tick labels are well-behaved, chosen intelligently to not overlap and spaced to span the data range evenly.
However when I imported data into numpy arrays, the tick labels are a mess. It appears that it has added a tick label for each datapoint, rather than auto-generating a sensible scale.
Why is my data causing it to not be automatic?
How do I get MPL to do this automatically for real-world data with irregularly-spaced X/Y data?

""Normal"" behavior:

import matplotlib.pyplot as plt
import numpy as np
import numpy.random as rnd

x = np.array(  range(1000000)  )
y = rnd.rand(1,1000000)[0]

fig, ax = plt.subplots()
ax.plot(x,y)

Resulting plot, as expected:


Real-world data with non-equally-spaced X-axis data, imported from file.
Snippet of data file:
-1900.209922,-106.022
-1900.176409,-103.902
-1900.142897,-112.337
-1900.109384,-109.252
...

Plotting script:
import numpy as np
import matplotlib.pyplot as plt
import csv

# Read CSV file
with open(r""graph.csv"", encoding='utf-8-sig') as fp:
    reader = csv.reader(fp, delimiter="","", quotechar='""', )
    data_read = [row for row in reader]
#end with file

d = np.array(data_read).T   # transpose

x = d[0][0:10]
y = d[1][0:10]

fig, ax = plt.subplots()
ax.plot( x, y, ""."" )
fig.show()

I get messy tick labels:

Zoomed in, you can see it added ticks at exactly my data points:


If I change the X-data to a linear array, then it auto-ticks the x-axis, putting labels at intuitive locations (not at datapoints):
y = d[1][0:100]
x = range( len(y) )   # integer x-axis points

fig, ax = plt.subplots()
ax.plot( x, y, ""."" )
fig.show()


By the way, even if I load 20,000 data points, such that the y axis spans from -106 --> -88 (in case the values were too closely spaced), the y-axis labels still collide:
y[-1]
Out[31]: '-88.109'

y[0]
Out[32]: '-106.022'


Ultimately I'll be loading a large number of datapoints (200,000), so need this solved.
","This is an extremely common issue and it occurs when plotting string data rather than numeric. Essentially, the csv package is reading in the data as strings rather than numbers. You can fix this by following this answer, but you have two other options.

Use pandas.read_csv to read in the data properly. It also has plotting methods. It is the recommended way to handle numeric csv data.
Convert d to be a numpy array of floats as shown below.

# when creating d
d = np.array(data_read, dtype=float).T

# if d is already created
d = d.astype(float)


P.S. You can save figures using plt.savefig(""filename.png"") rather than taking screenshots of them.
",matplotlib
matplotlib share axes like in pltsubplots but using the mplfigure api,"I know how to make subplots with shared axes using the pyplot API:
from matplotlib import pyplot as plt

(fig, axes) = plt.subplots(3, 1, sharex=True)


But I can't replicate this effect using the matplotlib.figure.Figure API. I'm doing approximately the following. (Warning: I can't isolate the code because it's embedded in a whole Qt GUI, and if I take it out, I can't get the figure to display at all.)
from matplotlib.figure import Figure

n_axes = 3
fig = Figure()
axes = [fig.add_subplot(n_axes, 1, n+1)
        for n in range(n_axes)]
for ax in axes[:-1]:
    ax.sharex(axes[-1])

The ax.sharex command seems to have no effect.
For what it's worth, I switched to the plt.subplots method and everything seems to be working fine, but this smells like a bug or deficiency in matplotlib.
","The sharex function is working as intended. You can see this if you add some dummy data to your example. The x-axis of the individual subplots are not being hidden after setting the shared axis in the for-loop. You can do this manually by adding:
ax.get_xaxis().set_visible(False)

Here is the full code:
from matplotlib.figure import Figure

n_axes = 3
fig = Figure()

axes = [fig.add_subplot(n_axes, 1, n + 1) for n in range(n_axes)]
for ax in axes[:-1]:
    ax.sharex(axes[-1])
    ax.get_xaxis().set_visible(False)

I have also added some dummy data to visualize the result:
import numpy as np
from matplotlib.figure import Figure

n_axes = 3
fig = Figure()

axes = [fig.add_subplot(n_axes, 1, n + 1) for n in range(n_axes)]
for ax in axes[:-1]:
    ax.sharex(axes[-1])
    ax.get_xaxis().set_visible(False)

x = np.linspace(0, 10, 10)
data = [np.sin(x), np.cos(x), np.tan(x) / 10]
for ax, y, start in zip(axes, data,range(len(data))):
    ax.plot(x[start:], y[start:])
fig.savefig(""df.png"")


Hope this helps
",matplotlib
how do i remove an image overlay in matplotlib,"Using matplotlib and python, I have a grey-scale image of labeled objects, on which I want to draw a homogeneously coloured overlay image with a position and shape based on a changeable input parameter - an object identifier. Basically an outline and enhancement of on of the objects in the image.
I can generate the overlay, and re-generate it correctly (I think) every time the input value changes. But I don't know how to clear the previous overlay before drawing the new one. So, in the end, the grey-scale image is overlaid with multiple overlays.
This is what I have tried, and it doesn't work. 'overlay',and 'object_data' are defined and used in the calling function:
    def overlay_object(object_num):
        try:
            overlay
        except NameError:
            # nothing
            pass
        else:
            # remove old overlay
            for handle in overlay:
                handle.remove()

        # color the selected object
        componentMask = (object_data == object_num) 
        masked = ma.masked_where(componentMask == 0, componentMask)
        overlay = ax.imshow(masked, 'jet', interpolation='none', alpha=0.5)
        return overlay


Edit:
This is the creation of the grey-scale image in the main program:
fig, ax = plt.subplots()
ax.imshow(object_data, cmap='gray') 
ax.axis('off')

","If you’re trying to update overlay on a grayscale without accumulating overlays, you should use this approach:
import matplotlib.pyplot as plt
import numpy as np
import numpy.ma as ma

def create_interactive_overlay(object_data):
    """"""
    Creates a figure with a grayscale base image and functions to update overlays.
    
    Parameters:
    object_data : numpy.ndarray
        The labeled image data where each object has a unique integer value
    
    Returns:
    fig : matplotlib.figure.Figure
        The figure object
    update_overlay : function
        Function to call to update the overlay
    """"""
    # Create the figure and base image
    fig, ax = plt.subplots()
    ax.imshow(object_data, cmap='gray')
    ax.axis('off')
    
    # Initialize overlay as None
    overlay_artist = [None]  # Use list to allow modification in nested function
    
    def update_overlay(object_num):
        """"""
        Updates the overlay to highlight a specific object number.
        
        Parameters:
        object_num : int
            The object identifier to highlight
        """"""
        # Remove existing overlay if it exists
        if overlay_artist[0] is not None:
            overlay_artist[0].remove()
        
        # Create mask for selected object
        component_mask = (object_data == object_num)
        masked = ma.masked_where(component_mask == 0, component_mask)
        
        # Create new overlay
        overlay_artist[0] = ax.imshow(masked, cmap='jet', 
                                    interpolation='none', alpha=0.5)
        
        # Redraw the figure
        fig.canvas.draw_idle()
    
    return fig, update_overlay

# Example usage:
""""""
# Create sample data
object_data = np.zeros((100, 100))
object_data[20:40, 20:40] = 1
object_data[60:80, 60:80] = 2

# Create interactive figure
fig, update_overlay = create_interactive_overlay(object_data)

# Update overlay for different objects
update_overlay(1)  # Highlight object 1
plt.pause(1)  # Pause to see the change
update_overlay(2)  # Highlight object 2

plt.show()
""""""

In the above solution, the overlay management is handled by:

Keeping track of the overlay artist using a list (to allow modification in the nested function)
Properly removing the previous overlay before creating a new one
Using draw_idle() to update the figure

The code is structured to separate the setup from the update functionality:

create_interactive_overlay handles the initial setup
update_overlay handles the dynamic updates

To use this in your code, you would do something like this:
fig, update_overlay = create_interactive_overlay(object_data)

# When you want to highlight a different object:
update_overlay(new_object_number)

Hope this helps
",matplotlib
add text with pdfpages  matplotlib,"Following this example of the official documentation, I can create a pdf file with the plots that I want in different pages. But I would like to add some text to the page(not inside the plot) and I've tried in this way without success:
with PdfPages('multipage_pdf.pdf') as pdf:
    fig = plt.figure(figsize=(11.69,8.27))
    x = df1.index
    y1 = df1[col1]
    y2 = df1[col2]
    plt.plot(x, y1, label=col1)
    plt.plot(x, y2, label=col2)
    plt.legend(loc='best')
    plt.grid(True)
    plt.title('Title')
    txt = 'this is an example'
    plt.text(1,1,txt)
    pdf.savefig()
    plt.close()

How can I show also the text this is an example?
Is it possible to create also a first page with only text?
Thanks in advance
","The text 'this is an example' is placed at position (1,1) in data coordinates. If your data range is different, it might be out of the plot. It would make sense to specify it in figure coordinates. Those range from 0 to 1, where 0,0 is the lower left corner and 1,1 is the upper right corner.
E.g.
plt.text(0.05,0.95,txt, transform=fig.transFigure, size=24)

This example 
import datetime
import numpy as np
from matplotlib.backends.backend_pdf import PdfPages
import matplotlib.pyplot as plt

with PdfPages('multipage_pdf.pdf') as pdf:
    fig = plt.figure(figsize=(11.69,8.27))
    plt.plot([1,2,3], [1,3,2], label=""col1"")
    plt.plot([1,2,3],  [2,1,3], label=""col2"")
    plt.legend(loc='best')
    plt.grid(True)
    plt.title('Title')
    txt = 'this is an example'
    plt.text(0.05,0.95,txt, transform=fig.transFigure, size=24)
    pdf.savefig()
    plt.close()

creates this plot

You cannot create an empty pdf page. But of course you can mimic one by creating a figure without content, or an empty figure with just text.
import datetime
import numpy as np
from matplotlib.backends.backend_pdf import PdfPages
import matplotlib.pyplot as plt

with PdfPages('multipage_pdf.pdf') as pdf:
    firstPage = plt.figure(figsize=(11.69,8.27))
    firstPage.clf()
    txt = 'This is the title page'
    firstPage.text(0.5,0.5,txt, transform=firstPage.transFigure, size=24, ha=""center"")
    pdf.savefig()
    plt.close()

    fig = plt.figure(figsize=(11.69,8.27))
    plt.plot([1,2,3], [1,3,2], label=""col1"")
    plt.plot([1,2,3],  [2,1,3], label=""col2"")
    plt.legend(loc='best')
    plt.grid(True)
    plt.title('Title')
    txt = 'this is an example'
    plt.text(0.05,0.95,txt, transform=fig.transFigure, size=24)
    pdf.savefig()
    plt.close()


",matplotlib
zlabel does not show up in 3d matplotlib scatter plot,"The z-label does not show up in my figure. What is wrong?
import matplotlib.pyplot as plt

fig = plt.figure()
ax = fig.add_subplot(111, projection='3d')
ax.set_xlabel(""x"")
ax.set_ylabel(""y"")
ax.set_zlabel(""z"")
plt.show()

Output
Neither ax.set_zlabel(""z"") nor ax.set(zlabel=""z"") works. The x- and y-labels work fine.
","That's a padding issue.

labelpadfloat The distance between the axis label and the tick labels.
Defaults to rcParams[""axes.labelpad""] (default: 4.0) = 4.

You can use matplotlib.axis.ZAxis.labelpad to adjust this value for the z-axis :
import matplotlib.pyplot as plt

fig = plt.figure()
ax = fig.add_subplot(111, projection=""3d"")
ax.set_xlabel(""x"")
ax.set_ylabel(""y"")
ax.set_zlabel(""StackOverflow"", rotation=90)
ax.zaxis.labelpad=-0.7 # <- change the value here
plt.show();

Output :

",matplotlib
how can i save a figure to pdf with a specific page size and padding,"I have generated a matplotlib figure that I want to save to a PDF. So far, this is straightforward.
import matplotlib.pyplot as plt
x = [1, 2, 3, 4]
y = [3, 5, 4, 7]
plt.scatter(x, y)
plt.savefig(
    ""example.pdf"",
    bbox_inches = ""tight""
)
plt.close()

However, I would like the figure to appear in the middle of a standard page and have some white space around the figure, rather than fill up the entire page. How can I tweak the above to achieve this?
","Another option is to use constrained layout and set a rectangle that you want the all the plot elements to be contained in.
import matplotlib.pyplot as plt

x = [1, 2, 3, 4]
y = [3, 5, 4, 7]

fig, ax = plt.subplots(figsize=(8.3, 11.7), layout='constrained')
fig.get_layout_engine().set(rect=(0.15, 0.3, 0.7, 0.4))  # left, bottom, width, height in fractions of the figure

ax.scatter(x, y)

fig.savefig('example.pdf')

plt.close()


",matplotlib
how to graph learning curve of logistic regression,"I am running a Logistic Regression and would like to plot the Learning Curve of this to get a feel for the data. Here is my code thus far:
 from sklearn import metrics,preprocessing,cross_validation
  from sklearn.feature_extraction.text import TfidfVectorizer
  import sklearn.linear_model as lm
  import pandas as p
  loadData = lambda f: np.genfromtxt(open(f,'r'), delimiter=' ')

  print ""loading data..""
  traindata = list(np.array(p.read_table('train.tsv'))[:,2])
  testdata = list(np.array(p.read_table('test.tsv'))[:,2])
  y = np.array(p.read_table('train.tsv'))[:,-1]

  tfv = TfidfVectorizer(min_df=3,  max_features=None, strip_accents='unicode',  
        analyzer='word',token_pattern=r'\w{1,}',ngram_range=(1, 2), use_idf=1,smooth_idf=1,sublinear_tf=1)

  rd = lm.LogisticRegression(penalty='l2', dual=True, tol=0.0001, 
                             C=1, fit_intercept=True, intercept_scaling=1.0, 
                             class_weight=None, random_state=None)

  X_all = traindata + testdata
  lentrain = len(traindata)

  print ""fitting pipeline""
  tfv.fit(X_all)
  print ""transforming data""
  X_all = tfv.transform(X_all)

  X = X_all[:lentrain]
  X_test = X_all[lentrain:]

  print ""20 Fold CV Score: "", np.mean(cross_validation.cross_val_score(rd, X, y, cv=20, scoring='roc_auc'))

  print ""training on full data""
  rd.fit(X,y)
  pred = rd.predict_proba(X_test)[:,1]
  testfile = p.read_csv('test.tsv', sep=""\t"", na_values=['?'], index_col=1)
  pred_df = p.DataFrame(pred, index=testfile.index, columns=['label'])
  pred_df.to_csv('benchmark.csv')
  print ""submission file created..""

What I would like to create is something like this, so I can have a better understanding of what is going on:

How can I do this?
","Not quite as general as it should be, but it'll do the job with a little fiddling on your end.
from matplotlib import pyplot as plt
from sklearn import metrics
import numpy as np

def data_size_response(model,trX,teX,trY,teY,score_func,prob=True,n_subsets=20):

    train_errs,test_errs = [],[]
    subset_sizes = np.exp(np.linspace(3,np.log(trX.shape[0]),n_subsets)).astype(int)

    for m in subset_sizes:
        model.fit(trX[:m],trY[:m])
        if prob:
            train_err = score_func(trY[:m],model.predict_proba(trX[:m]))
            test_err = score_func(teY,model.predict_proba(teX))
        else:
            train_err = score_func(trY[:m],model.predict(trX[:m]))
            test_err = score_func(teY,model.predict(teX))
        print ""training error: %.3f test error: %.3f subset size: %.3f"" % (train_err,test_err,m)
        train_errs.append(train_err)
        test_errs.append(test_err)

    return subset_sizes,train_errs,test_errs

def plot_response(subset_sizes,train_errs,test_errs):

    plt.plot(subset_sizes,train_errs,lw=2)
    plt.plot(subset_sizes,test_errs,lw=2)
    plt.legend(['Training Error','Test Error'])
    plt.xscale('log')
    plt.xlabel('Dataset size')
    plt.ylabel('Error')
    plt.title('Model response to dataset size')
    plt.show()

model = # put your model here
score_func = # put your scoring function here
response = data_size_response(model,trX,teX,trY,teY,score_func,prob=True)
plot_response(*response)

The data_size_response function takes a model (in your case a instantiated LR model), a pre-split dataset (train/test X and Y arrays you can use the train_test_split function in sklearn to generate this), and a scoring function as input and iterates through your dataset training on n exponentially spaced subsets and returns the ""learning curve"". There is also a plotting function for visualizing this response.
I would have liked to use cross_val_score like your example but it would require modifying sklearn source to get back training scores in addition to the test scores it already provides. The prob argument is whether or not to use a predict_proba vs predict method on the model which is necessary for certain model/scoring function combinations e.g. roc_auc_score.
Example plot on a subset of the MNIST dataset:

Let me know if you have any questions!
",matplotlib
how to create candlestick chart using matplotlib only,"I have a problem with displaying vlines in the place where the body of the candle is very small.
Maybe someone has already solved a similar problem.
fig, axs = plt.subplots(1)

fig.set_size_inches(12.0, 6.0)

axs.vlines(df.index, df['Start_g'], df['End_g'], color='green', linestyle='-', lw=10)
axs.vlines(df.index, df['Start_r'], df['End_r'], color='red', linestyle='-', lw=10)

axs.vlines(df.index, df['Low_g'], df['High_g'], color='green', linestyle='-', lw=1)
axs.vlines(df.index, df['Low_r'], df['High_r'], color='red', linestyle='-', lw=1)

plt.show()


I tried setting dpi to 200 fig.set_dpi(200), it helps but the candles overlap each other

","To display the body of the candle I used a Rectangle
and to display Low and High I used Line2D
Input data should be pandas.DataFrame with columns Open High Low Close
df = df[['Open', 'High', 'Low', 'Close']]


import numpy as np
import matplotlib.patches as mpatches
from matplotlib.collections import PatchCollection
import matplotlib.lines as mlines
import matplotlib.pyplot as plt


def create_collection(df):
    
    l = len(df)

    grid = []
    height = []
    colors = []
    
    for i in range(l):
        if df.loc[i, 'Close'] > df.loc[i, 'Open']:
            grid.append([i, df.loc[i, 'Open']])
            height.append(df.loc[i, 'Close'] - df.loc[i, 'Open'])
            colors.append('green')
        elif df.loc[i, 'Close'] < df.loc[i, 'Open']:
            grid.append([i, df.loc[i, 'Close']])
            height.append(df.loc[i, 'Open'] - df.loc[i, 'Close'])
            colors.append('red')
    grid = np.array(grid)

    patches = []
    lines = []
    width = 0.5
    
    for i in range(l):
        
        rect = mpatches.Rectangle(grid[i]-[width/2, 0.0], width, height[i])
        patches.append(rect)
        line = mlines.Line2D([i, i], [df.loc[i, 'Low'], df.loc[i, 'High']], lw=0.75, color=colors[i])
        lines.append(line)

    collection = PatchCollection(patches, cmap=plt.cm.hsv)
    collection.set_facecolors([colors[i] for i in range(l)])
    collection.set_linewidth(1.0)
    collection.set_edgecolors([colors[i] for i in range(l)])
    
    return collection, lines

collection, lines = create_collection(df)

fig, axs = plt.subplots()
fig.set_figwidth(640)
fig.set_figheight(320)
fig.set_size_inches(12.0, 6.0)

axs.add_collection(collection)
[axs.add_line(lines[i]) for i in range(len(df))]

plt.axis('auto')
plt.show()

output here
",matplotlib
matplotlib pltshow isn39t showing graph,"My plotting code doesn't seem to be showing the graph (lines 12 to 59 are probably not breaking it, but I included them just in case - I had data that took a while to put into a sorted list).
I've messed around with it and tried different things but I think the main problem is that I don't understand what figure(), plt.show(), import matplotlib.pyplot as plt,  from pylab import * and some other lines actually mean. I don't know when I need to use them or why I need to use them.  
Could someone help me and explain how to draw an .svg of two lists and have it show at the end with details about why each line is included and when I put plt. in front and when I put ax. in front and when I don't put anything in front, etc? Sorry this will take so long to answer, but I really don't understand matplotlib or any of the examples on their website.
import matplotlib
matplotlib.use('SVG')
import matplotlib.pyplot as plt
import string, math
from pylab import *
from decimal import *

name = raw_input(""Enter the filename:\n"")

myfile = open(""datafiles/""+name+"".data"", 'r')

xData = []
yData = []
plots = [name]
mydata = myfile.readlines()

i = 0
N = len(mydata)
while (i < N):
    string = mydata[i]
    data = [str(x) for x in string.split("" "")]
    data=filter(lambda x: len(x)>0, data)
    xData.append(data[1])
    yData.append(data[2])
    i = i + 1

i = 0
while (i < N):
    if (float(xData[i]) <= 0):
        xData[i] = """"
        yData[i] = """"
    if (xData[i] == ""nan"" or xData[i] == ""-nan""):
        xData[i] = """"
        yData[i] = """"
    i = i + 1

xData=filter(lambda x: len(x)>0, xData)
yData=filter(lambda x: len(x)>0, yData)

N = len(xData)
i = 0

while (i < N):
    xData[i] = float(xData[i])
    yData[i] = float(yData[i])
    i = i + 1

j = 0
while (j < N):
    i = 0 
    while (i < (N-j-1)):
        if (xData[i]>xData[i+1]):
            a, b = xData[i], xData[i+1]
            xData[i+1], xData[i] = a, b

            a, b = yData[i], yData[i+1]
            yData[i+1], yData[i] = a, b
        i = i + 1
    j = j + 1

plot = plt.figure(1)
plt.plot(xData, yData)
plt.show(1)
plt.savefig(name)

","You are attempting to use a backend that will not produce graphics with plt.show(). Instead you need to use another backend such as WXAgg or QT4agg, the selection of which will depend on your system. See this information on Matplotlib's backends. Instead, you should use only plt.savefig('filename.svg') if you desire to have a file in the svg format. The resulting file will be in your working directory, you only need to open it after your script has finished.
To elaborate a bit to answer some of your other questions about not understanding what individual lines mean: 
plt.show() will produce an interactive plot on your screen, assuming you are using a backend (renderer) that supports plotting to your user interface.
import matplotlib.pyplot as plt simply imports the functions and classes from the pyplot library from the matplotlib package, and the as plt part is sort of like making a nickname to access those functions easier. For example, plt.show(), plt.figure, etc. instead of having to type out matplotlib.pyplot.show() every time. On the other hand, from pylab import * imports all of the functions without the prefix. In general, I would avoid using import * because it can be confusing to read back your code later. Also, pylab probably isn't something you need for the code you've shown here.
plt.figure() is the command that initializes your figure. In this case, since you used plot = plt.figure, you can then type plot.plot(xData, yData), because your variable plot now is part of the figure class. You would use ax for example if you had some additional axes, subplots or color bars on which you needed to perform some action.
I would really recommend going through the pyplot tutorial on the matplotlib website to give you a more thorough, but still relatively brief and simple introduction to using matplotlib.
",matplotlib
plotting a matplotlib pie chart using 2 columns,"I'm using this dataset and I want to plot a pie chart using matplotlib to show the percentage of males and females who have their loans approved i.e, Loan_Status == Y.
plot.pie() only takes x and labels for the data, so, I'm not able to figure how I should involve the gender column and where I should filter the Ys from the Loan_Status column.
I've tried this:
plt.pie(dataset[['Loan_Status', 'Gender']].value_counts().values, labels=['Male Approved', 'Male Denied', 'Female Approved', 'Female Denied'], colors=['#00db92', '#0049db'], autopct='%1.1f%%')
plt.title('Loan Approval by Gender')
plt.show()

Which shows this output

But I only want the approvals not the denials.
","If you only want the approvals, then you should only use the rows with ""Y"".
dataset[dataset[""Loan_Status""] == 'Y'][""Gender""].value_counts().values

",matplotlib
how can i get a certain number of evenly spaced points along the octagon perimeter,"I want to get the coordinates of a number of points that together form an octagon.
For a circle this is done easily as follows:
import numpy as np

n = 100
x = np.cos(np.linspace(0, 2 * np.pi, n))
y = np.sin(np.linspace(0, 2 * np.pi, n))

coordinates = list(zip(x, y))

By changing n I can increase/decrease the ""angularity"". Now I want to do the same for an octagon. I know that an octagon has 8 sides and the angle between each side 45 degrees.
Let's assume that the perimeter of the octagon is 30.72m. Each side has therefore a length of 3.79m.
perimeter = 30.72
n_sides = 8
angle = 45

How can I get n coordinates that represent this octagon?
Edit:
With the help of the answers of @lastchance and @mozway I am able to generate an octagon. My goal is to get evenly-spaced n coordinates from the perimeter of this octagon.
If n = 8 these coordinates correspond to the corners of the octagon, but I'm interested in cases where n > 8
","Let's use some math.

Each triangle in the polygon is isosceles. Assuming r the radius of the containing circle and a each side of the polygon we have:
n_sides = 8
perimeter = n_sides * a
a/2 = sin(pi/n_sides) / r # isosceles = 2 equal right triangles
perimeter = n_sides * 2 * sin(pi/n_sides) / r
r =  perimeter/(2 * n_sides * sin(pi/n_sides))

Thus, the coordinates are:
perimeter = 30.72
n_sides = 8

r = perimeter/(2 * n_sides * np.sin(np.pi/n_sides))
x = r * np.cos(np.linspace(0, 2 * np.pi, n_sides, endpoint=False))
y = r * np.sin(np.linspace(0, 2 * np.pi, n_sides, endpoint=False))

Note the endpoint=False in linspace.
As a graph:
ax = plt.subplot(aspect=1)
ax.plot(x, y, marker='o')

Output:

If you want one extra point to ""close"" the polygon (the last point being the same as the first point):
x = r * np.cos(np.linspace(0, 2 * np.pi, n_sides+1))
y = r * np.sin(np.linspace(0, 2 * np.pi, n_sides+1))


Now, let's use shapely to check that the calculation is correct:
from shapely.geometry import Polygon

Polygon(zip(x, y)).length # 30.72


Interpolating n points on the octagon/polygon
Now that we have a polygon, we can interpolate n points along its perimeter.
Let's generate a ""closed"" polygon (i.e. n_sides+1 point in which the last one is equal to the first), create a LineString and interpolate n values along it:
from shapely.geometry import LineString

perimeter = 30.72
n_sides = 8
n = 12

# compute the points of the polygon
r = perimeter/(2 * n_sides * np.sin(np.pi/n_sides))
x = r * np.cos(np.linspace(0, 2 * np.pi, n_sides+1))
y = r * np.sin(np.linspace(0, 2 * np.pi, n_sides+1))

# create a line string
# interpolate n points on the perimeter
line = LineString(zip(x, y))

coords = np.concatenate(list(line.interpolate(x).coords
                             for x in np.linspace(0, line.length, n,
                                                  endpoint=False)))

# plot
ax = plt.subplot(aspect=1)
ax.plot(x, y)
ax.plot(*coords.T, ls='', marker='o', label='n = 8')

Alternatively, performing the interpolation with numpy.interp:
perimeter = 30.72
n_sides = 8
n = 12

# compute the points of the polygon
r = perimeter/(2 * n_sides * np.sin(np.pi/n_sides))
x = r * np.cos(np.linspace(0, 2 * np.pi, n_sides+1))
y = r * np.sin(np.linspace(0, 2 * np.pi, n_sides+1))

# interpolate the n points along the perimeter
ref = np.linspace(0, 1, n_sides+1)*perimeter # distance to origin: ref
dist = np.linspace(0, 1, n+1)*perimeter      # distance to origin: n points
new_x = np.interp(dist, ref, x)   # interpolated x
new_y = np.interp(dist, ref, y)   # interpolated y

ax = plt.subplot(aspect=1)
ax.plot(x, y)
ax.plot(new_x, new_y, ls='', marker='o')

Output:

",matplotlib
editing the labels for a matplotlib colorbar,"I am creating an XY plot of two values and shading them based on timestamps. I am trying to edit the labels/ticks on the colorbar, but when I edit the labels I either lose the colorbar, the labels dont come through correctly or a mixture of both. Ideally, I want to create ~6 ticks based on the start and end time of the data I am plotting.
enter image description here
code I am currently using:
import pandas as pd
import matplotlib.pyplot as plt
import matplotlib.dates as mdates
import numpy as np

# Generate a range of timestamps
start_date = '2023-01-01'
end_date = '2023-01-31'
timestamps = pd.date_range(start=start_date, end=end_date, freq='h')  # hourly frequency

# Generate random numbers
np.random.seed(0)  # for reproducibility
random_numbers1 = np.random.rand(len(timestamps))
random_numbers2 = np.random.rand(len(timestamps))

# Create a DataFrame with the generated data
df = pd.DataFrame({
    'Timestamp': timestamps,
    'RandomValue1': random_numbers1,
    'RandomValue2': random_numbers2
})

# Convert the Timestamp column to datetime format
df['Timestamp'] = pd.to_datetime(df['Timestamp'])

# Create the plot
plt.figure(figsize=(10, 6))
sc = plt.scatter(df['RandomValue1'], df['RandomValue2'], c=df['Timestamp'], cmap='viridis', alpha=0.5)
cbar = plt.colorbar(sc, label='Time')
plt.xlabel('Random Values')
plt.ylabel('Random Values')
plt.title(f'Plot of Random Values shaded by Time')

# Convert timestamps to numerical values
num_timestamps = mdates.date2num(df['Timestamp'])

# Customize the colorbar ticks and labels to reflect timestamps
num_ticks = 6  # Number of ticks you want on the colorbar
ticks = pd.date_range(start=df['Timestamp'].min(), end=df['Timestamp'].max(), periods=num_ticks)
cbar.set_ticks(mdates.date2num(ticks))
cbar.set_ticklabels([tick.strftime('%Y-%m-%d %H:%M') for tick in ticks])

plt.show()

","Try out this updated code. This code creates a well-labelled colorbar with approximately 6 evenly spaced ticks corresponding to their timestamps. Let me know if it helps.
import pandas as pd
import matplotlib.pyplot as plt
import matplotlib.dates as mdates
import numpy as np

# Generate a range of timestamps
start_date = '2023-01-01'
end_date = '2023-01-31'
timestamps = pd.date_range(start=start_date, end=end_date, freq='h')  # hourly frequency

# Generate random numbers
np.random.seed(0)  # for reproducibility
random_numbers1 = np.random.rand(len(timestamps))
random_numbers2 = np.random.rand(len(timestamps))

# Create a DataFrame with the generated data
df = pd.DataFrame({
    'Timestamp': timestamps,
    'RandomValue1': random_numbers1,
    'RandomValue2': random_numbers2
})

# Convert the Timestamp column to datetime format
df['Timestamp'] = pd.to_datetime(df['Timestamp'])

# Create the plot
plt.figure(figsize=(10, 6))
sc = plt.scatter(df['RandomValue1'], df['RandomValue2'], c=mdates.date2num(df['Timestamp']), cmap='viridis', alpha=0.5)
cbar = plt.colorbar(sc, label='Time')
plt.xlabel('Random Values')
plt.ylabel('Random Values')
plt.title('Plot of Random Values shaded by Time')

# Customize the colorbar ticks and labels
num_ticks = 6  # Number of ticks you want on the colorbar
ticks = pd.date_range(start=df['Timestamp'].min(), end=df['Timestamp'].max(), periods=num_ticks)
tick_positions = mdates.date2num(ticks)  # Convert to numerical positions
cbar.set_ticks(tick_positions)
cbar.set_ticklabels([tick.strftime('%Y-%m-%d %H:%M') for tick in ticks])

plt.show()

How this code works:
The code generates 6 evenly spaced datetime ticks using pd.date_range. mdates.date2num() converts these ticks to numerical values and stores in the variable tick_positions. cbar.set_ticks(tick_positions) sets the colorbar ticks and cbar.set_ticklabels() applies the formatted labels.
Basically: The timestamps are turned into numbers so they can be used in the plot, and the colorbar is set up with evenly spaced markers that show the times in a clear, readable format. This makes it easy to match the colorbar to the data and understand what it represents.
",matplotlib
indexerror  descartes polygonpatch wtih shapely,"I used to use shapely to make a cirle and plot it on a previously populated plot. This used to work perfectly fine. Recently, I am getting an index error. I broke my code to even the simplest of operations and it cant even do the simplest of circles.
import descartes
import shapely.geometry as sg
import matplotlib.pyplot as plt

circle = sg.Point((0,0)).buffer(1)

# Plot the cricle
fig = plt.figure()
ax = fig.add_subplot(111)
patch = descartes.PolygonPatch(circle)
ax.add_patch(patch)
plt.show()

Below is the error I am getting now. I feel it might be a new version mismatch of something that could have happened. I tried uninstalling and re-installing the last known stable version and that didnt help either
---------------------------------------------------------------------------
IndexError                                Traceback (most recent call last)
Cell In[20], line 6
      4 fig = plt.figure()
      5 ax = fig.add_subplot(111)
----> 6 patch = descartes.PolygonPatch(circle)
      7 ax.add_patch(patch)
      8 plt.show()

File ~/env/lib/python3.8/site-packages/descartes/patch.py:87, in PolygonPatch(polygon, **kwargs)
     73 def PolygonPatch(polygon, **kwargs):
     74     """"""Constructs a matplotlib patch from a geometric object
     75 
     76     The `polygon` may be a Shapely or GeoJSON-like object with or without holes.
   (...)
     85 
     86     """"""
---> 87     return PathPatch(PolygonPath(polygon), **kwargs)

File ~/env/lib/python3.8/site-packages/descartes/patch.py:62, in PolygonPath(polygon)
     58     else:
     59         raise ValueError(
     60             ""A polygon or multi-polygon representation is required"")
---> 62 vertices = concatenate([
     63     concatenate([asarray(t.exterior)[:, :2]] +
     64                 [asarray(r)[:, :2] for r in t.interiors])
     65     for t in polygon])
     66 codes = concatenate([
     67     concatenate([coding(t.exterior)] +
     68                 [coding(r) for r in t.interiors]) for t in polygon])
     70 return Path(vertices, codes)

File ~/env/lib/python3.8/site-packages/descartes/patch.py:63, in <listcomp>(.0)
     58     else:
     59         raise ValueError(
     60             ""A polygon or multi-polygon representation is required"")
     62 vertices = concatenate([
---> 63     concatenate([asarray(t.exterior)[:, :2]] +
     64                 [asarray(r)[:, :2] for r in t.interiors])
     65     for t in polygon])
     66 codes = concatenate([
     67     concatenate([coding(t.exterior)] +
     68                 [coding(r) for r in t.interiors]) for t in polygon])
     70 return Path(vertices, codes)

IndexError: too many indices for array: array is 0-dimensional, but 2 were indexed

","So from what I could tell, this issue comes from a broken implementation of shapely within descartes.
My speculation is that shapely changed how it handles Polygon exteriors and descartes simply hasn't been updated.
I don't know if it is the best idea, but I edited my installation of descartes directly to fix this issue:

Navigate to your descartes installation and open patch.py.

At line 62 you should see this piece of code:
 vertices = concatenate([
 concatenate([asarray(t.exterior)[:, :2]] + [asarray(r)[:, :2] for r in t.interiors])
 for t in polygon])



Simply change t.exterior to t.exterior.coords. This hopefully should fix your issue.
    vertices = concatenate([
    concatenate([asarray(t.exterior.coords)[:, :2]] + [asarray(r)[:, :2] for r in t.interiors])
    for t in polygon])

I'm trying to find a way to provide the descartes devs with this feedback.
",matplotlib
editing specific labels in stacked bar chart with matplotlib,"I am following this tutorial/example to create stacked bar charts, however in my case I am representing various types of support tickets that are being received by specific technicians. As such, not all of the types are always present, so some of the bar values are zero and not all of the totals for each row add up to the same amount. I am trying to find a way to edit the labels such that they are either transparent or are simply not applied if a given bar width is zero, as right now there is a '0' label appearing between other categories. I am able to access the widths via something like the following:
    for rect in rects:
        if rect.get_width() == 0:
            #delete label, set opacity, etc

However, if I try to do something like rect.set_width(None) it errors out, which makes me think I can't just get rid of the label. It also does not appear that there is a way to set the opacity with matplotlib directly. Does anyone know of a way to remove the extraneous labels?
stacked bar chart showing extraneous lables
","In your loop, ax.bar_label contains a list of Annotation objects. You can loop over each of these objects to get their text with .get_text() and if it's '0' you use .set_text("""") to remove the label.
for i, (colname, color) in enumerate(zip(category_names, category_colors)):
    widths = data[:, i]
    starts = data_cum[:, i] - widths
    rects = ax.barh(labels, widths, left=starts, height=0.5,
                    label=colname, color=color)
    r, g, b, _ = color
    text_color = 'black' if r * g * b < 0.5 else 'darkgrey'
    
    all_labels = ax.bar_label(rects, label_type='center', color=text_color)
    for lab in all_labels:
        if lab.get_text() == '0': 
            lab.set_text("""")  

",matplotlib
how to reduce unnecessary white spaces in matplotlib subplot2grid,"I'm creating some plots with histograms and a color bar, but I'm struggling with the huge white gaps between subplots and I don't know how to reduce them. This an example of a more complex code:
import numpy as np
import matplotlib.pyplot as plt

ihist = np.array([
    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],
    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],
    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],
    [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],
    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],
    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],
    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],
    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],
    [0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],
    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],
])

vhist = np.array([
    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],
    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],
    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],
    [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],
    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],
    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],
    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],
    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],
    [0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],
    [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0],
])


mins = [1014.3484983803353, -168.92777938399416]
maxs = [5420.578637599565, 1229.7294914536292]
labels = ['x ($\\AA$)', 'y ($\\AA$)']

fig = plt.figure()
# SIAs
iax = plt.subplot2grid((9, 2), (1, 0), rowspan=8)
iax.set_xlabel(labels[0])
iax.set_xlim(mins[0], maxs[0])
iax.set_ylabel(labels[1])
iax.set_ylim(mins[1], maxs[1])
iax.imshow(ihist, origin=""lower"", extent=[mins[0], maxs[0], mins[1], maxs[1]])
# Vacancies
vax = plt.subplot2grid((9, 2), (1, 1), rowspan=8)
vax.set_xlabel(labels[0])
vax.set_xlim(mins[0], maxs[0])
vax.set_ylabel(labels[1])
vax.set_ylim(mins[1], maxs[1])
vax.yaxis.set_label_position(""right"")
vax.yaxis.tick_right()
vax_img = vax.imshow(vhist, origin=""lower"", extent=[mins[0], maxs[0], mins[1], maxs[1]])
# Color bar
cax = plt.subplot2grid((9, 2), (0, 0), colspan=2)
cbar = fig.colorbar(vax_img, cax=cax, orientation=""horizontal"")
cbar.set_label(""Counts per ion"")
cbar.ax.xaxis.set_ticks_position(""top"")
cbar.ax.xaxis.set_label_position(""top"")
plt.tight_layout()
plt.show()

And this is the output:

as you can see there are unnecessary white spaces between the color bar and the histograms, and between the histograms and the bottom of the figure. I want to remove them.
I think they are caused by things like plt.subplot2grid((9, 2), (1, 0), rowspan=8) but I did it that way to reduce the vertical size of the color bar.
Note: in the real code, limits are obtained on the fly, so histograms might have same hight and width or not.
","This is due to the aspect ratio setting of imshow. If you use:
...
iax.imshow(ihist, origin=""lower"", extent=[mins[0], maxs[0], mins[1], maxs[1]], aspect=""auto"")

...
vax_img = vax.imshow(vhist, origin=""lower"", extent=[mins[0], maxs[0], mins[1], maxs[1]], aspect=""auto"")

i.e., add aspect=""auto"", to your imshow calls, it should look as you expect. Note that you may want to switch to using add_gridspec and add_subplot, to allow finer degrees of control over the grid. E.g., an equivalent would be:
...
fig = plt.figure()
# SIAs

# add a two by two grid using the required height ratios
gs = fig.add_gridspec(
    2, 2, height_ratios=(1, 9)
)

iax = fig.add_subplot(gs[1, 0])
iax.set_xlabel(labels[0])
iax.set_xlim(mins[0], maxs[0])
iax.set_ylabel(labels[1])
iax.set_ylim(mins[1], maxs[1])
iax.imshow(ihist, origin=""lower"", extent=[mins[0], maxs[0], mins[1], maxs[1]], aspect=""auto"")

# Vacancies
vax = fig.add_subplot(gs[1, 1])
vax.set_xlabel(labels[0])
vax.set_xlim(mins[0], maxs[0])
vax.set_ylabel(labels[1])
vax.set_ylim(mins[1], maxs[1])
vax.yaxis.set_label_position(""right"")
vax.yaxis.tick_right()
vax_img = vax.imshow(vhist, origin=""lower"", extent=[mins[0], maxs[0], mins[1], maxs[1]], aspect=""auto"")

# Color bar
cax = fig.add_subplot(gs[0, :])
cbar = fig.colorbar(vax_img, cax=cax, orientation=""horizontal"")
cbar.set_label(""Counts per ion"")
cbar.ax.xaxis.set_ticks_position(""top"")
cbar.ax.xaxis.set_label_position(""top"")

# also try constrained layout rather than tight layout
fig.set_layout_engine(""constrained"")
plt.show()

",matplotlib
capture jupyter notebook or colab doesn39t work for storing cell output that display image or table pandas,"I have read docs about %%capture cap. It only work for text representation. I expect it stored everything what has been displayed into cell output; image, table, even html element.
And then I can load it into another cell with same identical output:
CELL_A
%%capture cap

import matplotlib.pyplot as plt

# Data
x = [1, 2]
y = [3, 4]

# Create a simple plot
plt.plot(x, y)

# Add labels to the axes
plt.xlabel('X-axis')
plt.ylabel('Y-axis')

# Add a title to the plot
plt.title('Simple Plot Example')

# Show the plot
plt.show()

# Save the captured output to a text file
with open('stdout.txt', 'w') as file:
  file.write(cap.stdout)

CELL_B
#@title Reloading CELL_A output

with open('stdout.txt', 'r') as file:
  cell_a_out = file.read()

display(cell_a_out)

It should be display matplotlib image. I know matplotlib provide save figure method to save the image. But I expect to save everything that has been displayed in the cell output, not only image. Therefore if the cell returning output table of pandas and matplotlib image it can be stored in the single file.
","As I discussed in my comments your post reflects several key misunderstandings.

What is being displayed by matplotlib with your example plotting code in your OP (your CELL_A). (It isn't an image!)
That you can use captured output in the same cell you call it (see your CELL_A in your OP). Here takluyver puts it succinctly as, ""%%capture won't define the variable until after the code inside it has run, so it's not meant to be used in the cell it's capturing.""

Here is how you can do what you say in the title and more using %%capture.
First cell (your CELL_A but removing the ill-informed idea of using the captured object before cell completely run):
CELL 1
%%capture cap

import matplotlib.pyplot as plt

# Data
x = [1, 2]
y = [3, 4]

# Create a simple plot
plt.plot(x, y)

# Add labels to the axes
plt.xlabel('X-axis')
plt.ylabel('Y-axis')

# Add a title to the plot
plt.title('Simple Plot Example')

# Show the plot
plt.show()

(By the way, plt.show() is generally no longer needed because when using the Python kernel, modern Jupyter sees a matplotlib plot object being made and displays it whether plt.show() is there or not. Though here because you want to use %%capture it's good to use because otherwise you may get cap including the last expression evaluated as part of the cap output, for example the plot title object.)
Note the object made by the code in the previous cell if you remove the %%capture line isn't an image at this point. It would be a matplotlib plot object that Jupyter would handle displaying.
Reshowing the captured output based on here in Cell #2:
CELL 2
#Reloading CELL 1 output
cap.show()

(Alternatively,cap() as the content of the cell will also work to later display the plot.)
Alternative CELL 2 option
from IPython.display import display
display(cap.outputs[0])

Okay, what if you did want to make the object generated in cell #1 an image and display it in the notebook?
First, we'll assign the plot object to a handle that we can call again make an it into an image and in the second step display that image in the notebook.
ALTERNATIVE CELL 1
%%capture cap

import matplotlib.pyplot as plt

# Data
x = [1, 2]
y = [3, 4]

# Create a simple plot
my_plot = plt.plot(x, y)

# Add labels to the axes
plt.xlabel('X-axis')
plt.ylabel('Y-axis')

# Add a title to the plot
plt.title('Simple Plot Example')

# Show the plot
plt.show()

Note the only difference is I assigned a handle to the plot object for ease in recalling subsequently.
Next based on here we can show the plot in another cell. (The %%capture cap swallowed the output and so it wasn't shown as output from the first cell already.)
ALTERNATIVE CELL 2
my_plot[0].figure

Let's save that matplotlib object as an image file:
ALTERNATIVE CELL 3
# make an image file of the matplotlib object
my_plot[0].figure.savefig(""my_plot_as_image.png"")

ALTERNATIVE CELL 4
#display the image of the plot made
from IPython.display import Image
Image(""my_plot_as_image.png"")

Note that when the Jupyter .ipynb file gets saved it will encode the image file within the saved .ipynb as Base64 code. (You can even recover as a file that embedded image from within the code later if you need to, see here.)
If you wanted to combine it with a dataframe there are number of options, see 'related' below for some ideas, as well as dataframe_image if you wanted to do it with an image if the plot is stored as an image, too.
Then, you could use jupyter nbconvert commands to save HTML, see here. And extract the HTML you wanted for an alternative option to what you describe for your answer to get HTML of your items.
I like PDFs for reports and so I't probably combine images of the plot and dataframe into PDF using ReportLab or Pillow.
Finally...
Your title and original post also seems to reflect an XY problem. You seem to at the end of your post are trying to do this to make a file that is just the plot and dataframe. In the future research and ask about the goal you have in mind and not how you think you might be able to accomplish it. See why not to do that here.
Related:

answers to 'Combining Jupyter rich display and matplotlib charts'
answers to 'Do Not Print Plot'

",matplotlib
calculate the volume of 3d plot python,"A simple code that builds a 3D graph based on the coordinates of points:
import pandas as pd
import matplotlib.pyplot as plt

with open('data\\PS.txt', 'r') as f:
    df = pd.read_csv(f, sep="","")
    
x = df['x'].values.tolist()
y = df['y'].values.tolist()
z = df['z'].values.tolist()

fig = plt.figure()

ax = fig.add_subplot(111, projection='3d')
ax.plot_trisurf(x, y, z)

plt.show()  

Here is the 3D figure itself:
enter image description here
Now it is necessary to calculate the volume of this figure.
How do I calculate the volume of a figure?
File contents ""PS.txt"" it looks like this:
1,152691.129,213258.428,631.716
2,152691.316,213256.857,631.827
3,152692.729,213255.315,631.945
4,152695.035,213250.459,631.882
5,152696.787,213246.130,632.107
6,152698.298,213242.364,631.578
7,152699.839,213239.028,631.723
8,152700.217,213237.356,631.350
9,152701.883,213233.297,631.206
10,152702.981,213229.478,631.392
11,152705.247,213224.878,631.609
12,152706.485,213222.668,632.294
13,152708.542,213219.988,632.765
14,152708.552,213220.002,632.750
15,152711.060,213215.836,632.809
16,152713.617,213211.422,632.346
17,152718.564,213202.802,631.409
18,152722.731,213197.100,631.362
19,152726.272,213192.046,631.498
20,152730.236,213186.253,631.371
21,152734.179,213180.031,631.452
22,152737.690,213175.369,631.344
23,152741.310,213177.118,631.309
24,152748.978,213182.575,631.163
25,152756.270,213188.301,631.247
26,152759.305,213190.724,631.267
27,152762.942,213192.492,631.256
28,152765.987,213194.916,631.162
29,152769.216,213197.108,631.358
30,152771.065,213200.136,631.245
31,152769.791,213202.537,631.736
32,152767.962,213205.946,631.614
33,152764.967,213211.123,631.717
34,152762.501,213215.213,631.571
35,152758.903,213221.344,631.816
36,152757.514,213225.138,632.015
37,152754.682,213229.676,631.975
38,152751.918,213234.024,631.889
39,152748.804,213239.425,632.106
40,152745.263,213245.349,631.928
41,152741.336,213251.451,631.912
42,152737.827,213256.240,632.071
43,152733.422,213263.828,632.060
44,152729.597,213269.791,632.211
45,152727.207,213273.974,632.440
46,152725.795,213276.797,632.435
47,152725.042,213277.064,632.554
48,152723.422,213276.584,632.513
49,152715.066,213272.147,632.442
50,152709.221,213268.936,632.387
51,152702.480,213264.648,632.467
52,152697.043,213261.739,632.231
53,152693.678,213259.727,632.141
54,152718.231,213206.304,629.044
55,152720.573,213203.068,628.925
56,152723.331,213199.193,628.898
57,152723.927,213196.993,628.870
58,152728.193,213190.552,628.813
59,152731.767,213185.322,628.960
60,152735.617,213179.673,629.032
61,152737.550,213176.409,629.168
62,152740.349,213177.727,629.093
63,152747.789,213183.248,628.736
64,152755.001,213188.781,628.874
65,152758.970,213192.543,629.045
66,152761.357,213193.329,629.181
67,152766.041,213196.920,629.269
68,152767.944,213199.372,629.367
69,152768.401,213201.151,629.219
70,152764.862,213207.360,629.122
71,152761.754,213212.748,629.312
72,152757.478,213220.474,629.310
73,152753.623,213228.269,629.393
74,152748.167,213237.432,629.557
75,152742.455,213247.561,629.629
76,152737.519,213254.209,629.883
77,152733.522,213259.618,630.021
78,152732.286,213261.395,629.511
79,152728.106,213268.732,629.516
80,152726.549,213271.128,629.326
81,152723.805,213274.373,629.317
82,152721.984,213273.677,629.192
83,152716.138,213270.812,629.574
84,152710.821,213268.060,629.567
85,152706.132,213265.473,629.309
86,152701.330,213262.217,628.895
87,152697.388,213260.418,628.983
88,152694.490,213258.500,629.225
89,152693.393,213258.304,629.621
90,152694.345,213256.439,629.329
91,152696.949,213251.571,629.259
92,152698.927,213246.215,629.155
93,152700.346,213242.499,629.009
94,152702.244,213238.054,628.970
95,152703.983,213233.276,628.897
96,152705.018,213229.281,629.012
97,152706.526,213226.382,629.127
98,152707.929,213223.484,629.261
99,152711.127,213219.338,629.268
100,152713.425,213215.829,629.093
101,152716.183,213210.623,628.744
102,152699.816,213255.436,628.810
103,152703.999,213257.574,628.649
104,152708.446,213259.993,629.049
105,152715.256,213263.596,629.281
106,152719.561,213265.977,629.373
107,152724.048,213268.319,629.370
108,152726.984,213263.115,629.579
109,152724.089,213260.586,629.322
110,152718.138,213256.749,629.407
111,152713.111,213253.939,629.257
112,152709.817,213251.502,628.953
113,152707.199,213249.989,628.670
114,152702.574,213247.411,628.844
115,152705.577,213241.685,628.898
116,152710.906,213244.101,628.792
117,152715.875,213246.241,628.887
118,152717.636,213247.094,629.211
119,152725.472,213251.103,629.307
120,152728.123,213252.443,629.553
121,152731.916,213254.534,629.897
122,152736.446,213247.816,629.464
123,152732.745,213245.056,629.233
124,152727.080,213241.423,629.124
125,152721.365,213238.400,628.874
126,152714.975,213234.559,628.970
127,152708.530,213231.420,628.871
128,152712.677,213225.120,629.003
129,152716.666,213227.440,628.856
130,152722.213,213230.296,628.799
131,152726.407,213232.656,628.672
132,152731.916,213235.770,628.876
133,152736.523,213238.443,629.147
134,152740.674,213240.783,629.344
135,152744.868,213234.142,629.359
136,152740.977,213231.527,629.126
137,152735.601,213228.189,628.759
138,152730.823,213225.042,628.534
139,152726.882,213222.278,628.481
140,152722.476,213218.921,628.594
141,152718.191,213215.736,628.733
142,152722.108,213210.698,628.572
143,152727.304,213212.925,628.314
144,152732.755,213215.602,628.129
145,152738.899,213219.785,628.257
146,152741.104,213221.031,628.425
147,152745.450,213223.517,628.798
148,152749.876,213225.960,629.039
149,152753.427,213219.757,629.075
150,152748.345,213216.114,628.684
151,152743.630,213212.799,628.256
152,152737.131,213209.291,628.304
153,152730.671,213206.158,628.415
154,152726.464,213203.380,628.630
155,152729.335,213198.258,628.759
156,152733.621,213200.345,628.489
157,152738.154,213202.717,628.415
158,152744.284,213206.303,628.709
159,152748.297,213209.516,628.499
160,152752.498,213212.363,628.883
161,152754.904,213213.741,629.372
162,152757.685,213211.582,629.352
163,152758.353,213208.933,629.254
164,152760.346,213209.123,629.314
165,152760.941,213206.380,629.142
166,152760.128,213202.271,629.014
167,152757.335,213198.594,628.844
168,152755.759,213197.339,628.822
169,152763.289,213200.301,629.009
170,152759.572,213197.326,628.975
171,152756.591,213199.072,628.574
172,152759.201,213202.392,628.390
173,152759.462,213204.784,628.457
174,152756.864,213206.414,628.606
175,152757.359,213208.721,628.897
176,152756.466,213210.988,629.069
177,152751.499,213204.059,628.700
178,152746.819,213201.102,628.749
179,152741.889,213198.339,628.611
180,152739.042,213196.746,628.669
181,152735.780,213194.626,628.932
182,152731.677,213192.081,628.950
183,152735.933,213187.082,628.857
184,152740.588,213190.671,628.946
185,152745.251,213194.414,628.803
186,152749.876,213197.718,628.695
187,152753.870,213200.839,628.717
188,152754.703,213194.185,628.870
189,152748.420,213189.568,628.785
190,152740.765,213183.848,628.788
Thank you all.
","The first trick was reading your text file into a dataframe with x, y, and z columns (not a necessary step for finding volume). Since it's not a csv or a regularly delimited file, it doesn't read easily with pd.read_csv(). You could either clean up the raw file first to make it more easily readable or clean it up after reading it in. I chose the latter.
import numpy as np, pandas as pd
from scipy.spatial import ConvexHull

df = pd.read_csv('PS.txt', sep=' ', header=None).stack().str.split(',', expand=True)
df = pd.concat([df[[1,2,3]].rename(columns={1:'x', 2:'y', 3:'z'}),
                df[[4,5,6]].dropna().rename(columns={4:'x', 5:'y', 6:'z'})], ignore_index=True)

>>> df
             x           y        z
0   152691.129  213258.428  631.716
1   152691.316  213256.857  631.827
2   152692.729  213255.315  631.945
3   152695.035  213250.459  631.882
4   152696.787  213246.130  632.107
... ... ... ...
375 152753.870  213200.839  628.717
376 152754.703  213194.185  628.870
377 152748.420  213189.568  628.785
378 152740.765  213183.848  628.788
379 152691.129  213258.428  631.716
380 rows × 3 columns

For finding the volume of convex polyhedrons you can use scipy.spatial.ConvexHull.
volume = ConvexHull(df.values).volume

>>> volume
13977.540571063422

Alternatively, you could just extract the points to find the volume without using a dataframe.
raw_string = '1,152691.129,213258.428,631.716 2,152691.316,213256.857,631.827 3,152692.729,213255.315,631.945 4,152695.035,213250.459,631.882 5,152696.787,213246.130,632.107 6,152698.298,213242.364,631.578 7,152699.839,213239.028,631.723 8,152700.217,213237.356,631.350 9,152701.883,213233.297,631.206 10,152702.981,213229.478,631.392 11,152705.247,213224.878,631.609 12,152706.485,213222.668,632.294 13,152708.542,213219.988,632.765 14,152708.552,213220.002,632.750 15,152711.060,213215.836,632.809 16,152713.617,213211.422,632.346 17,152718.564,213202.802,631.409 18,152722.731,213197.100,631.362 19,152726.272,213192.046,631.498 20,152730.236,213186.253,631.371 21,152734.179,213180.031,631.452 22,152737.690,213175.369,631.344 23,152741.310,213177.118,631.309 24,152748.978,213182.575,631.163 25,152756.270,213188.301,631.247 26,152759.305,213190.724,631.267 27,152762.942,213192.492,631.256 28,152765.987,213194.916,631.162 29,152769.216,213197.108,631.358 30,152771.065,213200.136,631.245 31,152769.791,213202.537,631.736 32,152767.962,213205.946,631.614 33,152764.967,213211.123,631.717 34,152762.501,213215.213,631.571 35,152758.903,213221.344,631.816 36,152757.514,213225.138,632.015 37,152754.682,213229.676,631.975 38,152751.918,213234.024,631.889 39,152748.804,213239.425,632.106 40,152745.263,213245.349,631.928 41,152741.336,213251.451,631.912 42,152737.827,213256.240,632.071 43,152733.422,213263.828,632.060 44,152729.597,213269.791,632.211 45,152727.207,213273.974,632.440 46,152725.795,213276.797,632.435 47,152725.042,213277.064,632.554 48,152723.422,213276.584,632.513 49,152715.066,213272.147,632.442 50,152709.221,213268.936,632.387 51,152702.480,213264.648,632.467 52,152697.043,213261.739,632.231 53,152693.678,213259.727,632.141 54,152718.231,213206.304,629.044 55,152720.573,213203.068,628.925 56,152723.331,213199.193,628.898 57,152723.927,213196.993,628.870 58,152728.193,213190.552,628.813 59,152731.767,213185.322,628.960 60,152735.617,213179.673,629.032 61,152737.550,213176.409,629.168 62,152740.349,213177.727,629.093 63,152747.789,213183.248,628.736 64,152755.001,213188.781,628.874 65,152758.970,213192.543,629.045 66,152761.357,213193.329,629.181 67,152766.041,213196.920,629.269 68,152767.944,213199.372,629.367 69,152768.401,213201.151,629.219 70,152764.862,213207.360,629.122 71,152761.754,213212.748,629.312 72,152757.478,213220.474,629.310 73,152753.623,213228.269,629.393 74,152748.167,213237.432,629.557 75,152742.455,213247.561,629.629 76,152737.519,213254.209,629.883 77,152733.522,213259.618,630.021 78,152732.286,213261.395,629.511 79,152728.106,213268.732,629.516 80,152726.549,213271.128,629.326 81,152723.805,213274.373,629.317 82,152721.984,213273.677,629.192 83,152716.138,213270.812,629.574 84,152710.821,213268.060,629.567 85,152706.132,213265.473,629.309 86,152701.330,213262.217,628.895 87,152697.388,213260.418,628.983 88,152694.490,213258.500,629.225 89,152693.393,213258.304,629.621 90,152694.345,213256.439,629.329 91,152696.949,213251.571,629.259 92,152698.927,213246.215,629.155 93,152700.346,213242.499,629.009 94,152702.244,213238.054,628.970 95,152703.983,213233.276,628.897 96,152705.018,213229.281,629.012 97,152706.526,213226.382,629.127 98,152707.929,213223.484,629.261 99,152711.127,213219.338,629.268 100,152713.425,213215.829,629.093 101,152716.183,213210.623,628.744 102,152699.816,213255.436,628.810 103,152703.999,213257.574,628.649 104,152708.446,213259.993,629.049 105,152715.256,213263.596,629.281 106,152719.561,213265.977,629.373 107,152724.048,213268.319,629.370 108,152726.984,213263.115,629.579 109,152724.089,213260.586,629.322 110,152718.138,213256.749,629.407 111,152713.111,213253.939,629.257 112,152709.817,213251.502,628.953 113,152707.199,213249.989,628.670 114,152702.574,213247.411,628.844 115,152705.577,213241.685,628.898 116,152710.906,213244.101,628.792 117,152715.875,213246.241,628.887 118,152717.636,213247.094,629.211 119,152725.472,213251.103,629.307 120,152728.123,213252.443,629.553 121,152731.916,213254.534,629.897 122,152736.446,213247.816,629.464 123,152732.745,213245.056,629.233 124,152727.080,213241.423,629.124 125,152721.365,213238.400,628.874 126,152714.975,213234.559,628.970 127,152708.530,213231.420,628.871 128,152712.677,213225.120,629.003 129,152716.666,213227.440,628.856 130,152722.213,213230.296,628.799 131,152726.407,213232.656,628.672 132,152731.916,213235.770,628.876 133,152736.523,213238.443,629.147 134,152740.674,213240.783,629.344 135,152744.868,213234.142,629.359 136,152740.977,213231.527,629.126 137,152735.601,213228.189,628.759 138,152730.823,213225.042,628.534 139,152726.882,213222.278,628.481 140,152722.476,213218.921,628.594 141,152718.191,213215.736,628.733 142,152722.108,213210.698,628.572 143,152727.304,213212.925,628.314 144,152732.755,213215.602,628.129 145,152738.899,213219.785,628.257 146,152741.104,213221.031,628.425 147,152745.450,213223.517,628.798 148,152749.876,213225.960,629.039 149,152753.427,213219.757,629.075 150,152748.345,213216.114,628.684 151,152743.630,213212.799,628.256 152,152737.131,213209.291,628.304 153,152730.671,213206.158,628.415 154,152726.464,213203.380,628.630 155,152729.335,213198.258,628.759 156,152733.621,213200.345,628.489 157,152738.154,213202.717,628.415 158,152744.284,213206.303,628.709 159,152748.297,213209.516,628.499 160,152752.498,213212.363,628.883 161,152754.904,213213.741,629.372 162,152757.685,213211.582,629.352 163,152758.353,213208.933,629.254 164,152760.346,213209.123,629.314 165,152760.941,213206.380,629.142 166,152760.128,213202.271,629.014 167,152757.335,213198.594,628.844 168,152755.759,213197.339,628.822 169,152763.289,213200.301,629.009 170,152759.572,213197.326,628.975 171,152756.591,213199.072,628.574 172,152759.201,213202.392,628.390 173,152759.462,213204.784,628.457 174,152756.864,213206.414,628.606 175,152757.359,213208.721,628.897 176,152756.466,213210.988,629.069 177,152751.499,213204.059,628.700 178,152746.819,213201.102,628.749 179,152741.889,213198.339,628.611 180,152739.042,213196.746,628.669 181,152735.780,213194.626,628.932 182,152731.677,213192.081,628.950 183,152735.933,213187.082,628.857 184,152740.588,213190.671,628.946 185,152745.251,213194.414,628.803 186,152749.876,213197.718,628.695 187,152753.870,213200.839,628.717 188,152754.703,213194.185,628.870 189,152748.420,213189.568,628.785 190,152740.765,213183.848,628.788'
points = []
for x in raw_string.split(' '):
    current_line = [float(x) for x in x.split(',')[1:]]
    if len(current_string)%3==0:
        if len(current_line)!=3:
            for n in range(1, int((len(current_line)/3)+1)):
                points.append(current_line[3*(n-1):3*n])
        else:
            points.append(current_line)
    else:
        continue
volume = ConvexHull(points).volume

>>> volume
13977.540571063422

",matplotlib
mplot3d fill_between extends over axis limits,"I have questions related to creating a simple lineplot in Python with mplot3D where the area under the plot is filled. I am using Python 2.7.5 on RedHatEnterprise 7.2, matplotlib 1.2.0 and numpy 1.7.2.
Using the code below, I am able to generate a line plot. This is displayed as expected with the beginning / end of the plot set by the limits of the imported data set.
I am then trying to fill the area between the line plot and -0.1 using the answer given by Bart from Plotting a series of 2D plots projected in 3D in a perspectival way. This works, however, the filled area is continued beyond the limits of the data set. This is also the case when running the example from the link.
This screen shot shows the plot generated with filled area extending beyond the set axis limits.


How do I achieve that the filled area is only the range of the data set or the axis limits whichever is smaller?
How do I add a legend for those plots onto the figure?

Code as follows:
from numpy import *
import matplotlib.pylab as plt
from mpl_toolkits.mplot3d import Axes3D

x,y = genfromtxt(""data.dat"",unpack=True)

fig = plt.figure()
ax = fig.add_subplot(111, projection='3d')

ax.add_collection3d(plt.fill_between(x,y,-0.1, color='orange', alpha=0.3,label=""filled plot""),1, zdir='y')

ax.plot(x,y,1,zdir=""y"",label=""line plot"")
ax.legend()

ax.set_xlim3d(852.353,852.359)
ax.set_zlim3d(-0.1,5)
ax.set_ylim3d(0,2)
ax.get_xaxis().get_major_formatter().set_useOffset(False)

plt.show()

","I don't know how to put fill_between working the way you want it to, but I can provide an alternative using a 3D polygon:
from numpy import *
import matplotlib.pylab as plt
from mpl_toolkits.mplot3d import Axes3D
from mpl_toolkits.mplot3d.art3d import Poly3DCollection # New import

#x,y = genfromtxt(""data.dat"",unpack=True)
# Generated some random data
w = 3
x,y = np.arange(100), np.random.randint(0,100+w,100)
y = np.array([y[i-w:i+w].mean() for i in range(3,100+w)])
z = np.zeros(x.shape)
fig = plt.figure()
ax = fig.add_subplot(111, projection='3d')

#ax.add_collection3d(plt.fill_between(x,y,-0.1, color='orange', alpha=0.3,label=""filled plot""),1, zdir='y')
verts = [(x[i],z[i],y[i]) for i in range(len(x))] + [(x.max(),0,0),(x.min(),0,0)]
ax.add_collection3d(Poly3DCollection([verts],color='orange')) # Add a polygon instead of fill_between

ax.plot(x,z,y,label=""line plot"")
ax.legend()
ax.set_ylim(-1,1)
plt.show()

The code above generates some random data. Builds vertices from it and plots a polygon with those vertices. This will give you the plot you wish (but does not use fill_between). The result is:

",matplotlib
how do i remove overflow along the zaxis for a 3d matplotlib surface,"I'm trying to graph a 3d mesh surface with matplotlib and constrain the limits of the graph.  The X and Y axes are correctly constrained, but there is overflow in the Z-Axis.
What am I missing?  Here's my code:
import matplotlib.pyplot as plt
import numpy as np
from mpl_toolkits import mplot3d
from mpl_toolkits.mplot3d import Axes3D

fig = plt.figure(figsize=(10,10))

x = np.linspace(-6,6,100)
y = np.linspace(-6,6,100)
X,Y = np.meshgrid(x,y)

def f(x,y):
    return x**2 + 3*y

Z = f(X,Y)


ax = plt.axes(projection = '3d')
ax.plot_surface(X,Y,Z,cmap='viridis')

ax.title.set_text(""z=x**2+3y"")
ax.set_xlabel('x')
ax.set_ylabel('y')
ax.set_zlabel('z')

ax.set_zlim3d(zmin=-3,zmax=5)
ax.set_xlim3d(xmin=-6,xmax=6)
ax.set_ylim3d(ymin=-6,ymax=6)

plt.show()

The graph:

Edit:
When I add clipping/min/max to the Z values, the graph is a little better, but it sets z values outside the bounds to the bounds themselves.  Both of the following suggestions do this.  Perhaps it's because I'm on a mac?
z_tmp = np.maximum(np.minimum(5,Z),-3)

z_temp = np.clip(Z, -3, 5, None)


","Your data is outside the axis boundaries. Try rotate the view and you will notice.
z = x**2 + 3*y

If you want to only show a defined area of the data you could add a max() min() limitation on the Z data to exclude the data outside your wanted limitations.
Z = f(X,Y)

z_tmp = np.maximum(np.minimum(5,Z),-3)


ax = plt.axes(projection = '3d')
ax.plot_surface(X,Y,z_tmp,cmap='viridis')

I'm not sure the matplotlib behaves as it should in your default case.
",matplotlib
trim data outside 3d plot in matplotlib,"I have a set of PDF that I need to plot for a certain section of the PDF domain. However, when I plot my lines on a 3d plot I get tails for each PDF,

Is there a clean way to not plot the tails that happen outside my plot limits?  I know I can change the data to NaNs to achieve the same effect but I want to do this in matplotlib.  Here is my current workaround code,
`# trim the data
y = np.ones(PDF_x.shape)*PDF_x
y[y>95]= np.nan
y[y<75]= np.nan


# plot the data
fig = plt.figure()
ax = fig.gca(projection='3d')
for i in range(PDF_capacity.shape[1]):
    ax.plot(life[i]*np.ones((PDF_x.shape)),y,PDF_capacity[:,i], label='parametric curve')

# set the axis limits
ax.set_ylim(75,95)

# add axis labels
ax.set_xlabel('charge cycles to failure point of 75% capacity')
ax.set_ylabel('capacity at 100 charge cycles')
ax.set_zlabel('probability')`

After trimming I can make the following plot,

","Masking the data with nan in the way you're doing it is a good and practical solution. 
Since matplotlib 3D plots are projections into 2D space, it would be hard to implement automatic clipping. While I do think it would be possible, I'm not convinced that it's worth the effort. First, because you would need to treat different kinds of plots differently, second, because at least in some cases it would probably turn out that masking the data is still the best choice. Now, doing a complex subclassing of the plotting objects just to do the same thing that can be manually done in one or two lines is probably overkill. 
My clear recommendation would therefore be to use the solution you already have. Especially since it does not seem to have any drawbacks so far. 
",matplotlib
how can i make a density plot with logscaled axes in matplotlib,"I want to plot a scalar density as a function of two variables x and y, which can potentially be scaled logarithmically. I essentially run simulations for each pair of x and y and want to report the data using a nice colormap. However, I run into the problem that I cannot make imshow scale the data correctly. While pcolormesh works reliably, it produces files that are orders of magnitudes larger and often cannot be displayed without artifacts (like thin white lines between data points).
Here's some code to reproduce the problem:
import numpy as np
import matplotlib.pyplot as plt
from matplotlib.image import NonUniformImage

# calculate axis positions
x = np.geomspace(1, 100, 5)
dx = np.sqrt(x[1] / x[0])  # half the difference between points in logspace
y = np.linspace(0, 1, 3)
dy = (y[1] - y[0]) / 2  # half the difference between points in linspace
extent = (x[0] / dx, x[-1] * dx, y[0] - dy, y[-1] + dy)

# get some random image data to plot
z = np.random.uniform(size=(len(x), len(y)))
# create figure axes
fig, ax = plt.subplots(ncols=3, figsize=(12, 3))

# use imshow to plot array
ax[0].imshow(z.T, origin=""lower"", aspect=""auto"", extent=extent)
ax[0].set_xscale(""log"")
ax[0].set_title(""imshow"")

# use NonUniformImage to plot array
im = NonUniformImage(ax[1], extent=extent)
im.set_data(x, y, z.T)
ax[1].add_image(im)
ax[1].set_xscale(""log"")
ax[1].set_title(""NonUniformImage"")

# use pcolormesh to plot array
x2 = np.geomspace(*extent[:2], 6)
y2 = np.linspace(*extent[2:], 4)
ax[2].pcolormesh(x2, y2, z.T)
ax[2].set_title(""pcolormesh"")

# set axis scales
for i in range(3):
    ax[i].set_xlim(*extent[:2])
    ax[i].set_ylim(*extent[2:])
    ax[i].set_xscale(""log"")

plt.show()

Running this example results in the following picture

Clearly, imshow is distorting the image, presumably because it assumes that the image contains data on a linearly scaled axis.
The second panel shows my attempt at using NonUniformImage, which gets things completely wrong for some reason.
The third panel shows what I want to see, albeit with using pcolormesh, which has the severe drawbacks I mentioned above.
Essentially, I just want to show a ""normal"" image with rectangular pixels of equal size on a log-scaled axis. I think this should be possible, but I was not able to achieve this. I also vaguely remember that the example shown in the first column used to work a few years back, but apparently this is not the case anymore. Any help with solving this would be much appreciated!
Note that this older answer does not work properly since it simply adds an axes with logarithmic ticks, so the user cannot reliably interact with the result (e.g., to change the ticks afterwards).
","(Updated answer)
If I understand correctly, currently imshow with log scale axes stretches a linear image. NonUniformImage doesn't seem to do such transformation. So, maybe the following works for you:
Using NonUniformImage with linear
import numpy as np
import matplotlib.pyplot as plt
from matplotlib.image import NonUniformImage

# calculate axis positions
x = np.geomspace(1, 100, 5)
dx = np.sqrt(x[1] / x[0])  # half the difference between points in logspace
y = np.linspace(0, 1, 3)
dy = (y[1] - y[0]) / 2  # half the difference between points in linspace
extent = (x[0] / dx, x[-1] * dx, y[0] - dy, y[-1] + dy)

# get some random image data to plot
z = np.random.uniform(size=(len(x), len(y)))
# create figure axes
fig, ax = plt.subplots(ncols=3, figsize=(12, 3))

# use imshow to plot array
ax[0].imshow(z.T, origin=""lower"", aspect=""auto"", extent=extent)
ax[0].set_title(""imshow"")

# use NonUniformImage to plot array
im = NonUniformImage(ax[1], extent=extent, interpolation='nearest')
# calculate linear ""cell centers"" for the given extent
x_lin = np.linspace(*extent[:2], 11)[1::2]
im.set_data(x_lin, y, z.T)
ax[1].add_image(im)
ax[1].set_xscale(""log"")
ax[1].set_title(""NonUniformImage"")

# use pcolormesh to plot array
x2 = np.geomspace(*extent[:2], 6)
y2 = np.linspace(*extent[2:], 4)
ax[2].pcolormesh(x2, y2, z.T)
ax[2].set_title(""pcolormesh"")

# set axis scales
for i in range(3):
    ax[i].set_xlim(*extent[:2])
    ax[i].set_ylim(*extent[2:])
    ax[i].set_xscale(""log"")

plt.show()


(Original answer)
Using imshow with linear axes and adding a dummy log scale ticks
Here is a solution using imshow with the given extent, without setting log scale. Create a second x-axis via ax.twiny() with log scale, and only show the x ticks of that new x-axis.
import matplotlib.pyplot as plt
import numpy as np

# calculate axis positions
x = np.geomspace(1, 100, 5)
dx = np.sqrt(x[1] / x[0])  # half the difference between points in logspace
y = np.linspace(0, 1, 3)
dy = (y[1] - y[0]) / 2  # half the difference between points in linspace
extent = (x[0] / dx, x[-1] * dx, y[0] - dy, y[-1] + dy)

# get some random image data to plot
z = np.random.uniform(size=(len(x), len(y)))
# create figure axes
fig, ax = plt.subplots(figsize=(6, 3))

# use imshow to plot the array
ax.imshow(z.T, origin=""lower"", aspect=""auto"", extent=extent)
ax.set_title(""imshow with twiny"")

# create a twin ax
ax2 = ax.twiny()
# set the twin ax to log
ax2.set_xscale(""log"")
# copy the limits of the original ax
ax2.set_xlim(ax.get_xlim())
# by default, twiny()'s ticks are put at the top, place them at the bottom
ax2.tick_params(axis='x', which='both', labelbottom=True, bottom=True, labeltop=False, top=False)
# hide the original ticks
ax.tick_params(axis='x', which='both', labelbottom=False, bottom=False)
plt.show()


",matplotlib
plotting heatmap with gridlines in matplotlib misses gridlines,"I am trying to plot a heatmap with gridlines. This is my code (adapted from this post):
# Plot a heatmap with gridlines

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from functional import seq

arr = np.random.randn(3, 20)

plt.tight_layout()
ax = plt.subplot(111)
ax.imshow(arr, cmap='viridis')

xr = ax.get_xlim()
yr = ax.get_ylim()
ax.set_xticks(np.arange(max(xr))-0.5, minor=True)
ax.set_yticks(np.arange(max(yr))-0.5, minor=True)
ax.grid(which='minor', snap=False, color='k', linestyle='-', linewidth=1)
ax.tick_params(which='major', bottom=False, left=False)
ax.tick_params(which='minor', bottom=False, left=False)
ax.set_xticklabels([])
ax.set_yticklabels([])
for spine in ax.spines.values():
    spine.set_visible(False)


plt.show()

I get the following plot (cropped to content):

There are no vertical grid lines after the 3rd, 8th, and third to last columns. As this script uses minor ticks to plot grid lines, I also thought to print the xticks and make the same plot without hiding the labels:
print('xticks:', np.arange(max(xr))-0.5)

xticks: [-0.5  0.5  1.5  2.5  3.5  4.5  5.5  6.5  7.5  8.5  9.5 10.5 11.5 12.5
 13.5 14.5 15.5 16.5 17.5 18.5]


which show that all the necessary ticks are there. What could be the issue here?
","I have no idea why this happens. Using the major grid instead fixes this issue:
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

arr = np.random.randn(3, 20)

plt.tight_layout()
ax = plt.subplot(111)
ax.imshow(arr, cmap='viridis')

xr = ax.get_xlim()
yr = ax.get_ylim()
ax.set_xticks(np.arange(max(xr))-0.5, minor=False)
ax.set_yticks(np.arange(max(yr))-0.5, minor=False)

ax.grid(which='major', snap=False, color='k', linestyle='-', linewidth=1)

ax.tick_params(which='major', bottom=False, left=False)
ax.tick_params(which='minor', bottom=False, left=False)

plt.show()

",matplotlib
adding matplotlib plot directly to word document without saving image to disk first,"I can add a plot generated by matpltlib to word file (using python-docx) if I save the plot as a .png file first using the following lines of code
plt.savefig(path_to_file)

doc.add_picture(path_to_file, width=Inches(3))

Is there a way I can just add the plot directlty to the word document without saving as an image first. Using
doc.add_picture(plt, width=Inches(3))

produces a module 'matplotlib.pyplot' has no attribute 'seek' error
","The python-docx function add_picture takes a path or a stream as first argument, and matplotlib can return a figure as a buffer, so presumably the following should work:
import matplotlib.pyplot as plt

from io import BytesIO

fig, ax = plt.subplots()
... 

# render the figure
fig.canvas.draw()

# save to buffer
buffer = BytesIO()
fig.savefig(buffer, format=""png"")

# hand buffer to python-docx
doc.add_picture(buffer)

",matplotlib
prevent matplotlib from redrawing while data are being added,"I was trying to plot some points (via geopandas) and discovered adding them gets progressively slower and slower up to the point where each takes a second and more at only few hundreds. This is harly usable and definitely abnormal.
My guess is, matplotlib redraws everything everytime new data is added to the figure, which is backed up by cProfile:
   Ordered by: cumulative time

   ncalls  tottime  percall  cumtime  percall filename:lineno(function)
   143915  479.433    0.003  479.602    0.003 collections.py:352(draw)
   137549    0.238    0.000  478.297    0.003 collections.py:1014(draw)
...

Searching for a solution to this problem proved hard. I have no idea why the library would redraw on every change when nothing is being displayed. There are some recommendations to use a static backend and to switch to non-interactive mode. Using corresponding configurations however does nothing for the performance, as the library uses static backend and non-interactive mode by default.
Stepping through the source I am seeing no point at which interactive mode would be even considered, or a call to a draw() could be conditionally intercepted.
Is there a way to make matplotlib postpone drawing until all data are added to the plot and are ready to be shown?
Here's the script to reproduce the issue. Let's see if it makes a difference.
from pandas import DataFrame
from geopandas import GeoDataFrame, points_from_xy
from matplotlib import pyplot

def plot_point(figure, geodata):
    geodata.plot(ax=figure.gca())

figure = pyplot.figure()

data = DataFrame({})
lat = 0
lon = 1
geodata = GeoDataFrame(data, geometry=points_from_xy([lon], [lat]))

for i in range(300):
    print(i)
    plot_point(figure, geodata)

","The answer is ""not out of the box"". There are no settings to disable redraws in the library.
Some backends might batch redraws as a side effect. The base implementation, which Agg and most others use, redraws whenever it can. The comments never claim to provide any protection from that, but leave it as an option to inheriting classes.
So it is possible to implement a custom backend with a more conservative redraw strategy and use that. In my case the following was enough:
from matplotlib.backend_bases import _Backend
from matplotlib.backends.backend_agg import FigureCanvasAgg, _BackendAgg

class FigureCanvasAggLazy(FigureCanvasAgg):
    def draw_idle(self, *args, **kwargs):
        pass # No intermediate draws needed if you are only saving to a file

@_Backend.export
class _BackendAggLazy(_BackendAgg):
    FigureCanvas = FigureCanvasAggLazy

",matplotlib
trying to time the quotinquot operator,"I have a question about how the in operator works internally. So to test it I tried to graph the time it takes to find a member of a list, in different parts of the list.
import time
import matplotlib.pyplot as plt

# list size
size = 100000
# number of points
n = 100000

# display for not having to count zeros
if size >= 1000000:
    new_size = str(int(size/1000000))+""M""
else:
    new_size = str(int(size/1000))+""K""

if n >= 1000000:
    new_n = str(int(n/10000000))+""M""
elif n >= 1000:
    new_n = str(int(n/1000))+""K""
else:
    new_n = n

lst = list(range(size))
result = []
for number in range(0,size+1,int(size/n)):
    start_time = time.time_ns()
    if number in lst:
        end_time = time.time_ns()
        total_time = (end_time-start_time)/1000000000 #convert ns to seconds
        result.append([total_time,number])
        print(number,total_time)

x_values, y_values = zip(*result)
plt.scatter(x_values, y_values, c='red', marker='o',s=5)
plt.xlabel('Time (sec)')
plt.ylabel('Number')
plt.title(f'List length: {new_size}\nNumber of points: {new_n}\n\nTime to find number in list')
plt.grid(True)
plt.show()

From what I saw, in works internally by calling iter which sequentially takes all elements of the iterable, starting from the first. So I would expect that searching further in a list takes more and more time.
The list has to be above a certain length so that python takes a long enough time to find the number that we can measure, but fiddling around with different list sizes and number of points I found out that the points cluster around vertical lines on multiples of 0.001s, which is very bizzare to me. I could see some horizontal lines forming due to how python works internally, but not vertical ones. I even used time.time_ns() to have even more time precision, but it still happens.
I mean, how can it take the same time to find 539 and 94,598 inside a [0,1, ... 100000] list if it always starts from 0?

","This test seems to suggest to me that searching for a number in a list takes longer and longer based on where the item falls in the list.
import timeit

setup = """"""
size = 1_000_000
size_minus_one = size - 1
data = list(range(size))
""""""

print(timeit.timeit(""0 in data"", setup=setup, number=1_000))
print(timeit.timeit(""size_minus_one in data"", setup=setup, number=1_000))

I assume some people are not satisfied with that answer as the vertical bars are not ""explained"". Let's do that as well.
The vertical bars are an artifact of the resolution of the time.time_ns(). Using time.perf_counter_ns() as recommended by @no-comment gives us the following result confirming that the time to find an item in a list is directly proportional to the location of the item in the list.

Via this code:
import time
import matplotlib.pyplot as plt

# list size
size = 100000
# number of points
n = 100000

# display for not having to count zeros
if size >= 1000000:
    new_size = str(int(size/1000000))+""M""
else:
    new_size = str(int(size/1000))+""K""

if n >= 1000000:
    new_n = str(int(n/10000000))+""M""
elif n >= 1000:
    new_n = str(int(n/1000))+""K""
else:
    new_n = n

clock = time.perf_counter_ns
lst = list(range(size))
result = []
for number in range(0,size+1, int(size/n)):
    start_time = clock()
    if number in lst:
        result.append([clock() - start_time, number])

x_values, y_values = zip(*result)
plt.scatter(x_values, y_values, c='red', marker='o',s=5)
plt.xlabel('Time')
plt.ylabel('Number')
plt.title(f'List length: {new_size}\nNumber of points: {new_n}\n\nTime to find number in list')
plt.grid(True)
plt.show()

",matplotlib
matplotlib  making labels for violin plots,"I usually make labels for bars in the following manner using parameter 'label' in the method 'bar'.
axes[0].bar(x, y, bar_width, label='abc')
axes[0].legend()

Now I'd like to plot violin plots and make label for each collection as follows, but it doesn't work since 'violinplot' doesn't have the parameter 'label'.
axes[0].violinplot(data1, label='abc1')
axes[1].violinplot(data2, label='abc2')

Can anyone help me out to make a label for each collection?
","As it was mentioned in comment, some plots in matplotlib don't support legends. Documentation still provides a simple way to add custom legends for them: http://matplotlib.org/users/legend_guide.html#proxy-legend-handles
Main idea : add 'fake' objects, which can be not shown in the plot, then use it to form a handles list for legend method.
    import random
    import numpy as np
    import matplotlib.pyplot as pl
    import matplotlib.patches as mpatches
    from itertools import repeat

    red_patch = mpatches.Patch(color='red')
    # 'fake' invisible object

    pos   = [1, 2, 4, 5, 7, 8]
    label = ['plot 1','plot2','ghi','jkl','mno','pqr']
    data  = [np.random.normal(size=100) for i in pos]

    fake_handles = repeat(red_patch, len(pos))

    pl.figure()
    ax = pl.subplot(111)
    pl.violinplot(data, pos, vert=False)
    ax.legend(fake_handles, label)
    pl.show()


",matplotlib
importerror no module named 39matplotlib39  using anaconda tensorflow environment,"I'm just trying to learn Tensorflow, but am totally new to Python, so I'm using Anaconda
I created a conda environment: 
$ conda create −n tensorflow python =3.5

Of course I activated my conda environment
$ source activate tensorflow

Then I played a bit around in Spyder, tried to plot a MNIST-digit (copy-paste code from my tutor which is tested several times), it includes of course 
import matplotlib.pyplot as plt
[...]
plt.plot(number)

but executing the Python file with bash gives me: 
(tensorflow) leon@leon-linux:~/ANNsCourse/Session1$ python helloWorld.py
Traceback (most recent call last):
  File ""helloWorld.py"", line 10, in <module>
    import matplotlib.pyplot as plt
ImportError: No module named 'matplotlib'

I'm quite confused right now, as the (tensorflow) in the bash obviously denotes that my conda tensorflow environment works (at least from my understanding). Also, from what I understood, conda should have matplotlib built in, right? And it should also load this in my conda tensorflow environment, right? This is what my tutor's slide said 

There is no need to install further packages like numpy or matplotlib, since Anaconda contains current versions of them already.'

and also what I was able to take from everything I Googled and StackOverflowed. Neither Googling nor StackOverflowing gave me any good answer (might also just be because I don't understand enough yet). 
My best guess would be that I still have to include matplotlib into my tensorflow conda environment, but this would be contradicting both my tutor & Google, while I also would not know how to do this.
edit: conda list gave me that matplotlib was not in my tensorflowenvironment, so I went
conda install matplotlib

I'm still afraid something is wrong with my conda tensorflow environment, shouldn't the matplotlib have been in there by default? It also told me: 
Package plan for installation in environment /home/leon/.conda/envs/tensorflow:

The following NEW packages will be INSTALLED:

cycler:           0.10.0-py35_0    
dbus:             1.10.10-0        
expat:            2.1.0-0          
fontconfig:       2.12.1-3         
freetype:         2.5.5-2          
glib:             2.50.2-1         
gst-plugins-base: 1.8.0-0          
gstreamer:        1.8.0-0          
icu:              54.1-0           
jpeg:             9b-0             
libffi:           3.2.1-1          
libgcc:           5.2.0-0          
libiconv:         1.14-0           
libpng:           1.6.27-0         
libxcb:           1.12-1           
libxml2:          2.9.4-0          
matplotlib:       2.0.0-np112py35_0
mkl:              2017.0.1-0       
numpy:            1.12.0-py35_0    
pcre:             8.39-1           
pyparsing:        2.1.4-py35_0     
pyqt:             5.6.0-py35_2     
python-dateutil:  2.6.0-py35_0     
pytz:             2016.10-py35_0   
qt:               5.6.2-3          
sip:              4.18-py35_0      
six:              1.10.0-py35_0    

Proceed ([y]/n)? y

Which tells me also numpy was missing? Can someone confirm this to be correct now, or is there something fishy with my conda? 
","You just created a conda environment named tensorflow and switched into it.  You haven't installed the tensorflow package or any of the default anaconda packages.
To do that, do
conda create -n tensorflow python=3.5 anaconda # install anaconda3 default packages
source activate tensorflow # switch into it
conda install -c conda-forge tensorflow # install tensorflow

",matplotlib
how to add labels to 3d plot,"I have the following code which generates a 3D plot. I am trying to label the plot lines, but they are not ending up where I expect them.
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
import numpy as np

# Data
names = [
    ""A"", ""B"", ""C"", ""D"", ""E"", ""F"",
    ""G"", ""H"", ""I"", ""J"", ""K"", ""L""
]
wins = [
    [0, 14, 20, 24, 29, 33, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39],
    [0, 7, 13, 17, 23, 27, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30],
    [0, 5, 8, 11, 15, 16, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19],
    [0, 7, 11, 17, 20, 25, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29],
    [0, 9, 14, 22, 29, 36, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42],
    [0, 6, 10, 16, 20, 24, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29],
    [0, 7, 13, 20, 26, 31, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34],
    [0, 10, 13, 18, 24, 29, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33, 33],
    [0, 5, 11, 13, 16, 21, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26],
    [0, 12, 15, 18, 21, 25, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30],
    [0, 9, 11, 12, 17, 20, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26],
    [0, 15, 20, 25, 27, 33, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37],
]
weeks = [f""Week{i+1}"" for i in range(18)]  # X-axis categories (nb of weeks)

# Convert names into numerical indices for the Y-axis
x = np.arange(len(weeks))      # X-axis 
y = np.arange(len(names))      # Y-axis 
wins_array = np.array(wins)    # wins accumulated per weeks and per names

# Create a 3D plot
fig = plt.figure(figsize=(18, 18))
ax = fig.add_subplot(111, projection='3d')

# Plot each line and add labels
for i, y_week in enumerate(y):
    ax.plot(x, wins_array[i], zs=y_week, zdir='x', label=weeks[i])
    # Add shaded area below the line
    ax.bar(x, wins_array[i], zs=y_week, zdir='x', alpha=0.1)
    # Add labels directly on the curves
    for j, my_label in enumerate(wins_array[i]):
        ax.text(i, my_label, y_week, f""{my_label}"", color=""red"", fontsize=8, ha=""center"")

# Customize labels
ax.set_zlabel('Wins')              # Z-axis is wins
ax.set_xticks(y)                   # X-ticks are names
ax.set_xticklabels(names, rotation=45)  # Name labels on X-axis
ax.set_yticks(x)                   # Y-ticks are weeks
ax.set_yticklabels(weeks)          # Week labels on Y-axis

plt.show()

This is the current result:

The result I am trying to achieve is something like this:

How do I fix my code to achieve my desired result?
","It is easier to separate the drawing and the text annotation:
# Plot each line 
for i, y_week in enumerate(y):
    ax.plot(x, wins_array[i], zs=y_week, zdir='x', label=weeks[i])
    # Add shaded area below the line
    ax.bar(x, wins_array[i], zs=y_week, zdir='x', alpha=0.1)

# Add labels directly on the curves
for i in y:
    for j, k in zip(x, wins_array[i]):
        ax.text(i, j, k, f""{k}"", color=""red"", fontsize=8, ha=""center"")


",matplotlib
seaborn boxplot add annotation labels for max values,"I would like to show some values in a Seaborn box plot such as Total# observations, Mean, Mix/Max value for each box plot series. Is there a way to show these in the plot?
import seaborn as sns
import matplotlib.pyplot as plt

sns.set_theme(style=""ticks"")

# Initialize the figure with a logarithmic x axis
f, ax = plt.subplots(figsize=(7, 6))
ax.set_xscale(""log"")

# Load the example planets dataset
planets = sns.load_dataset(""planets"")

# Plot the orbital period with horizontal boxes
sns.boxplot(
    planets, x=""distance"", y=""method"", hue=""method"",
    whis=[0, 100], width=.6, palette=""vlag""
)

# Add in points to show each observation
sns.stripplot(planets, x=""distance"", y=""method"", size=4, color="".3"")

# Tweak the visual presentation
ax.xaxis.grid(True)
ax.set(ylabel="""")
sns.despine(trim=True, left=True)

Example output:

","Just use matplotlib text function with the ""data coordinates"" for x and y:
mytext = 'Total: XX \n Mean: YY \n Min Value: AA \n Max Value: BB'
ax.text(x=15000, y='Radial Velocity', 
        s=mytext, style='italic', 
        color='white', bbox = {'facecolor': 'black'}, 
        fontsize=8, verticalalignment='center')
plt.show()


",matplotlib
how do i update a matplotlib plot in realtime and also have a repl,"I am in a situation where I have a primitive REPL with some simple commands. These commands generate data and put the data into a threadsafe queue. I then have a function that takes the data from the queue and plots it on a matplotlib plot.
What I would ultimately like to do is to be able to have a REPL command that creates a thread that generates data upon starting and puts the data into the queue. The data would then be plotted as it arrives from the queue into a matplotlib plot. I am having trouble understanding how to implement this.
My first idea was to just implement the REPL as while loop in the main thread and then have the function with matplotlib run in a separate thread. Unfortunately, matplotlib seems to require running in the main thread of the application. So I could not pursue that.
The next thing I tried was to put the REPL into its own thread and keep a matplotlib function in the main thread. Here's my main.py:
import threading
from queue import Queue
from typing import List

from repl import REPL
from stripchart import live_stripchart


def main():

    queue :Queue[List[float]] = Queue(maxsize = 0)
    quit: threading.Event = threading.Event()

    # A primitive REPL to control application. It runs
    # in a separate thread. And puts data into the queue.
    repl = REPL(queue, quit)
    repl.start()

    # live_stripchart is a function that plots data from the queue.
    # It runs in the main thread (because matplotlib seems to require this?)
    live_stripchart(queue, quit) # 
    repl.join()
    

if __name__ == ""__main__"":
    main()


The REPL consists of a simple while loop that runs in a separate thread. I feel like this is a problem. Is it OK to use input statements that take data from keyboard input in a separate thread? ¯\_(ツ)_/¯, but it seems like it works.
import re
import threading
from queue import Queue
from typing import List
import random

fake_data_cmd_regex :re.Pattern = re.compile(r'^fake_data\s+\-n\s*(\d+)\s*$')


class REPL(threading.Thread):

    def __init__(self, queue :Queue[List[float]], quit :threading.Event):
        self.queue :Queue[List[float]] = queue
        self.quit = quit
        super().__init__()


    def run(self):
        try:
            # Here's a primitive REPL to contol the application
            while True:
                command = input('> ')

                if command == 'help':
                    print(""quit"")
                    print(""    Exit the application."")
                    print(""fake_data -n <number>"")
                    print(""    Generate some data and put it in queue."")
                    continue

                if command == 'quit':
                    self.quit.set() # fire quit event to stop theads that check it.
                    break           # exit this loop, terminating this thread.

                match = fake_data_cmd_regex.match(command)
                if match:
                    n, = match.groups()
                    print(f""generating {n} fake data points..."")
                    self.queue.put([random.random() for _ in range(int(n))])
                    continue 

                if command == '':
                    continue
                
                print(""Unknown command. Type 'help' for help."")

        except KeyboardInterrupt:
            self.quit.set() # stop well monitor if it's running

    
        print(f""cli done"")  

Finally, I have the stripchart, this takes the data from the queue and plots it. This function runs in the main thread of the program.
import matplotlib.pyplot as plt
import threading
from queue import Queue, Empty
from typing import List

def live_stripchart(queue: Queue[List[float]], quit: threading.Event):

    plt.ion()
    fig, ax = plt.subplots()
    x, y = [], []
    i = 0

    line, = ax.plot([], [], 'b-')  # Initialize an empty plot line

    while not quit.is_set():
        try:
            # grab data from the queue
            data = queue.get(block=True, timeout=0.5)

            # Update x and y
            x.extend(range(i, i + len(data)))
            y.extend(data)
            i += len(data)

            # Keep only the most recent 100 points
            if len(x) > 100:
                x = x[-100:]
                y = y[-100:]

            # autoscale the view
            line.set_data(x, y)
            ax.relim()
            ax.autoscale_view()

            # actually plot it
            plt.draw()
            plt.pause(0.1)
        except Empty:
            plt.pause(0.1) # gracefully handle timeout

    print(""plot_ui done"", flush=True)
    plt.close(fig)


When I run this, it mostly does what I want. The matplotlib window appears and I can add data to it with my REPL command fake_data -n 100. The problem is that the icon for the matplotlib window keeps blinking in the taskbar (this is windows 10). This makes me think something is wrong.

Here's the commands I issue in the REPL...

I get even more problems when I try to continuously update the queue with more data generated from yet another thread. That's more involved to explain, so I am just posting a question about this more simplified situation to see if this is even the right path.
Is this approach correct?
Why is the icon for matplotlib blinking continuously?
Is it OK to implement a REPL as I have done in it's own thread?
","
Using IPython you can load your script, parallelize Matplotlib  (using get_ipython().run_line_magic('matplotlib', '')), and have a REPL where everything you defined in your script (e.g., t, pi and sin) is ready to be used interactively.
Here is a simple example, your more complex case may be treaed in the same way.
$ cat one.py
get_ipython().run_line_magic('matplotlib', '')
import matplotlib.pyplot as plt
from numpy import cos, linspace, pi, sin

t = linspace(0, 2*pi, 101)

plt.plot(t/pi, cos(t))
plt.xlabel('t/π')

$ ipython3-3.12 one.py -i
Python 3.12.7 (main, Oct 01 2024, 15:35:42) [GCC]
Type 'copyright', 'credits' or 'license' for more information
IPython 8.30.0 -- An enhanced Interactive Python. Type '?' for help.
Using matplotlib backend: qtagg

In [1]: plt.plot(t/pi, sin(t))
Out[1]: [<matplotlib.lines.Line2D at 0x7f1f91e32660>]

In [2]:                                                                            
Do you really want to exit ([y]/n)? 
$ 

",matplotlib
2d stacking and optimizing,"I have big rectangle (pallet), and would like to stack boxes (smaller rectangles) in way to optimize space usage, currently my problem is I'm not sure how I can rotate the last row/column properly to maximize the utilization of the available space.
I have tried:
Rect class
from typing import List, Tuple
import matplotlib.pyplot as plt
import matplotlib.patches as patches

class Rect:
    def __init__(self, corner: Tuple[float, float], size: Tuple[float, float]):
        self.length = max(size)
        self.width  = min(size)
        self.center = (corner[0] + self.length/2, corner[1] + self.width/2)
        self.size = (self.length, self.width)

    @property
    def min_corner(self) -> Tuple[float, float]:
        return (self.center[0] - self.size[0]/2, self.center[1] - self.size[1]/2)

    @property
    def max_corner(self) -> Tuple[float, float]:
        return (self.center[0] + self.size[0]/2, self.center[1] + self.size[1]/2)

    @property
    def area(self) -> float:
        return self.length * self.width

    def collides_with(self, other: 'Rect') -> bool:
        """"""Checks if this rectangle collides with another rectangle.""""""
        self_min_x, self_min_y = self.min_corner
        self_max_x, self_max_y = self.max_corner

        other_min_x, other_min_y = other.min_corner
        other_max_x, other_max_y = other.max_corner

        # Check for overlap
        return (
            (
                (self_max_x < other_max_x and self_max_y < other_max_y) and 
                (self_max_x > other_min_x and self_max_y > other_min_y)
            ) 
            or
            (
                (other_max_x < self_max_x and other_max_y < self_max_y) and 
                (other_max_x > self_min_x and other_max_y > self_min_y)
            )
        )

    def get_patch(self):
        """"""Returns a matplotlib Rectangle patch for visualization.""""""
        x, y = self.min_corner
        rect_width, rect_height = self.size
        return patches.Rectangle(
            (x, y),
            rect_width,
            rect_height,
            edgecolor='red',
            facecolor='lightgreen',
            linewidth=1
        )

Pallet Class
class Pallet:
    def __init__(self, size: Tuple[float, float]):
        self.size = size
        self.length = max(size)
        self.width  = min(size)
        self.rects: List[Rect] = []

    def add_rect(self, rect: Rect) -> bool:
        """"""Attempts to add a rectangle to the pallet. Returns True if successful, False otherwise.""""""
        if rect.area > self.length * self.width:
            return False

        # Check if the rectangle's corners are inside the pallet size
        max_corner = rect.max_corner
        min_corner = rect.min_corner
        x_max, y_max = max_corner
        x_min, y_min = min_corner
        if (not (0 <= x_max <= self.length and 0 <= y_max <= self.width)) or (x_min< 0 or y_min<0):
            print(""Out of Pallet"")
            return False

        for r in self.rects:
            if r.collides_with(rect):
                print(""Collision"")
                return False

        self.rects.append(rect)
        return True

    def fill_with_rects(self, rect_size: Tuple[float, float]):
        rect_length = rect_size[0]
        rect_width = rect_size[1]

        rows_x = int(self.length // rect_length)
        cols_y = int(self.width // rect_width)

        for i in range(rows_x):
            for j in range(cols_y):
                cx = rect_length * (i)
                cy = rect_width * (j)
                corner = (cx, cy)
                box = Rect(corner, (rect_length, rect_width))
                box_added = pallet.add_rect(box)

    def visualize(self):
        fig, ax = plt.subplots(figsize=(10, 8))
        ax.set_xlim(0, self.length)
        ax.set_ylim(0, self.width)
        ax.set_aspect('equal')
        for box in self.rects:
            box_patch = box.get_patch()
            ax.add_patch(box_patch)
        ax.set_xlabel(""Pallet Length"")
        ax.set_ylabel(""Pallet Width"")

        plt.grid(True)
        plt.show()

Test
if __name__==""__main__"":
    # Filling a pallet
    pallet = Pallet(size=(120, 100))
    pallet.fill_with_rects((32, 17))
    print(""Number of rectangles in pallet:"", len(pallet.rects))
    pallet.visualize()

Current result:

Desired result:
Additional box column in the end as there still space to contain more rectangles without collision.
","To expand on my comment above: rectangle packing / the knapsack problem is a well-known problem with many heuristics implemented in python, so unless there are significant additional constraints, there is no need to reinvent the wheel here.

import numpy as np
import matplotlib.pyplot as plt

from rectpack import newPacker

pallet = (120, 100)
rectangle = (32, 17)

packer = newPacker()
packer.add_bin(*pallet)
theoretical_maximum = np.prod(pallet) // np.prod(rectangle)
for _ in range(theoretical_maximum):
    packer.add_rect(*rectangle)
packer.pack()
rects = [((x, y), w, h) for (_, x, y, w, h, _) in packer.rect_list()]

print(""Theoretical maximum based on bin and rectangle areas:"", theoretical_maximum)
print(""Number of rectangles in pallet:"", len(rects))
# Theoretical maximum based on bin and rectangle areas: 22
# Number of rectangles in pallet: 19

fig, ax = plt.subplots()
for rect in rects:
    ax.add_patch(plt.Rectangle(*rect, color=np.random.rand(3)))
ax.add_patch(plt.Rectangle((0, 0), pallet[0], pallet[1], color=""black"", fill=False))
ax.autoscale_view()
plt.show()


",matplotlib
how to generate points on spherical surface making equal parts,"I would like to generate a sphere containing n euqal parts. For example, I want to divide my spherical surface into 36 X 36 parts. Therefore, it should contain 1296 equal parts in total. I do not have clue how to generate points in spherical (3D) space.
I want my plot looking like this but in place of line, I want only point (where two lines intersect).
I know only formulas mentioned below,
X = R * np.sin(PHI) * np.cos(THETA)
Y = R * np.sin(PHI) * np.sin(THETA)
Z = R * np.cos(PHI)

How would I generate points make equal parts in sphere?

","
to make phi and theta vary along [0,180] and [0,360]: use numpy.linspace.

to get all possible combinations of cos(phi) * cos(theta): use the outer product numpy.outer.

to split along equal angles, use z = sin(phi); to split into rectangles of equal area, split equally along the z-axis.


import numpy as np

def split_sphere(R = 1, horizontal_split = 36, vertical_split = 36, method=""equal_angles""):
    theta = np.linspace(0,360,horizontal_split+1)
    if method == ""equal_angles"":
        phi = np.linspace(0, 180, vertical_split+1)
        c = np.cos(phi)
        s = np.sin(phi)
    elif method == ""equal_area"":
        c = np.linspace(-1, 1, vertical_split+1)
        s = 1 - c**2
    else:
        raise(ValueError('method must be ""equal_angles"" or ""equal_area""'))
    x = R * np.outer(s, np.cos(theta))
    y = R * np.outer(s, np.sin(theta))
    z = R * np.outer(c, np.ones(horizontal_split+1))
    return x, y, z

def main():
    import matplotlib.pyplot as plt
    x,y,z = split_sphere()
    fig = plt.figure()
    ax = fig.add_subplot(projection='3d')
    ax.scatter(x,y,z)
    plt.show()

if __name__=='__main__':
    main()


",matplotlib
39invalid value encountered in double_scalars39 warning possibly numpy,"As I run my code I get these warnings, always in groups of four, sporadically. I have tried to locate the source by placing debug messages before and after certain statements to pin-point its origin.
Warning: invalid value encountered in double_scalars
Warning: invalid value encountered in double_scalars
Warning: invalid value encountered in double_scalars
Warning: invalid value encountered in double_scalars

Is this is a Numpy warning, and what is a double scalar?
From Numpy I use
min(), argmin(), mean() and random.randn()

I also use Matplotlib
","It looks like a floating-point calculation error. Check the numpy.seterr function to get more information about where it happens.
",matplotlib
move span center to click position not showing range when clicking in subplot with shown range,"I have created a 2 plot subplot. Selecting the range in both subplots works fine. Clicking into the upper subplot (the zoomed plot) shifts both views fine, but when clicking in the lower (total view) plot, the selected range (SpanSelector) region vanishes. What am I missing.
As a next step I intend to ass a cross-hair cursor to the top (zoomed) plot.
Attaches the code-example:
import sys
import numpy as np
from matplotlib.backends.backend_qt5agg import FigureCanvasQTAgg as FigureCanvas
import matplotlib.pyplot as plt
from matplotlib.widgets import MultiCursor
from PyQt5.QtWidgets import QMainWindow,QVBoxLayout
from PyQt5.QtWidgets import QApplication
from PyQt5 import QtCore, QtGui, QtWidgets
from matplotlib.ticker import FuncFormatter
from matplotlib.widgets import SpanSelector
import matplotlib.ticker as ticker

class MainWindow_code_serarch(object):

    def setup_code_serarch(self, MainWindow):
        MainWindow.setObjectName(""MainWindow"")
        MainWindow.resize(1024, 800)
        self.centralwidget = QtWidgets.QWidget(MainWindow)
        self.centralwidget.setObjectName(""centralwidget"")
        self.verticalLayoutWidget = QtWidgets.QWidget(self.centralwidget)
        self.verticalLayoutWidget.setGeometry(QtCore.QRect(0, 0, 1024, 800))
        self.verticalLayoutWidget.setObjectName(""verticalLayoutWidget"")
        self.verticalLayout = QtWidgets.QVBoxLayout(self.verticalLayoutWidget)
        self.verticalLayout.setContentsMargins(0,0,0,0)
        self.verticalLayout.setObjectName(""verticalLayout"")


        self.figure = plt.figure()
        self.canvas = FigureCanvas(self.figure)
        self.verticalLayout.addWidget(self.canvas)
        axes, axes2 = self.figure.subplots(nrows=2, sharex=True)
    
        datacount = 100000
    
        data_x = []
        data_y = []
    
        for i in range(1, datacount):
            data_x.append(i)
            if i % 250 <= 50:
                data_y.append(np.nan)
            else:
                data_y.append(np.sin(i/100)+0.05*np.sin(i))
    
        #** matplotlib subplot *********************************************
        self.figure.subplots_adjust(left=0.03, right=0.97, top=0.975, bottom=0.075, wspace=0, hspace=0.2)
    
        ax1, ax2 = self.figure.subplots(2, height_ratios=[0.8,0.2])
        ax1.grid()
        ax1.tick_params(axis = 'x', length=0)
        ax1.tick_params(axis = 'y', length=0)

        ax2.tick_params(axis = 'x', length=0)
        ax2.tick_params(axis = 'y', length=0)
        ax2.grid()
    
        def format_tick_labels(x, pos):
            return '{0:2.0e}'.format(x)

        ax2.xaxis.set_major_formatter(FuncFormatter(format_tick_labels))
        
        ax2.plot(data_x, data_y, linewidth = 0.5)
        ax2.set_xlim(0, datacount)
        ax2.set_ylim(-1.2, 1.2)
    
        self.line2, = ax1.plot([], [], linewidth=0.5)
    
        def onselect_span(xmin, xmax):
            print('onselect_span')
            indmin, indmax = np.searchsorted(data_x, (xmin, xmax))
        
            print(f'Span: select indmin = {indmin}, indmax = {indmax}')

            region_x = data_x[indmin:indmax]   
            region_y = data_y[indmin:indmax]   
        

            if len(region_x)>=2:
                self.line2.set_data(region_x, region_y)
                ax1.set_xlim(region_x[0], region_x[-1])     #select from region start till region end...
                ax1.set_ylim(-1.2,1.2)#region_y[0], region_y[-1])
                self.span2.extents = (region_x[0], region_x[-1])
                self.canvas.draw_idle()
                self.mouse_SpanSelected = True;    
            else: 
                print('onselect_span: region too small') 
            
            self.span1.clear()
            print('onselect_span end')
  
    
        self.span1 = SpanSelector(
            ax1,
            onselect_span,
            ""horizontal"",
            useblit=True,
            props=dict(alpha=0.3, facecolor=""tab:red""),
            interactive=True,
            drag_from_anywhere=True,
            grab_range = 3,
        )
    
        self.span2 = SpanSelector(
            ax2,
            onselect_span,
            ""horizontal"",
            useblit=True,
            props=dict(alpha=0.3, facecolor=""tab:red""),
            interactive=True,
            drag_from_anywhere=True,
            grab_range = 3,
        )
    
        def onclick(event):
            global ix
            ix = event.xdata
            print('onclick end')

                
        def onrelease(event):
            print('onrelease')
            global ix, ixrel
            ixrel = event.xdata

            if abs(ix-ixrel)<=2:
                print('Release: region too small')
                width_half = int ((self.line2._x[-1] - self.line2._x[0])/2)
                self.span2.extents = (ixrel - width_half, ixrel + width_half)
                onselect_span(ixrel - width_half, ixrel + width_half)
                self.span2.update()
            
            self.canvas.draw_idle()
            
            print('onrelease end')
        
        
        click_id = self.figure.canvas.mpl_connect('button_press_event', onclick)
        relaese_id = self.figure.canvas.mpl_connect('button_release_event', onrelease)

        self.span2.extents = (4000,15501)  #set selectred region
        onselect_span(4000,15501)

        self.canvas.draw()
    
        MainWindow.setCentralWidget(self.centralwidget)


if __name__ == ""__main__"":
    import sys
    app = QtWidgets.QApplication(sys.argv)
    MainWindow = QtWidgets.QMainWindow()
    ui = MainWindow_code_serarch()
    ui.setup_code_serarch(MainWindow)
    MainWindow.show()
    sys.exit(app.exec_())

Image: at start:
this is how it looks after starting the app

when I click in the upper graph the range shifts

when I click in the lower graph the range disappears => this is my problem

","The thing is that when you click, that is like selecting an empty span (I take that you are already aware of that, since you instrument the code with a lot of print that tell you exactly what happen, with what values).
And SpanSelector is removed when you select an empty span. And this occurs even before you had a chance to call your callbacks.
(I have a vague feeling, but very vague, I didn't take time to understand why you did all these things with a callback not only for select but also for click and release, that you are recoding what SpanSelector is already doing).
You can prevent that removal by playing with parameter minspan of SpanSelectors. Make it minspan=-1, and then they will never be removed. Sure, it is a aberration theoretically (default minspan=0 is supposed to be the least value that makes sense), but in your case, since you will then do what it takes to put it back if needed...
At any rate, it seems to be working like I think  you expect
So, my minimal reproducible answer :-) (it is only a copy of your question, with two additional minspan=-1)
import sys
import numpy as np
from matplotlib.backends.backend_qt5agg import FigureCanvasQTAgg as FigureCanvas
import matplotlib.pyplot as plt
from matplotlib.widgets import MultiCursor
from PyQt5.QtWidgets import QMainWindow,QVBoxLayout
from PyQt5.QtWidgets import QApplication
from PyQt5 import QtCore, QtGui, QtWidgets
from matplotlib.ticker import FuncFormatter
from matplotlib.widgets import SpanSelector
import matplotlib.ticker as ticker

class MainWindow_code_serarch(object):

    def setup_code_serarch(self, MainWindow):
        MainWindow.setObjectName(""MainWindow"")
        MainWindow.resize(1024, 800)
        self.centralwidget = QtWidgets.QWidget(MainWindow)
        self.centralwidget.setObjectName(""centralwidget"")
        self.verticalLayoutWidget = QtWidgets.QWidget(self.centralwidget)
        self.verticalLayoutWidget.setGeometry(QtCore.QRect(0, 0, 1024, 800))
        self.verticalLayoutWidget.setObjectName(""verticalLayoutWidget"")
        self.verticalLayout = QtWidgets.QVBoxLayout(self.verticalLayoutWidget)
        self.verticalLayout.setContentsMargins(0,0,0,0)
        self.verticalLayout.setObjectName(""verticalLayout"")


        self.figure = plt.figure()
        self.canvas = FigureCanvas(self.figure)
        self.verticalLayout.addWidget(self.canvas)
        axes, axes2 = self.figure.subplots(nrows=2, sharex=True)
    
        datacount = 100000
    
        data_x = []
        data_y = []
    
        for i in range(1, datacount):
            data_x.append(i)
            if i % 250 <= 50:
                data_y.append(np.nan)
            else:
                data_y.append(np.sin(i/100)+0.05*np.sin(i))
    
        #** matplotlib subplot *********************************************
        self.figure.subplots_adjust(left=0.03, right=0.97, top=0.975, bottom=0.075, wspace=0, hspace=0.2)
    
        ax1, ax2 = self.figure.subplots(2, height_ratios=[0.8,0.2])
        ax1.grid()
        ax1.tick_params(axis = 'x', length=0)
        ax1.tick_params(axis = 'y', length=0)

        ax2.tick_params(axis = 'x', length=0)
        ax2.tick_params(axis = 'y', length=0)
        ax2.grid()
    
        def format_tick_labels(x, pos):
            return '{0:2.0e}'.format(x)

        ax2.xaxis.set_major_formatter(FuncFormatter(format_tick_labels))
        
        ax2.plot(data_x, data_y, linewidth = 0.5)
        ax2.set_xlim(0, datacount)
        ax2.set_ylim(-1.2, 1.2)
    
        self.line2, = ax1.plot([], [], linewidth=0.5)
    
        def onselect_span(xmin, xmax):
            print('onselect_span')
            indmin, indmax = np.searchsorted(data_x, (xmin, xmax))
        
            print(f'Span: select indmin = {indmin}, indmax = {indmax}')

            region_x = data_x[indmin:indmax]   
            region_y = data_y[indmin:indmax]   
        

            if len(region_x)>=2:
                self.line2.set_data(region_x, region_y)
                ax1.set_xlim(region_x[0], region_x[-1])     #select from region start till region end...
                ax1.set_ylim(-1.2,1.2)#region_y[0], region_y[-1])
                self.span2.extents = (region_x[0], region_x[-1])
                self.canvas.draw_idle()
                self.mouse_SpanSelected = True;    
            else: 
                print('onselect_span: region too small') 
            
            self.span1.clear()
            print('onselect_span end')
  
    
        self.span1 = SpanSelector(
            ax1,
            onselect_span,
            ""horizontal"",
            useblit=True,
            props=dict(alpha=0.3, facecolor=""tab:red""),
            interactive=True,
            drag_from_anywhere=True,
            minspan=-1,                      ## <<<<<<<< HERE
            grab_range = 3,
        )
    
        self.span2 = SpanSelector(
            ax2,
            onselect_span,
            ""horizontal"",
            useblit=True,
            props=dict(alpha=0.3, facecolor=""tab:red""),
            interactive=True,
            drag_from_anywhere=True,
            minspan=-1,                            ## <<<<<<<< AND HERE
            grab_range = 3,
        )
    
        def onclick(event):
            global ix
            ix = event.xdata
            print('onclick end')

                
        def onrelease(event):
            print('onrelease')
            global ix, ixrel
            ixrel = event.xdata

            if abs(ix-ixrel)<=2:
                print('Release: region too small')
                width_half = int ((self.line2._x[-1] - self.line2._x[0])/2)
                self.span2.extents = (ixrel - width_half, ixrel + width_half)
                onselect_span(ixrel - width_half, ixrel + width_half)
                self.span2.update()
            
            self.canvas.draw_idle()
            
            print('onrelease end')
        
        
        click_id = self.figure.canvas.mpl_connect('button_press_event', onclick)
        relaese_id = self.figure.canvas.mpl_connect('button_release_event', onrelease)

        self.span2.extents = (4000,15501)  #set selectred region
        onselect_span(4000,15501)

        self.canvas.draw()
    
        MainWindow.setCentralWidget(self.centralwidget)


if __name__ == ""__main__"":
    import sys
    app = QtWidgets.QApplication(sys.argv)
    MainWindow = QtWidgets.QMainWindow()
    ui = MainWindow_code_serarch()
    ui.setup_code_serarch(MainWindow)
    MainWindow.show()
    sys.exit(app.exec_())

",matplotlib
how to create major and minor gridlines with different linestyles,"I am currently using matplotlib.pyplot to create graphs and would like to have the major gridlines solid and black and the minor ones either greyed or dashed.
In the grid properties, which=both/major/mine, and then color and linestyle are defined simply by linestyle. Is there a way to specify minor linestyle only?
The appropriate code I have so far is
plt.plot(current, counts, 'rd', markersize=8)
plt.yscale('log')
plt.grid(b=True, which='both', color='0.65', linestyle='-')

","Actually, it is as simple as setting major and minor separately:
In [9]: plot([23, 456, 676, 89, 906, 34, 2345])
Out[9]: [<matplotlib.lines.Line2D at 0x6112f90>]

In [10]: yscale('log')

In [11]: grid(visible=True, which='major', color='b', linestyle='-')

In [12]: grid(visible=True, which='minor', color='r', linestyle='--')

The gotcha with minor grids is that you have to have minor tick marks turned on too.  In the above code this is done by yscale('log'), but it can also be done with plt.minorticks_on().

Note: before matplotlib 3.5, visible parameter was named b
",matplotlib
horizontal and vertical colorbars on a single plot in python,"I have KDE plot generated using seaborn on which I overlay some scattered points with some size. I need to have two colorbars to represent the data of KDE and the scattered values's size (as shown below).
I tried to use the answer provided here at Stackoverflow, however, it is not generating the desired results as shown in the screenshot below

Question: How can I correct the horizontal colorbar too so that there is some vertical gap between colorbar and the main plot? Also, the length of the horizontal bar is a bit too much and needs to have the same length as the width of the main plot.
MWE:
import matplotlib.pyplot as plt
from mpl_toolkits import axes_grid1
import seaborn as sns
import numpy as np

def add_colorbar(im, aspect=20, pad_fraction=0.5, orientation='vertical', **kwargs):
    """"""Add a color bar to an image plot with an option for horizontal or vertical colorbars.""""""
    divider = axes_grid1.make_axes_locatable(im.axes)
    
    if orientation == 'horizontal':
        width = axes_grid1.axes_size.AxesX(im.axes, aspect=1./aspect)  # Horizontal colorbar
        pad = axes_grid1.axes_size.Fraction(pad_fraction, width)
        cax = divider.append_axes(""bottom"", size=width, pad=pad)  # Place at the bottom
    else:
        width = axes_grid1.axes_size.AxesY(im.axes, aspect=1./aspect)  # Default vertical colorbar
        pad = axes_grid1.axes_size.Fraction(pad_fraction, width)
        cax = divider.append_axes(""right"", size=width, pad=pad)  # Place at the right
    
    return im.axes.figure.colorbar(im, cax=cax, orientation=orientation, **kwargs)

# Dummy data 
x_values = np.random.rand(100)
y_values = np.random.rand(100)
size_values = np.random.rand(100)
kd = np.random.rand(100, 2)
kde_params_x = np.mean(kd[:, 0])
kde_params_y = np.mean(kd[:, 1])

# Create a plot
fig, ax = plt.subplots(figsize=(8, 8))

# Plot the multidimensional KDE for the winning data
kde = sns.kdeplot(x=kd[:, 0], y=kd[:, 1], fill=True, cmap='crest', bw_adjust=0.5, alpha=0.7, ax=ax)

# colorbar for the KDE plot
add_colorbar(kde.collections[0], orientation='horizontal')  # orientation to 'horizontal'

# Overlay the empirical scatter plot with contrasting colors
scatter = ax.scatter(x_values, y_values, s=2,   c=size_values, cmap='plasma', alpha=0.8, edgecolor='black', linewidth=0.5, label='Empirical Data')

# Add a colorbar for the scatter plot (vertical, as before)
add_colorbar(scatter, orientation='vertical')  # vertical orientation

plt.show()

","If you replace your add_colorbar function calls by this:
plt.colorbar(kde.collections[0], shrink=0.8, orientation='horizontal', location='bottom', pad=0.05, anchor=(0.0, 0.5))
plt.colorbar(scatter, shrink=1.0, orientation='vertical', location='right', pad=0.05, anchor=(0.0, 0.5))

You will get:

",matplotlib
matplotlib how to make two histograms have the same bin width,"I've spent some time searching the interwebs for an answer for this, and I have tried looking all over SO for an answer too, but I think I do not have the correct terminology down... Please excuse me if this is a duplicate of some known problem, I'd happily delete my post and refer to that post instead!
In any case, I am trying to plot two histograms on the same figure in Matplotlib. My two data sources are lists of 500 elements long. To provide an illustration of the problem I am facing, please see the following image:

As you can see, the histogram has uneven bin sizes under default parameters, even though the number of bins is the same. I would like to guarantee that the bin widths for both histograms are the same. Is there any way I can do this?
Thanks in advance!
","I think a consistent way that will easily work for most cases, without having to worry about what is the distribution range for each of your datasets, will be to put the datasets together into a big one, determine the bins edges and then plot:
a = np.random.random(100) * 0.5                   # A uniform distribution
b = 1 - np.random.normal(size=100) * 0.1          # A normal distribution 
bins = np.histogram(np.hstack((a,b)), bins=40)[1] # Get the bin edges
plt.hist(a, bins)
plt.hist(b, bins)


",matplotlib
alternative to python39s timesleep,"I'm performing realtime data processing + display, and I hit our database every 60 seconds. I'd like to not use time.sleep() for waiting every 60 seconds, as it removes control from me (namely REPL access to variables, which isn't necessary but nice) and freezes matplotlib charts. 
Is there an alternative? Ideally, something that would initially give control to the user, and after 60 seconds, take control away, run some code, and update a plot, then give control back to the user. (When I say control, I mean REPL control). 
Any ideas?
","If you don't need to take away user control, there's a very easy way to do this: Create a threading.Timer.
What you want to do is take the ""continuation"" of the function—that is, everything that would come after the time.sleep—and move it into a separate function my_function, then schedule it like this:
threading.Timer(60, my_function).start()

And at the end of my_function, it schedules a new Timer with the exact same line of code.
Timer is a pretty clunky interface and implementation, but it's built into the stdlib. You can find recipes on ActiveState and modules on PyPI that provide better classes that, e.g., run multiple timers on one thread instead of a thread per timer, let you schedule recurring calls so you don't have to keep rescheduling yourself, etc. But for something that just runs every 60 seconds, I think you may be OK with Timer.
One thing to keep in mind: If the background job needs to deal with any of the same data the user is dealing with in the REPL, there is a chance of a race condition. Often in an interactive environment (especially in Python, thanks to the GIL), you can just lay the onus on the user to not cause any races. If not, you'll need some kind of synchronization.
Another thing to keep in mind: If you're trying to do GUI work, depending on the GUI you're using (I believe matplotlib is configurable but defaults to tkinter?), you may not be able to update the GUI from a background thread.
But there's actually a better solution in that case anyway. GUI programs have an event loop that runs in some thread or other, and almost every event loop ever design has a way to schedule a timer in that thread. For tkinter, if you have a handle to the root object, just call root.after(60000, my_function) instead of threading.Timer(60, my_function).start(), and it will run on the same thread as the GUI, and without wasting any unnecessary resources.
",matplotlib
how to increase the space between the subplots and the figure,"I'm using a python code to plot 3D surface. However, the z-axis label get cut by the figure. Here is the code :
import matplotlib.pyplot as plt
import numpy as np

fig = plt.figure(figsize=(12, 10), facecolor='lightblue')
x = np.linspace(0, 10)
y = np.linspace(0, 10)
X, Y = np.meshgrid(x, y)

for idx in range(4):
    Z = np.cos(X) - np.sin(np.pi*idx/4 * Y)
    ax3D =  fig.add_subplot(2, 2, idx+1, projection='3d')
    ax3D.plot_surface(X, Y, Z, cmap=""viridis"")
    ax3D.set_zlabel(""Title"")

plt.show()

The result : 3D surface plots
Is it a possible to include the axis label in the axe ? Or to increase the space subplots and the figure ?
I have tried different options such as : plt.subplots_adjust(left=0, bottom=0, right=0.8, top=0.7, wspace=0.5, hspace=0.2) or fig.tight_layout(); but nothing seems to resolve my problem.
","One solution is to zoom out to decrease the size of each subplot (set_box_aspect).
One can also play with the three angles that defines the view: elevation, azimuth, and roll (view_init).
fig = plt.figure(figsize=(12/2, 10/2), facecolor='lightblue')
x = np.linspace(0, 10)
y = np.linspace(0, 10)
X, Y = np.meshgrid(x, y)

for idx in range(4):
    Z = np.cos(X) - np.sin(np.pi*idx/4 * Y)
    ax3D =  fig.add_subplot(2, 2, idx+1, projection='3d')
    ax3D.view_init(elev=30, azim=70, roll=0)  
    ax3D.set_box_aspect(aspect=(1,1,1), zoom=0.8)
    ax3D.plot_surface(X, Y, Z, cmap=""viridis"")
    ax3D.set_zlabel(""Title"")
fig.tight_layout()
plt.show()


",matplotlib
efficient access to data in a series of transient python scripts,"Pandoc has a filter that accepts Python snippets and uses (for example) Matplotlib to generate charts. I want to produce documents that produce many charts from a common data source (e.g. a pandas data frame).
As an example:
Here's the first chart:

~~~{.matplotlib}
import sqlite3
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

conn = sqlite3.connect('somedb.db')
query = '''SELECT something'''

df = pd.read_sql_query(query, conn).dropna()
fig, ax = plt.subplots()
ax.something()
~~~

The problem is that every chart has to regenerate the data frame, which is expensive. What I'd like to do is:

Run a script at the beginning of the Markdown document that creates the data source and makes it available efficiently to subsequent filter calls.
Use the data to create as many charts as I need from the existing data source.
Shut down the data source with the pandoc call ends (or maybe with a time-to-live parameter).

Any ideas?
","The author of pandoc-plot kindly provided the following answer in Github:
Out-of-the-box there's no handling of your use-case in the pandoc-plot filter. Each code block that gets turned into a plot is intended to be independent from all others. This has many benefits, most importantly performance -- I wrote pandoc-plot for book-sized workloads, with close to 100 figures.
The reason using preamble isn't working is because the preamble script gets copy-pasted into every code block before pandoc-plot renders a figure. Therefore, the creation of your dataframe will still be duplicated.
I would recommend you proceed with a script to wrap your usage of pandoc. For example (assuming you use bash):
# Run a script that goes through your expensive computation,
# storing the results as a CSV i
python create-data.py

# Render the document, where plots can reference the file created by 
# your python script instead of re-creating the pandas dataframe for every plot
pandoc -f pandoc-plot ...

# Clean up temporary data file if you know where it is

You can communicate between the bash script above and your document plots using environment variables.
",matplotlib
how to annotate maths equation in graph using both latex and fstring formatting,"Here's the code snippet : annotation of mathematical equation of the cuve.
# equation annotation 
plt.annotate(s =f""$exp({poly_regr.intercept_[0] : 0.2f} - {-poly_regr.coef_[0,0] : 0.2f}x + {poly_regr.coef_[0,1]: 0.4f}x^2)$"",
             xy = (1975, 0.4*(10**13)),
             color = ""black"",
             fontsize = 13,
             fontweight = ""normal""
             )


what's the possible way to update the f-string code to get the desired anotation with a beautiful mathematical equations.
or even how to write more complex things , for example :
","You can do it using LaTeX syntax. For example, replace the string in plt.annotate with
fr""$e^{{({poly_regr.intercept_[0] : 0.2f} - {-poly_regr.coef_[0,0] : 0.2f}x + {poly_regr.coef_[0,1]: 0.4f}x^2)}}$""

or with
r""$e^{(\alpha + \beta + 48x)}$""

",matplotlib
mismatch between the volume shape and the axes grid in matplotlib,"I have written a script to visualize a 3D volume using Matplotlib. The decay volume is explicitly centered at x = y = 0, but the grid displayed appears displaced relative to the volume. This seems to be an issue with the grid, not the decay volume definition.
The script is provided below, and I also attach the result of its execution.
Could you please tell me what might be causing the grid to be misaligned, and how can I fix it? I believe the issue is with how the grid limits or alignment are set.
The script:
# funcs/setup.py

from mpl_toolkits.mplot3d.art3d import Poly3DCollection
import matplotlib.pyplot as plt

z_min = 32
z_max = 82  
Delta_x_in = 1 
Delta_x_out = 4
Delta_y_in = 2.7
Delta_y_out = 6.2 

def x_max(z):
    return (Delta_x_in/2 * (z - z_max) / (z_min - z_max) +
            Delta_x_out/2 * (z - z_min) / (z_max - z_min))

def y_max(z):
    return (Delta_y_in/2 * (z - z_max) / (z_min - z_max) +
            Delta_y_out/2 * (z - z_min) / (z_max - z_min))

def plot_decay_volume(ax):
    x_min_zmin = -x_max(z_min)
    x_max_zmin_val = x_max(z_min)
    y_min_zmin = -y_max(z_min)
    y_max_zmin_val = y_max(z_min)

    x_min_zmax = -x_max(z_max)
    x_max_zmax_val = x_max(z_max)
    y_min_zmax = -y_max(z_max)
    y_max_zmax_val = y_max(z_max)

    vertices = [
        [x_min_zmin, y_min_zmin, z_min],         
        [x_max_zmin_val, y_min_zmin, z_min],     
        [x_max_zmin_val, y_max_zmin_val, z_min], 
        [x_min_zmin, y_max_zmin_val, z_min],     
        [x_min_zmax, y_min_zmax, z_max],         
        [x_max_zmax_val, y_min_zmax, z_max],     
        [x_max_zmax_val, y_max_zmax_val, z_max], 
        [x_min_zmax, y_max_zmax_val, z_max]      
    ]

    edges = [
        [vertices[0], vertices[1]],
        [vertices[1], vertices[2]],
        [vertices[2], vertices[3]],
        [vertices[3], vertices[0]],
        [vertices[4], vertices[5]],
        [vertices[5], vertices[6]],
        [vertices[6], vertices[7]],
        [vertices[7], vertices[4]],
        [vertices[0], vertices[4]],
        [vertices[1], vertices[5]],
        [vertices[2], vertices[6]],
        [vertices[3], vertices[7]]
    ]

    for edge in edges:
        xs, ys, zs = zip(*edge)
        ax.plot(xs, ys, zs, color='gray', linewidth=1)

    faces = [
        [vertices[0], vertices[1], vertices[2], vertices[3]],
        [vertices[4], vertices[5], vertices[6], vertices[7]],
        [vertices[0], vertices[1], vertices[5], vertices[4]],
        [vertices[1], vertices[2], vertices[6], vertices[5]],
        [vertices[2], vertices[3], vertices[7], vertices[6]],
        [vertices[3], vertices[0], vertices[4], vertices[7]]
    ]

    face_collection = Poly3DCollection(faces, linewidths=0.5, edgecolors='gray', alpha=0.3)
    face_collection.set_facecolor('lightgray')
    ax.add_collection3d(face_collection)

def visualize_decay_volume():
    fig = plt.figure(figsize=(10, 8))
    ax = fig.add_subplot(111, projection='3d')
    plot_decay_volume(ax)
    ax.set_xlabel('X (m)')
    ax.set_ylabel('Y (m)')
    ax.set_zlabel('Z (m)')
    x_lim = max(abs(x_max(z_min)), abs(x_max(z_max))) + 1
    y_lim = max(abs(y_max(z_min)), abs(y_max(z_max))) + 1
    ax.set_xlim(-x_lim, x_lim)
    ax.set_ylim(-y_lim, y_lim)
    ax.set_zlim(z_min - 5, z_max + 5)
    ax.set_title('Decay Volume')
    plt.show()

if __name__ == ""__main__"":
    visualize_decay_volume()

The output:

I have removed the axes scaling, but got a similar problem:
","This is simply a result of your choice of z-limits. You've chosen to adjust the limits so the volume is not on the bottom plane, so the perspective makes it look like the volume isn't centered. If you adjust ax.set_zlim(z_min - 5, z_max + 5) to be ax.set_zlim(z_min, z_max + 5) you will see that the volume appears centered.

",matplotlib
python heatmap with categorical color and continuous transparency,"I want to make a heatmap in python (seaborn, matplotlib, etc) with two dimensions of information. I have a categorical value I want to assign to color, and a continuous variable (i.e. between 0-100 or 0-1) I want to assign to transparency, so each box has its own color and transparency (or intensity).
for example:
colors = pd.DataFrame([['b','g','r'],['black','orange','purple'],['r','yellow','white']])
transparency = pd.DataFrame([[0.1,0.2,0.3],[0.9,0.1,0.2],[0.1,0.6,0.3]])

how can I make a heatmap from this data such that the top left box is blue in color and 10% transparency (or 10% opaqueness, whichever), and so on?
The best idea I have so far is to turn the colors into integer values, add those to the transparency values, and then make a custom colormap where each integer has a different color, ranging from white to the color in between the integer values. That sounds complicated to make and I'm hoping there's a built-in way to do this.
Any ideas?
","You could draw individual rectangles, giving each a specific color and transparency:
import matplotlib.pyplot as plt
from matplotlib.patches import Rectangle, Patch
import pandas as pd

colors = pd.DataFrame([['b', 'g', 'r'], ['black', 'orange', 'purple'], ['r', 'yellow', 'white']])
transparency = pd.DataFrame([[0.1, 0.2, 0.3], [0.9, 0.1, 0.2], [0.1, 0.6, 0.3]])

fig, ax = plt.subplots()

for i, (color_col, transp_col) in enumerate(zip(colors.columns, transparency.columns)):
    for j, (color, transp) in enumerate(zip(colors[color_col], transparency[transp_col])):
        ax.add_patch(Rectangle((i - 0.5, j - 0.5), 1, 1,
                               facecolor=color, alpha=transp, edgecolor='none', lw=0))

ax.invert_yaxis()  # start at the top
ax.autoscale(enable=True, tight=True)  # recalculate axis limits
ax.set_xticks(range(len(colors.columns)), colors.columns)
ax.set_yticks(range(len(colors.index)), colors.index)
plt.show()


",matplotlib
how do you display the scale in meters the north arrow and the axes in latitude and longitude on a map with geopandas,"With reference to this issue, is it possible to have the scale bar (projected in meters, so 3857 for example) with the x,y axes in latitude, longitude projection (4326) and the north arrow?
I don't see a turnkey solution to do this with geopandas. While this seems to be basic settings for map display with GIS. Is there a technical reason for this?
import geopandas as gpd
from matplotlib_scalebar.scalebar import ScaleBar
import matplotlib.pyplot as plt

df = gpd.read_file(gpd.datasets.get_path('nybb'))
ax = df.to_crs(4326).plot()
ax.add_artist(ScaleBar(1)) #how add ScaleBar for df in 3857? 
plt.show()

","From this, it looks like you have to compute the great circle distance between two locations A and B with coordinates
A=[longitudeA,latitudeA] and B=[longitudeA+1,latitudeA], at the latitude you are interested in (in your case ~40.7°). To compute the great circle distance you can use the 'haversine_distances' from sklearn (here) and multiply it by the radius of the earth 6371000 to get the distance in meters.
Once you get this distance dx, you can just pass it to your scalebar with ScaleBar(dx=dx,units=""m"").
So overall, the code looks like that:
import numpy as np
import geopandas as gpd
from matplotlib_scalebar.scalebar import ScaleBar
import matplotlib.pyplot as plt
from sklearn.metrics.pairwise import haversine_distances

df = gpd.read_file(gpd.datasets.get_path('nybb'))
ax = df.to_crs(4326).plot()
A=[-74.5*np.pi/180.,40.7*np.pi/180.] #Latitude of interest here 40.7 deg, longitude -74.5
B=[-73.5*np.pi/180.,40.7*np.pi/180.] ##Latitude of interest here 40.7 deg, longitude -74.5+1
dx=(6371000)*haversine_distances([A,B])[0,1]
ax.add_artist(ScaleBar(dx=dx,units=""m"")) 
plt.show()

And the output gives:

",matplotlib
a right way to represent 4 dimension points using colors in a matplotlib scatter,"I'm currently trying to represent a set of 4 dimensional points in a 3D space using a matplotlib scatter. For do that, I represent the 4th dimension as the color of the 3D point. 
According to that, I want to print colored points. Therefore, the color of this points depends on the 4th component of the point. 
I want to use the spectral color map. 
I've already succeeded but using a greyscale, and this repesentation is not enought for me. 
I really need to use the spectral color map. So this following code was my last try before ask here:
inicioVertices=5
finalVertices=10
fig = plt.figure()
ax = fig.add_subplot(111, projection='3d')
datos={}
for vertice in range(inicioVertices,finalVertices):
    print(vertice)
    for arista in range(vertice, vertice*(vertice-1)/2):
       for k in range(vertice//4,vertice+1):
           media=0.0
           for n in range(10):
               g=nx.dense_gnm_random_graph(vertice,arista)
               inicio=time.time()
               recubrimientoVertices(g,k)
               diferencia=time.time()-inicio
           media+=diferencia
           aux=media
           media=aux/10

           datos[(vertice,arista,k)]=media
           mMin=0.00054
           mMax=0.067

           normalizada=(media-mMin)/(mMax-mMin)
           cmap = cm.ScalarMappable( cmap = plt.get_cmap('spectral'))

           print(media)
           ax.scatter(vertice, arista, k, c= cmap.to_rgba(normalizada), marker='o',s=40)
print(""max""+str(max(datos.values())))
print(""min""+str(min(datos.values())))
ax.set_xlabel('Vertices')
ax.set_ylabel('Aristas')
ax.set_zlabel('K')
plt.show()

media is the 4th component value and normalizada is the normalized value for this component so normalizada always be a number in this interval [0,1]. This is the representation returned by the previous code:

As you can see, all dots are printed in black. I hope someone can help me with this, thank you.
","Just an example to plot a 3D scatter plot and using an user defined colour map. 
import matplotlib.cm as cmx
from mpl_toolkits.mplot3d import Axes3D
import matplotlib.pyplot as plt
import matplotlib
import numpy as np

def scatter3d(x,y,z, cs, colorsMap='jet'):
    cm = plt.get_cmap(colorsMap)
    cNorm = matplotlib.colors.Normalize(vmin=min(cs), vmax=max(cs))
    scalarMap = cmx.ScalarMappable(norm=cNorm, cmap=cm)
    fig = plt.figure()
    ax = Axes3D(fig)
    ax.scatter(x, y, z, c=scalarMap.to_rgba(cs))
    scalarMap.set_array(cs)
    fig.colorbar(scalarMap,label='Test')
    plt.show()

x = np.random.uniform(0,1,50)
y = np.random.uniform(0,1,50)
z = np.random.uniform(0,1,50)

When I call scatter3d(x,y,z,x+y) for example, I get the following with x+y being my colormap:

",matplotlib
stacked bar with values,"    +------------+------+------+---------+
    | promo_name | in   | out  | model   |
    +------------+------+------+---------+
    | A          | 0,87 | 0,13 | plan    |
    +------------+------+------+---------+
    | B          | 0,77 | 0,23 | plan    |
    +------------+------+------+---------+
    | C          | 0,54 | 0,46 | plan    |
    +------------+------+------+---------+
    | A          | 0,59 | 0,41 | predict |
    +------------+------+------+---------+
    | B          | 0,50 | 0,50 | predict |
    +------------+------+------+---------+
    | C          | 0,46 | 0,54 | predict |
    +------------+------+------+---------+

Hello. I have a dataframe as in the table above, I would like to draw a stacked barplot with values, for example:

But so far it only works like this, my code is below:
ratio.set_index(['promo_name', 'model'], inplace=True)
ratio = ratio.reindex(index = [('A', 'plan'), ('A', 'predict'),
                              ('B', 'plan'), ('B', 'predict'),
                              ('C', 'plan'), ('C', 'predict')])
plt.rcParams[""figure.figsize""] = (12,5)
ratio.plot(kind='bar', stacked=True)
plt.xticks(rotation=30)
plt.show()

But until I get the wrong thing, how do I fix it?

","Using matplotlib you can easily play around with the locations and widths of the bars: 
 import matplotlib.pyplot as plt

# plan, predict
Aout = (0.87, 0.59)
Ain = (0.13, 0.41)
Bout = (0.77, 0.50)
Bin = (0.23, 0.50)  
Cout = (0.54, 0.46)
Cin = (0.46, 0.54)
width = 1.0       # the width of the bars

A_positions = [0, 1]  # Positions for A bars
p1A = plt.bar([0, 1], (1.0, 1.0), width, color='g', label='out')
p2A = plt.bar([0, 1], Ain, width, color='b', label='in')

B_positions = [3, 4]  # Positions for B bars
p1B = plt.bar(B_positions, (1.0, 1.0), width, color='g')
p2B = plt.bar(B_positions, Bin, width, color='b')

C_positions = [6, 7]  # Positions for C bars
p1C = plt.bar(C_positions, (1.0, 1.0), width, color='g')
p2C = plt.bar(C_positions, Cin, width, color='b')

positions = A_positions + B_positions + C_positions  # All together for ticks
plt.xticks(positions, ('A (plan)', 'A (predict)', 'B (plan)', 'B (predict)', 'C (plan)', 'C (predict)'))
plt.xticks(rotation=70)
plt.yticks([1, 0])
plt.legend()
plt.tight_layout()

And add text labels (you can play around with the location):
plt.text(A_positions[0] - 0.25, Ain[0] / 2, str(Ain[0]))
plt.text(A_positions[0] - 0.25, Aout[0] / 2, str(Aout[0]))
plt.text(A_positions[1] - 0.25, Ain[1] / 2, str(Ain[1]))
plt.text(A_positions[1] - 0.25, Aout[1] / 2 + 0.5, str(Aout[1]))

plt.text(B_positions[0] - 0.25, Bin[0] / 2, str(Bin[0]))
plt.text(B_positions[0] - 0.25, Bout[0] / 2, str(Bout[0]))
plt.text(B_positions[1] - 0.25, Bin[1] / 2, str(Bin[1]))
plt.text(B_positions[1] - 0.25, Bout[1] / 2 + 0.5, str(Bout[1]))


plt.text(C_positions[0] - 0.25, Cin[0] / 2, str(Cin[0]))
plt.text(C_positions[0] - 0.25, Cout[0] / 2 + 0.5, str(Cout[0]))
plt.text(C_positions[1] - 0.25, Cin[1] / 2, str(Cin[1]))
plt.text(C_positions[1] - 0.25, Cout[1] / 2 + 0.5, str(Cout[1]))


",matplotlib
how to sample point using latinhyper cube sampling in a random volume,"I have a a dataset composed of XYZ coordinates, which can be rendered in 3D bar chart as the picture below.

The main question I have is: how to generate X points (X being a user input) inside said volume ensuring the best representation possible ?
My first approach was to use the LatinHyper Cube sampling method, but as the name suggests it supposed a cubic design space. Here the volume could be any shape.
I then tried to applied the method described here: https://math.stackexchange.com/questions/2174751/generate-random-points-within-n-dimensional-ellipsoid I hoped the method could be tweaked to any shape rather the an ellipsoid. It is not the case (or at least, I failed to do so).
As a ""bonus"" question, I am not happy with the 3D histogram approach, I would rather have a ""proper"" volume. Using griddata implies a cubic base which is not the case here.
EDIT:
I managed to plot my data as a 3D surface, which is fine for my usage using the following:
data_2d = [
    [2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 0],
    [0, 0, 6, 8, 10, 12, 14, 16, 18, 20, 22],
    [0, 0, 0, 0, 0, 12, 14, 16, 18, 20, 22],
    [0, 0, 0, 0, 0, 12, 14, 16, 18, 20, 0],
    [0, 0, 0, 0, 0, 0, 14, 16, 18, 0, 0],
    [0, 0, 0, 0, 0, 0, 14, 16, 18, 0, 0],
    [0, 0, 0, 0, 0, 0, 14, 16, 18, 0, 0],
    [0, 0, 0, 0, 0, 0, 14, 16, 18, 0, 0],
]
# data_2d: - rows are Hs from 1 to 8 (8 rows)
#          - columns are Tp from 2 to 22 (10 columns)
#          - content is the wind speed from 2 to 22

data_array = np.array(data_2d)
x_data, y_data = np.meshgrid(np.linspace(2, 22, 11), np.linspace(1, 8, 8))


ax = plt.axes(projection=""3d"")
ax.plot_surface(x_data, y_data, data_array, cmap=""viridis"", edgecolor=""black"")
ax.set_xlabel(""x"")
ax.set_ylabel(""y"")
ax.set_zlabel(""z"")


","Actually, I split the issue in two steps:

Generate LHS sampling in the XY plan bounded by a polygon
For each sampled draw a random Z value bounded by an arbitrary minimum value and the calculated upper Z value.

Below is a MWE for whoever would be interested:
    from random import uniform
    import matplotlib.pyplot as plt
    import numpy as np
    import pandas as pd
    from scipy.interpolate import CloughTocher2DInterpolator as CT
    from scipy.stats import qmc
    from shapely.geometry import Point, Polygon

    data_2d = [
    [2, 4, 6, 8, 10, 12, 14, 16, 18, 20, np.nan],
    [np.nan, np.nan, 6, 8, 10, 12, 14, 16, 18, 20, 22],
    [np.nan, np.nan, np.nan, np.nan, np.nan, 12, 14, 16, 18, 20, 22],
    [np.nan, np.nan, np.nan, np.nan, np.nan, 12, 14, 16, 18, 20, np.nan],
    [np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, 14, 16, 18, np.nan, np.nan],
    [np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, 14, 16, 18, np.nan, np.nan],
    [np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, 14, 16, 18, np.nan, np.nan],
    [np.nan, np.nan, np.nan, np.nan, np.nan, np.nan, 14, 16, 18, np.nan, np.nan],
]
# data_2d: - rows are Hs from 1 to 8 (8 rows)
#          - columns are Tp from 2 to 22 (10 columns)
#          - content is the wind speed from 2 to 22

tp_hs_ws = pd.DataFrame(data_2d)
tp_hs_ws.columns = [np.arange(2, 24, 2)]
tp_hs_ws.index = [np.arange(1, 9, 1)]
x_data, y_data = np.meshgrid(np.arange(2, 24, 2), np.arange(1, 9, 1))

non_nan_coord = [
    (2, 1),(20, 1),(22, 2),(22, 3),(22, 3),(20, 4),(18, 5),(18, 8),(14, 8),(14, 5),(12, 4),(12, 3),(10, 2),(6, 2),(2, 1)]

polygon = Polygon(non_nan_coord)
xp, yp = polygon.exterior.xy

points = LHS_Points_in_Polygon(polygon, nb_points)
xs = [point.x for point in points]
ys = [point.y for point in points]

# Keep only the unique LHS samples
xs = pd.Series(xs).unique()
ys = pd.Series(ys).unique()
xs_grid, ys_grid = np.meshgrid(xs, ys)

# Interpolate initial wind speed on the LHS Hs/Tp grid
zz = []
for z in (np.array(data_2d)).ravel():
    if str(z) == ""nan"":
        z = 0
    zz.append(z)

xy = np.c_[x_data.ravel(), y_data.ravel()]
CT_interpolant = CT(xy, zz)
Ws = CT_interpolant(xs_grid, ys_grid)

# Select the wind speed associated to the LHS Tp/Hs samples
ws = []
for idx_tp, _ in enumerate(xs_grid.ravel()):
    ws.append(Ws.ravel()[idx_tp])
# Make the LHS samples in square matrix form
ws_LHS = np.reshape(ws, (len(xs_grid), len(ys_grid)))
# The diagonal of wind speed LHS samples is corresponding to the XY coordinates sampled
ws_LHs_diag = ws_LHS.diagonal()

# Create random wind speed between 2m/s (arbitrary lower bound) and the LSH sampled wind speed value (upper bound)
# This ensure to produce a point XYZ always contained with the voume Tp/Hs/Wind speed
random_ws = [uniform(2, ws) for ws in ws_LHs_diag]

The function LHS_Points_in_Polygon is inspired by this solution.
def LHS_Points_in_Polygon(polygon, number):
    minx, miny, maxx, maxy = polygon.bounds
    sampler = qmc.LatinHypercube(d=2, scramble=False)
    sample = sampler.random(n=number)
    l_bounds = np.min((minx, miny))
    u_bounds = np.max((maxx, maxy))
    points = []
    while len(points) < number:
        for x, y in qmc.scale(sample, l_bounds, u_bounds):
            pnt = Point(x, y)
            if polygon.contains(pnt):
                points.append(pnt)
    return points

Below is the outcome:


",matplotlib
problems plotting timestamps on the xaxis with matplotlib,"I am working on a Python script that loads several CSV files containing timestamps and ping data and then displays them on a plot. The X-axis is supposed to display the timestamps in HH:MM format, with the timestamps coming from multiple CSV files that record different ping values for different addresses.
The challenge is that I only want to display a limited number of timestamps for the X axis, e.g. 10-12 timestamps, based on the number of data points in the CSV files. I also want to ensure that the X-axis is correctly labeled with the appropriate timestamps and associated ping values.
Problem:
The plot shows the data, but the timestamps on the X-axis are not correct and too few ticks appear. Only the first timestamp is displayed and only 8 ticks are generated on the X-axis.
In addition, the X-axis ticks do not seem to match the timestamps from the data correctly, which affects the readability of the plot.
Goal:
The X-axis should correctly display timestamps in the format HH:MM:SS for all addresses from the CSV files.
I would like to have a limited number of timestamps (approx. 10-12) on the X-axis based on the data points in the CSV files.
It is important to mention that the information for the plot is stored in x_labels and x_positions.
11 subdivisions are also correctly created and saved for 99 data records, but these are still displayed incorrectly.
Example:
x_positions: [0.0, 2.55, 5.1, 7.65, 10.216666666666667, 12.766666666666667, 15.316666666666666, 17.866666666666667, 20.416666666666668, 22.983333333333334, 25.533333333333335]
x_labels: ['17:24:43', '17:27:16', '17:29:49', '17:32:22', '17:34:56', '17:37:29', '17:40:02', '17:42:35', '17:45:08', '17:47:42', '17:50:15']
This is the picture I get, but it should have 11 dividing lines on the X axis and all of them should be labeled

Here is some test Data, I store in the csv:
Time,Ping (ms)
17:24:43,0.1
17:25:00,0.2
17:25:17,0.23
17:25:34,0.12
17:25:51,0.23
17:26:08,0.123
17:26:25,0.321
17:26:42,0.231

Here is My Code:
import os
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
from datetime import datetime, timedelta

# Funktion zum Laden der Daten aus den CSV-Dateien
def load_data(folder):
    data = {}
    for root, dirs, files in os.walk(folder):
        for file in files:
            if file.endswith("".csv""):
                address = file.replace('_', '.').replace('.csv', '')
                file_path = os.path.join(root, file)
                df = pd.read_csv(file_path)
                df['Time'] = pd.to_datetime(df['Time'], format='%H:%M:%S')
                df['Ping (ms)'] = df['Ping (ms)'].apply(lambda x: 0 if x == 0 else x)
                data[address] = df
    return data

# Funktion zum Erstellen des Plots
def plot_data(data):
    plt.figure(figsize=(14, 8))
    colors = generate_colors(len(data))

    # Bestimme die Anzahl der Datenpunkte für eine einzelne Adresse
    df = next(iter(data.values()))  # Wähle den ersten DataFrame aus
    total_data_points = len(df)

    # Berechne den dif-Wert
    dif = total_data_points // 10
    if dif < 1:
        dif = 1

    # Sammle alle Zeitstempel für die X-Achse
    x_labels = []
    x_positions = []
    for i in range(0, len(df), dif):
        time = df['Time'].iloc[i]
        x_labels.append(time.strftime('%H:%M:%S'))
        x_positions.append((time - min(df['Time'])).total_seconds() / 60)

    # Plotten der Ping-Daten für jede Adresse
    for idx, (address, df) in enumerate(data.items()):
        df['Time_diff'] = (df['Time'] - min(df['Time'])).dt.total_seconds() / 60
        mask_timeout = df['Ping (ms)'] == 0
        mask_normal = ~mask_timeout

        plt.plot(df['Time_diff'][mask_normal], df['Ping (ms)'][mask_normal], label=address, color=colors[idx % len(colors)])
        plt.plot(df['Time_diff'][mask_timeout], df['Ping (ms)'][mask_timeout], color='r', lw=2)

    # Anpassen der X-Achse
    plt.xticks(x_positions, x_labels, rotation=45, ha='right')

    plt.xlabel('Time')
    plt.ylabel('Ping (ms)')
    plt.title('Ping Times for Different Addresses')
    plt.legend()
    plt.grid(True)
    plt.tight_layout()
    plt.show()

def generate_colors(n):
    colors = []
    for i in range(n):
        hue = i / n
        colors.append(plt.cm.hsv(hue))
    return colors

# Main-Funktion
def main():
    data_folder = input(""Bitte geben Sie den Pfad zum Ordner mit den CSV-Dateien ein: "")
    if not os.path.exists(data_folder):
        print(f""Der Ordner {data_folder} existiert nicht."")
        return

    data = load_data(data_folder)
    plot_data(data)

if __name__ == ""__main__"":
    main()

","Every time you add a new plot, a new axis is added for both 'x' and 'y'. And I'm unsure if you can control which axis will be on top. so the workaround that I can think about is to set the ticks param for the 'x' axis every time you add a new plot:
for idx, (address, df) in enumerate(data.items()):
    df['Time_diff'] = (df['Time'] - min(df['Time'])).dt.total_seconds() / 60
    mask_timeout = df['Ping (ms)'] == 0
    mask_normal = ~mask_timeout

    plt.plot(df['Time_diff'][mask_normal], df['Ping (ms)'][mask_normal], label=address, color=colors[idx % len(colors)])
    plt.tick_params(axis='x', which='both', labelbottom=False)
    plt.plot(df['Time_diff'][mask_timeout], df['Ping (ms)'][mask_timeout], color='r', lw=2)
    plt.tick_params(axis='x', which='both', labelbottom=False)

And set it back to true (in my example for the minor) right after you set your xticks:
plt.xticks(x_positions, x_labels, rotation=45, ha='right')
plt.tick_params(axis='x', which='minor', labelbottom=True)

I believe it's true for your 'y' axis as well (your graph shows that Google pings better than your local devices).
",matplotlib
how to adjust the size of one subplot independently of other subplots in a matplotlib figure,"I want to have horizontally aligned 3D and 2D plots, where the y-axis of the 2D plot is the same height as the z-axis of the 3D plot. The following code produces a default output:
import matplotlib.pyplot as plt
fig = plt.figure()
fig.set_size_inches(10, 4)
fig.subplots_adjust(wspace=0.5)

# 3D surface plot
ax1 = fig.add_subplot(121, projection=""3d"")
ax1.set(zlabel=""Value"")

# 2D line plot
ax2 = fig.add_subplot(122)
ax2.set(ylabel=""Value"")

plt.show() 


I want to shrink down the right hand side 2D subplot so that the y-axis (""Value"") is a similar height to (or preferably the same height as) the z-axis (""Value"") on the 3D plot (ideally I want them to line up).
I haven't been able to find a way to change subplot sizes independently like this in matplotlib (setting relative ratios and  gridspec don't seem able to achieve this).
Any help would be much appreciated.
","You can use set_position() to change the dimensions of one of the subplot:
plt.figure(1).axes[1].set_position([0.6,0.4,0.25,0.3])  # left, bottom, width, height

It gives:

",matplotlib
pandas numpy matplotlib overflowerror date value out of range,"I have code to crawl through every file in a directory and plot every csv file it comes across. Each CSV has a header of the form
Timestamp, P1rms (A), P2rms (A), P3rms (A), P4rms (A), P5rms (A), Vrms (V), P1 theta, P2 theta, P3 theta, P4 theta, P5 theta

Mon Sep 30 00:00:00 2013, 128, 128, 180, 177, 192, 43, 7, 7, 8, 8, 48
Mon Sep 30 00:00:01 2013, 127, 127, 182, 178, 193, 43, 8, 8, 8, 8, 49
# ect....

I am developing a fft visualization option, and I am running into an overflow error when I fft my data sets. here is my exact problem:
When I run my code:
#!/usr/bin/env python
from pandas import *
import matplotlib.pyplot as plt
import os
import sys
import platform
import numpy.fft as np

# name of plots folder
plotfold='plots'

# System specific info
if platform.system()=='Darwin':comsep=""/""
else: comsep=""\\""

# How many columns should I plot?
numcol=6

if len(sys.argv)<2:
   print 'usage: ./canaryCrawler.py [-c] or [-f] rootdir'
   quit()

if len(sys.argv)>2:
   ylim=1500
   root = sys.argv[2]
else:
   ylim=1200
   root = sys.argv[1]

for subdir, dirs, files in os.walk(root):

    # plot each file
    for file in files:

        if str(file)[-4:]=='.csv':

            print 'plotting '+str(file)+'...'
            # load csv as data frame
            df=pandas.io.parsers.read_csv(subdir+comsep+file)
            for i in range(0,len(df.Timestamp)):
                df.Timestamp[i] = datetime.strptime(df.Timestamp[i], '%a %b %d %H:%M:%S %Y')

            # We only want the first 6 columns
            df = df.ix[:,0:numcol]

            if len(sys.argv)>=2:
                if sys.argv[1]=='-c' or sys.argv[1]=='-f':
                    plotfold='plots_Specialty'
                    df2 = df
                    df=pandas.DataFrame(df2.Timestamp)
                    df['Residence']=df2['P1rms (A)']+df2['P2rms (A)']
                    df['Specialty']=df2['P3rms (A)']+df2['P4rms (A)']
                    if sys.argv[1]=='-f':
                       df2['Residence']=np.fft(df['Residence'])
                       df2['Specialty']=np.fft(df['Specialty'])
                       df=df2
                       print 'Fourier Transformation Complete'
                       plotfold='plots_Specialty_fft'

            # set up plot
            plt.figure()
            df.plot(df.Timestamp,alpha=0.6,linewidth=2.3) # add transparency to see overlapping colors
            plt.tight_layout(pad=1.08)
            plt.legend(loc='best') # add legend in non-intrusive location
            plt.legend(loc=5,prop={'size':14}) # 
            plt.ylabel('Current')
            plt.xlabel('Time')
            plt.gcf().autofmt_xdate()
            plt.gcf().set_size_inches(12.7,9.2)
            plt.gca().set_ylim([0,ylim])

            stamp = df.Timestamp[0]
            day = datetime.strftime(stamp,'%a')
            DOM=datetime.strftime(stamp,'%d')
            month =  datetime.strftime(stamp,'%b')
            year =  datetime.strftime(stamp,'%Y')

            plt.title(subdir+'   '+day+' '+month+' '+DOM+' '+year)

            # keep plot

            # check for existing plots folder, 
            # create one if it doesn't exist
            if plotfold not in os.listdir(subdir):
                print '** adding plots directory to ',subdir
                os.mkdir(subdir+comsep+plotfold)

            # save in plots directory
            spsubs = str(subdir).split(comsep)
            filnam=''
            for piece in range(len(spsubs)-4,len(spsubs)-1):
                filnam+='_'+spsubs[piece]
            filnam+='_'+str(file)[:-4]
            saveto=subdir+comsep+plotfold+comsep+filnam
            print '**** saving plot to ',saveto
            plt.savefig(saveto)
plt.close()

I get this error:
kilojoules$ ./canaryCrawler.py -f 35ca7/
/opt/local/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/pandas-0.14.0-py2.7-macosx-10.9-x86_64.egg/pandas/io/excel.py:626: UserWarning: Installed openpyxl is not supported at this time. Use >=1.6.1 and <2.0.0.
  .format(openpyxl_compat.start_ver, openpyxl_compat.stop_ver))
plotting 2014Aug04.csv...
Fourier Transformation Complete
/opt/local/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/numpy-1.8.1-py2.7-macosx-10.9-x86_64.egg/numpy/core/numeric.py:460: ComplexWarning: Casting complex values to real discards the imaginary part
  return array(a, dtype, copy=False, order=order)
/opt/local/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/matplotlib/tight_layout.py:225: UserWarning: tight_layout : falling back to Agg renderer
  warnings.warn(""tight_layout : falling back to Agg renderer"")
**** saving plot to  35ca7/2014/Aug/plots_Specialty_fft/_Aug_35ca7_2014_2014Aug04
plotting 2014Aug05.csv...
Fourier Transformation Complete
**** saving plot to  35ca7/2014/Aug/plots_Specialty_fft/_Aug_35ca7_2014_2014Aug05
plotting 2014Aug07.csv...
Fourier Transformation Complete
**** saving plot to  35ca7/2014/Aug/plots_Specialty_fft/_Aug_35ca7_2014_2014Aug07
Traceback (most recent call last):
  File ""./canaryCrawler.py"", line 97, in <module>
    plt.savefig(saveto)
  File ""/opt/local/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/matplotlib/pyplot.py"", line 561, in savefig
    return fig.savefig(*args, **kwargs)
  File ""/opt/local/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/matplotlib/figure.py"", line 1421, in savefig
    self.canvas.print_figure(*args, **kwargs)
  File ""/opt/local/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/matplotlib/backend_bases.py"", line 2220, in print_figure
    **kwargs)
  File ""/opt/local/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/matplotlib/backends/backend_agg.py"", line 505, in print_png
    FigureCanvasAgg.draw(self)
  File ""/opt/local/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/matplotlib/backends/backend_agg.py"", line 451, in draw
    self.figure.draw(self.renderer)
  File ""/opt/local/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/matplotlib/artist.py"", line 55, in draw_wrapper
    draw(artist, renderer, *args, **kwargs)
  File ""/opt/local/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/matplotlib/figure.py"", line 1034, in draw
    func(*args)
  File ""/opt/local/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/matplotlib/artist.py"", line 55, in draw_wrapper
    draw(artist, renderer, *args, **kwargs)
  File ""/opt/local/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/matplotlib/axes.py"", line 2086, in draw
    a.draw(renderer)
  File ""/opt/local/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/matplotlib/artist.py"", line 55, in draw_wrapper
    draw(artist, renderer, *args, **kwargs)
  File ""/opt/local/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/matplotlib/lines.py"", line 562, in draw
    drawFunc(renderer, gc, tpath, affine.frozen())
  File ""/opt/local/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/matplotlib/lines.py"", line 938, in _draw_lines
    self._lineFunc(renderer, gc, path, trans)
  File ""/opt/local/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/matplotlib/lines.py"", line 978, in _draw_solid
    renderer.draw_path(gc, path, trans)
  File ""/opt/local/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/matplotlib/backends/backend_agg.py"", line 145, in draw_path
    self._renderer.draw_path(gc, path, transform, rgbFace)
OverflowError: Allocated too many blocks

I specified the agg.path.chunksize parameter in matplotlibrc to o agg.path.chunksize : 10000000. I only get this error when I run the -f fft option. How can I prevent this error?
","Not sure but try pdf, svg  backends
#!/usr/bin/env python
from pandas import *
import matplotlib.pyplot as plt # Insert just before import matplotlib as mpl
mpl.use('pdf')                  # Insert just before import matplotlib as mpl
import matplotlib as mpl

# ['pdf', 'pgf', 'Qt4Agg', 'GTK', 'GTKAgg', 'ps', 'agg', 
# 'cairo', 'MacOSX', 'GTKCairo', 'WXAgg', 'template', 'TkAgg', 
# 'GTK3Cairo', 'GTK3Agg', 'svg', 'WebAgg', 'CocoaAgg', 'emf', 'gdk', 'WX']

# (...)
plt.savefig('svg.pdf') # Consider file extension (!)

",matplotlib
userwarning figurecanvasagg is noninteractive and thus cannot be shown pltshow,"I am using

Windows 10
PyCharm 2021.3.3 Professional Edition
python 3.11.5
matplotlib 3.8.1

How can I permanently resolve this issue in my development environment?
import numpy as np
import matplotlib
matplotlib.use('Agg')
import matplotlib.pyplot as plt

# Read data from file, skipping the first row (header)
data = np.loadtxt('cm.dat', skiprows=1)

# Initialize reference point
x0, y0, z0 = data[0]

# Compute squared displacement for each time step
SD = [(x - x0)**2 + (y - y0)**2 + (z - z0)**2 for x, y, z in data]

# Compute the cumulative average of SD to get MSD at each time step
MSD = np.cumsum(SD) / np.arange(1, len(SD) + 1)

# Generate time steps
t = np.arange(1, len(SD) + 1)

# Create a log-log plot of MSD versus t
plt.figure(figsize=(8, 6))
plt.loglog(t, MSD, marker='o')

plt.title('Mean Squared Displacement vs Time')
plt.xlabel('Time step')
plt.ylabel('MSD')
plt.grid(True, which=""both"", ls=""--"")
plt.show()

C:\Users\pc\AppData\Local\Programs\Python\Python311\python.exe C:/git/RouseModel/tau_plot.py
C:\git\RouseModel\tau_plot.py:29: UserWarning: FigureCanvasAgg is non-interactive, and thus cannot be shown
  plt.show()

Process finished with exit code 0


","I have the same issue. In my case, I installed the PyQt5==5.15.10. After that, I run my code successfully.
pip install PyQt5==5.15.10 or pip install PyQt5 with python==3.11
But from 2024, you guys should install version PyQt6 or the last version with python==3.12 or later.
",matplotlib
matplotlib x axis not as expected,"I want the x-axis labels to be exactly as in the file, but it's converting them.  Also, I don't want the thick black line above the labels.  And, I'd like the plot to extend fully to both sides of the area without the empty spaces on the left and right.
plot:

python script:
#!/usr/bin/python3
import matplotlib.pyplot as plt
import numpy as np

a,b = np.genfromtxt(""test01.txt"", usecols=(0,1), unpack=True, delimiter=""\t"", dtype='str')
y = list(map(float, b))

plt.figure(figsize=(9, 5)) 
plt.plot(a, y, linewidth=0.7)
xticks = plt.xticks()[0]
xtick_labels = ["""" if i % 100 != 0 else x for i, x in enumerate(xticks)]
plt.xticks(xticks, xtick_labels, fontsize=8)
plt.xticks(rotation=90)
plt.yticks(np.arange(100, 185, 5))
plt.ylim(110, 185)
plt.xlabel(""Time"")
plt.ylabel(""Temp in F"")
plt.show()

sample data from the file:
00:00:02    170.9
00:00:03    171.7
00:00:04    171.9
00:00:04    171.8
00:00:05    171.4
00:00:06    170.9
00:00:07    170.1
00:00:08    169.4
00:00:09    168.5
00:00:10    167.6

","
If what you want to do is to use the strings in the file as the labels of the x-ticks, you have to proceed carefully:

save the (too many) positions of the x-ticks and rhe corresponding labels in two lists, positions and texts
remove the x-ticks completely
construct two lists p and t to store the positions and the texts that we want on the final drawing
using p and t, place the selected pairs on the final drawing

Eventually, rotate the labels and prescribe a tight layout, otherwise the labels are truncated.
import matplotlib.pyplot as plt
import numpy as np

data = '''\
00:00:02    170.9
00:00:03    171.7
00:00:04    171.9
00:00:04    171.8
00:00:05    171.4
00:00:06    170.9
00:00:07    170.1
00:00:08    169.4
00:00:09    168.5
00:00:10    167.6'''

x, y = zip(*[line.split() for line in data.split('\n')])
y = [float(s) for s in y]

plt.plot(x, y)

# this is the interesting part
# save the x ticks
positions, texts = plt.xticks()
# remove the x ticks
plt.xticks([],[])
# select the ticks we want from the saved ones
p, t = [], []
for k in range(len(positions)):
    if not k%3:
        p.append(positions[k])
        t.append(texts[k])
# set the ticks according to our preference
plt.xticks(p, t)
# end of the interesting part
plt.xticks(rotation=+90)
plt.tight_layout()
plt.show()

",matplotlib
funcanimation only shows one frame,"I am trying to implement a genetic algorithm in Python and to display the result of my algorithm I wanted to create an animation using FuncAnimation from matplotlib.animation.
However, I ran into the problem that I ended up with only one image. The most interesting thing is that in Google Colab I get one frame, but in Pycharm (the IDE I usually use) I get another one.
Here is my code:
import numpy as np
import matplotlib.pyplot as plt
from matplotlib.animation import FuncAnimation

def fitness_function(x, y):
    return x ** 2 + y ** 2

def initialize_population(pop_size, bounds):
    return np.random.uniform(bounds[0], bounds[1], (pop_size, 2))

def select(population, fitnesses):
    indices = np.argsort(fitnesses)[:len(population) // 2]
    return population[indices]

def crossover(parents, offspring_size):
    offsprings = []
    for _ in range(offspring_size):
        p1, p2 = parents[np.random.choice(len(parents), size=2, replace=False)]
        alpha = np.random.rand()
        child = alpha * p1 + (1 - alpha) * p2
        offsprings.append(child)
    return np.array(offsprings)

def mutate(population, bounds, mutation_rate=0.1):
    for i in range(len(population)):
        if np.random.rand() < mutation_rate:
            population[i] += np.random.uniform(-1, 1, size=2)
            population[i] = np.clip(population[i], bounds[0], bounds[1])
    return population

def genetic_algorithm(pop_size=500, generations=500, bounds=(-10, 10)):
    population = initialize_population(pop_size, bounds)
    history = []

    for gen in range(generations):
        fitnesses = np.array([fitness_function(x, y) for x, y in population])
        parents = select(population, fitnesses)
        offspring_size = pop_size - len(parents)
        offspring = crossover(parents, offspring_size)
        population = np.vstack((parents, offspring))
        population = mutate(population, bounds)
        history.append(population.copy())

        if gen % 50 == 0:
            print(f""Generation {gen}: Best fitness = {fitnesses.min():.4f}"")

    return history

history = genetic_algorithm()

fig, ax = plt.subplots(figsize=(8, 8))
ax.set_xlim(-10, 10)
ax.set_ylim(-10, 10)
ax.set_title(""Evolution of Genetic Algorithm Population"")
ax.set_xlabel(""X"")
ax.set_ylabel(""Y"")

colors = np.linspace(0, 1, len(history))

scat = ax.scatter([], [], c=[], cmap='viridis', s=50, vmin=0, vmax=1)

ax.plot([0, 0], [-10, 10], color='blue', linewidth=1)
ax.plot([-10, 10], [0, 0], color='blue', linewidth=1)

def update(frame):
    print(f""Frame: {frame}, History Length: {len(history)}"")
    data = history[frame]
    scat.set_offsets(data)
    scat.set_array(np.full(len(data), colors[frame]))
    ax.set_title(f""Generation {frame+1}/{len(history)}"")
    return scat,

anim = FuncAnimation(fig, update, frames=len(history), interval=50, repeat=False)
plt.show()
print(plt.get_backend())

print(f""Number of generations: {len(history)}"")


I've read a lot of similar questions, but none of the answers worked for me.
For example, I ran the code that is marked as the required answer in this question, however I still only got one frame.
If I just haven't found a question that has an answer to mine, I apologize in advance.
Also, if you suddenly notice any mistakes in the algorithm, I would be grateful if you could point them out to me.
I'm using:
Python 3.9.13
matplotlib 3.9.3
numpy 2.0.2
","I ran your code on my own PC and It worked just fine. I don't really know why It doesn't work for you in Pycharm, maybe some environmental version difference in packages? I'm not sure your the one who should find that out yourself but I have this answer for you:
I ran the code on google colab and It showed 1 frame only like you said. What you should do instead is to use a method called ""save"" for anim to save your animation as .gif and then it will render the frames and saves your animation you can just double click on it to play it on google colab or show it with ""IPython.display"".
Here is a version of your code to use in google colab:
import numpy as np
import matplotlib.pyplot as plt
from matplotlib.animation import FuncAnimation

def fitness_function(x, y):
    return x ** 2 + y ** 2

def initialize_population(pop_size, bounds):
    return np.random.uniform(bounds[0], bounds[1], (pop_size, 2))

def select(population, fitnesses):
    indices = np.argsort(fitnesses)[:len(population) // 2]
    return population[indices]

def crossover(parents, offspring_size):
    offsprings = []
    for _ in range(offspring_size):
        p1, p2 = parents[np.random.choice(len(parents), size=2, replace=False)]
        alpha = np.random.rand()
        child = alpha * p1 + (1 - alpha) * p2
        offsprings.append(child)
    return np.array(offsprings)

def mutate(population, bounds, mutation_rate=0.1):
    for i in range(len(population)):
        if np.random.rand() < mutation_rate:
            population[i] += np.random.uniform(-1, 1, size=2)
            population[i] = np.clip(population[i], bounds[0], bounds[1])
    return population

def genetic_algorithm(pop_size=500, generations=500, bounds=(-10, 10)):
    population = initialize_population(pop_size, bounds)
    history = []

    for gen in range(generations):
        fitnesses = np.array([fitness_function(x, y) for x, y in population])
        parents = select(population, fitnesses)
        offspring_size = pop_size - len(parents)
        offspring = crossover(parents, offspring_size)
        population = np.vstack((parents, offspring))
        population = mutate(population, bounds)
        history.append(population.copy())

        if gen % 50 == 0:
            print(f""Generation {gen}: Best fitness = {fitnesses.min():.4f}"")

    return history

history = genetic_algorithm()

fig, ax = plt.subplots(figsize=(8, 8))
ax.set_xlim(-10, 10)
ax.set_ylim(-10, 10)
ax.set_title(""Evolution of Genetic Algorithm Population"")
ax.set_xlabel(""X"")
ax.set_ylabel(""Y"")

colors = np.linspace(0, 1, len(history))

scat = ax.scatter([], [], c=[], cmap='viridis', s=50, vmin=0, vmax=1)

ax.plot([0, 0], [-10, 10], color='blue', linewidth=1)
ax.plot([-10, 10], [0, 0], color='blue', linewidth=1)

def update(frame):
    print(f""Frame: {frame}, History Length: {len(history)}"")
    data = history[frame]
    scat.set_offsets(data)
    scat.set_array(np.full(len(data), colors[frame]))
    ax.set_title(f""Generation {frame+1}/{len(history)}"")
    return scat,

anim = FuncAnimation(fig, update, frames=len(history), interval=50, repeat=False)
plt.show()
anim.save('myanim.gif', writer='pillow')
print(plt.get_backend())

print(f""Number of generations: {len(history)}"")

import IPython.display as display
display.Image(open('myanim.gif','rb').read())

Let me know if this helps you or not.
",matplotlib
python  remove borders from charts and legend,"I have the following plot:
dfA.plot.bar(stacked=True, color=[colorDict.get(x, '#333333') for x in 
dfA.columns],figsize=(10,8))
plt.legend(loc='upper right', bbox_to_anchor=(1.4, 1))

Which displays this:

I want to remove all of the borders of the chart and legend i.e. the box around the chart (leaving the axis numbers like 2015 and 6000 etc)
All of the examples I find refer to spines and 'ax', however I have not built my chart using fig = plt.figure() etc.
Anyone know how to do it?
","You can remove the border of the legend by using the argument frameon=False in the call to plt.legend().
If you only have one figure and axes active, then you can use plt.gca() to get the current axes. Alternatively df.plot.bar returns an axes object (which I would suggest using because plt.gca() might get confusing when working with multiple figures). Therefore you can set the visibility of the spines to False:
ax = dfA.plot.bar(stacked=True, color=[colorDict.get(x, '#333333') for x in 
dfA.columns],figsize=(10,8))
plt.legend(loc='upper right', bbox_to_anchor=(1.4, 1), frameon=False)

for spine in ax.spines:
    ax.spines[spine].set_visible(False)

    # Color of the spines can also be set to none, suggested in the comments by ScoutEU 
    # ax.spines[spine].set_color(""None"")

",matplotlib
plotting quiver plots in matplotlib,"I want to plot the slope field for: 0.5*sin(0.5*pi*x)*sqrt(y+7)
import numpy as np
import matplotlib.pyplot as plt

# Specify the grid of dots 
x = np.arange(-3,3,0.3) 
y = np.arange(-2,4,0.3)
X, Y = np.meshgrid(x,y)

# Create unit vectors at each dot with correct slope
dy = 0.5*(np.sin(x*np.pi*0.5))*np.sqrt(y+7)
dx = np.ones(dy.shape)
norm = np.sqrt(X**2 + Y**2) 
dyu = dy/norm
dxu = dx/norm

# Plot everything 
plt.quiver(X,Y,dxu,dyu,)
plt.show()

and I got the second image below. I am trying to replicate the first image. How can I make uniform slope lines like that? Why am I getting variable-length lines anyway?


","So, the first thing to do is remove the arrowheads, which @jasonharper shows in the comments can be done by adding these options to your quiver call: headwidth=0, headlength=0, headaxislength=0.
Next is to deal with the length. You're currently normalizing by X and Y when you should be normalizing by dx and dy. I would actually redefine dx and dy to use X and Y  so they have the same shapes. And you will also want to make the pivot=""mid"" change that @jasonharper mentioned in their comment. With these changes, your code would look like this:
import numpy as np
import matplotlib.pyplot as plt

plt.close(""all"")

x = np.arange(-3, 3, 0.3) 
y = np.arange(-2, 4, 0.3)
X, Y = np.meshgrid(x, y)

dX = np.ones_like(Y)
dY = 0.5*np.sin(X*np.pi*0.5)*np.sqrt(Y+7)
norm = np.sqrt(dX**2 + dY**2)
dXu = dX/norm
dYu = dY/norm

plt.quiver(X, Y, dXu, dYu,
           headwidth=0, headlength=0, headaxislength=0, 
           pivot=""mid"")
plt.show()

Result:


To control the length, you'll need to change two things. The first thing would be to multiply dX and dY by the desired length. The second thing would be to add the following options to quiver: angles=""xy"", scale_units=""xy"", scale=1 to make sure the vectors are the desired length.
import numpy as np
import matplotlib.pyplot as plt

plt.close(""all"")

x = np.arange(-3, 3, 0.3) 
y = np.arange(-2, 4, 0.3)
X, Y = np.meshgrid(x, y)

dX = np.ones_like(Y)
dY = 0.5*np.sin(X*np.pi*0.5)*np.sqrt(Y+7)
norm = np.sqrt(dX**2 + dY**2)
dXu = dX/norm
dYu = dY/norm

L= 0.2
dXs = L*dXu
dYs = L*dYu

plt.quiver(X, Y, dXs, dYs,
           headwidth=0, headlength=0, headaxislength=0, 
           angles=""xy"", scale_units=""xy"", scale=1,
           pivot=""mid"")
plt.show()

",matplotlib
changing the y axis in matplotlib,"How can you change the y-axis in matplotlib such that you have specified ticks ([0, -1, -8, -44, -244]) but they are evenly spread over the y-axis (the gap between 0 and -1 is just as big as the gap between -44 and -244 if you visualise the figure). So you do not have an linear distribution on the y-axis but it makes you plot look better. The distance between the ticks should be equal and this makes your graph look better.
I tried to use the normal yticks first. I also tried the method where you use ax.set_yticks and ax.set_ytickslabels. However, it did not give me the wanted result.
","It would be great to have a minimally working example to see what kind of data/plot are you working with. But generally speaking, it sounds like you need to transform your data/axis, rather than just selecting custom ticks.
You could do this with a custom scale in matplotlib. Below, I have used interpolation to force the [-244, -44, -8, -1, 0] to have equally spaced values (in this case [0..4]). The advantage of using a scaling function rather than scaling your input data is that the yticks, etc. can be specified in the original scale.
I've assumed that you really need a particular set of values to be equally spaced but you should also checkout other (simpler) scaling methods like set_yscale('log') if they make your plots look better.
import matplotlib.pyplot as plt
import numpy as np

# Sample data
x = np.linspace(0, 10, 100)
y = 255*(np.sin(x)/2 - 0.5)

# Define the values that need to be equally spaced
y_scale = [-244, -44, -8, -1, 0]
y_scale_vals = np.arange(len(y_scale))

# Interpolate such that -244 -> 0, -44 -> 1, -8 -> 2, -1 -> 3, 0 -> 4
def forward(y):
    return np.interp(y, y_scale, y_scale_vals)
def inverse(y):
    return np.interp(y, y_scale_vals, y_scale)

fig, ax = plt.subplots(1,2, layout='constrained')
ax[0].plot(x, y)
ax[0].set_title('Linear scale')

ax[1].plot(x, y)
ax[1].set_yscale('function', functions=(forward, inverse))
ax[1].yaxis.set_ticks(y_scale)
ax[1].set_title('Custom scale')


",matplotlib
matplotlib savefig background always transparent,"Problem
I cannot seem to get savefig() to actually save a PNG file without a transparent figure background.
This is having read and tried all the suggestions previously posted, answered, cursed about and also going through the API docs a bunch of times.  I've read it all, but still can't get non-transparent figure faces
Background
I'm using matplotlib and savefig to create a PNG file.  (env: macos - latest anaconda modules using PY 3.7).
I am trying this out of jupyter however - so hopefully it's not something completely screwed up with only how ipython in jupyter does it - though I don't see how that could be the case
I did read through the previous many posts about the (confusing as hell) nature of savefig doing it's own thing with backgrounds, and did/tried everything as suggested (and as written in the latest savefig api docs).
In particular, I've tried all the following without sucess:

specifying facecolor in the savefig() call (with/without transparency)
savefig.facecolor: white in the style mpl file I'm using

When savefig'ing my figure background is always transparent.

Can anyone tell me what the !@#$!# I'm missing here???
Code
Here's what I''m using, which spits out figure with transparent background, regardless of what I do.
In particular the 2nd call below (with savefig(..., transparent=False)) will make the axes not transparent - but the figure itself is still transparent!)
import numpy as np
import matplotlib as mpl
import matplotlib.style as style

a = np.array([-3.2, 0.1, 1.5, 3.3, 8.5])
b = np.array([1.1, 1.8, 1.95, 2.3, 4.3])
labels = ['a', 'bc', 'def', 'g', 'ggghhh']

stylefile = './util/plot_config/aqs_default.mplstyle'
# the file above does contain an entry of:
# savefig.facecolor: white
#
to_res = 1024
dpi = 100
inches = (to_res/dpi, to_res/dpi)

style.use(stylefile)
%matplotlib   

fig = mpl.figure.Figure(figsize=inches, dpi=dpi, facecolor='white')
ax = fig.subplots()

for x, y, l in zip(a,b,labels):
    ax.scatter(x,y,label=l)
ax.legend()
ax.set_xlabel('Some x')
ax.set_ylabel('Attenuation $\mu$ (cm$^{-1}$)')

ax.set_title('blah', y=1.03)
fig.suptitle('Linearity $\mu$')

# for me, _both_ calls below result in the figure having a transparent background:

fig.savefig('a.png', facecolor=fig.get_facecolor(), transparent=True)
fig.savefig('b.png', facecolor=fig.get_facecolor(), transparent=False)

","Unfortunately, it seems that frameon is not supported anymore as of matplotlib 3.3.
I solved the transparency issue by setting facecolor='white', transparent=False options in savefig()
",matplotlib
why is the plot of the refractive index wavelengthdependent fresnel39s equation not showing as expected,"I want to reproduce the reflectance spectrum of a thin film whose complex refractive index is wavelength-dependent (the complex refractive index data, named N2 in code, can be obtained from here).
Using fresnel's equations  for medium 1: air, medium 2: thin film, and medium 3: air :
  ; 
The reflection coefficient   
where \delta is defined as 
And finally the reflectance equation 
Since the refractive indices have a dependence on the wavelength, then the refraction angle   have the same dependence, using the Snell equation: 
I wrote the following code:
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

## Function definition
def Ang_Refrac(na,nb,angle_a):
ang_refrac = np.arcsin( (na/nb).real * np.sin(angle_a) )
return ang_refrac

# Data
Thin_film = pd.read_csv(""Si_refractive index.txt"", delimiter='\t')

## Variable definition

# thin film thickness, [d]: nm
d = 200

# Wave-length
lamb = Thin_film[Thin_film.columns[0]]

# Complex refractive index
N1 = np.ones(len(lamb))
N2 = Thin_film[Thin_film.columns[1]] + Thin_film[Thin_film.columns[1]]*1j
N3 = np.ones(len(lamb))

# Angle:
ang_1_s = 0  #sexagesimal

ang_1 = ang_1_s*np.pi/180  #radians
ang_2 = [] #radians
ang_3 = [] #radians
for i in range(len(N2)):
    ang_refrac_12 = Ang_Refrac(N1[i],N2[i],ang_1)
    ang_refrac_23 = Ang_Refrac(N2[i],N3[i],ang_refrac_12)
    ang_2.append(ang_refrac_12)
    ang_3.append(ang_refrac_23)

## Reflectance
R_s = np.zeros(len(lamb))
R_p = np.zeros(len(lamb))

for i in range(len(lamb)):
    # S-type polarization
    r12_s = (N1[i]*np.cos(ang_1) - N2[i]*np.cos(ang_2[i]))/(N1[i]*np.cos(ang_1) + N2[i]*np.cos(ang_2[i]))
    r23_s = (N2[i]*np.cos(ang_2[i]) - N3[i]*np.cos(ang_3[i]))/(N2[i]*np.cos(ang_2[i]) + N3[i]*np.cos(ang_3[i]))

    # P-type polarization
    r12_p = (N2[i]*np.cos(ang_1) - N1[i]*np.cos(ang_2[i])) / (N2[i]*np.cos(ang_1) + N1[i]*np.cos(ang_2[i]))
    r23_p = (N3[i]*np.cos(ang_2[i]) - N2[i]*np.cos(ang_3[i])) / (N3[i]*np.cos(ang_2[i]) + N2[i]*np.cos(ang_3[i]))

    # Phase shift
    delta = 2 * np.pi * (1/lamb[i]) * d * np.sqrt(abs(N2[i])**2 - (np.sin(ang_1)**2))

    # Reflection coefficient
    r123_s = (r12_s + r23_s*np.exp(2j*delta)) / (1 - r12_s*r23_s*np.exp(2j*delta))
    r123_p = (r12_p + r23_p*np.exp(2j*delta)) / (1 - r12_p*r23_p*np.exp(2j*delta))

    # Reflectance
    R_s[i] = abs(r123_s)**2
    R_p[i] = abs(r123_p)**2


# Reflectance normalization
R_s_normalized = (R_s - min(R_s)) / (max(R_s) - min(R_s))
R_p_normalized = (R_p - min(R_p)) / (max(R_p) - min(R_p))

# Plotting
plt.title(""Refletância $\phi="" + str(ang_1_s) + ""$°"")
plt.xlim(300, 1000)  # Definindo os limites do eixo x
plt.plot(lamb, R_s_normalized, 'go', label=""Polarização S"", markersize=4)
plt.plot(lamb, R_p_normalized, 'b-', label=""Polarização P"")
plt.legend()
plt.show()

Resulting graph:

Expected graph: Obtained from here

Why are the results not the same?
","You can try this. The major changes are:

N2 is given by n + i.k, rather than n + i.n as you had

there is just N2 ** 2 in the expression for delta, not abs(N2)**2, which would lose an important complex part

I think the reflection coefficients should have a '+', not a '-' sign in the denominator (though I could be wrong).


I found a little bit of background (and a few typos) in https://ir.lib.nycu.edu.tw/bitstream/11536/27329/1/000186822000019.pdf
Code:
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

## Function definition
def Ang_Refrac(na,nb,angle_a):
    ang_refrac = np.arcsin( (na/nb) * np.sin(angle_a) )
    return ang_refrac

# Data
Thin_film = pd.read_csv(""Si_refractive index.txt"", delimiter='\t')

d = 200                                         # thin film thickness, [d]: nm
lamb = Thin_film[Thin_film.columns[0]]          # Wave-length

# Complex refractive index       n + ik
num = len(lamb)
N1 = np.ones(num)
N2 = Thin_film[Thin_film.columns[1]] + Thin_film[Thin_film.columns[2]]*1j      # <=========== NOTE ======
N3 = np.ones(num)

# Angle:
ang_1_s = 0
ang_1 = ang_1_s * np.pi / 180
ang_2 = [] #radians
ang_3 = [] #radians
for i in range(num):
    ang_refrac_12 = Ang_Refrac(N1[i],N2[i],ang_1)
    ang_refrac_23 = Ang_Refrac(N2[i],N3[i],ang_refrac_12)
    ang_2.append(ang_refrac_12)
    ang_3.append(ang_refrac_23)

## Reflectance
R_s = np.zeros(num)
R_p = np.zeros(num)

for i in range(num):
    # S-type polarization
    r12_s = (N1[i]*np.cos(ang_1) - N2[i]*np.cos(ang_2[i]))/(N1[i]*np.cos(ang_1) + N2[i]*np.cos(ang_2[i]))
    r23_s = (N2[i]*np.cos(ang_2[i]) - N3[i]*np.cos(ang_3[i]))/(N2[i]*np.cos(ang_2[i]) + N3[i]*np.cos(ang_3[i]))

    # P-type polarization
    r12_p = (N2[i]*np.cos(ang_1) - N1[i]*np.cos(ang_2[i])) / (N2[i]*np.cos(ang_1) + N1[i]*np.cos(ang_2[i]))
    r23_p = (N3[i]*np.cos(ang_2[i]) - N2[i]*np.cos(ang_3[i])) / (N3[i]*np.cos(ang_2[i]) + N2[i]*np.cos(ang_3[i]))

    # Phase shift
    delta = 2 * np.pi * ( d / lamb[i] ) * np.sqrt( N2[i]**2 - np.sin(ang_1)**2)       # <====== NOTE ====

    # Reflection coefficient
    r123_s = (r12_s + r23_s*np.exp(2j*delta)) / (1 + r12_s*r23_s*np.exp(2j*delta))    # <====== NOTE ====
    r123_p = (r12_p + r23_p*np.exp(2j*delta)) / (1 + r12_p*r23_p*np.exp(2j*delta))    # <====== NOTE ====

    # Reflectance
    R_s[i] = abs( r123_s ) ** 2
    R_p[i] = abs( r123_p ) ** 2


# Reflectance normalization
R_s_normalized = ( R_s - min( R_s ) ) / ( max( R_s ) - min( R_s ) )
R_p_normalized = ( R_p - min( R_p ) ) / ( max( R_p ) - min( R_p ) )

# Plotting
plt.title( r""Refletancia $\phi="" + str(ang_1_s) + ""$ deg"")
plt.xlim(300, 1000)  # Definindo os limites do eixo x
plt.plot(lamb, R_s_normalized, 'go', label=""Polarizacao S"", markersize=4)
plt.plot(lamb, R_p_normalized, 'b-', label=""Polarizacao P"")
plt.legend()
plt.show()

Output (note: some systematic difference in the normalisation from your ""expected"")

",matplotlib
interactive large plot with 20 million sample points and gigabytes of data,"I have got a problem (with my RAM) here: it's not able to hold the data I want to plot. I do have sufficient HD space. Is there any solution to avoid that ""shadowing"" of my data-set?
Concretely I deal with Digital Signal Processing and I have to use a high sample-rate. My framework (GNU Radio) saves the values (to avoid using too much disk space) in binary. I unpack it. Afterwards I need to plot. I need the plot zoomable, and interactive. And that is an issue. 
Is there any optimization potential to this, or another software/programming language (like R or so) which can handle larger data-sets? Actually I want much more data in my plots. But I have no experience with other software. GNUplot fails, with a similar approach to the following. I don't know R (jet).
import matplotlib.pyplot as plt
import matplotlib.cbook as cbook
import struct

""""""
plots a cfile

cfile - IEEE single-precision (4-byte) floats, IQ pairs, binary
txt - index,in-phase,quadrature in plaintext

note: directly plotting with numpy results into shadowed functions
""""""

# unpacking the cfile dataset
def unpack_set(input_filename, output_filename):
    index = 0   # index of the samples
    output_filename = open(output_filename, 'wb')

    with open(input_filename, ""rb"") as f:

        byte = f.read(4)    # read 1. column of the vector

        while byte != """":
        # stored Bit Values
            floati = struct.unpack('f', byte)   # write value of 1. column to a variable
            byte = f.read(4)            # read 2. column of the vector
            floatq = struct.unpack('f', byte)   # write value of 2. column to a variable
            byte = f.read(4)            # next row of the vector and read 1. column
            # delimeter format for matplotlib 
            lines = [""%d,"" % index, format(floati), "","",  format(floatq), ""\n""]
            output_filename.writelines(lines)
            index = index + 1
    output_filename.close
    return output_filename.name

# reformats output (precision configuration here)
def format(value):
    return ""%.8f"" % value            

# start
def main():

    # specify path
    unpacked_file = unpack_set(""test01.cfile"", ""test01.txt"")
    # pass file reference to matplotlib
    fname = str(unpacked_file)
    plt.plotfile(fname, cols=(0,1)) # index vs. in-phase

    # optional
    # plt.axes([0, 0.5, 0, 100000]) # for 100k samples
    plt.grid(True)
    plt.title(""Signal-Diagram"")
    plt.xlabel(""Sample"")
    plt.ylabel(""In-Phase"")

    plt.show();

if __name__ == ""__main__"":
    main()

Something like plt.swap_on_disk() could cache the stuff on my SSD ;)
","So your data isn't that big, and the fact that you're having trouble plotting it points to issues with the tools.   Matplotlib has lots of options and the output is fine, but it's a huge memory hog and it fundamentally assumes your data is small.  But there are other options out there.
So as an example, I generated a 20M data-point file 'bigdata.bin' using the following:
#!/usr/bin/env python
import numpy
import scipy.io.numpyio

npts=20000000
filename='bigdata.bin'

def main():
    data = (numpy.random.uniform(0,1,(npts,3))).astype(numpy.float32)
    data[:,2] = 0.1*data[:,2]+numpy.exp(-((data[:,1]-0.5)**2.)/(0.25**2))
    fd = open(filename,'wb')
    scipy.io.numpyio.fwrite(fd,data.size,data)
    fd.close()

if __name__ == ""__main__"":
    main()

This generates a file of size ~229MB, which isn't all that big; but you've expressed that you'd like to go to even larger files, so you'll hit memory limits eventually.
Let's concentrate on non-interactive plots first.  The first thing to realize is that vector plots with glyphs at each point are going to be a disaster -- for each of the 20 M points, most of which are going to overlap anyway, trying to render little crosses or circles or something is going to be a diaster, generating huge files and taking tonnes of time.   This, I think is what is sinking matplotlib by default.   
Gnuplot has no trouble dealing with this:
gnuplot> set term png
gnuplot> set output 'foo.png'
gnuplot> plot 'bigdata.bin' binary format=""%3float32"" using 2:3 with dots


And even Matplotlib can be made to behave with some caution (choosing a raster back end, and using pixels to mark points):
#!/usr/bin/env python
import numpy
import matplotlib
matplotlib.use('Agg')
import matplotlib.pyplot as plt

datatype=[('index',numpy.float32), ('floati',numpy.float32), 
        ('floatq',numpy.float32)]
filename='bigdata.bin'

def main():
    data = numpy.memmap(filename, datatype, 'r') 
    plt.plot(data['floati'],data['floatq'],'r,')
    plt.grid(True)
    plt.title(""Signal-Diagram"")
    plt.xlabel(""Sample"")
    plt.ylabel(""In-Phase"")
    plt.savefig('foo2.png')

if __name__ == ""__main__"":
    main()  


Now, if you want interactive, you're going to have to bin the data to plot, and zoom in on the fly.   I don't know of any python tools that will help you do this offhand.  
On the other hand, plotting-big-data is a pretty common task, and there are tools that are up for the job.   Paraview is my personal favourite, and VisIt is another one.  They both are mainly for 3D data, but Paraview in particular does 2d as well, and is very interactive (and even has a Python scripting interface).  The only trick will be to write the data into a file format that Paraview can easily read.
",matplotlib
explain why numpy should not be imported from source directory,"Disclaimer of research:
I have examined the following other StackOverflow questions:

How to import numpy in python shell
How can I use numpy without installing it?
Import a module from a relative path

Perhaps to some, those may answer my question, but according to my knowledge, I still do not understand the situation.
I am trying to import numpy so that matplotlib will work, but upon execution of the __init__.py file in the numpy folder, the following error message is displayed:
ImportError: Error importing numpy: you should not try to import numpy from
    its source directory; please exit the numpy source tree, and relaunch
    your python intepreter from there.

Explain what it means to import something from its source directory as opposed to some other way of importing it. Does it mean that it should not be source code when it is imported? Or does it mean that it literally is just the wrong directory/folder that I am importing. I know that one other StackOverflow answer is:

The message is fairly self-explanatory; your working directory should not be the numpy source directory when you invoke Python; numpy should be installed and your working directory should be anything but the directory where it lives.

However, I don't understand this. Aren't you supposed to import things that you want to work with? I'm assuming that the import command combines the source directory into your current working directory in this statement. 
I also read the other answers such as:

Using distutils to install local directories
Using virtualenv to create a virtual system directory
Using Enthought's EPD to have numpy pre-installed in what I believe to be the system directory,
and
Using a command like $ dpkg -i --force-not-root --root=$HOME mypackagename.deb to create what I believe is some kind of sub-system directory that is treated like a system directory.

So, correct me if I'm wrong, but does numpy somehow strongly require to be somehow installed in the main system directory?
Machine status:
I am using Windows machines without administrative privlidges.
They have Python 3.3 Shell as well as matplotlib installed.
When running command prompt, python and python3 are not recognized. I have to run the Python shell from the applications menu.
I can successfull begin importing matplotlib from even my own directory, different from theirs, but it stops upon reaching __init__.py of the numpy module, if it exists and reports the error stated above.
Update:
Luckily, my administrators were able to directly install numpy correctly in the site-packages folder. Thank you for answering my question though. I understand the situation a lot more because of you.
","numpy includes extension modules written in C. You will need to build these extension modules before the numpy package is complete. The most robust way to do this is to build it and install it to site-packages like normal. You can also install it to another directory using the standard distutils options for this. However, once you have installed it, you should change your directory out of the source tree. Python starts looking for packages in your current directory, so the presence of the incomplete numpy package (without the necessary built C extension modules) will be picked up first and lead to the error that message that you quote. This happens a lot, so we give a long message explaining what to do.
",matplotlib
attributeerror module 39matplotlib39 has no attribute 39scatter39,"I'm trying to make cluster of latitude and longitude.
the code gave an error in plt.scatter(data['Lng'],data['Lat']) line
the error is:
AttributeError: module 'matplotlib' has no attribute 'scatter' 

code:
import numpy as np
import pandas as pd
import matplotlib as plt
import seaborn as sns
sns.set()
from sklearn.cluster import KMeans
data = pd.read_csv(""pk.csv"") 
data.head()
lat_long = data.drop(['country', 'iso2','admin', 'capital','population', 
'population_proper'] , axis = 1)
lat_long.head()
plt.scatter(data['Lng'],data['Lat']) # error here

","It should be:
import matplotlib.pyplot as plt

",matplotlib
value error can only convert an array of size 1 to a python scalar,"I want to plot a heatmap centered around a value of 1 showing the expression of genes corresponding to cell lines using this as code:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt   
from matplotlib.colors import TwoSlopeNorm
import seaborn as sns

data = pd.read_csv(r""C:\Users\Hbeen\Desktop\aGPCR Pancreas.csv"", header=1, index_col=0)

fig, ax = plt.subplots()
rdgn = sns.diverging_palette(h_neg=130, h_pos=10, s=99, l=55, sep=3, as_cmap=True)
divnorm = TwoSlopeNorm (vmin=data.min(), vcenter=1.00, vmax=data.max())
sns.heatmap(data, cmap='coolwarm', norm=divnorm, fmt ='.0%', 
        linewidths=0.5, linecolor='black', cbar=True, ax=ax)

plt.title(""Expression von AGPCRs in Pankreas Zelllinien"")
plt.xlabel(""Gene"")
plt.ylabel(""Zelllinien"")
plt.xticks(rotation=90, fontsize=6)
plt.yticks(fontsize=6)
plt.show()

Everytime I run it it throws a ValueError which I don't know how to fix as I am completly new to coding. Seeing as I have no idea why this error pops up I have no idea how to even begin to fix it. I tried looking for answers on this site but none included pandas in the problem so they weren't that helpful for me.
runfile('C:/Users/Hbeen/Desktop/unbenannt0.py', wdir='C:/Users/Hbeen/Desktop')
Traceback (most recent call last):

  File ~\anaconda3\Lib\site-packages\spyder_kernels\py3compat.py:356 in compat_exec
    exec(code, globals, locals)

  File c:\users\hbeen\desktop\unbenannt0.py:17
    divnorm = TwoSlopeNorm (vmin=data.min(), vcenter=1.00, vmax=data.max())

  File ~\anaconda3\Lib\site-packages\matplotlib\colors.py:1493 in __init__
    super().__init__(vmin=vmin, vmax=vmax)

  File ~\anaconda3\Lib\site-packages\matplotlib\colors.py:1279 in __init__
    self._vmin = _sanitize_extrema(vmin)

  File ~\anaconda3\Lib\site-packages\matplotlib\colors.py:208 in _sanitize_extrema
    ret = ex.item()

  File ~\anaconda3\Lib\site-packages\pandas\core\base.py:418 in item
    raise ValueError(""can only convert an array of size 1 to a Python scalar"")

ValueError: can only convert an array of size 1 to a Python scalar

This is the error I get, apparently it happens when it is  trying to run
TwoSlopeNorm (vmin=data.min(), vcenter=1.00, vmx=data.max()

","The error indicates that when you call data.min(), and data.max(), it returns objects of pandas.Series, and not scalar values. Because these series passed to the TwoSlopeNorm expect a vmin and vmax to be given a single value but are in fact arrays (series) with their contents of several values.
To fix that, you might use.min().min() and .max().max() and would be sure that you are taking the minimum or maximum value of the whole DataFrame. That means these extract scalar values of the min and max of all cells in the table rather than series of these values.
",matplotlib
how do i make a single legend for many subplots,"I am plotting the same type of information, but for different countries, with multiple subplots with Matplotlib. That is, I have nine plots on a 3x3 grid, all with the same for lines (of course, different values per line).
However, I have not figured out how to put a single legend (since all nine subplots have the same lines) on the figure just once.
How do I do that?
","There is also a nice function get_legend_handles_labels() you can call on the last axis (if you iterate over them) that would collect everything you need from label= arguments:
handles, labels = ax.get_legend_handles_labels()
fig.legend(handles, labels, loc='upper center')

If the pyplot interface is being used instead of the Axes interface, use:
handles, labels = plt.gca().get_legend_handles_labels()

To remove legends from subplots, see Remove the legend on a matplotlib figure.
To merge twinx legends, see Secondary axis with twinx(): how to add to legend.
",matplotlib
39matplotlib39 has no attribute 39cm39 when deploying an app,"I'm trying to deploy an app developed using Dash which use some matplotlib functions. When I run locally the application, it runs smoothly and everything is okay, however, when I deploy it using the same code (I'm deploying using heroku) the matplotlib version that is installed (which is the current 2.2.3) no longer find the attrbute 'cm' to create a colormap.
import matplotlib
cmap = matplotlib.cm.get_cmap('Reds')

Actual Outcome
Logs of the heroku app
Matplotlib version
I'm using Windows 10, with matplotlib 2.2.3 locally and in the deploy and, locally, my Python version is 3.6.6.
Thank you!
Kind regards,
Renan
","You need to import matplotlib.cm for this to work.
import matplotlib.cm
cmap = matplotlib.cm.get_cmap('Reds')

",matplotlib
matplotlib does not consistently transform plotted elements with pltxscale39log39,"When plotting data, vertical line gets incorrectly transformed by matplotlib. With linear x axis, my curve and the vertical line pointing to a specific location on the curve match perfectly.

After plt.xscale('log') the end point of the vertical line is not on my curve anymore.

import numpy as np
import matplotlib.pyplot as plt

# Generate example data: x values from 1 to 16 (log scale)
x = np.array([1, 2, 4, 8, 16])
y = np.array([0.1, 0.2, 0.3, 0.6, 0.9])

# Perform linear interpolation to find where y crosses 0.5
crossings = np.where(np.diff(np.sign(y - 0.5)))[0]

if len(crossings) > 0:
    crossing_index = crossings[0]
    x1, x2 = x[crossing_index], x[crossing_index + 1]
    y1, y2 = y[crossing_index], y[crossing_index + 1]
    
    # Linear interpolation to find exact x for y = 0.5
    x_nd50 = x1 + (0.5 - y1) * (x2 - x1) / (y2 - y1)
    
    print(f""Interpolated x (ND50) = {x_nd50}"")

# Plot the data with a logarithmic x scale
plt.plot(x, y, label=""Data"", color=""blue"")

# Plot the vertical line at the interpolated ND50 value
plt.plot([x_nd50, x_nd50], [0, 0.5], color='red', linestyle=""--"", alpha=0.7)
plt.scatter(x_nd50, 0.5, color='red', marker='x', alpha=0.7)

# First screenshot taken at this point!

# Set x-axis to log scale (log2)
plt.xscale('log')

# Second screenshot taken at this point!

# Show the plot
plt.xlabel('Log-scaled X')
plt.ylabel('Y')
plt.legend()
plt.show()


","You calculate the x-position using linear interpolation. But on a log-plot, linear interpolation doesn't work.
In fact, Matplotlib shows straight lines on the log-plot, but these are not the same straight lines as on the linear-plot: they would be curved if they were the same, and vice versa. Matplotlib just tries its best, having no extra information about the curve shape between the points. (And it assumes you picked log-space so that curves would show as straight lines, so it defaults to straight lines between the points.)
Hence you need to calculate the x-position using logarithm x-data.
You can solve it as follows:
[...]

xlog = True
# Perform linear interpolation to find where y crosses 0.5
crossings = np.where(np.diff(np.sign(y - 0.5)))[0]

if len(crossings) > 0:
    crossing_index = crossings[0]
    x1, x2 = x[crossing_index], x[crossing_index + 1]
    y1, y2 = y[crossing_index], y[crossing_index + 1]

    if xlog:
        x1 = np.log10(x1)
        x2 = np.log10(x2)
    # Linear interpolation to find exact x for y = 0.5
    x_nd50 = x1 + (0.5 - y1) * (x2 - x1) / (y2 - y1)
    if xlog:
        x_nd50 = 10**x_nd50
    print(f""Interpolated x (ND50) = {x_nd50}"")

[...]

# Set x-axis to log scale (log2)
if xlog:
    plt.xscale('log')

[...]

",matplotlib
plot contours from discrete data in matplotlib,"How do I make a contourf plot where the areas are supposed to be discrete (integer array instead of float)?
The values should discretely mapped to color indices. Instead matplotlib just scales the result across the whole set of colors.
Example:
import numpy as np
from matplotlib import pyplot as plt

axes = (np.linspace(-2, 2, 100), np.linspace(-2, 2, 100))
xx, yy = np.meshgrid(*axes, indexing=""xy"")
fig, ax = plt.subplots()
z = np.abs(xx * yy).astype(int)  # values 0, 1, 2, 3, 4
z[z==0] = 4
ax.contourf(xx, yy, z, cmap=""Set1"")



","Now I got it :)  Thanks @jared, pcolormesh was the right function, but I have to explicitly map the colors as the plotted variable:
import numpy as np
from matplotlib import pyplot as plt

axes = (np.linspace(-2, 2, 100), np.linspace(-2, 2, 100))
xx, yy = np.meshgrid(*axes, indexing=""xy"")
fig, ax = plt.subplots()
z = np.abs(xx * yy).astype(int)  # values 0, 1, 2, 3, 4
z[z==0] = 4

cmap = plt.get_cmap(""Set1"")   
z_color = cmap(z)  # shape (100, 100, 4) with `z` as index
ax.pcolormesh(xx, yy, z_color)


",matplotlib
put specific name on plttitle using dataframe,"I want to plot a graph with the column name in the title.
For example:
import pandas as pd
begin_date = '2019-01-01'
    
df = pd.DataFrame ({""date"": pd.date_range(begin_date, periods=5),
  ""John"": [42, 40, 47, 49, 51],
  ""Ana"": [50, 43, 41, 46,47],
  ""Goku"": [55, 58, 62, 64, 69],                  
})


import matplotlib.pyplot as plt
plt.plot(df[""John""])

I'll plot the graph using this last line with the John values.
I want to create something like this:
plt.title(""Name of graph: John"")

But how do I select the column name and put it in plt.title?
I have to do this in a dataframe which has more than 20 names, and I want to automate this.
Is that possible?
","You can create a for loop -
for column in df.columns[1:]:
    plt.figure():
    plt.plot(df[""date""], df[column]) 
    plt.show()

",matplotlib
how to save a matplotlib figure with automatic height to pdf,"I have the following problem: I want to save a figure with a specific width, but auto-determine its height. Let's look at an example:
import matplotlib.pyplot as plt
import numpy as np

fig,ax=plt.subplots(figsize=(5,5),layout='constrained')
x=np.linspace(0,2*np.pi)
y=np.sin(x)
ax.set_aspect('equal')
ax.plot(x,y)
plt.show()
fig.savefig('test.pdf',format='pdf')

Here, I want the figure to be 5 inches wide, I want the axis to use up all the horizontal space, but I don't really care about the exact vertical size. Basically exactly, what plt.show() gives me:

fig.savefig() gives me a lot of whitespace on top and below the figure (obviously, because I have defined figsize=(5,5)). Using the option bbox_inches='tight' almost does what I want to, but it re-sizes the x-direction of the figure (in this case to roughly 5.1 inches).
I have thus not found any way of saving this figure with a width of exactly 5 inches but an auto-determined height. The only way I can achieve what I want is to manually decrease the figsize until I see that the figure starts shrinking in x-direction.
","Thanks to RuthC for providing the answer in a comment, the following seems to solve my problem:
fig.savefig('test.pdf', format='pdf',bbox_inches='tight', pad_inches='layout')

https://matplotlib.org/stable/users/prev_whats_new/whats_new_3.8.0.html#pad-inches-layout-for-savefig
",matplotlib
why does my shapefile color a different polygon than intended,"I have risk data which I would like to color on a map according to the risk level. I read a shapefile and a csv data file which I then merge. This works very well working with adm1 shapefile.
When I run the same script with adm2 shapefile, the results are completely weird: The polygons colored are way far from the polygon with data. I have the attached map 3 examples of the colored polygons and in black dot the location of the data.
I will appreciate if anyone can give a clue of what is going on, and if possible how to resolve this.
Python script used to produce the plot:
#!/home/zmumba/anaconda3/bin/python

import pandas as pd
import geopandas as gpd
import matplotlib.pyplot as plt
from matplotlib.colors import ListedColormap
from mpl_toolkits.basemap import Basemap  # optional ?

map_df = gpd.read_file(""/home/zmumba/DA/Dekad_Progs/Shapefiles/Lesotho/geoBoundaries-LSO-ADM2-all/geoBoundaries-LSO-ADM2.shx"")
risks_df=pd.read_csv(""/home/zmumba/DA/Dekad_Progs/Output/H1dRrisks.csv"")

map_df[""risk""] = map_df.merge(risks_df, left_on=""shapeName"", right_on=""District"")[""risk""]

colors = {1: ""green"", 2: ""yellow"", 3: ""orange"", 4: ""red""}  # or a list
labels = {1: ""no risk"", 2: ""low risk"", 3: ""medium risk"", 4: ""high risk""}
catego = map_df[""risk""].astype(str).str.cat(map_df[""risk""].map(labels), sep=""- "")

fig, ax = plt.subplots(figsize=(5, 5))
plt.title(f'Risk of Heavy 24hr Rain: 20-24Nov', y=1.04)

map_df.plot(
column=catego,
categorical=True,
edgecolor=""k"",
linewidths=0.8,
alpha=0.7,
cmap=ListedColormap([c for r,c in colors.items() if r in map_df[""risk""].unique()]),
legend=True,
legend_kwds={
""title"": ""Risk Level"",
""shadow"": True,
""loc"": ""lower right"",
""fontsize"": 10,
},
ax=ax,
)
ax.set_axis_off()
plt.savefig('H1dRriskmap.png', dpi=300)

plt.show()

Just to add some clarification to my problem, 
The attached image shows the polygon numbers from the shapefile. Coloring 42 colors 41, coloring 43 colors 42.
The coordinates of 41, 42 and 43 are
-29.3892289999999,  28.3056183629316  -> 41
-29.2870792999999,  29.2715247780569  -> 42
-29.2255776170000,  27.6546474532047  -> 43

These coordinates are the centroids of the polygons taken from the shapefile itself. So it cannot be a problem of coordinates.
Could there be something wrong in the python code?
The code reads from a file  which is in the format:
""District"",""risk""
""name1"",1
""name2"",1
...
""name78"",1

where 1 can b2 2, 3, or 4 depending on the risk level.
I would be glad to try doing the same thing in R, but the problem with R is, it will say ""no package sf"" and so on.
","As I see it, your error is in fact in your python-code... here:
map_df[""risk""] = map_df.merge(risks_df, left_on=""shapeName"", right_on=""District"")[""risk""]

To clarify:
Both your dataframes map_df and risks_df are indexed by numeric values (0,1,2,3 ...).
If you run the merge ( map_df.merge(risks_df, ...)), it will return a new DataFrame with a new index (which is again a numeric index 0,1,2...).
Now if you do map_df[""risk""] = map_df.merge(...)[""risk""] you'll write the results of the merge based on their merge-index to the index-values in map_df.
This works because both indexes are numeric, but it is in no way guaranteed to be  correct!
Just check your results... I guess you'll find that the ""risk"" values are not correctly assigned.
To make sure you assign the values correctly, I'd suggest to replace the line above with something like this:
map_df = map_df.set_index(""shapeName"")
map_df[""risks""] = risks_df.set_index(""District"")[""risk""]

",matplotlib
how to plot data from a kml file using matplotlib on python 37 and windows 10quot,"I will first give a little bit of context to my problem.
I have obtained a .kml file of the territorial seas around the world on this site, and I would like to display it not on Google Earth but on a matplotlib.pyplot plot (with a cartopy map if possible too). The .kml file looks like this:
<?xml version=""1.0"" encoding=""UTF-8""?>
<kml xmlns=""http://www.opengis.net/kml/2.2"" xmlns:gx=""http://www.google.com/kml/ext/2.2"" xmlns:kml=""http://www.opengis.net/kml/2.2"" xmlns:atom=""http://www.w3.org/2005/Atom"">
<NetworkLink>
    <name>Territorial Seas (12NM) v3</name>
    <description><![CDATA[Flanders Marine Institute (2019). Maritime Boundaries Geodatabase: Territorial Seas (12NM), version 3. Available online at <a href=""http://www.marineregions.org"">http://www.marineregions.org</a> https://doi.org/10.14284/387. Consulted on YYYY-MM-DD.]]></description>
    <Link>
        <href>http://geo.vliz.be/geoserver/gwc/service/kml/MarineRegions:eez_12nm.png.kml</href>
    </Link>
</NetworkLink>
</kml>

For that I saws on this other StackOverflow question that using fastkml to read the file was possible.
So this is the test.py code I am trying to run (it comes from the usage guide):
from fastkml import  kml

filename = ""C:\\Users\\dumasal\\Documents\\GOOGLE_EARTH\\MarineRegions-eez_12nm.kml""
with open(filename, 'rt', encoding=""utf-8"") as myfile:
    doc=myfile.read()
    print(doc)
    
    # Create the KML object to store the parsed result
    k = kml.KML()
    
    # Read in the KML string
    k.from_string(doc)
    print('k = ', k)
    
    ### Next we perform some simple sanity checks ###
    
    # Check that the number of features is correct
    # This corresponds to the single ``Document``
    features = list(k.features())
    print(len(features))
    
    # Check that we can access the features as a generator
    # (The two Placemarks of the Document)
    print(features[0].features())
    f2 = list(features[0].features())
    print(len(f2))
    
    # Check specifics of the first Placemark in the Document
    print(f2[0])
    print(f2[0].description)
    print(f2[0].name)
    
    # Check specifics of the second Placemark in the Document
    print(f2[1].name)
    f2[1].name = ""ANOTHER NAME""
    
    # Verify that we can print back out the KML object as a string
    print(k.to_string(prettyprint=True))

When I ran it I got the error: ValueError: Unicode strings with encoding declaration are not supported. Please use bytes input or XML fragments without declaration..
I looked the error up on google and found this GitHub page where they were saying that the ""from_string()"" function only takes bytes so the beginning of my code could be changed to:
from fastkml import  kml

filename = ""C:\\Users\\dumasal\\Documents\\GOOGLE_EARTH\\MarineRegions-eez_12nm.kml""
with open(filename, 'r') as myfile:
    doc=myfile.read().encode('UTF-8')
    print(doc)
    
    # Create the KML object to store the parsed result
    k = kml.KML()
    
    # Read in the KML string
    k.from_string(doc)
    print('k = ', k)

    ### Next we perform some simple sanity checks ###
    
    # Check that the number of features is correct
    # This corresponds to the single ``Document``
    features = list(k.features())
    print(len(features))

And strangely enough the ValueError stopped appearing. However now I get the error:
    print(features[0].features())
IndexError: list index out of range

this is because my variables features = [], and I don't know why.
So could you either explain to me why the features variable is empty, or a more direct method to plot a .kml file with python and matplotlib?
Thank you very much!
","One issue is the KML file you have is a super-overlay, which is auto-generated as multiple KML ""files"" referenced as NetworkLinks in sub regions and few KML python packages support recursive NetworkLinks directly. The fastkml module you're using does not implement NetworkLinks so the content is skipped.
The pyKML package can parse the KML file and iterate over the KML layers referenced in the Network Links.
You can do something like this to iterate over the NetworkLinks and the KML content.
import requests
import re
from pykml import parser

count = 0

def walk(elt):
    global count
    for elt in elt.getchildren():
        tag = re.sub(r'^.*\}', '', elt.tag)
        if tag in ['Folder', 'Document']:
            walk(elt)
        elif tag == 'NetworkLink':
            print(tag)
            print(elt.Link.href)
            if count == 10:
                # for debugging stop after 10 links
                raise Exception(""too many links"")
            count += 1
            response = requests.get(elt.Link.href)
            walk(parser.fromstring(response.content))
        elif tag == 'GroundOverlay':
            print(tag)
            # do something with the ground overlay
        else:
            # other tag; e.g. Region, comment, etc
            print("">>"", tag)

with open(""MarineRegions-eez_12nm.kml"", 'rb') as myfile:
    root = parser.parse(myfile).getroot()
walk(root)

The MarineRegions KML is a super-overlay that recursively sub-divides into smaller regions so trying to plot the GroundOverlay at all levels will be overwrite the larger low-res overlays with smaller hi-res overlays. You need to decide if you only want the hi-res ground overlays or the low-res overlays. For example, if the KML content at a given level doesn't have any NetworkLinks then it's at the lowest level.
Note GeoServer has two different types of super-overlays: raster and vector. The KML you're using is a raster super-overlay. You may want to check if there is a vector overlay available in the case dealing with vectors might be easier than dealing with ground overlay images.
",matplotlib
matplotlib cmget_cmapname num_steps,"My python code uses
plt.cm.getcmap(""coolwarm"", num_steps)
This causes a deprecation warning.
The get_cmap function was deprecated in Matplotlib 3.7 and will be removed in 3.11. Use matplotlib.colormaps[name] or matplotlib.colormaps.get_cmap() or pyplot.get_cmap() instead.
however - both suggested methods do not provide a ""num_steps"" parameter. How do I need to update my statement to be compatible?
","In [29]: import matplotlib
    ...: 
    ...: cw = matplotlib.colormaps['coolwarm']
    ...: cw10 = matplotlib.colormaps['coolwarm'].resampled(10)
    ...: print(cw.N, cw10.N)
256 10

In [30]: 

",matplotlib
matplotlib plots lose transparency when saving as pseps,"I'm having an issue with attempting to save some plots with transparent ellipsoids on them if I attempt to save them with .ps/.eps extensions. 
Here's the plot saved as a .png:

If I choose to save it as a .ps/.eps here is what it looks like:

How I got around this, was to use ImageMagick to convert the original png to a ps. The only problem is that the image in png format is about 90k, and it becomes just under 4M after conversion. This is not good since I have a lot of these images, and it will take too much time to compile my latex document. Does anyone have a solution to this?
","The problem is that eps does not support transparencies natively.
There are few options:

rasterize the image and embed in a eps file (like @Molly suggests) or exporting to pdf and converting with some external tool (like gs) (which usually relies as well on rasterization)
'mimic' transparency, giving a colour that looks like the transparent one on a given background.

I discussed this for sure once on the matplotlib mailing list, and I got the suggestion to rasterize, which is not feasible as you get either pixellized or huge figures. And they don't scale very nicely when put into, e.g., a publication.
I personally use the second approach, and although not ideal, I found it good enough. I wrote a small python script that implements the algorithm from this SO post to obtain a solid RGB representation of a colour with a give transparency
EDIT
In the specific case of your plot try to use the zorder keyword to order the parts plotted. Try to use zorder=10 for the blue ellipse, zorder=11 for the green and zorder=12 for the hexbins. 
This way the blue should be below everything, then the green ellipse and finally the hexbins. And the plot should be readable also with solid colors. And if you like the shades of blue and green that you have in png, you can try to play with mimic_alpha.py.
EDIT 2
If you are 100% sure that you have to use eps, there are a couple of workarounds that come to my mind (and that are definitely uglier than your plot): 

Just draw the ellipse borders on top of the hexbins. 
Get centre and amplitude of each hexagon, (possibly discard all zero bins) and make a scatter plot using the same colour map as in hexbin and adjusting the marker size and shape as you like. You might want to redraw the ellipses borders on top of that

",matplotlib
set pad between arrow and text in annotate function,"How do I set the distance (padding) between the arrow and the text in matplotlib's annotate function? Sometimes the text ends up being too close to the arrow and I would like to move them a little further apart. 
Basic example: 
import matplotlib.pyplot as plt

plt.annotate('Here it is!',xy=(-1,-1),xytext=(0,0),
             arrowprops=dict(arrowstyle='->',lw=1.5))

plt.xlim(-10,10)
plt.ylim(-10,10)

plt.show()


","For fancy arrows you can play with the bbox properties:
fig, ax = plt.subplots(1, 3, figsize=(7, 3))
pad_val = [-5, 0, 5]
for a,p in zip(ax, pad_val):
    a.annotate('Here it is!\npad={}'.format(p),xy=(-1,-1),xytext=(1,1),
                arrowprops=dict(arrowstyle='-|>', fc=""k"", ec=""k"", lw=1.5),
                bbox=dict(pad=p, facecolor=""none"", edgecolor=""none""))
    a.set_xlim(-10,10)
    a.set_ylim(-10,10)


Here the drawback is that you can't add a color behind the annotation (facecolor=""none"" is mandatory), or the arrow will always stick to the border of the frame and it might be ugly.
HTH
",matplotlib
adding a rectangle patch and text patch to 3d collection in matplotlib,"Problem Statement
I'm attempting to add two patches -- a rectangle patch and a text patch -- to the same space within a 3D plot. The ultimate goal is to annotate the rectangle patch with a corresponding value (about 20 rectangles across 4 planes -- see Figure 3).  The following code does not get all the way there, but does demonstrate a rendering issue where sometimes the text patch is completely visible and sometimes it isn't -- interestingly, if the string doesn't extend outside the rectangle patch, it never seems to become visible at all. The only difference between Figures 1 and 2 is the rotation of the plot viewer image. I've left the cmap code in the example below because it's a requirement of the project (and just in case it affects the outcome).
Things I've Tried

Reversing the order that the patches are drawn.
Applying zorder values -- I think art3d.pathpatch_2d_to_3d is overriding that.
Creating a patch collection -- I can't seem to find a way to add the rectangle patch and the text patch to the same 3D collection.

Conclusion
I suspect that setting zorder to each patch before adding them to a 3D collection may be the solution, but I can't seem to find a way to get to that outcome. Similar questions suggest this, but I haven't been able to apply their answers to this problem specifically.
Environment
macOS: Big Sur 11.2.3
Python 3.8
Matplotlib 3.3.4
Figure 1

Figure 2

Figure 3

The Code
Generates Figures 1 and 2 (not 3).
#! /usr/bin/env python3
# -*- coding: utf-8 -*-

from matplotlib.patches import Rectangle, PathPatch
from matplotlib.text import TextPath
from matplotlib.transforms import Affine2D
import mpl_toolkits.mplot3d.art3d as art3d
import matplotlib.pyplot as plt
from matplotlib.colors import Normalize


plt.style.use('dark_background')
fig = plt.figure()
ax = fig.gca(projection='3d')
cmap = plt.cm.bwr
norm = Normalize(vmin=50, vmax=80)
base_color = cmap(norm(50))

# Draw box
box = Rectangle((25, 25), width=50, height=50, color=cmap(norm(62)), ec='black', alpha=1)
ax.add_patch(box)
art3d.pathpatch_2d_to_3d(box, z=1, zdir=""z"")

# Draw text
text_path = TextPath((60, 50), ""xxxx"", size=10)
trans = Affine2D().rotate(0).translate(0, 1)
p1 = PathPatch(trans.transform_path(text_path))
ax.add_patch(p1)
art3d.pathpatch_2d_to_3d(p1, z=1, zdir=""z"")

ax.set_xlabel('x')
ax.set_xlim(0, 100)
ax.set_xticklabels([])
ax.xaxis.set_pane_color(base_color)

ax.set_ylabel('y')
ax.set_ylim(0, 100)
ax.set_yticklabels([])
ax.yaxis.set_pane_color(base_color)

ax.set_zlabel('z')
ax.set_zlim(1, 4)
ax.set_zticks([1, 2, 3, 4])
ax.zaxis.set_pane_color(base_color)
ax.set_zticklabels([])

plt.show()


","This is a well-known problem with matplotlib 3D plotting: objects are drawn in a particular order, and those plotted last appear on ""top"" of the others, regardless of which should be in front in a ""true"" 3D plot.
See the FAQ here:
https://matplotlib.org/stable/api/toolkits/mplot3d/faq.html#my-3d-plot-doesn-t-look-right-at-certain-viewing-angles

My 3D plot doesn’t look right at certain viewing angles
This is probably the most commonly reported issue with mplot3d. The problem is that – from some viewing angles – a 3D object would appear in front of another object, even though it is physically behind it. This can result in plots that do not look “physically correct.”
Unfortunately, while some work is being done to reduce the occurrence of this artifact, it is currently an intractable problem, and can not be fully solved until matplotlib supports 3D graphics rendering at its core.
The problem occurs due to the reduction of 3D data down to 2D + z-order scalar. A single value represents the 3rd dimension for all parts of 3D objects in a collection. Therefore, when the bounding boxes of two collections intersect, it becomes possible for this artifact to occur. Furthermore, the intersection of two 3D objects (such as polygons or patches) can not be rendered properly in matplotlib’s 2D rendering engine.
This problem will likely not be solved until OpenGL support is added to all of the backends (patches are greatly welcomed). Until then, if you need complex 3D scenes, we recommend using MayaVi.

",matplotlib
what does a 4element tuple argument for 39bbox_to_anchor39 mean in matplotlib,"In the ""Legend location"" section of the ""Legend guide"" in the matplotlib website, there's a small script where line 9 is plt.legend(bbox_to_anchor=(0., 1.02, 1., .102), loc=3, ncol=2, mode=""expand"", borderaxespad=0.). All the tuples I've seen passed to bbox_to_anchor have 2 elements in it, but this one has 4. What does each element mean if the tuple passed has 4 elements? 

I was looking at it in the pyplot.legend docs, and it said something about bbox_transform coordinates. So I looked around and found matplotlib.transforms.Bbox with a static from_bounds(x0, y0, width, height). 

I was guessing the setup for the 4-tuple parameter was based on this from_bounds. I copied the script to Spyder, did %matplotlib in an Ipython console, and changed some values. It seemed to make sense, but when I tried only changing .102 to something like 0.9, the legend didn't change. I think the tuple is based on from_bounds, I just don't know why changing the last value in the tuple did nothing. 
","You're right, the 4-tuple in plt.legend(bbox_to_anchor=(0., 1.02, 1., .102), loc=3) is set as (x0, y0, width, height) where (x0,y0) are the lower left corner coordinates of the bounding box.
While those parameters set the bounding box for the legend, the legend's actual vertical size is shrunk to the size that is needed to put the elements in. Further its position is determined only in conjunction with the loc parameter. The loc parameter sets the alignment of the legend inside the bounding box, such that for some cases, no difference will by seen when changing the height, compare e.g. plot (2) and (4).
 
",matplotlib
attributeerror while trying to load the pickled matplotlib figure,"I used pickle to dump matplotlib figure as shown in an answer in SO. Below is the code snippet-
import pickle
import matplotlib.pyplot as plt
fig, ax = plt.subplots()
ax.plot([1,2,3],[10,-10,30])
pickle.dump(fig, open('fig.pickle', 'wb'))

Below is the code snippet to load the pickled figure-
import pickle
figx = pickle.load(open('fig.pickle', 'rb'))
figx.show()

The above code shows following error-
AttributeError: 'NoneType' object has no attribute 'manager'
Figure.show works only for figures managed by pyplot, normally created by pyplot.figure().

I am using Python 3.6.3 on Ubuntu 14.04 LTS 64 Bit OS. Below are the more details of my environment-
> import matplotlib
> matplotlib.__version__
'2.1.0'
> matplotlib.get_backend()
'Qt5Agg'
> import sys
> sys.version_info
sys.version_info(major=3, minor=6, micro=3, releaselevel='final', serial=0)

PS: My questions seem similar to this question asked at SO. However, it is different since the provided answer is not running and throwing exceptions.
","You need a canvas manager before you can show your figure. The same concept from question Matplotlib: how to show a figure that has been closed applies, you can create a function to make a dummy figure and steal its manager, as below (credit to Jean-Sébastien who wrote the answer above). 
def show_figure(fig):

    # create a dummy figure and use its
    # manager to display ""fig""  
    dummy = plt.figure()
    new_manager = dummy.canvas.manager
    new_manager.canvas.figure = fig
    fig.set_canvas(new_manager.canvas)

With this function you can then run:
show_figure(figx)
figx.show()

",matplotlib
what to do with matplotlib graphs given in a list that a function returns or what is an alternative function architecture,"I have a function that works something like this.
import numpy as np
import matplotlib.pyplot as plt

def plot_from_dave(n = 100, r = 10):
    
    my_list = []
    
    for i in range(r):
        
        fig, ax = plt.subplots()
        x = np.random.normal(0, 1, n)
        y = np.random.normal(0, 1, n)
        ax.scatter(x, y)
        
        my_list.append((fig, ax))
    
    return(my_list)

The function creates many plots and then saves the figure-axis tuples to a list that is returned.
I now want to access those figures and axes for downstream customization of the plots. However, accessing them has been problematic. For instance, in the below, I get nothing.
np.random.seed(2024)
plot_list = plot_from_dave(10, 3)
plot_list[0]
plt.show()

How can I access these figures and axes to apply further customization outside of the function? Alternatively, how can I set up the function to allow customization of the plots?
","You return a list, so you can either index it or loop over it:
import numpy as np
import matplotlib.pyplot as plt


def plot_from_dave(n=100, r=10):
    my_list = []

    for i in range(r):
        fig, ax = plt.subplots()
        x = np.random.normal(0, 1, n)
        y = np.random.normal(0, 1, n)
        ax.scatter(x, y)

        my_list.append((fig, ax))

    return (my_list)


figs = plot_from_dave(r=4)
for i, (fig, ax) in enumerate(figs):
    ax.set_title(f'Title {i+1}')
    # set the color of the dots to a random color
    ax.collections[0].set_facecolor(np.random.rand(3))
    fig.show()

figs[2][1].set_title('Title special 3')
figs[2][1].collections[0].set_facecolor('red')
figs[2][0].show()

This generates 4 plots (and then displays them), giving them different titles and colours after plot_from_dave() completes and returns. It also specifically selects the 3rd one and sets its title and a specific colour and then shows it again.
Note that you don't have to use enumerate() - my example only does so to get a nice index number that can be used to give them the numbered titles. If you don't need something like that, you can of course just for fix, ax in figs:.
",matplotlib
matplotlib 3d line plot,"I am trying to plot a two series of data in 3d as lines but cant find a good example of how to structure the data to do this in python.
My example data is:
x = np.array([[1,1,1,1,5,1,1,1,1], [4.5,5,5,5,5.5,5,5,5,4.5]])

But I presume to do this I need 3 series of data.
Here is a sample of what the output should look like

I am assuming I need to add extra rows to the array, but not sure if I should try to build a 3d array or plot each axis with its own separate arrays?
In which case axis
y1 = np.ones(9)
z = np.array([0,1,2,3,4,5,6,7,8])

I did have a look here and read the documentation here but still could not work out how to apply it to what I am trying to do.
My attempt:
import matplotlib.pylab as pl
from mpl_toolkits.mplot3d import Axes3D
import numpy as np
pl.figure()

ax = pl.subplot(projection='3d')
data = np.array([[1,1,1,1,5,1,1,1,1], [4,5,5,5,5,5,5,5,4]])
y1 = np.ones(9)
z = np.array([1,2,3,4,5,6,7,8,9])

ax.plot(x, y, z, color = 'r')

UPDATE
This code displays the pseudo data, but ideally x would only have two axes - does it have to be axes of matching size? x just wants to have two ticks at 1, and another at 2.
import matplotlib.pyplot as pl
from mpl_toolkits.mplot3d import Axes3D
import numpy as np
pl.figure()

ax = pl.subplot(projection='3d')
z = np.array([[1,1,1,1,5,1,1,1,1], [1,4.5,5,5,5,5,5,5,4]])
x = np.ones(9)
y1 = np.array([1,2,3,4,5,6,7,8,9])
ax.plot(x, y1, z[0], color = 'r')
ax.plot(x*2, y1, z[1], color = 'g')

ax.set_xlabel('x')
ax.set_box_aspect(aspect = (1,1,2))


","Try this:
import matplotlib.pyplot as pl
from mpl_toolkits.mplot3d import Axes3D
import numpy as np
pl.figure(figsize = (12,12))

ax = pl.subplot(projection='3d')
z = np.array([[1,1,1,1,5,1,1,1,1], [0.5,4.5,5,4.8,5.1,5,5,5,4]])
x = np.ones(9)
y1 = np.array([1,2,3,4,5,6,7,8,9])
ax.plot(x, y1, z[0], color = 'r')
ax.plot(x*2, y1, z[1], color = 'g')

ax.set_xlabel('x \n freq')
ax.set_ylabel('y \n time (s)')
ax.set_zlabel('z\n magnitude')

ax.set_box_aspect(aspect = (0.5,1,1))
ticks = np.array([0,1,2])
ax.set_xticks(ticks) 


",matplotlib
how to perserve layout of figure with aspecquotequalquot after change of axis lims,"I have 2x2 grid of axes in one figure that I want to be aligned with zero offset to ideal rectangle. Unfortunatelly I also need to use ax.set_aspect(""equal"") which seem to cause a lot of problems.
The minimal code can look like this:
import matplotlib.pyplot as plt

fig, axes = plt.subplots(2,2,
                    figsize = (15,6.04),
                    gridspec_kw = dict(
                        wspace = 0,
                        hspace = 0
                        ),
                       )
for ax in axes[:,1]:
    ax.yaxis.tick_right()
    ax.yaxis.set_label_position(""right"")

for ax in axes.flatten():
    ax.set_xlim(0,15)
    ax.set_ylim(-4, 2)
    ax.set_aspect(""equal"")

Setting the figsize does the trick, but only for one set of lims. I would like to have it automatically recalculated for any xlim/ylim ratio. Is there an easy way to do this (for example by deducing that 0.04 shift in the figsize y)?
","The newer layout='compressed' can do that (https://matplotlib.org/stable/users/explain/axes/constrainedlayout_guide.html#grids-of-fixed-aspect-ratio-axes-compressed-layout).  Note however that if you have tick labels that over-spill the edges of the subplot, ""compressed"" will make spaces between the subplots.
import matplotlib.pyplot as plt

fig, axes = plt.subplots(2,2,
                    figsize = (6, 4),
                        sharex=True, sharey=True,
                       layout='compressed')
fig.get_layout_engine().set(w_pad=0, h_pad=0, wspace=0, hspace=0)
for ax in axes[:,1]:
    ax.yaxis.tick_right()
    ax.yaxis.set_label_position(""right"")
for ax in axes.flatten():
    ax.tick_params(direction=""in"")
    ax.set_xlim(0.0000001, 14.999)
    ax.set_ylim(-3.99999, 1.99999)
    ax.set_aspect(""equal"")
plt.show()

",matplotlib
locking matplotlib xaxis range and then plotting on top of it,"I can do the following in base R plotting.
x1 <- c(3, 4)
y1 <- c(5, 8)
x2 <- c(2, 5)
y2 <- c(5, 7)
plot(x1, y1, type = 'l')
lines(x2, y2, col = 'red')


The key point is that, when I call that final lines command, the plotting function has already fixed the x-axis.
How can I do this in matplotlib to superimpose a new set of data on top of the current axis graph while keeping the x-axis range the same as before but allowing the y-axis range to accommodate the vertical range of the new points?
The code below fails because it expands the x-axis.
import matplotlib.pyplot as plt
x1 = [3, 4]
y1 = [5, 8]
x2 = [2, 5]
y2 = [4, 9]
fig, ax = plt.subplots()
ax.plot(x1, y1)
ax.plot(x2, y2)
plt.show()
plt.close()


The code below fails by cropping a bit too tight compared to how just a plot of x1 and y1 would be.
fig, ax = plt.subplots()
ax.plot(x1, y1)
ax.plot(x2, y2)
ax.set_xlim([min(x1), max(x1)])
plt.show()
plt.close()


The code below has a little bit of padding to the right and to the left of the maximum and minmum values of x1 but does not superimpose the orange line for x2 and y2.
fig, ax = plt.subplots()
ax.plot(x1, y1)
# ax.plot(x2, y2)
# ax.set_xlim([min(x1), max(x1)])
plt.show()
plt.close()


So how can I get matplotlib to superimpose the second set of data on top of the original graph while allowing the vertical axis to expand to include new y-values, yet keep the x-axis at the original range that contains a bit of padding to the right and the left?
","Your second approach is quite close to what I think you want. You just need to store the original x-limits and then use those to reset the plot after it's resized:
import matplotlib.pyplot as plt

x1 = [3, 4]
y1 = [5, 8]
x2 = [2, 5]
y2 = [4, 9]

fig, ax = plt.subplots()
ax.plot(x1, y1)
xlim = ax.get_xlim()
ax.plot(x2, y2)
ax.set_xlim(xlim)
plt.show()
plt.close()


",matplotlib
matplotlib polar chart not showing all xyticks,"
Issue 1: The x-ticks (pie pieces) aren't ordered from 0 to 24 (my bad, should be 1)
Issue 2: all y-ticks (rings) aren't showing
Issue 3: Someone seems to have eaten a part of the polar chart ...
I expect to see 31 rings, and 24 ""pie pieces"".
","import matplotlib.pyplot as plt
import numpy as np

fig = plt.figure()

ax = fig.add_subplot(111, projection='polar')

ax.set_xticks(np.arange(1, 25) * np.pi / 12) 
ax.set_xticklabels([str(number) for number in range(1, 25)])

ax.set_yticks(range(1, 32)) 
ax.set_yticklabels([str(number) for number in range(1, 32)])

ax.grid(True)

ax.scatter(np.pi / 12 * 24, 31)  

ax.set_ylim(0, 32)

plt.show()


",matplotlib
plotting cumulative distribution from data,"I have a large data to plot the ECDF but got confused, so I decided using small data subset, which still didn't make sentence to me (as complete to what I read from the source).
For that, I produced a synthetic MWE to replicate the problem. Say I have the following df:
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import seaborn as sns

sns.set(style=""whitegrid"")

# DataFrame
df = pd.DataFrame(
    {'id': [54, 54, 54, 54, 54, 16, 16, 16, 50, 50, 28, 28, 28, 19, 19, 32, 32, 32, 81, 81, 81, 81, 81],
     'user_id': [10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 84, 84, 84, 84, 84, 179, 179, 179, 179, 179],
     'trip_id': [101, 101, 101, 101, 101, 101, 101, 101, 102, 102, 102, 102, 102, 841, 841, 841, 841, 841, 1796, 1796,
                 1796, 1796, 1796],
     'travel_mode': ['train', 'train', 'train', 'train', 'train', 'walk', 'walk', 'walk', 'train', 'train', 'train',
                             'train', 'train', 'taxi', 'taxi', 'bus', 'bus', 'bus', 'train', 'train', 'train', 'train', 'train']}
)

In this example, 50% of the trips (2/4) were travelled by 1 user. I want to plot the number of trips per user. So Proceeded like so:
# number of trips per user
trips_per_user = df.groupby('user_id')['trip_id'].nunique()

trips_per_user
         trip_id
user_id     
  10       2
  84       1
  179      1

# Create a DataFrame for plotting
plot_data = trips_per_user.reset_index(name='num_trips')

plot_data
    user_id num_trips
0     10     2
1     84     1
2    179     1

Now, plotting the ECDF.
# ECDF
plt.figure(figsize=(5, 4))
sns.ecdfplot(data=plot_data, x='num_trips', stat='proportion', complementary=False)
plt.xlabel('Number of Trips')
plt.ylabel('Cumulative Proportion')

Output:

Obviously, I am not doing this correctly.

1 trip was travelled in 50% of the data (not about 70% as in the plot obtained).
The ecdf curve isn't starting from 0.

Required answer:
I wanted to plot something like below (from the source):

","You have 3 values in your plot_data dataset (2 unique): [1, 1, 2], for the first unique point (1), you have 2 items ([1, 1]) out of 3, so 67%.
If you want to count 2 trips for 2, you have to weight your ecdfplot:
sns.ecdfplot(data=plot_data, x='num_trips', weights='num_trips',
             stat='proportion')

Output:

",matplotlib
plotting a second scaled y axis in matplotlib from one set of data,"I have two views of the same data, which calls for the need to have another y-axis which is scaled appropriately from the first natural y-axis. So when I plot my {x,y} data, the left y-axis shows y, but the right y-axis also shows 1/y or any other function. I do not ever want to plot {x, f(x)} or {x, 1/y}.
Now to complicate matters I am using the .plt style of interaction rather than the axis method.  
plt.scatter(X, Y, c=colours[count], alpha=1.0, label=chart, lw = 0)
plt.ylabel(y_lbl)
plt.xlabel(x_lbl)

Is there another way - with plt? Or is it a case of generating two overlain plots and changing the alpha appropriately?
","I had to check your previous (duplicate) question and all the comments to understand what you actually want. So to just get a secondary y-axis you can still use twinx. Then you can use set_ylim make sure it has the same limits as the first. To put tick labels according to some function (in your case 1/y) you can use a custom FuncFormatter.
import matplotlib.pyplot as plt
import numpy as np
import matplotlib.ticker as mticker

fig, ax1 = plt.subplots(1,1)
ax1.set_xlabel('x')
ax1.set_ylabel('y')

# plot something
x = np.linspace(0.01, 10*np.pi, 1000)
y = np.sin(x)/x
ax1.plot(x, y)

# add a twin axes and set its limits so it matches the first
ax2 = ax1.twinx()
ax2.set_ylabel('1/y')
ax2.set_ylim(ax1.get_ylim())

# apply a function formatter
formatter = mticker.FuncFormatter(lambda x, pos: '{:.3f}'.format(1./x))
ax2.yaxis.set_major_formatter(formatter)

plt.show()

Result:

",matplotlib
how to selectively label bars using pltbar_label,"When drawing a bar plot in matplotlib using bars = plt.bar(...), the returned object can be passed  to plt.bar_label(bars, labels) for easy labelling.
plt.bar_label(bars, labels) labels all of the bars. I want to only label a subset (the first three), ideally using plt.bar_label(). I don't want to invoke
plt.text() or plt.annotate().
Is there a way of slicing out the bars I want from bars, and only having those labelled using plt.bar_label()?

Example below shows the default behaviour where all bars get labelled.

#Imports
import matplotlib.pyplot as plt
import pandas as pd

# Data for testing
values = [10, 9, 10.5, 4, 3.1, 2]
labels = [
    'sensor calibration', 'sensor response', 'noise floor estimate',
    'noise floor theta', 'weighted response coef', 'linear estimate'
]

#Bar plot with labels
ax = plt.figure(figsize=(5, 2)).add_subplot()
bars = ax.bar(range(6), values, color='lightgray')
ax.set(xlabel='x', ylabel='value', title='Data')
ax.spines[['top', 'right']].set_visible(False)

#Add all bar labels
label_kwargs = dict(rotation=90, weight='bold', fontsize=8, label_type='center')
ax.bar_label(bars, labels, **label_kwargs)

","An easy option would be to provide empty labels for the bars that shouldn't be labeled:
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd

# Data for testing
values = [10, 9, 10.5, 4, 3.1, 2]
labels = [
    'sensor calibration', 'sensor response', 'noise floor estimate',
    'noise floor theta', 'weighted response coef', 'linear estimate'
]

# Bar plot with labels
fig, ax = plt.subplots(figsize=(5, 3))
bars = ax.bar(range(6), values, color='lightgray')
ax.set(xlabel='x', ylabel='value', title='Data')
ax.spines[['top', 'right']].set_visible(False)

# Add centered bar labels
k = 3
label_kwargs = dict(rotation=90, weight='bold', fontsize=8, label_type='center')
ax.bar_label(bars, labels[:k] + [''] * (len(bars) - k), **label_kwargs)

plt.show()


Here is a variant that places centered labels for the long bars, and top labels for the shorter bars:
# Bar plot with labels
fig, ax = plt.subplots(figsize=(5, 3))
bars = ax.bar(range(6), values, color='lightgray')
ax.set(xlabel='x', ylabel='value', title='Data')
ax.spines[['top', 'right']].set_visible(False)

cutoff = 5  # bars higher will get centered labels, lower bars get labels on top
# Add the centered bar labels
label_kwargs = dict(rotation=90, weight='bold', fontsize=8, label_type='center')
ax.bar_label(bars, ['' if val <= cutoff else lbl for val, lbl in zip(values, labels)], **label_kwargs)
# Add bar labels at the top of the other bars
label_kwargs['label_type'] = 'edge'
label_kwargs['padding'] = 2
ax.bar_label(bars, ['' if val > cutoff else lbl for val, lbl in zip(values, labels)], **label_kwargs)

plt.show()


",matplotlib
matplotlib colorbar with fixed aspect ratio,"I have a grid (say 2x2) of subplots with constrained layout and I want to have a colorbar of the total height of the figure right next to it as follows:
fig, axes = plt.subplots(nrows=2,ncols=2,layout='constrained')

for ax in axes.flat:

    stuff = ax.pcolormesh(np.random.randn(20, 20), cmap='viridis')
    
#    ax.set_box_aspect(1/2)

fig1.colorbar(stuff,ax= axes.ravel().tolist())  # add a color legend

plt.show()

This looks fine and exactly how I want it. However, as soon as I uncomment the line where I set the (box) aspect, the colorbar is too high.
I tried different things like inset_axis which is suggested in the tutorial (https://matplotlib.org/stable/users/explain/axes/colorbar_placement.html#manual-placement-of-colorbars) but as I want the colorbar to extend over two axes, this doesn't work.
Any help?
","You can use ImageGrid from mpl_toolkits.axes_grid1
from mpl_toolkits.axes_grid1 import ImageGrid

I used random data:
# Example data
Z = np.random.rand(10, 10)
extent = (-10, 10, -10, 10)

# Define aspect ratio (width-to-height)
aspect_ratio = .5  # Width is twice the height

Create the figure
fig = plt.figure(figsize=(10, 8))  

# Create a 2x2 grid of images with a single colorbar on the right
grid = ImageGrid(
    fig, 111,  # Position in the figure
    nrows_ncols=(2, 2),  
    axes_pad=(0.1, 0.4),  
    label_mode=""L"",  
    share_all=True,  # Share axes limits
    cbar_location=""right"",  # Place the colorbar on the right
    cbar_mode=""single"",  # One colorbar for all plots
    cbar_size=""3%"",  # Slimmer colorbar
    cbar_pad=0.2,  # Padding between grid and colorbar
)

Add images to the grid and control the aspect ratio
for ax in grid:
    im = ax.imshow(Z, extent=extent, cmap=""viridis"", aspect=1/aspect_ratio)

Add the colorbar
grid.cbar_axes[0].colorbar(im)

Configure ticks and axes labels
for cax in grid.cbar_axes:
    cax.tick_params(labelright=True)  # Show ticks on the right
grid.axes_llc.set(xticks=[-10, 0, 10], yticks=[-10, 0, 10])



",matplotlib
matplotlib 300 cannot import name 39get_backend39 from 39matplotlib39,"Using Windows 10, anaconda as a package manager. I have a base environment running python 3.7 where matplotlib works fine. When I create a new environment and install both keras and matplotlib, I start to run into problems:
>>> import matplotlib.pyplot as plt
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""C:\...\Anaconda3\envs\keras_env\lib\site-packages\matplotlib\__init__.py"", line 1111, in <module>
    rcParamsOrig = RcParams(rcParams.copy())
  File ""C:\...\Anaconda3\envs\keras_env\lib\site-packages\matplotlib\__init__.py"", line 891, in __getitem__
    from matplotlib import pyplot as plt
  File ""C:\...\Anaconda3\envs\keras_env\lib\site-packages\matplotlib\pyplot.py"", line 32, in <module>
    import matplotlib.colorbar
  File ""C:\...\Anaconda3\envs\keras_env\lib\site-packages\matplotlib\colorbar.py"", line 40, in <module>
    import matplotlib._constrained_layout as constrained_layout
  File ""C:\...\Anaconda3\envs\keras_env\lib\site-packages\matplotlib\_constrained_layout.py"", line 52, in <module>
    from matplotlib.legend import Legend
  File ""C:\...\Anaconda3\envs\keras_env\lib\site-packages\matplotlib\legend.py"", line 43, in <module>
    from matplotlib.offsetbox import HPacker, VPacker, TextArea, DrawingArea
  File ""C:\...\Anaconda3\envs\keras_env\lib\site-packages\matplotlib\offsetbox.py"", line 33, in <module>
    from matplotlib.image import BboxImage
  File ""C:\...\Anaconda3\envs\keras_env\lib\site-packages\matplotlib\image.py"", line 19, in <module>
    from matplotlib.backend_bases import FigureCanvasBase
  File ""C:\...\Anaconda3\envs\keras_env\lib\site-packages\matplotlib\backend_bases.py"", line 46, in <module>
    from matplotlib import (
ImportError: cannot import name 'get_backend'

Any suggestions? This is a fresh installation of conda. All I've done to get here is run conda create --name keras_env keras matplotlib, enter the environment, and try to import matplotlib. These are the packages conda installs:
## Package Plan ##

environment location: C:\...\Anaconda3\envs\keras_env

added / updated specs:
- keras
- matplotlib


The following NEW packages will be INSTALLED:

_tflow_select:       2.2.0-eigen
absl-py:             0.5.0-py36_0
astor:               0.7.1-py36_0
blas:                1.0-mkl
ca-certificates:     2018.03.07-0
certifi:             2018.10.15-py36_0
cycler:              0.10.0-py36h009560c_0
freetype:            2.9.1-ha9979f8_1
gast:                0.2.0-py36_0
grpcio:              1.12.1-py36h1a1b453_0
h5py:                2.8.0-py36h3bdd7fb_2
hdf5:                1.10.2-hac2f561_1
icc_rt:              2017.0.4-h97af966_0
icu:                 58.2-ha66f8fd_1
intel-openmp:        2019.0-118
jpeg:                9b-hb83a4c4_2
keras:               2.2.4-0
keras-applications:  1.0.6-py36_0
keras-base:          2.2.4-py36_0
keras-preprocessing: 1.0.5-py36_0
kiwisolver:          1.0.1-py36h6538335_0
libpng:              1.6.35-h2a8f88b_0
libprotobuf:         3.6.0-h1a1b453_0
markdown:            3.0.1-py36_0
matplotlib:          3.0.0-py36hd159220_0
mkl:                 2019.0-118
mkl_fft:             1.0.6-py36hdbbee80_0
mkl_random:          1.0.1-py36h77b88f5_1
numpy:               1.15.3-py36ha559c80_0
numpy-base:          1.15.3-py36h8128ebf_0
openssl:             1.0.2p-hfa6e2cd_0
pip:                 10.0.1-py36_0
protobuf:            3.6.0-py36he025d50_0
pyparsing:           2.2.2-py36_0
pyqt:                5.9.2-py36h6538335_2
python:              3.6.7-h33f27b4_0
python-dateutil:     2.7.3-py36_0
pytz:                2018.5-py36_0
pyyaml:              3.13-py36hfa6e2cd_0
qt:                  5.9.6-vc14h1e9a669_2
scipy:               1.1.0-py36h4f6bf74_1
setuptools:          40.4.3-py36_0
sip:                 4.19.8-py36h6538335_0
six:                 1.11.0-py36_1
sqlite:              3.25.2-hfa6e2cd_0
tensorboard:         1.11.0-py36he025d50_0
tensorflow:          1.11.0-eigen_py36h346fd36_0
tensorflow-base:     1.11.0-eigen_py36h45df0d8_0
termcolor:           1.1.0-py36_1
tornado:             5.1.1-py36hfa6e2cd_0
vc:                  14.1-h0510ff6_4
vs2015_runtime:      14.15.26706-h3a45250_0
werkzeug:            0.14.1-py36_0
wheel:               0.32.2-py36_0
wincertstore:        0.2-py36h7fe50ca_0
yaml:                0.1.7-hc54c509_2
zlib:                1.2.11-h8395fce_2

","This issue has been reported here and has been fixed here. The fix will be available in matplotlib 3.0.1, which is scheduled to be release within the next few days. 
Until then you may either use python <= 3.6.6 with matplotlib 3.0.0. Or you may use matplotlib 2.2.3 or you may try the fix proposed in the linked issue, namely to create a matplotlibrc file in one of the paths where matplotlib would find it.
",matplotlib
how to arrange multiple interactive figures in a jupyter notebook,"I try to arrange multiple figures in a jupyter notebook using ipywidget and matplotlib but it's not working as I expect to... When I run cells invidually its working well but when I rerun the whole notebook nothing shows.
You can have a glimpse of what I try to do and help me by finding what's wrong in my whole notebook.
Cell 1
import ipywidgets as widgets
import numpy as np
import matplotlib.pyplot as plt

%matplotlib widget

pi = np.pi
t = np.linspace(0,2*pi,100)
fig1 = plt.figure(1,figsize=(6,2))
ax1 = fig1.add_subplot(1, 1, 1, aspect=1)

def update_plot(phase,amplitude):
    ax1.clear()
    ax1.plot(t,amplitude*np.sin(t+phase), color='r', label='Sinus')
    ax1.legend(loc=0)

phase = widgets.FloatSlider(min=-2,max=2,value=0,description='Phase:')
amplitude = widgets.FloatSlider(min=0,max=8,value=1.5,description='Amplitude:')
widgets.interactive(update_plot, phase=phase, amplitude=amplitude)

Cell2
fig2 = plt.figure(2,figsize=(6,2))
ax2 = fig2.add_subplot(1, 1, 1, aspect=1)
t2 = np.linspace(0,2*pi,100)

def update_plot(phase,amplitude):
    ax2.clear()
    ax2.plot(t2,amplitude*np.cos(t2+phase), color='b', label='Cosinus')
    ax2.legend(loc=0)

phase = widgets.FloatSlider(min=-2,max=2,value=0,description='Phase:')
amplitude = widgets.FloatSlider(min=0,max=8,value=1,description='Amplitude:')
widgets.interactive(update_plot, phase=phase, amplitude=amplitude)

","To see plots you can just add
fig1
fig2 etc at the end of each figure cell.
",matplotlib
how to highlight specific xvalue ranges,"I'm making a visualization of historical stock data for a project, and I'd like to highlight regions of drops. For instance, when the stock is experiencing significant drawdown, I would like to highlight it with a red region. 
Can I do this automatically, or will I have to draw a rectangle or something? 
","Have a look at axvspan (and axhspan for highlighting a region of the y-axis).
import matplotlib.pyplot as plt

plt.plot(range(10))
plt.axvspan(3, 6, color='red', alpha=0.5)
plt.show()


If you're using dates, then you'll need to convert your min and max x values to matplotlib dates.  Use matplotlib.dates.date2num for datetime objects or matplotlib.dates.datestr2num for various string timestamps.
import matplotlib.pyplot as plt
import matplotlib.dates as mdates
import datetime as dt

t = mdates.drange(dt.datetime(2011, 10, 15), dt.datetime(2011, 11, 27),
                  dt.timedelta(hours=2))
y = np.sin(t)

fig, ax = plt.subplots()
ax.plot_date(t, y, 'b-')
ax.axvspan(*mdates.datestr2num(['10/27/2011', '11/2/2011']), color='red', alpha=0.5)
fig.autofmt_xdate()
plt.show()


",matplotlib
how to redraw figure on event in matplotlib,"I'm trying to pre-generate and store matplotlib figures in python, and then display them on a keyboard event (left-right cursor keys).
It partially seems working, but fails after the first keypress.
Any idea, what am I doing wrong?
import matplotlib.pyplot as plt
import numpy as np


def new_figure(title, data):
    fig,ax = plt.subplots() 
    plt.plot(data, label=title) 
    ax.set_xlabel('x-axis')
    ax.set_ylabel('value')
    plt.legend()
    plt.title(title)
    plt.close(fig)
    return fig


def show_figure(fig):
    dummy = plt.figure() 
    new_manager = dummy.canvas.manager 
    new_manager.canvas.figure = fig 
    fig.set_canvas(new_manager.canvas) 


def redraw(event, cnt):
    event.canvas.figure.clear()
    dummy = event.canvas.figure
    new_manager = dummy.canvas.manager
    new_manager.canvas.figure = figs[cnt]
    figs[cnt].set_canvas(new_manager.canvas)
    event.canvas.draw()


def keypress(event):
    global cnt
    if event.key == 'right':
        cnt += 1
        cnt %= mx
    elif event.key == 'left':
        cnt -= 1
        if cnt < 0:
            cnt = mx-1
    redraw(event, cnt)


d = range(0, 360)
data = []
data.append(np.sin(np.radians(d)))
data.append(np.cos(np.radians(d)))
data.append(np.tan(np.radians(d)))
titles = ['sin','cos','tan']
mx = len(data)

figs = []
for i in range(mx):
    fig = new_figure(titles[i], data[i])
    figs.append(fig)

cnt = 0

show_figure(figs[0])
figs[0].canvas.mpl_connect('key_press_event', keypress)
plt.show()

The error I get eventually is:
  File ""C:\Program Files\Python39\lib\tkinter\__init__.py"", line 1636, in _configure
    self.tk.call(_flatten((self._w, cmd)) + self._options(cnf))
_tkinter.TclError: invalid command name "".!navigationtoolbar2tk.!button2""

","Not sure about the root cause of the error, but one way to avoid this is to fully replace the figure and reconnect the key_press_event:
def redraw(event, cnt):
    event.canvas.figure = figs[cnt]
    event.canvas.mpl_connect('key_press_event', keypress)
    event.canvas.draw()

",matplotlib
matplotlib scatter color by categorical factors,"I have a basic scatter where the x and y are float. But I want to change the color of the marker based on a third categorical variable. The categorical variable is in a string form. This seems to cause an issue. 
To use the iris dataset- here is the code I think I would use:
#Scatter of Petal
x=df['Petal Length']
y=df['Petal Width']
z=df['Species']
plt.scatter(x, y, c=z, s=15, cmap='hot')
plt.xlabel('Petal Width')
plt.ylabel('Petal Length')
plt.title('Petal Width vs Length')

But I get an error that: could not convert string to float: iris-setosa
Do I have to change the categorical variable to a numeric one before I run, or is there something I can do with the data in its current format?
Thanks
update: the entire traceback is:
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
<ipython-input-47-d67ee3bffc3b> in <module>()
      3 y=df['Petal Width']
      4 z=df['Species']
----> 5 plt.scatter(x, y, c=z, s=15, cmap='hot')
      6 plt.xlabel('Petal Width')
      7 plt.ylabel('Petal Length')

/Users/mpgartland1/anaconda/lib/python2.7/site-packages/matplotlib/pyplot.pyc in scatter(x, y, s, c, marker, cmap, norm, vmin, vmax, alpha, linewidths, verts, hold, **kwargs)
   3198         ret = ax.scatter(x, y, s=s, c=c, marker=marker, cmap=cmap, norm=norm,
   3199                          vmin=vmin, vmax=vmax, alpha=alpha,
-> 3200                          linewidths=linewidths, verts=verts, **kwargs)
   3201         draw_if_interactive()
   3202     finally:

/Users/mpgartland1/anaconda/lib/python2.7/site-packages/matplotlib/axes/_axes.pyc in scatter(self, x, y, s, c, marker, cmap, norm, vmin, vmax, alpha, linewidths, verts, **kwargs)
   3605 
   3606         if c_is_stringy:
-> 3607             colors = mcolors.colorConverter.to_rgba_array(c, alpha)
   3608         else:
   3609             # The inherent ambiguity is resolved in favor of color

/Users/mpgartland1/anaconda/lib/python2.7/site-packages/matplotlib/colors.pyc in to_rgba_array(self, c, alpha)
    420             result = np.zeros((nc, 4), dtype=np.float)
    421             for i, cc in enumerate(c):
--> 422                 result[i] = self.to_rgba(cc, alpha)
    423             return result
    424 

/Users/mpgartland1/anaconda/lib/python2.7/site-packages/matplotlib/colors.pyc in to_rgba(self, arg, alpha)
    374         except (TypeError, ValueError) as exc:
    375             raise ValueError(
--> 376                 'to_rgba: Invalid rgba arg ""%s""\n%s' % (str(arg), exc))
    377 
    378     def to_rgba_array(self, c, alpha=None):

ValueError: to_rgba: Invalid rgba arg ""Iris-setosa""
to_rgb: Invalid rgb arg ""Iris-setosa""
could not convert string to float: iris-setosa

","As your traceback tells you, you can't pass a string to the color parameter. You can pass either colors, or an array of values that it will interpret as colors itself. 
See:
http://matplotlib.org/api/pyplot_api.html?highlight=plot#matplotlib.pyplot.plot
There is probably a more elegant way, but one implementation would be the following (I used the following dataset: https://raw.githubusercontent.com/pydata/pandas/master/pandas/tests/data/iris.csv):
import matplotlib.pyplot as plt
import matplotlib.colors as colors
import matplotlib.cm as cmx
from pandas import read_csv

df = read_csv('iris.csv')

#Scatter of Petal
x=df['PetalLength']
y=df['PetalWidth']

# Get unique names of species
uniq = list(set(df['Name']))

# Set the color map to match the number of species
z = range(1,len(uniq))
hot = plt.get_cmap('hot')
cNorm  = colors.Normalize(vmin=0, vmax=len(uniq))
scalarMap = cmx.ScalarMappable(norm=cNorm, cmap=hot)

# Plot each species
for i in range(len(uniq)):
    indx = df['Name'] == uniq[i]
    plt.scatter(x[indx], y[indx], s=15, color=scalarMap.to_rgba(i), label=uniq[i])

plt.xlabel('Petal Width')
plt.ylabel('Petal Length')
plt.title('Petal Width vs Length')
plt.legend(loc='upper left')
plt.show()

Gives something like this:

Edit: explicitly add labels for the legend.
",matplotlib
how can i plot different plots for indexes of a dataframe,"I'm new to pandas and trying to learn and ran into this problem.
I've a dataframe and I am trying to plot the rows of the dataframe for each index value. I want different plots (subplots) for each index in the dataframe.
ANDAMAN & NICOBAR ISLANDS   264296  32413   80984   1286    338 31  564 669
ARUNACHAL PRADESH   401876  27045   418732  3287    162815  771 362553  6648
CHANDIGARH  852574  51447   8720    138329  1160    1960    246 1014

Here the string values are the indexes and i want different plots for these indexes of the dataframe.
How can it be done?
","I think you you need is T for transpose and plot with parameter subplots=True:
print(df)

Output:
                                1      2       3       4       5     6       7     8
0                                                                                   
ANDAMAN & NICOBAR ISLANDS  264296  32413   80984    1286     338    31     564   669
ARUNACHAL PRADESH          401876  27045  418732    3287  162815   771  362553  6648
CHANDIGARH                 852574  51447    8720  138329    1160  1960     246  1014

Where column 0 is in your index which contains locations.
The use df.T to transpose the dataframe in to a ""column"" format vs ""rows"".
df.T

Output:
0  ANDAMAN & NICOBAR ISLANDS  ARUNACHAL PRADESH  CHANDIGARH
1                     264296             401876      852574
2                      32413              27045       51447
3                      80984             418732        8720
4                       1286               3287      138329
5                        338             162815        1160
6                         31                771        1960
7                        564             362553         246
8                        669               6648        1014

And, now let's generate plots:
df.T.plot(subplots=True)

Output:

",matplotlib
enforcing matplotlib tick labels not wider than the axes,"I need to make a very compact plot with shared y-axis using matplotlib. To make it compact and neat, I will not have any wspace. It looks good with my data.
But the x-tick labels overlap, making them unreadable.
Is there a way to make the x tick locator not place ticks at the 'edge' of the axes, make the labels adjust the placement so they fall inside the axes width, or make them autodetect the collisions? Or is there a better way to avoid the collision of x tick labels when placing axes close together?
EDIT: I updated the code so it reproduces my original manual and limited-adjusted plots, but also includes the answer, for reference
import matplotlib.pyplot as plt
import matplotlib.ticker
import matplotlib
matplotlib.rcParams['xtick.labelsize'] = 5
matplotlib.rcParams['ytick.labelsize'] = 5

def mkplot():
    fig,axs = plt.subplots(1,2,figsize=(2,2),gridspec_kw={'wspace':0},sharey=True)
    axs[0].plot([0.1,0.2,0.3],[0,2,3])
    axs[0].xaxis.set_major_formatter(matplotlib.ticker.PercentFormatter(xmax=1))
    axs[1].plot([3,2,1],[1,2,3])
    axs[1].yaxis.set_tick_params(labelleft=False,size=0)
    return fig,axs

#######################
fig,axs = mkplot()
fig.suptitle('No adjustment')

#######################
fig,axs= mkplot()
axs[0].set_xlim(0.05,0.32)
axs[0].set_xticks([0.1,0.2,0.3])
axs[1].set_xlim(0.7,3.2)
axs[1].set_xticks([1,2,3])
fig.suptitle('Manual limits and ticks')

#######################
fig,axs = mkplot() 
axs[0].get_xticklabels()[-2].set_horizontalalignment('right')
axs[1].get_xticklabels()[+1].set_horizontalalignment('left')
fig.suptitle('Manual alignment')




","
You can manipulate the text of the tick labels, specifically their horizontal alignment.
Beware that the list of text objects is augmented by two, e.g., for the second axis it is ""0"", ""1"", ""2"", ""3"", ""4"".
import matplotlib.pyplot as plt

fig, (ax0, ax1) = plt.subplots(1, 2, figsize=(2.2, 2),
                               gridspec_kw={'wspace':0},
                               sharey=True)
ax0.plot([0.1, 0.2, 0.3], [0, 2, 3])
ax1.plot([3, 2, 1], [1, 2, 3])

l0 = ax0.get_xticklabels()
l0[-2].set_horizontalalignment('right')
ax0.set_xticklabels(l0)

l1 = ax1.get_xticklabels()
l1[+1].set_horizontalalignment('left')
ax1.set_xticklabels(l1)

fig.savefig('Figure_1.png')

",matplotlib
how to animate a discontinuous function in python,"Using Python 3.12, Matplotlib 3.9.2
I'm trying to animate the velocity-time graph of object B that would stop moving after some time, which make the function discontinuous. However, the velocity function is connected at said discontinuity by a line.
Is there any way to delete that line in my animation?
Here is the code that I used:
import numpy as np
import matplotlib.pyplot as plt
from matplotlib.animation import FuncAnimation 

t = np.linspace(0, 2, 100)
v = -10*t
v = np.where(v <= -10, 0, v)

fig = plt.figure()
plt.axis([0, 3, -12, 12])

av, = plt.plot([],[])

def animate(frame):
   av.set_data(t[:frame],v[:frame])

   return av,

ani = FuncAnimation(fig, animate, frames=len(t))

plt.show()


","Define a threshold for identifying large steps. (or some other metric)
delta_v_threshold = 5

Calculate the difference between consecutive velocity values:
delta_v = np.abs(np.diff(v))

Then identify the steps that are 'jumps'
large_step_indices = np.where(delta_v > delta_v_threshold)[0] + 1

Insert NaN/ replace with Nan to create discontinuity and avoid line drawing:
v_discontinuous = v.copy()
v_discontinuous[large_step_indices] = np.nan

",matplotlib
matplotlib not accepting c3939 for a transparent filling color in scatter plot any more,"I was just reusing some old source code that has worked before and does not any more with the error message: ValueError: 'c' argument must be a color, a sequence of colors, or a sequence of numbers, not ''. I was creating a scatter plot with data from two different classes and highlighting some of the data (ML example with two classes and showing test data). Current Versions: Python 3.12.5, numpy 2.1.1 and matplotlib 3.9.2
Minimal code example:
import matplotlib.pyplot as plt
import numpy as np

X = np.random.randint(1, 26, size=(100, 2))

plt.figure(1)
plt.scatter(X[0:50, 0], X[0:50, 1], c = 'blue', marker='+', s=100)
plt.scatter(X[50:100, 0], X[50:100, 1], c = 'red', marker='+', s=100)
plt.scatter(X[40:60, 0], X[40:60, 1], c = '', marker='s', s=100, edgecolor = 'black')
plt.show()

I checked on my old laptop with Python 3.7.4, numpy 1.21.6 and matplotlib 3.3.0 and I get the desired result:

I also tried to modify the plot statement as follows plt.scatter(X[40:60, 0], X[40:60, 1], c = None, marker='s', s=100, edgecolor = 'black') but get a result with filled square boxes in the plot:

","Thanks RuthC. That solves it. Also interesting that the color is not case sensitive: c=""none"", c=""None"", c=""NONE"" etc. all work.
",matplotlib
accessing the end of of a file being written while live plotting of high speed datastream,"My question refers to the great answer of the following question:
Real time data plotting from a high throughput source
As the gen.py code of this answer was growing fast, I wrote own version gen_own.py below, which essentially imposes a delay of 1 ms before writing a new data on the file. I also adapted the code plot.py and wrote my own plot_own.py essentially adding debugging statements. Although I tried to read the doc on the several components of the f.seek(0, io.SEEK_END) line, there are still several points that I don't understand. Here are all the questions that I have
My question is: how can we adapt plot_own.py to work with gen_own.py (with a slower datastream)
Here is the code gen_own.py:
#!/usr/bin/env python3

import time
import random

LIMIT_TIME = 100  # s
DATA_FILENAME = ""data.txt""


def gen_data(filename, limit_time):
    start_time = time.time()
    elapsed_time = time.time() - start_time
    old_time = time.time()
    with open(filename, ""w"") as f:
        while elapsed_time < limit_time:
            new_time = time.time()
            if new_time > old_time + 0.001:
                f.write(f""{time.time():30.12f} {random.random():30.12f}\n"")  # produces 64 bytes
                f.flush()
                old_time = time.time()
                elapsed = old_time - start_time
            

gen_data(DATA_FILENAME, LIMIT_TIME)

for competeness here is the code of gen.py (copied from original question)
#!/usr/bin/env python3

import time
import random

LIMIT_TIME = 100  # s
DATA_FILENAME = ""data.txt""


def gen_data(filename, limit_time):
    start_time = time.time()
    elapsed_time = time.time() - start_time
    with open(filename, ""w"") as f:
        while elapsed_time < limit_time:
            f.write(f""{time.time():30.12f} {random.random():30.12f}\n"")  # produces 64 bytes
            f.flush()
            elapsed = time.time() - start_time
            

gen_data(DATA_FILENAME, LIMIT_TIME)

Here is the code plot_own.py:
#!/usr/bin/env python3


import io
import time
import matplotlib.pyplot as plt
import matplotlib as mpl
import matplotlib.animation


BUFFER_LEN = 64
DATA_FILENAME = ""data.txt""
PLOT_LIMIT = 20
ANIM_FILENAME = ""video.gif""


fig, ax = plt.subplots(1, 1, figsize=(10,8))
ax.set_title(""Plot of random numbers from `gen.py`"")
ax.set_xlabel(""time / s"")
ax.set_ylabel(""random number / #"")
ax.set_ylim([0, 1])


def get_data(filename, buffer_len, delay=0.0):
    with open(filename, ""r"") as f:
        print(""f.seek(0, io.SEEK_END): "" + str(f.seek(0, io.SEEK_END)))
        data = f.read(buffer_len)
        print(""f.tell(): "" + str(f.tell()))
        print(""f.readline(): "" + f.readline())
        print(""data: "" + data)
        if delay:
            time.sleep(delay)
    return data


def animate(i, xs, ys, limit=PLOT_LIMIT, verbose=False):
    # grab the data
    try:
        data = get_data(DATA_FILENAME, BUFFER_LEN)
        if verbose:
            print(data)
        x, y = map(float, data.split())
        if x > xs[-1]:
            # Add x and y to lists
            xs.append(x)
            ys.append(y)
            # Limit x and y lists to 10 items
            xs = xs[-limit:]
            ys = ys[-limit:]
        else:
            print(f""W: {time.time()} :: STALE!"")
    except ValueError:
        print(f""W: {time.time()} :: EXCEPTION!"")
    else:
        # Draw x and y lists
        ax.clear()
        ax.set_ylim([0, 1])
        ax.plot(xs, ys)


# save video (only to attach here) 
#anim = mpl.animation.FuncAnimation(fig, animate, fargs=([time.time()], [None]), interval=1, frames=3 * PLOT_LIMIT, repeat=False)
#anim.save(ANIM_FILENAME, writer='imagemagick', fps=10)
#print(f""I: Saved to `{ANIM_FILENAME}`"")

# show interactively
anim = mpl.animation.FuncAnimation(fig, animate, fargs=([time.time()], [None]), interval=1)
plt.show()
plt.close()

Here is the output of plot_own.py when run simultaneously with gen.py
f.seek(0, io.SEEK_END): 36998872
f.tell(): 36998936
f.readline():      1731141285.629011392593                 0.423847536979

data:        1731141285.629006385803                 0.946414017554

f.seek(0, io.SEEK_END): 37495182
f.tell(): 37495246
f.readline():      1731141285.670451402664                 0.405303398216

data:        1731141285.670446395874                 0.103460518242

f.seek(0, io.SEEK_END): 38084306
f.tell(): 38084370
f.readline():      1731141285.719735860825                 0.360983611461

data:        1731141285.719730854034                 0.318057761442

Here is the output of plot_own.py when run simultaneously with gen_own.py
W: 1731141977.7246473 :: EXCEPTION!
f.seek(0, io.SEEK_END): 156426
f.tell(): 156426
f.readline():
data:
W: 1731141977.7611823 :: EXCEPTION!
f.seek(0, io.SEEK_END): 158472
f.tell(): 158472
f.readline():
data:
W: 1731141977.79479 :: EXCEPTION!
f.seek(0, io.SEEK_END): 160518
f.tell(): 160518
f.readline():        1731141977.828338146210                 0.165056626254

data:
W: 1731141977.8283837 :: EXCEPTION!
f.seek(0, io.SEEK_END): 162626
f.tell(): 162626
f.readline():
data:
W: 1731141977.8621912 :: EXCEPTION!
f.seek(0, io.SEEK_END): 164734
f.tell(): 164734
f.readline():
data:

","Even without delay, you have to note that only 1 in 2000 lines are being read and printed and displayed, with delay of 1ms it is 1 in 20 line, but in it there is some issue in seeking end and reading which causes data to be empty several times,

you can implement the method tail function from this nice answer

therefore your plot_own.py becomes:
#!/usr/bin/env python3


import io
import os
import subprocess
import time
import matplotlib.pyplot as plt
import matplotlib as mpl
import matplotlib.animation

def tail(f, lines=1, _buffer=4098):
    """"""Tail a file and get X lines from the end""""""
    # place holder for the lines found
    lines_found = []

    # block counter will be multiplied by buffer
    # to get the block size from the end
    block_counter = -1

    # loop until we find X lines
    while len(lines_found) < lines:
        try:
            f.seek(block_counter * _buffer, os.SEEK_END)
        except IOError:  # either file is too small, or too many lines requested
            f.seek(0)
            lines_found = f.readlines()
            break

        lines_found = f.readlines()

        # we found enough lines, get out
        # Removed this line because it was redundant the while will catch
        # it, I left it for history
        # if len(lines_found) > lines:
        #    break

        # decrement the block counter to get the
        # next X bytes
        block_counter -= 1

    return lines_found[-lines:]

BUFFER_LEN = 64
DATA_FILENAME = ""data.txt""
PLOT_LIMIT = 20
ANIM_FILENAME = ""video.gif""


fig, ax = plt.subplots(1, 1, figsize=(10,8))
ax.set_title(""Plot of random numbers from `gen.py`"")
ax.set_xlabel(""time / s"")
ax.set_ylabel(""random number / #"")
ax.set_ylim([0, 1])


def get_data(filename, buffer_len, delay=0.0):
    with open(filename, ""r"") as f:
        data=tail(f, 1, 65)[0]
        print(data)

        if delay:
            time.sleep(delay)
    return data


def animate(i, xs, ys, limit=PLOT_LIMIT, verbose=False):
    # grab the data
    try:
        data = get_data(DATA_FILENAME, BUFFER_LEN)
        if data:
            if verbose:
                print(data)
            x, y = map(float, data.split())
            if x > xs[-1]:
                # Add x and y to lists
                xs.append(x)
                ys.append(y)
                # Limit x and y lists to 10 items
                xs = xs[-limit:]
                ys = ys[-limit:]
            else:
                print(f""W: {time.time()} :: STALE!"")
    except ValueError:
        print(f""W: {time.time()} :: EXCEPTION!"")
    else:
        # Draw x and y lists
        ax.clear()
        ax.set_ylim([0, 1])
        ax.plot(xs, ys)


# save video (only to attach here) 
#anim = mpl.animation.FuncAnimation(fig, animate, fargs=([time.time()], [None]), interval=1, frames=3 * PLOT_LIMIT, repeat=False)
#anim.save(ANIM_FILENAME, writer='imagemagick', fps=10)
#print(f""I: Saved to `{ANIM_FILENAME}`"")

# show interactively
anim = mpl.animation.FuncAnimation(fig, animate, fargs=([time.time()], [None]), interval=1)
plt.show()
plt.close()

or

as for your error
you can just make sure data is not empty before plotting so exception is not raised in your plot_own.py:

def animate(i, xs, ys, limit=PLOT_LIMIT, verbose=False):
    # grab the data
    try:
        data = get_data(DATA_FILENAME, BUFFER_LEN)
        if data:
            if verbose:
                print(data)
            x, y = map(float, data.split())
            if x > xs[-1]:
                # Add x and y to lists
                xs.append(x)
                ys.append(y)
                # Limit x and y lists to 10 items
                xs = xs[-limit:]
                ys = ys[-limit:]
            else:
                print(f""W: {time.time()} :: STALE!"")
    except ValueError:
        print(f""W: {time.time()} :: EXCEPTION!"")
    else:
        # Draw x and y lists
        ax.clear()
        ax.set_ylim([0, 1])
        ax.plot(xs, ys)

yes you are still losing data,but this second code is best, i.e. just validate data in your code before plotting with if data:
Another approach would be to use que, possibly with some heuristics like display every 1 in 5 line,or display according to speed
",matplotlib
how to mimic a jupyter notebook cell in regular python script to plot multiple images and easily review them,"I tend to switch back & forth between Jupyter notebooks and the Wing IDE for debugging code. What I love about the Jupyter notebook is that I can run a cell and generate a sequence of images, with code like this:
import matplotlib.pyplot as plt
for i in range(10):
    ...do stuff...
    plt.plot(image)
    plt.show()

All the images end up nicely one after another in the jupyter cell output region, and can be easily reviewed by scrolling through that region.
Often though I want to go into the debugger to write new code or debug something, and I have not found a good mechanism to do something similar. 
Any suggestions how to go about this ?
One option would be to write a function that writes all images to disk and then just view those with a photo viewing application, but I wonder if there is another way.
","As of now, in 2024, you can do exactly this in VS Code.
Open the notebook in VS Code with python extension.
Then run all the cells up to the cell you want to debug.
Set a breakpoint in the cell you want to debug.
Select the Debug cell option, and VS Code will go in debug mode. It will stop at your breakpoint, and you can step through it, and also enter the code that resides in the other cells.

This is an answer pre VS Code...
I figured a way to do it that works pretty nice:

use a non-interactive back-end to avoid images from popping up

replace all calls to plt.show() with own function my_plt_show(), in which:

call to plt.show()
save each fig to a scratch dir, make sure to close the fig.
write a file 'all.html' in that scratch dir that will display the images. This is the all.html to write:

<html> <img src=""img_0000.jpg"" /> <br /> <img src=""img_0001.jpg"" /> <br /> <img src=""img_0002.jpg"" /> <br /> <img src=""img_0003.jpg"" /> <br />

after running or while debugging, just keep that all.html file open in a browser, and it behaves almost like the output of a jupyter cell.


Here my example code:
I_AM_IN_JUPYTER = False
SCRATCH_IMAGE_DIR = 'C:\\Work\\ScratchImages'
SCRATCH_IMAGE_NUM = 0

if I_AM_IN_JUPYTER:
    get_ipython().magic('matplotlib inline')
else:
    # use non-interactive back-end to avoid images from popping up
    # See: http://stackoverflow.com/questions/9622163/save-plot-to-image-file-instead-of-displaying-it-using-matplotlib-so-it-can-be
    from matplotlib import use
    use('Agg') 

# function to show a plot or write it to disk
def my_plt_show():
    global I_AM_IN_JUPYTER, SCRATCH_IMAGE_NUM, f_html
    plt.show()
    if I_AM_IN_JUPYTER == False:
        # at start
        if SCRATCH_IMAGE_NUM == 0:
            # clean out the scratch image dir
            files = glob.glob(SCRATCH_IMAGE_DIR+'\\*')
            for f in files:
                os.remove(f)  
            # open 'all.html' that displays all the images written
            f_html = open(SCRATCH_IMAGE_DIR+'\\all.html', 'w')
            f_html.write('<html>\n')
            
        # save all images to a scratch dir
        fname = 'img_{:04d}.jpg'.format(SCRATCH_IMAGE_NUM)
        plt.savefig(SCRATCH_IMAGE_DIR+'\\'+fname)
        fig = plt.gcf() # get reference to the current figure
        plt.close(fig)  # and close it
        f_html.write('<img src=""'+fname+'"" /> <br />\n') 
        f_html.flush() # flush it directly to disk, for debug purposes.        
        SCRATCH_IMAGE_NUM += 1    

",matplotlib
presenting complex table data in chart for a single slide,"Tables allow to summarise complex information. I have a table similar following one (this is produce for this question) in my latex document, like so:
\documentclass{article}
\usepackage{graphicx} % Required for inserting images
\usepackage{tabularx}
\usepackage{booktabs}
\usepackage{makecell}

\begin{document}

\begin{table}[bt]
    \caption{Classification results.}
    \label{tab:baseline-clsf-reprt}
    \setlength{\tabcolsep}{1pt} % Adjust column spacing
    \renewcommand{\arraystretch}{1.2} % Adjust row height
    \begin{tabular}{lcccccccccccc}
        \toprule
        & \multicolumn{3}{c}{Data1} &
          \multicolumn{3}{c}{\makecell{Data2 \\ (original)}} &
          \multicolumn{3}{c}{\makecell{Data2 \\ (experiment 3)}} &
          \multicolumn{3}{c}{\makecell{Data2 \\ (experiment 4)}} \\
        \cmidrule(r{1ex}){2-4}
        \cmidrule(r{1ex}){5-7}
        \cmidrule(r{1ex}){8-10}
        \cmidrule(r{1ex}){11-13}
        & Precision & Recall & F1 & Precision & Recall & F1 & Precision & Recall & F1 & Precision & Recall & F1 \\
        \midrule
        Apple  & 0.61 & 0.91 & 0.71 & 0.61 & 0.72 & 0.91 & 0.83   & 0.62 & 0.71 & 0.62 & 0.54 & 0.87 \\
        
        Banana  & 0.90 & 0.32 & 0.36 & 0.86 & 0.81 & 0.53 & 0.61 & 0.69 & 0.68 & 0.72 & 0.56 & 0.57 \\
        
        Orange   & 0.23 & 0.35 & 0.18 & 0.56 & 0.56 & 0.56 & 0.54 & 0.55 & 0.55 & 0.55 & 0.57 & 0.63 \\
        
        Grapes   & 0.81 & 0.70 & 0.76 & 0.67 & 0.47 & 0.54 & 0.85 & 0.28 & 0.42 & 0.38 & 0.66 & 0.48 \\
        
        Mango & 0.31 & 0.23 & 0.45 & 0.87 & 0.54 & 0.73 & 0.63 & 0.57 & 0.63 & 0.75 & 0.29 & 0.34 \\
        \bottomrule
    \end{tabular}
\end{table}

\end{document}


Which gives:

Now, I preparing a slide deck, and I needed to present the classification results in just one slide. To show results of each dataset for each fruit and metric.
My attempts didn't result in a chart that's meaning (showing all info in the table).
First attempt:
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import seaborn as sns

datasets = ['Data1', 'Data2-Orig', 'Data2-Exp3', 'Data2-Exp4']
fruits = ['Apple', 'Banana', 'Orange', 'Grapes', 'Mango']
metrics = ['Precision', 'Recall', 'F1']
colors = ['#1f77b4', '#ff7f0e', '#2ca02c']  # Colors for Precision, Recall, F1

data = {
    'Fruit': ['Apple', 'Banana', 'Orange', 'Grapes', 'Mango'],
    'Data1_Precision': [0.61, 0.90, 0.23, 0.81, 0.31],
    'Data1_Recall': [0.91, 0.32, 0.35, 0.70, 0.23],
    'Data1_F1': [0.71, 0.36, 0.18, 0.76, 0.45],
    'Data2-Orig_Precision': [0.61, 0.86, 0.56, 0.67, 0.87],
    'Data2-Orig_Recall': [0.72, 0.81, 0.56, 0.47, 0.54],
    'Data2-Orig_F1': [0.91, 0.53, 0.56, 0.54, 0.73],
    'Data2-Exp3_Precision': [0.83, 0.61, 0.54, 0.85, 0.63],
    'Data2-Exp3_Recall': [0.62, 0.69, 0.55, 0.28, 0.57],
    'Data2-Exp3_F1': [0.71, 0.68, 0.55, 0.42, 0.63],
    'Data2-Exp4_Precision': [0.62, 0.72, 0.55, 0.38, 0.75],
    'Data2-Exp4_Recall': [0.54, 0.56, 0.57, 0.66, 0.29],
    'Data2-Exp4_F1': [0.87, 0.57, 0.63, 0.48, 0.34]
}

df = pd.DataFrame(data)

# Reshape data for Seaborn
df_melted = df.melt(id_vars='Fruit', 
                    var_name='Metric', 
                    value_name='Score')

# Split the 'Metric' column into separate columns for easier grouping
df_melted[['Dataset', 'Measure']] = df_melted['Metric'].str.split('_', expand=True)
df_melted.drop(columns='Metric', inplace=True)

plt.figure(figsize=(12, 8))
sns.set_style(""whitegrid"")

# Create grouped bar plot
sns.barplot(
    data=df_melted, 
    x='Fruit', 
    y='Score', 
    hue='Dataset', 
    ci=None
)

# Customize plot
plt.title('Classification Results by Fruit and Dataset')
plt.xlabel('Fruit type')
plt.ylabel('Score')
plt.legend(title='Dataset', bbox_to_anchor=(1.05, 1), loc='upper left')

# Show plot
plt.tight_layout()

Gives:

Second attempt:
fig, ax = plt.subplots(figsize=(14, 8))

# Set the width of each bar and spacing between groups
bar_width = 0.2
group_spacing = 0.25
x = np.arange(len(fruits))

# Plot bars for each dataset and metric combination
for i, dataset in enumerate(datasets):
    for j, metric in enumerate(metrics):
        # Calculate the position for each bar within each group
        positions = x + i * (len(metrics) * bar_width + group_spacing) + j * bar_width
        # Plot each metric bar
        ax.bar(positions, 
               df[f'{dataset}_{metric}'], 
               width=bar_width, 
               label=f'{metric}' if i == 0 else """", 
               color=colors[j])

# Customize x-axis and labels
ax.set_xticks(x + (len(datasets) * len(metrics) * bar_width + (len(datasets) - 1) * group_spacing) / 2 - bar_width / 2)
ax.set_xticklabels(fruits)
ax.set_xlabel('Fruit type')
ax.set_ylabel('Score ')
ax.set_title('Classification Results by Dataset, Fruit, and Metric')

# Create custom legend for metrics
metric_legend = [plt.Line2D([0], [0], color=colors[i], lw=4) for i in range(len(metrics))]
ax.legend(metric_legend, metrics, title=""Metrics"", loc=""upper left"", bbox_to_anchor=(1.05, 1))

plt.tight_layout()
plt.show()

This gives:

All these plots does not present the result in a way people can easily flow in a presentation. And adding the original table doesn't just make sense. People cannot easily flow the results in a table as I talk.
How would you recommend plotting the results in this table for adding to a slide?
","I would definitely go for some kind of heatmap. Any barplot-like graphic would be cluttered.
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

data = {
    'Fruit': ['Apple', 'Banana', 'Orange', 'Grapes', 'Mango'],
    'Data1-Precision': [0.61, 0.90, 0.23, 0.81, 0.31],
    'Data1-Recall': [0.91, 0.32, 0.35, 0.70, 0.23],
    'Data1-F1': [0.71, 0.36, 0.18, 0.76, 0.45],
    'Data2-Orig-Precision': [0.61, 0.86, 0.56, 0.67, 0.87],
    'Data2-Orig-Recall': [0.72, 0.81, 0.56, 0.47, 0.54],
    'Data2-Orig-F1': [0.91, 0.53, 0.56, 0.54, 0.73],
    'Data2-Exp3-Precision': [0.83, 0.61, 0.54, 0.85, 0.63],
    'Data2-Exp3-Recall': [0.62, 0.69, 0.55, 0.28, 0.57],
    'Data2-Exp3-F1': [0.71, 0.68, 0.55, 0.42, 0.63],
    'Data2-Exp4-Precision': [0.62, 0.72, 0.55, 0.38, 0.75],
    'Data2-Exp4-Recall': [0.54, 0.56, 0.57, 0.66, 0.29],
    'Data2-Exp4-F1': [0.87, 0.57, 0.63, 0.48, 0.34]
}

df = pd.DataFrame(data)

df_melted = df.melt(id_vars='Fruit', var_name='Dataset-Metric', value_name='Score')
df_melted[['Dataset', 'Metric']] = df_melted['Dataset-Metric'].str.extract(r'(.+)-(.+)')
heatmap_data = df_melted.pivot(index='Fruit', columns=['Dataset', 'Metric'], values='Score')

plt.figure(figsize=(14, 8))
sns.heatmap(
    heatmap_data,
    annot=True,
    fmt="".2f"",
    cmap=""YlGnBu"",
    linewidths=0.5,
    cbar_kws={'label': 'Score'}
)
plt.title('Classification Results Heatmap')
plt.xlabel('Dataset and Metric')
plt.ylabel('Fruit')
plt.tight_layout()
plt.show()



which gives

But if you absolutely want to stick to barplots, choose to do it in 3d:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D

data = {
    'Fruit': ['Apple', 'Banana', 'Orange', 'Grapes', 'Mango'],
    'Data1-Precision': [0.61, 0.90, 0.23, 0.81, 0.31],
    'Data1-Recall': [0.91, 0.32, 0.35, 0.70, 0.23],
    'Data1-F1': [0.71, 0.36, 0.18, 0.76, 0.45],
    'Data2-Orig-Precision': [0.61, 0.86, 0.56, 0.67, 0.87],
    'Data2-Orig-Recall': [0.72, 0.81, 0.56, 0.47, 0.54],
    'Data2-Orig-F1': [0.91, 0.53, 0.56, 0.54, 0.73],
    'Data2-Exp3-Precision': [0.83, 0.61, 0.54, 0.85, 0.63],
    'Data2-Exp3-Recall': [0.62, 0.69, 0.55, 0.28, 0.57],
    'Data2-Exp3-F1': [0.71, 0.68, 0.55, 0.42, 0.63],
    'Data2-Exp4-Precision': [0.62, 0.72, 0.55, 0.38, 0.75],
    'Data2-Exp4-Recall': [0.54, 0.56, 0.57, 0.66, 0.29],
    'Data2-Exp4-F1': [0.87, 0.57, 0.63, 0.48, 0.34]
}

df = pd.DataFrame(data)

df_melted = df.melt(id_vars='Fruit', var_name='Dataset-Metric', value_name='Score')
df_melted[['Dataset', 'Metric']] = df_melted['Dataset-Metric'].str.extract(r'(.+)-(.+)')

fruits = df_melted['Fruit'].unique()
datasets = df_melted['Dataset'].unique()
metrics = df_melted['Metric'].unique()

x = np.array([np.where(fruits == fruit)[0][0] for fruit in df_melted['Fruit']])
y = np.array([np.where(datasets == dataset)[0][0] for dataset in df_melted['Dataset']])
z = np.array([np.where(metrics == metric)[0][0] for metric in df_melted['Metric']])

scores = df_melted['Score'].values

fig = plt.figure(figsize=(12, 8))
ax = fig.add_subplot(111, projection='3d')

dx = dy = 0.4 
dz = scores 

colors = plt.cm.viridis(scores / max(scores))  
ax.bar3d(x, y, np.zeros_like(z), dx, dy, dz, color=colors, alpha=0.8)

ax.set_xlabel('Fruit')
ax.set_ylabel('Dataset')
ax.set_zlabel('Score')

ax.set_xticks(range(len(fruits)))
ax.set_xticklabels(fruits, rotation=45)
ax.set_yticks(range(len(datasets)))
ax.set_yticklabels(datasets)
ax.set_zticks(np.linspace(0, 1, 6))

plt.title('3D Bar Plot of Classification Results')
plt.tight_layout()
plt.show()

which gives

BUT, I still think a heatmap is more readable.
",matplotlib
properly reset matplotlibrcparams dictionary to its original defaults,"This answer mentions that either
fig = plt.figure()
fig.patch.set_facecolor('black')

or
plt.rcParams['figure.facecolor'] = 'black'

will change the value in the rcParams dictionary for the key 'figure.facecolor'.
Suppose that my script has made several changes to the values in a nondeterministic way based on user interaction, and I want to undo all of that and go back to matplotlib's default parameters and behavior.
In the beginning of the script I could check matplotlib.rcParams and store either the whole dictionary, or values for certain keys, and then restore them one at a time or with the .update() method, but I don't know if that's wise because I don't know how else the matplotlib.RcParams instance is used (it's not just a dictionary). It does have a .setdefault() method but I can't understand what help returns on that:
Help on method setdefault in module collections.abc:

setdefault(key, default=None) method of matplotlib.RcParams instance
    D.setdefault(k[,d]) -> D.get(k,d), also set D[k]=d if k not in D

Is there some kind of restore the original default values feature, or should I just wing-it by updating the whole thing with the copy that I've stored?
","Per my understanding and answers to How to recover matplotlib defaults after setting stylesheet you should be able to do this:
import matplotlib
matplotlib.rcParams.update(matplotlib.rcParamsDefault)

You could also check the site-packages/matplotlib/mpl-data folder for the file named matplotlibrc. It should have the entire default values there.
",matplotlib
how to plot bar graphs with same x coordinates side by side 39dodged39,"import matplotlib.pyplot as plt

gridnumber = range(1,4)

b1 = plt.bar(gridnumber, [0.2, 0.3, 0.1], width=0.4,
                label=""Bar 1"", align=""center"")

b2 = plt.bar(gridnumber, [0.3, 0.2, 0.2], color=""red"", width=0.4,
                label=""Bar 2"", align=""center"")


plt.ylim([0,0.5])
plt.xlim([0,4])
plt.xticks(gridnumber)
plt.legend()
plt.show()

Currently b1 and b2 overlap each other. How do I plot them separately like so:

","There is an example in the matplotlib site. Basically, you just shift the x values by width. Here is the relevant bit:
import numpy as np
import matplotlib.pyplot as plt

N = 5
menMeans = (20, 35, 30, 35, 27)
menStd =   (2, 3, 4, 1, 2)

ind = np.arange(N)  # the x locations for the groups
width = 0.35       # the width of the bars

fig = plt.figure()
ax = fig.add_subplot(111)
rects1 = ax.bar(ind, menMeans, width, color='royalblue', yerr=menStd)

womenMeans = (25, 32, 34, 20, 25)
womenStd =   (3, 5, 2, 3, 3)
rects2 = ax.bar(ind+width, womenMeans, width, color='seagreen', yerr=womenStd)

# add some
ax.set_ylabel('Scores')
ax.set_title('Scores by group and gender')
ax.set_xticks(ind + width / 2)
ax.set_xticklabels( ('G1', 'G2', 'G3', 'G4', 'G5') )

ax.legend( (rects1[0], rects2[0]), ('Men', 'Women') )

plt.show()

",matplotlib
show matplotlib plots and other gui in ubuntu wsl1 amp wsl2,"So it seems on ubuntu for windows (windows subsystem for linux) people are suggesting we need to use Agg backend and just save images, not show plots.
import matplotlib
matplotlib.use('Agg') # no UI backend

import matplotlib.pyplot as plt
import numpy as np

t = np.arange(0.0, 2.0, 0.01)
s = 1 + np.sin(2*np.pi*t)
plt.plot(t, s)
plt.title('About as simple as it gets, folks')

#plt.show()
plt.savefig(""matplotlib.png"")  #savefig, don't show

How could we get it to where plt.show() would actually show us an image?  My current option is to override plot.show() to instead just savefig a plot-148123456.png  under /mnt/c/Users/james/plots/ in windows and just have an explorer window open viewing the images.
I suppose I could host that folder and use a browser.
My goal is to be able to run simple examples like the code above without changing the code to ftp the images somewhere etc.  I just want the plot to show up in a window.  
Has anyone figured out a decent way to do it?  
","Ok, so I got it working as follows.  I have Ubuntu on windows, with anaconda python 3.6 installed.

Download and install VcXsrv or Xming (X11 for Windows) from sourceforge(see edit below)
sudo apt-get update
sudo apt-get install python3.6-tk (you may have to install a different python*-tk depnding on the python version you're using)
pip install matplotlib (for matplotlib. but many other things now work too)
export DISPLAY=localhost:0.0 (add to ~/.bashrc to make permanent. see WSL2 below)

Anyways, after all that, this code running in ubuntu on wsl worked as is:
import matplotlib.pyplot as plt
import numpy as np

t = np.arange(0.0, 2.0, 0.01)
s = 1 + np.sin(2*np.pi*t)
plt.plot(t, s)

plt.title('About as simple as it gets, folks')
plt.show()

result:

Maybe this is better done through a Jupyter notebook or something, but it's nice to have basic command-line python matplotlib functionality in Ubuntu for Windows on Subsystem for Linux, and this makes many other gui apps work too.
For example you can install xeyes, and it will say to install x11-apps and installing that will install GTK which a lot of GUI apps use.  But the point is once you have your DISPLAY set correctly, and your x server on windows, then most things that would work on a native ubuntu will work for the WSL.
Edit 2019-09-04 : Today I was having issues with 'unable to get screen resources' after upgrading some libraries.  So I installed VcXsrv and used that instead of Xming.  Just install from https://sourceforge.net/projects/vcxsrv/ and run xlaunch.exe, select multiple windows, next next next ok.  Then everything worked.
Edit for WSL 2 users 2020-06-23
WSL2 (currently insider fast ring) has GPU/docker support so worth upgrade. However it runs in vm. For WSL 2, follow same steps 1-4 then:

the ip is not localhost. it's in resolv.conf so run this instead (and include in ~/.bashrc):

 export DISPLAY=`grep -oP ""(?<=nameserver ).+"" /etc/resolv.conf`:0.0


Now double-check firewall: Windows Security -> Firewall & network protection -> Allow an app through firewall -> make sure VcXsrv has both public and private checked. (When Launching xlaunch first time, you might get a prompt to allow through firewall. This works too. Also, if VcXsrv is not in list of apps, you can manually add it, eg from 'C:\program files\vcxsrv\vcxsrv.exe')
Launch VcXsrv with ""Disable access control"" ticked

Note: a few WSL2 users got error like couldn't connect to display ""172.x.x.x:0"". If that's you try to check the IP address stored in DISPLAY with this command:   echo $DISPLAY. If the showed IP seems to be wrong (i.e. ""8.8.8.8"" or another not working IP address) you need to change the code in ~/.bashrc showed in the point 5 to something that will get your instance's ip address. One user said this worked: export DISPLAY=$(ifconfig | grep inet | awk '{print $2}' | head -n 1 | awk '{print $0"":0""}').  However for some others it did not work. YMMV, but just find your IP and use if for DISPLAY.  For most WSL2 users, the command in #5 works.
Edit for Windows 11 : if MS convinced you to throw out your old computer and buy one with a TPM and so you got Windows 11, you get GUI for free. I hope they add upgrade path to do that on Windows 10 because Win10 will be like XP and last a long time since MS decided you need recent computer even though Win11 would work fine on old computers.
",matplotlib
create an arc between two points in matplotlib,"I am trying to recreate the chart below using matplotlib:

I have most of it done but, I just cant figure out how to create the arcs between the years:
import matplotlib.pyplot as plt
from scipy.interpolate import interp1d
import numpy as np
import pandas as pd

colors = [""#CC5A43"",""#2C324F"",""#5375D4"",]

data = {
    ""year"": [2004, 2022, 2004, 2022, 2004, 2022],
    ""countries"" : [ ""Denmark"", ""Denmark"", ""Norway"", ""Norway"",""Sweden"", ""Sweden"",],
    ""sites"": [4,10,5,8,13,15]
}
df= pd.DataFrame(data)
df = df.sort_values([ 'year'], ascending=True ).reset_index(drop=True)
df['ctry_code'] = df.countries.astype(str).str[:2].astype(str).str.upper()
df['year_lbl'] =""'""+df['year'].astype(str).str[-2:].astype(str)
sites = df.sites
lbl1 = df.year_lbl


fig, ax = plt.subplots( figsize=(6,6),sharex=True, sharey=True, facecolor = ""#FFFFFF"", zorder= 1)


ax.scatter(sites, sites, s= 340, c= colors*2 , zorder = 1)
ax.set_xlim(0, sites.max()+3)
ax.set_ylim(0, sites.max()+3)
ax.axline([ax.get_xlim()[0], ax.get_ylim()[0]], [ax.get_xlim()[1], ax.get_ylim()[1]], zorder = 0, color =""#DBDEE0"" )

for i, l1 in zip(range(0,6), lbl1) :
    ax.annotate(l1, (sites[i], sites[i]), color = ""w"",va= ""center"", ha = ""center"")


ax.set_axis_off()

Which gives me this:

I have tried both mpatches.arc and patches and path but cant make it work.
","Semicircle arc between two points
To draw a semicircle between two points:

the center of the two points will be the center of the circle
for a circular arc, width and height both need to be set to the diameter; that diameter is the distance between the two points (square root of sum of squares of the x and y differences)
the starting angle can be calculated by the arc tangent of the vector from one point to the other
the final angle will be 180º further

Encapsulated in a function, together with a little test:
import matplotlib.pyplot as plt
from matplotlib.patches import Arc
import numpy as np

def draw_semicircle(x1, y1, x2, y2, color='black', lw=1, ax=None):
    '''
    draw a semicircle between the points x1,y1 and x2,y2
    the semicircle is drawn to the left of the segment
    '''
    ax = ax or plt.gca()
    # ax. Scatter([x1, x2], [y1, y2], s=100, c=color)
    startangle = np.degrees(np.arctan2(y2 - y1, x2 - x1))
    diameter = np.sqrt((x2 - x1) ** 2 + (y2 - y1) ** 2)  # Euclidian distance
    ax.add_patch(Arc(((x1 + x2) / 2, (y1 + y2) / 2), diameter, diameter, theta1=startangle, theta2=startangle + 180,
                     edgecolor=color, facecolor='none', lw=lw, zorder=0))

angle = np.linspace(0, 38, 80)
x = angle * np.cos(angle)
y = - angle * np.sin(angle)
fig, ax = plt.subplots()
for x1, y1, x2, y2 in zip(x[:-1], y[:-1], x[1:], y[1:]):
    draw_semicircle(x1, y1, x2, y2, color='fuchsia', lw=2)

ax.set_aspect('equal')  # show circles without deformation
ax.autoscale_view()  # fit the arc into the data limits
ax.axis('off')
plt.show()


Specific code for the data in the question
Here is an adaption of the code for your case (180º arc on a 45º line).  The text can be positioned using the x coordinate of the first and the y coordinate of the second point.
import matplotlib.pyplot as plt
from matplotlib.patches import Arc
import pandas as pd
import math

colors = [""#CC5A43"", ""#2C324F"", ""#5375D4""]
data = {
    ""year"": [2004, 2022, 2004, 2022, 2004, 2022],
    ""countries"": [""Denmark"", ""Denmark"", ""Norway"", ""Norway"", ""Sweden"", ""Sweden""],
    ""sites"": [4, 10, 5, 8, 13, 15]
}
df = pd.DataFrame(data)
df = df.sort_values(['year'], ascending=True).reset_index(drop=True)
df['ctry_code'] = df.countries.astype(str).str[:2].astype(str).str.upper()
df['year_lbl'] = ""'"" + df['year'].astype(str).str[-2:].astype(str)
sites = df.sites
lbl1 = df.year_lbl
countries = df.ctry_code

fig, ax = plt.subplots(figsize=(6, 6), sharex=True, sharey=True, facecolor=""#FFFFFF"", zorder=1)

ax.scatter(sites, sites, s=340, c=colors * 2, zorder=1)
ax.set_xlim(0, sites.max() + 3)
ax.set_ylim(0, sites.max() + 3)
ax.set_aspect('equal')
ax.axline([ax.get_xlim()[0], ax.get_ylim()[0]], [ax.get_xlim()[1], ax.get_ylim()[1]], zorder=0, color=""#DBDEE0"")

for site, l1 in zip(sites, lbl1):
    ax.annotate(l1, (site, site), color=""w"", va=""center"", ha=""center"")

for x1, x2, color, country in zip(sites[:len(sites) // 2], sites[len(sites) // 2:], colors, countries):
    center = (x1 + x2) / 2
    diameter = math.sqrt((x2 - x1) ** 2 + (x2 - x1) ** 2)  # Euclidian distance
    ax.add_patch(Arc((center, center), diameter, diameter, theta1=45, theta2=225,
                     edgecolor=color, facecolor='none', lw=2))
    ax.annotate(country, (x1, x2), color=color, va=""center"", ha=""center"",
                bbox=dict(boxstyle=""round, pad=0.5"", facecolor=""aliceblue"", edgecolor=color, lw=2))
ax.set_axis_off()
plt.show()


",matplotlib
matplotlib  qobjectmovetothread current thread 0x831f000 is not the object39s thread 0x95b40e0 cannot move to target thread 0x831f000,"There were existing posts regarding this error, but not in conjunction with using matplotlib.
QObject::moveToThread: Current thread (0x831f000) is not the object's thread (0x95b40e0).
Cannot move to target thread (0x831f000)

qt.qpa.plugin: Could not load the Qt platform plugin ""xcb"" in ""/home/me/miniconda3/envs/venv/lib/python3.9/site-packages/cv2/qt/plugins"" even though it was found.
This application failed to start because no Qt platform plugin could be initialized. Reinstalling the application may fix this problem.

Available platform plugins are: xcb, eglfs, minimal, minimalegl, offscreen, vnc, webgl.

Aborted (core dumped)

","Setting the environment variable QT_PLUGIN_PATH, in terminal, may resolve the error:
export QT_PLUGIN_PATH=/home/me/miniconda3/envs/venv/lib/python3.9/site-packages/cv2/qt/plugins

If not, install the dependencies:
sudo apt-get install libxcb1

pip install pyqt5

",matplotlib
adjusting axe position after applying constrained_layout,"Using constrained_layout, I can get this plot:
import matplotlib.pyplot as plt
import numpy as np

N = 200
var_xx = 1**2  # var x = std x squared
var_yy = 1**2
cov_xy = 0.5
cov = np.array([[var_xx, cov_xy], [cov_xy, var_yy]])

rng = np.random.default_rng()
pairs = rng.multivariate_normal([0, 0], cov, size=N, check_valid=""raise"")


mosaic = [[""."", ""top""], [""left"", ""main""]]
fig, axarr = plt.subplot_mosaic(mosaic, constrained_layout=True, width_ratios=[0.5, 1], height_ratios=[0.5, 1])

axarr[""main""].scatter(pairs[:, 0], pairs[:, 1], alpha=0.5)
axarr[""top""].hist(pairs[:, 0], bins=20)
axarr[""left""].hist(pairs[:, 0], bins=20, orientation=""horizontal"")

axarr[""left""].sharey(axarr[""main""])
axarr[""top""].sharex(axarr[""main""])

axarr[""top""].tick_params(labelbottom=False)
axarr[""main""].tick_params(labelleft=False)
ticklabels = axarr[""top""].get_yticklabels()

axarr[""main""].set_xlabel(""x"")
axarr[""left""].set_ylabel(""y"")
axarr[""left""].set_xlabel(""PDF"")
axarr[""top""].set_ylabel(""PDF"")



The horizontal spacing between the subplots is larger than the vertical one, due to constrained layout leaving space for the tick and axis labels of the top subplot. I would like to ignore this and reduce the horizontal spacing to the same as the vertical one.
One approach I tried was to set the main axe position after, hence by adding this at the end of the code:
pos_main = axarr[""main""].get_position().transformed(fig.dpi_scale_trans)
pos_top = axarr[""top""].get_position().transformed(fig.dpi_scale_trans)
pos_left = axarr[""left""].get_position().transformed(fig.dpi_scale_trans)

space = pos_top.ymin - pos_main.ymax
pos_main.update_from_data_x([pos_left.xmax + space, pos_main.xmax])
axarr[""main""].set_position(pos_main.transformed(fig.dpi_scale_trans.inverted()))

However, this disables completely constrained_layout for this axe, hence leading to poor results.

How should I first apply constrained_layout, then disable it, and adjust the axis positions?
","Other options:
Matplotlib v3.10 will soon be released.  You can install the pre-release version with
pip install matplotlib==3.10.0rc1

In the new release, we can tell constrained layout to ignore an axis:
axarr[""top""].yaxis.set_in_layout(False)


Before v3.10, we could make the axis invisible so constrained layout ignores it and then make it visible again after turning the layout engine off.
axarr[""top""].yaxis.set_visible(False)
fig.canvas.draw()
fig.set_layout_engine(""none"")
axarr[""top""].yaxis.set_visible(True)

",matplotlib
error 15 initializing libiomp5dylib but found libiomp5dylib already initialized,"Getting the error message when using matplotlib:

Error #15: Initializing libiomp5.dylib, but found libiomp5.dylib
  already initialized
      OMP: Hint: This means that multiple copies of the OpenMP runtime have been linked into the program. That is dangerous, since it can
  degrade performance or cause incorrect results. The best thing to do
  is to ensure that only a single OpenMP runtime is linked into the
  process, e.g. by avoiding static linking of the OpenMP runtime in any
  library. As an unsafe, unsupported, undocumented workaround you can
  set the environment variable KMP_DUPLICATE_LIB_OK=TRUE to allow the
  program to continue to execute, but that may cause crashes or silently
  produce incorrect results. For more information, please see
  http://www.intel.com/software/products/support/.

","Do the following to solve the issue:
import os

os.environ['KMP_DUPLICATE_LIB_OK']='True'

Answer found at: https://github.com/dmlc/xgboost/issues/1715
Be aware of potential side-effects:

but that may cause crashes or silently produce incorrect results.

",matplotlib
matplotlib stackplot gradient,"Is it possible to fill the ""Odds"" area with a gradient from left (green) to right (transparent)? I would like to do this in a plot to indicate uncertainty.
import numpy as np
import matplotlib.pyplot as plt

x = [1, 2, 3, 4, 5]
y1 = [1, 1, 2, 3, 5]
y2 = [0, 4, 2, 6, 8]
y3 = [1, 3, 5, 7, 9]

y = np.vstack([y1, y2, y3])

labels = [""Fibonacci "", ""Evens"", ""Odds""]

fig, ax = plt.subplots()
ax.stackplot(x, y1, y2, y3, labels=labels)
ax.legend(loc='upper left')
plt.show()

fig, ax = plt.subplots()
ax.stackplot(x, y)
plt.show()



","Based on this answer, you could do:
import numpy as np
import matplotlib.pyplot as plt

x = [1, 2, 3, 4, 5]
y1 = [1, 1, 2, 3, 5]
y2 = [0, 4, 2, 6, 8]
y3 = [1, 3, 5, 7, 9]

y = np.vstack([y1, y2, y3])

labels = [""Fibonacci "", ""Evens"", ""Odds""]

fig, ax = plt.subplots()
stacks = ax.stackplot(x, y1, y2, y3, labels=labels)
ax.legend(loc='upper left')

xlim = ax.get_xlim()
ylim = ax.get_ylim()

stacks[-1].set_facecolor([1, 1, 1, 0])  # set original face color to transparent

verts = np.vstack([p.vertices for p in stacks[-1].get_paths()])
gradient = ax.imshow(
    np.linspace(0, 1, 256).reshape(1, -1), 
    cmap='Greens_r',  # reversed Green colormap
    aspect='auto',
    extent=[verts[:, 0].min(), verts[:, 0].max(), verts[:, 1].min(), verts[:, 1].max()]
)

gradient.set_clip_path(stacks[-1].get_paths()[0], transform=ax.transData)
ax.set_xlim(xlim)
ax.set_ylim(ylim)

fig.show()


",matplotlib
use line2d to plot line in matplotlib,"I have the data:
x = [10,24,23,23,3]
y = [12,2,3,4,2]

I want to plot it using matplotlib.lines.Line2D(xdata, ydata). I tried:
import matplotlib.lines
matplotlib.lines.Line2D(x, y)

But how do I show the line?
","You should add the line to a plot and then show it:
In [13]: import matplotlib.pyplot as plt

In [15]: from matplotlib.lines import Line2D      

In [16]: fig = plt.figure()

In [17]: ax = fig.add_subplot(111)

In [18]: x = [10,24,23,23,3]

In [19]: y = [12,2,3,4,2]

In [20]: line = Line2D(x, y)

In [21]: ax.add_line(line)
Out[21]: <matplotlib.lines.Line2D at 0x7f4c10732f60>

In [22]: ax.set_xlim(min(x), max(x))
Out[22]: (3, 24)

In [23]: ax.set_ylim(min(y), max(y))
Out[23]: (2, 12)

In [24]: plt.show()

The result:

",matplotlib
a logarithmic colorbar in matplotlib scatter plot,"I would like to make the colors of the points on the scatter plot correspond to the value of the void fraction, but on a logarithmic scale to amplify differences. I did this, but now when I do plt.colorbar(), it displays the log of the void fraction, when I really want the actual void fraction. How can I make a log scale on the colorbar with the appropriate labels of the void fraction, which belongs to [0.00001,1]?
Here is an image of the plot I have now, but the void fraction colorbar is not appropriately labeled to correspond to the true void fraction, instead of the log of it.

fig = plt.figure()
plt.scatter(x,y,edgecolors='none',s=marker_size,c=np.log(void_fraction))
plt.colorbar()
plt.title('Colorbar: void fraction')

Thanks for your help.
","There is now a section of the documentation describing how color mapping and normalization works
The way that matplotlib does color mapping is in two steps, first a Normalize function (wrapped up by the sub-classes of matplotlib.colors.Normalize) which maps the data you hand in to [0, 1].  The second step maps values in [0,1] -> RGBA space.
You just need to use the LogNorm normalization class, passed in with the norm kwarg.
plt.scatter(x,y,edgecolors='none',s=marker_size,c=void_fraction,
                norm=matplotlib.colors.LogNorm())

When you want to scale/tweak data for plotting, it is better to let matplotlib do the transformations than to do it your self.

Normalize doc
LogNorm doc
matplotlib.color doc

",matplotlib
how to annotate end of lines using python and matplotlib,"With a dataframe and basic plot such as this:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

np.random.seed(123456)
rows = 75
df = pd.DataFrame(np.random.randint(-4,5,size=(rows, 3)), columns=['A', 'B', 'C'])
datelist = pd.date_range(pd.datetime(2017, 1, 1).strftime('%Y-%m-%d'), periods=rows).tolist()
df['dates'] = datelist 
df = df.set_index(['dates'])
df.index = pd.to_datetime(df.index)
df = df.cumsum()

df.plot()


What is the best way of annotating the last points on the lines so that you get the result below?

","In order to annotate a point use ax.annotate(). In this case it makes sense to specify the coordinates to annotate separately. I.e. the y coordinate is the data coordinate of the last point of the line (which you can get from line.get_ydata()[-1]) while the x coordinate is independent of the data and should be the right hand side of the axes (i.e. 1 in axes coordinates). You may then also want to offset the text a bit such that it does not overlap with the axes.
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

rows = 75
df = pd.DataFrame(np.random.randint(-4,5,size=(rows, 3)), columns=['A', 'B', 'C'])
datelist = pd.date_range(pd.datetime(2017, 1, 1).strftime('%Y-%m-%d'), periods=rows).tolist()
df['dates'] = datelist 
df = df.set_index(['dates'])
df.index = pd.to_datetime(df.index)
df = df.cumsum()

ax = df.plot()

for line, name in zip(ax.lines, df.columns):
    y = line.get_ydata()[-1]
    ax.annotate(name, xy=(1,y), xytext=(6,0), color=line.get_color(), 
                xycoords = ax.get_yaxis_transform(), textcoords=""offset points"",
                size=14, va=""center"")

plt.show()


",matplotlib
histogram for discrete values with matplotlib,"I sometimes have to histogram discrete values with matplotlib.  In that case, the choice of the binning can be crucial: if you histogram [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10] using 10 bins, one of the bins will have twice as many counts as the others.  In other terms, the binsize should normally be a multiple of the discretization size.
While this simple case is relatively easy to handle by myself, does anyone have a pointer to a library/function that would take care of this automcatically, including in the case of floating-point data where the discretization size could be slightly varying due to FP rounding?
Thanks.
","Given the title of your question, I will assume that the discretization size is constant.
You can find this discretization size (or at least, strictly, n times that size as you may not have two adjacent samples in your data)
np.diff(np.unique(data)).min()

This finds the unique values in your data (np.unique), finds the differences between then (np.diff).  The unique is needed so that you get no zero values.  You then find the minimum difference.  There could be problems with this where discretization constant is very small - I'll come back to that.
Next - you want your values to be in the middle of the bin - your current issue is because both 9 and 10 are on the edges of the last bin that matplotlib automatically supplies, so you get two samples in one bin.
So - try this:
import matplotlib.pyplot as plt
import numpy as np

data = range(11)
data = np.array(data)

d = np.diff(np.unique(data)).min()
left_of_first_bin = data.min() - float(d)/2
right_of_last_bin = data.max() + float(d)/2
plt.hist(data, np.arange(left_of_first_bin, right_of_last_bin + d, d))
plt.show()

This gives:


Small non-integer discretization
We can make a bit more of a testing data set e.g.
import random 

data = []
for _ in range(1000):
    data.append(random.randint(1,100))
data = np.array(data)
nasty_d = 1.0 / 597 #Arbitrary smallish discretization
data = data * nasty_d

If you then run that through the array above and have a look at the d that the code spits out you will see

>>> print(nasty_d)
0.0016750418760469012
>>> print(d)
0.00167504187605


So - the detected value of d is not the ""real"" value of nasty_d that the data was created with.  However - with the trick of shifting the bins by half of d to get the values in the middle - it shouldn't matter unless your discretization is very very small so your down in the limits of precision of a float or you have 1000s of bins and the difference between detected d and ""real"" discretization can build up to such a point that one of the bins ""misses"" the data point.  It's something to be aware of, but probably won't hit you.
An example plot for the above is


Non uniform discretization / most appropriate bins...
For further more complex cases, you might like to look at this blog post I found.  This looks at ways of automatically ""learning"" the best bin widths from (continuous / quasi-continuous) data, referencing multiple standard techniques such as Sturges' rule and Freedman and Diaconis' rule before developing its own Bayesian dynamic programming method.
If this is your use case - the question is far broader and may not be suited to a definitive answer on Stack Overflow, although hopefully the links will help.
",matplotlib
adding a labeled point to a venn diagram in matplotlibvenn,"Say, I am using python and the matplotlib-venn package to create some Venn diagrams. However, I wanted to include a labeled point inside one of the circles. That way I can show that point x is an element of a set A. Is there a way to simply add a point to a diagram in matplotlib-venn?
EDIT: I added a little picture to demonstrate.

Minimal Working Example:
This code will just create the venn diagram but without the point
from matplotlib import pyplot as plt
import numpy as np
from matplotlib_venn import venn2
plt.figure(figsize=(4,4))
v = venn2(subsets = (3, 2, 1))
plt.show()

","The Venn diagram is centered at x,y = 0,0. Just plot your point at the desired x,y.
from matplotlib import pyplot as plt
from matplotlib_venn import venn2
plt.figure(figsize=(4,4))
v = venn2(subsets = (3, 2, 1))

plt.axhline(0, linestyle='--')
plt.axvline(0, linestyle='--')

plt.plot(-0.5,0.2,'bo')
plt.text(-0.6,0.2, 'A')

plt.show()

",matplotlib
problem with symbol opacity of errorbar within legend,"I'm trying to indicate perfectly symbols within legend once I want to plot complicated combinations of line and errorbar in grid plots. I noticed that it's not easy to apply desired opacity for any symbol kinds when they are error bar.
I have tried following checking this post unsuccessfully.
import matplotlib.pyplot as plt
from matplotlib.collections import PathCollection
from matplotlib.legend_handler import HandlerPathCollection, HandlerLine2D, HandlerErrorbar


x1 = np.linspace(0,1,8)
y1 = np.random.rand(8)

# Compute prediction intervals
sum_of_squares_mid = np.sum((x1 - y1) ** 2)
std_mid            = np.sqrt(1 / (len(x1) - 2) * sum_of_squares_mid)

# Plot the prediction intervals
y_err_mid = np.vstack([std_mid, std_mid]) * 1.96

plt.plot(x1, y1, 'bo', label='label', marker=r""$\clubsuit$"",  alpha=0.2)                                                 # Default alpha is 1.0.
plt.errorbar(x1, y1, yerr=y_err_mid, fmt=""o"", ecolor=""#FF0009"", capsize=3, color=""#FF0009"", label=""Errorbar"", alpha=.1)  # Default alpha is 1.0.


def update(handle, orig):
    handle.update_from(orig)
    handle.set_alpha(1)

plt.legend(handler_map={PathCollection : HandlerPathCollection(update_func = update),
                            plt.Line2D : HandlerLine2D(        update_func = update),
                          plt.errorbar : HandlerErrorbar(      update_func = update) # I added this but it deos not apply alpha=1 only for errobar symbol in legend
                        })

plt.show()

My current output:

","It appears that you were not specifying the right handler for the second Artist which is an ErrorbarContainer, thus the set_alpha(1) instruction was not executed for that object.
Indeed, importing
import matplotlib.pyplot as plt
from matplotlib.container import ErrorbarContainer
from matplotlib.legend_handler import HandlerLine2D, HandlerErrorbar

and then modifying the handler_map to
def update(handle, orig):
    handle.update_from(orig)
    handle.set_alpha(1)

leg = plt.legend(handler_map={
    plt.Line2D : HandlerLine2D(update_func = update),
    ErrorbarContainer: HandlerErrorbar(update_func = update)
                        })

results in

Hope this helps!
",matplotlib
stop seaborn changing matplotlib plot style,"I have already found an entry that deals with a similar topic, but the suggestion doesn't work here. 
How can I use seaborn without changing the matplotlib defaults?
If I missed something I am grateful for every link.
I want to create a plot with matplotlib after having created a plot with seaborn. However, the settings of seaborn seem to affect the matplotlib appearance (I realize that seaborn is an extension of matplotlib). This happens even though I clear, close the plot etc with.
    sns.reset_orig()
    plt.clf()
    plt.close()

Complete example code:
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import numpy as np
# data
df = pd.DataFrame(np.array([[1, 1], [2, 2], [3, 3]]),columns=['x', 'y'])

###### seaborn plot #######
fig=sns.JointGrid(x=df['x'],
                  y=df['y'],
                  )
#fill with scatter and distribution plot
fig = fig.plot_joint(plt.scatter, color=""b"")                            
fig = fig.plot_marginals(sns.distplot, kde=False, color=""b"")

#axis labels
fig.set_axis_labels('x','y')        

#set title
plt.subplots_adjust(top=0.92)
title='some title'
fig.fig.suptitle(title)

#clear and close figure
sns.reset_orig()
plt.clf()
plt.close()        

###### matplotlib plot #######
#define data to plot
x = df['x']
y = df['y']

#create figure and plot
fig_mpl, ax = plt.subplots()
ax.plot(x,y,'.')
ax.grid(True)
ax.set_xlabel('x')
ax.set_ylabel('y')
title='some title'
ax.set_title(title)
plt.close()

The seaborn plot always looks the same:
seaborn plot
But the apperance of the matplotlib plot differs. The normal one without creating a seaborn plot in front:
mpl plot normal
and how it changes if using the shown code: 
mpl with sns in front
how do I stop this behaviour, avoid the seaborn influencing the other plots?
","When you import seaborn the default styling is changed.
You can change the style that matplotlib applies to plots with the plt.style.use command.
To get a list of available styles you can use plt.style.available. To change back to the classic matplotlib style you'll want to use plt.style.use('classic').
",matplotlib
how do i set the figure title and axes labels font size,"I am creating a figure in Matplotlib like this:
from matplotlib import pyplot as plt

fig = plt.figure()
plt.plot(data)
fig.suptitle('test title')
plt.xlabel('xlabel')
plt.ylabel('ylabel')
fig.savefig('test.jpg')

I want to specify font sizes for the figure title and the axis labels. I need all three to be different font sizes, so setting a global font size (mpl.rcParams['font.size']=x) is not what I want. How do I set font sizes for the figure title and the axis labels individually?
","Functions dealing with text like label, title, etc. accept parameters same as matplotlib.text.Text. For the font size you can use size/fontsize:
from matplotlib import pyplot as plt    

fig = plt.figure()
plt.plot(data)
fig.suptitle('test title', fontsize=20)
plt.xlabel('xlabel', fontsize=18)
plt.ylabel('ylabel', fontsize=16)
fig.savefig('test.jpg')

For globally setting title and label sizes, mpl.rcParams contains axes.titlesize and axes.labelsize. (From the page):
axes.titlesize      : large   # fontsize of the axes title
axes.labelsize      : medium  # fontsize of the x any y labels

(As far as I can see, there is no way to set x and y label sizes separately.)
And I see that axes.titlesize does not affect suptitle. I guess, you need to set that manually.
",matplotlib
plot single data with two y axes two units in matplotlib,"I'm trying to plot a single time series, but want to represent it in two units on left and right axes. Here's what I have done so far.
fig, ax1 = plt.subplots()
t = np.arange(1,50,1)
s1 = np.sin(t)*20000+40000 #synthetic ts, but closer to my data 
ax1.plot(t, s1, 'b-')

ax1.set_xlabel('time')
ax1.set_ylim(20000,70000)
ax1.set_ylabel('km3/year')

km3yearToSv=31.6887646*(1/1e6)

ax2 = ax1.twinx()
s2 = s1*km3yearToSv
ax2.plot(t, s2, 'b-')
ax2.set_ylim(20000*km3yearToSv,70000*km3yearToSv)
ax2.set_ylabel('Sv')


By adjusting the ylim(), I can get it to seem as a single line, but some aliasing can be seen. I would prefer if I don't have to plot the data twice.
Any suggestions?
","There is no need to plot it twice, this much should give you the result you want:
ax2 = ax1.twinx()
ax2.set_ylim(20000*km3yearToSv, 70000*km3yearToSv)
ax2.set_ylabel('Sv')

A more robust way to do it is to first extract the limits of the plot (in case you change them, and they're no longer 20000 and 70000, or you want the plot to be able to automatically adjust the limits:
ax2 = ax1.twinx()
mn, mx = ax1.get_ylim()
ax2.set_ylim(mn*km3yearToSv, mx*km3yearToSv)
ax2.set_ylabel('Sv')


Altogether, with some other small tweaks:
import numpy as np
import matplotlib.pyplot as plt

mean, amp = 40000, 20000
t = np.arange(50)
s1 = np.sin(t)*amp + mean #synthetic ts, but closer to my data 

fig, ax1 = plt.subplots()
ax1.plot(t, s1, 'b-')

ax1.set_xlabel('time')
mn, mx = ax1.set_ylim(mean-amp, mean+amp)
ax1.set_ylabel('km$^3$/year')

km3yearToSv = 31.6887646e-6

ax2 = ax1.twinx()
ax2.set_ylim(mn*km3yearToSv, mx*km3yearToSv)
ax2.set_ylabel('Sv')

",matplotlib
draw a circle with periodic boundary conditions matplotlib,"I am doing a project that involves lattices. A point of coordinates (x0, y0) is chosen randomly and I need to color blue all the points that are in the circle of center (x0, y0) and radius R and red all the other points and then draw a circle around.
The tricky part is that there is periodic boundary conditions, meaning that if my circle is near the left border then I need to draw the rest of it on the right side, the same goes for up and down.
Here is my code that plots the lattice, I have managed to color the points depending on whether or not they are in the circle but I am yet to draw the circle.
from matplotlib import pyplot as plt
import numpy as np

class lattice:

    def __init__(self, L):
        self.L = L
        self.positions = np.array([[[i, j] for i in range(L)] for j in range(L)])
    
    def draw_lattice(self, filename):
        X = self.positions[:, :, 0].flatten()
        Y = self.positions[:, :, 1].flatten()
        plt.scatter(X, Y, s=10)
        plt.xticks([])
        plt.yticks([])
        plt.title(""Lattice"")
        plt.savefig(filename)
    
    def dist_centre(self):
        x0, y0 = np.random.randint(0, self.L), np.random.randint(0, self.L)
        self.c0 = (x0, y0)
        self.distance = np.zeros((self.L, self.L))

        for i in range(self.L):
            for j in range(self.L):
                x = self.positions[i, j, 0]
                y = self.positions[i, j, 1]
                # Distance with periodic boundary conditions.
                Dx = -self.L/2 + ((x0-x)+self.L/2)%self.L
                Dy = -self.L/2 + ((y0-y)+self.L/2)%self.L
                dist = np.sqrt(Dx**2 + Dy**2)
                self.distance[i, j] = dist

    def draw_zone(self, filename, R):
        colormap = np.where(self.distance <= R, ""blue"", ""red"").flatten()

        X = self.positions[:, :, 0].flatten()
        Y = self.positions[:, :, 1].flatten()
        plt.clf()
        plt.scatter(X, Y, s=10, color=colormap)
        plt.xticks([])
        plt.yticks([])
        plt.title(""Lattice"")
        plt.savefig(filename)

if __name__ == ""__main__"":
    L = 10
    R = 3
    filename = ""test.pdf""
    latt = lattice(L)
    latt.draw_lattice(filename)
    latt.dist_centre()
    latt.draw_zone(filename, R)

The formula for the distance is modified because of the periodic boundary conditions.

","The comment from Tino_D gave me the answer. I imagined a bigged lattice, my lattice and 8 lattices surrounding it and drew a total of 9 circles with a centers that were translated to another sub lattice and then I restricted my plot to the original lattice.
    def draw_zone(self, filename, R):
        colormap = np.where(self.distance <= R, ""blue"", ""red"").flatten()

        X = self.positions[:, :, 0].flatten()
        Y = self.positions[:, :, 1].flatten()

        x0, y0 = self.c0
        centers = [(x0-L, y0-L), (x0-L, y0), (x0-L, y0+L),
                   (x0, y0-L), (x0, y0), (x0, y0+L),
                   (x0+L, y0-L), (x0+L, y0), (x0+L, y0+L)]
        
        plt.clf()

        for (x,y) in centers:
            circle = plt.Circle((x, y), R, alpha=0.2, color=""grey"")
            plt.gca().add_patch(circle)

        plt.scatter(X, Y, s=10, color=colormap)
        plt.xticks([])
        plt.yticks([])
        plt.xlim(0, self.L-1)
        plt.ylim(0, self.L-1)
        plt.title(""Lattice"")
        plt.savefig(filename)


",matplotlib
how to draw scaleindependent horizontal bars with tips in matplotlib,"I want to create a plot that shows genomic coding regions as arrows that may contain colorfully highlighted domain regions.
In principle it is something like this:
import numpy as np
import matplotlib.patches as patches
import matplotlib.pyplot as plt

def test(bar_height=0.8, figsize=(10, 6), arrow_headlen=0.2, dpi=600):
    X0 = np.arange(0, 10, 1)
    X1 = X0 + 2
    Y = np.arange(0, 10, 1)

    fig = plt.figure(figsize=figsize)
    ax = fig.add_subplot(111)

    data_2_px = ax.transData.transform  # data domain to figure
    px_2_data = ax.transData.inverted().transform  # figure to data domain

    # get arrow head_length as fraction of arrow width
    # so that it doesnt grow longer with longer x-axis
    dy = bar_height * arrow_headlen
    dpx = data_2_px([(0, dy)]) - data_2_px([(0, 0)])
    arrowlen = (px_2_data([(dpx[0, 1], dpx[0, 0])]) - px_2_data([(0, 0)]))[0, 0]

    ax.barh(y=Y, left=X0, width=X1 - X0, height=bar_height, color=""0.5"")
    for y, x1 in zip(Y, X1):
        yl = y - 0.49 * bar_height  # low arrow corner (avoid being draw 1 px too low)
        yh = y + 0.49 * bar_height  # high arrow corner (avoid being draw 1 px too high)
        arrow = patches.Polygon([(x1, yl), (x1, yh), (x1 + arrowlen, y)], color=""0.5"")
        ax.add_patch(arrow)

    # highlight parts of arrows
    ax.barh(y=Y, left=X0 + 0.5, width=(X1 - X0) / 2, height=bar_height, color=""blue"")

    fig.savefig(""./test_from_savefig.png"", dpi=dpi)
    plt.show()

This draws 10 transcript ""arrows"" in gray and each of these transcripts contains a region highlighted in blue.
When plt.show() opens this in a viewer and I save it from there I get this image (A):

The picture that is saved by fig.savefig() with higher DPI however gives this image (B):

As you can see the arrow heads are suddenly not flush with the arrow base anymore.
It seems that they were scaled differently than the bars.
But both of them are defined in the data domain.
So shouldn't they still be flush.
Image A is what I want to create:

gray arrows of a defined width whose head width is the same as the arrow base width
arrow heads that do not grow longer or shorter depending on the x-axis
possibility to highlight a part of the arrow base

However, I also want to be able to save this plot as raster graphic in a higher resolution.
Why don't I use FancyArrow?
FancyArrow would be a more straight-forward way of defining arrows.
However, they are not drawn in a very reproducible way.
Sometimes a FancyArrow is drawn 1 pixel higher or lower.
This means If I draw a gray FancyArrow and then a blue rectangle over it, there will sometimes be some visible misalignment (e.g. a 1 pixel gray line visible behind the blue area).
I have found that only barh is able to draw a bar of different colors that actually looks like it belongs together.
","To have the arrow heads flush with the arrow bodies even when modifying the dpi value, set linewidth=0 in the patches.Polygon initialization.
Your code then gives this image (saved as PNG with 600 DPI)

Please note that for some reason, now a thin white line appears at the conjunction of some body-head pairs (see the second one starting from top, for example).
To go deeper into this latter issue, I have refactored the code using the rectangle corners to define the triangle vertex (which I paste below), but the result looks just the same.
import numpy as np
import matplotlib.patches as patches
import matplotlib.pyplot as plt

def test(arrow_height=0.8, arrow_bodylen=2.0, arrow_headlen=0.2, 
         figsize=(10, 6), dpi=600):
    # Define points corresponding to arrow tails
    X_tail = np.arange(0, 10, 1)
    Y = np.arange(0, 10, 1)

    # Create the figure
    fig = plt.figure(figsize=figsize)    
    ax = fig.add_subplot(111)

    # Draw arrow bodies (i.e. rectangles)
    h_bars = ax.barh(y=Y,
                     left=X_tail, 
                     width=arrow_bodylen, 
                     height=arrow_height, 
                     color=""0.5"",
                     linewidth = 0  # though it does not seem to have any effect
                    )
    # Loop through rectangles to add heads (i.e. triangles) to arrow bodies
    for bar in h_bars.patches:
        # `get_corners()` return the rectangle corners, starting from `bar.xy`
        # (here it is lower-left corner) in counter-clockwise order. 
        bar_LL, bar_LR, bar_TR, bar_TL = bar.get_corners()
        assert bar_LR[0] == bar_TR[0] # probably could be removed...
        # Define the point corresponding to the arrow tip
        arrow_tip = ( bar_LR[0] + arrow_height * arrow_headlen,
                     (bar_LR[1] + bar_TR[1]) / 2
                     )
        # Define a triangle between the right end of the bar and the arrow tip
        arrow = patches.Polygon([bar_LR, 
                                 bar_TR, 
                                 arrow_tip
                                 ], 
                                color=""0.5"",
                                linewidth = 0
                                )
        ax.add_patch(arrow) # draw the triangle
    
    # Highlight parts of the arrow bodies
    ax.barh(y=Y, 
            left=X_tail + 0.25 * arrow_bodylen, 
            width=arrow_bodylen / 2, 
            height=arrow_height, 
            color=""blue"",
            linewidth = 0
            )
    
    fig.savefig(""./test_from_savefig.png"", dpi=dpi)
    plt.show()

test()

Hope this helps at least a bit!
",matplotlib
pandas fails to hide nan entries from stacked line graphs,"Say I have the following data:
Date,release,count
2019-03-01,buster,0
2019-03-01,jessie,1
2019-03-01,stretch,74
2019-08-15,buster,25
2019-08-15,jessie,1
2019-08-15,stretch,49
2019-10-07,buster,35
2019-10-07,jessie,1
2019-10-07,stretch,43
2019-10-08,buster,40
2019-10-08,jessie,1
2019-10-08,stretch,38
2019-10-09,buster,46
2019-10-09,jessie,1
2019-10-09,stretch,33
2019-10-23,buster,46
2019-10-23,jessie,1
2019-10-23,stretch,31
2019-11-25,buster,46
2019-11-25,jessie,1
2019-11-25,stretch,29
2020-01-13,buster,48
2020-01-13,jessie,1
2020-01-13,stretch,28
2020-01-29,buster,50
2020-01-29,jessie,1
2020-01-29,stretch,26
2020-03-10,buster,54
2020-03-10,jessie,1
2020-03-10,stretch,22
2020-04-14,buster,55
2020-04-14,jessie,0
2020-04-14,stretch,21
2020-05-11,buster,57
2020-05-11,jessie,0
2020-05-11,stretch,17
2020-05-25,buster,61
2020-05-25,jessie,0
2020-05-25,stretch,14
2020-06-10,buster,62
2020-06-10,stretch,12
2020-07-01,buster,69
2020-07-01,stretch,3
2020-10-30,buster,74
2020-10-30,stretch,2
2020-11-18,buster,76
2020-11-18,stretch,2
2021-08-26,bullseye,1
2021-08-26,buster,86
2021-08-26,stretch,1
2021-10-08,bullseye,4
2021-10-08,buster,86
2021-10-08,stretch,1
2021-11-11,bullseye,4
2021-11-11,buster,84
2021-11-11,stretch,1
2021-11-17,bullseye,4
2021-11-17,buster,85
2021-11-17,stretch,0

And the following code:
import pandas as pd
import matplotlib.pyplot as plt

# Load the data
df = pd.read_csv('subset.csv')

# Pivot the data to a suitable format for plotting
df = df.pivot_table(index=""Date"", columns='release', values='count', aggfunc='sum')

# Convert the index to datetime and sort it
df.index = pd.to_datetime(df.index)

print(df)

# Plotting the data with filled areas
fig, ax = plt.subplots(figsize=(12, 6))
df.plot(ax=ax, kind=""area"", stacked=True)

plt.show()

It generates the following graph:

In the above graph, the jessie line should have stopped after 2020-05-25, in the middle of the graph. But it just keeps going, a little energizer bunny of a line, all the way to the right of the graph, even though it's actually NaN. In the print(df) output, we can see this is the underlying dataframe after the pivot:
release     bullseye  buster  jessie  stretch
Date                                         
2019-03-01       NaN     0.0     1.0     74.0
2019-08-15       NaN    25.0     1.0     49.0
2019-10-07       NaN    35.0     1.0     43.0
2019-10-08       NaN    40.0     1.0     38.0
2019-10-09       NaN    46.0     1.0     33.0
2019-10-23       NaN    46.0     1.0     31.0
2019-11-25       NaN    46.0     1.0     29.0
2020-01-13       NaN    48.0     1.0     28.0
2020-01-29       NaN    50.0     1.0     26.0
2020-03-10       NaN    54.0     1.0     22.0
2020-04-14       NaN    55.0     0.0     21.0
2020-05-11       NaN    57.0     0.0     17.0
2020-05-25       NaN    61.0     0.0     14.0
2020-06-10       NaN    62.0     NaN     12.0
2020-07-01       NaN    69.0     NaN      3.0
2020-10-30       NaN    74.0     NaN      2.0
2020-11-18       NaN    76.0     NaN      2.0
2021-08-26       1.0    86.0     NaN      1.0
2021-10-08       4.0    86.0     NaN      1.0
2021-11-11       4.0    84.0     NaN      1.0
2021-11-17       4.0    85.0     NaN      0.0

Interestly, if you look closely, you can also see the ""bullseye"" (blue) line is actually present since the beginning of the graph as well.
So, what's going on? Is matplotlib or pandas or something in there plotting NaN as ""zero"" instead of ""not in this graph?
And dropna is not the answer here: it drops entires rows or columns, I would need to drop  cell which makes no sense here.
Note that my previous iteration of this graph, using bars, doesn't have that issue:

Simply replace area with bar in the above to reproduce. The problem with the bar graph is it doesn't respect the scale of the X axis (time).
","You should set the line width to zero:
ax = plt.subplot()
df.plot(ax=ax, kind='area', lw=0, stacked=True)

Output:

with a line plot
The same issue also happens with a line plot, in which case the above would evidently not be a solution.
In this case, on can compute the stacked data with cumsum:
df.cumsum(axis=1).plot()

Output:

",matplotlib
resize a figure automatically in matplotlib,"Is there a way to automatically resize a figure to properly fit contained plots in a matplotlib/pylab image?  
I'm creating heatmap (sub)plots that differ in aspect ratio according to the data used.
I realise I could calculate the aspect ratio and manually set it, but surely there's an easier way? 
","Use bbox_inches='tight'
import numpy as np
import matplotlib.pyplot as plt
import matplotlib.cm as cm

X = 10*np.random.rand(5,3)

fig = plt.figure(figsize=(15,5),facecolor='w') 
ax = fig.add_subplot(111)
ax.imshow(X, cmap=cm.jet)

plt.savefig(""image.png"",bbox_inches='tight',dpi=100)

...only works when saving images though, not showing them.
",matplotlib
heatmap on top of image,"I have Images of different objects (Pascal Voc) and I have a probability heatmap. I want to visualize it by plotting the image and somehow the heatmap on top of it. What would be the best way to do that?
I was thinking about using the alpha channel like this:
im_heat = np.zeros((image.shape[0],image.shape[1],4))
im_heat[:,:,:3] = image
im_heat[:,:,3] = np.rint(255/heatmap)
plt.imshow(im_heat, cmap='jet')
plt.colorbar()

How do I customize the colorbar to be from min(heatmap) to max(heatmap)?
Or is there any better way to visualize the probabilities?
","You can stack images and plots with matplotlib and then choose which handle to use for the colorbar. Using contourf the colorbar min and max values will be based on your heatmap (or you can pass vmin=min(heatmap) and vmax=max(heatmap) to contourf to be explicit about this range). The problem with this is the heatmap will cover you image (and setting transparency will make the entire thing transparent). The best option is to make a colormap which is transparent when near zero, as follows,
import numpy as np
import matplotlib.pyplot as plt
import matplotlib.colors as mcolors
import Image

#2D Gaussian function
def twoD_Gaussian((x, y), xo, yo, sigma_x, sigma_y):
    a = 1./(2*sigma_x**2) + 1./(2*sigma_y**2)
    c = 1./(2*sigma_x**2) + 1./(2*sigma_y**2)
    g = np.exp( - (a*((x-xo)**2) + c*((y-yo)**2)))
    return g.ravel()


def transparent_cmap(cmap, N=255):
    ""Copy colormap and set alpha values""

    mycmap = cmap
    mycmap._init()
    mycmap._lut[:,-1] = np.linspace(0, 0.8, N+4)
    return mycmap


#Use base cmap to create transparent
mycmap = transparent_cmap(plt.cm.Reds)


# Import image and get x and y extents
I = Image.open('./deerback.jpg')
p = np.asarray(I).astype('float')
w, h = I.size
y, x = np.mgrid[0:h, 0:w]

#Plot image and overlay colormap
fig, ax = plt.subplots(1, 1)
ax.imshow(I)
Gauss = twoD_Gaussian((x, y), .5*x.max(), .4*y.max(), .1*x.max(), .1*y.max())
cb = ax.contourf(x, y, Gauss.reshape(x.shape[0], y.shape[1]), 15, cmap=mycmap)
plt.colorbar(cb)
plt.show()

which gives,

",matplotlib
matplotlibpatchesrectangle produces rectangles with unequal size of linewidth,"I am using matplotlib to plot the columns of a matrix as separate rectangles using matplotlib.patches.Rectangle. Somehow, all the ""inner"" lines are wider than the ""outer"" lines? Does somebody know what's going on here? Is this related to this Github issue?
Here's an MRE:
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import matplotlib.patches as patches

# set seed
np.random.seed(42)

# define number of cols and rows
num_rows = 5
num_cols = 5

# define gap size between matrix columns
column_gap = 0.3

# define linewidth
linewidth = 5

# Determine the width and height of each square cell
cell_size = 1  # Set the side length for each square cell

# Initialize the matrix
matrix = np.random.rand(num_rows, num_cols)

# Create the plot
fig, ax = plt.subplots(figsize=(8,6))

# Create a seaborn color palette (RdYlBu) and reverse it
palette = sns.color_palette(""RdYlBu"", as_cmap=True).reversed()

# Plot each cell individually with column gaps
for i in range(num_rows):
    for j in range(num_cols):
        
        # Compute the color for the cell
        color = palette(matrix[i, j])
        
        if column_gap > 0:
            edgecolor = 'black'
        else:
            edgecolor = None
        
        # Add a rectangle patch with gaps only in the x-direction
        rect = patches.Rectangle(
            (j * (cell_size + column_gap), i * cell_size),  # x position with gap applied to columns only
            cell_size,                                      # width of each cell
            cell_size,                                      # height of each cell
            facecolor=color,
            edgecolor=edgecolor,
            linewidth=linewidth
        )
        
        ax.add_patch(rect)

if column_gap > 0:
    
    # Remove the default grid lines and ticks
    ax.spines[:].set_visible(False)

# Set axis limits to fit all cells
ax.set_xlim(0, num_cols * (cell_size + column_gap) - column_gap)
ax.set_ylim(0, num_rows * cell_size)

# Disable x and y ticks
ax.set_xticks([])
ax.set_yticks([])

fig.show()

which produces:

","Your rectangles' edges are getting clipped by the axis boundaries.
Add clip_on=False to Rectangle:
        rect = patches.Rectangle(
            (j * (cell_size + column_gap), i * cell_size),  # x position with gap applied to columns only
            cell_size,                                      # width of each cell
            cell_size,                                      # height of each cell
            facecolor=color,
            edgecolor=edgecolor,
            linewidth=linewidth,
            clip_on=False,
        )

Output (small size for the demo):

To better see what's going on, let's add some transparency to your rectangles and change the axis background color:
ax.patch.set_facecolor('red')


",matplotlib
is there a way to connect the lines of a scatterline plot with the ends of the markers instead of the centers in python,"I use this code to make a scatter+line plot with Matplotlib:
import matplotlib.pyplot as plt
    
zero = [1,2,3,4,5]
eq = [1.2,3,5,7,11]     
plt.plot(zero,'r_--', eq,'k_--',ms = 30, markeredgewidth=5)
plt.xlabel('Steps',fontsize=20, fontname='Helvetica', fontweight='bold')
plt.ylabel('Steps',fontweight='bold',fontsize=20, fontname='Helvetica')
plt.xticks([0,1,2,3,4], fontsize=20, fontname='Helvetica')
plt.yticks(fontsize =18, fontname='Helvetica')
plt.figure(figsize=(8,6))
plt.show()     
plt.close()

However, the lines in the resulting graph connect to the centers of the markers, as in the red example in the image. How can I get a result like the black example instead, where the lines connect to the endpoints of the markers?

","As per @J_H suggestion, you have to cheat a bit and -for each datapoint- introduce two datapoints corresponding to the ends of the markers.
You can use the following list comprehensions to define the new datapoints
import matplotlib.pyplot as plt
    
zero = [1,2,3,4,5]
eq = [1.2,3,5,7,11]

# For each datapoint in your initial dataset, define two points, with same Y of the original point, and X values one slightly smaller and one slightly larger than the actual datapoint (i.e. x - offset and x + offset).
offset = 0.175 # adjust the offset value manually or -better- based on marker size

zeroX = [i
         for x in range(len(zero)) 
         for i in (x-offset, x+offset)]
zeroY = [i
         for y in zero
         for i in (y, y)]
eqX =   [i
         for x in range(len(eq)) 
         for i in (x-offset, x+offset)]
eqY =   [i
         for y in eq 
         for i in (y, y)]

plt.plot(zeroX, zeroY, eqX, eqY) # plot the lines without markers
plt.plot(zero,'r_', eq,'k_',ms = 30, markeredgewidth=5) # plot the markers only 
plt.xlabel('Steps',fontsize=20, fontname='Helvetica', fontweight='bold')
plt.ylabel('Steps',fontweight='bold',fontsize=20, fontname='Helvetica')
plt.xticks([0,1,2,3,4], fontsize=20, fontname='Helvetica')
plt.yticks(fontsize =18, fontname='Helvetica')
plt.show()     
plt.close()

and get the plot below as a result

As commented in the code, it would be better to have the offset automatically adjusted based on the marker size rather than manually.
Hope this helps!
",matplotlib
matplotlib polar contourf plot continuous across theta origin,"I have data in format E(freq, theta), where E is a 2D array and freq and theta are 1D arrays. 
The following portion of code produces the attached figure. However, I would like to make the contourf plot continuous across the 0-degree origin (i.e. no wedge of white space along the 0 azimuth).
I've explored the matplotlib documentation, and posted questions very extensively and can't seem to find a solution for this issue. Any ideas?
Code: 
[r, th] = np.meshgrid(freq,theta)

fig = plt.figure()
ax = fig.add_subplot(111, polar=True)
ax.set_theta_zero_location('N')
ax.set_theta_direction(-1)

cntf = ax.contourf(th,r,np.log10(E),cmap='jet',extend='both',
    levels=np.linspace(np.mean(np.log10(E)), np.amax(np.log10(E)), 15))

ax.set_rlim(0, .3)
label_position=ax.get_rlabel_position()
ax.text(np.radians(label_position+25),ax.get_rmax()/1.5,'f (Hz)',
        rotation=label_position,ha='center',va='center')

Produced plot:

","Something similar to this:
https://stackoverflow.com/a/22129714/9324652

dtheta = np.diff(theta).mean()
wrp_theta = np.concatenate((theta, theta[-1:] + dtheta))
wrp_E = np.concatenate((E, E[0:1, :]), axis=0)

",matplotlib
customizing legend in seaborn histplot subplots,"I am trying to generate a figure with 4 subplots, each of which is a Seaborn histplot. The figure definition lines are:
fig,axes=plt.subplots(2,2,figsize=(6.3,7),sharex=True,sharey=True)
(ax1,ax2),(ax3,ax4)=axes
fig.subplots_adjust(wspace=0.1,hspace=0.2)

I would like to define strings for legend entries in each of the subplots. As an example, I am using the following code for the first subplot:
sp1=sns.histplot(df_dn,x=""ktau"",hue=""statind"",element=""step"", stat=""density"",common_norm=True,fill=False,palette=colvec,ax=ax1)
ax1.set_title(r'$d_n$')
ax1.set_xlabel(r'max($F_{a,max}$)')
ax1.set_ylabel(r'$\tau_{ken}$')
legend_labels,_=ax1.get_legend_handles_labels()
ax1.legend(legend_labels,['dep-','ind-','ind+','dep+'],title='Stat.ind.')

The legend is not showing correctly (legend entries are not plotted and the legend title is the name of the hue variable (""statind""). Please note I have successfully used the same code for other figures in which I used Seaborn relplots instead of histplots.
","The main problem is that ax1.get_legend_handles_labels() returns empty lists (note that the first return value are the handles, the second would be the labels).
Basically, there are two ways to change the legend labels. One is to reuse the ""handles"" of the existing legend. Another approach is to rename the values.
Creating a new legend using the old legend handles
To get the handles, you can do
legend = ax1.get_legend()
handles = legend.legend_handles

(In older matplotlib versions, the name was legend.legendHandles)
A new legend can be created starting from some handles of the existing legend. Creating a new legend automatically removes an existing one.
Also note that to be sure of the order of the labels, it helps to set hue_order. Here is some example code to show the ideas:
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import seaborn as sns

df_dn = pd.DataFrame({'ktau': np.random.randn(4000).cumsum(),
                      'statind': np.repeat([*'abcd'], 1000)})

fig, ax1 = plt.subplots()
sns.histplot(df_dn, x=""ktau"", hue=""statind"", hue_order=['a', 'b', 'c', 'd'],
             element=""step"", stat=""density"", common_norm=True, fill=False, ax=ax1)
ax1.set_title(r'$d_n$')
ax1.set_xlabel(r'max($F_{a,max}$)')
ax1.set_ylabel(r'$\tau_{ken}$')
legend = ax1.get_legend()
handles = legend.legend_handles
ax1.legend(handles, ['dep-', 'ind-', 'ind+', 'dep+'], title='Stat.ind.')
plt.show()


Temporarily renaming the values in the dataframe
Renaming the column values, will put these in the legend. The title can either be changed by changing the column name, or by using sns.movelegend(..., title=...). Its name may be a bit misleading, as sns.movelegend() not only serves to move the legend's location, but also other properties (font size, number of columns, ...).
fig, ax1 = plt.subplots()
df_renamed = df_dn.replace({'statind': {'a': 'dep-', 'b': 'ind-', 'c': 'ind+', 'd': 'dep+'}})

sns.histplot(df_renamed, x=""ktau"", hue=""statind"",
             element=""step"", stat=""density"", common_norm=True, fill=False, ax=ax1)
ax1.set_title(r'$d_n$')
ax1.set_xlabel(r'max($F_{a,max}$)')
ax1.set_ylabel(r'$\tau_{ken}$')
sns.move_legend(ax1, loc='best', title='Stat.ind.')
plt.show()

",matplotlib
speedup matplotlib creation of frame images for animation,"I am simulating some physical processes with Python and wish to regularly plot the situation of the system in order to make an animation. What I have been doing so far is to draw a snapshot of the system at regular intervals, save it to file, and finally convert the images into a video using convert or ffmpeg. Lately, however, the plot has been growing in complexity and it takes now 2-3 seconds to generate just one image, which is too long for my needs.
In my system, I have some background elements that are fixed and never change, and some other elements that are subject to movements depending on time. As a minimum working example, consider the following code:
import matplotlib.pyplot as plt
import numpy as np
import time


class MWE():
    def __init__(self):
        self.x = 0.0
        self.y = 0.0
        self.dt = 0.01
        self.imageCounter = 0
        self.elapsed_time = 0.0

    def evolve(self):
        theta = 2 * np.pi * self.elapsed_time
        self.x = np.cos(theta)
        self.y = np.sin(theta)
        self.elapsed_time += self.dt

    def draw(self):
        fig, ax = plt.subplots(figsize=(2, 2), layout='constrained')
        ax.set_aspect('equal')
        ax.axis('off')

        ax.set_xlim((-2, 2))
        ax.set_ylim((-2.2, 2))

        for center_x in np.linspace(-2,2, 21)[1::2]:
            for center_y in np.linspace(-2,2, 21)[1::2]:
                blue_circle = plt.Circle((center_x, center_y), 0.1, color='blue', fill=False)
                ax.add_patch(blue_circle)

        red_circle = plt.Circle((self.x, self.y), 0.1, color='red', fill=True)
        ax.add_patch(red_circle)
        self.imageCounter += 1

        ax.text(-1.9, -2.2, f'Time: {self.elapsed_time:.2f} s', color='black', size=8)

        fig.savefig(f'MWE_{str(self.imageCounter).zfill(3)}.png')
        plt.clf()
        plt.cla()
        plt.close()


mwe = MWE()

start_time = time.time()
while mwe.elapsed_time < 1.0:
    mwe.evolve()
    mwe.draw()
print(f""Execution: {time.time() - start_time:.2f} s"" % ())

convert -resize 100% -delay 2 -loop 0 MWE_*.png loop.gif


The execution on my computer takes Execution: 51.32 s, therefore I am looking for alternative way to handle the plots, be it keeping in memory the background drawing, or using an additional package for the animation.
","Without any fancy blitting (which probably speeds up things even more dramatically), these few simple changes bring execution time down to Execution: 4.10 s on my side:

initialize figure only once
draw artists only once and update positions
switch to agg backend to avoid unnecessary guis


import matplotlib.pyplot as plt
import numpy as np
import time

plt.switch_backend(""agg"")

class MWE():
    def __init__(self):
        self.x = 0.0
        self.y = 0.0
        self.dt = 0.01
        self.imageCounter = 0
        self.elapsed_time = 0.0
        
        self.init_figure()

    def init_figure(self):
        self.fig, ax = plt.subplots(figsize=(2, 2), layout='constrained')
        ax.set_aspect('equal')
        ax.axis('off')

        ax.set_xlim((-2, 2))
        ax.set_ylim((-2.2, 2))

        for center_x in np.linspace(-2,2, 21)[1::2]:
            for center_y in np.linspace(-2,2, 21)[1::2]:
                blue_circle = plt.Circle((center_x, center_y), 0.1, color='blue', fill=False)
                ax.add_patch(blue_circle)

        self.red_circle = plt.Circle((self.x, self.y), 0.1, color='red', fill=True)
        ax.add_patch(self.red_circle)
        self.info_text = ax.text(-1.9, -2.2, f'Time: {self.elapsed_time:.2f} s', color='black', size=8)

    def evolve(self):
        theta = 2 * np.pi * self.elapsed_time
        self.x = np.cos(theta)
        self.y = np.sin(theta)
        self.elapsed_time += self.dt

    def draw(self):
        self.red_circle.set_center((self.x, self.y))
        self.info_text.set_text(f'Time: {self.elapsed_time:.2f} s')
        self.imageCounter += 1

        self.fig.savefig(f'MWE_{str(self.imageCounter).zfill(3)}.png')


mwe = MWE()
mwe.init_figure()

# %%
start_time = time.time()
while mwe.elapsed_time < 1.0:
    mwe.evolve()
    mwe.draw()
print(f""Execution: {time.time() - start_time:.2f} s"" % ())


",matplotlib
how to create single column 3x1 plot in a 3x2 subplot,"I want to create a plot with 2 columns, with the 1st one has only 1 plot and the 2nd one has 3. I try to set it up by using the following code
import matplotlib.pyplot as plt

def format_axes(fig):
    for i, ax in enumerate(fig.axes):
        ax.text(0.5, 0.5, ""ax%d"" % (i+1), va=""center"", ha=""center"")
        ax.tick_params(labelbottom=False, labelleft=False)

fig = plt.figure()
ax1 = plt.subplot(222)
ax2 = plt.subplot(224)
ax3 = plt.subplot(121)
axes = [ax1, ax2, ax3]
format_axes(fig)
plt.tight_layout()
plt.show()

But then I struggle to make 4th plot because subplot doesn't support index > 5, and I don't know how to make each plot figure size satisfies the given proportion.
I tried again using gridspec by using the code below
import matplotlib.pyplot as plt
from matplotlib.gridspec import GridSpec

def format_axes(fig):
    for i, ax in enumerate(fig.axes):
        ax.text(0.5, 0.5, ""ax%d"" % (i+1), va=""center"", ha=""center"")
        ax.tick_params(labelbottom=False, labelleft=False)

fig = plt.figure(layout=""constrained"")

gs = GridSpec(3, 2, figure=fig)
ax1 = fig.add_subplot(gs[: , 0])
ax2 = fig.add_subplot(gs[0, -1])
ax3 = fig.add_subplot(gs[1, 1])
ax4 = fig.add_subplot(gs[-1, 1])
format_axes(fig)

plt.show()

However, my problem arises again because I couldn't manipulate each plot's size.
Is there a universal way to make a column size plot for bigger nrows x ncols subplots? And could someone explain gridspec for me, I copied the code from matplotlib.org and changed some values.
","Not sure which layout exactly you are trying to achieve, but if you're new to this I think that plt.subplot_mosaic might be easier, tweaking axes' sizes with height_ratios and widths_ratios.
import matplotlib.pyplot as plt


fig, axd = plt.subplot_mosaic(
    """"""
    AB
    AC
    AD
    """""",
    constrained_layout=True,
    height_ratios=[1, 3, 2],
    width_ratios=[1, 2],
)

# axd is a dict of Axes, e.g. use axd[""A""]

def label_axes(axd):
    for ax_name, ax in axd.items():
        ax.text(0.5, 0.5, ax_name, va=""center"", ha=""center"")
        ax.tick_params(labelbottom=False, labelleft=False)

label_axes(axd)

fig.show()


",matplotlib
problems with updating colorbar with matplotlibslider listener in jupyter,"The problem that I am having is during updating the colorbar of an plt.imshow plot. Here's the code I will try to break it down and to explain some stuff in it afterwards.

For the first cell in Jupyter, I have the functions, the imports and the input parameters:

# Imports ##############################################################
import numpy as np
%matplotlib notebook
import matplotlib.pyplot as plt
import matplotlib.colors as colors
from matplotlib.widgets import Slider
from scipy.ndimage import label, find_objects
# Functions ############################################################
def intensity_distribution(r, z, PMax, w0, zR):
    wZ = w0 * np.sqrt(1 + (z / zR)**2)  # beam radius at z (Gaussian beam spreading)
    I0 = 2 * PMax / (np.pi * wZ**2) # peak intensity at radius wZ
    return I0 * np.exp(-2 * r**2 / wZ**2), wZ  # Gaussian intensity distribution, beam radius
def get_circle_ROI(I, r, threshold):
    ROI = (I > threshold).astype(""uint8"")  # binary mask for regions above the threshold
    labels, features = label(ROI) # label connected regions
    slices = find_objects(labels) # get bounding box slices for labeled regions
    xSlice, ySlice = slices[0] # extract x and y slices of the largest feature
    ROIHeight = (xSlice.stop - xSlice.start) * (r[1] - r[0]) * 1e6 # convert height to micrometers
    ROIWidth = (ySlice.stop - ySlice.start) * (r[1] - r[0]) * 1e6 # convert width to micrometers
    cx = (ySlice.start + ySlice.stop) // 2 # x-coordinate of the center
    cy = (xSlice.start + xSlice.stop) // 2 # y-coordinate of the center
    centre = (r[cy] * 1e6, r[cx] * 1e6) # convert center coordinates to micrometers
    radius = min(ROIWidth, ROIHeight) / 2 # radius is the smaller dimension's half-width
    return centre, radius
def update_plot(PMax, zOffset):
    """"""Update the heatmap based on new parameters.""""""
    global colorbar
    # Calculate intensity distribution at given z offset
    I, wZ = intensity_distribution(np.sqrt(R**2 + Z**2), zOffset, PMax, BEAM_RADIUS_AT_FOCUS, zR)
    I /= 1e6  # convert intensity from W/m² to W/mm²
    I += 0.01  # small offset for better visualization contrast
    max_intensity = I.max()  # maximum intensity in the current distribution
    # Calculate the on-axis peak intensity at focus in W/mm²
    I0 = (2 * PMax) / (np.pi * BEAM_RADIUS_AT_FOCUS**2)  # peak intensity in W/m² at z = 0
    I0 /= 1e6  # convert peak intensity to W/mm²
    # Calculate the Full Width at Half Maximum (FWHM) in micrometers
    centre, fwhm = get_circle_ROI(I, r, max_intensity / 2)
    _, tenth = get_circle_ROI(I, r, max_intensity / 10)
    # Clear and update plot
    ax.clear()  # clear current axes
    # Display the updated intensity distribution as a heatmap
    im = ax.imshow(I, extent=[r[0]*1e6, r[-1]*1e6, r[0]*1e6, r[-1]*1e6], norm=colors.LogNorm(vmax=14000))
    ax.set_xlabel(""x (μm)"")  # label for x-axis
    ax.set_ylabel(""y (μm)"")  # label for y-axis
    # Add plot title with z offset, FWHM, and max intensity in W/mm²
    ax.set_title(f""FWHM = {fwhm:.1f} μm\n""
                 f""Radius at 10% of total power = {tenth:.2f} μm\n""
                 f""Max power = {I.max():.2f} W/mm²"",
                 loc=""left"")
    # Draw a circle representing the FWHM boundary
    cirlcefwhm = plt.Circle(centre, fwhm, color='white', fill=False, linestyle='--', linewidth=2, label=""FWHM"")
    cirlce10 = plt.Circle(centre, tenth, color='white', fill=False, linestyle='--', linewidth=2, label=""10% of I$_max$"")
    ax.add_patch(cirlcefwhm)  # add the FWHM circle to the plot
    ax.add_patch(cirlce10)  # add the circle where power is 10% of max
    #### Problematic starts here ####
    if colorbar is not None:   # if colorbar already exists, remove it
        colorbar.remove()
    colorbar = plt.colorbar(im, ax=ax, label=""Intensity (W/mm²)"")  # create new colorbar in W/mm²
    fig.draw_without_rendering() # redraw based on the recommendation of matplotlib instead of colorbar.draw_all()
    fig.canvas.draw()  # redraw figure to reflect updates
    #### Problematic ends here ####
def sliders_on_changed(val):
    ''' Slider update function '''
    power = power_slider.val * MAX_LASER_POWER / 100  # calculate current power level in watts
    z_offset = z_offset_slider.val / 1000 # convert slider z offset from mm to meters
    update_plot(power, z_offset) # update the plot with new parameters
# Inputs ###############################################################
WAVELENGTH = 10.6e-6  # wavelength in meters
MAX_LASER_POWER = 80  # max laser power in watts
BEAM_WIDTH_AT_FOCUS = 120e-6  # beam width at focus in meters
BEAM_RADIUS_AT_FOCUS = BEAM_WIDTH_AT_FOCUS / 2  # beam radius at focus in meters
zR = np.pi * BEAM_RADIUS_AT_FOCUS**2 / WAVELENGTH  # Rayleigh range in meters
gridSize = 100 # resolution
r = np.linspace(-500e-6, 500e-6, gridSize)  # range for spatial coordinates in meters
R, Z = np.meshgrid(r, r) # create grid for spatial coordinates
colorbar = None # init colorbar

The problematic part would be this one:
#### Problematic starts here ####
if colorbar is not None:   # if colorbar already exists, remove it
    colorbar.remove()
colorbar = plt.colorbar(im, ax=ax, label=""Intensity (W/mm²)"")  # create new colorbar in W/mm²
fig.draw_without_rendering() # redraw based on the recommendation of matplotlib instead of colorbar.draw_all()
fig.canvas.draw()  # redraw figure to reflect updates
#### Problematic ends here ####

Am not so sure why it is not working. What happens is that the colorbar appears for the first update and then dissapears after a change in the Z-offset or the power percentage. Basically after a change in any of the values.

The second cell of the jupyter notebook has the call for the functions:

fig, ax = plt.subplots()
plt.subplots_adjust(left=0.25, bottom=0.35)  # leave space for sliders
update_plot(50, 0)
ax_power = plt.axes([0.25, 0.2, 0.65, 0.03], facecolor = ""lightgray"")
ax_z_offset = plt.axes([0.25, 0.15, 0.65, 0.03], facecolor = ""lightgray"")
power_slider = Slider(ax_power, 'Power (%)', 0.1, 100, valinit=50)
z_offset_slider = Slider(ax_z_offset, 'Z-Offset (mm)', 0, 5.0, valinit=0)
power_slider.on_changed(sliders_on_changed)
z_offset_slider.on_changed(sliders_on_changed)

Running this will get you to a UI that looks like this:

And then when I move any of the sliders the colorbar disappears:

The updating of the colorbar seems to be a common problem, I saw a lot of stackoverflow questions about it, I even answered one once.
The solution however is not really working with the slider listeners.
P.S.: Changing the backend does not really help. However, using %matplotlib qt revealed a traceback:
Traceback (most recent call last):
  File ""C:\ProgramData\anaconda3\Lib\site-packages\matplotlib\cbook\__init__.py"", line 309, in process
    func(*args, **kwargs)
  File ""C:\ProgramData\anaconda3\Lib\site-packages\matplotlib\widgets.py"", line 603, in <lambda>
    return self._observers.connect('changed', lambda val: func(val))
                                                          ^^^^^^^^^
  File ""C:\Users\User\AppData\Local\Temp\ipykernel_26252\2170461726.py"", line 66, in sliders_on_changed
    update_plot(power, z_offset) # update the plot with new parameters
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""C:\Users\User\AppData\Local\Temp\ipykernel_26252\2170461726.py"", line 57, in update_plot
    colorbar.remove()
  File ""C:\ProgramData\anaconda3\Lib\site-packages\matplotlib\colorbar.py"", line 1041, in remove
    self.ax.remove()
  File ""C:\ProgramData\anaconda3\Lib\site-packages\matplotlib\artist.py"", line 242, in remove
    self._remove_method(self)
  File ""C:\ProgramData\anaconda3\Lib\site-packages\matplotlib\figure.py"", line 944, in delaxes
    self._axstack.remove(ax)
  File ""C:\ProgramData\anaconda3\Lib\site-packages\matplotlib\figure.py"", line 92, in remove
    self._axes.pop(a)
KeyError: <Axes: label='<colorbar>', ylabel='Intensity (W/mm²)'>

Some stuff that I tried:

There is a method for colorbar called draw_all()
Matplotlib also suggests using fig.draw_without_rendering()
My own solution
Setting the limits of colorbar without deleting the colorbar

Can someone please help me in figuring out what's wrong? I understand ofc that the code is quite long, I can shorten it to a simple example if necessary.

I am using

Jupyter 6.5.4
Matplotlib 3.7.2

","I managed to solve this problem on my old version by using the following lines to update the colormap:
if colorbar is not None: # if colorbar already exists, update it
    colorbar.mappable.set_clim(vmax=max_intensity)
else:    
    colorbar = plt.colorbar(im, ax=ax, label=""Intensity (W/mm²)"") # create new colorbar in W/mm²

Updating Anaconda was not needed in this case. The code above works both for the old version and the old version (under the %matplotlib qt backend)
Proof:


The advice about using ipympl lead to a RuntimeError under a clean new anaconda installation. So, really no need for that.

Using interact() does not solve anything.
",matplotlib
plotting a histogram with timestamp,"I have a list of pandas timestamps in the following format:
[Timestamp('2022-01-01 21:00:00'), 
 Timestamp('2022-01-02 21:15:00'), 
 Timestamp('2022-01-03 21:00:00'), 
 Timestamp('2022-01-04 20:00:00'), 
 Timestamp('2022-01-05 21:00:00'),
 ....
 ]

Now what I want to do is basically look at the time of the day and plot a histogram which denotes how much percentage of these stamps are at 21:00, 21:15 etc. I know I can extract the hour and minute from the timestamps but having trouble figuring out how to plot the histogram where the labels would be the hours and minute and the bars would represent the percentage.
So, my attempt is as follows:
labels, counts = np.unique(histogram, return_counts=True)

all_sum = sum(counts)
percentages = [i * 100 / all_sum for i in counts]

bars = plt.bar(labels, counts, align=""center"", width=13, color=""blue"", edgecolor=""black"")

for i, p in enumerate(bars):
    width = p.get_width()
    height = p.get_height()
    x, y = p.get_xy()
    print(x, y)
    plt.text(x + width / 2, y + height * 1.01, ""{0:.2f}"".format(percentages[i]) + ""%"", ha=""center"", weight=""bold"")

plt.gca().set_xticks(labels)
plt.grid(False)
plt.tight_layout()

plt.show()

However, this results in a plot as follows:

Not sure why these bars are overlapping...
","Generate appropriately formatted strings from the datetimes, group by the strings, count, and leverage the pandas plotting methods:
import pandas as pd
import matplotlib.ticker as mtick
import matplotlib.pyplot as plt


datetimes = [('2022-01-01 21:00:00'), 
             ('2022-01-02 21:15:00'), 
             ('2022-01-03 21:00:00'), 
             ('2022-01-04 20:00:00'), 
             ('2022-01-05 21:00:00'),]
df = pd.DataFrame({""datetime"": pd.to_datetime(datetimes)})

fig, ax = plt.subplots(1, 1)
((df.groupby(df.datetime.dt.strftime(""%H:%M"")).size() / len(df) * 100)
   .plot.bar(ax=ax))

ax.yaxis.set_major_formatter(mtick.PercentFormatter())
ax.grid(False)


",matplotlib
plotting a list of lattitude longitude valuetuples in python,"I have a list of (latitude, logitude, value)-Tuples I want to show using Basmap. What's the easiest way to do that?
data = [(1, 2, 0.12323),
        (2, 5, 0.23232),
        (4, 52,0.23131)
        .
        . tenthousand times this.
        .
       ]

","I haven't used basemap, but it looks like what you want is the scatter() method:
https://matplotlib.org/basemap/stable/users/examples.html
Which takes the same parameters as the matplotlib.pyplot.scatter method:
https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.scatter.html
",matplotlib
capturing matplotlib coordinates with mouse clicks using ipywidgets in jupyter notebook,"Short question
I want to capture coordinates by clicking different locations with a mouse on a Matplotlib figure inside a Jupyter Notebook. I want to use ipywidgets without using any Matplotlib magic command (like %matplotlib ipympl) to switch the backend and without using extra packages apart from Matplotlib, ipywidgets and Numpy.
Detailed explanation
I know how to achieve this using the ipympl package and the corresponding Jupyter magic command %matplotlib ipympl to switch the backend from inline to ipympl (see HERE).
After installing ipympl, e.g. with conda install ipympl, and switching to the ipympl backend, one can follow this procedure to capture mouse click coordinates in Matplotlib.
import matplotlib.pyplot as plt

# Function to store mouse-click coordinates
def onclick(event):
    x, y = event.xdata, event.ydata
    plt.plot(x, y, 'ro')
    xy.append((x, y))

# %%
# Start Matplotlib interactive mode
%matplotlib ipympl  

plt.plot([0, 1])
xy = []     # Initializes coordinates
plt.connect('button_press_event', onclick)


However, I find this switching back and forth between inline and ipympl backend quite confusing in a Notebook.
An alternative for interactive Matplotlib plotting in Jupyter Notebook is to use the ipywidgets package. For example, with the interact command one can easily create sliders for Matplotlib plots, without the need to switcxh backend. (see HERE).
from ipywidgets import interact
import numpy as np
import matplotlib.pyplot as plt

x = np.linspace(0, 2 * np.pi)

def update(w=1.0):
    plt.plot(np.sin(w * x))
    plt.show()

interact(update);


However, I have not found a way to use the ipywidgets package to capture (x,y) coordinates from mouse clicks, equivalent to my above example using ipympl.
","Short answer
Capturing mouse clicks on a non-interactive Matplotlib figure is not possible – that's what the interactive backends are for. If you want to avoid switching back and forth between non-interactive and interactive backends, maybe try the reverse approach: Rather than trying to get interactivity from non-interactive plots, use an interactive backend by default, and disable interactivity where it is not necessary.
Detailed answer
What Matplotlib says
Regarding interactivity, Matplotlib's documentation explicitly states (emphasis by me):

To get interactive figures in the 'classic' notebook or Jupyter lab, use the ipympl backend (must be installed separately) which uses the ipywidget framework.

And further down:

The default backend in notebooks, the inline backend, is not [interactive]. backend_inline renders the figure once and inserts a static image into the notebook when the cell is executed. Because the images are static, they cannot be panned / zoomed, take user input, or be updated from other cells.

I guess that should make the situation pretty clear.
Interactivity with ipywidgets
As you noted, you can interact with (static) Matplotlib figures using ipywidgets. What happens there, however, is the following: The widgets (e.g. the slider that you show) are interactive, while the figure is still not. So ""interactivity"" in this context means interacting with a widget that then triggers the re-rendering of a static image. This use case and setup is fundamentally different from trying to interactively capture inputs from a static image.
Proposed approach
What I would suggest is:

Install ipympl, as it is meant to be used for your purpose.
If you want to avoid switching back and forth between backends, set your interactive backend once for your notebook, and disable interactive features in plots where you don't need them. Following Matplotlib's ""comprehensive ipympl example"", the display() function can be used for this purpose.

Altogether, this could look as follows in code:
%matplotlib widget
# Alternatively: %matplotlib ipympl
import matplotlib.pyplot as plt
import numpy as np

# Provide some dummy data
x = np.linspace(-10, 10, num=10000)
y1 = x ** 2
y2 = x ** 3

# Plot `y1` in an interactive plot
def on_click(event):
    plt.plot(event.xdata, event.ydata, ""ro"")

plt.connect(""button_press_event"", on_click)
plt.plot(x, y1)

# Plot `y2` in a 'static' plot
with plt.ioff():
    plt.figure()  # Create new figure for 2nd plot
    plt.plot(x, y2)
    display(plt.gcf())  # Display without interactivity

The resulting notebook would look as follows:

Semi-off-topic: ""interactive mode""
You might have noticed that display() is used in connection with ioff() for the static figure here. And although ioff() is documented as the function to, quote, disable interactive mode, it is not the one that is responsible for disabling click capturing etc. here. In this context, ""interactive"" refers to yet another concept, which is explained with the isinteractive() function; namely,

… whether plots are updated after every plotting command. The interactive mode is mainly useful if you build plots from the command line and want to see the effect of each command while you are building the figure.

In the given example, we don't want plotting commands to have immediate effects on the output, because this would mean that already the figure() and plot() calls would render the figure (with all its interaction capabilities in our original sense!), rather than only rendering it (as a static image) when we call display(). Moreover, we would get two outputs of our figure: one (interactive) plot because of the figure() and plot() calls, one (static) plot because of the display() call. To suppress the first one, we use an ioff() context.
",matplotlib
gaps in a matplotlib plot of categorical data,"When I have numerical data, say index by some kind of time, it is straightforward to plot gaps in the data. For instance, if I have values at times 1, 2, 3, 5, 6, 7, I can set an np.nan at time 4 to break up the plot.
import numpy as np
import matplotlib.pyplot as plt
x = [1, 2, 3, 4, 5, 6, 7]
y = [10, 20, 30, np.nan, 10, 20, 30]
plt.plot(x, y)
plt.show()
plt.close()


That sure beats the alternative of just skipping time 4!
import numpy as np
import matplotlib.pyplot as plt
x = [1, 2, 3, 5, 6, 7]
y = [10, 20, 30, 10, 20, 30]
plt.plot(x, y)
plt.show()
plt.close()


However, I now have a y variable that is categorical. Mostly, the plotting is straightforward: just use the categories as the y.
import numpy as np
import matplotlib.pyplot as plt
x = [1, 2, 3, 5, 6, 7]
y = [""cat"", ""cat"", ""dog"", ""dog"", ""cat"", ""cat""]
plt.plot(x, y)
plt.show()
plt.close()


This puts the categories on the y-axis, just as I want. However, when I do my np.nan trick to get the gap, I get a point plotted at np.nan.
import numpy as np
import matplotlib.pyplot as plt
x = [1, 2, 3, 4, 5, 6, 7]
y = [""cat"", ""cat"", ""dog"", np.nan, ""dog"", ""cat"", ""cat""]
plt.plot(x, y)
plt.show()
plt.close()


How can I get my plots to go cat cat dog on 1, 2, 3, and then dog cat cat on 5, 6, 7, leaving a gap at 4?
","To create a gap in a categorical plot handle np.nan differently asmatplotlib doesn’t natively interpret np.nan in categorical contexts (Gets converted to 'nan' string.).
Solution:
Splitting the data and then plot each segment separately.
x = [1, 2, 3, 5, 6, 7]
y = [""cat"", ""cat"", ""dog"", ""dog"", ""cat"", ""cat""]

# Plot each segment separately to introduce a gap
plt.plot(x[:3], y[:3]) 
plt.plot(x[3:], y[3:]) 


Alternative using Masks:
x = [1, 2, 3, 4, 5, 6, 7]
y = [""cat"", ""cat"", ""dog"", ""gap"", ""dog"", ""cat"", ""cat""]

# Convert categorical data to numbers using a mapping
categories = [""cat"", ""dog""]
y_numeric = [categories.index(val) if val in categories else np.nan for val in y]

y_masked = np.ma.masked_where(np.isnan(y_numeric), y_numeric)

plt.plot(x, y_masked)
plt.yticks(range(len(categories)), categories)


This allows gaps without splitting anything.
",matplotlib
increasing the space for x axis labels in matplotlib,"I'm plotting, but find that I need to increase the area underneath chart such that I can plot the labels vertically but in a font size that is not so tiny. At the moment, I have:
plt.figure(count_fig) fig, ax = plt.subplots() 
rects1 = ax.bar(ind, ratio_lst, width, color='r', linewidth=1, alpha=0.8, log=1) 
ax.set_ylabel('') 
ax.set_title('') 
ax.set_xticks(ind_width) 
ax.set_xticklabels(labels_lst, rotation='vertical', fontsize=6)

At the moment it works, but the labels often run-off the edge of the plot.
","subplots_adjust will do it. You can play with the bottom keyword to get a good placement of the bottom of the plot.
fig.subplots_adjust(bottom=0.2)

",matplotlib
pandas bar chart with paired columns,"I have a DataFrame with paired columns. I want to plot it such that each pair of columns has a unique color, and one column of each pair has an empty fill.
I tried this:
import pandas as pd

df = pd.DataFrame({
    ('A', '1'): [1, 2, 3],
    ('A', '2'): [4, 5, 6],
    ('B', '1'): [7, 8, 9],
    ('B', '2'): [10, 11, 12]
})

df.plot.bar(color=['C0', 'none', 'C1', 'none'], edgecolor=['C0', 'C0', 'C1', 'C1'])

This almost works! But it applies the edgecolors row-wise instead of column-wise.

I asked ChatGPT to save my butt. It gave me a solution that works (see lightly modified version below), but it's very wordy. My question is, is there a simpler way to do this, ideally using DataFrame.plot?
import pandas as pd
import matplotlib.pyplot as plt

# Sample DataFrame
df = pd.DataFrame({
    ('A', '1'): [1, 2, 3],
    ('A', '2'): [4, 5, 6],
    ('B', '1'): [7, 8, 9],
    ('B', '2'): [10, 11, 12]
})

# Define colors for each pair
colors = ['C0', 'C1']

# Create a bar chart
fig, ax = plt.subplots()

# Number of columns
num_cols = len(df.columns)

# Bar width
bar_width = 0.2

# Plot each pair of columns
for i in range(0, num_cols, 2):
    color_i = colors[i//2]
    ax.bar(df.index + i*bar_width, df.iloc[:, i], bar_width, 
           label=str(df.columns[i]), color=color_i, edgecolor=color_i)
    ax.bar(df.index + (i+1)*bar_width, df.iloc[:, i+1], bar_width, 
           label=str(df.columns[i+1]), color='none', edgecolor=color_i)

# Add labels, title, and legend
ax.set_xlabel('Index')
ax.set_ylabel('Values')
ax.set_title('Bar chart with paired columns')
ax.set_xticks(df.index + bar_width * (num_cols / 2 - 0.5))
ax.set_xticklabels(df.index)
ax.legend()


","Here's one approach:

Use np.repeat to repeat each color n times, where n = len(df).
Assign via plt.patches.Patch.set_edgecolor.
Use plt.legend afterwards to process the updates there.

import matplotlib.pyplot as plt
import pandas as pd
import numpy as np

df = pd.DataFrame({
    ('A', '1'): [1, 2, 3],
    ('A', '2'): [4, 5, 6],
    ('B', '1'): [7, 8, 9],
    ('B', '2'): [10, 11, 12]
})

fig, ax = plt.subplots()

df.plot.bar(ax=ax, color=['C0', 'none', 'C1', 'none'])

edgecolors = np.repeat(['C0', 'C0', 'C1', 'C1'], len(df))

for edgecolor, patch in zip(edgecolors, ax.patches):
    patch.set_edgecolor(edgecolor)
    
plt.legend()
plt.tight_layout()
plt.show()

Plot:

",matplotlib
superimpose plot with background image chart,"I am trying to use an existing graph as a background for new data that I want to plot on top of the graph.
I have been able to do so when using a graph with all information contained within the axes and using the extent parameter of plt.imshow because then I just have to scale the image.
I would like to scale and shift the background graph. Replotting the background is not an option in the real use case.
Here is what I tried so far :

Generation of a background graph (reproducible example)

import matplotlib.pyplot as plt

fig, ax = plt.subplots()
ax.plot([0, 5, 10], [8, 5, 12])
ax.set_xlim(0, 20)
ax.set_ylim(0, 15)
ax.set_title('Background graph')
fig.show()
fig.savefig('bg_graph.png')



Use plt.imshow() to add the background graph and then superimpose my data.

bg_img = plt.imread('bg_graph.png')
fig, ax = plt.subplots()
ax.imshow(bg_img, extent=[0,50,0,50])
ax.scatter([4.9, 5.2], [7, 4.9])
fig.show()
fig.savefig('result.png')


I have made a mockup of the expected result using Excel :

Is there a method to stretch a new graph onto existing axis (from an image) in order to plot new pieces of data ? I assume that the coordinates of the axis in the image are known or can be guessed through trial an error. One way to rephrase this is to say that I would like to stretch the new plot to the image and not the other way around.
","We can follow this answer to a related question and adapt it to your needs (see code comments for explanations):
import matplotlib.pyplot as plt

bg_img = plt.imread('stackoverflow/bg.png')  # TODO: Adjust as necessary
bg_width, bg_xlim, bg_ylim = 6, (0, 20), (0, 15)

# Create a figure with the same aspect ratio and scale as the image.
# This provides the axes in which we will plot our new data
figsize = (bg_width, bg_width * bg_img.shape[0] / bg_img.shape[1])
fig, axes = plt.subplots(nrows=1, ncols=1, figsize=figsize)
axes.patch.set_alpha(0.0)  # Make new figure's area transparent
axes.set_xlim(*bg_xlim)  # Adjust limits to background's limits
axes.set_ylim(*bg_ylim)
axes.scatter([4.9, 5.2], [7, 4.9], color='red')  # Plot our new data
# Optionally, turn off axes, as we already have them from
# the background and they will not match perfectly:
plt.axis('off')
    
background_ax = plt.axes([0, 0, 1, 1])  # Create dummy subplot for background
background_ax.set_zorder(-1)  # Set background subplot behind the other
background_ax.imshow(bg_img, aspect='auto')  # Show background image
plt.axis('off')  # Turn off axes that surround the background

For me, using the background image that you shared and loading it as bg.png results in the following plot:

What if adjusting the whitespace is necessary?
Luckily, the layout of the whitespace in your background image seems to match Matplotlib's defaults. If that was not the case, however, we could use subplots_adjust() on the foreground plot, together with a bit of trial and error, to make the axes of the foreground plot and background image align as perfectly as possible. In this case, I would initially leave the axes of the foreground plot turned on (and thus comment out the first plt.axis('off') in the code above) to make adjustments easier.
To demonstrate this, I created a version of your background image with additional green padding (called bg_padded.png in the code below), which looks as follows:

I then adjusted the code from above as follows:
import matplotlib.pyplot as plt

bg_img = plt.imread('stackoverflow/bg_padded.png')  # TODO: Adjust as necessary
bg_width, bg_xlim, bg_ylim = 7.5, (0, 20), (0, 15)

# Create a figure with the same aspect ratio and scale as the image.
# This provides the axes in which we will plot our new data
figsize = (bg_width, bg_width * bg_img.shape[0] / bg_img.shape[1])
fig, axes = plt.subplots(nrows=1, ncols=1, figsize=figsize)
# Adjust padding of foreground plot to padding of background image
plt.subplots_adjust(left=.2, right=.82, top=.805, bottom=0.19)
axes.patch.set_alpha(0.0)  # Make new figure's area transparent
axes.set_xlim(*bg_xlim)  # Adjust limits to background's limits
axes.set_ylim(*bg_ylim)
axes.scatter([4.9, 5.2], [7, 4.9], color='red')  # Plot our new data
# Optionally, turn off axes, as we already have them from
# the background and they will not match perfectly:
# plt.axis('off')
    
background_ax = plt.axes([0, 0, 1, 1])  # Create dummy subplot for background
background_ax.set_zorder(-1)  # Set background subplot behind the other
background_ax.imshow(bg_img, aspect='auto')  # Show background image
plt.axis('off')  # Turn off axes that surround the background

Changes are:

I loaded bg_padded.png rather than bg.png (obviously);
I changed bg_width to 7.5 to account for the increased size of the background image and, with it, for the relative decrease in size (e.g. of the fonts) in the foreground plot;
I added the line plt.subplots_adjust(left=.2, right=.82, top=.805, bottom=0.19) to adjust for the padding.

This time, I also left the first plt.axis('off') commented out, as mentioned above, to see and to show how well the axes of the background image and the foreground plot match. The result looks as follows:

",matplotlib
how do i maintain image size when using a colorbar,"I'm trying to plot two versions of the same image side by side. When I plot the figure without the color bar for one of the images, it seems to have the right sizes:

But when I add a color bar to the image in the left, it scales the image down somehow:

Here's the code where I have commented out the lines for color bar:
def plot_amaps(self, anisotropy_map, parallel):
        timepoint = self.t * self.timestep
        amap_directory = self.directory + ""amaps/""
        fig = plt.figure(facecolor='w', dpi=180)

        ax1 = fig.add_subplot(121)
        fig.subplots_adjust(top=0.85)
        ax1.grid(False)
        txt = ""Mean(r) = %.3f SD(r)= %.3f t=%dmin""
        txt = txt %(self.mean, self.sd, timepoint)
        ax1.set_title(txt)

        amap = ax1.imshow(anisotropy_map, cmap=""jet"", clim = self.clim)
        #divider = make_axes_locatable(ax1)
        #cax = divider.append_axes('right', size='5%', pad=0.05)
        #fig.colorbar(amap, cax=cax)

        ax2 = fig.add_subplot(122)
        ax2.set_title(""Intensity image"", fontsize=10)
        ax2.imshow(parallel, cmap=""gray"")
        ax2.grid(False)
        ax1.axis('off')
        ax2.axis('off')

        if self.save is True:
            self.make_plot_dir(amap_directory)
            name = self.cell + ""_time_""+str(timepoint)
            plt.savefig(amap_directory+name+self.saveformat, bbox_inches='tight')
        else:
            plt.show()
        plt.close('all')

What am I doing wrong, and how can I make sure that the two images are of the same size?
","When using 
divider = make_axes_locatable(ax1)
cax = divider.append_axes('right', size='5%', pad=0.05)

you explicitely ask for a 5% smaller axes. So if you don't want that, you should not create the axes for the colorbar using make_axes_locatable.
Instead you can simply create an axes at any point on the figure using
cax = fig.add_axes([left, bottom, width, height])

where left, bottom, width, height are in figure units ranging from 0 to 1. Then add the colorbar to it.
If you want the colorbar in the middle, you could previously make some space using 
plt.subplots_adjust(wspace=0.3)

Of course you would have to experiment a bit with the numbers. 
",matplotlib
matplotlib zoomed in xaxis at beginning and end,"i have a signal with a relativly high frequency and i want to have a detailed look at the start of the recorded signal and the end. Like the signal is 1 hour long and i want the first 10 seconds and the last 10 seconds kind of zoomed in (on the x-axis) and the middle section ""normal"". I allready found this method 'axs.set_yscale('function', functions=(forward, inverse))' which should be able to define a custom scale, but i'm unable to understand how this works and i can not find a lot of documentation on this method.
I can not share the real data, but the data looks very similar to a sinus, so one can use this plot to visualize it:
fig, axs = plt.subplots()
x = np.arange(0, 1000 * np.pi, 0.1)
y = 2 * np.sin(x) + 3
axs.plot(x, y)

","I think this is basically what you want.  This maps 0-10 to 0-10, 10 to 30 pi to 10 to 20, and 30pi-10 to 1000 pi from 20 to 30 (eg 1/3 each).  It includes extra values one either side in case your data extends beyond the limits you have specified.
(Edit I only went to 30 pi because the plot was just a blue splotch if you went to 1000 pi, but its the same idea)
import matplotlib.pyplot as plt
import numpy as np

fig, axs = plt.subplots()
x = np.arange(0, 30 * np.pi, 0.1)
y = 2 * np.sin(x) + 3
axs.plot(x, y)

xdata = np.array([-1e6, 0, 10, np.max(x) - 10, np.max(x), 1e6])
# make 0 to 10 linear, 10 to max(x)-10 much faster linear
# max(x)-10 to max(x) slower linear
xnew = np.array([-1e6, 0, 10, 20, 30, 1e6])


def forward(x):
    return np.interp(x, xdata, xnew)


def inverse(x):
    return np.interp(x, xnew, xdata)

axs.set_xscale('function', functions=(forward, inverse))


",matplotlib
how to fill spaces between subplots with a color in matplotlib,"With the following code :
nb_vars=4

fig, axs = plt.subplots(4,4,figsize=(8,8), gridspec_kw = {'wspace':0.20, 'hspace':0.20}, dpi= 100)
for i_ax in axs:
    for ii_ax in i_ax:
        ii_ax.set_yticklabels([])
for i_ax in axs:
    for ii_ax in i_ax:
        ii_ax.set_xticklabels([])

The space between the subplots is white. How is it possible to colour them ? And with different colors ?
See for example this figure :

","You could add patches in between the axes:
from matplotlib import patches

nb_vars=4

# colors between two axes in a row
r_colors = [['#CC0000', '#CC0000', '#CC0000'],
            ['#0293D8', '#0293D8', '#0293D8'],
            ['#FF8E00', '#FF8E00', '#FF8E00'],
            ['#ABB402', '#ABB402', '#ABB402'],
           ]

# colors between two axes in a column
c_colors = [['#CC0000', '#0293D8', '#FF8E00', '#ABB402'],
            ['#CC0000', '#0293D8', '#FF8E00', '#ABB402'],
            ['#CC0000', '#0293D8', '#FF8E00', '#ABB402'],
           ]

fig, axs = plt.subplots(4, 4, figsize=(4, 4),
                        gridspec_kw = {'wspace':0.20, 'hspace':0.20}, dpi= 100)
h, w = axs.shape

for r, i_ax in enumerate(axs):
    for c, ii_ax in enumerate(i_ax):
        ii_ax.set_yticklabels([])
        ii_ax.set_xticklabels([])
        ii_ax.plot([-r, r], [-c, c]) # plot dummy line for demo
        bbox = ii_ax.get_position()
        if r+1 < h:
            ii_ax.add_patch(patches.Rectangle((bbox.x0, bbox.y0),
                                              bbox.width, -0.2,
                                              facecolor=c_colors[r][c],
                                              zorder=-1, clip_on=False,
                                              transform=fig.transFigure, figure=fig
                                             ))
        if c+1 < w:
            ii_ax.add_patch(patches.Rectangle((bbox.x1, bbox.y0),
                                              0.2, bbox.height,
                                              facecolor=r_colors[r][c],
                                              zorder=-1, clip_on=False,
                                              transform=fig.transFigure, figure=fig
                                             ))

Output:

",matplotlib
how can i make the xaxis of my 2d histogram use dates while avoiding overflow errors from matplotlib axis formatting,"I am working with a set of monthly averaged time-series data that spans 20+ years and have put the data into a pandas dataframe. The index of the dataframe is composed of the datetime objects that span the time range of the dataset. I have successfully created a 2D histogram subplot of both time and another parameter, proton speed. The x-axis of the histogram was created by what seems like a default action, but I'm not sure how to interpret it. I have been trying to format the x-axis using matplotlib commands, primarily the date locator/formatter functions, but they keep throwing a massive overflow error that ends with: ""OverflowError: int too big to convert.""
I have not been successful in finding a good solution with other questions or through the documentation.
These are the imports I have used so far:
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from datetime import datetime, date, time
import matplotlib.dates as mdates

The following is the pandas dataframe that I have been using. I apologize if the formatting is weird. I wasn't sure how to share the table, so I copied the dataframe directly from my notebook. The columns should be tab delimited here.
Datetime  proton_density proton_temp  He4toprotons proton_speed x_dot_RTN Proton Mass Flux
----------------------------------------------------------------------------------------                    
1998-01-23  11.625  58930.0 0.0224  380.90  379.91  7.406307e-19
1998-02-19  9.569   64302.0 0.0294  380.99  380.23  6.097867e-19
1998-03-18  8.767   66770.0 0.0348  384.00  383.19  5.630929e-19
1998-04-14  7.410   121090.0    0.0352  448.44  446.58  5.558023e-19
1998-05-11  7.881   102230.0    0.0271  421.21  419.87  5.552362e-19
... ... ... ... ... ... ...
2021-09-19  8.244   55183.0 0.0356  384.52  383.22  5.302183e-19
2021-10-16  9.664   70601.0 0.0115  418.50  416.21  6.764725e-19
2021-11-12  6.137   93617.0 0.0256  450.47  449.30  4.624021e-19
2021-12-09  4.889   96768.0 0.0177  426.52  424.99  3.487845e-19
2022-01-05  7.280   85944.0 0.0310  434.17  433.01  5.286752e-19

Here is the code I have used to make my histogram:
ax_example = plt.subplot2grid((3, 6), (2, 1), colspan = 2)

H,xedges,yedges = np.histogram2d(SWEPAM_dataframe.index, SWEPAM_dataframe.proton_speed, bins=[50,50])
ax_example.pcolor(xedges, yedges, H.T)
ax_example.set_xlabel(""Year"")
ax_example.set_ylabel(""Proton Speed (km/s)"")

The result was this:

As you can see, the x-axis is not in datetime by default, it seems. I'm not actually sure how to interpret the default x-axis values, but that's not as important here. I have found that I should be using some combination of ax2.xaxis.set_major_locator(loc) and ax2.xaxis.set_major_formatter(fmt). However, anytime I try to use these commands I get the aforementioned overflow error and am prevented from turning the x-axis of my histogram into the desired dates.
","I could reproduce your issue. Why xedges returns such high numbers (in the 10^17) has to see with how matplotlib reads datetime objects, in what unit of time since epoch.

Indeed prior reformating such as in Format of datetime in pyplot axis seems a valid option
but there may be better ones, e.g. using date2num (Convert datetime objects to Matplotlib dates), as discussed in Pandas vs matplotlib datetime.

I have been trying to make it function reliably to provide a full answer.
Also this overflow error was already reported in Set xaxis data to datetime in matplotlib without receiving a convincing answer.

Alternatively, seaborn is better than matplotlib at handling the datetime dtype in pandas dataframes without requiring further manipulations on the axes:
import seaborn as sns

# with input: (without setting `""Datetime""` as index)
df = pd.DataFrame(columns = ['Datetime','proton_density','proton_temp','He4toprotons','proton_speed','x_dot_RTN','Proton_Mass_Flux'],
                  data = [['1998-01-23',11.625,58930.0,0.0224,380.90,379.91,7.406307e-19],
                          ['1998-02-19', 9.569,64302.0,0.0294,380.99,380.23,6.097867e-19],
                          ['1998-03-18', 8.767,66770.0,0.0348,384.00,383.19,5.630929e-19],
                          ['1998-04-14',7.410,121090.0,0.0352,448.44,446.58,5.558023e-19],
                          ['1998-05-11',7.881,102230.0,0.0271,421.21,419.87,5.552362e-19],
                          ['2021-09-19', 8.244,55183.0,0.0356,384.52,383.22,5.302183e-19],
                          ['2021-10-16', 9.664,70601.0,0.0115,418.50,416.21,6.764725e-19],
                          ['2021-11-12', 6.137,93617.0,0.0256,450.47,449.30,4.624021e-19],
                          ['2021-12-09', 4.889,96768.0,0.0177,426.52,424.99,3.487845e-19],
                          ['2022-01-05', 7.280,85944.0,0.0310,434.17,433.01,5.286752e-19]])
df['Datetime'] = pd.to_datetime(df['Datetime'])

This will then produce the expected 2D histogramm and axes labels:
sns.histplot(df, x=""Datetime"", y=""proton_speed"")

",matplotlib
how to save a matrix as a png image and reload it to recover the original matrix,"I would like to save a 96x96 matrix as an image.
The values in the matrix range from -1 to 1.
When I save the matrix using
plt.imsave(path, matrix, cmap='rainbow'),

it gets stored as a 96x96x4 RGBA image, and each channel's values are rescaled to the 0–255 range.
This makes it difficult to retrieve the original values of the matrix.
Is there a way to preserve the original matrix values when saving and reloading the image?
I attempted the following approach to retrieve the original matrix:
loaded_image = plt.imread(path)
restored_image = np.mean(loaded_image[:, :, :3], axis=2)
print(restored_image )

However, this method only reproduces a matrix with the same dimensions as the original matrix, but the values differ from those of the original matrix.
","There is not a single good answer to your question, as it is not quite clear to me why you want to save your matrix as an image in the first place. Consider the following: Most images that are used in standard use cases (think of showing pictures online, creating plots of data, etc.) are 8-bit RGB(A) or grayscale images, meaning you have 3 or 4 channels for RGB (red, green, blue, and maybe opacity) or 1 channel for grayscale (intensity), each storing 256 integer values (0 to 255). Your data, on the other hand, is floating point data in a very different range (-1 to 1). So a standard, 8-bit image may not be the ideal medium to store your matrix.
There are multiple options you could choose from, depending on the actual task that you are trying to solve:

Since you seem to work with Numpy, you might consider storing your data directly as a Numpy array – have a look at numpy.save() and numpy.load() for that.
If you want to store your data in a Numpy-independent format, you could even consider storing it as a text file, given your matrix only contains 96×96 values – have a look at numpy.savetxt() and numpy.loadtxt() for that. There are other, more advanced formats for storing array data in a language-agnostic way, e.g. HDF5, but I guess that would be a bit too much here.
If you really want to store your matrix as an image, but with minimum loss of information, you could consider using a bit of a less standard image format, such as floating-point TIFF. The tifffile library should support this.
Finally, if you want to use a ""standard"" (8-bit) image, you need to normalize your image data before saving (convert from [-1, 1] to [0, 255]) and after loading (convert from [0, 255] to [-1, 1]). You will lose precision there though, as already mentioned. The following code achieves this, for example:
import numpy as np
from PIL import Image

rand = np.random.default_rng(seed=0xC0FFEE)

# Produce some exemplary data in the given range
mat = rand.uniform(low=-1., high=1., size=(96, 96))
# Move to range [0, 255], then convert to integers
mat_uint8 = np.round((255 / 2) * (mat + 1)).astype(np.uint8)
# Save as 8-bit PNG image
Image.fromarray(mat_uint8).save(""matrix.png"")
# Load and reconstruct original matrix
mat_rec = np.asarray(Image.open(""matrix.png"")).astype(float) * (2 / 255) - 1
assert(np.allclose(mat, mat_rec, atol=1e-2))
print(f""Max. abs. difference: {np.abs(mat_rec - mat).max():.5f}"")
# Prints 0.00392



",matplotlib
in matplotlib why is the parameter for marker size not the same for different methods,"In matplotlib.pyplot.scatter, the parameter for marker size is s, while the parameter in other methods, such as plot, is simply markersize. I know that s is the marker size in points ** 2, so there is of course a difference compared to markersize, but why is it necessary to distinguish between s and markersize?
It seems, to me at least, that it would be simpler to just use one of the parameters for marker size in all types of plots.
I would like to know, as it can be difficult to remember, which parameter should be used in which plotting method.
","Looking at the documentation for the different plot types, the reasoning is not explicitly stated, but we can infer that the reason for this difference would be that in a scatter plot, the s parameter is a positional argument with a default of None, and expects to receive an array of values as the size of the markers in a scatter plot is typically conveying additional information.

Axes.scatter(x, y, s=None, c=None, marker=None, cmap=None, norm=None, vmin=None, vmax=None, alpha=None, linewidths=None, *, edgecolors=None, plotnonfinite=False, data=None, **kwargs)

While it will default to rcParams['lines.markersize'] ** 2 if no value is passed, the s parameter accepts either a float value to use as a constant for all points, or an array-like, shape (n,), giving a different size for each data point.

s : float or array-like, shape (n, ), optional
The marker size in points**2 (typographic points are 1/72 in.). Default is rcParams['lines.markersize'] ** 2.
The linewidth and edgecolor can visually interact with the marker size, and can lead to artifacts if the marker size is smaller than the linewidth.
If the linewidth is greater than 0 and the edgecolor is anything but 'none', then the effective size of the marker will be increased by half the linewidth because the stroke will be centered on the edge of the shape.
To eliminate the marker edge either set linewidth=0 or
edgecolor='none'.

Because s is a positional parameter with a default, you don't have to pass it as a keyword argument; you can simply use scatter(x, y, s) where x, y, and s are all array-like objects with shape (n, ), or you can pass a float value for s.
For the plot method, the markersize or ms argument is a keyword argument only, and only accepts a float.  All of the other fully optional parameters are also passed as keyword arguments only. In fact, the plot method has no required positional arguments.

Axes.plot(*args, scalex=True, scaley=True, data=None, **kwargs)

",matplotlib
bar chart not resize,"I got a problem trying to plot the larger version of this data (it has weeks till today) having all data in that plot space makes not legible the bars labels (3rd image as reference), I don't know what the problem is because it also plot a void figure with the size I gave. Pd. This was my best trial to make a combination of stacked and grouped chart, if someona has a better way to make it, please tell 
Chart to resize
Void figure
example of the data 3rd image
import pandas as pd
import matplotlib.pyplot as plt

# Provided data
data = {
    'TYPE': ['a', 'b', 'b', 'a', 'a', 'b', 'a'],
    'PRODUCT': ['Helium', 'Silver', 'Copper', 'Hidrogen', 'Helium', 'Silver', 'Silver'],
    'Amount': [62, 15, 70, 4, 3, 4, 25],
    'WEEK': [1, 1, 1, 1, 2, 2, 1]
}

# Create DataFrame
df_ini = pd.DataFrame(data)

# Plotting the bar chart grouped by week and showing all the products
df_pivot = df_ini.pivot_table(
    index=['WEEK','TYPE'],
    columns='PRODUCT',
    values='Amount',
    aggfunc='sum',
    fill_value=0
)

# Plotting the bar chart
plt.figure(figsize=(10, 6))
ax = df_pivot.plot(kind='bar', stacked=True)

# Custom x-axis labels
new_labels = [f'{week} / {type}' for week, type in df_pivot.index]
ax.set_xticklabels(new_labels,rotation=45,ha='right')

# Adding labels and title
plt.title('Claims by week')
plt.xlabel('Week')
plt.ylabel('Claims')
plt.legend(title='Product')
plt.tight_layout()
plt.show()

","Remove the below line from your code. It is creating one extra blank Figure.
plt.figure(figsize=(10, 6))

",matplotlib
how do i turnoff this horizontal line in the fft plot,"I am trying to show the trend in observation data and the corresponding fft . A horizontal line keeps appearing in the fft, part that I do not need to show in the chart.
I give a MWE (pseudo data) below.
import numpy as np
from scipy.fft import fft, fftfreq
import matplotlib.pyplot as plt

observations = np.array([ 3.78998207e-01,  3.05199629e-01,  2.29614343e-01,  1.86568613e-01,
                         1.83449462e-01,  1.77892746e-01,  1.66237352e-01,  1.81950778e-01,
                         9.88351226e-02,  1.29674430e-01,  7.08703360e-02,  3.64963487e-02,
                         2.75641060e-02,  6.21573753e-02,  8.51043646e-02,  5.32184940e-02,
                         6.47005530e-02, -6.41628893e-02, -1.86618020e-01, -4.08624200e-02,
                        -2.71649960e-02, -8.22041576e-03,  9.13242105e-03,  1.67080717e-01,
                        -1.37465317e-01,  2.74977101e-04,  4.47602122e-02,  8.27649668e-02,
                        -5.60661808e-02, -2.26248880e-01, -1.54768403e-01, -4.46428484e-02,
                        4.57611677e-02,  9.83215698e-02,  9.22357256e-02, -1.23436114e-01,
                      -2.76981909e-01, -1.98824586e-01, -2.33452893e-01, -2.57550630e-01,
                      -9.13919527e-02,  2.64029442e-02, -5.44394568e-02,  4.02010984e-01,
                        3.27256645e-01,  2.14259077e-01,  5.08021357e-01,  5.55141121e-01,
                        6.11203693e-01,  5.34086779e-01,  2.19652659e-01,  1.71635054e-01,
                        1.30867565e-01,  1.25133212e-01,  1.02010973e-01,  1.16727950e-02,
                        2.84545455e-02, -1.73553706e-02, -1.33998184e-01, -1.36456573e-01,
                      -1.68706794e-01, -1.28378379e-01, -1.43710423e-01, -2.02454545e-01,
                      -4.30164457e-01, -5.19982175e-01, -3.74452537e-01, -3.64076796e-01,
                      -3.20950700e-01, -2.34052515e-01, -1.37158482e-01,  2.80797054e-02,
                        7.04379682e-02,  1.13920696e-01,  1.26391389e-01,  9.31688808e-02,
                        1.46000000e-01,  1.18380338e-01,  5.18909438e-02,  1.11584791e-01,
                        6.43582617e-02, -6.36856386e-02, -9.16134931e-02, -1.02616820e-01,
                      -4.43179890e-01, -1.28223431e+00, -1.86160058e+00, -1.43772912e+00,
                      -1.21047880e+00, -7.21282278e-01, -1.65349241e-01,  4.58791266e-02,
                        2.42897190e-01,  3.26587994e-01,  3.15827382e-01, -5.29090909e-02,
                        8.97887313e-03,  2.61194000e-02, -2.24566234e-01, -9.18572710e-02])

observed_fft = fft(observations)

fs = 100
n = observations.size
fft_fre = fftfreq(n, d=1/fs)
x_time = np.arange(len(observations))

fig, axs = plt.subplots(2)
axs[0].plot(x_time, observations)
axs[1].plot(fft_fre, np.abs(observed_fft))

Output:

I know this horizontal line appears because the FFT frequencies in fft_fre include both positive and negative frequencies, thus a symmetrical plot around zero frequency (and I needed to show both the +ve and ve frequencies).
But isn't there a workaround to turn-off the line connecting the last negative frequency to the first positive frequency?
","Yes, there is a workaround.
Shift the FFT Output (This makes it easier to deal with the data in sequential manner):
fft_fre_shifted = fftshift(fft_fre)
fft_magnitude_shifted = np.abs(fftshift(observed_fft))

Then determine the midpoint:
if n % 2 == 0:
    midpoint = n // 2
else:
    midpoint = (n // 2) + 1

Then insert a Nan at the midpoint to break the data so the first positive and last negative frequency are not connected:
fft_fre_plot = np.insert(fft_fre_shifted, midpoint, np.nan)
fft_magnitude_plot = np.insert(fft_magnitude_shifted, midpoint, np.nan)

You should get this:

If adding the extra element to the array is an issue then you can plot the positive and negative frequency arrays separately as follows:
# Separate negative and positive frequencies
neg_freqs = fft_fre_shifted[:midpoint]
neg_magnitude = fft_magnitude_shifted[:midpoint]

pos_freqs = fft_fre_shifted[midpoint:]
pos_magnitude = fft_magnitude_shifted[midpoint:]

Then plot as:
# FFT plot without connecting negative and positive frequencies
axs[1].plot(neg_freqs, neg_magnitude, color='green', label='Negative Frequencies')
axs[1].plot(pos_freqs, pos_magnitude, color='red', label='Positive Frequencies')
axs[1].set_title('FFT Magnitude Spectrum')
axs[1].set_xlabel('Frequency (Hz)')
axs[1].set_ylabel('Magnitude')
axs[1].legend()
axs[1].grid(True)

To achieve this:

Then you can continue with the positive and negative arrays unaffected by the NaN. the green and red is just for explaining, you can make them both the same colour as you like.
",matplotlib
how can i animate object a thrown upwards from an given height then freefalling with matplotlib,"Using Python 3.12, Matplotlib 3.9.2,
From height h=20(m), throw object A upwards with a given velocity v and drop object B to let it free-falling. Air resistance is neglectable. Calculate v so that object A falls to the ground dt =2 seconds(s) after object B and animate. g = 9.81 (m/s2)
I've been trying to animate said problem with matplotlib using 'set_data', but the results are unsatisfying for previous positions of A is kept which made the animation overlap itself after a while. I want the previous data to be removed, but I couldn't find any functions to help it.
Here is the code that I've used:
import numpy as np
import matplotlib.pyplot as plt
from matplotlib.animation import FuncAnimation  

#Input Variables
h, g, dt= 20, 9.81, 2

#Calculated Variables
tmp = np.sqrt(2*h/g) + dt
v = 0.5*g*tmp - h/tmp 

#Arrays 
t = np.linspace(0, tmp , 100)
hA = h + v*t - 0.5*g*t**2
hB = h - 0.5*g*t**2

#Plot
fig, ax = plt.subplots()
plt.axis([0, 10, 0, 100])
ax.set_title(""Physic 1"")
plt.grid()

#Define object A and object B
A, = ax.plot([],[],""o"",markersize=4,color=""red"")
B, = ax.plot([],[],""o"",markersize=4,color=""blue"")

#Animation
def animate(frame):
    A.set_data([2],hA[:frame])
    B.set_data([6],hB[:frame])
    return A,B

ani=FuncAnimation(fig, animate, frames=len(t)+2, interval=25, repeat =False)
plt.show()

","When you use hA[:frame] you're passing all the points up to index frame. Since you only want a single index, you should do hA[frame]. You will need to wrap that result in a list so that y is a sequence (otherwise you get the error mentioned in the comments). In the end, you'll have this:
def animate(frame):
    A.set_data([2], [hA[frame]])
    B.set_data([6], [hB[frame]])
    return A, B

This can also be done using scatter plots by changing the corresponding lines to this:
# the zorder argument puts the scatter points on top of the grid markings
A = ax.scatter([], [], s=16, color=""red"", zorder=2)
B = ax.scatter([], [], s=16, color=""blue"", zorder=2)

def animate(frame):
    A.set_offsets([2, hA[frame]])
    B.set_offsets([6, hB[frame]])
    return A, B

Lastly, your hA and hB arrays are the same length as t, so the number of frames should be len(t), not len(t)+2.
",matplotlib
remove border from matplotlib 3d pane,"I would like to remove the borders from my 3D scene as described below. Any idea how to do that?

Here the code to generate the current scene:
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D

# Create figure
plt.style.use('dark_background') # Dark theme
fig = plt.figure()
ax = fig.add_subplot(111, projection='3d')

# Make panes transparent
ax.xaxis.pane.fill = False # Left pane
ax.yaxis.pane.fill = False # Right pane

# Remove grid lines
ax.grid(False)

# Remove tick labels
ax.set_xticklabels([])
ax.set_yticklabels([])
ax.set_zticklabels([])

# Print chart
file_path = 'charts/3d.png'
fig.savefig(file_path, bbox_inches='tight', pad_inches=0.05, transparent=True)

","I usually set the alpha channel to 0 for spines and panes, and finally I remove the ticks: 
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D

# Create figure
plt.style.use('dark_background') # Dark theme
fig = plt.figure()
ax = fig.add_subplot(111, projection='3d')

# Make panes transparent
ax.xaxis.pane.fill = False # Left pane
ax.yaxis.pane.fill = False # Right pane

# Remove grid lines
ax.grid(False)

# Remove tick labels
ax.set_xticklabels([])
ax.set_yticklabels([])
ax.set_zticklabels([])

# Transparent spines
ax.w_xaxis.line.set_color((1.0, 1.0, 1.0, 0.0))
ax.w_yaxis.line.set_color((1.0, 1.0, 1.0, 0.0))
ax.w_zaxis.line.set_color((1.0, 1.0, 1.0, 0.0))

# Transparent panes
ax.w_xaxis.set_pane_color((1.0, 1.0, 1.0, 0.0))
ax.w_yaxis.set_pane_color((1.0, 1.0, 1.0, 0.0))

# No ticks
ax.set_xticks([]) 
ax.set_yticks([]) 
ax.set_zticks([])

",matplotlib
pyplot animation not working properly when pltpause is used,"I'm trying to create a pyplot simulation using plt.pause() but I can't make even the simplest example work. This is the code:
import matplotlib.pyplot as plt
import numpy as np

np.random.seed(19680801)
data = np.random.random((50, 50, 50))

fig, ax = plt.subplots()

for i, img in enumerate(data):
    ax.clear()
    ax.imshow(img)
    ax.set_title(f""frame {i}"")
    plt.pause(0.1)

The issue seems to have something to do with the last line of code (plt.pause(0.1)). Without this line the final output shows the final frame of the simulation—frame 49 (indicating the whole loop has finished). If I include the final line and run the simulation, the output stops at the first frame—frame 0 (and the simulation doesn't progress to the next step in the loop). I've also tried to set plt.pause(0.0) but that had exactly same effect as setting it to any other number.
I'm on mac, using python 3.12 under jupyter notebook 7.2.1. I would appreciate any advice. Thank you.
","I think this example is not made to work in Jupyter notebooks. Getting matplotlib animation to run in notebooks can be cumbersome....
Try this minimal example to get started. Does it work?:
import matplotlib.animation
import matplotlib.pyplot as plt
import numpy as np
plt.rcParams[""animation.html""] = ""jshtml""
plt.rcParams['figure.dpi'] = 150  
plt.ioff()
fig, ax = plt.subplots()

x= np.linspace(0,10,100)
def animate(t):
    plt.cla()
    plt.plot(x-t,x)
    plt.xlim(0,10)

matplotlib.animation.FuncAnimation(fig, animate, frames=10)

see Inline animations in Jupyter for more information (this example was also taken from there)
Edit:
It should also work with the magic %matplotlib command that will open an interactive window instead of using the cell output. See Plots that varies over the time on Python Matplotlib with Jupyter
",matplotlib
how to properly scale cmapcolor,"Below is a fully-functioning example of my problem.  The color that cmap assigns based on the values I give to it does not match the color that it should be on the color bar.  For example, the fragment with distance=1.75 is yellow when according to the colorbar it should be light purple.  How can I assign the colors properly to the bars?
#Create some fake example data and store it in a dataframe
exampledf=pd.DataFrame()

import random as rd
tempname='Template'
queryname='Query'

templatenamelst=[]
querynamelst=[]
startlst=[]
endlst=[]
querystartslst=[]
queryendslst=[]
distancelst=[]

for i in range(20):
    start=i*5
    end=(i*5)+25
    distance=rd.uniform(0, 4)
    
    querystart=rd.randint(0, 95)
    queryend=querystart+25
    
    templatenamelst.append(tempname)
    querynamelst.append(queryname)
    startlst.append(start)
    endlst.append(end)
    distancelst.append(distance)
    querystartslst.append(querystart)
    queryendslst.append(queryend)

exampledf['TempName']=templatenamelst
exampledf['QueryName']=querynamelst
exampledf['TempStarts']=startlst
exampledf['TempEnds']=endlst
exampledf['QueryStarts']=querystartslst
exampledf['QueryEnds']=queryendslst
exampledf['Distances']=distancelst

display(exampledf)

    
#Build the plot from the example data

import random as rd
import numpy as np
import pandas as pd
import warnings
import matplotlib.pyplot as plt
from matplotlib import cm
from matplotlib.colors import ListedColormap, LinearSegmentedColormap
warnings.filterwarnings('ignore') 

TempName=exampledf.iloc[0]['TempName']
QueryName=exampledf.iloc[0]['QueryName']
NumSeqFrags=len(exampledf)

TempLength=templength+2

Distanceslst=list(exampledf.Distances)

XMax=max(list(exampledf.TempEnds))
cmapz = cm.get_cmap('plasma')

img = plt.imshow(np.array([[0,max(Distanceslst)]]), cmap=""plasma"")
img.set_visible(False)

barheight=6

plt.barh(width=TempLength, left=0, height=barheight, y=0,color='Black')
ticklabels=[TempName]
ticklocations=[0]

#Add the fragments
for i in range (len(exampledf)):
    cmapcolor=cmapz(Distanceslst[i])
    width=int(exampledf.iloc[i]['TempEnds'])-int(exampledf.iloc[i]['TempStarts'])
    start=int(exampledf.iloc[i]['TempStarts'])

    yloc=(barheight+4)*i+barheight

    plt.barh(width=width, left=start, height=barheight, y=yloc,color=cmapcolor)
    
    fullname=str(exampledf.iloc[i]['QueryName'])+'('+str(exampledf.iloc[i]['QueryStarts'])+':'+str(exampledf.iloc[i]['QueryEnds'])+')'+'(Distance='+str(round(Distanceslst[i],2))+')'
    ticklabels.append(fullname)
    ticklocations.append(yloc)

plt.yticks(ticks=ticklocations, labels=ticklabels,fontsize=8)    

plt.colorbar(orientation=""horizontal"",fraction=0.05,label='Distance')

plt.show() 



","The Colormap object's __call__ method takes a number between zero and one.
For scaling arbitrary data into that range see the matplotlib.colors.Normalize class:
from matplotlib.colors import Normalize
norm = Normalize(vmin = min(Distanceslst), vmax = max(Distanceslst))
...
cmapcolor=cmapz(norm(Distanceslst[i]))

",matplotlib
syncing matplotlib imshow coordinates,"I'm trying to create an image using networkx, save that image to use later, and then overlay a plot over top of it later.  However, when I try to load the image in and make new points, the scale seems off.  I've tried everything I can find to make them sync, and I'm not sure what else to try at this point.  Here's a simple example:
import networkx as nx
import matplotlib.pyplot as plt
import numpy as np

fig = plt.figure()
G = nx.dodecahedral_graph()
pos = nx.spring_layout(G)
plt.box(False)
nx.draw_networkx_edges(G, pos=pos)
fig.canvas.draw()

data = np.array(plt.gcf().canvas.get_renderer().buffer_rgba(), dtype=np.uint8)
extent = list(plt.xlim() + plt.ylim())


So now I have a graph and have saved that image to data, and have saved the range of that graph to extent.  I then want to replot that graph from data and overlay the nodes of the graph, in the positions stored in pos.
plt.imshow(data, extent=extent)
plt.box(False)
nx.draw_networkx_nodes(G, pos=pos, node_color='green')


For some reason, the scale of the original image is shrunk, so the nodes end up being at a larger scale and not matching the edges.  Is it something in the way I'm saving the image?
","It seems that matplotlib adds padding to the sides of the image when saving the data from the plot. You can remove this padding by adding fig.tight_layout(pad=0) to the code like so:
import networkx as nx
import matplotlib.pyplot as plt
import numpy as np

fig = plt.figure()
G = nx.dodecahedral_graph()
pos = nx.spring_layout(G)
plt.box(False)
nx.draw_networkx_edges(G, pos=pos)
fig.tight_layout(pad=0)
fig.canvas.draw()

data = np.array(plt.gcf().canvas.get_renderer().buffer_rgba(), dtype=np.uint8)
extent = list(plt.xlim() + plt.ylim())

",matplotlib
how can i adjust the white empty space around a matplotlib table to match the table size,"There's extra white, empty space around my Matplotlib table, causing my table to get squished as if the figsize wasn't big enough. If I increase the figsize, the table displays normally, but then the white, empty space gets even bigger. Additionally, the title gets misplaced over the table when the number of rows passes a certain number (around 35 or so).
import matplotlib.pyplot as plt

def create_pivot_table(
        title: str,
        pivot_table: pd.core.frame.DataFrame,
):
    """"""
    Creates a Matplotlib table from a Pandas pivot table.
    Returns fig and ax.
    """"""

    fig_width = 1 + len(pivot_table.columns) * 0.6
    fig_height = 1 + len(pivot_table) * 0.3
    figsize=(fig_width, fig_height)
    fig, ax = plt.subplots(figsize=figsize)

    title = title
    ax.set_title(
        title,
        fontsize=16,
        weight='bold',
        loc='center'
    )
    ax.axis('off')

    table = ax.table(
        cellText=pivot_table.values,
        colLabels=pivot_table.columns,
        rowLabels=pivot_table.index,
        cellLoc='center',
        loc='center'
    )

    table.auto_set_font_size(False)

    # Formatting the table with alternating colors to make it more readable
    for (row, column), cell in table.get_celld().items():
        if row == 0:
            cell.set_text_props(weight='bold')
            cell.set_fontsize(8)
            cell.set_height(0.05)

            if column % 2 == 0:
                cell.set_facecolor('#dadada')
            else:
                cell.set_facecolor('#ffffff')
        else:
            cell.set_fontsize(10)
            cell.set_height(0.03)

            if row % 2 == 0:
                cell.set_facecolor('#ffffff')
            else:
                cell.set_facecolor('#e6f7ff')

            if column % 2 == 0:
                cell.set_facecolor('#dadada' if row % 2 == 0 else '#b9d5d3')
            else:
                cell.set_facecolor('#ffffff' if row % 2 == 0 else '#e6f7ff')

    return fig, ax

Dummy data:
import pandas as pd
import numpy as np

data = np.random.randint(1, 100, size=(3, 4))
df = pd.DataFrame(data, index=['A', 'B', 'C'], columns=[1, 2, 3, 4])

title = 'Title'

fig, ax = create_pivot_table(
    title=title,
    pivot_table=df
)

Result:

Desired outcome: the same table, minus the empty white space, so the table displays correctly. Additionally, I'd like to center the title and table within the figure, which does not happen with the code I wrote (depending on the table and title size).
Thanks in advance for the help!
","I think you need to try suptitle for title and play with bbox parameter of  table.
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np

def create_pivot_table(
        title: str,
        pivot_table: pd.core.frame.DataFrame,
):
    """"""
    Creates a Matplotlib table from a Pandas pivot table.
    Returns fig and ax.
    """"""

    fig_width = 1 + len(pivot_table.columns) * 0.6
    fig_height = 1 + len(pivot_table) * 0.3
    figsize=(fig_width, fig_height)
    fig, ax = plt.subplots(figsize=figsize)

    fig.suptitle(title, fontsize=16, weight='bold', y=0.95)

    ax.axis('off')

    table = ax.table(
        cellText=pivot_table.values,
        colLabels=pivot_table.columns,
        rowLabels=pivot_table.index,
        cellLoc='center',
        loc='center',
        bbox=[0,0,1,0.95]
    )

    table.auto_set_font_size(False)

    # Formatting the table with alternating colors to make it more readable
    for (row, column), cell in table.get_celld().items():
        if row == 0:
            cell.set_text_props(weight='bold')
            cell.set_fontsize(8)
            cell.set_height(0.05)

            if column % 2 == 0:
                cell.set_facecolor('#dadada')
            else:
                cell.set_facecolor('#ffffff')
        else:
            cell.set_fontsize(10)
            cell.set_height(0.03)

            if row % 2 == 0:
                cell.set_facecolor('#ffffff')
            else:
                cell.set_facecolor('#e6f7ff')

            if column % 2 == 0:
                cell.set_facecolor('#dadada' if row % 2 == 0 else '#b9d5d3')
            else:
                cell.set_facecolor('#ffffff' if row % 2 == 0 else '#e6f7ff')

    return fig, ax

data = np.random.randint(1, 100, size=(3, 4))
df = pd.DataFrame(data, index=['A', 'B', 'C'], columns=[1, 2, 3, 4])

title = 'Title'

fig, ax = create_pivot_table(
    title=title,
    pivot_table=df
)

plt.show()

",matplotlib
39offset points39 in matplotlibpyplotannotate gives unexpected results,"I am using the following code to generate a plot with a sine curve marked with 24 'hours' over 360 degrees. Each 'hour' is annotated, however the arrow lengths decrease (shrivel?) with use and even their direction is incorrect.
The X axis spans 360 degrees whereas the Y axis spans 70 degrees. The print statement verifies that the arrows on 6 and 18 hours have the same length and are vertical, according to the offsets specified. This is not so as seen in the resulting plot:

Matplotlib version = 3.9.2; Numpy version = 2.1.1
Here is the python code:
#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import numpy as np
import matplotlib.pyplot as plt

arrow_style = {'arrowstyle': '-|>'}

def make_plot():

    fig, ax = plt.subplots()
    ax.axis([0,+360,-35,+35])
    ax.set(xlabel = 'X (degrees)', ylabel = 'Y (degrees)', title='Vanishing arrow length example')

    degree = np.pi / 180.0  # degrees to radians
    arrow_angle = 72.0      # degrees
    arrow_length = 27.0
    eps = 23.43927945

    eps_x = np.linspace(0, 360, 200)
    eps_y = -eps * np.sin(2 * np.pi * eps_x / 360)
    ax.plot(eps_x, eps_y, 'r')

    hr_x = np.linspace(0, 360, 25)
    hr_y = -eps * np.sin(2 * np.pi * hr_x / 360)

    i = 0
    for x in hr_x:
        ax.plot(hr_x[i], hr_y[i],'bo')
        if hr_y[i] > 0 and arrow_angle > 0: arrow_angle = -75   # degrees
        arrow_x = np.cos(arrow_angle*degree) * arrow_length
        arrow_y = np.sin(arrow_angle*degree) * arrow_length
        ax.annotate(str(i), xy=(hr_x[i], hr_y[i]), xytext=(arrow_x, arrow_y), \
                    xycoords='data', textcoords='offset points', arrowprops=arrow_style)
        print(""arrow_x {:.2f}  arrow_y {:.2f}  arrow_angle {:.1f} at {} hours"" \
              .format(arrow_x, arrow_y, arrow_angle, i))

        if hr_y[i] <= 0: arrow_angle += 3.0
        if hr_y[i] > 0:  arrow_angle -= 3.0
        i += 1

    ax.grid()
    ax.axhline(0.0)
    ax.axvline(180.0)
    plt.show()

    return fig

make_plot().savefig('vanishing_arrow_length.png')

I have only a few days experience with matplotlib so I guess this is a really simple user error. However the documentation is no help to me in this case.
","By default the text is aligned at its bottom left corner.  You might want to change this to align at its centre.
ax.annotate(str(i), xy=(hr_x[i], hr_y[i]), xytext=(arrow_x, arrow_y),
            xycoords='data', textcoords='offset points', arrowprops=arrow_style,
            verticalalignment='center', horizontalalignment='center')



",matplotlib
conditional marker for scatterplot matplotlib,"I am trying to plot x1 and x2 against each other, and if class == 1, then marker should be +, if class == 0, then the marker should just be a dot.
The file.csv is structured like this, the class column which is the conditional will only be either 1 or 0:



x1
x2
mark




1
2
0


9
4
1


0
5
1


2
6
0



Here's the code I have:
df = pd.read_csv('file.csv')
print(df)
x1 = df['x1'].to_numpy()
x2 = df['x2'].to_numpy()
mark = df['class'].to_numpy()
        
figure, ax = plt.subplots()

for i in range(0,80,1):
    if mark[i] > 0:
        ax.plot(x1[i], color=""red"", marker='o')
    else:
        ax.plot(x1[i], color=""blue"", marker='x')

This is what the result SHOULD look like: 
","I would plot it this way for all the data, if you're going to select a subset, consider using a new dataframe to select the subset:
import matplotlib.pyplot as plt

# the rows where class is 1 and plot them with '+' marker
plt.scatter(df[df['class']==1]['x1'], df[df['class']==1]['x2'], marker='+')

# the rows where class is 0 and plot them with 'o' marker 
plt.scatter(df[df['class']==0]['x1'], df[df['class']==0]['x2'], marker='o')

plt.xlabel('x1') 
plt.ylabel('x2') 
plt.title('Scatter plot x1 vs x2') 
plt.show()

",matplotlib
pyplottable working with different colspans and rowspans,"I'm looking for a way to add columns that can span multiple rows, and rows that can span multiple columns.
I currently have the code below to get the first row in.
# Calculate log-scaled widths
table_widths = [0.001, 0.002, 0.063, 2.0, 63.0, 150.0]
log_table_widths = np.diff(np.log10(table_widths))
log_table_widths = log_table_widths / log_table_widths.sum()

# Normalize widths to sum to 1
log_table_widths = log_table_widths / log_table_widths.sum()

table = ax.table(cellText=[['Clay', 'Silt', 'Sand', 'Gravel', 'Cobbles']], cellLoc='center', loc='bottom', colWidths=log_table_widths)
table_widths = []
table.auto_set_font_size(False)
table.set_fontsize(8)
table.scale(1, 1.5)

To get the following result:

However I need to add another row to the table that will have some columns span over the next row. While cells on the current row will have to span over multiple columns. Like so:

Preferably with the bottom row being the top row, but not a disaster.
I've tried going at this alone and getting the help from GitHub CoPilot and MS CoPilot. However with no luck, the best we could come up with is the following:
# Calculate log-scaled widths
table_widths = [0.001, 0.002, 0.063, 2.0, 63.0, 150.0]
log_table_widths = np.diff(np.log10(table_widths))
log_table_widths = log_table_widths / log_table_widths.sum()

# Normalize widths to sum to 1
log_table_widths = log_table_widths / log_table_widths.sum()

# Create the table
cell_text = [
  ['Clay', 'Silt', 'Fine', 'Medium', 'Coarse', 'Fine', 'Medium', 'Coarse'],
  ['', '', 'Sand', 'Sand', 'Sand', 'Gravel', 'Gravel', 'Gravel'],
]
col_labels = ['Clay', 'Silt', 'Fine', 'Medium', 'Coarse', 'Fine', 'Medium', 'Coarse']
col_widths = [log_table_widths, log_table_widths, log_table_widths/3, log_table_widths/3, log_table_widths/3, log_table_widths/3, log_table_widths/3, log_table_widths/3]

# Add the table to the plot
table = ax.table(cellText=cell_text, colLabels=col_labels, cellLoc='center', loc='bottom', colWidths=col_widths)
table.auto_set_font_size(False)
table.set_fontsize(8)
table.scale(1, 1.5)

# Adjust cell alignment to avoid ambiguity
for key, cell in table.get_celld().items():
  cell.set_text_props(ha='center', va='center')

Giving me the following error:
ValueError: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()

With no clue on how to solve it.
For reproducability you can use:
fig, ax = plt.subplots()
fig.set_figwidth(18)
fig.set_figheight(12)
fig.set_dpi(80)
# draw vertical line at: 0.002mm, 0.063mm, 2.0mm, 63mm
ax.axvline(x=0.002, color='red', linestyle='--')
ax.axvline(x=0.063, color='red', linestyle='--')
ax.axvline(x=2.0, color='red', linestyle='--')
ax.axvline(x=63.0, color='red', linestyle='--')
ax.set_xlim(0.001, 150)

# Calculate log-scaled widths
table_widths = [0.001, 0.002, 0.063, 2.0, 63.0, 150.0]
log_table_widths = np.diff(np.log10(table_widths))
log_table_widths = log_table_widths / log_table_widths.sum()

# Normalize widths to sum to 1
log_table_widths = log_table_widths / log_table_widths.sum()

# Create the table
cell_text = [
  ['Clay', 'Silt', 'Fine', 'Medium', 'Coarse', 'Fine', 'Medium', 'Coarse'],
  ['', '', 'Sand', 'Sand', 'Sand', 'Gravel', 'Gravel', 'Gravel'],
]
col_labels = ['Clay', 'Silt', 'Fine', 'Medium', 'Coarse', 'Fine', 'Medium', 'Coarse']
col_widths = [log_table_widths, log_table_widths, log_table_widths/3, log_table_widths/3, log_table_widths/3, log_table_widths/3, log_table_widths/3, log_table_widths/3]

# Add the table to the plot
table = ax.table(cellText=cell_text, colLabels=col_labels, cellLoc='center', loc='bottom', colWidths=col_widths)
table.auto_set_font_size(False)
table.set_fontsize(8)
table.scale(1, 1.5)

# Adjust cell alignment to avoid ambiguity
for key, cell in table.get_celld().items():
  cell.set_text_props(ha='center', va='center')

fig.savefig('fig.png', format='png', bbox_inches='tight')


EDIT:
I've managed to get rid of the error. It was caused by defining the col_widths variable where I was filling it with lists instead of their corresponding value.
I have now defined it like this, might find a better solution for this later down the line.
col_widths = [log_table_widths[0], log_table_widths[1], log_table_widths[2] / 3, log_table_widths[2] / 3, log_table_widths[2] / 3, log_table_widths[3] / 3, log_table_widths[3] / 3, log_table_widths[3] / 3, log_table_widths[4]]

My table now looks like so:

Although I have not figured out how to merge the rows and cells.
I did find this post: Matplotlib table with double headers
Where multiple tables are created to show multiple headers. But this, sadly will not work for merging cells in a column and only a row.
","Your mistake is that log_table_widths is an array, not a scalar.
Matplotlib tables don't support merged cells. You can simulate it by removing the cell edges:
import matplotlib.pyplot as plt
import matplotlib.table as mtable
import numpy as np

xlims = [0.001, 150]
hlines = [0.002, 0.063, 2.0, 63.0]

fig, ax = plt.subplots(figsize=(18,12), dpi=80)
ax.set(xscale='log', xlim=xlims)
ax.xaxis.set_visible(False)

for x in hlines:
  ax.axvline(x=x, color='red', linestyle='--')

log_table_widths = np.diff(np.log10(np.r_[xlims[0], hlines, xlims[1]]))
log_table_widths /= log_table_widths.sum()
log_table_widths  = np.r_[
     log_table_widths[0],
     log_table_widths[1] / 3,
     log_table_widths[1] / 3,
     log_table_widths[1] / 3,
     log_table_widths[2] / 3,
     log_table_widths[2] / 3,
     log_table_widths[2] / 3,
     log_table_widths[3] / 3,
     log_table_widths[3] / 3,
     log_table_widths[3] / 3,                          
     log_table_widths[4]
     ]

texts = [
    ['', 'Fine', 'Middle', 'Coarse','Fine', 'Middle', 'Coarse', 'Fine', 'Middle', 'Coarse', ''],
    ['Clay', '','Silt','', '', 'Sand', '', '','Gravel','', 'Cobbles'],
]

table = ax.table(cellText=texts, cellLoc='center', loc='bottom', colWidths=log_table_widths, fontsize=8)
table.scale(1, 1.5)
for c in range(2, 9, 3):
  table.get_celld()[(1,c-1)].visible_edges='BTL'
  table.get_celld()[(1,c)].visible_edges='BT'
  table.get_celld()[(1,c+1)].visible_edges='BTR'


Update:
The merge even numbers of cells you'll need to re-position the text. Because of the drawing order of the table, this can't be done by a simple set_position. The only way I found is to translate the text by a transform:
import matplotlib.pyplot as plt
import matplotlib.transforms as transforms
import numpy as np

xlims = [0.001, 150]
hlines = [0.002, 0.063, 2.0, 63.0]

fig, ax = plt.subplots(figsize=(18,12), dpi=80)
ax.set(xscale='log', xlim=xlims)
ax.xaxis.set_visible(False)

for x in hlines:
  ax.axvline(x=x, color='red', linestyle='--')

log_table_widths = np.diff(np.log10(np.r_[xlims[0], hlines, xlims[1]]))
log_table_widths /= log_table_widths.sum()
log_table_widths  = np.r_[
     log_table_widths[0],
     log_table_widths[1] / 3,
     log_table_widths[1] / 3,
     log_table_widths[1] / 3,
     log_table_widths[2] / 3,
     log_table_widths[2] / 3,
     log_table_widths[2] / 3,
     log_table_widths[3] / 3,
     log_table_widths[3] / 3,
     log_table_widths[3] / 3,                          
     log_table_widths[4]
     ]

texts = [
    ['', 'Fine', 'Medium', 'Coarse','Fine', 'Medium', 'Coarse', 'Fine', 'Medium', 'Coarse', ''],
    ['Clay', '','Silt','', '', 'Sand', '', '','Gravel','', 'Cobbles'],
]

table = ax.table(cellText=texts, cellLoc='center', loc='bottom', colWidths=log_table_widths, fontsize=8)
table.scale(1, 1.5)
for c in range(2, 9, 3):
  table.get_celld()[(1,c-1)].visible_edges = 'BTL'
  table.get_celld()[(1,c)].visible_edges = 'BT'
  table.get_celld()[(1,c+1)].visible_edges = 'BTR'

transform = transforms.Affine2D().translate(0, 10)
for c in (0, 10):
  table.get_celld()[(0,c)].visible_edges = 'TRL'
  table.get_celld()[(1,c)].visible_edges = 'BRL'
  table.get_celld()[(1,c)].get_text().set_transform(transform)


",matplotlib
how do i change the tiling of hatch applied to a patch in matplotlib,"How do I change the way that matplotlib tiles custom hatching in a patch? I would like to tile the hatch in a grid pattern (like you would square tile) as opposed to the alternating row (like you would lay brick).
┌───┐┌───┐┌───┐
│   ││   ││   │
└───┘└───┘└───┘
┌───┐┌───┐┌───┐
│   ││   ││   │
└───┘└───┘└───┘

vs
┌───┐┌───┐┌───┐
│   ││   ││   │
└───┘└───┘└───┘
──┐┌───┐┌───┐┌─
  ││   ││   ││
──┘└───┘└───┘└─

Here's sample code that generates a simple custom hatch in matplotlib:
import matplotlib.pyplot as plt
import matplotlib.patches as patches
import matplotlib.hatch

square_path = patches.Polygon(
    [[-0.4, -0.4], [4.0, -0.4], [0.4, 0.4], [-0.4, 0.4]],
    closed=True, fill=False).get_path()

class SquareHatch(matplotlib.hatch.Shapes):
    """"""Custom hatches defined by a path drawn inside [-0.5, 0.5] square.

    Identifier 's'.
    """"""
    filled = True
    size = 1.0
    path = square_path

    def __init__(self, hatch, density):
        """"""Initialize the custom hatch.""""""
        self.num_rows = int((hatch.count('s')) * density)
        self.shape_vertices = self.path.vertices
        self.shape_codes = self.path.codes
        matplotlib.hatch.Shapes.__init__(self, hatch, density)

matplotlib.hatch._hatch_types.append(SquareHatch)

# Create a figure and axis
self.fig, self.ax = plt.subplots()
# Create a square patch with hatching
square = patches.Rectangle(xy=(0.25, 0.25), width=0.5, height=0.5, hatch='s', fill=False)
# Add the square patch to the axis
self.ax.add_patch(square)
# Display the plot
plt.show()

I've been searching and I would think this would be an option, but I'm not finding it. AI said it was possible but gave me an option that didn't work... I'd altar my tile, but I can't work out a way to make my pattern work with an offset 2nd row.
","The issue comes from the set_vertices_and_codes function in the Shapes superclass:
def set_vertices_and_codes(self, vertices, codes):
    offset = 1.0 / self.num_rows
    shape_vertices = self.shape_vertices * offset * self.size
    shape_codes = self.shape_codes
    if not self.filled:
        shape_vertices = np.concatenate(  # Forward, then backward.
            [shape_vertices, shape_vertices[::-1] * 0.9])
        shape_codes = np.concatenate([shape_codes, shape_codes])
    vertices_parts = []
    codes_parts = []
    for row in range(self.num_rows + 1):
        ####### Offsetting occurs here #############
        if row % 2 == 0:
            cols = np.linspace(0, 1, self.num_rows + 1)
        else:
            cols = np.linspace(offset / 2, 1 - offset / 2, self.num_rows)
        ####### Offsetting occurs here #############
        row_pos = row * offset
        for col_pos in cols:
            vertices_parts.append(shape_vertices + [col_pos, row_pos])
            codes_parts.append(shape_codes)
    np.concatenate(vertices_parts, out=vertices)
    np.concatenate(codes_parts, out=codes)

Changing the cols to np.linspace(0, 1, self.num_rows+1) regardless of row number removes the offset, but now the self.num_shapes set earlier in the __init__ of the Shape class is off. Ultimately, I found it easier for the custom hatch to just copy-paste the Shape class with some minor edits to remove the offsetting.
class SquareHatch(matplotlib.hatch.HatchPatternBase):
    """"""Custom hatches defined by a path drawn inside [-0.5, 0.5] square.

    Identifier 's'.
    """"""
    filled = True
    size = 1.0
    path = square_path

    def __init__(self, hatch, density):
        """"""Initialize the custom hatch.""""""
        self.num_rows = int((hatch.count('s')) * density)
        self.shape_vertices = self.path.vertices
        self.shape_codes = self.path.codes

        self.num_shapes = (self.num_rows+1)**2
        self.num_vertices = (self.num_shapes *
                                 len(self.shape_vertices) *
                                 (1 if self.filled else 2))

    def set_vertices_and_codes(self, vertices, codes):
        offset = 1.0 / self.num_rows
        shape_vertices = self.shape_vertices * offset * self.size
        shape_codes = self.shape_codes
        if not self.filled:
            shape_vertices = np.concatenate(  # Forward, then backward.
                [shape_vertices, shape_vertices[::-1] * 0.9])
            shape_codes = np.concatenate([shape_codes, shape_codes])
        vertices_parts = []
        codes_parts = []
        for row in range(self.num_rows + 1):

            cols = np.linspace(0, 1, self.num_rows + 1)

            row_pos = row * offset
            for col_pos in cols:
                vertices_parts.append(shape_vertices + [col_pos, row_pos])
                codes_parts.append(shape_codes)
        np.concatenate(vertices_parts, out=vertices)
        np.concatenate(codes_parts, out=codes)

",matplotlib
setting tickslabel in matplotlib disables mouse cursor position,"Whenever I plot something in matplotlib, moving the mouse over the plot shows at the bottom the x and y position of the cursor. This is very convenient when exploring data.
Now, If I set the ticklabels to something else, the x position of the cursor at the bottom of the plot is empty, while the gui keeps track of the y position. Is there a way to keep getting the x position?
This is a simple example of this happening:
import numpy as np
fig, ax = plt.subplots()
x=np.linspace(0,np.pi)
y=np.sin(x)
ax.plot(x,y)
ax.set_xticks([0,np.pi/2,np.pi])
ax.set_xticklabels(['0','pi/2','pi'])
plt.show()

","No, there is no easy way to do this.
When you set the labels with a list of strings you set the xaxis.major_formatter to be FixedFormatter (doc) which has some peculiar behavior to make it work (it always returns '', unless you give it a third argument which given when it labels the x-axis, but not otherwise).   The marking of where the cursor is is generated using the major_formatter and when you call a FixedFormatter it returns '', which is what is displayed.
If you really want this, you have to write your own call back, see Color values in imshow for matplotlib? for a starting point.  (or subclass axes and re-implement format_coord (around line 2903 in axes.py in current matplotlib master branch)
",matplotlib
how do i animate a point moving over a grid without it becoming a line,"I have two sets of x,y coordinates that describe the motions of two planets orbiting the sun over time. My assignment asks me to animate this data in Python using Matplotlib, but my syllabus doesn't cover it well. I've managed to animate the trajectory with the help of MatPlotLib's documentation, but I am unable to display a moving dot at the current locations of the planets:
import matplotlib.pylab as plt
import matplotlib.animation as animation

earthx = planetRs[0][0] #these are arrays of x and y coordinates of Earth and Mars
earthy = planetRs[0][1]
marsx = planetRs[2][0]
marsy = planetRs[2][1]


fig, ax = plt.subplots()

line1 = ax.plot(marsx[0], marsy[0], label='Mars')[0]
line2 = ax.plot(earthx[0], earthy[0], label='Earth')[0]
ax.set(xlim=[-2, 2], ylim=[-2, 2], xlabel='X position', ylabel='Y position')
ax.legend()

def update(frame):
    line1.set_xdata(marsx[:frame])
    line1.set_ydata(marsy[:frame])
    line2.set_xdata(earthx[:frame])
    line2.set_ydata(earthy[:frame])
    return line1, line2

ani = animation.FuncAnimation(fig=fig, func=update, frames=60, interval=30)
plt.close()
ani

Every time I try, it becomes a line with dots at each x,y coordinate. I want the dots to merely show the most recent x,y location of both planets. Could someone help me out?
","You can plot single points for Earth and Mars, and then update each point's position to the current frame coordinates.
import matplotlib.pyplot as plt
import matplotlib.animation as animation

earthx = planetRs[0][0] #these are arrays of x and y coordinates of Earth and Mars
earthy = planetRs[0][1]
marsx = planetRs[2][0]
marsy = planetRs[2][1]


fig, ax = plt.subplots()

line1 = ax.plot([], [], 'ro', label='Mars')[0]
line2 = ax.plot([], [], 'bo', label='Earth')[0]
ax.set(xlim=[-2, 2], ylim=[-2, 2], xlabel='X position', ylabel='Y position')
ax.legend()
ax.grid(False)

def init():
    line1.set_data([], [])
    line2.set_data([], [])
    return line1, line2

def update(frame):
    line1.set_data(marsx[frame], marsy[frame])
    line2.set_data(earthx[frame], earthy[frame])
    return line1, line2

ani = animation.FuncAnimation(
    fig=fig, func=update, init_func=init,
    frames=len(earthx), interval=30, blit=True)
plt.close()
ani


",matplotlib
having problem with pyplot feature on streamlit,"#pie chart
import pylab
df_count_percent = (df_count['count_of_purchase/campaign'] / df_count['count_of_purchase/campaign'].sum()) * 100
plot0 = df_count_percent.plot.pie(y='count_of_purchase/campaign', fontsize=17,autopct='%0.2f%%',pctdistance=1.2,
                                     labeldistance=None)

pylab.ylabel('')
pylab.xlabel('')

plt.title('Count of Purchase or Campaign as Percentage (Pie)', fontsize=19,fontweight=""bold"",pad=6)

plt.legend(loc='upper right', fontsize=9,prop={'size': 17}, bbox_to_anchor=(2,1))
plot0=plt.figure(figsize=(16, 16))
plt.show()


# Plot a bar chart
figure_0=df_count.plot.barh( y=""count_of_purchase/campaign"",
         color='green',fontsize=15,edgecolor=""#111111"")

plt.title('Count of Purchase or Campaign', fontsize=16,fontweight=""bold"",pad=12)
figure_0.set_xlabel('\nFrequency',fontsize=14,fontweight=""bold"")
figure_0=plt.figure()
plt.show()

# Plot a bar chart as percentage
df_count_percent = (df_count['count_of_purchase/campaign'] / df_count['count_of_purchase/campaign'].sum()) * 100
fig=df_count_percent.plot.barh( y=""count_of_purchase/campaign"",fontsize=15 ,color='red',edgecolor=""#111111"")

plt.title('Count of Purchase or Campaign as Percentage',fontsize=16,fontweight=""bold"",pad=12) 

fig.set_xlabel('\nPercentage',fontsize=14,fontweight=""bold"")
fig = plt.figure(figsize = (18, 18))
plt.show()

Hello friends, I'm trying to plot the above three graphs on streamlit (pie and bar graphs), but the pie graph and the y=count_of_purchase/campaign graph are blank. I tried this code for streamlit: st.pyplot(fig) How can I plot these truly on streamlit? What is the problem, I don't know? Normally, the code is working on spyder/jupyter but the problem is with the interface. I couldn't show it on streamlit. I am waiting for your helps...
","st.pyplot is expecting a Matplotlib Figure object and That object can be accessed by adding a .figure attribute to df.plot.barh(stacked=True):
import pylab
df_count_percent = (df_count['count_of_purchase/campaign'] / df_count['count_of_purchase/campaign'].sum()) * 100
plot0 = df_count_percent.plot.pie(y='count_of_purchase/campaign', fontsize=17,figsize=(16, 16),autopct='%0.2f%%',pctdistance=1.2,
                                   labeldistance=None,stacked=True)

pylab.ylabel('')
pylab.xlabel('')

plt.title('Count of Purchase or Campaign as Percentage (Pie)', fontsize=19,fontweight=""bold"",pad=6)

plt.legend(loc='upper right', fontsize=9,prop={'size': 17}, bbox_to_anchor=(2,1))
  
plot0=plot0.figure
st.pyplot(plot0)

",matplotlib
how to cache a plot in streamlit,"I have built a dashboard in streamlit where you can select a client_ID and have SHAP plots displayed (Waterfall and Force plot) to interpret the prediction of credit default for this client.
I also want to display a SHAP summary plot with the whole train dataset. The later does not change every time you make a new prediction, and takes a lot of time to plot, so I want to cache it. I guess the best approach would be to use st.cache but I have not been able to make it.
Here below is the code I have unsuccessfully tried in main.py:
I first define the function of which I want to cache the output (fig), then I execute the output in st.pyplot. It works without the st.cache decorator, but as soon as I add it and rerun the app, the function summary_plot_all runs indefinitely
IN:
@st.cache    
def summary_plot_all():
    fig, axes = plt.subplots(nrows=1, ncols=1)
    shap.summary_plot(shapvs[1], prep_train.iloc[:, :-1].values, 
    prep_train.columns, max_display=50)
    return fig
    
st.pyplot(summary_plot_all())

OUT (displayed in streamlit app)

Running summary_plot_all().

Does anyone know what's wrong or a better way of caching a plot in streamlit ?
version of packages:
streamlit==0.84.1, 
matplotlib==3.4.2, 
shap==0.39.0

","Try
import matplotlib

@st.cache(hash_funcs={matplotlib.figure.Figure: lambda _: None})
def summary_plot_all():
    fig, axes = plt.subplots(nrows=1, ncols=1)
    shap.summary_plot(shapvs[1], prep_train.iloc[:, :-1].values, 
    prep_train.columns, max_display=50)
    return fig

Check this streamlit github issue
",matplotlib
how to convert a numpy array to pil image applying matplotlib colormap,"I want to take a NumPy 2D array which represents a grayscale image, and convert it to an RGB PIL image while applying some of the matplotlib colormaps.
I can get a reasonable PNG output by using the pyplot.figure.figimage command:
dpi = 100.0
w, h = myarray.shape[1]/dpi, myarray.shape[0]/dpi
fig = plt.figure(figsize=(w,h), dpi=dpi)
fig.figimage(sub, cmap=cm.gist_earth)
plt.savefig('out.png')

Although I could adapt this to get what I want (probably using StringIO do get the PIL image), I wonder if there is not a simpler way to do that, since it seems to be a very natural problem of image visualization. Let's say, something like this:
colored_PIL_image = magic_function(array, cmap)

","Quite a busy one-liner, but here it is:

First ensure your NumPy array, myarray, is normalised with the max value at 1.0.
Apply the colormap directly to myarray.
Rescale to the 0-255 range.
Convert to integers, using np.uint8().
Use Image.fromarray().

And you're done:
from PIL import Image
from matplotlib import cm
im = Image.fromarray(np.uint8(cm.gist_earth(myarray)*255))

with plt.savefig():

with im.save():

",matplotlib
seaborn can i add a quotsecond huequot or similar within a stripplot with dodgetrue,"Let's say I have a plot that looks like so:
import numpy as np
df = sns.load_dataset('iris')
dfm = pd.melt(df, id_vars=[""species""])
dfm = dfm.query('variable in [""sepal_length"", ""sepal_width""]')
sns.stripplot(data=dfm, x=""species"", y=""value"", hue=""variable"", dodge=True)
plt.legend(bbox_to_anchor=(1.05, 1), loc=2)


Let's also say there's another column with important info in my data, such as ""potency"".
dfm['potency'] = np.random.randint(1, 6, dfm.shape[0])
I would like to highlight the potency corresponding to each point in my plot with darkening colors (high potency -> darker). Is this possible?
I know that hue='potency' would do just this but then I could not use dodge to separate the data into sepal_width and and sepal_length chunks.
","You could overlay multiple stripplots with different alphas:
start, end = dfm['potency'].agg(['min', 'max'])
for k, v in dfm.groupby('potency'):
    sns.stripplot(data=v.assign(variable=v['variable']+f' / potency={k}'),
                  x=""species"", y=""value"", hue=""variable"", dodge=True,
                  alpha=(k-start+1)/(end-start+1)
                 )
plt.legend(bbox_to_anchor=(1.05, 1), loc=2)

Output:

A variant with multiple colors in a paired palette:
pal = sns.color_palette(""Paired"")

for i, (k, v) in enumerate(dfm.groupby('potency')):
    sns.stripplot(data=v.assign(variable=v['variable']+f' / potency={k}'),
                  x=""species"", y=""value"", hue=""variable"", dodge=True,
                  palette=pal[2*i:2*(i+1)]
                 )
plt.legend(bbox_to_anchor=(1.05, 1), loc=2)

Output:

",matplotlib
axes class  set explicitly size widthheight of axes in given units,"I want to to create a figure using matplotlib where I can explicitly specify the size of the axes, i.e. I want to set the width and height of the axes bbox.
I have looked around all over and I cannot find a solution for this. What I typically find is how to adjust the size of the complete Figure (including ticks and labels), for example using fig, ax = plt.subplots(figsize=(w, h))
This is very important for me as I want to have a 1:1 scale of the axes, i.e. 1 unit in paper is equal to 1 unit in reality. For example, if xrange is 0 to 10 with major tick = 1 and x axis is 10cm, then 1 major tick = 1cm. I will save this figure as pdf to import it to a latex document.
This question brought up a similar topic but the answer does not solve my problem (using plt.gca().set_aspect('equal', adjustable='box') code)
From this other question I see that it is possible to get the axes size, but not how to modify them explicitly.
Any ideas how I can set the axes box size and not just the figure size. The figure size should adapt to the axes size.
Thanks!
For those familiar with pgfplots in latex, it will like to have something similar to the scale only axis option (see here for example).
","The axes size is determined by the figure size and the figure spacings, which can be set using figure.subplots_adjust().  In reverse this means that you can set the axes size by setting the figure size taking into acount the figure spacings:
import matplotlib.pyplot as plt

def set_size(w,h, ax=None):
    """""" w, h: width, height in inches """"""
    if not ax: ax=plt.gca()
    l = ax.figure.subplotpars.left
    r = ax.figure.subplotpars.right
    t = ax.figure.subplotpars.top
    b = ax.figure.subplotpars.bottom
    figw = float(w)/(r-l)
    figh = float(h)/(t-b)
    ax.figure.set_size_inches(figw, figh)
          
fig, ax=plt.subplots()

ax.plot([1,3,2])

set_size(5,5)

plt.show()

",matplotlib
plottable customize bars,"Given this dataframe:
import pandas as pd    
df = pd.DataFrame({'Grade':[2,3,4,5], 'start':[0,0,20,10], 'end':[100,90,80,100]})
    
       Grade  start  end
    0      2      0  100
    1      3      0   90
    2      4     20   80
    3      5     10  100

I would like to generate a table with it via plottable library with 2 columns: Grade and a column called ""Progress"" with bars (though a green arrow would be amazing) that starts at 'start' and ends at 'end' where the x-axis starts at 0 and ends at 100. Both 'start' and 'end should be annotated.
Here's an example of what I'm looking for in that column:

If ""Start"" starts beyond 0, then the bar (or arrow) should start at that relativel position on the ""x-axis"". The annotations should alwasy be on the left and right sides fo the bars (arrows), as in the example above.
From the ""docs"", I did find this example for bar charts:
ColumnDefinition(
            ""D"",
            width=1.25,
            plot_fn=bar,
            plot_kw={
                ""cmap"": cmap,
                ""plot_bg_bar"": True,
                ""annotate"": True,
                ""height"": 0.5,
                ""lw"": 0.5,
                ""formatter"": decimal_to_percent,
                ""xlim"":[low, high],
            },
        ),

...but I cannot find any reference to control the annotations in terms of labels and positions. I found that I can add ""xlim"" to the plot_kws but now I need to get start and end from the passed dataframe to determine those values per row.
","
import matplotlib.pyplot as plt

grades = '2 3 4 5'.split()
starts = [0,0,20,10]
ends = [100,90,80,100]

fig, ax= plt.subplots(layout='constrained')
ax.axis('off')
t = plt.table(
  cellText=list(zip(grades, ['']*4)),
  colLabels=['Grade', 'Progress'],
  colWidths=[0.2, 0.3],
  cellLoc='center',
  loc='center',
  edges='',
  )

# apparently this is necessary to have the cells initialized
fig.canvas.draw()

trax = ax.transAxes
for row in range(1, 5):
    cell = t[row, 1]
    plt.plot((c._x0, c._x0+c._width), (c._y0+0.01, c._y0+0.01),
             lw=0.3, color='black', transform=trax)
    startx = c._x0+c._width*starts[row-1]/100
    dx = c._width*(ends[row-1]-starts[row-1])/100
    plt.arrow(startx, c._y0+0.0225, dx, 0.0,
    color='forestgreen', width=0.006,
    length_includes_head=True,  transform=trax)
plt.show()

",matplotlib
how to do a scatter plot with empty circles in python,"In Python, with Matplotlib, how can a scatter plot with empty circles be plotted?  The goal is to draw empty circles around some of the colored disks already plotted by scatter(), so as to highlight them, ideally without having to redraw the colored circles.
I tried facecolors=None, to no avail.
","From the documentation for scatter:
Optional kwargs control the Collection properties; in particular:

    edgecolors:
        The string ‘none’ to plot faces with no outlines
    facecolors:
        The string ‘none’ to plot unfilled outlines

Try the following:
import matplotlib.pyplot as plt 
import numpy as np 

x = np.random.randn(60) 
y = np.random.randn(60)

plt.scatter(x, y, s=80, facecolors='none', edgecolors='r')
plt.show()


Note: For other types of plots see this post on the use of markeredgecolor and markerfacecolor.
",matplotlib
flyer color in seaborn boxplot with palette,"I have a seaborn boxplot with a categorical variable to use for hue and a dictionary with a color for each category given as the palette argument. MWE:
import seaborn as sns
from matplotlib import pyplot as plt

cdict = {""First"" : ""gold"",
         ""Second"": ""blue"",
         ""Third"" : ""red""}
df = sns.load_dataset(""titanic"")[[""sex"",""fare"",""class""]]
fig, ax = plt.subplots()
sns.boxplot(data=df, x=""sex"", y=""fare"", hue=""class"",palette=cdict, ax=ax)
plt.show()

Giving:

I would like to have the fliers in the same color as the boxes (face or edge color). My plot is relatively crowded, so without this it is difficult to quickly see which outlier corresponds to which category.
","As you are using the hue parameter I don't think you can pass through the flierprops keyword argument to sns.boxplot. Instead we can iterate over the containers/boxes and dynamically fetch/set the colours based on the boxes that we see
fig, ax = plt.subplots()
sns.boxplot(data=df, x=""sex"", y=""fare"", hue=""class"", palette=cdict, ax=ax)

for container in ax.containers:
    for box in container:
        current_colour = box.box.get_facecolor()
        box.fliers.set_markerfacecolor(current_colour)
        # uncomment to set edge colour
        # box.fliers.set_markeredgecolor(current_colour)

resulting in

",matplotlib
with matplotlib how to align text with axes position independant of tight_layout,"I want to have two text annotations on a Figure, that are aligned with the x positions of the Axes in the FIgure (x0 and x1). I can set it to work correctly normally, but when I call fig.tight_layout the text stays where it was before the layout changes.
Is there a way to have the text positions determined dynamically to where the Axes ends up being?
I know calling fig.tight_layout before setting the text works, but I wonder how to make the text's position dependant of the axes positions all the time.
Here is a sample code:
import matplotlib.pyplot as plt
fig, ax = plt.subplots()
ax_bbox = ax.get_position()

fig.text(ax_bbox.x0, 0.01, ""left text"", ha=""left"")
fig.text(ax_bbox.x1, 0.01, ""right text"", ha=""right"")

fig.tight_layout()  # this makes the axes position to change

plt.show()

The figure without fig.tight_layout
without tight layout
The figure with fig.tight_layout
with tight layout
","It would be by giving the x and y coordinates related to the ax coords, specifying it by giving its transform as an argument:
import matplotlib.pyplot as plt

fig, ax = plt.subplots()

fig.text(0, -0.06, ""left text"", ha=""left"", va=""top"",  transform=ax.transAxes)
fig.text(1, -0.06, ""right text"", ha=""right"", va=""top"", transform=ax.transAxes)

fig.tight_layout()

plt.show()


",matplotlib
use an irregularly spaced noncategorical axis on a categorical plot in seaborn,"I'm creating a violin plot in Seaborn, which by default, assumes that the x-axis is categorical, and therefore evenly spaces the data, rather than scaling it by a value. I would like the spacing between the individual violins to be defined by values associated with each violin, rather than just spacing them evenly. I have read a number of things suggesting that I can overwrite the defaults with matplotlib commands, but can't get anything to work. 
sns.set(palette='muted', color_codes=True)
f, axes = plt.subplots(2, 2, figsize=(8,5))
sns.violinplot(x = lsdf['6MO_CUM_MBO/1000FT'], y = lsdf.RELATIVE_DEPTH,
data=lsdf, palette=""Blues"", ax=axes[0,0])

I think the key issue here, is I'm not exactly sure what Seaborn's defaults are controling. Do I need to modify the axes object created by subplots? or the ax=[0,0] object?
The only answer I found to a similar question had a solution which was just, ""do it in matplotlib,"" but I need the plots available in seaborn. Thanks for your help.

","Well, I eventually solved this, sort of... I caved and used pure matplotlib, gave up on Seaborn. The matplotlib violinplot takes an array-like positions argument which when passed numeric values, auto-scales the x-axis and behaves exactly as plt.plot or any plot where marker position and axis range are derived from the input data. I still used seaborn.set() to get the nice Seaborn aesthetic. There is matplotlib violin plot customization documentation which has nice examples of how to edit the details of the violins which allowed me to customize the violins and mimic all parts of the Seaborn violin plot.
",matplotlib
pandas plot line with different line styles,"I am plotting several lines on the same plot, using the ggplot style. With this style, the lines become all solid lines. So the visibility is not good. How can I change each line to have different styles, e.g., one with dashed lines, or something?
import pandas as pd
import matplotlib.pyplot as plt
plt.style.use('ggplot')

fig,ax = plt.subplots(figsize=(15,5))
ax.set_title('Loss curve', fontsize=15)
ax.set_ylabel('Loss')
ax.set_xlabel('Epoch')
df1.plot.line(ax=ax,x='epoch',y=[""train_loss""])
df2.plot.line(ax=ax,x='epoch',y=[""train_loss""])
plt.show()


","You can use linestyle to change each line with different styles.
Here is an example :
import pandas as pd
import matplotlib.pyplot as plt
plt.style.use('ggplot')

fig,ax = plt.subplots(figsize=(15,5))
ax.set_title('Loss curve', fontsize=15)
ax.set_ylabel('Loss')
ax.set_xlabel('Epoch')

df1 = pd.DataFrame({'epoch' : [10,20,30,40,50,60],
                    'train_loss' : [6,5,4,3,2,1]})

df2 = pd.DataFrame({'epoch' : [10,20,30,40,50,60],
                    'train_loss' : [6.5,5.5,4.5,3.5,2.5,1.5]})

df1.plot.line(ax=ax,x='epoch',y=[""train_loss""], 
              linewidth= 1.5, linestyle='-.')
df2.plot.line(ax=ax,x='epoch',y=[""train_loss""], linewidth= 1.5,
              linestyle='-')
plt.show()

The above code will show the graph as following :

At plt.plot.line(), you can check more styles as solid, dash, dash-dot, dotted, and etc.
",matplotlib
numerically stable softmax,"Is there a numerically stable way to compute softmax function below?
I am getting values that becomes Nans in Neural network code.
np.exp(x)/np.sum(np.exp(y))

","The softmax exp(x)/sum(exp(x)) is actually numerically well-behaved. It has only positive terms, so we needn't worry about loss of significance, and the denominator is at least as large as the numerator, so the result is guaranteed to fall between 0 and 1.
The only accident that might happen is over- or under-flow in the exponentials. Overflow of a single or underflow of all elements of x will render the output more or less useless.
But it is easy to guard against that by using the identity softmax(x) = softmax(x + c) which holds for any scalar c: Subtracting max(x) from x leaves a vector that has only non-positive entries, ruling out overflow and at least one element that is zero ruling out a vanishing denominator (underflow in some but not all entries is harmless).
Footnote: theoretically, catastrophic accidents in the sum are possible, but you'd need a ridiculous number of terms. For example, even using 16 bit floats which can only resolve 3 decimals---compared to 15 decimals of a ""normal"" 64 bit float---we'd need between 2^1431 (~6 x 10^431) and 2^1432 to get a sum that is off by a factor of two.
",numpy
apply function only on slice of array under jit,"I am using JAX, and I want to perform an operation like
@jax.jit
def fun(x, index):
    x[:index] = other_fun(x[:index])
    return x

This cannot be performed under jit. Is there a way of doing this with jax.ops or jax.lax?
I thought of using jax.ops.index_update(x, idx, y) but I cannot find a way of computing y without incurring in the same problem again.
","The previous answer by @rvinas using dynamic_slice works well if your index is static, but you can also accomplish this with a dynamic index using jnp.where. For example:
import jax
import jax.numpy as jnp

def other_fun(x):
    return x + 1

@jax.jit
def fun(x, index):
  mask = jnp.arange(x.shape[0]) < index
  return jnp.where(mask, other_fun(x), x)

x = jnp.arange(5)
print(fun(x, 3))
# [1 2 3 3 4]

",numpy
sort columns of numpy unitary matrix such that highest element of each column is on the diagonal,"Take a unitary matrix U. I want to swap the columns such that the largest element of each column (in absolute value) is on the diagonal (modulo ties). What is the best way to do this in numpy?
","Maybe you can try the example below (but as I said in the comment, you cannot always guarantee all of the max absolute values by columns could be arranged along the diagonal).
import numpy as np
np.random.seed(0)

# Create a unitary matrix U
M = np.random.rand(3,3)
U, _ = np.linalg.qr(M)

# Find the indices of the largest element in each column (in absolute value)
largest_indices = np.argmax(np.abs(U), axis=0)

# Sort the indices based on the values in descending order
sorted_indices = np.argsort(largest_indices)

# Reorder the columns of U based on the sorted indices
U_out = U[:, sorted_indices]

print(f'U = \n {U}')
print(f'U_out = \n {U_out}')

and you will see
U = 
 [[-0.617629    0.02482174 -0.7860778 ]
 [-0.61320586 -0.64104698  0.46155968]
 [-0.49245609  0.76710015  0.41115004]]
U_out = 
 [[-0.617629   -0.7860778   0.02482174]
 [-0.61320586  0.46155968 -0.64104698]
 [-0.49245609  0.41115004  0.76710015]]

and you could verify the unitary property of U by
print(f'U@U.T = \n {U@U.T}')

which shows
U@U.T = 
 [[1.00000000e+00 1.13938611e-16 1.20208703e-16]
 [1.13938611e-16 1.00000000e+00 7.53666611e-17]
 [1.20208703e-16 7.53666611e-17 1.00000000e+00]]

",numpy
how to solve the pytorch runtimeerror numpy is not available without upgrading numpy to the latest version because of other dependencies,"I am running a simple CNN using Pytorch for some audio classification on my Raspberry Pi 4 on Python 3.9.2 (64-bit). For the audio manipulation needed I am using librosa. librosa depends on the numba package which is only compatible with numpy version <= 1.20.
When running my code, the line
spect_tensor = torch.from_numpy(spect).double()

throws the RuntimeError:
RuntimeError: Numpy is not available

Searching the internet for solutions I found upgrading Numpy to the latest version to resolve that specific error, but throwing another error, because Numba only works with Numpy <= 1.20.
Is there a solution to this problem which does not include searching for an alternative to using librosa?
","Just wanted to give an update on my situation. I downgraded torch to version 0.9.1 which solved the original issue. Now OpenBLAS is throwing a warning because of an open MPLoop. But for now my code is up and running.
",numpy
how to accelerate the crosscorrelation computation of two 2d matrices in python,"I am using Python to compute the cross-correlation of two 2D matrices, and I have implemented three different methods. Below is my experimental code along with their execution times:
import numpy as np  
from scipy.signal import fftconvolve  

# Randomly generate 2D matrices  
a = np.random.randn(64, 4200)  
b = np.random.randn(64, 4200)  

# Using np.correlate  
def test_1(a, b):  
    return np.array([np.correlate(s_row, t_row, mode=""full"") for s_row, t_row in zip(a, b)])[:, len(a[0]) - 1:].sum(axis=0)  

# Using scipy.signal.fftconvolve  
def test_2(a, b):  
    return np.array([fftconvolve(s_row, t_row[::-1], mode=""full"") for s_row, t_row in zip(a, b)])[:, len(a[0]) - 1:].sum(axis=0)  

# Using scipy.signal.fftconvolve but for 2D  
def test_3(a, b):  
    return fftconvolve(a, np.fliplr(b), mode=""full"")[:, len(a[0]) - 1:].sum(axis=0)  

# Performance testing  
for i in range(100):  
    x = test_1(a, b)  # 19.8 seconds  
for i in range(100):  
    x = test_2(a, b)  # 1.1 seconds  
for i in range(100):  
    x = test_3(a, b)  # 3.8 seconds

Even if I reverse b in advanced, the cost of time remains unaltered.
My computer configuration is:
Operating System: Windows 11
CPU: i7-12700F
Memory: 16GB
I have already used from concurrent.futures import ThreadPoolExecutor, as_completed to speed up the multiple loops. I am looking for further ways to accelerate this computation.
Are there any other algorithm optimization suggestions that could help reduce the computation time? Thank you.
","I do not think you can find a much faster implementation than that using a sequential code on CPU. This answer explains why. It also provide a slightly-faster experimental FFTW-based implementation which is up to 40% faster, and alternative possibly-faster solutions.

Profiling and analysis
The FFT library used by Scipy, which is pypocketfft is surprisingly pretty fast (faster than the FFTW library in this case). test_2 spend most of its time in this native library so using a native code or even vectorization would not make the code much faster. Indeed, 60~65% of the time is spent in the function pocketfft::detail::rfftp::exec, 15~20% in Python overheads, ~10% in pocketfft::detail::general_r2c/pocketfft::detail::general_c2r, and ~10% in other operations like copies and the main multiplication. As a result, writing your own specialized native module directly using pypocketfft would make this computation only 15~20% faster at best. I do not think it is worth the effort.
Low-level profiling results show that the computing function test_2 is clearly compute bound. The same thing is true for the provided FFTW functions in this answer. That being said, the library pypocketfft only use scalar instructions. Alternative libraries like the FFTW can theoretically benefit from SIMD units.
However, in practice, this is far from being easy to write an implementation both using an efficient algorithm and SIMD instructions. The Intel MKL (alternative library) certainly does that. The FFTW only uses SIMD instructions in the complex-to-complex FFTs. This is why the function test_fftw_2 (provided in at the end of this answer) is slower than the other FFTW-based functions (also provided).

Faster implementation with the FFTW
I tried the FFTW (fully vectorized code), but it is significantly slower in this case (despite the name of the library meaning ""Fastest Fourier Transform in the West""). This is because 4200*2-1 has 2 big prime factors and the FFTW underlying algorithm (AFAIK Cooley-Tukey) is inefficient for length having such big prime factors (it is much better for power of 2 length). In fact, the computation is about 8 times faster for a length of 8192 instead of 8399 (your use-case). Note I tried two versions: one in-place with only complex-to-complex FFTs and one out-of-place with real-to-complex/complex-to-real FFTs. The later was supposed to be faster but it is slower in practice... Maybe I missed something critical in my implementations.
We can cheat by using bigger array for the FFT, that is a kind of padding. Indeed, using an array size which is divisible by a big power of two can mitigate the aforementioned performance issue (related to prime factors). For example, here we can use an array size of 1024*9=9216 (the smallest number divisible by 1024 bigger than n*2-1). While, we could use a smaller power-of-two base to be as close as possible to n*2-1 (increasing the efficiency by computing useful numbers), this also cause the algorithm the be less efficient (due to a smaller power-of-two factor). In this case, the FFTW implementation is about as fast as the Scipy code with a FFTW_MEASURE plan (this results in a 7% faster code which is a tiny performance boost). With the FFTW_PATIENT, it is actually faster by about 25%! The thing is FFTW_PATIENT plans can be pretty slow to make. The initialization time can only be amortized with at least several thousands of iterations and not just 100. Note that pre-allocating all arrays and plan once before the loop results in a few percent faster execution.
I also found out that the FFTW is sub-optimal and computing chunks of line is ~10% faster than computing the whole array in one row. As a result, the final implementation test_fftw_4 is about 40% faster than test_2 (still without considering the first execution slowed down due to the planning time). This implementation is still a bit faster than test_2 without expensive planning (e.g. FFTW_MEASURE). This last implementation efficiently use a CPU core: it massively use the SIMD units, it is pretty cache-friendly and spend nearly all its time in the native FFTW library. It can also be easily parallelized on few cores (regarding the chunk size).
Here is the different FFTW functions I tested:
import pyfftw
import numpy as np

# Initial FFTW implementation (slower than Scipy/pypocketfft)
def test_fftw_1(a, b):
    m = a.shape[0]
    n = a.shape[1]
    assert b.shape[1] == n

    # Input/output FFT temporary arrays
    a2 = pyfftw.empty_aligned((m, 2*n-1), dtype='complex128')
    b2 = pyfftw.empty_aligned((m, 2*n-1), dtype='complex128')
    r2 = pyfftw.empty_aligned((m, 2*n-1), dtype='complex128')
    a2[:,:n] = a
    a2[:,n:] = 0
    b2[:,:n] = b[:,::-1]
    b2[:,n:] = 0

    # See FFTW_ESTIMATE, FFTW_MEASURE or FFTW_PATIENT for varying 
    # performance at the expense of a longuer plannification time
    axes = (1,)
    flags = ['FFTW_MEASURE']
    a2_fft_plan = pyfftw.FFTW(a2, a2, axes, 'FFTW_FORWARD', flags)
    b2_fft_plan = pyfftw.FFTW(b2, b2, axes, 'FFTW_FORWARD', flags)
    r2_ifft_plan = pyfftw.FFTW(r2, r2, axes, 'FFTW_BACKWARD', flags)

    # Actual execution of the plans
    a2_fft_plan()
    b2_fft_plan()
    np.multiply(a2, b2, out=r2)
    r2_ifft_plan()

    return r2.real[:, n-1:].sum(axis=0)

# Supposed to be faster, but slower in practice 
# (because it seems not to use SIMD instructions in practice)
def test_fftw_2(a, b):
    m = a.shape[0]
    n = a.shape[1]
    assert b.shape[1] == n

    # Input FFT temporary arrays
    a2 = pyfftw.empty_aligned((m, 2*n-1), dtype='float64')
    b2 = pyfftw.empty_aligned((m, 2*n-1), dtype='float64')
    a2[:,:n] = a
    a2[:,n:] = 0
    b2[:,:n] = b[:,::-1]
    b2[:,n:] = 0

    # Temporary and output arrays
    a2_fft = pyfftw.empty_aligned((m, n), dtype='complex128')
    b2_fft = pyfftw.empty_aligned((m, n), dtype='complex128')
    r2_fft = pyfftw.empty_aligned((m, n), dtype='complex128')
    r2 = pyfftw.empty_aligned((m, 2*n-1), dtype='float64')

    # Same thing for planning
    axes = (1,)
    flags = ['FFTW_MEASURE']
    a2_fft_plan = pyfftw.FFTW(a2, a2_fft, axes, 'FFTW_FORWARD', flags)
    b2_fft_plan = pyfftw.FFTW(b2, b2_fft, axes, 'FFTW_FORWARD', flags)
    r2_ifft_plan = pyfftw.FFTW(r2_fft, r2, axes, 'FFTW_BACKWARD', flags)

    a2_fft_plan()
    b2_fft_plan()
    np.multiply(a2_fft, b2_fft, out=r2_fft)
    r2_ifft_plan()

    return r2.real[:, n-1:].sum(axis=0)

# Pretty-fast FFTW-based implementation (faster than Scipy/pypocketfft without considering the planning time)
# However, the first call is pretty slow (e.g. dozens of seconds)
def test_fftw_3(a, b):
    m = a.shape[0]
    n = a.shape[1]
    assert b.shape[1] == n

    # Input/output FFT temporary arrays
    block_size = 1024
    size = (2*n-1 + block_size-1) // block_size * block_size
    a2 = pyfftw.empty_aligned((m, size), dtype='complex128')
    b2 = pyfftw.empty_aligned((m, size), dtype='complex128')
    r2 = pyfftw.empty_aligned((m, size), dtype='complex128')
    a2[:,:n] = a
    a2[:,n:] = 0
    b2[:,:n] = b[:,::-1]
    b2[:,n:] = 0

    axes = (1,)
    flags = ['FFTW_PATIENT']
    a2_fft_plan = pyfftw.FFTW(a2, a2, axes, 'FFTW_FORWARD', flags)
    b2_fft_plan = pyfftw.FFTW(b2, b2, axes, 'FFTW_FORWARD', flags)
    r2_ifft_plan = pyfftw.FFTW(r2, r2, axes, 'FFTW_BACKWARD', flags)

    # Actual execution of the plans
    a2_fft_plan()
    b2_fft_plan()
    np.multiply(a2, b2, out=r2)
    r2_ifft_plan()

    diff = size - (2*n-1)
    return r2.real[:, -diff-n:-diff].sum(axis=0)

# Fastest FFTW-based implementation
def test_fftw_4(a, b):
    return sum([test_fftw_3(a[i*8:(i+1)*8], b[i*8:(i+1)*8]) for i in range(8)])


Alternative solutions
The Intel MKL is certainly faster than pypocketfft, but this certainly requires a native C/C++ code wrapped to be callable from Python (not so simple to do). I think this is the best option in sequential on CPU.
An alternative solution is to use GPUs. For example, CuPy supports FFW computations though this requires Nvidia GPUs (due to CUDA). There is also the quite-new portable pyvkfft module based on VkFFT (supported by AMD). Be aware that double-precision computations are only fast on (expensive) server-side GPUs, not client-side ones (i.e. personal computers).
Note using single-precision should results in faster execution timings on both CPU and GPU (especially on client-side GPUs).

References
You can find a list of interesting C++ library and a benchmark here.
",numpy
what is rtol for in numpy39s allclose function,"In numpy.allclose() there are two tolerance factors used to determine if two arrays are close enough to count as the same. There is the relative tolerance rtol and absolute tolerance atol. From the docs:
numpy.allclose(a, b, rtol=1e-05, atol=1e-08)

Also from the docs:

If the following equation is element-wise True, then allclose returns True.
absolute(a - b) <= (atol + rtol * absolute(b))

Mathematically I understand this, but I am confused about the point of rtol. Why not just use a single tolerance value tol, and if |a-b| < tol, then return False? Obviously, following the above equation, I could just do this manually by setting rtol to zero, thereby making everything symmetric. What is the point of the symmetry-breaking rtol factor?
Related question
How allclose() works?
","The confusing part is that the equation shows both parameters being used at the same time. Look at it like this instead:

Usecase 1: absolute tolerance (atol): absolute(a - b) <= atol
Usecase 2: relative tolerance (rtol): absolute(a - b) <= rtol * absolute(b)

An alternative way to implement both with a single tolerance parameter would be to add a flag that determines if the toerance is relative or absolute. Separating the use-cases like that breaks down in the usecase where array values can be both large and zero. If only one array can have zeros, make that one a and use the asymmetrical equation to your benefit without atol. If either one can have zeros, simply set rtol to some acceptable value for large elements, and set atol to the value you want to kick in for zeros.
You generally want to use rtol: since the precision of numbers and calculations is very much finite, larger numbers will almost always be less precise than smaller ones, and the difference scales linearly (again, in general). The only time you use atol is for numbers that are so close to zero that rounding errors are liable to be larger than the number itself.
Another way to look at it is atol compares fixed decimal places, while rtol compares significant figures.
A caveat is that both of the default values of the function are non-zero: rtol=1e-05 and atol=1e-08. That means that if you want to use only one or the other of them, you must explicitly set the other to zero.
",numpy
how to filter a lot of colors out of an image the numpy way,"I have an image, from which I would like to filter some colors: if a given pixel is in my set of colors, I would like to replace it by a white pixel. With an image called original of shape (height, width, 3) and a color like ar=np.array([117,30,41]), the following works fine:
mask = (original == ar).all(axis=2)
original[mask] = [255, 255, 255]

The trouble is, the set of colors I want to exclude is quite big (~37000). Using the previous code in a loop (for ar in colors) works again, but is quite slow. Is there a faster way to do it?
For now, my set of colors is in a numpy array of shape (37000, 3). I'm sure that all of these colors are present on my image, and I'm also sure that they are unique.
","A simple way to solve this would be a look up table. A look up table with a boolean for every color would only cost 256 * 256 * 256 * 1 bytes = 16 MiB, and would enable you to determine if a color is in your list of disallowed colors in constant time.
Here is an example. This code generates an image with multiple colors. It filters out some of those colors using two approaches. The first approach is the one you describe in the question. The second approach is the lookup table.
import numpy as np


# Only used for generating image. Skip this if you already have an image.
image_colors = np.array([
    (100, 100, 100),
    (200, 200, 200),
    (255, 255, 0),
    (255, 0, 0),
])

image_colors_to_remove = [
    (255, 255, 0),
    (255, 0, 0),
]

# Generate image
resolution = (800, 600)
np.random.seed(42)
image = np.random.randint(0, len(image_colors), size=resolution)
image = np.array(image_colors)[image].astype(np.uint8)
# image = np.random.randint(0, 256, size=(*resolution, 3))

# Slow approach
def remove_colors_with_for(image, image_colors_to_remove):
    image = image.copy()
    for c in image_colors_to_remove:
        mask = (image == c).all(axis=2)
        image[mask] = [255, 255, 255]
    return image

# Fast approach
def remove_colors_with_lookup(image, image_colors_to_remove):
    image = image.copy()
    colors_remove_lookup = np.zeros((256, 256, 256), dtype=bool)
    image_colors_to_remove = np.array(image_colors_to_remove).T
    colors_remove_lookup[tuple(image_colors_to_remove)] = 1
    image_channel_first = image.transpose(2, 0, 1)
    mask = colors_remove_lookup[tuple(image_channel_first)]
    image[mask] = [255, 255, 255]
    return image


new_image = remove_colors_with_for(image, image_colors_to_remove)
new_image2 = remove_colors_with_lookup(image, image_colors_to_remove)
print(""Same as for loop?"", np.all(new_image2 == new_image))

",numpy
two dimensional fit with python,"I need to fit a function
z(u,v) = C u v^p

That is, I have a two-dimensional data set, and I have to find two parameters, C and p.  Is there something in numpy or scipy that can do this in a straightforward manner?  I took a look at scipy.optimize.leastsq, but it's not clear to me how I would use it here.
","def f(x,u,v,z_data):
  C = x[0]
  p = x[1]
  modelled_z = C*u*v**p
  diffs = modelled_z - z_data
  return diffs.flatten() # it expects a 1D array out. 
       # it doesn't matter that it's conceptually 2D, provided flatten it consistently

result = scipy.optimize.leastsq(f,[1.0,1.0], # initial guess at starting point
                        args = (u,v,z_data) # alternatively you can do this with closure variables in f if you like
                              )
# result is the best fit point

For your specific function you might be able to do it better - for example, for any given value of p there is one best value of C that can be determined by straightforward linear algebra.
",numpy
how to change numpy array into grayscale opencv image,"How can I change numpy array into grayscale opencv image in python?
After some processing I got an array with following atributes: max value is: 0.99999999988,
min value is 8.269656407e-08 and type is: <type 'numpy.ndarray'>. I can show it as an image using cv2.imshow() function, but I can't pass it into cv2.AdaptiveTreshold() function because it has wrong type:
error: (-215) src.type() == CV_8UC1 in function cv::adaptiveThreshold

How can I convert this np.array to correct format? 
","As the assertion states, adaptiveThreshold() requires a single-channeled 8-bit image.
Assuming your floating-point image ranges from 0 to 1, which appears to be the case, you can convert the image by multiplying by 255 and casting to np.uint8:
float_img = np.random.random((4,4))
im = np.array(float_img * 255, dtype = np.uint8)
threshed = cv2.adaptiveThreshold(im, 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 3, 0)

",numpy
cumsum reset at nan,"If I have a pandas.core.series.Series named ts of either 1's or NaN's like this:
3382   NaN
3381   NaN
...
3369   NaN
3368   NaN
...
15     1
10   NaN
11     1
12     1
13     1
9    NaN
8    NaN
7    NaN
6    NaN
3    NaN
4      1
5      1
2    NaN
1    NaN
0    NaN

I would like to calculate cumsum of this serie but it should be reset (set to zero) at the location of the NaNs like below:
3382   0
3381   0
...
3369   0
3368   0
...
15     1
10     0
11     1
12     2
13     3
9      0
8      0
7      0
6      0
3      0
4      1
5      2
2      0
1      0
0      0

Ideally I would like to have a vectorized solution !
I ever see a similar question with Matlab : 
Matlab cumsum reset at NaN?
but I don't know how to translate this line d = diff([0 c(n)]); 
","A simple Numpy translation of your Matlab code is this:
import numpy as np

v = np.array([1., 1., 1., np.nan, 1., 1., 1., 1., np.nan, 1.])
n = np.isnan(v)
a = ~n
c = np.cumsum(a)
d = np.diff(np.concatenate(([0.], c[n])))
v[n] = -d
np.cumsum(v)

Executing this code returns the result array([ 1.,  2.,  3.,  0.,  1.,  2.,  3.,  4.,  0.,  1.]). This solution will only be as valid as the original one, but maybe it will help you come up with something better if it isn't sufficient for your purposes.
",numpy
how to get row numbers of maximum elements in a 2d numpy array,"I have a 2D array a given by:
a = np.array([[2, 3, 1, 9], [0, 5, 4, 7], [2, 4, 6, 8]])

[[2 3 1 9]
 [0 5 4 7]
 [2 4 6 8]]

I would like to get row numbers of maximum elements column-wise, i.e. given by np.amax(a, axis=0), which in my example are [0, 1, 2, 0]. What is the Numpy magic to accomplish it?
","I believe you're looking for np.argmax.
print(np.argmax(a, axis=0))

Result:
[0 1 2 0]

",numpy
convert rgb to class  single integer value,"I have a numpy array from a rgb image (64,64,3) and I need to convert each existing rgb combination to one class, represented by an integer value. So that in the end I have an array (64,64) which contains integer values (0-N). These values represent specific rgb combination from the picture. Of course every rgb combination just gets one value.
In short: Every color is one class, and every pixel has one fitting class-value (0-N) :)
Obviously its not a big problem, I could just go through each pixel, check the rgb values and if they are not in an ""already discovered RGB"" tempList, I add these values and give those rgb values an integer value representing the class, otherwise I search in the tempList for the rgb values and give the integer value I wrote down in the List - or something like that.
But to be honest, I need to do this for a lot of images and I try to get better with python. So I want to know if someone has a more efficent way to do this? I scrolled through the libaries and boards and couldn't find a good approach.
","You can convert three 8 bit integers into a 32bit integer and easily recover the three integer back. The idea is to use bitwise operations, this way each 8 bit represents one of the rgb colors. This way you already know the N = 16777215 (including zero) = 256**3.
The following code can do this:
def rgbtoint32(rgb):
    color = 0
    for c in rgb[::-1]:
        color = (color<<8) + c
        # Do not forget parenthesis.
        # color<< 8 + c is equivalent of color << (8+c)
    return color

def int32torgb(color):
    rgb = []
    for i in range(3):
        rgb.append(color&0xff)
        color = color >> 8
    return rgb

rgb = [32,253,200]
color = rgbtoint32(rgb)
rgb_c = int32torgb(color)

print(rgb)
print(color)
print(rgb_c)

This gives:
[32, 253, 200]
13172000
[32, 253, 200]

Update:
Using ""view"" from numpy, as denoted below by ""Mad Physicist
"", one can efficiently do the above process as
rgb = np.array([[[32,253,200], [210,25,42]]],dtype = np.uint8)
size_d = list(rgb.shape)
size_d[2] = -1

# Converting to 2D int32 array
colorint32 = np.dstack((rgb,np.zeros(rgb.shape[:2], 'uint8'))).view('uint32').squeeze(-1)

# Converting back from the int32 array to RGB space
rgb_c = colorint32.view('uint8').reshape(size_d)[:,:,:3]

# Print results
print(rgb)
print(colorint32)
print(rgb_c)

Which gives
[[[ 32 253 200]
  [210  25  42]]]
[[13172000  2759122]]
[[[ 32 253 200]
  [210  25  42]]]

",numpy
matrix multiplication 2d and 1d,"Example:
import numpy as np

x = np.array([[1., 2., 3.], [4., 5., 6.]])
y = np.ones(3)

np.dot(x , y)

Result:
array([ 6., 15.])

How is this possible when I have a matrix of (2x3) by (1x3)? It should be a 3x1 matrix.
","y is not a 1x3 matrix but a 1-D vector (check y.size if in doubt). If you have a look at the documentation you can find

If a is an N-D array and b is a 1-D array, it is a sum product over the last axis of a and b.

That's why it is returning a 1-D vector of two elements.
",numpy
typeerror unhashable type 39series3939numpyndarray39,"I'm trying to prepare data for visualization with seaborn.
So I need to get number of different type of sessions for a multiple line chart.
With 
session_cnt = df.groupby(df['EVENT_DATETIME'].dt.date, df['CUSTOMER_ID']).agg(session_count=('SESSION_ID', 'nunique'), app_session_cnt=('APP_SESSION_ID', 'nunique')).reset_index()
I got
TypeError: unhashable type 'series'

So I did that:
session_cnt = df.groupby(df['EVENT_DATETIME'].dt.date, df['CUSTOMER_ID'].values).agg(session_count=('SESSION_ID', 'nunique'), app_session_cnt=('APP_SESSION_ID', 'nunique')).reset_index()
But got
TypeError: unhashable type 'numpy.ndarray'

I'd like to understand which column to check when using groupby
and getting TypeError, because now I'm only guessing.
Maybe I need to read a good article on that error.
","Groupby takes a ""mapping, function, label, pd.Grouper or list of such"" :
session_cnt = (df.groupby([df['EVENT_DATETIME'].dt.date, df['CUSTOMER_ID']]) 
                 .agg(session_count=('SESSION_ID', 'nunique'), 
                      app_session_cnt=('APP_SESSION_ID', 'nunique'))
                 .reset_index()
)

",numpy
how to explain pandas higher performances compared to numpy with 500k rows,"In some sources, I found that pandas works faster than numpy with 500k rows or more. Can someone explain this to me?

Pandas have a better performance when the number of rows is 500K or more.
— Difference between Pandas VS NumPy - GeeksforGeeks


If the number of rows of the dataset is more than five hundred thousand (500K), then the performance of Pandas is better than NumPy.
— Pandas Vs NumPy: What’s The Difference? [2023] - InterviewBit


[...] Pandas generally performs better than numpy for 500K rows or more [...]
— PandaPy - firmai on GitHub

I tried to find where this fact came from. I couldn't figure it out and couldn't see any information from the documentation.
","Adding to the discussion, here are those tests in the linked page reproduced with some minor changes to see if anything has changed since that original post was made almost 8 years ago and python and many of its libraries have upgraded quite a bit since then. According to python.org the newest version of python available at the time of his post was 3.6 .
Here is the source code, copied from the linked page and updated to be runnable as posted here, plus a few minor changes for convenience.
import pandas
import matplotlib.pyplot as plt
import seaborn
import numpy

import sys
import time

NUMBER_OF_ITERATIONS = 10
FIGURE_NUMBER = 0

def bench_sub(mode1_inputs: list, mode1_statement: str, mode2_inputs: list, mode2_statement: str) -> tuple[bool, list[float], list[float]]:

    mode1_results = []
    mode1_times = []
    mode2_results = []
    mode2_times = []

    for inputs, statementi, results, times in (
            (mode1_inputs, mode1_statement, mode1_results, mode1_times),
            (mode2_inputs, mode2_statement, mode2_results, mode2_times)
    ):
        for inputi in inputs:
            ast = compile(statementi, '<string>', 'exec')
            ast_locals = {'data': inputi}
            start_time = time.perf_counter_ns()
            for _ in range(NUMBER_OF_ITERATIONS):
                exec(ast, locals=ast_locals)
            end_time = time.perf_counter_ns()

            results.append(ast_locals['res'])
            times.append((end_time - start_time) / 10 ** 9 / NUMBER_OF_ITERATIONS)

    passing = True
    for results1, results2 in zip(mode1_results, mode2_results):
        if not passing:
            break
        try:
            if type(results1) in [pandas.Series, numpy.ndarray] and type(results2) in [pandas.Series, numpy.ndarray]:
                if type(results1[0]) is str:
                    isclose = set(results1) == set(results2)
                else:
                    isclose = numpy.isclose(results1, results2).all()
            else:
                isclose = numpy.isclose(results1, results2)
            if not isclose:
                passing = False
                break
        except (ValueError, TypeError):
            print(type(results1))
            print(results1)
            print(type(results2))
            print(results2)
            raise
    return passing, mode1_times, mode2_times

def bench_sub_plot(mode1_inputs: list, mode1_statement: str, mode2_inputs: list, mode2_statement: str, title: str, label1: str, label2: str, save_fig: bool = True) -> tuple[bool, list[float], list[float]]:
    passing, mode1_times, mode2_times = bench_sub(mode1_inputs, mode1_statement, mode2_inputs, mode2_statement)

    fig, ax = plt.subplots(2, dpi=100, figsize=(8, 6))
    mode1_x = [len(x) for x in mode1_inputs]
    mode2_x = [len(x) for x in mode2_inputs]

    ax[0].plot(mode1_x, mode1_times, marker='o', markerfacecolor='none', label=label1)
    ax[0].plot(mode2_x, mode2_times, marker='^', markerfacecolor='none', label=label2)
    ax[0].set_xscale('log')
    ax[0].set_yscale('log')
    ax[0].legend()
    ax[0].set_title(title + f' : {""PASS"" if passing else ""FAIL""}')
    ax[0].set_xlabel('Number of records')
    ax[0].set_ylabel('Time [s]')

    if mode1_x == mode2_x:
        mode_comp = [x / y for x, y in zip(mode1_times, mode2_times)]
        ax[1].plot(mode1_x, mode_comp, marker='o', markerfacecolor='none', label=f'{label1} / {label2}')
        ax[1].plot([min(mode1_x), max(mode1_x)], [1.0, 1.0], linestyle='dashed', color='#AAAAAA', label='parity')
        ax[1].set_xscale('log')
        ax[1].legend()
        ax[1].set_title(title + f' (ratio)\nValues <1 indicate {label1} is faster than {label2}')
        ax[1].set_xlabel('Number of records')
        ax[1].set_ylabel(f'{label1} / {label2}')
    plt.tight_layout()
    # plt.show()

    if save_fig:
        global FIGURE_NUMBER
        # https://stackoverflow.com/a/295152
        clean_title = ''.join([x for x in title if (x.isalnum() or x in '_-. ')])
        fig.savefig(f'outputs/{FIGURE_NUMBER:06}_{clean_title}.png')
        FIGURE_NUMBER += 1

    return passing, mode1_times, mode2_times

def _print_result_comparison(success: bool, times1: list[float], times2: list[float], input_lengths: list[int], title: str, label1: str, label2: str):
    print(title)
    print(f'  Test result: {""PASS"" if success else ""FAIL""}')
    field_width = 15
    print(f'{""# of records"":>{field_width}} {label1 + "" [ms]"":>{field_width}} {label2 + "" [ms]"":>{field_width}} {""ratio"":>{field_width}}')
    for input_length, time1, time2 in zip(input_lengths, times1, times2):
        print(f'{input_length:>{field_width}} {time1 * 1000:>{field_width}.03f} {time2 * 1000:>{field_width}.03f} {time1 / time2:>{field_width}.03f}')
    print()

def bench_sub_plot_print(mode1_inputs: list, mode1_statement: str, mode2_inputs: list, mode2_statement: str, title: str, label1: str, label2: str, all_lengths: list[int], save_fig: bool = True) -> tuple[bool, list[float], list[float]]:
    success, times1, times2 = bench_sub_plot(
        mode1_inputs,
        mode1_statement,
        mode2_inputs,
        mode2_statement,
        title,
        label1,
        label2,
        True
    )
    _print_result_comparison(success, times1, times2, all_lengths, title, label1, label2)
    return success, times1, times2


def _main():

    start_time = time.perf_counter_ns()

    # In [2]:
    iris = seaborn.load_dataset('iris')


    # In [3]:
    data_pandas: list[pandas.DataFrame] = []
    data_numpy: list[numpy.rec.recarray] = []
    all_lengths = [10_000, 100_000, 500_000, 1_000_000, 5_000_000, 10_000_000, 15_000_000]
    # all_lengths = [10_000, 100_000, 500_000] #, 1_000_000, 5_000_000, 10_000_000, 15_000_000]
    for total_len in all_lengths:
        data_pandas_i = pandas.concat([iris] * (total_len // len(iris)))
        data_pandas_i = pandas.concat([data_pandas_i, iris[:total_len - len(data_pandas_i)]])
        data_pandas.append(data_pandas_i)
        data_numpy.append(data_pandas_i.to_records())

    # In [4]:
    print('Input sizes [count]:')
    print(f'{""#"":>4} {""pandas"":>9} {""numpy"":>9}')
    for i, (data_pandas_i, data_numpy_i) in enumerate(zip(data_pandas, data_numpy)):
        print(f'{i:>4} {len(data_pandas_i):>9} {len(data_numpy_i):>9}')
    print()

    # In [5]:
    mb_size_in_bytes = 1024 * 1024
    print('Data sizes [MB]:')
    print(f'{""#"":>4} {""pandas"":>9} {""numpy"":>9}')
    for i, (data_pandas_i, data_numpy_i) in enumerate(zip(data_pandas, data_numpy)):
        print(f'{i:>4} {int(sys.getsizeof(data_pandas_i) / mb_size_in_bytes):>9} {int(sys.getsizeof(data_numpy_i) / mb_size_in_bytes):>9}')
    print()

    # In [6]:
    print(data_pandas[0].head())
    print()

    # In [7]:
    # ...

    # In [8]:
    success, times_pandas, times_numpy = bench_sub_plot_print(
        data_pandas,
        'res = data.loc[:, ""sepal_length""].mean()',
        data_numpy,
        'res = numpy.mean(data.sepal_length)',
        'Mean on Unfiltered Column',
        'pandas',
        'numpy',
        all_lengths,
        True
    )

    # In [9]:
    success, times_pandas, times_numpy = bench_sub_plot_print(
        data_pandas,
        'res = numpy.log(data.loc[:, ""sepal_length""])',
        data_numpy,
        'res = numpy.log(data.sepal_length)',
        'Vectorised log on Unfiltered Column',
        'pandas',
        'numpy',
        all_lengths,
        True
    )

    # In [10]:
    success, times_pandas, times_numpy = bench_sub_plot_print(
        data_pandas,
        'res = data.loc[:, ""species""].unique()',
        data_numpy,
        'res = numpy.unique(data.species)',
        'Unique on Unfiltered String Column',
        'pandas',
        'numpy',
        all_lengths,
        True
    )

    # In [11]:
    success, times_pandas, times_numpy = bench_sub_plot_print(
        data_pandas,
        'res = data.loc[(data.sepal_width > 3) & (data.petal_length < 1.5), ""sepal_length""].mean()',
        data_numpy,
        'res = numpy.mean(data[(data.sepal_width > 3) & (data.petal_length < 1.5)].sepal_length)',
        'Mean on Filtered Column',
        'pandas',
        'numpy',
        all_lengths,
        True
    )

    # In [12]:
    success, times_pandas, times_numpy = bench_sub_plot_print(
        data_pandas,
        'res = numpy.log(data.loc[(data.sepal_width > 3) & (data.petal_length < 1.5), ""sepal_length""])',
        data_numpy,
        'res = numpy.log(data[(data.sepal_width > 3) & (data.petal_length < 1.5)].sepal_length)',
        'Vectorised log on Filtered Column',
        'pandas',
        'numpy',
        all_lengths,
        True
    )

    # In [13]:
    success, times_pandas, times_numpy = bench_sub_plot_print(
        data_pandas,
        'res = data[data.species == ""setosa""].sepal_length.mean()',
        data_numpy,
        'res = numpy.mean(data[data.species == ""setosa""].sepal_length)',
        'Mean on (String) Filtered Column',
        'pandas',
        'numpy',
        all_lengths,
        True
    )

    # In [14]:
    success, times_pandas, times_numpy = bench_sub_plot_print(
        data_pandas,
        'res = data.petal_length * data.sepal_length + data.petal_width * data.sepal_width',
        data_numpy,
        'res = data.petal_length * data.sepal_length + data.petal_width * data.sepal_width',
        'Vectorized Math on Unfiltered Column',
        'pandas',
        'numpy',
        all_lengths,
        True
    )

    # In [16]:
    success, times_pandas, times_numpy = bench_sub_plot_print(
        data_pandas,
        'res = data.loc[data.sepal_width * data.petal_length > data.sepal_length, ""sepal_length""].mean()',
        data_numpy,
        'res = numpy.mean(data[data.sepal_width * data.petal_length > data.sepal_length].sepal_length)',
        'Vectorized Math in Filtering Column',
        'pandas',
        'numpy',
        all_lengths,
        True
    )

    end_time = time.perf_counter_ns()
    print(f'Total run time: {(end_time - start_time) / 10 ** 9:.3f} s')

if __name__ == '__main__':
    _main()

Here is the console output it generates:
Input sizes [count]:
   #    pandas     numpy
   0     10000     10000
   1    100000    100000
   2    500000    500000
   3   1000000   1000000
   4   5000000   5000000
   5  10000000  10000000
   6  15000000  15000000

Data sizes [MB]:
   #    pandas     numpy
   0         0         0
   1         9         4
   2        46        22
   3        92        45
   4       464       228
   5       928       457
   6      1392       686

   sepal_length  sepal_width  petal_length  petal_width species
0           5.1          3.5           1.4          0.2  setosa
1           4.9          3.0           1.4          0.2  setosa
2           4.7          3.2           1.3          0.2  setosa
3           4.6          3.1           1.5          0.2  setosa
4           5.0          3.6           1.4          0.2  setosa

Mean on Unfiltered Column
  Test result: PASS
   # of records     pandas [ms]      numpy [ms]           ratio
          10000           0.061           0.033           1.855
         100000           0.160           0.148           1.081
         500000           0.653           1.074           0.608
        1000000           1.512           2.440           0.620
        5000000          11.633          12.558           0.926
       10000000          23.954          25.360           0.945
       15000000          35.362          40.108           0.882

Vectorised log on Unfiltered Column
  Test result: PASS
   # of records     pandas [ms]      numpy [ms]           ratio
          10000           0.124           0.056           2.190
         100000           0.507           0.493           1.029
         500000           3.399           3.441           0.988
        1000000           5.396           6.867           0.786
        5000000          27.187          38.121           0.713
       10000000          55.497          72.609           0.764
       15000000          88.406         112.199           0.788

Unique on Unfiltered String Column
  Test result: PASS
   # of records     pandas [ms]      numpy [ms]           ratio
          10000           0.332           1.742           0.191
         100000           2.885          21.833           0.132
         500000          14.769         125.961           0.117
        1000000          29.687         264.521           0.112
        5000000         147.359        1501.378           0.098
       10000000         295.118        3132.478           0.094
       15000000         444.365        4882.316           0.091

Mean on Filtered Column
  Test result: PASS
   # of records     pandas [ms]      numpy [ms]           ratio
          10000           0.355           0.130           2.719
         100000           0.522           0.672           0.777
         500000           1.797           4.824           0.372
        1000000           4.602          10.827           0.425
        5000000          22.116          57.945           0.382
       10000000          43.076         116.028           0.371
       15000000          68.893         177.658           0.388

Vectorised log on Filtered Column
  Test result: PASS
   # of records     pandas [ms]      numpy [ms]           ratio
          10000           0.361           0.128           2.821
         100000           0.576           0.758           0.760
         500000           2.066           5.199           0.397
        1000000           5.259          11.523           0.456
        5000000          22.785          59.581           0.382
       10000000          47.527         121.882           0.390
       15000000          75.080         187.954           0.399

Mean on (String) Filtered Column
  Test result: PASS
   # of records     pandas [ms]      numpy [ms]           ratio
          10000           0.636           0.192           3.304
         100000           4.068           1.743           2.334
         500000          20.954           9.306           2.252
        1000000          41.938          18.522           2.264
        5000000         217.254          97.929           2.218
       10000000         434.242         197.289           2.201
       15000000         657.205         297.919           2.206

Vectorized Math on Unfiltered Column
  Test result: PASS
   # of records     pandas [ms]      numpy [ms]           ratio
          10000           0.168           0.049           3.415
         100000           0.385           0.338           1.140
         500000           3.193           5.018           0.636
        1000000           6.028           9.539           0.632
        5000000          32.640          48.235           0.677
       10000000          69.748          99.893           0.698
       15000000         107.528         159.040           0.676

Vectorized Math in Filtering Column
  Test result: PASS
   # of records     pandas [ms]      numpy [ms]           ratio
          10000           0.350           0.234           1.500
         100000           0.926           2.494           0.371
         500000           6.093          15.007           0.406
        1000000          12.641          30.021           0.421
        5000000          71.714         163.060           0.440
       10000000         145.373         326.206           0.446
       15000000         227.817         490.991           0.464

Total run time: 183.198 s

And here are the plots it generated:
















These results were generated with Windows 10, Python 3.13, on i9-10900K, and never got close to running out of memory so swap should not be a factor.
",numpy
finding solutions to linear system of equations with integer constraint in scipy,"I have a system of equations where each equation is a linear equation with boolean constraints. For  example:
x1 + x2 + x3 = 2
x1 + x4 = 1
x2 + x1 = 1

And each x_i is either 0 or 1. Sometimes there might be a small positive (<5) coefficient (for example x1 + 2 * x3 + x4 = 3. Basically a standard linear programming task. What I need to do is to find all x_i which are guaranteed to be 0 and all x_j which are guaranteed to be 1. Sorry if my terminology is not correct here but by guaranteed I mean that if you generate all possible solutions you in all of them all x_i will be 0 and in all of them x_j will be 1.
For example my equation has only 2 solutions:

1, 0, 1, 0
0, 1, 1, 1

So you do not have guaranteed 0 and have x_3 as a guaranteed 1.
I know how to solve this problem with or-tools by generating all solutions and it works for my usecases (equations are pretty constrained so usually there are < 500 solutions although the number of variables is big enough to make the whole combinatorial search impossible).
The big problem is that I can't use that library (system restrictions above my control) and only libraries available in my case are numpy and scipy. I found that scipy has scipy.optimize.linprog.
It seems like I have found a way to generate one solution
import numpy as np
from scipy.optimize import linprog

A_eq = np.array([
    [1, 1, 1, 0],  # x1 + x2 + x3 = 2
    [1, 0, 0, 1],  # x1 + x4 = 1
    [1, 1, 0, 0]   # x1 + x2 = 1
])
b_eq = np.array([2, 1, 1])
c = np.zeros(4)
bounds = [(0, 1)] * 4

res = linprog(c, A_eq=A_eq, b_eq=b_eq, bounds=bounds, method='highs-ipm')
if res.success:
    print(res.x)

But I can't find a way to generate all solutions. Also I am not sure whether there is a better way to do it as all I need to know is to find guaranteed values

P.S. this problem is important to me. I guarantee to add a 500 bounty on it, but system prevents me from doing it until 2 days will pass.
","You don't need to (fully) brute-force, and you don't need to find all of your solutions. You just need to find solutions for which each of your variables meets each of their extrema. The following is a fairly brain-off LP approach with 2n² columns and 2mn rows. It's sparse, and for your inputs does not need to be integral. That said, I somewhat doubt it will be the most efficient method possible.
import numpy as np
from scipy.optimize import milp, Bounds, LinearConstraint
import scipy.sparse as sp

lhs = np.array((
    (1, 1, 1, 0),
    (1, 0, 0, 1),
    (1, 1, 0, 0),
))
rhs = np.array((2, 1, 1))
m, n = lhs.shape

# Variables: n * 2 (minimize, maximize) * n
c = sp.kron(
    sp.eye_array(n),
    np.array((
        (+1,),
        (-1,),
    )),
)

b = np.tile(rhs, 2*n)
system_constraint = LinearConstraint(
    A=sp.kron(sp.eye_array(2*n), lhs, format='csc'),
    lb=b, ub=b,
)

result = milp(
    c=c.toarray().ravel(),  # must be dense
    integrality=0,
    bounds=Bounds(lb=0, ub=1),
    constraints=system_constraint,
)
assert result.success
extrema = result.x.reshape((n, 2, n))
mins = extrema[:, 0]
maxs = extrema[:, 1]
vmins = np.diag(mins)
vmaxs = np.diag(maxs)

print('Solutions for minima on the diagonal:')
print(mins)
print('Solutions for maxima on the diagonal:')
print(maxs)
print('Variable minima:', vmins)
print('Variable maxima:', vmaxs)
print('Guaranteed 0:', vmaxs < 0.5)
print('Guaranteed 1:', vmins > 0.5)

Solutions for minima on the diagonal:
[[-0.  1.  1.  1.]
 [ 1.  0.  1. -0.]
 [ 1.  0.  1. -0.]
 [ 1.  0.  1. -0.]]
Solutions for maxima on the diagonal:
[[ 1.  0.  1. -0.]
 [-0.  1.  1.  1.]
 [ 1.  0.  1. -0.]
 [-0.  1.  1.  1.]]
Variable minima: [-0.  0.  1. -0.]
Variable maxima: [1. 1. 1. 1.]
Guaranteed 0: [False False False False]
Guaranteed 1: [False False  True False]

There is a variant on this idea where

rather than using sparse modelling, you just loop
don't use LP at all
fix each variable at each of its extrema, and iteratively column-eliminate from the left-hand side
attempt a least-squares solution of the linear system, and infer a high residual to mean that there is no solution

This somewhat naively assumes that all solutions will see integer values, and (unlike milp) does not have the option to set integrality=1. For demonstration I was forced to add a row to get a residual.
import numpy as np

lhs = np.array((
    (1, 1, 1, 0),
    (1, 0, 0, 1),
    (1, 1, 0, 0),
    (0, 0, 1, 1),
))
rhs = np.array((2, 1, 1, 1))
m, n = lhs.shape
epsilon = 1e-12
lhs_select = np.ones(n, dtype=bool)

for i in range(n):
    lhs_select[i] = False
    x0, (residual,), rank, singular = np.linalg.lstsq(lhs[:, lhs_select], rhs)
    zero_solves = residual < epsilon
    x1, (residual,), rank, singular = np.linalg.lstsq(lhs[:, lhs_select], rhs - lhs[:, i])
    one_solves = residual < epsilon
    lhs_select[i] = True

    if zero_solves and not one_solves:
        print(f'x{i}=0, solution {x0.round(12)}')
    elif one_solves and not zero_solves:
        print(f'x{i}=1, solution {x1.round(12)}')

x0=1, solution [-0.  1.  0.]
x1=0, solution [ 1.  1. -0.]
x2=1, solution [1. 0. 0.]
x3=0, solution [ 1. -0.  1.]

",numpy
batch matrix multiplication in numpy,"I have two numpy arrays a and b of shape [5, 5, 5] and [5, 5], respectively. For both a and b the first entry in the shape is the batch size. When I perform matrix multiplication option, I get an array of shape [5, 5, 5]. An MWE is as follows.
import numpy as np

a = np.ones((5, 5, 5))
b = np.random.randint(0, 10, (5, 5))
c = a @ b
# c.shape is (5, 5, 5)

Suppose I were to run a loop over the batch size, i.e. a[0] @ b[0].T, it would result in an array of shape [5, 1]. Finally, if I concatenate all the results along axis 1, I would get a resultant array with shape [5, 5]. The code below better describes these lines.
a = np.ones((5, 5, 5))
b = np.random.randint(0, 10, (5, 5))
c = []
for i in range(5):
    c.append(a[i] @ b[i].T)
c = np.concatenate([d[:, None] for d in c], axis=1).T
# c.shape evaluates to be (5, 5)

Can I get the above functionality without using loop? For example, PyTorch provides a function called torch.bmm to compute this. Thanks.
","You can work this out using numpy einsum.
c = np.einsum('BNi,Bi ->BN', a, b)

Pytorch also provides this einsum function with slight change in syntax. So you can easily work it out. It easily handles other shapes as well.
Then you don't have to worry about transpose or squeeze operations. It also saves memory because no copy of existing matrices are created internally.
",numpy
how can i write zeros to a 2d numpy array by both row and column indices,"I have a large (90k x 90k) numpy ndarray and I need to zero out a block of it. I have a list of about 30k indices that indicate which rows and columns need to be zero. The indices aren't necessarily contiguous, so a[min:max, min:max] style slicing isn't possible.
As a toy example, I can start with a 2D array of non-zero values, but I can't seem to write zeros the way I expect.
import numpy as np

a = np.ones((6, 8))
indices = [2, 3, 5]

# I thought this would work, but it does not.
# It correctly writes to (2,2), (3,3), and (5,5), but not all
# combinations of (2, 3), (2, 5), (3, 2), (3, 5), (5, 2), or (5, 3)
a[indices, indices] = 0.0
print(a)

[[1. 1. 1. 1. 1. 1. 1. 1.]
 [1. 1. 1. 1. 1. 1. 1. 1.]
 [1. 1. 0. 1. 1. 1. 1. 1.]
 [1. 1. 1. 0. 1. 1. 1. 1.]
 [1. 1. 1. 1. 1. 1. 1. 1.]
 [1. 1. 1. 1. 1. 0. 1. 1.]]

# I thought this would fix that problem, but it doesn't change the array.
a[indices, :][:, indices] = 0.0
print(a)

[[1. 1. 1. 1. 1. 1. 1. 1.]
 [1. 1. 1. 1. 1. 1. 1. 1.]
 [1. 1. 1. 1. 1. 1. 1. 1.]
 [1. 1. 1. 1. 1. 1. 1. 1.]
 [1. 1. 1. 1. 1. 1. 1. 1.]
 [1. 1. 1. 1. 1. 1. 1. 1.]]

In this toy example, I'm hoping for this result.
[[1. 1. 1. 1. 1. 1. 1. 1.]
 [1. 1. 1. 1. 1. 1. 1. 1.]
 [1. 1. 0. 0. 1. 0. 1. 1.]
 [1. 1. 0. 0. 1. 0. 1. 1.]
 [1. 1. 1. 1. 1. 1. 1. 1.]
 [1. 1. 0. 0. 1. 0. 1. 1.]]

I could probably write a cumbersome loop or build some combinatorically huge list of indices to do this, but it seems intuitive that this must be supported in a cleaner way, I just can't find the syntax to make it happen. Any ideas?
","Based on hpaulj's comment, I came up with this, which works perfectly on the toy example.
a[np.ix_(indices, indices)] = 0.0
print(a)

[[1. 1. 1. 1. 1. 1. 1. 1.]
 [1. 1. 1. 1. 1. 1. 1. 1.]
 [1. 1. 0. 0. 1. 0. 1. 1.]
 [1. 1. 0. 0. 1. 0. 1. 1.]
 [1. 1. 1. 1. 1. 1. 1. 1.]
 [1. 1. 0. 0. 1. 0. 1. 1.]]

It also worked beautifully on the real data. It was faster than I expected and didn't noticeably increase memory consumption. Exhausting memory has been a constant concern with these giant arrays.
",numpy
convert a slice of a structured array to regular numpy array in numpy 114,"Note 1: None of the answers given to this question work in my case.
Note 2: The solution must work in NumPy 1.14.
Assume I have the following structured array:
arr = np.array([(105.0, 34.0, 145.0, 217.0)], dtype=[('a', 'f4'), ('b', 'f4'), ('c', 'f4'), ('d', 'f4')]).
Now I'm slicing into the structured data type like so:
arr2 = arr[['a', 'b']]
And now I'm trying to convert that slice into a regular array:
out = arr2[0].view((np.float32, 2))
which results in
ValueError: Changing the dtype of a 0d array is only supported if the itemsize is unchanged
What I would like to get is just a regular array like so:
[105.0, 34.0]
Note that this example is simplified in order to be minimal. In my real use case I'm obviously not dealing with an array that holds one element.
I know that this solution works:
out = np.asarray(list(arr2[0]))
but I thought there must be a more efficient solution than copying data that is already in a NumPy array into a list and then back into an array. I assume there is a way to stay in NumPy an maybe not actually copy any data at all, I just don't know how.
","The 1d array does convert with view:
In [270]: arr = np.array([(105.0, 34.0, 145.0, 217.0)], dtype=[('a', 'f4'), ('b','f4'), ('c', 'f4'), ('d', 'f4')])
In [271]: arr
Out[271]: 
array([(105., 34., 145., 217.)],
      dtype=[('a', '<f4'), ('b', '<f4'), ('c', '<f4'), ('d', '<f4')])
In [272]: arr.view('<f4')
Out[272]: array([105.,  34., 145., 217.], dtype=float32)

It's when we try to convert a single element, that we get this error:
In [273]: arr[0].view('<f4')
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
<ipython-input-273-70fbab8f61ba> in <module>()
----> 1 arr[0].view('<f4')

ValueError: Changing the dtype of a 0d array is only supported if the itemsize is unchanged

Earlier view often required a tweak in the dimensions.  I suspect that with recent changes to handling of structured arrays (most evident when indexing several fields at once), this error is a result, either intentionally or not.
In the whole array case it changed the 1d, 4 field array into a 1d, 4 element array, (1,) to (4,).  But changing the element, goes from () to (4,).
In the past I have recommended tolist as the surest way around problem with view (and astype):
In [274]: arr[0].tolist()
Out[274]: (105.0, 34.0, 145.0, 217.0)
In [279]: list(arr[0].tolist())
Out[279]: [105.0, 34.0, 145.0, 217.0]
In [280]: np.array(arr[0].tolist())
Out[280]: array([105.,  34., 145., 217.])

item is also a good way of pulling an element out of its numpy structure:
In [281]: arr[0].item()
Out[281]: (105.0, 34.0, 145.0, 217.0)

The result from tolost and item is a tuple.
You worry about speed.  But you are just converting one element. It's one thing to worry about the speed when using tolist on a 1000 item array, quite another when working with 1 element.
In [283]: timeit arr[0]
131 ns ± 1.31 ns per loop (mean ± std. dev. of 7 runs, 10000000 loops each)
In [284]: timeit arr[0].tolist()
1.25 µs ± 11.9 ns per loop (mean ± std. dev. of 7 runs, 1000000 loops each)
In [285]: timeit arr[0].item()
1.27 µs ± 2.39 ns per loop (mean ± std. dev. of 7 runs, 1000000 loops each)
In [286]: timeit arr.tolist()
493 ns ± 17.2 ns per loop (mean ± std. dev. of 7 runs, 1000000 loops each)
In [287]: timeit arr.view('f4')
1.74 µs ± 18.7 ns per loop (mean ± std. dev. of 7 runs, 1000000 loops each)


You could index the element in a way that doesn't reduce the dimension to 0 (not that it helps much with speed):
In [288]: arr[[0]].view('f4')
Out[288]: array([105.,  34., 145., 217.], dtype=float32)
In [289]: timeit arr[[0]].view('f4')
6.54 µs ± 15.9 ns per loop (mean ± std. dev. of 7 runs, 100000 loops each)
In [290]: timeit arr[0:1].view('f4')
2.63 µs ± 105 ns per loop (mean ± std. dev. of 7 runs, 100000 loops each)
In [298]: timeit arr[0][None].view('f4')
4.28 µs ± 160 ns per loop (mean ± std. dev. of 7 runs, 100000 loops each)


view still requires a change in shape;  consider a big array:
In [299]: arrs = np.repeat(arr, 10000)
In [301]: arrs.view('f4')
Out[301]: array([105.,  34., 145., ...,  34., 145., 217.], dtype=float32)
In [303]: arrs.shape
Out[303]: (10000,)
In [304]: arrs.view('f4').shape
Out[304]: (40000,)

The view is still 1d, where as we'd probably want a (10000,4) shaped 2d array.
A better view change:
In [306]: arrs.view(('f4',4))
Out[306]: 
array([[105.,  34., 145., 217.],
       [105.,  34., 145., 217.],
       [105.,  34., 145., 217.],
       ...,
       [105.,  34., 145., 217.],
       [105.,  34., 145., 217.],
       [105.,  34., 145., 217.]], dtype=float32)
In [307]: _.shape
Out[307]: (10000, 4)

This works with the 1 element array, whether 1d or 0d:
In [308]: arr.view(('f4',4))
Out[308]: array([[105.,  34., 145., 217.]], dtype=float32)
In [309]: _.shape
Out[309]: (1, 4)
In [310]: arr[0].view(('f4',4))
Out[310]: array([105.,  34., 145., 217.], dtype=float32)
In [311]: _.shape
Out[311]: (4,)

This was suggested in one of the answers in your link: https://stackoverflow.com/a/10171321/901925
Contrary to your comment there, it works for me:
In [312]: arr[0].view((np.float32, len(arr.dtype.names)))
Out[312]: array([105.,  34., 145., 217.], dtype=float32)
In [313]: np.__version__
Out[313]: '1.14.0'


With the edit:
In [84]: arr = np.array([(105.0, 34.0, 145.0, 217.0)], dtype=[('a', 'f4'), ('b','f4'), ('c', 'f4'), ('d', 'f4')])
In [85]: arr2 = arr[['a', 'b']]
In [86]: arr2
Out[86]: 
array([(105., 34.)],
      dtype={'names':['a','b'], 'formats':['<f4','<f4'], 'offsets':[0,4], 'itemsize':16})

In [87]: arr2.view(('f4',2))
...
ValueError: Changing the dtype to a subarray type is only supported if the total itemsize is unchanged

Note that the arr2 dtype includes an offsets value.  In a recent numpy version, multiple field selection has changed.  It is now a true view, preserving the original data - all of it, not just the selected fields.  The itemsize is unchanged:
In [93]: arr.itemsize
Out[93]: 16
In [94]: arr2.itemsize
Out[94]: 16

arr.view(('f4',4) and arr2.view(('f4',4)) produce the same thing.
So you can't view (change dtype) a partial set of the fields.  You have to first take the view of the whole array, and then select rows/columns, or work with tolist.
I'm using 1.14.0.  Release notes for 1.14.1 says:

The change in 1.14.0 that multi-field indexing of structured arrays returns a
  view instead of a copy has been reverted but remains on track for NumPy 1.15.
  Affected users should read the 1.14.1 Numpy User Guide section
  ""basics/structured arrays/accessing multiple fields"" for advice on how to
  manage this transition.

https://docs.scipy.org/doc/numpy-1.14.2/user/basics.rec.html#accessing-multiple-fields
This is still under development.  That doc mentions a repack_fields function, but that doesn't exist yet.
",numpy
how to use skimage to denoise 2d array with nan values,"I'm trying to apply the TV filter to 2D array which includes many nan values:
from skimage.restoration import denoise_tv_chambolle
import numpy as np

data_random = np.random.random ([100,100])*100
plt.imshow(data_random)
plt.imshow(denoise_tv_chambolle(data_random))

data_random[20:30, 50:60] = np.nan
data_random[30:40, 55:60] = np.nan
data_random[40:50, 65:75] = np.nan

plt.imshow(denoise_tv_chambolle(data_random))

The TV filter works well with all valid data, but will return a nan array if there're nan values.
Original data:

Deonised data:

Data with nan values:

","You can use a masked array:
m = np.isnan(data_random)
data = np.ma.masked_array(np.where(m, 0, data_random), m)
plt.imshow(denoise_tv_chambolle(data, weight=50))

Example output (with weight = 50):

For less artifacts you could fill the holes with the average instead of zero:
m = np.isnan(data_random)
data = np.ma.masked_array(np.where(m, np.nanmean(data_random), data_random), m)
plt.imshow(denoise_tv_chambolle(data, weight=50))

Output:

Another option would be to fill the holes with the nearest neighbors (e.g. with distance_transform_edt), then denoise, then restore the NaNs:
from scipy.ndimage import distance_transform_edt

m = np.isnan(data_random)
data_fill = data_random[tuple(distance_transform_edt(m, return_distances=False, return_indices=True))]
plt.imshow(np.where(m, np.nan, denoise_tv_chambolle(data_fill, weight=50)))

Output:

Intermediate data_fill:

",numpy
logisticregression unknown label type 39continuous39 using sklearn in python,"I have the following code to test some of most popular ML algorithms of sklearn python library:
import numpy as np
from sklearn                        import metrics, svm
from sklearn.linear_model           import LinearRegression
from sklearn.linear_model           import LogisticRegression
from sklearn.tree                   import DecisionTreeClassifier
from sklearn.neighbors              import KNeighborsClassifier
from sklearn.discriminant_analysis  import LinearDiscriminantAnalysis
from sklearn.naive_bayes            import GaussianNB
from sklearn.svm                    import SVC

trainingData    = np.array([ [2.3, 4.3, 2.5],  [1.3, 5.2, 5.2],  [3.3, 2.9, 0.8],  [3.1, 4.3, 4.0]  ])
trainingScores  = np.array( [3.4, 7.5, 4.5, 1.6] )
predictionData  = np.array([ [2.5, 2.4, 2.7],  [2.7, 3.2, 1.2] ])

clf = LinearRegression()
clf.fit(trainingData, trainingScores)
print(""LinearRegression"")
print(clf.predict(predictionData))

clf = svm.SVR()
clf.fit(trainingData, trainingScores)
print(""SVR"")
print(clf.predict(predictionData))

clf = LogisticRegression()
clf.fit(trainingData, trainingScores)
print(""LogisticRegression"")
print(clf.predict(predictionData))

clf = DecisionTreeClassifier()
clf.fit(trainingData, trainingScores)
print(""DecisionTreeClassifier"")
print(clf.predict(predictionData))

clf = KNeighborsClassifier()
clf.fit(trainingData, trainingScores)
print(""KNeighborsClassifier"")
print(clf.predict(predictionData))

clf = LinearDiscriminantAnalysis()
clf.fit(trainingData, trainingScores)
print(""LinearDiscriminantAnalysis"")
print(clf.predict(predictionData))

clf = GaussianNB()
clf.fit(trainingData, trainingScores)
print(""GaussianNB"")
print(clf.predict(predictionData))

clf = SVC()
clf.fit(trainingData, trainingScores)
print(""SVC"")
print(clf.predict(predictionData))

The first two works ok, but I got the following error in LogisticRegression call:
root@ubupc1:/home/ouhma# python stack.py 
LinearRegression
[ 15.72023529   6.46666667]
SVR
[ 3.95570063  4.23426243]
Traceback (most recent call last):
  File ""stack.py"", line 28, in <module>
    clf.fit(trainingData, trainingScores)
  File ""/usr/local/lib/python2.7/dist-packages/sklearn/linear_model/logistic.py"", line 1174, in fit
    check_classification_targets(y)
  File ""/usr/local/lib/python2.7/dist-packages/sklearn/utils/multiclass.py"", line 172, in check_classification_targets
    raise ValueError(""Unknown label type: %r"" % y_type)
ValueError: Unknown label type: 'continuous'

The input data is the same as in the previous calls, so what is going on here? 
And by the way, why there is a huge diference in the first prediction of LinearRegression() and SVR() algorithms (15.72 vs 3.95)?
","You are passing floats to a classifier which expects categorical values as the target vector. If you convert it to int it will be accepted as input (although it will be questionable if that's the right way to do it). 
It would be better to  convert your training scores by using scikit's labelEncoder function.
The same is true for your DecisionTree and KNeighbors qualifier.
from sklearn import preprocessing
from sklearn import utils

lab_enc = preprocessing.LabelEncoder()
encoded = lab_enc.fit_transform(trainingScores)
>>> array([1, 3, 2, 0], dtype=int64)

print(utils.multiclass.type_of_target(trainingScores))
>>> continuous

print(utils.multiclass.type_of_target(trainingScores.astype('int')))
>>> multiclass

print(utils.multiclass.type_of_target(encoded))
>>> multiclass

",numpy
generating all permutations efficiently,"I need to generate as fast as possible all permutations of integers 0, 1, 2, ..., n - 1 and have result as a NumPy array of shape (factorial(n), n), or to iterate through large portions of such an array to conserve memory.
Is there some built-in function in NumPy for doing this? Or some combination of functions.
Using itertools.permutations(...) is too slow, I need a faster method.
","Here's a NumPy solution that builds the permutations of size m by modifying the permutations of size m-1 (see more explanation further down):
from math import factorial

def permutations(n):
    a = np.zeros((factorial(n), n), np.uint8)
    f = 1
    for m in range(2, n+1):
        b = a[:f, n-m+1:]      # the block of permutations of range(m-1)
        for i in range(1, m):
            a[i*f:(i+1)*f, n-m] = i
            a[i*f:(i+1)*f, n-m+1:] = b + (b >= i)
        b += 1
        f *= m
    return a

Demo:
>>> permutations(3)
array([[0, 1, 2],
       [0, 2, 1],
       [1, 0, 2],
       [1, 2, 0],
       [2, 0, 1],
       [2, 1, 0]], dtype=uint8)

For n=10, the itertools solution takes 5.5 seconds for me while this NumPy solution takes 0.2 seconds.
How it proceeds: It starts with a zero-array of the goal size, which already contains the permutations for range(1) at the top-right (I ""dotted out"" the other parts of the array):
[[. . 0]
 [. . .]
 [. . .]
 [. . .]
 [. . .]
 [. . .]]

Then it turns that into the permutations of range(2):
[[. 0 1]
 [. 1 0]
 [. . .]
 [. . .]
 [. . .]
 [. . .]]

And then into the permutations of range(3):
[[0 1 2]
 [0 2 1]
 [1 0 2]
 [1 2 0]
 [2 0 1]
 [2 1 0]]

It does so by filling the next-left column and by copying/modifying the previous block of permutations downwards.
",numpy
modulenotfounderror no module named 39distutilsmsvccompiler39 when trying to install numpy 116,"I'm working inside a conda environment and I'm trying to downgrade numpy to version 1.16, but when running pip install numpy==1.16 I keep getting the following error:
$ pip install numpy==1.16
Collecting numpy==1.16
  Downloading numpy-1.16.0.zip (5.1 MB)
     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 5.1/5.1 MB 10.8 MB/s eta 0:00:00
  Preparing metadata (setup.py) ... done
Building wheels for collected packages: numpy
  Building wheel for numpy (setup.py) ... error
  error: subprocess-exited-with-error
  
  × python setup.py bdist_wheel did not run successfully.
  │ exit code: 1
  ╰─> [17 lines of output]
      Running from numpy source directory.
      /tmp/pip-install-jdof0z8r/numpy_4597057bbb504aa18b7bda112f0aa37f/numpy/distutils/misc_util.py:476: SyntaxWarning: ""is"" with a literal. Did you mean ""==""?
        return is_string(s) and ('*' in s or '?' is s)
      Traceback (most recent call last):
        File ""<string>"", line 2, in <module>
        File ""<pip-setuptools-caller>"", line 34, in <module>
        File ""/tmp/pip-install-jdof0z8r/numpy_4597057bbb504aa18b7bda112f0aa37f/setup.py"", line 415, in <module>
          setup_package()
        File ""/tmp/pip-install-jdof0z8r/numpy_4597057bbb504aa18b7bda112f0aa37f/setup.py"", line 394, in setup_package
          from numpy.distutils.core import setup
        File ""/tmp/pip-install-jdof0z8r/numpy_4597057bbb504aa18b7bda112f0aa37f/numpy/distutils/core.py"", line 26, in <module>
          from numpy.distutils.command import config, config_compiler, \
        File ""/tmp/pip-install-jdof0z8r/numpy_4597057bbb504aa18b7bda112f0aa37f/numpy/distutils/command/config.py"", line 19, in <module>
          from numpy.distutils.mingw32ccompiler import generate_manifest
        File ""/tmp/pip-install-jdof0z8r/numpy_4597057bbb504aa18b7bda112f0aa37f/numpy/distutils/mingw32ccompiler.py"", line 34, in <module>
          from distutils.msvccompiler import get_build_version as get_build_msvc_version
      ModuleNotFoundError: No module named 'distutils.msvccompiler'
      [end of output]
  
  note: This error originates from a subprocess, and is likely not a problem with pip.
  ERROR: Failed building wheel for numpy
  Running setup.py clean for numpy
  error: subprocess-exited-with-error
  
  × python setup.py clean did not run successfully.
  │ exit code: 1
  ╰─> [10 lines of output]
      Running from numpy source directory.
      
      `setup.py clean` is not supported, use one of the following instead:
      
        - `git clean -xdf` (cleans all files)
        - `git clean -Xdf` (cleans all versioned files, doesn't touch
                            files that aren't checked into the git repo)
      
      Add `--force` to your command to use it anyway if you must (unsupported).
      
      [end of output]
  
  note: This error originates from a subprocess, and is likely not a problem with pip.
  ERROR: Failed cleaning build dir for numpy
Failed to build numpy
ERROR: ERROR: Failed to build installable wheels for some pyproject.toml based projects (numpy)

How can I resolve this?
","What seemed to solve this issue was installing a specific version of setuptools (<65) into the conda environment:
conda install ""setuptools <65""

",numpy
optimizing the construction of a large matrix in numpy,"I am working on a part of my code where I need to construct a matrix S1 of size num x num, where num = d^2 is typically very large i.e., > 6000. Below is the part of the code.
S1 = np.zeros((num, num), dtype=np.complex128)
    for i in range(num):
        for j in range(num):
            S1[i][j] = np.trace(np.dot(b1[i], np.dot(b1[j], state)))

'b1' is a list of d x d matrices of length d^2, and 'state' is another d x d matrix, both of which were constructed earlier in my code.
Specifically, 'state' is a density matrix, and 'b1' is a set (list) of d x d Hermitian matrices satisfying the Hilbert-Schmidt inner product, thus forming an orthonormal basis which I use to represent 'state' later in my code (for some other purpose which is not relevant here). Based on the idea provided in this answer, I wrote the following function basis(n) which gives the output of b1:
def basis(n):

    list2=[]
    list3=[]
    list4=[]
    list5=[]
    list6=[]


    for i in range(0,n):
        for j in range(i+1,n):
        
        m1=np.zeros((n,n),dtype=np.complex128)
        m1[i,j]=1j
        m1[j,i]=1j
        
        list2.append(m1/np.sqrt(2))
        
        m2=np.zeros((n,n),dtype=np.complex128)
        m2[i,j]=-1
        m2[j,i]=1
        
        list3.append(m2/np.sqrt(2))
        
    for i in range(0,n-1):
    
        m3=np.zeros(n,dtype=np.complex128)
        m3[i]=1j
        m3[i+1]=-1j
    
        list4.append(m3)
   
        org=np.linalg.qr(np.array(list4).T)[0]
        l1=org[:,i]
        list5.append(l1)
 
        l2=np.diag(list5[i])
        list6.append(l2)
    
    return [np.identity(n)/np.sqrt(n)]+[mat*(-1j) for mat in 
           (list2+list3+list6)]

For example, when d=81, S1 has num=6561 rows and columns, resulting in 6561^2 iterations across the nested loops. As a result, the construction of S1 becomes extremely slow.
I have also checked that 'b1', 'state' are quite sparse. Hence, I am trying to find some way to exploit their sparsity to reduce the dimension/number of iterations.
Are there ways to optimize this code to construct S1 in a faster and efficient way?
I am still learning NumPy, and any suggestions to improve the performance of this code would be helpful. Thanks.
","A first trivial optimization is to move np.dot(b1[j], state) (let's call it tmp) in the for i in range(num): loop so it is not recomputed d**2 times (i.e. 6561 times faster). This should make the computation nearly twice faster.
Another optimization is not to compute a matrix multiplication b1[i] @ tmp (where @ is the operator for matrix multiplications). Indeed, np.trace needs only the diagonal of the result (and compute the sum of the items on this diagonal). Thus, the overall np.trace(...) expression can be computed with np.sum(b1[i] * tmp.T). Fortunately, tmp.T can also be precomputed. This operation should now make the whole operation d times faster (i.e. 81 times faster).
Here is the final code:
S1 = np.zeros((num, num), dtype=np.complex128)
for j in range(num):
    tmpT = (b1[j] @ state).T.copy()
    for i in range(num):
        S1[i][j] = np.sum(b1[i] * tmpT)

If b would not be a list we could optimize this further. Indeed, we can use np.einsum or even matrix multiplications so to compute np.sum(b1[i] * tmpT) for many i simultaneously. On a CPU with T cores, this would make the operation T times faster (i.e. 5~15 times faster in practice on a mainstream PC) because BLAS operations are parallel. Note you can parallelize this manually with tools like Numba, Cython and certainly NumExpr too (not as fast as BLAS' matrix multiplications but not that far either). In the end, it would be certainly 3 orders of magnitude faster.
",numpy
igraph graph from numpy or pandas adjacency matrix,"I have an adjacency matrix stored as a pandas.DataFrame:
node_names = ['A', 'B', 'C']
a = pd.DataFrame([[1,2,3],[3,1,1],[4,0,2]],
    index=node_names, columns=node_names)
a_numpy = a.as_matrix()

I'd like to create an igraph.Graph from either the pandas or the numpy adjacency matrices. In an ideal world the nodes would be named as expected.
Is this possible? The tutorial seems to be silent on the issue.
","In igraph you can use igraph.Graph.Adjacency to create a graph from an adjacency matrix without having to use zip. There are some things to be aware of when a weighted adjacency matrix is used and stored in a np.array or pd.DataFrame.

igraph.Graph.Adjacency can't take an np.array as argument, but that is easily solved using tolist.

Integers in adjacency-matrix are interpreted as number of edges between nodes rather than weights, solved by using adjacency as boolean.


An example of how to do it:
import igraph
import pandas as pd

node_names = ['A', 'B', 'C']
a = pd.DataFrame([[1,2,3],[3,1,1],[4,0,2]], index=node_names, columns=node_names)

# Get the values as np.array, it's more convenenient.
A = a.values

# Create graph, A.astype(bool).tolist() or (A / A).tolist() can also be used.
g = igraph.Graph.Adjacency((A > 0).tolist())

# Add edge weights and node labels.
g.es['weight'] = A[A.nonzero()]
g.vs['label'] = node_names  # or a.index/a.columns

You can reconstruct your adjacency dataframe using get_adjacency by:
df_from_g = pd.DataFrame(g.get_adjacency(attribute='weight').data,
                         columns=g.vs['label'], index=g.vs['label'])
(df_from_g == a).all().all()  # --> True

",numpy
how to extinguish cycle in my code when calculating emwa,"I'm calculating EWMA values for array of streamflow, and code is like below:
import polars as pl
import numpy as np

streamflow_data = np.arange(0, 20, 1)
adaptive_alphas = np.concatenate([np.repeat(0.3, 10), np.repeat(0.6, 10)])
streamflow_series = pl.Series(streamflow_data)
ewma_data = np.zeros_like(streamflow_data)
for i in range(1, len(streamflow_series)):
    current_alpha = adaptive_alphas[i]
    ewma_data[i] = streamflow_series[:i+1].ewm_mean(alpha=current_alpha)[-1]

# When set dtype of ewma_data to float when initial it, output is like this
Output: [0  0.58823529  1.23287671  1.93051717  2.67678771  3.46668163,  4.29488309  5.1560635   6.04512113  6.95735309  9.33379473 10.33353466, 11.33342058 12.33337091 13.33334944 14.33334021 15.33333625 16.33333457, 17.33333386 18.33333355]

# When I don't point dtype of ewma_data and dtype of streamflow_data is int, output will be floored
Output: [0  0  1  1  2  3  4  5  6  6  9 10 11 12 13 14 15 16 17 18]

But when length of streamflow_data is very big (such as >100000), this code will become very slow.
So how can I extinguish for in my code and don't influence its result?
Hope for your reply.
","If you have only few alpha values and/or have some condition on which alpha should be used for which row, you could use pl.coalesce(), pl.when() and pl.Expr.ewm_mean():
df = pl.DataFrame({
    ""adaptive_alpha"": np.concatenate([np.repeat(0.3, 10), np.repeat(0.6, 10)]),
    ""streamflow"": np.arange(0, 20, 1)
})

df.with_columns(
    pl.coalesce(
        pl.when(pl.col.adaptive_alpha == alpha)
        .then(pl.col.streamflow.ewm_mean(alpha = alpha))
        for alpha in df[""adaptive_alpha""].unique()
    ).alias(""ewma"")
).with_columns(ewma_int = pl.col.ewma.cast(pl.Int32))

shape: (20, 4)
┌────────────────┬────────────┬───────────┬──────────┐
│ adaptive_alpha ┆ streamflow ┆ ewma      ┆ ewma_int │
│ ---            ┆ ---        ┆ ---       ┆ ---      │
│ f64            ┆ i64        ┆ f64       ┆ i32      │
╞════════════════╪════════════╪═══════════╪══════════╡
│ 0.3            ┆ 0          ┆ 0.0       ┆ 0        │
│ 0.3            ┆ 1          ┆ 0.588235  ┆ 0        │
│ 0.3            ┆ 2          ┆ 1.232877  ┆ 1        │
│ 0.3            ┆ 3          ┆ 1.930517  ┆ 1        │
│ 0.3            ┆ 4          ┆ 2.676788  ┆ 2        │
│ …              ┆ …          ┆ …         ┆ …        │
│ 0.6            ┆ 15         ┆ 14.33334  ┆ 14       │
│ 0.6            ┆ 16         ┆ 15.333336 ┆ 15       │
│ 0.6            ┆ 17         ┆ 16.333335 ┆ 16       │
│ 0.6            ┆ 18         ┆ 17.333334 ┆ 17       │
│ 0.6            ┆ 19         ┆ 18.333334 ┆ 18       │
└────────────────┴────────────┴───────────┴──────────┘

",numpy
apply operation to all elements in matrix skipping numpynan,"I have an array filled with data only in lower triangle spaces, the rest is np.nan. I want to do some operations on this matrix, more precisely- with data elements, not nans, because I expect the behaviour when nans elements are skipped in vectorized operation to be much quicker.
I have two test arrays:
arr = np.array([
    [1.111, 2.222, 3.333, 4.444, 5.555],
    [6.666, 7.777, 8.888, 9.999, 10.10],
    [11.11, 12.12, 13.13, 14.14, 15.15],
    [16.16, 17.17, 18.18, 19.19, 20.20],
    [21.21, 22.22, 23.23, 24.24, 25.25]
])

arr_nans = np.array([
    [np.nan, np.nan, np.nan, np.nan, np.nan],
    [6.666, np.nan, np.nan, np.nan, np.nan],
    [11.11, 12.12, np.nan, np.nan, np.nan],
    [16.16, 17.17, 18.18, np.nan, np.nan],
    [21.21, 22.22, 23.23, 24.24, np.nan]
])

Thats the way I test them:
test = timeit.timeit('arr * 5 / 2.123', globals=globals(), number=1000)
test_nans = timeit.timeit('arr_nans * 5 / 2.123', globals=globals(), number=1000)

masked_arr_nans = np.ma.array(arr_nans, mask=np.isnan(arr_nans))
test_masked_nans = timeit.timeit('masked_arr_nans * 5 / 2.123', globals=globals(), number=1000)

print(test)                 # 0.0017232997342944145s
print(test_nans)            # 0.0017070993781089783s
print(test_masked_nans)     # 0.052730199880898s

I have created a mask array masked_arr_nans and masked all nans. But this way is far slower then the first two. I dont understand why.
The main question is- which is the quckest way to operate with arrays like arr_nans containing a lot of nans, probably there is a qicker approach then the ones I mentioned.
Side question is- why masked array works much slower?
","I think this hypothesis is incorrect:

I expect the behaviour when nans elements are skipped in vectorized operation to be much quicker

In your array the data is contiguous, which is among others why vectorization is fast. If you used a masked array, this doesn't change this fact, there will be as much data and the masked portions will need to be ignored during processing. This has an extra cost of verifying which data is masked or not. Skipping the data will still need to happen in the masked array.
Quite often, with vectorized operations, if is more efficient to perform extra operations and handle the data as contiguous values rather that trying to optimize the number of operations.
If really you need to perform several operations or complex/expensive computations on a subset of the data, I would advise to create a new array with just this data. The cost of selecting the data will be only paid once or will be lower than of the computations.
idx = np.tril_indices_from(arr, k=-1)
tril_arr = arr[idx]

# do several things with tril_arr

# restore a rectangular form
out = np.full_like(arr, np.nan)
out[idx] = tril_arr

Example
Let take your input array and perform repeated operations on it (for each operation we compute arr = 1 - arr). We either apply the operation on the full array or on the flattened lower triangle.
The cost of selecting the subset of the data is not worth it if we perform a few operations. After enough intermediate operations this become identical in speed:

Now let's use a more complex/expensive computation (arr = log(exp(arr))). Now we see two things:

After a threshold, it is faster to subset the data
The position of the threshold at which the two approaches (subset vs full) have the same speed is not the same than with the arr = 1-arr example:


As a rule of thumb, if the operation you want to perform on the non-masked values is cheap or non repeated, don't bother and apply it on the whole thing. If the operation is complex/expensive/repeated, then consider subsetting the data.
Plots above is a subset vs relative format:
arr = 1 - arr

arr = log(exp(arr))

",numpy
apply function for lower triangle of 2d array,"I have an array:
U = np.array([3, 5, 7, 9, 11])

I want to get a result like:
result = np.array([
    [  np.nan,      np.nan,      np.nan,      np.nan,    np.nan],
    [U[0] - U[1],   np.nan,      np.nan,      np.nan,    np.nan],
    [U[0] - U[2], U[1] - U[2],   np.nan,      np.nan,    np.nan],
    [U[0] - U[3], U[1] - U[3], U[2] - U[3],   np.nan,    np.nan],
    [U[0] - U[4], U[1] - U[4], U[2] - U[4], U[3] - U[4], np.nan]
])

I can use np.tril_indices(4, k=-1) to get indices of lower triangle without diagonal, but what is next?
","A naive approach that does more work than necessary is to compute the entire difference and select the elements you need:
np.where(np.arange(U.size)[:, None] > np.arange(U.size), U[:, None] - U, np.nan)

This is one of the times where np.where is actually useful over a simple mask, although it can be done with a mask as well:
result = np.full((U.size, U.size), np.nan)
index = np.arange(U.size)
mask = index[:, None] > index
result[mask] = [U[:, None] - U][mask]

A more efficient approach might be to use the indices more directly to index into the source:
result = np.full((U.size, U.size), np.nan)
r, c = np.tril_indices(U.size, k=-1)
result[r, c] = U[c] - U[r]

",numpy
how to implement a butterworth filter,"I am trying to implement a butterworthfilter with python
The data are from a CSV-File, which is called Samples.csv and looks like this
998,4778415
1009,209592
1006,619094
1001,785406
993,9426543
990,1408991
992,736118
995,8127334
...

The column calls Euclidian Norm. The range of the data are from 0 to 1679.286158 and there are 1838 rows.
This is the code I use:
from scipy.signal import filtfilt
from scipy import stats

import csv
import pandas as pd
import numpy as np

import matplotlib.pyplot as plt
import scipy
def plot():
    data=pd.read_csv('Samples.csv',sep="";"", decimal="","")
    sensor_data=data[['Euclidian Norm']]
    sensor_data=np.array(sensor_data)
    
    time=np.linspace(0,1679.286158,1838)
    plt.plot(time,sensor_data)
    plt.show()
    
    filtered_signal=bandPassFilter(sensor_data)
    plt.plot(time,sensor_data)
    plt.show()
    
    
def bandPassFilter(signal):
    fs = 4000.0
    lowcut=20.0
    highcut=50.0

    nyq=0.5*fs
    low=lowcut/nyq
    high=highcut/nyq

    order =2

    b,a=scipy.signal.butter(order,[low,high],'bandpass',analog=False)

    y=scipy.signal.filtfilt(b,a,signal,axis=0)

    return(y)

plot()


My problem is that nothing changes in my data. It doesn't filtered my data. The graph of the filtered data is the same like the source data. Does anyone know what could be wrong?
The first graph is the source data and the second graph is the filtered graph, which looks like the same graph to me.

","I can't comment yet.
You're never using filtered_signal and plot with the same arguments twice.
Here`s one of my implementations with added interpolation, very similar to yours:
def butterFit(data, freq, order=2):
    ar = scipy.signal.butter(order, freq)       # Gets params for filttilt
    return spfilter.filtfilt(ar[0], ar[1], data)


def plotFilteredSplines(timeframe, data, amount_points):
    # Generate evenly spread indices for the data points.
    indices = np.arange(0, len(data), amount_points)

    cutoff_freq = 2 / (2/10 * len(timeframe))
    # Reshape the data with butter :)
    data = butterFit(data, cutoff_freq)

    # Plot Fitlered data
    plt.plot(timeframe, data, '-.')
    interpol_x = np.linspace(timeframe[0], timeframe[-1], 100)

    # Get the cubic spline approx function
    interpolation = sp.interpolate.interp1d(timeframe, data, kind='cubic')

    # Plot the interpolation over the extended time frame.
    plt.plot(interpol_x, interpolation(interpol_x), '-r')


",numpy
nearest neighbor interpolation,"Say that I have an array:
arr = np.arange(4).reshape(2,2)

The array arr contains the elements
array([[0, 1],
       [2, 3]])

I want to increase the resolution of the array in such a way that the following is achieved:
np.array([0,0,1,1],
         [0,0,1,1],
         [2,2,3,3],
         [2,2,3,3]])

what is this operation called? Nearest-neighbor interpolation?
It is possible to get my desired output with the following
np.concat(np.repeat(arr,4).reshape(-1,2,2,2), axis=-1).reshape(4,4)

Is there a more general way of doing this for any kind of matrix?
","You're looking for ""nearest-neighbour upsampling"", instead of ""interpolation"".
A concise and efficient way to do this in numpy:
import numpy as np

arr = np.arange(6).reshape(2, 3)

upsampled = np.repeat(np.repeat(arr, 2, axis=0), 2, axis=1)

print(""Original Array:"")
print(arr)
print(""Upsampled Array:"")
print(upsampled)

Note: I changed your sample data to (2,3) to show that the original shape does not affect the function.
Result:
Original Array:
[[0 1 2]
 [3 4 5]]
Upsampled Array:
[[0 0 1 1 2 2]
 [0 0 1 1 2 2]
 [3 3 4 4 5 5]
 [3 3 4 4 5 5]]

If you need it to be more memory efficient, you can also use:
upsampled2 = np.kron(arr, np.ones((2, 2), dtype=int))

print(""Upsampled Array using Kronecker product:"")
print(upsampled2)

Result:
Upsampled Array using Kronecker product:
[[0 0 1 1 2 2]
 [0 0 1 1 2 2]
 [3 3 4 4 5 5]
 [3 3 4 4 5 5]]

Libraries like scipy may offer even more efficient methods of doing the same, if your data is very large.
",numpy
why numpyarray so slow when editing the data inside,"I wrote an algorithm that uses a long list. Since numpy.array should perform better in dealing with long data, I wrote another version using numpy.array.
list version:
import math
def PrimeTable(n:int) -> list[int]:
   nb2m1 = n//2 - 1
   l = [True]*nb2m1
   for i in range(1,(math.isqrt(n)+1)//2):
      if l[i-1]:
         for j in range(i*3,nb2m1,i*2+1):
            l[j] = False
   return [2] + [i for i,v in zip(range(3,n,2), l) if v]

array version:
import math, numpy
def PrimeTable2(n:int) -> list[int]:
   nb2m1 = n//2 - 1
   l = numpy.full(nb2m1, True)
   for i in range(1,(math.isqrt(n)+1)//2):
      if l[i-1]:
         for j in range(i*3,nb2m1,i*2+1):
            l[j] = False
   return [2] + [i for i,v in zip(range(3,n,2), l) if v]

It turns out that numpy.array version is 1x slower than list version. The graph:where y0 is PrimeTable and y1 is PrimeTable2
Why is numpy.array so slow, and how can I improve it?
","Numpy is optimized for vectorized operations, where large chunks of data are processed in bulk. Instead of updating element-by-element(l[j] = False) we can use slicing to update a range of values at once. Also reducing the number of python loops should make the code more effective.
I've further optimised the code by using a single boolean array. The sieve tracks only odd numbers directly. This eliminates the need for multiplication and division steps for odd indices during iteration. After the sieve is complete, the prime numbers are directly reconstructed from the indices, avoiding intermediate computations.
Try out this updated code. This should work better than the list version of code:
import numpy as np

def PrimeTable2_optimized_v2(n: int) -> list[int]:
    if n < 2:
        return []
    if n == 2:
        return [2]

    # Only consider odd numbers; even numbers > 2 are not prime
    sieve = np.ones(n // 2, dtype=bool)
    limit = int(n**0.5) + 1

    for i in range(1, limit // 2 + 1):
        if sieve[i]:
            start = 2 * i * (i + 1) 
            sieve[start::2 * i + 1] = False

    # Convert sieve to primes
    primes = 2 * np.nonzero(sieve)[0] + 1 
    primes[0] = 2 
    return primes.tolist()

",numpy
fastest exponentiation of numpy 3d matrix,"Q is a 3D matrix and could for example have the following shape:

(4000, 25, 25)

I want raise Q to the power n for {0, 1, ..., k} and sum it all.
Basically, I want to calculate

\sum_{i=0}^{k-1}Q^n

I have the following function that works as expected:
def sum_of_powers(Q: np.ndarray, k: int) -> np.ndarray:
    Qs = np.sum([
        np.linalg.matrix_power(Q, n) for n in range(k)
    ], axis=0)

    return Qs

Is it possible to speed up my function or is there a faster method to obtain the same output?
","We can perform this calculation in O(log k) matrix operations.
Let M(k) represent the k'th power of the input, and S(k) represent the sum of those powers from 0 to k. Let I represent an appropriate identity matrix.
Approach 1
If you expand the product, you'll find that (M(1) - I) * S(k) = M(k+1) - I. That means we can compute M(k+1) using a standard matrix power (which takes O(log k) matrix multiplications), and compute S(k) by using numpy.linalg.solve to solve the equation (M(1) - I) * S(k) = M(k+1) - I:
import numpy.linalg

def option1(Q, k):
    identity = numpy.eye(Q.shape[-1])
    A = Q - identity
    B = numpy.linalg.matrix_power(Q, k+1) - identity
    return numpy.linalg.solve(A, B)

Approach 2
The standard exponentation by squaring algorithm computes M(2*k) as M(k)*M(k) and M(2*k+1) as M(2*k)*M(1).
We can alter the algorithm to track both S(k-1) and M(k), by computing S(2*k-1) as S(k-1)*M(k) + S(k-1) and S(2*k) as S(2*k-1) + M(2*k):
import numpy

def option2(Q, k):
    identity = numpy.eye(Q.shape[-1])

    if k == 0:
        res = numpy.empty_like(Q)
        res[:] = identity
        return res

    power = Q
    sum_of_powers = identity

    # Looping over a string might look dumb, but it's actually the most efficient option,
    # as well as the simplest. (It wouldn't be the bottleneck even if it wasn't efficient.)
    for bit in bin(k+1)[3:]:
        sum_of_powers = (sum_of_powers @ power) + sum_of_powers
        power = power @ power
        if bit == ""1"":
            sum_of_powers += power
            power = power @ Q
    return sum_of_powers

",numpy
best way to interpolate a numpyndarray along an axis,"I have 4-dimensional data, say for the temperature, in an numpy.ndarray. 
The shape of the array is (ntime, nheight_in, nlat, nlon).
I have corresponding 1D arrays for each of the dimensions that tell me which time, height, latitude, and longitude a certain value corresponds to, for this example I need height_in giving the height in metres.
Now I need to bring it onto a different height dimension, height_out, with a different length.
The following seems to do what I want:
ntime, nheight_in, nlat, nlon = t_in.shape

nheight_out = len(height_out)
t_out = np.empty((ntime, nheight_out, nlat, nlon))

for time in range(ntime):
    for lat in range(nlat):
        for lon in range(nlon):
            t_out[time, :, lat, lon] = np.interp(
                height_out, height_in, t[time, :, lat, lon]
            )

But with 3 nested loops, and lots of switching between python and numpy, I don't think this is the best way to do it.
Any suggestions on how to improve this? Thanks
","scipy's interp1d can help:
import numpy as np
from scipy.interpolate import interp1d

ntime, nheight_in, nlat, nlon = (10, 20, 30, 40)

heights = np.linspace(0, 1, nheight_in)

t_in = np.random.normal(size=(ntime, nheight_in, nlat, nlon))
f_out = interp1d(heights, t_in, axis=1)

nheight_out = 50
new_heights = np.linspace(0, 1, nheight_out)
t_out = f_out(new_heights)

",numpy
how to duplicate each row of a matrix n times numpy,"I have a matrix with these dimensions (150,2) and I want to duplicate each row N times. I show what I mean with an example.
Input:
a = [[2, 3], [5, 6], [7, 9]]

suppose N= 3, I want this output: 
[[2 3]
 [2 3]
 [2 3]
 [5 6]
 [5 6]
 [5 6]
 [7 9]
 [7 9]
 [7 9]]

Thank you.
","Use np.repeat with parameter axis=0 as:
a = np.array([[2, 3],[5, 6],[7, 9]])

print(a)
[[2 3]
 [5 6]
 [7 9]]

r_a = np.repeat(a, repeats=3, axis=0)

print(r_a)
[[2 3]
 [2 3]
 [2 3]
 [5 6]
 [5 6]
 [5 6]
 [7 9]
 [7 9]
 [7 9]]

",numpy
generalized kronecker product with different type of product in numpy or scipy,"Consider two boolean arrays
import numpy as np

A = np.asarray([[True,  False],
                [False, False]])

B = np.asarray([[False, True],
                [True,  True]])

I want to take the kronecker product of A and B under the xor operation. The result should be:
C = np.asarray([[True,  False, False, True],
                [False, False, True,  True],
                [False, True,  False, True],
                [True,  True,  True,  True]])

More generally, is there a simple way to implement the Kronecker product with some multiplication operator distinct from the operator *, in this instance the xor operator ^?
","You could use broadcasting and reshaping:
m, n = A.shape
p, q = B.shape
C = (A[:, None, :, None] ^ B[None, :, None, :]).reshape(m*p, n*q)

Simplified:
C = (A[:, None, :, None] ^ B[None, :, None, :]
     ).reshape(A.shape[0]*B.shape[0], -1)

Also equivalent to:
C = (np.logical_xor.outer(A, B)
       .swapaxes(1, 2)
       .reshape(A.shape[0]*B.shape[0], -1)
     )

Or with explicit alignment using repeat/tile without reshaping:
p, q = B.shape
C = np.repeat(np.repeat(A, p, axis=0), q, axis=1) ^ np.tile(B, A.shape)

Output:
array([[ True, False, False,  True],
       [False, False,  True,  True],
       [False,  True, False,  True],
       [ True,  True,  True,  True]])

ND-generalization
for N dimensional inputs, one could follow the same logic by expanding the dimensions in an interleaved fashion with expand_dims, before reshaping to the element-wise product of the dimensions:
C = ( np.expand_dims(A, tuple(range(1, A.ndim*2, 2)))
    ^ np.expand_dims(B, tuple(range(0, A.ndim*2, 2)))
    ).reshape(np.multiply(A.shape, B.shape))

Interestingly, this is how kron is actually implemented in numpy (with some extra checks in place).
Variant with outer:
C = (np.logical_xor.outer(A, B)
       .transpose(np.arange(A.ndim+B.ndim)
                    .reshape(-1, 2, order='F')
                    .ravel())
       .reshape(np.multiply(A.shape, B.shape))
     )

",numpy
numpy grayscale image to black and white,"I use the MNIST dataset that contains 28x28 grayscale images represented as numpy arrays with 0-255 values. I'd like to convert images to black and white only (0 and 1) so that pixels with a value over 128 will get the value 1 and pixels with a value under 128 will get the value 0.
Is there a simple method to do so?
","Yes. Use (arr > 128) to get a boolean mask array of the same shape as your image, then .astype(int) to cast the bools to ints:
>>> import numpy as np
>>> arr = np.random.randint(0, 255, (5, 5))
>>> arr
array([[153, 167, 141,  79,  58],
       [184, 107, 152, 215,  69],
       [221,  90, 172, 147, 125],
       [ 93,  35, 125, 186, 187],
       [ 19,  72,  28,  94, 132]])
>>> (arr > 128).astype(int)
array([[1, 1, 1, 0, 0],
       [1, 0, 1, 1, 0],
       [1, 0, 1, 1, 0],
       [0, 0, 0, 1, 1],
       [0, 0, 0, 0, 1]])

",numpy
forward fill numpy matrix  mask with values based on condition,"I have the following matrix
import numpy as np


A = np.array([
    [0, 0, 0, 0, 1, 0, 1],
    [0, 0, 0, 0, 0, 0, 1],
    [1, 0, 0, 0, 0, 0, 0],
    [0, 0, 0, 0, 0, 0, 0]
]).astype(bool)

How do I fill all the rows column-wise after a column is True?
My desired output:
    [0, 0, 0, 0, 1, 1, 1],
    [0, 0, 0, 0, 0, 0, 1],
    [1, 1, 1, 1, 1, 1, 1],
    [0, 0, 0, 0, 0, 0, 0]

","You could use logical_or combined with accumulate:
np.logical_or.accumulate(A, axis=1)

Output:
array([[False, False, False, False,  True,  True,  True],
       [False, False, False, False, False, False,  True],
       [ True,  True,  True,  True,  True,  True,  True],
       [False, False, False, False, False, False, False]])

If you want integers, go with maximum:
np.maximum.accumulate(A.astype(int), axis=1)

array([[0, 0, 0, 0, 1, 1, 1],
       [0, 0, 0, 0, 0, 0, 1],
       [1, 1, 1, 1, 1, 1, 1],
       [0, 0, 0, 0, 0, 0, 0]])

",numpy
convert numpy type to python,"I have a list of dicts in the following form that I generate from pandas. I want to convert it to a JSON format.
list_val = [{1.0: 685}, {2.0: 8}]
output = json.dumps(list_val)

However, json.dumps throws an error:

TypeError: 685 is not JSON serializable

I am guessing it's a type conversion issue from numpy to python(?).
However, when I convert the values v of each dict in the array using np.int32(v) it still throws the error.
Here's the full code:
new = df[df[label] == label_new]
ks_dict = json.loads(content)
ks_list = ks_dict['variables']
freq_counts = []

for ks_var in ks_list:

    freq_var = dict()
    freq_var[""name""] = ks_var[""name""]
    ks_series = new[ks_var[""name""]]
    temp_df = ks_series.value_counts().to_dict()
    freq_var[""new""] = [{u: np.int32(v)} for (u, v) in temp_df.iteritems()]
    freq_counts.append(freq_var)

out = json.dumps(freq_counts)

","It looks like you're correct:
>>> import numpy
>>> import json
>>> json.dumps(numpy.int32(685))
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/usr/lib/python2.7/json/__init__.py"", line 243, in dumps
    return _default_encoder.encode(obj)
  File ""/usr/lib/python2.7/json/encoder.py"", line 207, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/usr/lib/python2.7/json/encoder.py"", line 270, in iterencode
    return _iterencode(o, 0)
  File ""/usr/lib/python2.7/json/encoder.py"", line 184, in default
    raise TypeError(repr(o) + "" is not JSON serializable"")
TypeError: 685 is not JSON serializable

The unfortunate thing here is that numpy numbers' __repr__ doesn't give you any hint about what type they are.  They're running around masquerading as ints when they aren't (gasp).  Ultimately, it looks like json is telling you that an int isn't serializable, but really, it's telling you that this particular np.int32 (or whatever type you actually have) isn't serializable.  (No real surprise there -- No np.int32 is serializable).  This is also why the dict that you inevitably printed before passing it to json.dumps looks like it just has integers in it as well.
The easiest workaround here is probably to write your own serializer1:
class MyEncoder(json.JSONEncoder):
    def default(self, obj):
        if isinstance(obj, numpy.integer):
            return int(obj)
        elif isinstance(obj, numpy.floating):
            return float(obj)
        elif isinstance(obj, numpy.ndarray):
            return obj.tolist()
        else:
            return super(MyEncoder, self).default(obj)

You use it like this:
json.dumps(numpy.float32(1.2), cls=MyEncoder)
json.dumps(numpy.arange(12), cls=MyEncoder)
json.dumps({'a': numpy.int32(42)}, cls=MyEncoder)

etc.
1Or you could just write the default function and pass that as the defaut keyword argument to json.dumps.  In this scenario, you'd replace the last line with raise TypeError, but ... meh.  The class is more extensible :-)
",numpy
quotruntimeerror numpy is not availablequot when using inverse_transform,"I got this error on python 3.12.7:
Traceback (most recent call last):
  File ""/Users/hongviet/Library/Mobile Documents/com~apple~CloudDocs/Documents/DataAnlalysCoding/First Week/baitap1.py"", line 186, in <module>
    main()
  File ""/Users/hongviet/Library/Mobile Documents/com~apple~CloudDocs/Documents/DataAnlalysCoding/First Week/baitap1.py"", line 179, in main
    original_scores = scaler.inverse_transform(features[i].numpy().reshape(1, -1)).flatten()

                                      


I dont know Why but i think it can be error by the data:
when I tried to transform them from the tensor, here is what method I find on github but it not true, Does anyone know how to fix or has another method

here is my code on main function
build()

# tạo dữ liệu mẫu
features, labels = generate_sample_data(12)
for i in range (12):
    print(features[i], labels[i])


# Tạo mô hình từ file đã lưu
model = MLP()
model.load_state_dict(torch.load('model.pth'))
model.eval()

# Chuẩn hóa dữ liệu
scaler = preprocessing.StandardScaler()
features = scaler.fit_transform(features)
features = torch.tensor(features, dtype=torch.float32)

# Dự đoán
with torch.no_grad():
    y_pred = model(features)
    y_pred = (y_pred > 0.5).float()
    print('Nhãn dự đoán:', y_pred)
    print('Nhãn thực tế:', labels)

# in ra các học sinh đạt dnah hiệu 
# và các học sinh không đạt danh hiệu với số điểm tương ứng mà sinh viên đó có
for i in range(12):
    original_scores = scaler.inverse_transform(features[i].numpy().reshape(1, -1)).flatten()
    if y_pred[i] == 1:
        print('Học sinh đạt danh hiệu với điểm:', features[i])
    else:
        print('Học sinh không đạt danh hiệu với điểm:', features[i])

","I have faced this error after the release of numpy 2.0. Not knowing which version of pytorch or numpy you are using. Try:
pip install ""numpy<2""

in your python package environment.
",numpy
implement maxmean poolingwith stride with numpy,"I wanted to know how to implement a simple max/mean pooling with numpy. I was reading Max and mean pooling with numpy, but unfortunately it assumed the stride was the same as the kernel size. Is there a numpythonic way to do this? Also it would be nice if this were to work for any dimension, but of course not neccesary.
","Here's a pure numpy implementation using stride_tricks:
import numpy as np
from numpy.lib.stride_tricks import as_strided


def pool2d(A, kernel_size, stride, padding=0, pool_mode='max'):
   '''
    2D Pooling

    Parameters:
        A: input 2D array
        kernel_size: int, the size of the window over which we take pool
        stride: int, the stride of the window
        padding: int, implicit zero paddings on both sides of the input
        pool_mode: string, 'max' or 'avg'
    '''
    # Padding
    A = np.pad(A, padding, mode='constant')

    # Window view of A
    output_shape = ((A.shape[0] - kernel_size) // stride + 1,
                    (A.shape[1] - kernel_size) // stride + 1)
    
    shape_w = (output_shape[0], output_shape[1], kernel_size, kernel_size)
    strides_w = (stride*A.strides[0], stride*A.strides[1], A.strides[0], A.strides[1])
    
    A_w = as_strided(A, shape_w, strides_w)

    # Return the result of pooling
    if pool_mode == 'max':
        return A_w.max(axis=(2, 3))
    elif pool_mode == 'avg':
        return A_w.mean(axis=(2, 3))

Example:
>>> A = np.array([[1, 1, 2, 4],
                  [5, 6, 7, 8],
                  [3, 2, 1, 0],
                  [1, 2, 3, 4]])

>>> pool2d(A, kernel_size=2, stride=2, padding=0, pool_mode='max')

array([[6, 8],
       [3, 4]])


https://cs231n.github.io/convolutional-networks/
",numpy
typeerror cannot convert numpyndarray to numpyndarray,"I'm not sure why but after getting a new install of windows and a new pycharm install I am having issues with running some previously functional code. I am now getting the above error with the code below. Is it a setup issue or has something changed that now makes this code not function? Error happens on the last line. The error doesn't make sense to me as there should be no conversion required for ndarray to ndarray.
import numpy as np
import pyodbc
import pandas as pd
import sqlalchemy as SQL
import torch
import datetime

# Setup your SQL connection
server = [hidden for security]
database = [hidden for security]
username = [hidden for security]
password = [hidden for security]
# This is using the pyodbc connection
cnxn = pyodbc.connect(
    'DRIVER={SQL Server};SERVER=' + server + ';DATABASE=' + database + ';UID=' + username + ';PWD=' + password)
cursor = cnxn.cursor()
# This is using the SQLAlchemy connection
engine_str = SQL.URL.create(
    drivername=""mssql+pyodbc"",
    username=username,
    password=password,
    host=server,
    port=1433,
    database=database,
    query={
        ""driver"": ""ODBC Driver 17 for SQL Server"",
        ""TrustServerCertificate"": ""no"",
        ""Connection Timeout"": ""30"",
        ""Encrypt"": ""yes"",
    },
)
engine = SQL.create_engine(engine_str)

storeemployee = []
regionalemployee = []
regionid = []
storeid = []

# get table from dev
with engine.connect() as connection:
    result = connection.execute(SQL.text(""SELECT StoreId, R_Num, RegionalMerchandiserEmployeeId, StoreMerchandiserEmployeeId from Staging.StoreMerchandiserInput""))
    for row in result:
        # set your variables = to the results
        storeemployee.append(row.StoreMerchandiserEmployeeId)
        regionalemployee.append(row.RegionalMerchandiserEmployeeId)
        regionid.append(row.R_Num)
        storeid.append(row.StoreId)
storeemployee = np.array(storeemployee)
regionalemployee = np.array(regionalemployee)
regionid = np.array(regionid)
storeid = np.array(storeid)

# StoreMerchandiserEmail
data = {'StoreMerchandiserEmployeeId': storeemployee, 'RegionalMerchandiserEmployeeId': regionalemployee,
        ""R_Num"": regionid, ""StoreId"":storeid}
FinalData = pd.DataFrame(data, columns=['StoreMerchandiserEmployeeId', 'RegionalMerchandiserEmployeeId', 'R_Num', 'StoreId'])

Edit -
Full Error Messaging:
Traceback (most recent call last):
  File ""C:\Users\Carter.Lowe\Documents\Python Files\Data Import 2.py"", line 56, in <module>
    FinalData = pd.DataFrame(data, columns=['StoreMerchandiserEmployeeId', 'RegionalMerchandiserEmployeeId', 'R_Num', 'StoreId'])
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""C:\Users\Carter.Lowe\AppData\Local\Programs\Python\Python312\Lib\site-packages\pandas\core\frame.py"", line 778, in __init__
    mgr = dict_to_mgr(data, index, columns, dtype=dtype, copy=copy, typ=manager)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""C:\Users\Carter.Lowe\AppData\Local\Programs\Python\Python312\Lib\site-packages\pandas\core\internals\construction.py"", line 443, in dict_to_mgr
    arrays = Series(data, index=columns, dtype=object)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""C:\Users\Carter.Lowe\AppData\Local\Programs\Python\Python312\Lib\site-packages\pandas\core\series.py"", line 490, in __init__
    index = ensure_index(index)
            ^^^^^^^^^^^^^^^^^^^
  File ""C:\Users\Carter.Lowe\AppData\Local\Programs\Python\Python312\Lib\site-packages\pandas\core\indexes\base.py"", line 7647, in ensure_index
    return Index(index_like, copy=copy, tupleize_cols=False)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""C:\Users\Carter.Lowe\AppData\Local\Programs\Python\Python312\Lib\site-packages\pandas\core\indexes\base.py"", line 565, in __new__
    arr = sanitize_array(data, None, dtype=dtype, copy=copy)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""C:\Users\Carter.Lowe\AppData\Local\Programs\Python\Python312\Lib\site-packages\pandas\core\construction.py"", line 654, in sanitize_array
    subarr = maybe_convert_platform(data)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""C:\Users\Carter.Lowe\AppData\Local\Programs\Python\Python312\Lib\site-packages\pandas\core\dtypes\cast.py"", line 139, in maybe_convert_platform
    arr = lib.maybe_convert_objects(arr)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""lib.pyx"", line 2538, in pandas._libs.lib.maybe_convert_objects
TypeError: Cannot convert numpy.ndarray to numpy.ndarray

","I just had the same error today and solved it by updating numpy to 2.0.0rc2.
",numpy
project one index from another array,"I have an array a of size (M, N, K).
And I have an array b of size (M, N) with integer values of [0, K-1].
How do I get the array... c of size (M, N), where
c[i, j] == a[i, j, b[i, j]]
in the simplest manner?
Which part of the indexing guide is it?
","You can use advanced indexing:
c = a[np.arange(M)[:, None], np.arange(N), b]

Output:
array([[ 0,  6, 12, 18],
       [24, 25, 31, 37],
       [43, 49, 50, 56]])

Taking @Vitalizzare's example:
# input
M, N, K = 3, 4, 5
a = np.arange(M*N*K).reshape(M, N, K)
b = np.arange(M*N).reshape(M, N) % K

# output
array([[ 0,  6, 12, 18],
       [24, 25, 31, 37],
       [43, 49, 50, 56]])

",numpy
how to get the numpy array i want,"How can I change the array like this?
arr = [
[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11],
[20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31],
[30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41],
...,
[130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141],
]

=> [
[ [0, 1], [20, 21], [30, 31], ,.., [130, 131]],
[ [2, 3], [22, 23], [32, 33], .., [132, 133]],
[ [4, 5], [24, 25], [34, 35], .., [134, 135]],
[ [6, 7], [26, 27], [36, 37], .., [136, 137]],
[ [8, 9], [28, 29], [38, 39], .., [138, 139]],
[ [10, 11], [30, 31], [40, 41], .., [140, 141]],
...
]

My attempt was best modified to the following, which was not the result I wanted.
[ [0, 1], [2, 3], [4, 5], ,.., [10, 11]],
[ [20, 21], [22, 23], [24, 25], .., [30, 31]],

","You can use reshape+swapaxes:
n, m = arr.shape
out = arr.reshape(n, m//2, 2).swapaxes(0, 1)

# or
out = arr.reshape(n, -1, 2).swapaxes(0, 1)

Example:
# input
arr = np.array([[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11],
                [20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31],
                [30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41],
                [130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141]])

# output
array([[[  0,   1], [ 20,  21], [ 30,  31], [130, 131]],
       [[  2,   3], [ 22,  23], [ 32,  33], [132, 133]],
       [[  4,   5], [ 24,  25], [ 34,  35], [134, 135]],
       [[  6,   7], [ 26,  27], [ 36,  37], [136, 137]],
       [[  8,   9], [ 28,  29], [ 38,  39], [138, 139]],
       [[ 10,  11], [ 30,  31], [ 40,  41], [140, 141]]])

",numpy
replace table in hdf5 file with a modified table,"I have an existing HDF5 file with multiple tables. I want to modify this HDF5 file: in one of the tables I want to drop some rows entirely, and modify values in the remaining rows.
I tried the following code:
import h5py
import numpy as np

with h5py.File(""my_file.h5"", ""r+"") as f:
    # Get array
    table = f[""/NASTRAN/RESULT/ELEMENTAL/STRESS/QUAD4_COMP_CPLX""]
    arr = np.array(table)
    
    # Modify array
    arr = arr[arr[:, 1] == 2]
    arr[:, 1] = 1

    # Write array back
    table[...] = arr

This code however results in the following error when run:
Traceback (most recent call last):

  File ""C:\_Work\test.py"", line 10, in <module>
    arr[arr[:, 1] == 2]

IndexError: too many indices for array: array is 1-dimensional, but 2 were indexed

So one of the problems seems to be that the numpy array arr that I've created is not a two-dimensional array. However I'm not sure exactly how to create a two-dimensional array out of the HDF5 table (or whether that is even the best approach here).
Would anyone here be able to help put me on the right path?
Edit
Output from h5dump on my dataset is as follows
HDF5 ""C:\_Work\my_file.h5"" {
DATASET ""/NASTRAN/RESULT/ELEMENTAL/STRESS/QUAD4_COMP_CPLX"" {
   DATATYPE  H5T_COMPOUND {
      H5T_STD_I64LE ""EID"";
      H5T_STD_I64LE ""PLY"";
      H5T_IEEE_F64LE ""X1R"";
      H5T_IEEE_F64LE ""Y1R"";
      H5T_IEEE_F64LE ""T1R"";
      H5T_IEEE_F64LE ""L1R"";
      H5T_IEEE_F64LE ""L2R"";
      H5T_IEEE_F64LE ""X1I"";
      H5T_IEEE_F64LE ""Y1I"";
      H5T_IEEE_F64LE ""T1I"";
      H5T_IEEE_F64LE ""L1I"";
      H5T_IEEE_F64LE ""L2I"";
      H5T_STD_I64LE ""DOMAIN_ID"";
   }
   DATASPACE  SIMPLE { ( 990 ) / ( H5S_UNLIMITED ) }
   ATTRIBUTE ""version"" {
      DATATYPE  H5T_STD_I64LE
      DATASPACE  SIMPLE { ( 1 ) / ( 1 ) }
   }
}
}

","This answer is specifically focused on OP's request in comments to ""throw away all rows where the value for PLY is not 2. Then in the remaining rows change the value for PLY from 2 to 1"".
The procedure is relatively straight-forward...if you know the tricks. Steps are summarized here, with matching comments in the code:

Created stress dataset object (but don't extract to an array).
Rename/move original output dataset to a saved name (not req'd but
good practice)
Create a new stress array by extracting row indices where PLY==2. This is the most sophisticated step. np.nonzero() returns row indices that match the condition stress_arr['PLY']==2, then uses them as indices to slice values from the array.
Modify all rows in the new array from PLY ID 2 to 1
Save the new array to a dataset with the original name

Code below:
with h5py.File('quad4_comp_cplx_test.h5', 'r+') as h5f:
    # Create stress dataset object
    stress_ds = h5f['/NASTRAN/RESULT/ELEMENTAL/STRESS/QUAD4_COMP_CPLX']
    ## stress array below not reqd
    ## stress_arr = stress_ds[()]  
    print(stress_ds.shape)   

    # Rename/move original output dataset to saved name 
    h5f.move('/NASTRAN/RESULT/ELEMENTAL/STRESS/QUAD4_COMP_CPLX',\
             '/NASTRAN/RESULT/ELEMENTAL/STRESS/QUAD4_COMP_CPLX_save')

    # Slice a stress array from dataset using indices where PLY==2   
    # modified reference from stress_arr to stress_ds
    ## mod_stress_arr = stress_arr[np.nonzero(stress_arr['PLY']==2)] 
    mod_stress_arr = stress_ds[np.nonzero(stress_ds['PLY']==2)]
    print(mod_stress_arr.shape) 

    # Modify PLY ID from 2 to 1 for all rows
    mod_stress_arr[:]['PLY'] = 1
        
    # Finally, save the ply stress array to a dataset with the original name
    h5f.create_dataset('/NASTRAN/RESULT/ELEMENTAL/STRESS/QUAD4_COMP_CPLX', 
                                    data=mod_stress_arr)

",numpy
how to convert a numpy 3d array into a meshgrid  stl,"I have a python numpy.ndarray which has 3 dimensions and I would like to have the eventual goal of turning this into an stl file so that I can 3D print it.
I have found many cool examples of people using functions with numpy.meshgrid or other packages which can take an x/y relationship over a z dimension and (eventually) use this to generate an stl. However, in my case, I do not have a mathematical relationship between x, y and z and I also do not have a list of vertices describing the faces of my array.
I have yet to see something where I can directly take a 3 dimensional numpy.ndarray and convert this into a meshgrid/stl/list of vertices, etc.
Do you see something that I am missing? :)
Thank you!
","You can use the marching cubes algorithm to convert a 3d voxel mask (1: object voxels, 0: background voxels; which is what you have, if I understand your question correctly) into a surface.
Here, I create the 50×60×70-element 3D NumPy array mask as an exemplary input array, which represents the volume of an ellipsoid with radii 10, 20, 30. I am then using vtkMarchingCubes, which is VTK's implementation of the marching cubes algorithm, to get its surface. Smoothing with vtkSmoothPolyDataFilter, which implements Laplacian smoothing, might improve the somewhat rough resulting surface:
import numpy as np
import vtk
from vtk.util.numpy_support import numpy_to_vtk

def ellipsoid_mask(shape=(50, 60, 70), radii=(10, 20, 30)):
    xyz = np.mgrid[*(slice(-s//2, s-s//2, 1) for s in shape)]
    return (sum((c/r)**2 for c, r in zip(xyz, radii)) <= 1).astype(np.uint8)

mask = ellipsoid_mask()  # Create binary mask of ellipsoid

# Convert to vtkImageData
vtk_data = vtk.vtkImageData()
vtk_data.SetDimensions(*mask.shape[::-1])
vtk_array = numpy_to_vtk(mask.ravel(), array_type=vtk.VTK_UNSIGNED_CHAR)
vtk_data.GetPointData().SetScalars(vtk_array)

# Apply marching cubes to get the surface
mc = vtk.vtkMarchingCubes()
mc.SetInputData(vtk_data)
mc.SetValue(0, 0.5)

# Smooth the surface
flt = vtk.vtkSmoothPolyDataFilter()
flt.SetInputConnection(mc.GetOutputPort())
flt.SetNumberOfIterations(25)
flt.SetRelaxationFactor(0.1)

# Save the resulting surfaces as STL
for src in (mc, flt):
    stl_writer = vtk.vtkSTLWriter()
    stl_writer.SetInputConnection(src.GetOutputPort())
    stl_writer.SetFileName(f""{src.__class__.__name__}_surface.stl"")
    stl_writer.Write()

Rendering the resulting vtkMarchingCubes_surface.stl file with MeshLab produces:

Rendering vtkSmoothPolyDataFilter_surface.stl produces:

",numpy
getting strange output when using group by apply with npselect function,"I am working with a Timeseries data wherein I am trying to perform outlier detection using IQR method.
Sample Data:
import pandas as pd
import numpy as np

df = pd.DataFrame({'datecol' : pd.date_range('2024-1-1', '2024-12-31'),
                   'val' : np.random.random.randin(low = 100, high = 5000, size = 8366})

my function:
def is_outlier(x):
    iqr = x.quantile(.75) - x.quantile(.25)
    outlier = (x <= x.quantile(.25) - 1.5*iqr) | (x >= x.quantile(.75) + 1.5*iqr)
    return np.select([outlier], [1], 0)

df.groupby(df['datecol'].dt.weekday)['val'].apply(is_outlier)

to which the output is something like below:
0    [1,1,0,0,....
1    [1,0,0,0,....
2    [1,1,0,0,....
3    [1,0,1,0,....
4    [1,1,0,0,....
5    [1,1,0,0,....
6    [1,0,0,1,....

I am expecting a single series as output which I can add back to the original dataframe as a flag column.
Can someone please help me with this
","You should use groupby.transform, not apply:
df['flag'] = df.groupby(df['datecol'].dt.weekday)['val'].transform(is_outlier)

Alternatively, explicitly return a Series and use group_keys=False:
def is_outlier(x):
    iqr = x.quantile(.75) - x.quantile(.25)
    outlier = (x <= x.quantile(.25) - 1.5*iqr) | (x >= x.quantile(.75) + 1.5*iqr)
    return pd.Series(np.where(outlier, 1, 0), index=x.index)

df['flag'] = (df.groupby(df['datecol'].dt.weekday, group_keys=False)
              ['val'].apply(is_outlier)
              )

Note that with a single condition, np.where should be preferred to np.select.
You could also use a vectorial approach with groupby.quantile:
wd = df['datecol'].dt.weekday
g = df.groupby(wd)['val']
q25 = g.quantile(.25)
q75 = g.quantile(.75)
iqr = wd.map(q75-q25)
df['flag'] = 1 - df['val'].between(wd.map(q25) - 1.5*iqr, wd.map(q75) + 1.5*iqr)

Output:
       datecol   val  flag
0   2024-01-01  3193     0
1   2024-01-02  1044     0
2   2024-01-03  2963     0
3   2024-01-04  4448     0
4   2024-01-05  1286     0
..         ...   ...   ...
361 2024-12-27  1531     0
362 2024-12-28  4565     0
363 2024-12-29  3396     0
364 2024-12-30  1870     0
365 2024-12-31  3818     0

",numpy
python  generate array of specific autocorrelation,"I am interested in generating an array(or numpy Series) of length N that will exhibit specific autocorrelation at lag 1. Ideally, I want to specify the mean and variance, as well, and have the data drawn from (multi)normal distribution. But most importantly, I want to specify the autocorrelation. How do I do this with numpy, or scikit-learn?
Just to be explicit and precise, this is the autocorrelation I want to control:
numpy.corrcoef(x[0:len(x) - 1], x[1:])[0][1]

","If you are interested only in the auto-correlation at lag one, you can generate an auto-regressive process of order one with the parameter equal to the desired auto-correlation; this property is mentioned on the Wikipedia page, but it's not hard to prove it.
Here is some sample code:
import numpy as np

def sample_signal(n_samples, corr, mu=0, sigma=1):
    assert -1 < corr < 1, ""Auto-correlation coefficient must be between -1 and 1""
    
    # Find out the offset `c` and the std of the white noise `sigma_e`
    # that produce a signal with the desired mean and variance.
    # See https://en.wikipedia.org/wiki/Autoregressive_model
    # under section ""Example: An AR(1) process"".
    c = mu * (1 - corr)
    sigma_e = np.sqrt((sigma ** 2) * (1 - corr ** 2))

    # Sample the auto-regressive process.
    signal = [np.random.normal(mu, sigma)]
    for _ in range(1, n_samples):
        signal.append(c + corr * signal[-1] + np.random.normal(0, sigma_e))

    return np.array(signal)

def compute_corr_lag_1(signal):
    return np.corrcoef(signal[:-1], signal[1:])[0][1]

# Examples.
print(compute_corr_lag_1(sample_signal(5000, 0.5)))
print(np.mean(sample_signal(5000, 0.5, mu=2)))
print(np.std(sample_signal(5000, 0.5, sigma=3)))

The parameter corr lets you set the desired auto-correlation at lag one and the optional parameters, mu and sigma, let you control the mean and standard deviation of the generated signal.
",numpy
numpy float to halffloat conversion rne when result is subnormal,"I'm trying to understand how NumPy implements rounding to nearest even when converting to a lower precision format, in this case, Float32 to Float16, specifically the case, when the number is normal in Float32, but it's rounded to a subnormal in Float16.
Link to the code:
https://github.com/numpy/numpy/blob/13a5c4e569269aa4da6784e2ba83107b53f73bc9/numpy/core/src/npymath/halffloat.c#L244-L365
My understanding is as follows,
In float32, the number has the bits



31
30
29
28
27
26
25
24
23
22
21
20
19
18
17
16
15
14
13
12
11
10
9
8
7
6
5
4
3
2
1
0




s
e0
e1
e2
e3
e4
e5
e6
e7
m0
m1
m2
m3
m4
m5
m6
m7
m8
m9
m10
m11
m12
m13
m14
m15
m16
m17
m18
m19
m20
m21
m22



        /*
         * If the last bit in the half significand is 0 (already even), and
         * the remaining bit pattern is 1000...0, then we do not add one
         * to the bit after the half significand. However, the (113 - f_exp)
         * shift can lose up to 11 bits, so the || checks them in the original.
         * In all other cases, we can just add one.
         */
        if (((f_sig&0x00003fffu) != 0x00001000u) || (f&0x000007ffu)) {m
            f_sig += 0x00001000u;
        }

The above code is used when breaking ties to nearest even. I don't understand why in the second part of the logical OR , we bitwise AND against 0x0000'07ffu (bits m12-m22)  and not 0x0000'ffffu (m11-m22) .
Once we've aligned the mantissa bits to be in the subnormal format for float16 (which is what the bit-shifting before this piece of code does), in the float32 number representation above we'd have m10 - m22 deciding which direction to round.
My understanding is that the second part of the OR checks whether the number is larger than the half-way, point, and if it is, then adds a one to the half-significand bit. But with the original number, isn't it only checking for a subset of the numbers that are above the half-way point? In the float16 number m9 would be the last precision that's going to remain. So we'll round up if,

m9 is 1, m10 is 1 and m11-m22 are all 0 (The first part of the OR)

m10 is 1, at least one of m11-m22 is 1 (to put the number above the half-way point)

can be simplified by adding 1 to m10, if any-of m11-m22 is 1. if m10 was already 1, the addition will bleed to m9, otherwise it'll stay unaffected. But, in the case of the NumPy code, the bits checked are m12-m22.


I'm not sure what I'm missing here. Is this a special case scenario?
I was expecting bits m11-m22 to be the ones that decide whether to add 1 and nor m12-m22.
","f_sig contains a significand-in-preparation for the binary16 result. (binary16 is the IEEE-754 name for what some people call a “half precision” floating-point format.) At this point, the code needs the significand bits in bits 22:13, because it is later going to shift them by 13 more bits, putting them in 9:0. In preparation for this, it shifted the bits according to the exponent. That shifted some bits out of f_sig.
Now it wants to test whether the low bit of the new significand (now in bit 13) is 0, the highest of the bits below the significand (in bit 12) is 1, and all the remaining bits are 0. Some of those remaining bits are in bits 11:0 of f_sig. But some of them may be gone. The shift according to the exponent shifted some of them out. So, to test whether those bits are 0, we look at them in the original significand in f.
Since the exponent shift shifted out at most 11 bits, we only have to look at the low 11 bits of f. The other bits of the original significand are still present in f_sig.
So, in (f_sig&0x00003fffu) != 0x00001000u) || (f&0x000007ffu), the left operand of || tests the original significand bits that are f_sig and the right operand tests the original significand bits that are in f. There may be some overlap; the latter may test some bits that are also in f_sig, but that does not matter.

My understanding is that the second part of the OR checks whether the number is larger than the half-way, point, and if it is, then adds a one to the half-significand bit.

No, it is not checking that. The test is true if and only if the trailing portion is not exactly ½ the least significant bit (LSB) of the new significand or the least significant bit is 1.
The reasoning is this:

The controlled statement, f_sig += 0x00001000u;, adds ½ the LSB, and the significand is later truncated at the LSB (f_sig >> 13). This provides the desired rounding in most cases: Adding ½ to trailing portions less than ½ does not carry, and adding ½ to trailing portions more than ½ does carry.
Further, in cases where the trailing portion is exactly ½ and we add ½, the addition carries, and this is the desired behavior for when the low bit of the LSB is 1.
So the only case where we do not want to do this addition is when the trailing portion is exactly ½ and the low bit of the LSB is 0.

",numpy
how do i print the two elements of an array and the subsequent sum of these elements goldbachs primes question,"I am trying to solve this problem: Goldbach Conjecture
Show with a program ""goldbach.py"" ​​that all even numbers up to 1000 can indeed be written as the sum of two primes. Specifically: for each even number, also show explicitly (on the screen) that it can be written as the sum of two primes, as in the example below
Even more important is of course if you find a number that does not meet Goldbach's suspicion. Make sure your program clearly displays such a discovery on the screen. Bingo!
python goldbach.py
16 = ...
18 = 5 + 13
20 = 3 + 17
22 = 5 + 17
24 = ...
Progress
So far, I have created a list where all the primes until 1000 are stored, and then I have created a list in which all the combination of primes of which the sum is an even number until 1000. I knew the format to have it print 3 + 17, but I am stuck in trying to have it say sum(pairs) = prime1 ""+"" prime2. Should be 3 + 17 = 20  for example. Also, I don't know how to have only 1 example of a pair of primes who's sum is of an even number until 1000. I need to break the loop some how.
Because the sum function was not working I found I could convert it to a ""numpy array"" and then use ""accumulate"". I just can't get it to work and know I'm getting the error message 'DeprecationWarning: elementwise == comparison failed; this will raise an error in the future.'
Could someone help me with the code?
from itertools import accumulate, islice
from numpy import array
import numpy as np

primes = []
pairs = []
numpy_pairs = np.asarray(pairs)

for num in range (4, 1000):
    for j in range (2, num):
        if (num % j) == 0:
            break
    else:        
        primes.append(num)

#for x in range(2,1000):
#   if x in primes:
#        print (""Ja, het getal {} komt voor in mijn primes"".format(x))


for x in range(2,1000):
    if x % 2 == 0:
        for prime1 in primes:
            for prime2  in primes:
                if prime1 + prime2 == x and [prime1, prime2] not in numpy_pairs and [prime2, prime1] not in numpy_pairs:
                    np.append(numpy_pairs,[prime1,prime2])                           
results = (""{}+{}={}"".format(i, j, k) for i, j in zip(numpy_pairs[0::2], 
numpy_pairs[1::2]) for k in accumulate(islice(numpy_pairs,numpy_pairs.stop)))

print('\n'.join(results))                        

","First things first, there are a lot of optimizations you can do to make the code logic better. the way you find primes can be improved, you only need to check numbers upto square root n to verify if n is a prime. Also, the actual verification of Goldbachs conjecture can be improved as well.
However, just focusing on the current code, You should stick to using lists if you want to append values, and to stop the code you need to use a sort of hack to stop the nested looping using the for-else syntax. Also, you can use f-strings to format strings nicely from python 3.6 onwards.
primes = []
pairs = []

for num in range (2, 1000): #modified. you forgot 2 and 3!
    for j in range (2, num):
        if (num % j) == 0:
            break
    else:        
        primes.append(num)

result = []
for x in range(2,1000):
    if x % 2 == 0:
        for prime1 in primes:
            for prime2  in primes:
                if prime1 + prime2 == x:
                    print(f""{x} = {prime1} + {prime2}"")
                    result.append((prime1, prime2))
                    break
            else: #this is a for-else syntax. enter this block if the for loop did not encounter a break
                continue #go to next iteration of the mid-level loop. This prevents the line afterwards from being executed in cases where the inner loop did not ""break""
            break #break the mid level loop if you reach this line.
        else:
            print(""You have done it! Bingo!!"")

",numpy
open and view npz file in python,"I have an unknown .npz file and would like to see whats inside. I am very new to Python.
>>> import numpy
>>> b = numpy.load('data.npz')
>>> print(b.files)
['arr_1', 'arr_0', 'arr_3', 'arr_2']

How do I see what these arr_i contain (i.e. dimension, values, etc.)?
","np.savez_compressed('filename.npz', array1=array1, array2=array2)
b = np.load('filename.npz')

And do b['array_1'], b['array_2'] and so on to retrieve data from each array.
",numpy
what is the meaning of numpy reduceat in python,"I am very new in Python and I read at this moment just the tutorial. 
I am confused about the reduceat() function. 
I saw the example:
np.add.reduceat([0,1,2,3,4,5,6,7],[0,4,1,5,2,6,3,7])[::2]

and result is:
array([ 6, 10, 14, 18])

How does it come out? Can some one explain for me?
","It is sort of like a rolling apply, see:
In [59]:
np.add.reduceat([0,1,2,3,4,5,6,7],[0,4])
Out[59]:
array([ 6, 22])

In [65]:    
np.add.reduceat([0,1,2,3,4,5,6,7],[4,1])
Out[65]:
array([ 4, 28])

In [66]:
np.add.reduceat([0,1,2,3,4,5,6,7],[1,5])
Out[66]:
array([10, 18])

In [64]:
np.add.reduceat([0,1,2,3,4,5,6,7],[5,2])
Out[64]:
array([ 5, 27])

In [61]:
np.add.reduceat([0,1,2,3,4,5,6,7],[2,6])
Out[61]:
array([14, 13])

In [67]:
np.add.reduceat([0,1,2,3,4,5,6,7],[6,3])
Out[67]:
array([ 6, 25])

In [62]:
np.add.reduceat([0,1,2,3,4,5,6,7],[3,7])
Out[62]:
array([18,  7])

If you want just the 1st value, you can get it done in just one shot:
In [63]:    
np.add.reduceat([0,1,2,3,4,5,6,7],[0,4,1,5,2,6,3,7])
Out[63]:
array([ 6,  4, 10,  5, 14,  6, 18,  7])

",numpy
loading text file containing both float and string using numpyloadtxt,"I have a text file, data.txt, which contains:
5.1,3.5,1.4,0.2,Iris-setosa
4.9,3.0,1.4,0.2,Iris-setosa
5.8,2.7,4.1,1.0,Iris-versicolor
6.2,2.2,4.5,1.5,Iris-versicolor
6.4,3.1,5.5,1.8,Iris-virginica
6.0,3.0,4.8,1.8,Iris-virginica

How do I load this data using numpy.loadtxt() so that I get a NumPy array after loading such as [['5.1' '3.5' '1.4' '0.2' 'Iris-setosa'] ['4.9' '3.0' '1.4' '0.2' 'Iris-setosa'] ...]?
I tried
np.loadtxt(open(""data.txt""), 'r',
           dtype={
               'names': (
                   'sepal length', 'sepal width', 'petal length',
                   'petal width', 'label'),
               'formats': (
                   np.float, np.float, np.float, np.float, np.str)},
           delimiter= ',', skiprows=0)

","If you use np.genfromtxt, you could specify dtype=None, which will tell genfromtxt to intelligently guess the dtype of each column. Most conveniently, it relieves you of the burder of specifying the number of bytes required for the string column. (Omitting the number of bytes, by specifying e.g. np.str, does not work.)
In [58]: np.genfromtxt('data.txt', delimiter=',', dtype=None, names=('sepal length', 'sepal width', 'petal length', 'petal width', 'label'))
Out[58]: 
array([(5.1, 3.5, 1.4, 0.2, 'Iris-setosa'),
       (4.9, 3.0, 1.4, 0.2, 'Iris-setosa'),
       (5.8, 2.7, 4.1, 1.0, 'Iris-versicolor'),
       (6.2, 2.2, 4.5, 1.5, 'Iris-versicolor'),
       (6.4, 3.1, 5.5, 1.8, 'Iris-virginica'),
       (6.0, 3.0, 4.8, 1.8, 'Iris-virginica')], 
      dtype=[('sepal_length', '<f8'), ('sepal_width', '<f8'), ('petal_length', '<f8'), ('petal_width', '<f8'), ('label', 'S15')])


If you do want to use np.loadtxt, then to fix your code with minimal changes, you could use:
np.loadtxt(""data.txt"",
   dtype={'names': ('sepal length', 'sepal width', 'petal length', 'petal width', 'label'),
          'formats': (np.float, np.float, np.float, np.float, '|S15')},
   delimiter=',', skiprows=0)

The main difference is simply changing np.str to |S15 (a 15-byte string).
Also note that 
open(""data.txt""), 'r' should be open(""data.txt"", 'r'). But since np.loadtxt can accept a filename, you don't really need to use open at all.
",numpy
how to calculate rolling  moving average using python  numpy  scipy,"There seems to be no function that simply calculates the moving average on numpy/scipy, leading to convoluted solutions.
My question is two-fold:

What's the easiest way to (correctly) implement a moving average with numpy?
Since this seems non-trivial and error prone, is there a good reason not to have the batteries included in this case?

","A simple way to achieve this is by using np.convolve.
The idea behind this is to leverage the way the discrete convolution is computed and use it to return a rolling mean. This can be done by convolving with a sequence of np.ones of a length equal to the sliding window length we want.
In order to do so we could define the following function:
def moving_average(x, w):
    return np.convolve(x, np.ones(w), 'valid') / w

This function will be taking the convolution of the sequence x and a sequence of ones of length w. Note that the chosen mode is valid so that the convolution product is only given for points where the sequences overlap completely.

Some examples:
x = np.array([5,3,8,10,2,1,5,1,0,2])

For a moving average with a window of length 2 we would have:
moving_average(x, 2)
# array([4. , 5.5, 9. , 6. , 1.5, 3. , 3. , 0.5, 1. ])

And for a window of length 4:
moving_average(x, 4)
# array([6.5 , 5.75, 5.25, 4.5 , 2.25, 1.75, 2.  ])


How does convolve work?
Lets have a more in depth look at the way the discrete convolution is being computed.
The following function aims to replicate the way np.convolve is computing the output values:
def mov_avg(x, w):
    for m in range(len(x)-(w-1)):
        yield sum(np.ones(w) * x[m:m+w]) / w 

Which, for the same example above would also yield:
list(mov_avg(x, 2))
# [4.0, 5.5, 9.0, 6.0, 1.5, 3.0, 3.0, 0.5, 1.0]

So what is being done at each step is to take the inner product between the array of ones and the current  window. In this case the multiplication by np.ones(w) is superfluous given that we are directly taking the sum of the sequence.
Bellow is an example of how the first outputs are computed so that it is a little clearer. Lets suppose we want a window of w=4:
[1,1,1,1]
[5,3,8,10,2,1,5,1,0,2]
= (1*5 + 1*3 + 1*8 + 1*10) / w = 6.5

And the following output would be computed as:
  [1,1,1,1]
[5,3,8,10,2,1,5,1,0,2]
= (1*3 + 1*8 + 1*10 + 1*2) / w = 5.75

And so on, returning a moving average of the sequence once all overlaps have been performed.
",numpy
when to use numpyrandomrandn and when numpyrandomrand,"In my deep learning exercise I had to initialize one parameter D1 of same size as A1 so what I did is:
D1 = np.random.randn(A1.shape[0],A1.shape[1]) 

But after computing further equations when I checked the results they didn't matched. Then after proper reading the docs I discovered that they have said to initialize D1 using rand() instead of randn():
D1 = np.random.rand(A1.shape[0],A1.shape[1]) 

But they didn't specified the reason for it as the code is working in both the cases. And also there was a doc for that exercise so I figured out the error, but how, when and why to choose out of these two?
","The difference between rand and randn is (besides the letter n) that rand returns random numbers sampled from a uniform distribution over the interval [0,1), while randn instead samples from a normal (a.k.a. Gaussian) distribution with a mean of 0 and a variance of 1.
In other words, the distribution of the random numbers produced by rand looks like this:

In a uniform distribution, all the random values are restricted to a specific interval, and are evenly distributed over that interval.  If you generate, say, 10000 random numbers with rand, you'll find that about 1000 of them will be between 0 and 0.1, around 1000 will be between 0.1 and 0.2, around 1000 will be between 0.2 and 0.3, and so on.  And all of them will be between 0 and 1 — you won't ever get any outside that range.
Meanwhile, the distribution for randn looks like this:

The first obvious difference between the uniform and the normal distributions is that the normal distribution has no upper or lower limits — if you generate enough random numbers with randn, you'll eventually get one that's as big or as small as you like (well, subject to the limitations of the floating point format used to store the numbers, anyway).  But most of the numbers you'll get will still be fairly close to zero, because the normal distribution is not flat: the output of randn is a lot more likely to fall between, say, 0 and 0.1 than between 0.9 and 1, whereas for rand both of these are equally likely.  In fact, as the picture shows, about 68% of all randn outputs fall between -1 and +1, while 95% fall between -2 and +2, and about 99.7% fall between -3 and +3.
These are completely different probability distributions.  If you switch one for the other, things are almost certainly going to break.  If the code doesn't simply crash, you're almost certainly going to get incorrect and/or nonsensical results.
",numpy
python implementation of statistical sweep operator,"I am learning some techniques for doing statistics with missing data from a book (Statistical Analysis with Missing Data by Little and Rubin). One particularly useful function for working with monotone non-response data is the Sweep Operator (details on page 148-151).  I know that the R module gmm has the swp function which does this but I was wondering if anyone has implemented this function in Python, ideally for Numpy matrices to hold the input data. I searched StackOverflow and also did several web searches without success. Thanks for any help.
Here is the definition.
A PxP symmetric matrix G is said to be swept on row and column k if it is replaced by another symmetric PxP matrix H with elements defined as follows:
    h_kk = -1/g_kk
    h_jk = h_kj = g_jk/g_kk for j != k
    h_jl = g_jl - g_jk g_kl / g_kk j != k, l != k
        
        
    G = [g11, g12, g13
         g12, g22, g23
         g13, g23, g33]   
    H = SWP(1,G) = [-1/g11, g12/g11, g13/g11
                   g12/g11, g22-g12^2/g11, g23-g13*g12/g11
                   g13/g11, g23-g13*g12/g11, g33-g13^2/g11]
    kvec = [k1,k2,k3]
    SWP[kvec,G] = SWP(k1,SWP(k2,SWP(k3,G)))

    Inverse function
    H = RSW(k,G)
    h_kk = -1/g_kk
    h_jk = h_kj = -g_jk/g_kk for j != k
    h_jl = g_jk g_kl / g_kk j != k, l != k    

    G == SWP(k,RSW(k,G)) == RSW(k,SWP(k,G))

","def sweep(g, k):
    g = np.asarray(g)
    n = g.shape[0]
    if g.shape != (n, n):
        raise ValueError('Not a square array')
    if not np.allclose(g - g.T, 0):
        raise ValueError('Not a symmetrical array')
    if k >= n:
        raise ValueError('Not a valid row number')
    #  Fill with the general formula
    h = g - np.outer(g[:, k], g[k, :]) / g[k, k]
    # h = g - g[:, k:k+1] * g[k, :] / g[k, k]
    # Modify the k-th row and column
    h[:, k] = g[:, k] / g[k, k]
    h[k, :] = h[:, k]
    # Modify the pivot
    h[k, k] = -1 / g[k, k]
    return h

I have no way of testing the above code, but I found an alternativee description here, which is valid for non-symmetrical matrices, which can be calculated as follows:
def sweep_non_sym(a, k):
    a = np.asarray(a)
    n = a.shape[0]
    if a.shape != (n, n):
        raise ValueError('Not a square array')
    if k >= n:
        raise ValueError('Not a valid row number')
    #  Fill with the general formula
    b = a - np.outer(a[:, k], a[k, :]) / a[k, k]
    # b = a - a[:, k:k+1] * a[k, :] / a[k, k]
    # Modify the k-th row and column
    b[k, :] = a[k, :] / a[k, k]
    b[:, k] = -a[:, k] / a[k, k]
    # Modify the pivot
    b[k, k] = 1 / a[k, k]
    return b

This one does give the correct results for the examples in that link:
>>> a = [[2,4],[3,1]]
>>> sweep_non_sym(a, 0)
array([[ 0.5,  2. ],
       [-1.5, -5. ]])
>>> sweep_non_sym(sweep_non_sym(a, 0), 1)
array([[-0.1,  0.4],
       [ 0.3, -0.2]])
>>> np.dot(a, sweep_non_sym(sweep_non_sym(a, 0), 1))
array([[  1.00000000e+00,   0.00000000e+00],
       [  5.55111512e-17,   1.00000000e+00]])

",numpy
newton raphson fails to approximate beyond two decimal points,"I've written a program which finds roots of a two function system using the Newton Raphson method. Its goal is to solve for two unknowns of a closed vector loop (sketches below). The issue is that it only works when the error tolerance is 0.01 and greater, in other cases (for example, when it's 0.001) it falls into an oscillation and seemingly never converges. I find this behaviour odd, since the fact that it succeeds in getting to 0.01 precision should prove its ability to land a guess inside the convergence radius, and from there on further approximation shouldn't face any problems...
import numpy as np
import random as rnd

TOLERANCE = 0.01
ITER_LIMIT = 50

track_x = 0
track_y = 0
track_incline = np.deg2rad(60)

r41 = 12.2/2
r1 = 28.88 - 10.95
r2 = 22.65 - track_x
r26 = 8/2 - track_y
r3 = 8
r4 = 12.2

theta41 = np.pi
theta3 = np.deg2rad(30)

def f1(x, y):
    return r41*np.sin(theta41) + r1*np.cos(theta41) + r2*np.sin(y) + r26*np.cos(y) - x*np.sin(y + track_incline) + r3*np.sin(theta3)

def f2(x, y):
    return r41*np.cos(theta41) - r1*np.sin(theta41) + r2*np.cos(y) - r26*np.sin(y) - x*np.cos(y + track_incline) + r3*np.cos(theta3) + r4

def jacobian_mat(x, y):
    return np.array([
        [-np.sin(y + track_incline), r2*np.cos(y) - r26*np.sin(y) - x*np.cos(y + track_incline)],
        [-np.cos(y + track_incline), -r2*np.sin(y) - r26*np.cos(y) + x*np.sin(y + track_incline)]
    ])

def compute_newton_raphson(x0, y0):
    for i in range(ITER_LIMIT):
        result = -np.matmul(np.array([f1(x0, y0), f2(x0, y0)]), np.linalg.inv(jacobian_mat(x0, y0))) + np.array([x0, y0])
        x = result[0]
        y = result[1]

        print(x, y)

        if max(np.abs(x - x0), np.abs(y - y0)) <= TOLERANCE:
            return (x, y)
        else:
            x0 = x
            y0 = y

    return None

while True:
    x0 = rnd.randrange(-100, 100)
    y0 = rnd.randrange(-100, 100)

    res = compute_newton_raphson(x0, y0)
    if res != None:
        print(f""RESULT:\nX = {res[0]}mm\nY = {np.rad2deg(res[1])}deg"")
        break

Here's a sketch of the problem that I'm modeling this program for, just in case it proves useful.
Angles
I had zero success playing around with the iteration count. Also double checked if f1 and f2 are properly constructed.
Any insight is greatly appreciated!
","Your Newton-Raphson formula is wrong.
Newton-Raphson says:
Xₙ₊₁ = Xₙ - J⁻¹F(Xₙ)
You are computing
Xₙ₊₁ = Xₙ - F(Xₙ)ᵀJ⁻¹
That is, equivalent of
Xₙ₊₁ = Xₙ - (J⁻¹)ᵀF(Xₙ)
Matrix multiplication is not commutative.
And matmul, or dot or @ have different interpretation of 1D-array, depending on where they are in the operation
See
M=np.array([[1,2],[3,4]])
X=np.array([5,6])
Col=np.array([[5],[6]]) # Same as X, as a column (a 2D array of 1 column)

Then
M@X # or np.dot(M,X) or np.matmult(M,X)
#[17,39]
# That is same as M@Col, but in 1D. So same as (M@Col).flatten()

X@M # or np.dot(X,M) or np.matmul(X,M)
#[23,34]
# That is same as Col.T@M, but in 1D
# Note that Col@M wouldn't work (you can't multiply a column by a matrix)

I insist on this column-matrix vs 1d-vector, because it explains why it worked in the first place. In maths, in matrix operation, we usually represent vectors as columns. So MX makes sense. XM doesn't even make sense: you can't multiply a 2×1 matrix by a 2×2 matrix.
The closest thing to multiplying (on the left) a vector by a matrix would be XᵀM, that is X as a row, (Col.T in previous code) by a matrix.
And XᵀM is the same as (MᵀX)ᵀ.
So what you are computing is Xₙ-((J⁻¹)ᵀF(Xₙ))ᵀ
which shouldn't make sense, since that is a column minus a row. But since X, and ((J⁻¹)ᵀF(Xₙ)) are 1D array anyway, the implicit ᵀ (the last one) is void. But the one on Mᵀ is still there and pretty important.
So, strictly speaking, in math, what you compute doesn't make sense.
But numpy operation (matmul, dot, @ — with some variants, but irrelevant in this case) does allow to combine 2D array (matrix) with 1D array. And when you do so, the 1D-array is, interpreted as a column or as a row, depending on what makes sense.
Reason why your code works (I mean: why it doesn't raise an error), ignoring the last implicit ᵀ. But still, what you compute is what you would get if you transposed the inverse of the jacobian matrix before using Newton-Raphson
Additional notes
There are other problems in your code. That are not the source of your problem. But could be, depending on the parameters.

y is obviously an angle. In radians. Why choose it between -100 and 100?

for both x and y, why use integer initial values? For x, you may say that if x normal range is between -100 and 100, then there is probably a integer starting point that is not so far (compared the whole range width) from the solution.


For for y, once corrected the 1st point, that means that you start only from -3, -2, -1, 0, 1, 2, or 3. If all of those starting points are too far from the solution for Newton-Raphson to converge, then you could try for ever... (fortunately, it is not the case. Plus, the two errors compensates a bit. Since 2π is, famously, not a rational, choosing a integer between -100 and 100 is equivalent to choosing among 200 different values between -π and π. for example 7 is equivalent to 0.7168...)
But, well, the idea behind your infinite while loop is to retry from another starting point if the iteration do not converge, until you find a starting point from which it does. So it is not a very good idea to start from a very finite set of possible starting points

It is not a good idea to solve J@(Xₙ₊₁-Xₙ) = - F(Xₙ) by inverting the jacobian. It is better to use np.solve (as Ali did in his answer) to do so (but, also, if we start trying not to do too much of numpy's job, then, probably, a better idea would be to use scipy.newton in the first place)

Rather than a short number of iterations, I would watch for absolute value of x0/y0 to detect divergence. If grows too much, you can stop the iteration and retry from a random value. And then, you can have a insanely big value for ITER_LIMIT since it should never be used (either the iteration converge and you are ok. Or it diverges, and you'll detect it. Or, very unlikely, it wander randomly but within a reasonible range, and, after all, that is the same as your random. The insanely big ITER_LIMIT role would be to ensure the iterations are stopped for the offside chance you end up in a stable cycle)


",numpy
trying to fix a numpy asscalar deprecation issue,"While trying to update an old Python script I ran into the following error:
module 'numpy' has no attribute 'asscalar'. Did you mean: 'isscalar'?

Specifically:
def calibrate(x, y, z):
  # H = numpy.array([x, y, z, -y**2, -z**2, numpy.ones([len(x), 1])])
  H = numpy.array([x, y, z, -y**2, -z**2, numpy.ones([len(x)])])
  H = numpy.transpose(H)
  w = x**2
  
  (X, residues, rank, shape) = linalg.lstsq(H, w)
  
  OSx = X[0] / 2
  OSy = X[1] / (2 * X[3])
  OSz = X[2] / (2 * X[4])
  
  A = X[5] + OSx**2 + X[3] * OSy**2 + X[4] * OSz**2
  B = A / X[3]
  C = A / X[4]
  
  SCx = numpy.sqrt(A)
  SCy = numpy.sqrt(B)
  SCz = numpy.sqrt(C)
  
  # type conversion from numpy.float64 to standard python floats
  offsets = [OSx, OSy, OSz]
  scale = [SCx, SCy, SCz]
  
  offsets = map(numpy.asscalar, offsets)
  scale = map(numpy.asscalar, scale)
  
  return (offsets, scale)

I found that asscalar has been deprecated since NumPy 1.16. I found one reference that said to use numpy.ndarray.item, but I have no clue how to do that.
I did try this:
offsets = map.item(offsets)
scale = map.item( scale)

but got this error:
AttributeError: type object 'map' has no attribute 'item'

How can I solve this?
","Just replace numpy.scalar using numpy.ndarray.item, that is, change
offsets = map(numpy.asscalar, offsets)
scale = map(numpy.asscalar, scale)

to
offsets = map(numpy.ndarray.item, offsets)
scale = map(numpy.ndarray.item, scale)

",numpy
replace all nan in a 2d array by the nanmean value of adjacent neighboring cells,"I have a 2D array witch include some nan and I would like do the following :

replace each nan in the array by the nanmean() value of adjacent neighboring cells.
if the neighboring cells are all nan, then the value is nan
if there are some nan in the neighboring cells then the value is the nanmean() value of adjacent neighboring cells.
Let's take some examples :

a = np.random.randint(0,100,60).astype(float)
a = a.reshape(6,10) 
a[1,1]= np.nan
a[2:5,4:7]=np.nan
a
array([[48., 84., 80., 59., 43., 60., 31., 37.,  4., 75.],
       [83., nan, 52., 34., 95., 15., 69.,  7.,  7., 16.],
       [10.,  6.,  6., 44., nan, nan, nan, 95., 28.,  4.],
       [12.,  1., 62., 96., nan, nan, nan, 66., 21., 80.],
       [41., 18.,  1., 49., nan, nan, nan, 27., 64., nan],
       [13., 33., 98., 85., 77., 20., 73., 57., 15., 28.]])

# In this array a[1,1] is a nan and should be replaced by 46.125
np.nanmean(np.array([48.,84.,80.,52.,6.,6.,10.,83.]))
46.125

# Another example a[2,4] is a nan and should be replaced by 56.8
np.nanmean(np.array([34.,95.,15.,44.,96.]))
56.8

etc.


So far I have tried the following techniques :

Using some loop : very easy but very slow if the array is big (1000 x 1000)
Scipy NearestNDInterpolator, interesting but replace all nan (nans surrounded by nans must remain nans!)

How to do this in the fastest way, without looping ?
","Use a 2D convolution (scipy.signal.convolve2d):
from scipy.signal import convolve2d

m = np.isnan(a)
kernel = np.ones((3, 3))

# sum of values
S1 = convolve2d(np.nan_to_num(a), kernel, mode='same')
# count of non-NaNs
S2 = convolve2d(~m, kernel, mode='same')

# optional: update the mask to ensure excluding cells
# fully surrounded with NaNs:
m &= S2!=0

# replace NaNs with mean (sum/count)
a[m] = S1[m]/S2[m]

Updated a:
array([[48.  , 84.  , 80.  , 59.  , 43.  , 60.  , 31.  , 37.  ,  4.  , 75.  ],
       [83.  , 46.12, 52.  , 34.  , 95.  , 15.  , 69.  ,  7.  ,  7.  , 16.  ],
       [10.  ,  6.  ,  6.  , 44.  , 56.8 , 59.67, 50.4 , 95.  , 28.  ,  4.  ],
       [12.  ,  1.  , 62.  , 96.  , 63.  ,   nan, 62.67, 66.  , 21.  , 80.  ],
       [41.  , 18.  ,  1.  , 49.  , 65.4 , 56.67, 48.6 , 27.  , 64.  , 41.6 ],
       [13.  , 33.  , 98.  , 85.  , 77.  , 20.  , 73.  , 57.  , 15.  , 28.  ]])

Intermediates:
# m
array([[False, False, False, False, False, False, False, False, False, False],
       [False,  True, False, False, False, False, False, False, False, False],
       [False, False, False, False,  True,  True,  True, False, False, False],
       [False, False, False, False,  True,  True,  True, False, False, False],
       [False, False, False, False,  True,  True,  True, False, False,  True],
       [False, False, False, False, False, False, False, False, False, False]])

# S1
array([[215., 347., 309., 363., 306., 313., 219., 155., 146., 102.],
       [231., 369., 365., 413., 350., 313., 314., 278., 273., 134.],
       [112., 232., 301., 389., 284., 179., 252., 293., 324., 156.],
       [ 88., 157., 283., 258., 189.,   0., 188., 301., 385., 197.],
       [118., 279., 443., 468., 327., 170., 243., 323., 358., 208.],
       [105., 204., 284., 310., 231., 170., 177., 236., 191., 107.]])

# S2
array([[3., 5., 5., 6., 6., 6., 6., 6., 6., 4.],
       [5., 8., 8., 8., 7., 6., 7., 8., 9., 6.],
       [5., 8., 8., 7., 5., 3., 5., 7., 9., 6.],
       [6., 9., 9., 6., 3., 0., 3., 6., 8., 5.],
       [6., 9., 9., 7., 5., 3., 5., 7., 8., 5.],
       [4., 6., 6., 5., 4., 3., 4., 5., 5., 3.]])

",numpy
how to calculate a gaussian kernel matrix efficiently in numpy,"def GaussianMatrix(X,sigma):
    row,col=X.shape
    GassMatrix=np.zeros(shape=(row,row))
    X=np.asarray(X)
    i=0
    for v_i in X:
        j=0
        for v_j in X:
            GassMatrix[i,j]=Gaussian(v_i.T,v_j.T,sigma)
            j+=1
        i+=1
    return GassMatrix
def Gaussian(x,z,sigma):
    return np.exp((-(np.linalg.norm(x-z)**2))/(2*sigma**2))

This is my current way. Is there any way I can use matrix operation to do this? X is the data points.
","Do you want to use the Gaussian kernel for e.g. image smoothing? If so, there's a function gaussian_filter() in scipy:
Updated answer
This should work - while it's still not 100% accurate, it attempts to account for the probability mass within each cell of the grid. I think that using the probability density at the midpoint of each cell is slightly less accurate, especially for small kernels. See https://homepages.inf.ed.ac.uk/rbf/HIPR2/gsmooth.htm for an example.
import numpy as np
import scipy.stats as st

def gkern(kernlen=21, nsig=3):
    """"""Returns a 2D Gaussian kernel.""""""

    x = np.linspace(-nsig, nsig, kernlen+1)
    kern1d = np.diff(st.norm.cdf(x))
    kern2d = np.outer(kern1d, kern1d)
    return kern2d/kern2d.sum()

Testing it on the example in Figure 3 from the link:
gkern(5, 2.5)*273

gives
array([[ 1.0278445 ,  4.10018648,  6.49510362,  4.10018648,  1.0278445 ],
       [ 4.10018648, 16.35610171, 25.90969361, 16.35610171,  4.10018648],
       [ 6.49510362, 25.90969361, 41.0435344 , 25.90969361,  6.49510362],
       [ 4.10018648, 16.35610171, 25.90969361, 16.35610171,  4.10018648],
       [ 1.0278445 ,  4.10018648,  6.49510362,  4.10018648,  1.0278445 ]])

The original (accepted) answer below accepted is wrong
The square root is unnecessary, and the definition of the interval is incorrect.
import numpy as np
import scipy.stats as st

def gkern(kernlen=21, nsig=3):
    """"""Returns a 2D Gaussian kernel array.""""""

    interval = (2*nsig+1.)/(kernlen)
    x = np.linspace(-nsig-interval/2., nsig+interval/2., kernlen+1)
    kern1d = np.diff(st.norm.cdf(x))
    kernel_raw = np.sqrt(np.outer(kern1d, kern1d))
    kernel = kernel_raw/kernel_raw.sum()
    return kernel

",numpy
how to convert matrix to block matrix using numpy,"Say I have a matrix like
Matrix = [[A11, A12, A13, A14], [A21, A22, A23, A24], [A31, A32, A33, A34], [A41, A42, A43, A44]],
and suppose I want to convert it to a block matrix
[[A,B], [C,D]],
where
A = [[A11, A12], [A21, A22]] B = [[A13, A14], [A23, A24]] C = [[A31, A32], [A41, A42]] D = [[A33, A34], [A43, A44]].
What do I need to type to quickly extract the matrices A, B, C, and D?
","Without using loops, you can reshape your array (and reorder the dimensions with moveaxis):
A, B, C, D = np.moveaxis(Matrix.reshape((2,2,2,2)), 1, 2).reshape(-1, 2, 2)

Or:
(A, B), (C, D) = np.moveaxis(Matrix.reshape((2,2,2,2)), 1, 2)

For a generic answer on an arbitrary shape:
x, y = Matrix.shape
(A, B), (C, D) = np.moveaxis(Matrix.reshape((2, x//2, 2, y//2)), 1, 2)

Output:
# A
array([['A11', 'A12'],
       ['A21', 'A22']], dtype='<U3')
# B
array([['A13', 'A14'],
       ['A23', 'A24']], dtype='<U3')
# C
array([['A31', 'A32'],
       ['A41', 'A42']], dtype='<U3')
# D
array([['A33', 'A34'],
       ['A43', 'A44']], dtype='<U3')

",numpy
how to get only the first occurrence of each increasing value in numpy array,"While working on first-passage probabilities, I encountered this problem. I want to find a NumPythonic way (without explicit loops) to leave only the first occurrence of strictly increasing values in each row of a numpy array, while replacing repeated or non-increasing values with zeros. For instance, if
arr = np.array([
    [1, 1, 1, 2, 2, 2, 3, 3, 3, 4, 4, 5, 5],
    [1, 1, 2, 2, 2, 3, 2, 2, 3, 3, 3, 4, 4],
    [3, 2, 1, 2, 1, 1, 2, 3, 4, 5, 4, 3, 2]])

I would like to get as output:
out = np.array([
    [1, 0, 0, 2, 0, 0, 3, 0, 0, 4, 0, 5, 0],
    [1, 0, 2, 0, 0, 3, 0, 0, 0, 0, 0, 4, 0],
    [3, 0, 0, 0, 0, 0, 0, 0, 4, 5, 0, 0, 0]])

","Maximum can be accumulated per-row:
>>> arr
array([[1, 1, 1, 2, 2, 2, 3, 3, 3, 4, 4, 5, 5],
       [1, 1, 2, 2, 2, 3, 2, 2, 3, 3, 3, 4, 4],
       [3, 2, 1, 2, 1, 1, 2, 3, 4, 5, 4, 3, 2]])
>>> np.maximum.accumulate(arr, axis=1)
array([[1, 1, 1, 2, 2, 2, 3, 3, 3, 4, 4, 5, 5],
       [1, 1, 2, 2, 2, 3, 3, 3, 3, 3, 3, 4, 4],
       [3, 3, 3, 3, 3, 3, 3, 3, 4, 5, 5, 5, 5]])

Then you can easily mask out non-increasing values:
>>> m_arr = np.maximum.accumulate(arr, axis=1)
>>> np.where(np.diff(m_arr, axis=1, prepend=0), arr, 0)
array([[1, 0, 0, 2, 0, 0, 3, 0, 0, 4, 0, 5, 0],
       [1, 0, 2, 0, 0, 3, 0, 0, 0, 0, 0, 4, 0],
       [3, 0, 0, 0, 0, 0, 0, 0, 4, 5, 0, 0, 0]])

",numpy
why keep numpy runtimewarning,"Here is a sample data, even there is no negative or np.nan, it still show error message:
Data:
   gvkey  sale  ebit
4   1000  44.8  16.8
5   1000  53.2  11.5
6   1000  42.9   6.2
7   1000  42.4   0.9
8   1000  44.2   5.3
9   1000  51.9   9.7

Function:
def calculate_ln_values(df):
    conditions_ebit = [
        df['ebit'] >= 0.0,
        df['ebit'] <  0.0
    ]
    choices_ebit = [
        np.log(1 + df['ebit']),
        np.log(1 - df['ebit']) * -1
    ]
    df['lnebit'] = np.select(conditions_ebit, choices_ebit, default=np.nan)
    
    conditions_sale = [
        df['sale'] >= 0.0,
        df['sale'] <  0.0
    ]
    choices_sale = [
        np.log(1 + df['sale']),
        np.log(1 - df['sale']) * -1
    ]
    df['lnsale'] = np.select(conditions_sale, choices_sale, default=np.nan)
    return df

Run
calculate_ln_values(data)

Error Warning:
C:\Users\quoc\anaconda3\envs\uhart\Lib\site-packages\pandas\core\arraylike.py:399: RuntimeWarning: invalid value encountered in log
  result = getattr(ufunc, method)(*inputs, **kwargs)
C:\Users\quoc\anaconda3\envs\uhart\Lib\site-packages\pandas\core\arraylike.py:399: RuntimeWarning: invalid value encountered in log
  result = getattr(ufunc, method)(*inputs, **kwargs)

I would very appreciate if someone could help me this issue
---- Edit: reply to Answer of @Emi OB and @Quang Hoang: ---------------
The formula as in the paper is:

ln(1+EBIT) if EBIT ≥ 0
-ln(1-EBIT) if EBIT < 0
so my code:
np.log(1 + df['ebit']),
np.log(1 - df['ebit']) * -1

follows the paper.
The part np.log(1 - df['ebit']) is impossible to be negative since it fall under the condition of ebit < 0.
","The problem is in this block of code:
    choices_ebit = [
        np.log(1 + df['ebit']),
        np.log(1 - df['ebit']) * -1
    ]

Here, you are calculating both formulas, for when ebit is positive and when it's negative, and storing them in choices_ebit. However, when ebit>=1, the second one will give you the runtime warning, and when ebit<=-1, the first one will give your the runtime warning.
In order to avoid calculating both formulas, you can factor them out into one with abs() on the one hand, and np.sign() on the other:
    df['lnebit'] = np.log(1 + df['ebit'].abs()) * np.sign(df['ebit'])

This meets your requirements:

when ebit>=0, sign(ebit) == 1 and abs(ebit) == ebit, so that resolves to log(1+ebit)
when ebit<=, sign(ebit) == -1 and abs(ebit) == -ebit, so that resolves to -log(1-ebit)

",numpy
the requested array has an inhomogeneous shape after 1 dimensions the detected shape was 33  inhomogeneous part,"I stuck in convert list to numpy. Convert list size is (33, n, 428). N is randomly difference that I don't know how numbers are consist. Here is error.
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
C:\Users\HILAB_~1\AppData\Local\Temp/ipykernel_22960/872733971.py in <module>
----> 1 X_train = np.array(X_train, dtype=np.float64)
      2 
      3 for epoch in range(EPOCH):
      4     X_train_ten, y_train_ten = Variable(torch.from_numpy(X_train)), Variable(torch.tensor(y_train, dtype=torch.float32, requires_grad=True))
      5     print(X_train_ten.size())

ValueError: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (33,) + inhomogeneous part.

and problem code is here.
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.3, random_state=42, shuffle=True
)

print(""[SIZE]\t\tTrain X size : {}, Train y size : {}\n\t\tTest X size : {}, Test y size : {}""\
        .format(len(X_train), len(y_train), len(X_test), len(y_test)))

train_dataloadloader = DataLoader(X_train)
test_dataloader = DataLoader(X_test)

X_train = np.array(X_train, dtype=np.float64)

I can't understand what does error means. Please help. thanks :D
","It means that whatever sequences X contains, they are not of the same length. You can check {len(e) for e in X); this is the set of all different lengths found in X.
Consider the following example:
>>> import numpy as np
>>> x = [[1, 2], [2, 3, 4]]
>>> np.array(x, dtype=np.float64)
[...]
ValueError: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (2,) + inhomogeneous part.

Here, the list x contains two other lists, one of length 2 and the other of length 3. They can't be combined into one array since the ""column"" dimension doesn't match.
",numpy
valueerror percolumn arrays must each be 1dimensional when trying to create a pandas dataframe from a dictionary why,"I'm trying to create a very simple Pandas DataFrame from a dictionary. The dictionary has 3 items, and the DataFrame as well. They are:

a list with the 'shape' (3,)
a list/np.array (in different attempts) with the shape(3, 3)
a constant of 100 (same value to the whole column)


Here is the code that succeeds and displays the preferred df

​
# from a dicitionary
>>>dict1 = {""x"": [1, 2, 3],
...         ""y"": list(
...             [
...                 [2, 4, 6], 
...                 [3, 6, 9], 
...                 [4, 8, 12]
...             ]
...             ),
...         ""z"": 100}

>>>df1 = pd.DataFrame(dict1)
>>>df1
   x           y    z
0  1   [2, 4, 6]  100
1  2   [3, 6, 9]  100
2  3  [4, 8, 12]  100


But then I assign a Numpy ndarray (shape 3, 3 )to the key y, and try to create a DataFrame from the dictionary. The line I try to create the DataFrame errors out. Below is the code I try to run, and the error I get (in separate code blocks for ease of reading.)


code

​
>>>dict2 = {""x"": [1, 2, 3],
...         ""y"": np.array(
...             [
...                 [2, 4, 6], 
...                 [3, 6, 9], 
...                 [4, 8, 12]
...             ]
...             ),
...         ""z"": 100}

>>>df2 = pd.DataFrame(dict2)  # see the below block for error


error

​
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
d:\studies\compsci\pyscripts\study\pandas-realpython\data-delightful\01.intro.ipynb Cell 10' in <module>
      1 # from a dicitionary
      2 dict1 = {""x"": [1, 2, 3],
      3          ""y"": np.array(
      4              [
   (...)
      9              ),
     10          ""z"": 100}
---> 12 df1 = pd.DataFrame(dict1)

File ~\anaconda3\envs\dst\lib\site-packages\pandas\core\frame.py:636, in DataFrame.__init__(self, data, index, columns, dtype, copy)
    630     mgr = self._init_mgr(
    631         data, axes={""index"": index, ""columns"": columns}, dtype=dtype, copy=copy
    632     )
    634 elif isinstance(data, dict):
    635     # GH#38939 de facto copy defaults to False only in non-dict cases
--> 636     mgr = dict_to_mgr(data, index, columns, dtype=dtype, copy=copy, typ=manager)
    637 elif isinstance(data, ma.MaskedArray):
    638     import numpy.ma.mrecords as mrecords

File ~\anaconda3\envs\dst\lib\site-packages\pandas\core\internals\construction.py:502, in dict_to_mgr(data, index, columns, dtype, typ, copy)
    494     arrays = [
    495         x
    496         if not hasattr(x, ""dtype"") or not isinstance(x.dtype, ExtensionDtype)
    497         else x.copy()
    498         for x in arrays
    499     ]
    500     # TODO: can we get rid of the dt64tz special case above?
--> 502 return arrays_to_mgr(arrays, columns, index, dtype=dtype, typ=typ, consolidate=copy)

File ~\anaconda3\envs\dst\lib\site-packages\pandas\core\internals\construction.py:120, in arrays_to_mgr(arrays, columns, index, dtype, verify_integrity, typ, consolidate)
    117 if verify_integrity:
    118     # figure out the index, if necessary
    119     if index is None:
--> 120         index = _extract_index(arrays)
    121     else:
    122         index = ensure_index(index)

File ~\anaconda3\envs\dst\lib\site-packages\pandas\core\internals\construction.py:661, in _extract_index(data)
    659         raw_lengths.append(len(val))
    660     elif isinstance(val, np.ndarray) and val.ndim > 1:
--> 661         raise ValueError(""Per-column arrays must each be 1-dimensional"")
    663 if not indexes and not raw_lengths:
    664     raise ValueError(""If using all scalar values, you must pass an index"")

ValueError: Per-column arrays must each be 1-dimensional

Why is it ending in error like that in the second attempt, even though the dimensions of both arrays are the same? What is a workaround for this issue?
","If you look closer at the error message and quick look at the source code here:
    elif isinstance(val, np.ndarray) and val.ndim > 1:
        raise ValueError(""Per-column arrays must each be 1-dimensional"")

You will find that if the dictionay value is a numpy array and has more than one dimension as your example, it throws an error based on the source code. Therefore, it works very well with list because a list has no more than one dimension even if it is a list of list.
lst = [[1,2,3],[4,5,6],[7,8,9]]
len(lst) # print 3 elements or (3,) not (3,3) like numpy array.

You can try to use np.array([1,2,3]), it will work because number of dimensions is 1 and try:
arr = np.array([1,2,3])
print(arr.ndim)  # output is 1

If it is necessary to use numpy array inside a dictionary, you can use .tolist() to convert numpy array to a list.
",numpy
typeerror ufunc 39isnan39 not supported for the input types and the inputs could not be safely coerced,"I am trying to convert a CSV into numpy array. In the numpy array, I am replacing few elements with NaN. Then, I wanted to find the indices of the NaN elements in the numpy array. The code is:
import pandas as pd
import matplotlib.pyplot as plyt
import numpy as np

filename = 'wether.csv'

df = pd.read_csv(filename,header = None )

list = df.values.tolist()
labels = list[0]
wether_list = list[1:]

year = []
month = []
day = []
max_temp = []

for i in wether_list:
    year.append(i[1])
    month.append(i[2])
    day.append(i[3])
    max_temp.append(i[5])

mid = len(max_temp) // 2
temps = np.array(max_temp[mid:])
temps[np.where(np.array(temps) == -99.9)] = np.nan
plyt.plot(temps,marker = '.',color = 'black',linestyle = 'none')
# plyt.show()

print(np.where(np.isnan(temps))[0])
# print(len(pd.isnull(np.array(temps))))

When I execute this, I am getting a warning and an error. The warning is:
    wether.py:26: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison
  temps[np.where(np.array(temps) == -99.9)] = np.nan

The error is:
Traceback (most recent call last):
  File ""wether.py"", line 30, in <module>
    print(np.where(np.isnan(temps))[0])
TypeError: ufunc 'isnan' not supported for the input types, and the inputs could not be safely coerced to any supported types according to the casting rule ''safe''

This is a part of the dataset which I am using:
83168,2014,9,7,0.00000,89.00000,78.00000, 83.50000
83168,2014,9,22,1.62000,90.00000,72.00000, 81.00000
83168,2014,9,23,0.50000,87.00000,74.00000, 80.50000
83168,2014,9,24,0.35000,82.00000,73.00000, 77.50000
83168,2014,9,25,0.60000,85.00000,75.00000, 80.00000
83168,2014,9,26,0.76000,89.00000,77.00000, 83.00000
83168,2014,9,27,0.00000,89.00000,79.00000, 84.00000
83168,2014,9,28,0.00000,90.00000,81.00000, 85.50000
83168,2014,9,29,0.00000,90.00000,79.00000, 84.50000
83168,2014,9,30,0.50000,89.00000,75.00000, 82.00000
83168,2014,10,1,0.02000,91.00000,75.00000, 83.00000
83168,2014,10,2,0.03000,93.00000,77.00000, 85.00000
83168,2014,10,3,1.40000,93.00000,75.00000, 84.00000
83168,2014,10,4,0.06000,89.00000,75.00000, 82.00000
83168,2014,10,5,0.22000,91.00000,68.00000, 79.50000
83168,2014,10,6,0.00000,84.00000,68.00000, 76.00000
83168,2014,10,7,0.17000,85.00000,73.00000, 79.00000
83168,2014,10,8,0.06000,84.00000,73.00000, 78.50000
83168,2014,10,9,0.00000,87.00000,73.00000, 80.00000
83168,2014,10,10,0.00000,88.00000,80.00000, 84.00000
83168,2014,10,11,0.00000,87.00000,80.00000, 83.50000
83168,2014,10,12,0.00000,88.00000,80.00000, 84.00000
83168,2014,10,13,0.00000,88.00000,81.00000, 84.50000
83168,2014,10,14,0.04000,88.00000,77.00000, 82.50000
83168,2014,10,15,0.00000,88.00000,77.00000, 82.50000
83168,2014,10,16,0.09000,89.00000,72.00000, 80.50000
83168,2014,10,17,0.00000,85.00000,67.00000, 76.00000
83168,2014,10,18,0.00000,84.00000,65.00000, 74.50000
83168,2014,10,19,0.00000,84.00000,65.00000, 74.50000
83168,2014,10,20,0.00000,85.00000,69.00000, 77.00000
83168,2014,10,21,0.77000,87.00000,76.00000, 81.50000
83168,2014,10,22,0.69000,81.00000,71.00000, 76.00000
83168,2014,10,23,0.31000,82.00000,72.00000, 77.00000
83168,2014,10,24,0.71000,79.00000,73.00000, 76.00000
83168,2014,10,25,0.00000,81.00000,68.00000, 74.50000
83168,2014,10,26,0.00000,82.00000,67.00000, 74.50000
83168,2014,10,27,0.00000,83.00000,64.00000, 73.50000
83168,2014,10,28,0.00000,83.00000,66.00000, 74.50000
83168,2014,10,29,0.03000,86.00000,76.00000, 81.00000
83168,2014,10,30,0.00000,85.00000,69.00000, 77.00000
83168,2014,10,31,0.00000,85.00000,69.00000, 77.00000
83168,2014,11,1,0.00000,86.00000,59.00000, 72.50000
83168,2014,11,2,0.00000,77.00000,52.00000, 64.50000
83168,2014,11,3,0.00000,70.00000,52.00000, 61.00000
83168,2014,11,4,0.00000,77.00000,59.00000, 68.00000
83168,2014,11,5,0.02000,79.00000,73.00000, 76.00000
83168,2014,11,6,0.02000,82.00000,75.00000, 78.50000
83168,2014,11,7,0.00000,83.00000,66.00000, 74.50000
83168,2014,11,8,0.00000,84.00000,65.00000, 74.50000
83168,2014,11,9,0.00000,84.00000,65.00000, 74.50000
83168,2014,11,10,1.20000,72.00000,65.00000, 68.50000
83168,2014,11,11,0.08000,77.00000,61.00000, 69.00000
83168,2014,11,12,0.00000,80.00000,61.00000, 70.50000
83168,2014,11,13,0.00000,83.00000,63.00000, 73.00000
83168,2014,11,14,0.00000,83.00000,65.00000, 74.00000
83168,2014,11,15,0.00000,82.00000,64.00000, 73.00000
83168,2014,11,16,0.00000,83.00000,64.00000, 73.50000
83168,2014,11,17,0.07000,84.00000,64.00000, 74.00000
83168,2014,11,18,0.00000,86.00000,71.00000, 78.50000
83168,2014,11,19,0.57000,78.00000,55.00000, 66.50000
83168,2014,11,20,0.05000,72.00000,56.00000, 64.00000
83168,2014,11,21,0.05000,77.00000,63.00000, 70.00000
83168,2014,11,22,0.22000,77.00000,69.00000, 73.00000
83168,2014,11,23,0.06000,79.00000,76.00000, 77.50000
83168,2014,11,24,0.02000,84.00000,78.00000, 81.00000
83168,2014,11,25,0.00000,86.00000,78.00000, 82.00000
83168,2014,11,26,0.07000,85.00000,77.00000, 81.00000
83168,2014,11,27,0.21000,82.00000,55.00000, 68.50000
83168,2014,11,28,0.00000,73.00000,53.00000, 63.00000
83168,2015,1,8,0.00000,80.00000,57.00000,
83168,2015,1,9,0.05000,72.00000,56.00000,
83168,2015,1,10,0.00000,72.00000,57.00000,
83168,2015,1,11,0.00000,80.00000,57.00000,
83168,2015,1,12,0.05000,80.00000,59.00000,
83168,2015,1,13,0.85000,81.00000,69.00000,
83168,2015,1,14,0.05000,81.00000,68.00000,
83168,2015,1,15,0.00000,81.00000,64.00000,
83168,2015,1,16,0.00000,78.00000,63.00000,
83168,2015,1,17,0.00000,73.00000,55.00000,
83168,2015,1,18,0.00000,76.00000,55.00000,
83168,2015,1,19,0.00000,78.00000,55.00000,
83168,2015,1,20,0.00000,75.00000,56.00000,
83168,2015,1,21,0.02000,73.00000,65.00000,
83168,2015,1,22,0.00000,80.00000,64.00000,
83168,2015,1,23,0.00000,80.00000,71.00000,
83168,2015,1,24,0.00000,79.00000,72.00000,
83168,2015,1,25,0.00000,79.00000,49.00000,
83168,2015,1,26,0.00000,79.00000,49.00000,
83168,2015,1,27,0.10000,75.00000,53.00000,
83168,2015,1,28,0.00000,68.00000,53.00000,
83168,2015,1,29,0.00000,69.00000,53.00000,
83168,2015,1,30,0.00000,72.00000,60.00000,
83168,2015,1,31,0.00000,76.00000,58.00000,
83168,2015,2,1,0.00000,76.00000,58.00000,
83168,2015,2,2,0.05000,77.00000,58.00000,
83168,2015,2,3,0.00000,84.00000,56.00000,
83168,2015,2,4,0.00000,76.00000,56.00000,

I am unable to rectify the error. How to overcome the warning in the 26th line? How can one solve this error?
Update :
when I try the same thing in different way like reading dataset from file instead of converting to dataframes, I am not getting the error. What would be the reason for that? The code is:
weather_filename = 'wether.csv'
weather_file = open(weather_filename)
weather_data = weather_file.read()
weather_file.close()

# Break the weather records into lines
lines = weather_data.split('\n')
labels = lines[0]
values = lines[1:]
n_values = len(values)

# Break the list of comma-separated value strings
# into lists of values.
year = []
month = []
day = []
max_temp = []
j_year = 1
j_month = 2
j_day = 3
j_max_temp = 5

for i_row in range(n_values):
    split_values = values[i_row].split(',')
    if len(split_values) >= j_max_temp:
        year.append(int(split_values[j_year]))
        month.append(int(split_values[j_month]))
        day.append(int(split_values[j_day]))
        max_temp.append(float(split_values[j_max_temp]))

# Isolate the recent data.
i_mid = len(max_temp) // 2
temps = np.array(max_temp[i_mid:])
year = year[i_mid:]
month = month[i_mid:]
day = day[i_mid:]
temps[np.where(temps == -99.9)] = np.nan

# Remove all the nans.
# Trim both ends and fill nans in the middle.
# Find the first non-nan.
i_start = np.where(np.logical_not(np.isnan(temps)))[0][0]
temps = temps[i_start:]
year = year[i_start:]
month = month[i_start:]
day = day[i_start:]
i_nans = np.where(np.isnan(temps))[0]
print(i_nans)

What is wrong in the first code and why the second doesn't even give a warning?
","What's the dtype of temps.  I can reproduce your warning and error with a string dtype:
In [26]: temps = np.array([1,2,'string',0])
In [27]: temps
Out[27]: array(['1', '2', 'string', '0'], dtype='<U21')
In [28]: temps==-99.9
/usr/local/bin/ipython3:1: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison
  #!/usr/bin/python3
Out[28]: False
In [29]: np.isnan(temps)
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
<ipython-input-29-2ff7754ed926> in <module>()
----> 1 np.isnan(temps)

TypeError: ufunc 'isnan' not supported for the input types, and the inputs could not be safely coerced to any supported types according to the casting rule ''safe''

First, comparing strings with the number gives this future warning.
Second, testing for nan produces the error.
Note that given the dtype, the nan assignment assigns a string value, not a float (np.nan is a float).
In [30]: temps[-1] = np.nan
In [31]: temps
Out[31]: array(['1', '2', 'string', 'nan'], dtype='<U21')

",numpy
linear regression model barely optimizes the intercept b,"I've programmed a linear regression model from scratch. I use the ""Sum of squared residuals"" as the loss function for gradient descent. For testing I use linear data (y=x)
When running the algorithm, intercept b barely changes. Thus the slope m is not calculated correctly.
%matplotlib qt5 
import numpy as np
import matplotlib as mpl
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split

X = np.array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])
y = np.array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=12345)

class LinearRegression():
    def __init__(self):
        self.X = None
        self.y = None
    
    def ssr(self, m, b):
        sum = 0
        for i in range(len(self.X)):
          sum += (self.y[i] - (m * self.X[i] + b) ) ** 2
        
        return sum

    def ssr_gradient(self, m, b):
        sum_m = 0
        sum_b = 0
        n = len(self.X)
        for i in range(n):
            error = self.y[i] - (m * self.X[i] + b)
            derivative_m = -(2/n) * self.X[i] * error  # Derivative w.r.t. m
            derivative_b = -(2/n) * error              # Derivative w.r.t. b
            sum_m += derivative_m
            sum_b += derivative_b

        return sum_m, sum_b
    
    def fit(self, X, y, m, b): # Gradient Descent
        self.X = X
        self.y = y

        M, B = np.meshgrid(np.arange(-10, 10, 0.1), np.arange(-10, 10, 0.1))
        SSR = np.zeros_like(M)
        for i in range(M.shape[0]):
            for j in range(M.shape[1]):
                SSR[i, j] = self.ssr(M[i, j], B[i, j])


        fig, axes = plt.subplots(1, 2, figsize=(12, 6))
        gd_model = fig.add_subplot(121, projection=""3d"", computed_zorder=False)
        lin_reg_model = axes[1] 

        current_pos = (m, b, self.ssr(m, b))
        learning_rate = 0.001
        min_step_size = 0.001
        max_steps = 1000
        current_steps = 0

        while(current_steps < max_steps):
            M_derivative, B_derivative = self.ssr_gradient(current_pos[0], current_pos[1])
            M_step_size, B_step_size = M_derivative * learning_rate, B_derivative * learning_rate

            if abs(M_step_size) < min_step_size or abs(B_step_size) < min_step_size:
                break

            M_new, B_new = current_pos[0] - M_step_size, current_pos[1] - B_step_size
            
            current_pos = (M_new, B_new, self.ssr(M_new, B_new))

            print(f""Parameters: m: {current_pos[0]}; b: {current_pos[1]}; SSR: {current_pos[2]}"")

            current_steps += 1
            
            x = np.arange(0, 10, 1)
            y = current_pos[0] * x + current_pos[1]
            lin_reg_model.scatter(X_train, y_train, label=""Train"", s=75, c=""#1f77b4"")
            lin_reg_model.plot(x, y)
            
            gd_model.plot_surface(M, B, SSR, cmap=""viridis"", zorder=0)
            gd_model.scatter(current_pos[0], current_pos[1], current_pos[2], c=""red"", zorder=1)
            gd_model.set_xlabel(""Slope m"")
            gd_model.set_ylabel(""Intercept b"")
            gd_model.set_zlabel(""Sum of squared residuals"")

            plt.tight_layout()
            plt.pause(0.001)
            
            gd_model.clear()
            lin_reg_model.clear()
        
        self.m = current_pos[0]
        self.b = current_pos[1]

    def predict(self, X_test):
        return self.m * X_test + self.b

lin_reg_model = LinearRegression()
lin_reg_model.fit(X_train, y_train, 1, 10)


This is the result for initial values m=1 and b=10:
Parameters: m: -0.45129949840919587; b: 9.50972664859535; SSR: 145.06534359577407

Obviously this isn't optimal because my data is linear. So the optimal parameters should be m=1 and b=0
But I cannot find the problem in my code. The algorithm prints different results based on the initial values, but it should print the same result over and over again as long as there is exactly one minimum of the SSR function.
I tried using different learning rates but the problem persists.
","Thanks to chrslg: I removed the division by n in the derivatives. After changing the parametres for the learning rate and max iteration, the algorithm is much faster and also optimizes b. Using more data than only the small toy data set was helpful. And thanks Dr. Snoopy: It was helpful to plot the loss for each epoch.
",numpy
how does numpy solve nth 5 and higher degree polynomials,"There is a function in NumPy that solves any polynomial with given coefficient (numpy.roots()). So how does NumPy solve it if there is no formula for 5th and higher degree polynomials?
I know about Newton's method but I wonder how exactly NumPy applies it.
I tried finding information about it in the NumPy documentation and other sources but I did not find anything about that function.
","So, the answer is in numpy documentation. But since that was the opportunity for me to play with it, I put in an answer an experiment illustrating how it can be done.
Let's say we want to solve zeros of
X**5 - 7*X**4 + 15*X**3 - 5*X**2 - 16*X + 12
I obtained this polynomial by doing
import sympy
X=sympy.symbols(""X"")
sympy.expand((X-1)*(X-2)**2*(X-3)*(X+1))

So, spoiler alert, we expect to find -1,1,2,3 as zeros, with 2 being a double one.
For that, we can build a companion matrix
import numpy as np
M=np.array([[0,0,0,0,-12], [1,0,0,0,+16], [0,1,0,0,+5], [0,0,1,0,-15], [0,0,0,1,+7]])
#array([[  0,   0,   0,   0, -12],
#       [  1,   0,   0,   0,  16],
#       [  0,   1,   0,   0,   5],
#       [  0,   0,   1,   0, -15],
#       [  0,   0,   0,   1,   7]])

The eigenvalues of this matrix are the zeros of the polynomial (since the characteristic polynomial of this matrix is the one we are interested in).
We can check this
np.linalg.eigvals(M)
#array([-1.        ,  1.        ,  1.99999992,  2.00000008,  3.        ])

So, that shifted the question:
how those eigenvalues are computed?
Since, at school, the way we learn to compute eigenvalues is... by solving zeros of characteristic polynomial!
So, obviously here that can't be the solution, or else it wouldn't help a lot.
There are several method.
For example, we could use gram-schmidt to compute both Q and R, such as M=QR
def QR(M):
    Q=np.zeros_like(M)
    R=np.zeros_like(M)
    for i in range(len(M)):
        ei=M[:,i].copy()
        for j in range(i):
            R[j,i] = Q[:,j]@M[:,i]
            ei -= R[j,i]/(Q[:,j]@Q[:,j]) * Q[:,j]
        R[i,i] = np.sqrt(ei@ei)
        Q[:,i] = ei / R[i,i]
    return Q,R

Then the iterative algorithm to compute the eigenvalues
A=M.copy()
for i in range(100): # That's an arbitrary number of iterations. In real life
        # we check for convergence. But we aren't in real life: in real life we
        # we don't code a QR decomposition in pure python :D
    Q,R=QR(A)
    A=R@Q


np.diag(A)
#array([ 3.        ,  2.02134245,  1.97865755, -1.01636249,  1.01636249])


Result are the eigenvalues of M, that is the zeros of polynomial.
",numpy
find the closest converging point of a group of vectors,"I am trying to find the point that is closest to a group of vectors.
For context, the vectors are inverted rays emitted from center of aperture stop after exiting a lens, this convergence is meant to locate the entrance pupil.
The backward projection of the exiting rays, while not converging at one single point due to spherical aberration, is quite close to converging toward a point, as illustrated in the figure below.

(For easier simulation the positive z is pointing down)
I believe that to find the closest point, I would be finding a point that has the shortest distance to all these lines. And wrote the method as follow:
def FindConvergingPoint(position, direction):

    A = np.eye(3) * len(direction) - np.dot(direction.T, direction)
    b = np.sum(position - np.dot(direction, np.dot(direction.T, position)), axis=0)

    return np.linalg.pinv(A).dot(b)

For the figure above and judging visually, I would have expected the point to be something around [0, 0, 20]
However, this is not the case. The method yielded a result of [  0., 188.60107764, 241.13690715], which is far from the converging point I was expecting.
Is my algorithm faulty or have I missed something about python/numpy?

Attached are the data for the vectors:
position = np.array([
    [0, 0, 0],
    [0, -1.62, 0.0314],
    [0, -3.24, 0.1262],
    [0, -4.88, 0.2859],
    [0, -6.53, 0.5136],
    [0, -8.21, 0.8135],
    [0, -9.91, 1.1913],
    [0, -11.64, 1.6551],
    [0, -13.43, 2.2166],
    [0, -15.28, 2.8944],
    [0, -17.26, 3.7289]
])

direction = np.array([
    [0, 0, 1],
    [0, 0.0754, 0.9972],
    [0, 0.1507, 0.9886],
    [0, 0.2258, 0.9742],
    [0, 0.3006, 0.9537],
    [0, 0.3752, 0.9269],
    [0, 0.4494, 0.8933],
    [0, 0.5233, 0.8521],
    [0, 0.5969, 0.8023],
    [0, 0.6707, 0.7417],
    [0, 0.7459, 0.6661]
])

","I'm assuming the answer to my question in the comment is that you have symmetry such that the ""closest point"" must lie on the z-axis. Furthermore, I'm assuming you are somewhat flexible about the notion of ""closest"".
First, let's remove the zeroth point from position and direction, since that will pass through all points on the z-axis (and cause numerical problems).
Next, let's find the distance from each position along direction that brings us back to the z-axis.
# transpose and remove the x-axis for simplicity
position = position.T[1:]
direction = direction.T[1:]

# more carefully normalize direction vectors
direction /= np.linalg.norm(direction, axis=0)

# find the distance from `position` along `direction` that
# brings us back to the z-axis.
distance = position[0] / direction[0]

# All these points should be on the z-axis
point_on_z = position - distance * direction
# array([[ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,
#          0.        ,  0.        ,  0.        ,  0.        ,  0.        ],
#        [21.45665199, 21.380772  , 21.34035527, 21.23103513, 21.09561354,
#         20.89001607, 20.608748  , 20.26801397, 19.79193392, 19.14234148]])

Indeed, the y-coordinate of all these points is 0, and the z-coordinate is nearly 20 for all of them, as you expected. If you are flexible about your notion of ""closest"", the mean of the z-coordinates will minimize the sum of square of distances to the point where the rays intersect the z-axis (but not necessarily the minimum distances between your point and the rays).
This might not solve exactly the problem you were hoping to solve, but hopefully you will find it ""useful"". If it's not spot on, let me know what assumptions I've made that I shouldn't, and we can try to find a solution to the refined question.
",numpy
numpy einsum  why did this happen,"Can you explain why this happened?
import numpy as np
a = np.array([[1,2],
              [3,4],
              [5,6]
              ])
b = np.array([[2,2,2],
              [2,2,2]])
print(np.einsum(""xy,zx -> yx"",a,b))

and output of the code is:[[ 4 12 20] [ 8 16 24]]  Which means the answer is calculated like this : ‍‍[1*2+1*2 , 3*2+3*2 , ...] 
But I expected it to be calculated like this: [[1*2 , 3*2 , 5*2],[2*2 , 4*2 , 6*2]] Where did I make a mistake?
","Your code is equivalent to:
(a[None] * b[..., None]).sum(axis=0).T

You start with a (x, y) and b (z, x).
First let's align the arrays:
# a[None]                                shape: (1, x, y)
array([[[1, 2],
        [3, 4],
        [5, 6]]])

# b[..., None]                           shape: (z, x, 1)
array([[[2],
        [2],
        [2]],

       [[2],
        [2],
        [2]]])

and multiply:
# a[None] * b[..., None]                 shape: (z, x, y)
array([[[ 2,  4],
        [ 6,  8],
        [10, 12]],

       [[ 2,  4],
        [ 6,  8],
        [10, 12]]])

sum over axis = 0 (z):
# (a[None] * b[..., None]).sum(axis=0)   shape: (x, y)
array([[ 4,  8],
       [12, 16],
       [20, 24]])

Swap x and y:
# (a[None] * b[..., None]).sum(axis=0).T shape: (y, x)

array([[ 4, 12, 20],
       [ 8, 16, 24]])

What you want is np.einsum('yx,xy->xy', a, b):
array([[ 2,  6, 10],
       [ 4,  8, 12]])

",numpy
getting attributeerror partially initialized module 39numpycorearrayprint39 has no attribute 39array2string39 most likely due to circular import eror,"I tried installing pandarallel but couldn't install due to some errors. Now when I try to simply import pandas and numpy, I am getting error:
import pandas as pd
import numpy as np

AttributeError: partially initialized module 'numpy.core.arrayprint' has no attribute 'array2string' (most likely due to circular import)

I am getting it if I try to import either pandas or numpy. I looked up other answers to similar partially initialied module errors, tried renaming numpy file, but it's still not fixing the issue.
I tried upgrading numpy using pip, still it didn't help.
Can someone please help me with this error.
","You have installed numpy in your local environment eg. C:/Users/username/AppData/Roaming/Python... which is conflicting with the global installation in C:/Python311/Lib/site-packages/numpy/__init.py__. This conflict is causing Python to load the wrong version or a corrupted version of numpy. To fix this you have to remove/uninstall it from local env and reinstall it with admin previliges to ensure a clean installation, either by using python -m pip install numpy in an admin command prompt or installing it globally.This resolves conflicts and ensures Python uses the correct version of numpy.
",numpy
how to extract sub arrays from a larger array with two start and two stop 1d arrays in python,"I am looking for a way to vectorize the following code,
# Let cube have shape (N, M, M)
sub_arrays = np.empty(len(cube), 3, 3)
row_start  = ... # Shape (N,) and are integers in range [0, M-2]
row_end    = ... # Shape (N,) and are integers in range [1, M-1]
col_start  = ... # Shape (N,) and are integers in range [0, M-2]
col_end    = ... # Shape (N,) and are integers in range [1, M-1]

# extract sub arrays from cube and put them in sub_arrays
for i in range(len(cube)):
    # Note that the below is extracting a (3, 3) sub array from cube
    sub_arrays[i] = cube[i, row_start[i]:row_end[i], col_start[i]:col_end[i]]

Instead of the loop, I would like to do something like,
sub_arrays = cube[:, row_start:row_end, col_start:col_end]

But this throws the exception,
TypeError: only integer scalar arrays can be converted to a scalar index

Is there instead some valid way to vectorize the loop?
","I believe this question is a duplicate of the one about Slicing along axis with varying indices. However, since it may not be obvious, I think it's okay to provide the answer in a new context with a somewhat different approach.
From what I can see, you want to extract data from the cube using a sliding window of a fixed size (3×3 in this case), applied to a separate slice along the first axis with varying shifts within the slices.
In contrast to the previously mentioned approach using as_strided, let's use sliding_window_view this time. As a result, we get two additional axes for row_start and col_start, followed by the window dimensions. Note that row_end and col_end appear as if they are equal to the corresponding starting points increased by a fixed square window side, which is 3 in this case:
from numpy.lib.stride_tricks import sliding_window_view

cube_view = sliding_window_view(cube, window_shape=(3, 3), axis=(1, 2))
output = cube_view[range(cube.shape[0]), row_start, col_start].copy()

That's all. But to be sure, let's compare the output with the original code, using test data:
import numpy as np
from numpy.random import randint
from numpy.lib.stride_tricks import sliding_window_view

n, m, w = 100, 10, 3     # w - square window size
row_start = randint(m-w+1, size=n)
col_start = randint(m-w+1, size=n)

# Test cube
cube = np.arange(n*m*m).reshape(n, m, m)

# Data to compare with
sub_arrays = np.empty((n, w, w), dtype=cube.dtype)
for i in range(cube.shape[0]):
    sub_arrays[i] = cube[i, row_start[i]:row_start[i]+w, col_start[i]:col_start[i]+w]    
    
# Subarrays from the sliding window view
cube_view = sliding_window_view(cube, window_shape=(w, w), axis=(1, 2))
output = cube_view[range(cube.shape[0]), row_start, col_start].copy()    

# No exceptions should occur at this step
assert np.equal(output, sub_arrays).all()

",numpy
asofjoin with multiple inequality conditions,"I have two dataframes: a (~600M rows) and b (~2M rows). What is the best approach for joining b onto a, when using 1 equality condition and 2 inequality conditions on the respective columns?

a_1 = b_1
a_2 >= b_2
a_3 >= b_3

I have explored the following paths so far:

Polars:

join_asof(): only allows for 1 inequality condition
join_where() with filter(): even with a small tolerance window, the standard Polars installation runs out of rows (4.3B row limit) during the join, and the polars-u64-idx installation runs out of memory (512GB)


DuckDB: ASOF LEFT JOIN: also only allows for 1 inequality condition
Numba: As the above didn't work, I tried to create my own join_asof() function - see code below. It works fine but with increasing lengths of a, it becomes prohibitively slow. I tried various different configurations of for/ while loops and filtering, all with similar results.

Now I'm running a bit out of ideas... What would be a more efficient way to implement this?
Thank you
import numba as nb
import numpy as np
import polars as pl
import time


@nb.njit(nb.int32[:](nb.int32[:], nb.int32[:], nb.int32[:], nb.int32[:], nb.int32[:], nb.int32[:], nb.int32[:]), parallel=True)
def join_multi_ineq(a_1, a_2, a_3, b_1, b_2, b_3, b_4):
    output = np.zeros(len(a_1), dtype=np.int32)

    for i in nb.prange(len(a_1)):

        for j in range(len(b_1) - 1, -1, -1):

            if a_1[i] == b_1[j]:

                if a_2[i] >= b_2[j]:

                    if a_3[i] >= b_3[j]:
                        output[i] = b_4[j]
                        break

    return output


length_a = 5_000_000
length_b = 2_000_000

start_time = time.time()
output = join_multi_ineq(a_1=np.random.randint(1, 1_000, length_a, dtype=np.int32),
                         a_2=np.random.randint(1, 1_000, length_a, dtype=np.int32),
                         a_3=np.random.randint(1, 1_000, length_a, dtype=np.int32),
                         b_1=np.random.randint(1, 1_000, length_b, dtype=np.int32),
                         b_2=np.random.randint(1, 1_000, length_b, dtype=np.int32),
                         b_3=np.random.randint(1, 1_000, length_b, dtype=np.int32),
                         b_4=np.random.randint(1, 1_000, length_b, dtype=np.int32))
print(f""Duration: {(time.time() - start_time):.2f} seconds"")

","Using Numba here is a good idea since the operation is particularly expensive. That being said, the complexity of the algorithm is O(n²) though it is not easy to do much better (without making the code much more complex). Moreover, the array b_1, which might not fit in the L3 cache, is fully read 5_000_000 times making the code rather memory bound.
We can strongly speed up the code by building an index so not to travel the whole array b_1, but only the values where a_1[i] == b_1[j]. This is not enough to improve the complexity since a lot of j values fulfil this condition. We can improve the (average) complexity by building a kind of tree for all nodes of the index but in practice, this makes the code much more complex and the time to build the tree would be so big that it actually does not worth doing that in practice. Indeed, a basic index is enough to strongly reduce the execution time on the provided random dataset (with uniformly distributed numbers). Here is the resulting code:
import numba as nb
import numpy as np
import time

length_a = 5_000_000
length_b = 2_000_000

a_1=np.random.randint(1, 1_000, length_a, dtype=np.int32)
a_2=np.random.randint(1, 1_000, length_a, dtype=np.int32)
a_3=np.random.randint(1, 1_000, length_a, dtype=np.int32)
b_1=np.random.randint(1, 1_000, length_b, dtype=np.int32)
b_2=np.random.randint(1, 1_000, length_b, dtype=np.int32)
b_3=np.random.randint(1, 1_000, length_b, dtype=np.int32)
b_4=np.random.randint(1, 1_000, length_b, dtype=np.int32)

IntList = nb.types.ListType(nb.types.int32)

@nb.njit(nb.int32[:](nb.int32[:], nb.int32[:], nb.int32[:], nb.int32[:], nb.int32[:], nb.int32[:], nb.int32[:]), parallel=True)
def join_multi_ineq_fast(a_1, a_2, a_3, b_1, b_2, b_3, b_4):
    output = np.zeros(len(a_1), dtype=np.int32)
    b1_indices = nb.typed.Dict.empty(key_type=nb.types.int32, value_type=IntList)
    for j in range(len(b_1)):
        val = b_1[j]
        if val in b1_indices:
            b1_indices[val].append(j)
        else:
            lst = nb.typed.List.empty_list(item_type=np.int32)
            lst.append(j)
            b1_indices[val] = lst
    kmean = 0
    for i in nb.prange(len(a_1)):
        if a_1[i] in b1_indices:
            indices = b1_indices[a_1[i]]
            v2 = a_2[i]
            v3 = a_3[i]
            for k in range(len(indices) - 1, -1, -1):
                j = indices[np.uint32(k)]
                #assert a_1[i] == b_1[j]
                if v2 >= b_2[j] and v3 >= b_3[j]:
                    output[i] = b_4[j]
                    break
    return output

%time join_multi_ineq_fast(a_1, a_2, a_3, b_1, b_2, b_3, b_4)

Note that, in average, only 32 k values are tested (which is reasonable enough not to build a more efficient/complicated data structure). Also please note that the result is strictly identical to the one provided by the naive implementation.

Benchmark
Here are results on my i5-9600KF CPU (6 cores):
Roman's code:        >120.00 sec     (require a HUGE amount of RAM: >16 GiB)
Naive Numba code:      24.85 sec
This implementation:    0.83 sec     <-----

Thus, this implementation is about 30 times faster than the initial code.
",numpy
cannot get numpy to be detected when installing aeneas,"On windows with python 3.13.1, when running pip install for something (aeneas) I cannot resolve: You must install numpy before installing aeneas
I've tried many different approaches, including following older stackoverflow posts on the matter.
I hoped this approach at least would work:
python -m venv myenv
.\myenv\Scripts\Activate
pip install numpy
pip list

Package Version
------- -------
numpy   2.2.1
pip     24.3.1

pip install aeneas

Collecting aeneas
  Using cached aeneas-1.7.3.0.tar.gz (5.5 MB)
  Installing build dependencies ... done
  Getting requirements to build wheel ... error
  error: subprocess-exited-with-error

  × Getting requirements to build wheel did not run successfully.
  │ exit code: 1
  ╰─> [3 lines of output]
      [ERRO] You must install numpy before installing aeneas
      [INFO] Try the following command:
      [INFO] $ sudo pip install numpy
      [end of output]

  note: This error originates from a subprocess, and is likely not a problem with pip.
error: subprocess-exited-with-error

pip install setuptools
pip install --upgrade pip setuptools wheel
pip install --no-build-isolation aeneas

...You must install numpy before installing aeneas

","The last two commands are right except that the package is old and requires numpy.distutils. That means it needs numpy version 1 and not 2. Also numpy.distutils was removed in python 3.12. So the solution is to use Python 3.11 and:
pip install --upgrade pip setuptools wheel ""numpy<2""
pip install --no-build-isolation aeneas

",numpy
pil generate an image from applying a gradient to a numpy array,"I have a 2d NumPy array with values from 0 to 1. I want to turn this array into a Pillow image. I can do the following, which gives me a nice greyscale image:
arr = np.random.rand(100,100)
img = Image.fromarray((255 * arr).astype(np.uint8))

Now, instead of making a greyscale image, I'd like to apply a custom gradient.
To clarify, instead of drawing bands of colors in a linear gradient as in this example, I'd like to specify apply a gradient colormap to an existing 2d array and turn it into a 3d array.
Example: If my gradient is [color1, color2, color3], then all 0s should be color1, all 1s should be color3, and 0.25 should be somewhere in between color1 and color2. I was already able to write a simple function that does this:
gradient = [(0, 0, 0), (255, 80, 0), (0, 200, 255)] # black -> orange -> blue

def get_color_at(x):
    assert 0 <= x <= 1
    n = len(gradient)
    if x == 1:
        return gradient[-1]
    pos = x * (n - 1)
    idx1 = int(pos)
    idx2 = idx1 + 1
    frac = pos - idx1
    color1 = gradient[idx1]
    color2 = gradient[idx2]
    color_in_between = [round(color1[i] * (1 - frac) + color2[i] * frac) for i in range(3)]
    return tuple(color_in_between)

So get_color_at(0) returns (0,0,0) and get_color_at(0.75) equals (153, 128, 102), which is this tan/brownish color in between orange and blue.
Now, how can I apply this to the original NumPy array? I shouldn't apply get_color_at directly to the NumPy array, since that would still give a 2d array, where each element is a 3-tuple. Instead, I think I want an array whose shape is (n, m, 3), so I can feed that to Pillow and create an RGB image.
If possible, I'd prefer to use vectorized operations whenever possible - my input arrays are quite large. If there is builtin-functionality to use a custom gradient, I would also love to use that instead of my own get_color_at function, since my implementation is pretty naive.
Thanks in advance.
","Method 1: vectorization of your code
Your code is almost already vectorized. Almost all operations of it can work indifferently on a float or on an array of floats
Here is a vectorized version
def get_color_atArr(arr):
    assert (arr>=0).all() and (arr<=1).all()
    n=len(gradient)
    gradient.append(gradient[-1])
    gradient=np.array(gradient, dtype=np.uint8)
    pos = arr*(n-1)
    idx1 = pos.astype(np.uint8)
    idx2 = idx1+1
    frac = (pos - idx1)[:,:,None]
    color1 = gradient[idx1]
    color2 = gradient[idx2]
    color_in_between = np.round(color1*(1-frac) + color2*frac).astype(np.uint8)

Basically, the changes are,

the assert (can't use a<b<c notation with numpy arrays). Note that this assert iterates all values of array to check for assertion. That is not for free. So I included it because you did. But you need to be aware that this is not a compile-time verification. It does run code to check all values, which is a non-negligible part of all execution time of the code.
more an implementation choice than a vectorization step (a pure translation of your code would have translated that if x==1 into some np.where, or masks. But I am never comfortable with usage of == on floats any way. So I prefer my way. Which costs nothing. It is not another iteration on the image. It adds a sentinel (In Donald Kuth sense of ""sentinel"": a few bytes that avoid special cases) to the gradient color. So that, in the unlikely even that arr is really 1.0, the gradient happen between last color and last color).
frac is broadcasted in 3D array, so that it can be used as a coefficient on 3d arrays color1 and color2
Plus of course, int or floor can't be used on numpy arrays

Method 2: not reinventing the wheel
Matplotlib (and, I am certain, many other libraries) already have a whole colormap module to deal with this kind of transformations. Let's use it
thresh=np.linspace(0,1,len(gradient))
cmap=LinearSegmentedColormap.from_list('mycmap', list(zip(thresh, np.array(gradient)/255.0)), N=256*len(gradient))
arr2 = cmap(arr)[:,:,:3]

This is building a custom colormap, using LinearSegmentedColormap, which takes, as 2nd argument, a list of pair (threshold, color).
Such as [(0, (0,0,0)), (0.3, (1,0,0)), (0.8, (0,1,0)), (1, (0,0,1))] for a color map that goes from black to red when x goes from 0 tom 0.3, then from red to green when x goes from 0.3 to 0.8, then from green to blue.
In this case, your gradient can be transformed to such a list, with just a zip with a linspace.
It takes a N= argument, since it creates a discretization of all possible colors (with interpolation in between). Here I take an exaggerated option (my N is more than the maximum number of different colors than can exist, once uint8d)
Also since it returns a RGBA array, and to remain strictly identical to what you did, I drop the A using [:,:,:3].
Of course, both method need the final translation into PIL, but you already know how to do that. For this one, it also needs mapping between 0 and 255, which I can do with your own code:
Image.fromarray((255 * arr).astype(np.uint8))
Note that, while using matplotlib colormap, you may want to take a tour at what that module has to offer. For example some of the zillions of already existing colormaps may suit you. Or some other way to build colors map non-linearly.
",numpy
how can i optimize python code for analysis of a large sales dataset,"I’m working on a question where I have to process a large set of sales transactions stored in a CSV file and summarize the results. The code is running slower than expected and taking too much time for execution, especially as the size of the dataset increases. I am using pandas to load and process the data, are there any optimizations I can make to reduce computational time and get the output faster. Here is the code i am using:
import pandas as pd
import numpy as np

# Sample dataset
n = 10**6  # million rows
np.random.seed(0)
transaction_ids = np.arange(1, n+1)
customer_ids = np.random.randint(100, 200, n)
sale_amounts = np.random.uniform(50, 500, n)
transaction_dates = pd.date_range('2023-01-01', periods=n, freq='T')

# DataFrame
df = pd.DataFrame({
    'transaction_id': transaction_ids,
    'customer_id': customer_ids,
    'sale_amount': sale_amounts,
    'transaction_date': transaction_dates
})

# Categorization function
def categorize_transaction(sale_amount):
    if sale_amount > 400:
        return 'High Value'
    elif sale_amount > 200:
        return 'Medium Value'
    else:
        return 'Low Value'

category_map = {
    'High Value': (df['sale_amount'] > 400),
    'Medium Value': (df['sale_amount'] > 200) & (df['sale_amount'] <= 400),
    'Low Value': (df['sale_amount'] <= 200)
}

df['category'] = np.select(
    [category_map['High Value'], category_map['Medium Value'], category_map['Low Value']], 
    ['High Value', 'Medium Value', 'Low Value'],
    default='Unknown'
)

# Aggregation
category_summary = df.groupby('category')['sale_amount'].agg(
    total_sales='sum', 
    avg_sales='mean', 
    transaction_count='count'
).reset_index()

# Additional optimization using 'transaction_date' for time-based grouping
df['transaction_month'] = df['transaction_date'].dt.to_period('M')
monthly_summary = df.groupby(['transaction_month', 'category'])['sale_amount'].agg(
    total_sales='sum', 
    avg_sales='mean', 
    transaction_count='count'
).reset_index()

print(category_summary.head())
print(monthly_summary.head())

","First of all, the df['category'] = np.select(...) line is slow because of the implicit conversion of all strings to a list of string objects. You can strongly speed this up by creating a categorical column rather than string-based one, since strings are inherently slow to compute.
df['category'] = pd.Categorical.from_codes(np.select(
    [category_map['High Value'], category_map['Medium Value'], category_map['Low Value']],
    [0, 1, 2],
    default=3
), ['High Value', 'Medium Value', 'Low Value', 'Unknown'])

This create a categorical column with 4 possible values (integers associated to predefined strings). This is about 8 times faster on my machine.
Once you use the above code, the aggregation is also running much faster (about 5 times) because Pandas operates on integers rather than slow string objets. It also speed up the very-last operation (about twice faster).
The df['transaction_date'].dt.to_period('M') is particularly slow. Directly using Numpy (with .astype('datetime64[M]')) does not make this faster. Since this operation is compute bound, you can parallelize it. Alternatively, you can write your own (parallel) implementation with Numba (or Cython) though this is tedious to write since one need to case about leap years (and possibly even leap seconds).

Update: You can make the first code even faster thanks to 8-bit integers (assuming there are less than 128 categories). This can be done by replacing [0, 1, 2] to np.array([0, 1, 2], dtype=np.int8). This is about 35% faster than the default 32-bit categories.
",numpy
how to get lebedev and gaussian spherical grid,"I am being reading some works on HRTF interpolation and spherical harmonics.
In such works regular spherical grids are often used, e.g., in this work, but I am missing how to compute them:
Hence, how to compute the Lebedev and the Gaussian spherical grids?
Is there a python package that easily return the list of points for a specific grid?
","Foe Lebedev grids, you could try qc-grid package.
pip install qc-grid

Then you can ask some points for Spherical Harmonics (sh) numerical integration:
from grid.angular import AngularGrid

sh_degree = 6
lebedev_grid = AngularGrid(degree = sh_degree)

print(f""3D points in Cartesian: {lebedev_grid.points}""

Alternatively, you could give AngularGrid the number of points you want. It seems to return the point in Cartesian coordinates.
",numpy
numpy convention for storing time series of vectors and matrices  items in rows or columns,"I'm working with discrete-time simulations of ODEs with time varying parameters. I have time series of various data (e.g. time series of state vectors generated by solve_ivp, time series of system matrices generated by my control algorithm, time series of system matrices in modal form, and so on).
My question: in what order should I place the indices? My intuition is that since numpy arrays are (by default) stored in row-major order, and I want per-item locality, each row should contain the ""item"" (i.e. a vector or matrix), and so the number of rows is the number of time points, and the number of columns is the dimension of my vector, e.g.:
x_k = np.array((5000, 4)) # a time series of 5000, 4-vectors
display(x_k[25]) # the 26th timepoint

Or for matrices I might use:
A_k = np.array((5000, 4, 4)) # a time series of 5000, 4x4-matrices

However, solve_ivp appears to do the opposite and returns a row-major array with the time series in columns (sol.y shape is (4, 5000)). Furthermore, transposing the result with .T just flips a flag to column-major so it is not really clear what the developers of solve_ivp and numpy intend me to do to write cache efficient code.
What are the conventions? Should I use the first index for the time index, as in my examples above, or last index as solve_ivp does?
","This is strongly dependent of the algorithms applied on your dataset. This problem is basically known as AoS versus SoA. For algorithm that does not benefit much from SIMD operations and accessing all fields, AoS can be better, otherwise SoA is often better. The optimal data structure is often AoSoA, but it is often a pain to manipulate (so it is rare in Numpy codes). On top of that, Numpy is not efficient to operate on arrays having a very small last axis because of the way it is currently implemented (more specifically because internal generators, unneeded function calls, and lack of function specialization which is hard to do due to the high number of possible combinations).

Example
Here is a first practical example showing this (center of a point cloud):
aos = np.random.rand(1024*1024, 2)
%timeit -n 10 aos.mean(axis=0)          # 17.6 ms ± 503 µs per loop

soa = np.random.rand(2, 1024*1024)
%timeit -n 10 soa.mean(axis=1)          #  1.7 ms ±  77 µs per loop

Here we can see that the SoA version is much faster (about 10 times). This is because the SoA version benefit from SIMD instruction while the former does not and suffer from the internal Numpy iterator overhead. Technically, please note that the AoS version could be implemented to be nearly as fast as the SoA version here but Numpy is not able to optimize this yet (nor any similar cases which are actually not so easy to optimize).

In your case
For matrices, Numpy can call BLAS functions on contiguous arrays, which is good for performance. However, a 4x4 matrix-vector operation takes a tiny amount of time: only few nanoseconds on mainstream CPUs (for AoS). Indeed, multiplying the 4x4 matrix rows by a vector takes only 4 AVX instructions that can typically be computed in only 2 cycles. Then comes the sum reduction which takes few nanoseconds too (~4 cycles per line for a naive hadd reduction that is 16 cycles for the whole matrix). Meanwhile, a BLAS function call from Numpy and the management of internal iterators takes significantly more than 10 ns per matrix to compute. This means most of the time will be spent in Numpy overheads with a AoS layout. Thus, a np.array((5000, 4, 4)) will certainly not be so bad, but clearly far from being optimal. You can strongly reduce these overheads by writing your own specialized implementation (with Cython/Numba) specifically designed for 4x4 matrices. Here is an example of relatively-fast AoS computation using Numba.
With a SoA data layout (i.e. (4, 4, 5000)), you can write your own vectorized operations (e.g. SoA-based matrix multiplication). A naive implementation will certainly not be very efficient either because creating/filling temporary Numpy array is expensive. However, temporary arrays can often be preallocated/reused and operations can be often done in-place so to reduce the overheads. On top of that, you can tune the size of the temporary array so it can fit in the L1 cache (though this is tedious to do since it makes the code more complex so generally Numpy users don't want to do that). That being said, calling Numpy functions from CPython also has a significant overhead (generally 0.2-3.0 µs on my i5-9600KF CPU). This is a problem since doing basic computation on 5000 double-precision floating-point numbers in the L1 cache typically takes less than 1 µs. As a result, there is a good chance for most of the time to be spent in CPython/Numpy overheads with a SoA array having only 5000 items manipulated only using Numpy. Here again, Cython/Numba can be used to nearly remove these overheads. The resulting Cython/Numba code should be faster on SoA than AoS arrays (mainly because of horizontal SIMD operations are generally inefficient and AoS operations tends to be hard to optimize, especially on modern CPUs with wide SIMD instruction set).

Conclusion
This is a complicated topic. In your specific case, I expect both SoA and AoS to be inefficient if you only use Numpy (but the SoA version might be a bit faster) : most of the time will be spent in overheads. As a result, the speed of the best implementation is dependent of the exact algorithm implementation and even the CPU used (so the best is to try which one is better in practice in practice).
That being said, I think using SoA is significantly better performance-wise than AoS. Indeed, codes operating on SoA arrays can be optimized more easily and further than AoS ones (see Cython/Numba or even native C code).
On top of that, SoA-based codes are much more likely to benefit from accelerators like GPUs. Indeed, GPUs are massively-SIMD hardware devices operating on wide SIMD vectors (e.g. 32 items at once). 4x4 contiguous AoS matrix operation are generally pretty inefficient on them, meanwhile SIMD-friendly SoA ones are cheap.
I advise you to write a clean/simple Numpy code first while preferring a SoA layout for your array, and then optimize slow parts of the code later (possibly with Cython/Numba/native codes). This strategy often results in relatively-clean codes that are simple to optimize.
",numpy
generating binary arrays with alternating values based on change indices in numpy,"I have an array a of increasing indexes, e.g. [2 5 9 10], which indicates positions of value change. Assuming the output values are 0 and 1, I want to get array b:
[0 0 1 1 1 0 0 0 0 1 0]

Is there a NumPy magic to transform a into b?
","One way among many others
a=np.array([2,5,9,10])
x=np.zeros((a.max()+1,), dtype=np.uint8)
x[a]=1
b=x.cumsum()%2

Some explanation (but I guess code, in this rare case, is its own explanation, since it is quite easy, once you see it)
x (after x[a]=1) contains 1 at each given position in a. So x.cumsum() contains a value that increments for each of those values: 0 for the 2 first, then 1  for 3 next then 2, then 3, then 4... So x.cumsum()%2 alternates between 1 and 0.
Note that I use np.uint8 type because I am cheap, and I can't help thinking ""why shoud I pay 32 bits when 8 are enough for a size 11 array"". But in reality, since 256 is even, it wouldn't really matter even if a had billions of values. Just x.cumsum() would roll back from 255 to 0 because of overflow. And then x.cumsum()%2 would have the same value.
",numpy
is there any difference between listarray and arraytolist in python,"The following variables seem to be similar but they aren't and I don't understand why:
import ujson
import numpy as np

arr = np.array([1, 2, 3, 4])
arr_1 = arr.tolist()
arr_2 = list(arr)

arr_1 == arr_2  # True

ujson.dumps({'arr': arr_1})  # it works
ujson.dumps({'arr': arr_2})  # it does not work (OverflowError: Maximum recursion level reached)

I am using Python-3.5.6, ujson-1.35 and numpy-1.16.4. 
Thank you a lot for your help!!
","numpy has its own numeric data types for different levels of precision.
These are created in ways that allow easy comparison with regular Python integers.
np.int32(3) == 3   # True
[np.int32(1), 4] == [1, np.int32(4)]    # True

(Lists are equal in Python if all elements at the same index are equal)
That is why your arr_1 == arr_2. 
They cannot readily be serialized to json, but the tolist method converts them to regular Python numeric types which do allow serialization.
",numpy
sampling without replacement with unequal weights,"I am trying to find an algorithm that selects 12 elements from a list of 25 according to the probability of each one of them. These elements cannot be repeated.
I've tried using the function without replacement, but I can't get it to keep the weights when I do 1000 iterations to check that it was done correctly. I have only succeeded if I use the function with replace = True.
Since that function returns duplicate items to me, then I clean the list removing the duplicates. By doing this, the probability that I get each number is not the one that I had defined at the beginning. I am aware that the error is here and that I should be using the function with replace = False, but I can't get it to give me the result.
import numpy as np
from collections import Counter 

nameList = ['AAAAAA', 'B', 'C', 'D','E','F','G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P','Q','R','S','T','U','V','W','X','ZZZZZZZZZZZZZZZZ']
probability_nameList = [0.10, 0.01, 0.02, 0.03, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04]

sampleNames =[]
sampleNamesControl =[]
cleaned_result = []
test_list = []

for i in range(1000):
    sampleNames += np.random.choice(nameList, 100,replace=True, p=probability_nameList)
    for item in sampleNames:    
        if not item in cleaned_result:
            cleaned_result += [item] 
    sampleNames =[]
    test_list += cleaned_result[0:12]
    cleaned_result = []

print(Counter(test_list))


Response:
Counter({'AAAAAA': 832, 'F': 522, 'X': 513, 'ZZZZZZZZZZZZZZZZ': 506, 'I': 505, 'L': 504, 'N': 501, 'S': 499, 'T': 498, 'U': 496, 'R': 492, 'O': 491, 'J': 489, 'P': 488, 'E': 487, 'V': 485, 'K': 482, 'Q': 479, 'H': 473, 'G': 471, 'M': 468, 'W': 461, 'D': 404, 'C': 297, 'B': 157})

As you can see, AAAAA had to be 10 times bigger than B, and that is not happening.
P(AAAAA)= 10xP(B)
","This is not a programming issue but fundamental maths: what you want is not possible.
You need to take into account conditional probabilities.
I'll give you a small example. Let's take 4 letters A,B,C,D and pick 2 of them, without replacement, with A having a 97% probability of being picked and the others 1%.
Given A's high probability, it will be picked most of the time. But once we have picked it, as we are without replacement, there is no more A and a 1/3 chance to pick one of the other 3 values.
Indeed:
from itertools import chain
from collections import Counter

Counter(chain.from_iterable(np.random.choice(list('ABCD'),
                                             2, replace=False,
                                             p=[.97, .01, .01, .01])
                            for i in range(10000)))

Output:
Counter({'A': 9995, 'C': 3358, 'B': 3317, 'D': 3330})

A is picked almost all the time and the others are picked about one third of the time.
NB. The end frequency of B,C,D is roughly 1/3 because the probability of A is close to 1, if p(A) was smaller, the calculation of the final frequencies would be a bit more complex. The final frequency of B (or C or D) is 0.97*1/3+0.01*(0.01/0.99)*2+0.01 so a bit above 1/3, and that of A is 0.97+0.01*(0.97/0.99)*3 so ~0.9994
",numpy
why does nparray use square brackets   while npzeros uses parentheses  ,"I'm learning NumPy and noticed a difference in how inputs are provided to some of its functions:
When using np.array(), I pass the data inside square brackets [], like this:
arr = np.array([1, 2, 3])

But when using np.zeros(), I pass the shape inside parentheses (), like this:
zeros_array = np.zeros((2, 3))

Why does np.array() require square brackets, while np.zeros() requires parentheses? Is there a specific reason behind this difference in syntax?
","The only ""requirement"" for that syntax is a sequence.  Both syntaxes work, although arrays tend to be lists and shapes tend to be tuples.  See below for the opposite example of your post:
>>> import numpy as np
>>> np.array((1, 2, 3))
array([1, 2, 3])
>>> np.zeros([2, 3])
array([[0., 0., 0.],
       [0., 0., 0.]])

The documentation for numpy.array specifically says ...any (nested) sequence.  numpy.zeros says int or tuple of ints but as implemented any sequence works.
",numpy
efficiently draw random samples without replacement from an array in python,"I need to draw random samples without replacement from a 1D NumPy array. However, performance is critical since this operation will be repeated many times.
Here’s the code I’m currently using:
import numpy as np

# Example array
array = np.array([10, 20, 30, 40, 50])

# Number of samples to draw
num_samples = 3

# Draw samples without replacement
samples = np.random.choice(array, size=num_samples, replace=False)

print(""Samples:"", samples)

While this works for one sample, it requires a loop to generate multiple samples, and I believe there could be a way to optimize or vectorize this operation to improve performance when sampling multiple times.

Is there a way to vectorize or otherwise optimize this operation?
Would another library (e.g., TensorFlow, PyTorch) provide better
performance for this task?
Are there specific techniques for bulk sampling that avoid looping in
Python?

","The code below generates random samples of a list without replacement in a vectorized manner. This solution is particularly useful when the number of simulations is large and the number of samples per simulation is low.
import numpy as np

def draw_random_samples(len_deck, n_simulations, n_cards):
    
    """"""
    Draw random samples from the deck.
    
    Parameters
    ----------
    len_deck : int
        Length of the deck.
    n_simulations : int
        How many combinations of cards are generated. (Doubles could occur.)
    n_cards : int
        How many cards to draw from the deck per simulation. (All cards are unique.)
    
    Returns
    -------
    indices : array-like
        Random indices of the deck. 
    
    """"""
    
    indices = np.random.randint(0, len_deck, (1, n_simulations))

    for i in range(1, n_cards):
        new_indices = np.random.randint(0, len_deck-i, n_simulations)
        new_indices += np.sum(new_indices >= indices - np.arange(i)[:,None], axis=0)

        indices = np.vstack((indices, new_indices))
        indices = np.sort(indices, axis=0)
    
    return indices.T

",numpy
is numpydivide a function or a class or something else,"In this code:
import numpy as np

a = np.array([1,2,3])
b = np.array([2,3,4])

c = np.divide(a,b)
d = np.divide.outer(a,b)
e = np.outer(a,b)

print(f""{c=}\n{d=}\n{e=}"")

It seems that numpy.divide contains outer, meanwhile numpy.divide is a function.
I’m really confused why numpy.divide behaves like both class(because it could contain something) and function(because it is callable).
print(type(np.divide)) returns <class 'numpy.ufunc'>. But why ufunc could have this effect?
My question is that: why np.divide is callable, meanwhile there is functions that is belongs to np.divide? sys.stderr.write is not a similar case as sys.stderr is not callable.
","An easy way to answer this type of question:
import numpy as np

print(type(np.divide))

Result:
<class 'numpy.ufunc'>

How can you call a class, or rather an instance of it? If a class has __call__, its instances can be called, and this ufunc is callable:
print(type(np.divide.__call__))

Result:
<class 'method-wrapper'>

It's not just that ""a similar notation is used"" - the instance is callable, no different from a function. A regular Python function is just an instance of the function class:
def fun():
    ...


print(type(fun))
print(type(fun.__call__))

Result:
<class 'function'>
<class 'method-wrapper'>

So, numpy.divide is both a function and an object, which is an instance of a class. If something has the type class 'something', it's not itself a class, it's an instance of that class, an object.
",numpy
numpyroot is returning the correctly plotted xintercept but labeling it incorrectly in pythoninexcel,"I am creating programs which produce worksheets for students. In this particular program student must sketch a polynomial given in factored form on a blank grid.

Excel generates random roots on a limited interval.
Excel then calculates the coefficients for the expanded (unfactored) polynomial.
A Python script in an adjacent cell then takes the coefficients and plots a graph of the polynomial, with x-intercepts plotted and labeled, for the worksheet key.

My problem is that some x-intercepts are plotted and labeled correctly and then one or more x-intercepts are plotted correctly but labeled incorrectly. Here is a screen cap with annotation:
I have confirmed with a graphing calculator that the coefficients are correct. They do produce the expected roots. I ran the Python script in Spyder and it worked as expected with all roots correctly labeled. Here is the Python script which is inserted in the 3rd spreadsheet cell from the left:
import numpy as np
import matplotlib.pyplot as plt

a = xl(""K2"")
b = xl(""L2"")
c = xl(""M2"")
d = xl(""N2"")
e = xl(""O2"")
r1 = xl(""F2"")
r2 = xl(""G2"")
r3 = xl(""H2"")
r4 = xl(""I2"")
r5 = xl(""J2"")

# Generate x values
x = np.linspace(r1-1, r3+1, 1000)

# Calculate y values for the polynomial
y = a*x**3 + b*x**2 + c*x + d
def polynomial(x):
    return a*x**3 + b*x**2 + c*x + d
    
# Find x-intercepts using numpy's roots function
coefficients = [a, b, c, d]  # coefficients of expanded polynomial
x_intercepts = np.roots(coefficients)

# Create the plot
plt.figure(figsize=(3,3), dpi = 400)
plt.plot(x, y, 'b-')

# Plot x-intercepts and add labels
for x_int in x_intercepts:
    plt.plot(x_int, polynomial(x_int), 'ro', zorder = 20)  # 'ro' for red circle markers
    plt.text(x_int, 20, f'{int(x_int)}', color = ""red"", fontsize=9, ha = ""center"")

# Customize the plot
plt.axhline(y=0, color='black', linestyle='-', alpha=1)  # x-axis
plt.axvline(x=0, color='black', linestyle='-', alpha=1)  # y-axis
plt.xticks([])
plt.yticks([])
plt.xlabel('x')
plt.ylabel('y')
plt.show()

","I added the following line to your for loop to see what's going on.
print(x_int, f""{int(x_int)}"")

The output is as follows:
-7.000000000000009 -7
5.000000000000005 5
0.9999999999999997 0

The int function takes a value and chops off the values after the decimal. Because the last term isn't exactly 1 (it's just below 1), int turns it into 0. If you know that you will always have integer roots, you should instead use the round function, i.e. your equivalent line should be as follows:
plt.text(x_int, 20, round(x_int), color=""red"", fontsize=9, ha=""center"")

Resulting plot:

",numpy
generating low discrepancy quasirandom sequences in pythonnumpyscipy,"There is already a question on this but the answer contains a broken link, and being over two years old, I'm hoping there's a better solution now :)
Low discrepancy quasi-random sequences, e.g. Sobol sequences, fill a space more uniformly than uniformly random sequences. Is there a good/easy way to generate them in python?
","I think the best alternative for Low Discrepancy sequences in Python is Sensitivity Analysis Library (SALib):
https://github.com/SALib/SALib
I think this is an active project and you can contact the author to check if the functionalities you need are already implemented. If that doesn't solve your problem, Corrado Chisari ported a SOBOL version made in Matlab (by John Burkardt) to Python, you can access it here:
http://people.sc.fsu.edu/~jburkardt/py_src/sobol/sobol.html
Someone cleaned up the comments in these sources and put them in the format of docstrings. It's much more readable and you can access it here:
https://github.com/naught101/sobol_seq
",numpy
how should i parse times in the japanese quot30hourquot format for data analysis,"I'm considering a data analysis project involving information on Japanese TV broadcasts. The relevant data will include broadcast times, and some of those will be for programs that aired late at night.
Late-night Japanese TV schedules follow a non-standard time format called the 30-hour system (brief English explanation here). Most times are given in normal Japan Standard Time, formatted as %H:%M. Times from midnight to 6 AM, however, are treated as an extension of the previous day and numbered accordingly, under the logic that that's how people staying up late experience them. For example, Macross Frontier was broadcast in Kansai at 1:25 AM, but it was written as 25:25.
I want to use this data in a Pandas or Polars DataFrame. Theoretically, it could be left as a string, but it'd be more useful to convert it to a standard format for datetimes -- either Python's built-in type, or the types used in NumPy or Polars. One simple approach could be:
from datetime import date, time, datetime
from zoneinfo import ZoneInfo

def process_30hour(d: date, t: str):
    h, m = [int(n) for n in t.split(':')] # assumes format 'HH:MM' for t
    if h > 23:
        h -= 24
        d += 1
    return datetime.combine(d, time(h, m), ZoneInfo('Japan'))

This could then be applied to a whole DataFrame with DataFrame.apply(). There may be a more performant way, however, especially considering the vectorization features of DataFrames -- both libraries recommend avoiding DataFrame.apply() if there's an alternative.
","IIUC, you could use create a datetime with '00:00' as time and add the hours as timedelta:
from datetime import date, time, datetime, timedelta
from zoneinfo import ZoneInfo

def process_30hour(d: date, t: str):
    h, m = map(int, t.split(':')) # assumes format 'HH:MM' for t
    return (datetime.combine(d, time(), ZoneInfo('Japan'))
           + timedelta(hours=h, minutes=m))

process_30hour(date(2024, 12, 20), '25:25')

Output:
datetime.datetime(2024, 12, 21, 1, 25, tzinfo=zoneinfo.ZoneInfo(key='Japan'))

The same logic can be used vectorially with pandas:
df = pd.DataFrame({'date': ['2024-12-20 20:25', '2024-12-20 25:25']})

# split string
tmp = df['date'].str.split(expand=True)

# convert to datetime/timedelta, combine
df['datetime'] = pd.to_datetime(tmp[0]) + pd.to_timedelta(tmp[1]+':00')

For fun, as a one-liner:
df['datetime'] = (df['date'].add(':00')
                  .str.split(expand=True)
                  .astype({0: 'datetime64[ns]',
                           1: 'timedelta64[ns]'})
                  .sum(axis=1)
                 )

Output:
               date            datetime
0  2024-12-20 20:25 2024-12-20 20:25:00
1  2024-12-20 25:25 2024-12-21 01:25:00

",numpy
how to calculate daily weights which satisfy certain conditions,"I have the following pandas dataframe which represents the consumption of 7 days (day_0 is today, day_-1 is yesterday etc) of 10 people (ids):
import pandas as pd
import numpy as np

df = pd.DataFrame(np.random.randint(8, 15, size=(10, 7)))
df.columns = ['day_0', 'day_-1', 'day_-2', 'day_-3', 'day_-4', 'day_-5', 'day_-6']
df.index.name = 'id'

print(df.reset_index())

   id  day_0  day_-1  day_-2  day_-3  day_-4  day_-5  day_-6
0   0    10      10      14       8      14      14      14
1   1    10      13      11      11       8      10      10
2   2    10      12       9      12       9      10      10
3   3    12      12       9      11       9      12      13
4   4    12      13       8      12       8      11       9
5   5    13       9       8      13       9      12      10
6   6     8       9       8      14       8      13      14
7   7    13      10      14      12       8       9      11
8   8     8       8      10      12      11      14      14
9   9    14      13      13       9      11      14      13

I would like to find daily weights (so in total 7 weights: w_0, w_-1, w_-2, w_-3, w_-4, w_-5, w_-6) which need to have the following properties:

w_0 > w_-1 > w_-2 > ... > w_-6 > 0
w_0 + w_-1 + w_-2 + ... + w_-6 = 7
the weighted average for exactly 7 out of 10 ids to be below a threshold (e.g. 11)

I can achieve prerequisites 1 & 2 by using the exponential decay function and later normalizing:
import numpy as np

n = 7

_lambda = 0.5

# Calculate the weights using exponential decay
weights = np.exp(-_lambda * np.arange(n))

# Normalize the weights so that their sum is equal to the length of the time series
weights *= n / np.sum(weights)

But I don't know how I could apply also prerequisite 3.
Is that possible? How can I do that in python?
","This does not use exponential decay, because that doesn't seem particularly useful to meet your requirements. Define an ILP with a disjunction constraint that at exactly one combination of n out of m IDs has a weighted mean less than or equal to a threshold:
import io

import numpy as np
import pandas as pd
import scipy.sparse
from scipy.optimize import milp, Bounds, LinearConstraint

with io.StringIO(
'''id,  0, -1, -2, -3, -4, -5, -6
    0, 10, 10, 14,  8, 14, 14, 14
    1, 10, 13, 11, 11,  8, 10, 10
    2, 10, 12,  9, 12,  9, 10, 10
    3, 12, 12,  9, 11,  9, 12, 13
    4, 12, 13,  8, 12,  8, 11,  9
    5, 13,  9,  8, 13,  9, 12, 10
    6,  8,  9,  8, 14,  8, 13, 14
    7, 13, 10, 14, 12,  8,  9, 11
    8,  8,  8, 10, 12, 11, 14, 14
    9, 14, 13, 13,  9, 11, 14, 13
''') as f:
    df = pd.read_csv(f, skipinitialspace=True, index_col=0)
df.columns = pd.Index(name='day', data=df.columns.astype(int))


m, n = df.shape  # number of IDs, days

'''
LP variables:
n weights
m weighted mean threshold binary predicates
'''

# The weight sum must be equal to n
sum_constraint = LinearConstraint(
    A=np.concatenate((
        np.ones(n), np.zeros(m),
    )),
    lb=n, ub=n,
)

# The weights must be strictly decreasing by this amount
min_decrease = 1e-2  # chosen fully arbitrarily
antimonotonic_constraint = LinearConstraint(
    A=scipy.sparse.diags_array(
        (
            np.ones(shape=n - 1),
            np.full(shape=n - 1, fill_value=-1),
        ),
        offsets=(0, 1), shape=(n - 1, n + m), format='csc',
    ),
    lb=min_decrease,
)

'''
For each binary threshold predicate,
pred = 1 iff weights.df_values/n <= threshold
lower bound:
pred >= 1 - (weights.values)/n/threshold
weights.values/threshold + pred*n >= n
upper bound:
pred <= 2 - (weights.values)/n/threshold
weights.values/threshold + pred*n <= 2*n
'''
threshold = 11
mean_constraint = LinearConstraint(
    A=scipy.sparse.hstack(
        (
            df.values/threshold,
            scipy.sparse.diags_array(
                np.full(shape=m, fill_value=n),
            ),
        ),
        format='csc',
    ),
    lb=n, ub=2*n,
)

# Exactly this many out of m IDs must be at the threshold or lower
n_up_to_threshold = 6
disjunction_constraint = LinearConstraint(
    A=np.concatenate((np.zeros(n), np.ones(m))),
    lb=n_up_to_threshold, ub=n_up_to_threshold,
)

result = milp(
    c=np.zeros(n + m),  # no optimisation objective
    integrality=np.concatenate((
        np.zeros(shape=n, dtype=np.uint8),  # weights are continuous
        np.ones(shape=m, dtype=np.uint8),   # predicates are binary
    )),
    bounds=Bounds(
        lb=np.concatenate((
            np.full(shape=n, fill_value=1e-2),  # minimum weight, arbitrary
            np.zeros(shape=m),  # binary predicate
        )),
        ub=np.concatenate((
            np.full(shape=n, fill_value=np.inf),
            np.ones(shape=m),  # binary predicate
        )),
    ),
    constraints=(
        sum_constraint,
        antimonotonic_constraint,
        mean_constraint,
        disjunction_constraint,
    ),
)
if not result.success:
    raise ValueError(result.message)

weights, threshold_preds = np.split(result.x, (n,))
means = df @ (weights/n)

print('weights =')
print(weights)
print('threshold predicates =')
print(threshold_preds)
print('means =')
print(means)

weights =
[2.96714286 1.01571429 1.00571429 0.99571429 0.98571429 0.02
 0.01      ]
threshold predicates =
[1. 1. 1. 0. 1. 0. 1. 0. 1. 0.]
means =
id
0    10.870612
1    10.439592
2    10.290204
3    11.005714
4    11.000000
5    11.130816
6     9.021429
7    11.847755
8     9.304490
9    12.576122
dtype: float64

Approximate solutions
To turn the hard threshold constraint into a soft constraint where the threshold moves to accommodate the input system,
import io

import numpy as np
import pandas as pd
import scipy.sparse
from scipy.optimize import milp, Bounds, LinearConstraint

with io.StringIO(
'''id,  0, -1, -2, -3, -4, -5, -6
    0, 10, 10, 14,  8, 14, 14, 14
    1, 10, 13, 11, 11,  8, 10, 10
    2, 10, 12,  9, 12,  9, 10, 10
    3, 12, 12,  9, 11,  9, 12, 13
    4, 12, 13,  8, 12,  8, 11,  9
    5, 13,  9,  8, 13,  9, 12, 10
    6,  8,  9,  8, 14,  8, 13, 14
    7, 13, 10, 14, 12,  8,  9, 11
    8,  8,  8, 10, 12, 11, 14, 14
    9, 14, 13, 13,  9, 11, 14, 13
''') as f:
    df = pd.read_csv(f, skipinitialspace=True, index_col=0)
df.columns = pd.Index(name='day', data=df.columns.astype(int))


m, n = df.shape  # number of IDs, days

'''
LP variables:
n weights
m weighted mean threshold binary predicates
1 threshold
1 threshold error
'''

# The weight sum must be equal to n
sum_constraint = LinearConstraint(
    A=np.concatenate((
        np.ones(n),           # weights
        np.zeros(m + 1 + 1),  # preds, threshold, error
    )),
    lb=n, ub=n,
)

# The weights must be strictly decreasing by this amount
min_decrease = 1e-3  # chosen fully arbitrarily
antimonotonic_constraint = LinearConstraint(
    A=scipy.sparse.diags_array(
        (
            np.ones(shape=n - 1),                 # current weight
            np.full(shape=n - 1, fill_value=-1),  # next weight
        ),
        offsets=(0, 1),                # current and next weight index
        shape=(n - 1, n + m + 1 + 1),  # after the weight positions, everything is zero
        format='csc',
    ),
    lb=min_decrease,
)

'''
For each binary threshold predicate,
pred = 1 iff weights.df_values/n <= threshold
Upper bound (forces predicate to 0):
weights.df_values + pM - n*threshold <= M
Lower bound (forces predicate to 1):
weights.df_values + pM - n*threshold >= 0
'''
M = 2*df.sum(axis=1).max()
threshold_constraint = LinearConstraint(
    A=scipy.sparse.hstack(
        (
            df.values,                 # weights
            scipy.sparse.diags_array(  # predicates
                np.full(shape=m, fill_value=M),
            ),
            np.full(shape=(m, 1), fill_value=-n),  # threshold
            scipy.sparse.csc_array((m, 1)),        # error (0)
        ),
        format='csc',
    ),
    lb=0, ub=M,
)

# Exactly this many out of m IDs must be at the threshold or lower
n_up_to_threshold = 5
count_constraint = LinearConstraint(
    A=np.concatenate((
        np.zeros(n),      # weights
        np.ones(m),       # predicates
        np.zeros(1 + 1),  # threshold, error
    )),
    lb=n_up_to_threshold, ub=n_up_to_threshold,
)

'''
The absolute error must be relative to the threshold
error >= target - threshold    threshold + error >= target
error >= threshold - target    threshold - error <= target
'''
target_threshold = 12
error_constraint = LinearConstraint(
    A=scipy.sparse.hstack(
        (
            scipy.sparse.csc_array((2, n + m)),  # weights, predicates
            np.array((
                (1, +1),  # threshold, error
                (1, -1),
            )),
        ),
        format='csc',
    ),
    lb=(target_threshold, -np.inf),
    ub=(np.inf, target_threshold),
)

result = milp(
    c=np.concatenate((
        np.zeros(n + m + 1),  # weights, preds, threshold are non-optimised
        np.ones(1),           # threshold error is minimised
    )),
    integrality=np.concatenate((
        np.zeros(shape=n, dtype=np.uint8),  # weights are continuous
        np.ones(shape=m, dtype=np.uint8),   # predicates are binary
        (0, 0),                             # threshold, error are continuous
    )),
    bounds=Bounds(
        lb=np.concatenate((
            np.full(shape=n, fill_value=1e-3),  # minimum weight, arbitrary
            np.zeros(shape=m),                  # binary predicate
            (-np.inf, -np.inf),                 # threshold, error unbounded
        )),
        ub=np.concatenate((
            np.full(shape=n, fill_value=np.inf),  # weight unbounded
            np.ones(shape=m),                     # binary predicate
            (np.inf, np.inf),                     # threshold, error unbounded
        )),
    ),
    constraints=(
        sum_constraint,
        antimonotonic_constraint,
        threshold_constraint,
        count_constraint,
        error_constraint,
    ),
)
if not result.success:
    raise ValueError(result.message)

weights, preds, (threshold,), (error,) = np.split(result.x, (n, n+m, n+m+1))
means = df @ (weights/n)

print(f'threshold = {threshold:.3f}')
print(f'error from target of {target_threshold} = {error:.6f}')
print('weights =')
print(weights)
print('threshold predicates =')
print(preds)
print('means =')
print(means.values)

threshold = 11.996
error from target of 12 = 0.003857
weights =
[6.975e+00 1.000e-02 5.000e-03 4.000e-03 3.000e-03 2.000e-03 1.000e-03]
threshold predicates =
[1. 1. 1. 0. 0. 0. 1. 0. 1. 0.]
means =
[10.00514286 10.00471429 10.00285714 11.99614286 11.99614286 12.98828571
  8.00714286 12.99228571  8.00757143 13.99357143]

",numpy
valueerror setting an array element with a sequence in vaex dataframe,"I've been given a CSV file from a previous project and I'm supposed to prepare some scripts with Python to plot the value it contains. The dataset in this CSV file holds data from electric and vibration signals. The data I'm interested in is stored at a column, ""DecompressedValue"", where each row holds a 16.000-elements-long array of float values, which represents a vibration/electric signal.
I want to use Vaex to exploit its higher performance features, but I found what I think is a bug when processing the signals. I started adapting code which works in Pandas.
import pandas as pd
import json 
signal_df = pd.read_csv('csv_test.csv', sep=';')
# The DecompressedValue column, despite being stored as a regular array, is read a long string, so in order to turn it into an array, json.loads() has to be applied to each value of the column
signal_df.DecompressedValue = signal_df.DecompressedValue.apply(lambda r: json.loads(r))

However, when trying to replicate the same functionality in Vaex, even if this code runs correctly, trying to access the dataframe after that produces an error (find vaex_test.csv for testing this code here).
import vaex

test = vaex.from_csv('vaex_test.csv', sep=';')
test['DecompressedValue'] = test['DecompressedValue'].apply(lambda r: json.loads(r))
test.head()

This produce a ValueError:
[12/19/24 12:50:48] ERROR    error evaluating: DecompressedValue at rows 0-5                      [dataframe.py](file:///C:/Users/user/AppData/Local/anaconda3/envs/py310env/lib/site-packages/vaex/dataframe.py):[4101](file:///C:/Users/user/AppData/Local/anaconda3/envs/py310env/lib/site-packages/vaex/dataframe.py#4101)
                             multiprocessing.pool.RemoteTraceback:                                                 
                             """"""                                                                                   
                             Traceback (most recent call last):                                                    
                               File                                                                                
                             ""c:\Users\user\AppData\Local\anaconda3\envs\py310env\lib\mu                  
                             ltiprocessing\pool.py"", line 125, in worker                                           
                                 result = (True, func(*args, **kwds))                                              
                               File                                                                                
                             ""c:\Users\user\AppData\Local\anaconda3\envs\py310env\lib\si                  
                             te-packages\vaex\expression.py"", line 1629, in _apply                                 
                                 result = np.array(result)                                                         
                             ValueError: setting an array element with a sequence. The requested                   
                             array has an inhomogeneous shape after 1 dimensions. The detected                     
                             shape was (5,) + inhomogeneous part.                                                  
                             """"""

I've reviewed questions with the same error but I don't think they are applicable since those questions are usually related to numpy arrays and I feel my problem is more related to Vaex idiosyncrasy.
","DataFrames in Pandas and Vaex are different.
To get the lists in your csv file inside Vaex DataFrame as lists and not strings, one way would be to let Pandas do the formatting and use vaex from_pandas:
test_pd = pd.read_csv('vaex_test.csv')
test_pd['DecompressedValue'] = test_pd['DecompressedValue'].apply(lambda r: json.loads(r)) 

test = vaex.from_pandas(test_pd)

print(test.head())
print(type(test['DecompressedValue'] ))
print(test[3])
print(test[3][0])  # 4th list from csv
print(test[3][0][0])

 #  DecompressedValue
   0  '[-0.004518906585872173, -0.004478906746953726, ...
   1  '[-0.0005845219711773098, -0.0002945219748653471...
   2  '[-0.006645397283136845, -0.006435397081077099, ...
   3  '[0.003976251929998398, 0.0019852519035339355, 0...
   4  '[0.003452450269833207, 0.0017284504137933254, 0...
   
 <class 'vaex.expression.Expression'>
 
 [[0.003976251929998398, 0.0019852519035339355, ...
 
 [0.003976251929998398, 0.0019852519035339355, ...

0.003976251929998398

",numpy
how to convert polars dataframe to numpy array which has certain dims,"I have a Polars DataFrame with 300 basins, each basin having 100,000 time records, and each time record consisting of 40 variables, totaling 30 million rows and 40 variables. How can I reconstruct it into a NumPy array with the shape (300, 100,000, 40) without disrupting the corresponding data indices?
Example：
shape: (10, 7)
┌──────────────┬─────────────┬─────────────┬─────────────┬─────────────┬─────────────┬─────────────┐
│ HQprecipitat ┆ IRprecipita ┆ precipitati ┆ precipitati ┆ randomError ┆ basin_id    ┆ time        │
│ ion          ┆ tion        ┆ onCal       ┆ onUncal     ┆ ---         ┆ ---         ┆ ---         │
│ ---          ┆ ---         ┆ ---         ┆ ---         ┆ f32         ┆ str         ┆ datetime[μs │
│ f32          ┆ f32         ┆ f32         ┆ f32         ┆             ┆             ┆ ]           │
╞══════════════╪═════════════╪═════════════╪═════════════╪═════════════╪═════════════╪═════════════╡
│ null         ┆ null        ┆ null        ┆ null        ┆ null        ┆ anhui_62909 ┆ 1980-01-01  │
│              ┆             ┆             ┆             ┆             ┆ 400         ┆ 09:00:00    │
│ null         ┆ null        ┆ null        ┆ null        ┆ null        ┆ anhui_62909 ┆ 1980-01-01  │
│              ┆             ┆             ┆             ┆             ┆ 400         ┆ 12:00:00    │
│ null         ┆ null        ┆ null        ┆ null        ┆ null        ┆ anhui_62909 ┆ 1980-01-01  │
│              ┆             ┆             ┆             ┆             ┆ 400         ┆ 15:00:00    │
│ null         ┆ null        ┆ null        ┆ null        ┆ null        ┆ anhui_62909 ┆ 1980-01-01  │
│              ┆             ┆             ┆             ┆             ┆ 400         ┆ 18:00:00    │
│ null         ┆ null        ┆ null        ┆ null        ┆ null        ┆ anhui_62909 ┆ 1980-01-01  │
│              ┆             ┆             ┆             ┆             ┆ 400         ┆ 21:00:00    │
│ null         ┆ null        ┆ null        ┆ null        ┆ null        ┆ anhui_62909 ┆ 1980-01-02  │
│              ┆             ┆             ┆             ┆             ┆ 400         ┆ 00:00:00    │
│ null         ┆ null        ┆ null        ┆ null        ┆ null        ┆ anhui_62909 ┆ 1980-01-02  │
│              ┆             ┆             ┆             ┆             ┆ 400         ┆ 03:00:00    │
│ null         ┆ null        ┆ null        ┆ null        ┆ null        ┆ anhui_62909 ┆ 1980-01-02  │
│              ┆             ┆             ┆             ┆             ┆ 400         ┆ 06:00:00    │
│ null         ┆ null        ┆ null        ┆ null        ┆ null        ┆ anhui_62909 ┆ 1980-01-02  │
│              ┆             ┆             ┆             ┆             ┆ 400         ┆ 09:00:00    │
│ null         ┆ null        ┆ null        ┆ null        ┆ null        ┆ anhui_62909 ┆ 1980-01-02  │
│              ┆             ┆             ┆             ┆             ┆ 400         ┆ 12:00:00    │
└──────────────┴─────────────┴─────────────┴─────────────┴─────────────┴─────────────┴─────────────┘
# It should be reshaped to a numpy array which shape is (1, 10, 7)
# 1 means amount of basins, 10 means amount of times, 7 means width or amount of variables.

","Now I use group_by and slice to complete this.
station_len = len(self.x['basin_id'].unique())
x_truncated = (self.x.group_by('basin_id', maintain_order=True).agg(pl.all().slice(0, len(self.x) // station_len)).
                       explode(pl.exclude(""basin_id""))

",numpy
broadcasting multiple versions of x_data that pair with the same y_data,"My deep learning architecture accepts an input vector with size 512 and an output vector with size 512 too.
The problem is I have X_data version that pairs with the same y_data.
I have these tensors:
(4, 8, 512) -> (batch_size, number of X_data version, input size to model architecture) (list of X_data)

(4, 512) -> (batch_size, output size to model architecture) (y_data)

This means:
X_data[0,0,:] is pairing with y_data[0,:]
X_data[0,1,:] is pairing with y_data[0,:]
...
X_data[0,7,:] is pairing with y_data[0,:]
X_data[1,0,:] is pairing with y_data[1,:]
X_data[1,1,:] is pairing with y_data[1,:]
...
X_data[1,7,:] is pairing with y_data[1,:]
...
X_data[3,7,:] is pairing with y_data[3,:]

What is the final tensor shape of X_data and y_data so that I can train the model?
Could you do that in NumPy?
","To reshape the arrays into (32, 512) arrays with X_data and y_data matched the way you specify, you could do something like this:
import numpy as np

X_data = np.random.rand(4, 8, 512)
y_data = np.random.rand(4, 512)

X_data, y_data = np.broadcast_arrays(X_data, y_data[:, None, :])
X_data = X_data.reshape(32, 512)
y_data = y_data.reshape(32, 512)
``

",numpy
when should i use hstackvstack vs append vs concatenate vs column_stack,"Simple question: what is the advantage of each of these methods. It seems that given the right parameters (and ndarray shapes) they all work seemingly equivalently. Do some work in place? Have better performance? Which functions should I use when?
","All the functions are written in Python except np.concatenate.  With an IPython shell you just use ??.
If not, here's a summary of their code:
vstack
concatenate([atleast_2d(_m) for _m in tup], 0)
i.e. turn all inputs in to 2d (or more) and concatenate on first

hstack
concatenate([atleast_1d(_m) for _m in tup], axis=<0 or 1>)

colstack
transform arrays with (if needed)
    array(arr, copy=False, subok=True, ndmin=2).T

append
concatenate((asarray(arr), values), axis=axis)

In other words, they all work by tweaking the dimensions of the input arrays, and then concatenating on the right axis.  They are just convenience functions.

And newer np.stack:
arrays = [asanyarray(arr) for arr in arrays]
shapes = set(arr.shape for arr in arrays)
result_ndim = arrays[0].ndim + 1
axis = normalize_axis_index(axis, result_ndim)
sl = (slice(None),) * axis + (_nx.newaxis,)

expanded_arrays = [arr[sl] for arr in arrays]
concatenate(expanded_arrays, axis=axis, out=out)

That is, it expands the dims of all inputs (a bit like np.expand_dims), and then concatenates.  With axis=0, the effect is the same as np.array.
hstack documentation now adds:

The functions concatenate, stack and
  block provide more general stacking and concatenation operations.

np.block is also new.  It, in effect, recursively concatenates along the  nested lists.
",numpy
get float64 for numpy calculation,"def chi2_distance(a,b):
   
    d=1-(np.dot(a,b)/(norm(a,axis=1)*norm(b)))
      
    return d

i get the variable display in 8 decimals in both two codes, although i forcely put the np dtype to float64 in below.
a.shape is r*n and b shape is (n,)
i did this :
def chi2_distance(a,b):
    a = a.astype(np.float64)
    b = b.astype(np.float64)
    d=1-(np.dot(a,b)/(norm(a,axis=1)*norm(b)))
      
    return d

i still get 8 decimal accuray in results
","@hpaulj answered correct also. Python and numpy use float64 for accuracy and calculation in fact. But numpy shows the numbers and variables in 8 decimals by its default, meanwhile python itself like to.list() as an instance shows the numbers 15 to 17 deciamals. This is just diplay, but both of them use float64 accuracy in calculation. Here in below code if you even delete (.float64) for two lines, again you get the same answer, as numpy and python by default calculate in float64. I got my problem and mistake and i wanted to share it with you. Maybe you had same mistake like me.
import numpy as np
from numpy.linalg import norm
def chi2_distance(a,b):
  a = a.astype(np.float64)
  b = b.astype(np.float64)
  d=(1-(np.dot(a,b)/(norm(a,axis=1)*norm(b)))).tolist()
  
  return d
k=np.array([[8.34567,2,4],[10000.99887,6,7]])
kk=np.array([100.3456,200,300])
print(k.shape)
print(kk.shape)
hh=chi2_distance(k,kk)
print(hh)

",numpy
finding all 1d arrays within a numpy array,"Given a numpy array of dimension n with each direction having length m, I would like to iterate through all 1-dimensional arrays of length m.
For example, consider:
import numpy as np
x = np.identity(4)
array([[1., 0., 0., 0.],
       [0., 1., 0., 0.],
       [0., 0., 1., 0.],
       [0., 0., 0., 1.]])

then I would like to find all all 1-dimensional arrays with length 4.  So the result should include all 4 rows, all 4 columns, and the 2 diagonals:
[x[i,:] for i in range(4)] + [x[:,i] for i in range(4)] + [np.array([x[i,i] for i in range(4)])] + [np.array([x[3-i,i] for i in range(4)])]

It's unclear to me how to generalize this to higher dimensional arrays since the position of the "":"" in the slice needs to iterate as well.  In a higher dimensional analogue with
slices = [[0, 0], [0, 1], [0, 2], [1, 0], [1, 1], [1, 2], [2, 0], [2, 1], [2, 2]]
we can get
[x[i,j,:] for (i,j) in slices]

but then I'm not sure how to proceed to iterate through the permutations of [i,j,:].
","Although the comprehensions are readable, for large arrays you probably want to use what numpy gives you:
import numpy as np

n = 4
# random n*n numpy array
arr = np.random.rand(n, n)
print(arr)

# this has all the data you need, relatively efficiently - but not in 1D shape
result = (
    np.vsplit(arr, n) +
    np.hsplit(arr, n) + 
    [arr.diagonal()] +
    [arr[np.arange(n), np.arange(n - 1, -1, -1)]]
)

# you can just flatten them as you used them:
for xs in result:
    print(xs.ravel())

# or flatten them into a new result:
result_1d = [xs.ravel() for xs in result]

Edit: user @Matt correctly pointed out in the comments that this solution only works for the case of a 2-dimensional array.
Things get a bit more complicated for an arbitrary number of dimensions n with size m across all dimensions. This works, but given the complexity, can probably be improved upon for simplicity:
import numpy as np
import itertools as it

# random n*n numpy array
m = 2
n = 3
arr = np.random.rand(*([m] * n))
print(arr)


def get_all_lines(arr):
    ndim = arr.ndim
    size = arr.shape[0]  # assuming the same size for all dimensions
    # generate each 1d slice along and across each axis
    for fixed in it.product(range(size), repeat=ndim - 1):
        for axis in range(ndim):
            yield arr[fixed[:axis] + (slice(None, ),) + fixed[axis:]]
    # generate each 1d diagonal for each combination of axes
    for d_dim in range(2, ndim+1):  # d_dim is the number of varying axes
        for fixed in it.product(range(size), repeat=(ndim - d_dim)):  # fixed indices for the other axes
            of, od = 0, 0  # offsets for accessing fixed values and directions
            # each varying axis can be traversed in one of two directions
            for d_tail in it.product((0, 1), repeat=d_dim - 1):  # dir is the direction for each varying axis
                d = (1, *d_tail)[::-1]  # deduplicate and reverse the direction
                for axes in it.combinations(range(ndim), d_dim):  # axes to vary
                    fm = d_dim * (d_dim + 1) // 2 - sum(axes)  # first dimension with a fixed index
                    dm = min(axes)  # first dimension with a varying index
                    yield [
                        arr[*[fixed[of := 0 if j == fm else of + 1]
                              if j not in axes else
                              (i if d[od := 0 if j == dm else od + 1] else size - (i + 1))
                              for j in range(ndim)]] for i in range(size)
                    ]


lines = get_all_lines(arr)
for line in lines:
    print(line)

The mentioned ""deduplication"" avoids including each diagonal twice (once in both directions).
Also note that this yields 1d arrays as well as lists of numbers, you can of course cast these appropriately.
",numpy
speed up numpy looking for best indices,"I have a numpy array that maps x-y-coordinates to the appropriate z-coordinates. For this I use a 2D array that represents x and y as its axes and contains the corresponding z values:
import numpy as np
x_size = 2000
y_size = 2500
z_size = 400
rng = np.random.default_rng(123)
z_coordinates = np.linspace(0, z_size, y_size) + rng.laplace(0, 1, (x_size, y_size))

So each of the 2000*2500 x-y-points is assigned a z-value (float between 0 and 400). Now I want to look up for each integer z and integer x which is the closest y-value, essentially creating a map that is of shape (x_size, z_size) and holds the best y-values.
The simplest approach is creating an empty array of target shape and iterating over each z value:
y_coordinates = np.empty((x_size, z_size), dtype=np.uint16)
for i in range(z_size):
    y_coordinates[:, i] = np.argmin(
        np.abs(z_coordinates - i),
        axis=1,
    )

however this takes about 11 s on my machine, which unfortunately is way to slow.
Surely using a more vectorised approach would be faster, such as:
y_coordinates = np.argmin(
    np.abs(
        z_coordinates[..., np.newaxis] - np.arange(z_size)
    ),
    axis=1,
)

Surprisingly this runs about 60% slower than the version above (tested at 1/10th size, since at full size this uses excessive memory).
Also wrapping the code blocks in functions and decorating them with numba's @jit(nopython=True) doesn't help.
How can I speed up the calculation?
","This answer provide an algorithm with an optimal complexity: O(x_size * (y_size + z_size)). This algorithm is the fastest one proposed so far (by a large margin). It is implemented in Numba using multiple threads.

Explanation of the approach
The idea is that there is no need to iterate over all Z values : we can iterate over z_coordinates line by line, and for each line of z_coordinates, we fill an array used to find the nearest value for each possible z. The best candidate for the value z is stored in arr[z].
In practice, there are tricky corner cases making things a more complicated. For example, due to rounding, I decided to fill the neighbours of arr[z] (i.e. arr[z-1] and arr[z+1]) so to make the algorithm simpler. Moreover, when there are not enough values so arr cannot be fully filled by all the values in a line of z_coordinates, we need to fill the holes in the arr. In some more complicated cases (combining rounding issue while kind of holes in arr), we need to correct the values in arr (or operate on more distant neighbours which is not efficient). The number of step in the correction function should always be a small constant, certainly <= 3 (it nerver reached 3 in practice in my tests). Note that, in practice, no corner case happens on the specific input dataset provided.
Each line is computed in parallel using multiple threads. I assume the array is not too small (to avoid to deal with more corner cases in the code and make it simpler) which should not be an issue. I also assume there are no special values like NaN in z_coordinates.

Resulting code
Here is the final code:
import numba as nb
import numpy as np

# Fill the missing values in the value-array if there is not enough values (e.g. pretty large z_size)
# (untested)
@nb.njit('(float64[::1], uint16[::1], int64)')
def fill_missing_values(all_val, all_pos, z_size):
    i = 0
    while i < z_size:
        # If there is a missing value
        if all_pos[i] == 0xFFFF:
            j = i
            while j < z_size and all_pos[j] == 0xFFFF:
                j += 1
            if i == 0:
                # Fill the hole based on 1 value (lower bound)
                assert j+1 < z_size and all_pos[j] == 0xFFFF and all_pos[j] != 0xFFFF
                for i2 in range(i, j):
                    all_val[i2] = all_val[j+1]
                    all_pos[i2] = all_pos[j+1]
            elif j == z_size:
                # Fill the hole based on 1 value (upper bound)
                assert i-1 >= 0 and all_pos[i-1] != 0xFFFF and all_pos[i] == 0xFFFF
                for i2 in range(i, j):
                    all_val[i2] = all_val[i-1]
                    all_pos[i2] = all_pos[i-1]
            else:
                assert i-1 >= 0 and j < z_size and all_pos[i-1] != 0xFFFF and all_pos[j] != 0xFFFF
                lower_val = all_val[i-1]
                lower_pos = all_pos[i-1]
                upper_val = all_val[j]
                upper_pos = all_pos[j]
                # Fill the hole based on 2 values
                for i2 in range(i, j):
                    if np.abs(lower_val - i2) < np.abs(upper_val - i2):
                        all_val[i2] = lower_val
                        all_pos[i2] = lower_pos
                    else:
                        all_val[i2] = upper_val
                        all_pos[i2] = upper_pos
            i = j
        i += 1

# Correct values in very pathological cases where z_size is big so there are not enough 
# values added to the value-array causing some values of the value-array to be incorrect.
# The number of `while` iteration should be always <= 3 in practice
@nb.njit('(float64[::1], uint16[::1], int64)')
def correct_values(all_val, all_pos, z_size):
    while True:
        stop = True
        for i in range(0, z_size-1):
            current = np.abs(all_val[i] - i)
            if np.abs(all_val[i+1] - i) < current:
                all_val[i] = all_val[i+1]
                all_pos[i] = all_pos[i+1]
                stop = False
        for i in range(1, z_size):
            current = np.abs(all_val[i] - i)
            if np.abs(all_val[i-1] - i) < current:
                all_val[i] = all_val[i-1]
                all_pos[i] = all_pos[i-1]
                stop = False
        if stop:
            break

@nb.njit('(float64[:,::1], int64)', parallel=True)
def compute_fastest(z_coordinates, z_size):
    x_size, y_size = z_coordinates.shape
    assert y_size >= 2 and z_size >= 2
    y_coordinates = np.empty((x_size, z_size), dtype=np.uint16)
    for x in nb.prange(x_size):
        all_pos = np.full(z_size, 0xFFFF, dtype=np.uint16)
        all_val = np.full(z_size, np.inf, dtype=np.float64)
        for y in range(0, y_size):
            val = z_coordinates[x, y]
            #assert not np.isnan(val)
            if val < 0: # Lower bound
                i = 0
                if np.abs(val - i) < np.abs(all_val[i] - i):
                    all_val[i] = val
                    all_pos[i] = y
            elif val >= z_size: # Upper bound
                i = z_size - 1
                if np.abs(val - i) < np.abs(all_val[i] - i):
                    all_val[i] = val
                    all_pos[i] = y
            else: # Inside the array of values
                offset = np.int32(val)
                for i in range(max(offset-1, 0), min(offset+2, z_size)):
                    if np.abs(val - i) < np.abs(all_val[i] - i):
                        all_val[i] = val
                        all_pos[i] = y
        fill_missing_values(all_val, all_pos, z_size)
        correct_values(all_val, all_pos, z_size)
        for i in range(0, z_size):
            y_coordinates[x, i] = all_pos[i]
    return y_coordinates


Performance results
Here are performance results on my machine with a i5-9600KF CPU (6 cores), Numpy 1.24.3, Numba 58.1, on Windows, for the provided input:
Naive fully vectorized code in the question:   113000 ms  (slow due to swapping)
Naive loop in the question:                      8460 ms
ZLi's implementation:                            1964 ms
Naive Numba parallel code with loops:             402 ms
PaulS' implementation:                            262 ms
This Numba code:                                   12 ms  <----------

Note the fully-vectorized code in the question use so much memory it cause memory swapping. It completely saturate my 32 GiB of RAM (about 24 GiB was available in practice) which is clearly not reasonable!
Note the PaulS' implementation is about equally fast with 32-bit and 64-bit on my machine. This is probably because the operation is compute-bound on my machine (dependent of the speed of the RAM).
This Numba implementation is 705 times faster than the fastest implementation in the question. It is also 22 times faster than the best answer so far! It also use a tiny amount of additional RAM for the computation (<1 MiB).
",numpy
issue about displaying circle on an existing image,"When I tried to illustrate the feature clusters in the image as the color circles. After I imported two libraries: I imported these two related libraries:
from PIL import Image
from PIL import ImageDraw

I used the code like the following:
draw = ImageDraw.Draw(img1)
draw.ellipse((x-10, y-10, x+10, y+10), fill=(255,0,0,0))

The error I get is as the following:
File ""assi_6.python"", line 85, in main
    draw = ImageDraw.Draw(img1)
  File ""/Users/qdai/anaconda/envs/vision/lib/python2.7/site-packages/PIL/ImageDraw.py"", line 284, in Draw
    return ImageDraw(im, mode)
  File ""/Users/qdai/anaconda/envs/vision/lib/python2.7/site-packages/PIL/ImageDraw.py"", line 59, in __init__
    im.load()
AttributeError: 'numpy.ndarray' object has no attribute ‘load’

","Mentioning numpy.ndarray in the error message gives a hint that the object img1 is not an image as PIL needs it for its methods. 
Try to create/load img1 using PIL methods to avoid that error. 
",numpy
how to replace specific entries of a numpy array based on its content,"So let's say I have a simple matrix made out of ndarrays (just an example of how  part of the data might look like):
import numpy as np
a = np.asarray([['1.0', 'Miami'],
   ['2.0', 'Boston'],
   ['1.4', 'Miami']]) 

I want to do data analysis in this complex data set ;) - I want to transform 'Miami' in 0 and Boston in 1 in order to use a really fancy ML algorithm.
What is a good way to accomplish this in Python?
(I am not asking for the obvious one of iterating and using a dictionary / if sentence to replace the entry) but more if there's a better way using NumPy or native Python to do this.
","pandas is a good tool for this.
First convert the array to a DataFrame:
In [11]: import pandas as pd

In [12]: df = pd.DataFrame(a, columns=['value', 'city'])

and then replace entries from the city column:
In [13]: df.city = df.city.replace({'Miami': 0, 'Boston': 1})

In [14]: df
Out[14]:
  value city
0   1.0    0
1   2.0    1
2   1.4    0

",numpy
render numpy array in fastapi,"I have found How to return a numpy array as an image using FastAPI?, however, I am still struggling to show the image, which appears just as a white square.
I read an array into io.BytesIO like so:
def iterarray(array):
    output = io.BytesIO()
    np.savez(output, array)
    yield output.get_value()

In my endpoint, my return is StreamingResponse(iterarray(), media_type='application/octet-stream')
When I leave the media_type blank to be inferred a zipfile is downloaded.
How do I get the array to be displayed as an image?
","Option 1 - Return image as bytes
The below examples show how to convert an image loaded from disk, or an in-memory image (in the form of numpy array), into bytes (using either PIL or OpenCV libraries) and return them using a custom Response directly. For the purposes of this demo, the below code is used to create the in-memory sample image (numpy array), which is based on this answer.
# Function to create a sample RGB image
def create_img():
    w, h = 512, 512
    arr = np.zeros((h, w, 3), dtype=np.uint8)
    arr[0:256, 0:256] = [255, 0, 0] # red patch in upper left
    return arr

Using PIL
Server side:
You can load an image from disk using Image.open, or use Image.fromarray to load an in-memory image (Note: For demo purposes, when the case is loading the image from disk, the below demonstrates that operation inside the route. However, if the same image is going to be served multiple times, one could load the image only once at startup and store it to the app instance, as described in this answer and this answer). Next, write the image to a buffered stream, i.e., BytesIO, and use the getvalue() method to get the entire contents of the buffer. Even though the buffered stream is garbage collected when goes out of scope, it is generally better to call close() or use the with statement, as shown here and in the example below.
from fastapi import Response
from PIL import Image
import numpy as np
import io

@app.get('/image', response_class=Response)
def get_image():
    # loading image from disk
    # im = Image.open('test.png')
    
    # using an in-memory image
    arr = create_img()
    im = Image.fromarray(arr)
    
    # save image to an in-memory bytes buffer
    with io.BytesIO() as buf:
        im.save(buf, format='PNG')
        im_bytes = buf.getvalue()
        
    headers = {'Content-Disposition': 'inline; filename=""test.png""'}
    return Response(im_bytes, headers=headers, media_type='image/png')

Client side:
The below demonstrates how to send a request to the above endpoint using Python requests module, and write the received bytes to a file, or convert the bytes back into PIL Image, as described here.
import requests
from PIL import Image

url = 'http://127.0.0.1:8000/image'
r = requests.get(url=url)

# write raw bytes to file
with open('test.png', 'wb') as f:
    f.write(r.content)

# or, convert back to PIL Image
# im = Image.open(io.BytesIO(r.content))
# im.save('test.png') 

Using OpenCV
Server side:
You can load an image from disk using cv2.imread() function, or use an in-memory image, which—if it is in RGB order, as in the example below—needs to be converted, as OpenCV uses BGR as its default colour order for images. Next, use cv2.imencode() function, which compresses the image data (based on the file extension you pass that defines the output format, i.e., .png, .jpg, etc.) and stores it in an in-memory buffer that is used to transfer the data over the network.
import cv2

@app.get('/image', response_class=Response)
def get_image():
    # loading image from disk
    # arr = cv2.imread('test.png', cv2.IMREAD_UNCHANGED)
    
    # using an in-memory image
    arr = create_img()
    arr = cv2.cvtColor(arr, cv2.COLOR_RGB2BGR)
    # arr = cv2.cvtColor(arr, cv2.COLOR_RGBA2BGRA) # if dealing with 4-channel RGBA (transparent) image

    success, im = cv2.imencode('.png', arr)
    headers = {'Content-Disposition': 'inline; filename=""test.png""'}
    return Response(im.tobytes(), headers=headers, media_type='image/png')

Client side:
On client side, you can write the raw bytes to a file, or use the numpy.frombuffer() function and cv2.imdecode() function to decompress the buffer into an image format (similar to this)—cv2.imdecode() does not require a file extension, as the correct codec will be deduced from the first bytes of the compressed image in the buffer.
url = 'http://127.0.0.1:8000/image'
r = requests.get(url=url) 

# write raw bytes to file
with open('test.png', 'wb') as f:
    f.write(r.content)

# or, convert back to image format    
# arr = np.frombuffer(r.content, np.uint8)
# img_np = cv2.imdecode(arr, cv2.IMREAD_UNCHANGED)
# cv2.imwrite('test.png', img_np)


Useful Information
Since you noted that you would like the image displayed similar to a FileResponse, using a custom Response to return the bytes should be the way to do this, instead of using StreamingResponse (as shown in your question). To indicate that the image should be viewed in the browser, the HTTP response should include the following Content-Disposition header, as described here and as shown in the above examples (the quotes around the filename are required, if the filename contains special characters):
headers = {'Content-Disposition': 'inline; filename=""test.png""'}

Whereas, to have the image downloaded rather than viewed (use attachment instead of inline):
headers = {'Content-Disposition': 'attachment; filename=""test.png""'}

If you would like to display (or download) the image using a JavaScript interface, such as Fetch API or Axios, have a look at the answers here and here.
As for StreamingResponse, if the entire numpy array/image is already loaded into memory, StreamingResponse would not be necessary at all (and certainly, should not be the preferred choice for returning data that is already loaded in memory to the client). StreamingResponse streams by iterating over the chunks provided by your iter() function. As shown in the implementation of StreamingResponse class, if the iterator/generator you passed is not an AsyncIterable, a thread from the external threadpool—see this answer for more details on that threadpool—will be spawned to run the synchronous iterator you passed, using Starlette's iterate_in_threadpool() function, in order to avoid blocking the event loop. It should also be noted that the Content-Length response header is not set when using StreamingResponse (which makes sense, since StreamingResponse is supposed to be used when you don't know the size of the response beforehand), unlike other Response classes of FastAPI/Starlette that set that header for you, so that the browser will know where the data ends. It should be kept that way, as if the Content-Length header is included (of which its value should match the overall response body size in bytes), then to the server StreamingResponse would look the same as Response, as the server would not use transfer-encoding: chunked in that case (even though at the application level the two would still differ)—take a look at Uvicorn's documentation on response headers and MDN'S documentation on Transfer-Encoding: chunked for further details. Even in cases where you know the body size beforehand, but would still need using StreamingResponse, as it would allow you to load and transfer data by specifying the chunk size of your choice, unlike FileResponse (see later on for more details), you should ensure not setting the Content-Length header on your own, e.g., StreamingResponse(iterfile(), headers={'Content-Length': str(content_length)}), as this would result in the server not using transfer-encoding: chunked (regardless of the application delivering the data to the web server in chunks, as shown in the relevant implementation).
As described in this answer:

Chunked transfer encoding makes sense when you don't know the size of
your output ahead of time, and you don't want to wait to collect it
all to find out before you start sending it to the client. That can
apply to stuff like serving the results of slow database queries, but
it doesn't generally apply to serving images.

Even if you would like to stream an image file that is saved on the disk, file-like objects, such as those created by open(), are normal iterators; thus, you could return them directly in a StreamingResponse, as described in the documentation and as shown below (if you find yield from f being rather slow, when using StreamingResponse, please have a look at this answer on how to read the file in chunks with the chunk size of your choice—which should be set based on your needs, as well as your server's resources). It should be noted that using FileResponse would also read the file contents into memory in chunks, instead of the entire contents at once. However, as can be seen in the implementation of FileResponse class, the chunk size used is pre-defined and set to 64KB. Thus, based on one's requirements, they should decide on which of the two Response classes they should use.
@app.get('/image')
def get_image():
    def iterfile():  
        with open('test.png', mode='rb') as f:  
            yield from f  
            
    return StreamingResponse(iterfile(), media_type='image/png')

Or, if the image was already loaded into memory instead, and then saved into a BytesIO buffered stream, since BytesIO is a file-like object (like all the concrete classes of io module), you could return it directly in a StreamingResponse (or, preferably, simply call buf.getvalue() to get the entire image bytes and return them using a custom Response directly, as shown earlier). In case of returning the buffered stream, as shown in the example below, please remember to call buf.seek(0), in order to rewind the cursor to the start of the buffer, as well as call close() inside a background task, in order to discard the buffer, once the response has been sent to the client.
from fastapi import BackgroundTasks

@app.get('/image')
def get_image(background_tasks: BackgroundTasks):
    # supposedly, the buffer already existed in memory
    arr = create_img()
    im = Image.fromarray(arr)
    buf = BytesIO()
    im.save(buf, format='PNG')

    # rewind the cursor to the start of the buffer
    buf.seek(0)
    # discard the buffer, after the response is returned
    background_tasks.add_task(buf.close)
    return StreamingResponse(buf, media_type='image/png')

Thus, in your case scenario, the most suited approach would be to return a custom Response directly, including your custom content and media_type, as well as setting the Content-Disposition header, as described earlier, so that the image is viewed in the browser.
Option 2 - Return image as JSON-encoded numpy array
The below should not be used for displaying the image in the browser, but it is rather added here for the sake of completeness, showing how to convert an image into a numpy array (preferably, using asarray() function), then return the data in JSON format, and finally, convert the data back to image on client side, as described in this and this answer. For faster alternatives to the standard Python json library, see this answer.
Using PIL
Server side:
from PIL import Image
import numpy as np
import json

@app.get('/image')
def get_image():
    im = Image.open('test.png')
    # im = Image.open('test.png').convert('RGBA') # if dealing with 4-channel RGBA (transparent) image 
    arr = np.asarray(im)
    return json.dumps(arr.tolist())

Client side:
import requests
from PIL import Image
import numpy as np
import json

url = 'http://127.0.0.1:8000/image'
r = requests.get(url=url) 
arr = np.asarray(json.loads(r.json())).astype(np.uint8)
im = Image.fromarray(arr)
im.save('test_received.png')

Using OpenCV
Server side:
import cv2
import json

@app.get('/image')
def get_image():
    arr = cv2.imread('test.png', cv2.IMREAD_UNCHANGED)
    return json.dumps(arr.tolist())

Client side:
import requests
import numpy as np
import cv2
import json

url = 'http://127.0.0.1:8000/image'
r = requests.get(url=url) 
arr = np.asarray(json.loads(r.json())).astype(np.uint8)
cv2.imwrite('test_received.png', arr)

",numpy
convert image from cv_64f to cv_8u,"I want to convert an image of type CV_64FC1 to CV_8UC1 in Python using OpenCV.
In C++, using convertTo function, we can easily convert image type using following code snippet:
image.convertTo(image, CV_8UC1);

I have searched on Internet but unable to find any solution without errors. Any function in Python OpenCV to convert this?
","You can convert it to a Numpy array.
import numpy as np

# Convert source image to unsigned 8 bit integer Numpy array
arr = np.uint8(image)

# Width and height
h, w = arr.shape

It seems OpenCV Python APIs accept Numpy arrays as well. I've not tested it though. Please test it and let me know the result.
",numpy
geometric series calculate quotient and number of elements from sum and first amp last element,"Creating evenly spaced numbers on a log scale (a geometric progression) can easily be done for a given base and number of elements if the starting and final values of the sequence are known, e.g., with numpy.logspace and numpy.geomspace. Now assume I want to define the geometric progression the other way around, i.e., based on the properties of the resulting geometric series. If I know the sum of the series as well as the first and last element of the progression, can I compute the quotient and number of elements?
For instance, assume the first and last elements of the progression are a_0 and a_n and the sum of the series is s_n. I know from trial and error that it works out for n=9 and r≈1.404, but how could these values be computed?
","You have to solve the following two equations for r and n:
a:= An / Ao = r^(n - 1)

and
s:= Sn / Ao = (r^n - 1) / (r - 1)

You can eliminate n by
s = (r a - 1) / (r - 1)

and solve for r. Then n follows by log(a) / log(r) + 1.

In your case, from s = 50 and a = 15, we obtain r = 7/5 = 1.4 and n = 9.048...
It makes sense to round n to 9, but then r^8 = 15 (r ~ 1.40285) and r = 1.4 are not quite compatible.
",numpy
floor and ceil with number of decimals,"I need to floor a float number with an specific number of decimals.
So:
2.1235 with 2 decimals --> 2.12
2.1276 with 2 decimals --> 2.12  (round would give 2.13 which is not what I need)

The function np.round accepts a decimals parameter but it appears that the functions ceil and floor don't accept a number of decimals and always return  a number with zero decimals.
Of course I can multiply the number by 10^ndecimals, then apply floor and finally divide by 10^ndecimals
new_value = np.floor(old_value * 10**ndecimals) / 10**ndecimals

But I'm wondering if there's a built-in function that does this without having to do the operations.
","Neither Python built-in nor numpy's version of ceil/floor support precision.
One hint though is to reuse round instead of multiplication + division (should be much faster):
def my_ceil(a, precision=0):
    return np.round(a + 0.5 * 10**(-precision), precision)

def my_floor(a, precision=0):
    return np.round(a - 0.5 * 10**(-precision), precision)

UPD:
As pointed out by @aschipfl, for whole values np.round will round to the nearest even, which will lead to unexpected results, e.g. my_ceil(11) will return 12. Here is an updated solution, free of this problem:
def my_ceil(a, precision=0):
    return np.true_divide(np.ceil(a * 10**precision), 10**precision)

def my_floor(a, precision=0):
    return np.true_divide(np.floor(a * 10**precision), 10**precision)

",numpy
have numpyconcatenate return proper subclass rather than plain ndarray,"I have a numpy array subclass, and I'd like to be able to concatenate them.
import numpy as np

class BreakfastArray(np.ndarray):
    def __new__(cls, n=1):
        dtypes=[(""waffles"", int), (""eggs"", int)]
        obj = np.zeros(n, dtype=dtypes).view(cls)
        return obj
        
b1 = BreakfastArray(n=1)
b2 = BreakfastArray(n=2)
con_b1b2 = np.concatenate([b1, b2])

print(b1.__class__, con_b1b2.__class__)

this outputs <class '__main__.BreakfastArray'> <class 'numpy.ndarray'>, but I'd like the concatenated array to also be a BreakfastArray class. It looks like I probably need to add a __array_finalize__ method, but I can't figure out the right way to do it.
","Expanding simon's solution,
this is what I settled on so other numpy functions fall-back to standard ndarray (so, numpy.unique(b2[""waffles""]) works as expected). Also a slight change to concatenate so it will work for any subclasses as well.
import numpy as np

HANDLED_FUNCTIONS = {}

class BreakfastArray(np.ndarray):
    def __new__(cls, *args, n=1, **kwargs):
        dtypes=[(""waffles"", int), (""eggs"", int)]
        obj = np.zeros(n, dtype=dtypes).view(cls)
        return obj

    def __array_function__(self, func, types, args, kwargs):
        # If we want ""standard numpy behavior"",
        # convert any BreakfastArray to ndarray views
        if func not in HANDLED_FUNCTIONS:
            new_args = []
            for arg in args:
                if issubclass(arg.__class__, BreakfastArray):
                    new_args.append(arg.view(np.ndarray))
                else:
                    new_args.append(arg)
            return func(*new_args, **kwargs)
        if not all(issubclass(t, BreakfastArray) for t in types):
            return NotImplemented
        return HANDLED_FUNCTIONS[func](*args, **kwargs)

def implements(numpy_function):
    def decorator(func):
        HANDLED_FUNCTIONS[numpy_function] = func
        return func
    return decorator

@implements(np.concatenate)
def concatenate(arrays):
    result = arrays[0].__class__(n=sum(len(a) for a in arrays))
    return np.concatenate([np.asarray(a) for a in arrays], out=result)

",numpy
how to divide 2d matrix according to specified position,"I have a feature map that needs to be divied with another 2d constant, the feature map has shape [H, W, ...] , the 2d constant has shape [H, W] , however when I use np.divide it says these two tensor can not be broadcast to a common shape:
import numpy as np

a = np.random.rand(4, 3 ,3)
b = np.random.rand(4, 3)

c = np.divide(a, b, where=b > 0)

as the numpy broadcast rules say b will be broadcast to shape [1, 4, 3] instead of [4, 3, 1] , thus I can't do this operation with boradcast.
I would like to divide my feature map with that constant tensor where the constant is not 0 to avoid zero-div error, what should I do ?
","Personally, I would probably write:
import numpy as np

np.random.seed(3)  # Set seed for reproducibility
a = np.random.rand(4, 3 ,3)
b = np.random.normal(size=(4, 3))  # Use `normal` to produce more `where` misses

b = b[..., np.newaxis]
c = np.where(b > 0, a / b, a)

However, to stay closer to your code, one could write equivalently:
import numpy as np

np.random.seed(3)  # Set seed for reproducibility
a = np.random.rand(4, 3 ,3)
b = np.random.normal(size=(4, 3))  # Use `normal` to produce more `where` misses

c = a.copy()
np.divide(a, b[..., np.newaxis], where=b[..., np.newaxis] > 0, out=c)

Notable changes to your code:

Manually add a dimension to b so that its shape is compatible with a (i.e. expand the 4×3 array to a 4×3×1 array).
Provide an explicit output array out=c with np.divide(), because its where argument only produces meaningful results in connection with a given out array (see documentation of np.divide()).
Use a copy of a as the output array, meaning that in places where b <= 0, the given a value will be the result instead (which is what you want, if I read your question correctly).

In either case, the result will be contained in c.
",numpy
unexpected unresolved attribute reference 39all39 for class 39bool39,"When I do a matrix multiplication with numpy (Python 3.12.1, numpy 1.26.4, PyCharm 2024.3.1 (Professional Edition)), I get this warning which I believe is wrong:

Unresolved attribute reference 'all' for class 'bool'

Proof:

Minimial Reproducible Example
import numpy as np

a_matrix = np.array([[1, 2], [3, 4]])
b_matrix = np.array([[10], [20]])
a = [5.0]
b = [2.0]

if ((a_matrix @ np.array([[round(a[0], 0)], [round(b[0], 0)]])) == b_matrix).all():
    print(""Success"")

","The issue is with the == operator. It appears the numpy class definition in the numpy library specifies that it returns a typing.Any with a type hint, so pycharm assumes it was the usual return type of builtins.bool.
Here is the line from the numpy library that pycharm is using, copied for context and posterity and abbreviated to the key components for brevity:
class _ArrayOrScalarCommon:
    ...
    def __eq__(self, other: Any, /) -> Any: ...
    ...
class ndarray(_ArrayOrScalarCommon, Generic[_ShapeT_co, _DType_co]):
    ...

I confirmed that the typing.Any hint causes this issue by creating a custom class and seeing what the return type hint suggested and it matched exactly. This code runs without errors for the same reason yours does, it returns a numpy.ndarray which supports the .all method.
from typing import Any
import numpy


class A:
    def __eq__(self, other) -> Any:
        return numpy.array([])


a = A()
(a == a).all()


Rearranging doesn't remove the issue, but makes the issue more obvious for the next person to come along and check your code with a static type checker by very clearly indicating what you expect the return type of that operation to be.
from typing import Any
import numpy


class A:
    def __eq__(self, other) -> Any:
        return numpy.array([])


a = A()
equal_result: numpy.ndarray = a == a
equal_result.all()


Whether you consider this to be a pycharm error by assuming that a return type of typing.Any is a builtins.bool or if it is a numpy error by type hinting typing.Any on a == between two numpy.ndarray error is up to you.
This has been discussed in the numpy community before, here is what appears to be the main discussion on the topic.
",numpy
generating two orthogonal vectors that are orthogonal to a particular direction,"What is the simplest and most efficient ways in numpy to generate two orthonormal vectors a and b such that the cross product of the two vectors equals another unit vector k, which is already known?
I know there are infinitely many such pairs, and it doesn't matter to me which pairs I get as long as the conditions axb=k and a.b=0 are satisfied. 
","The Gram-Schmidt procedure will do exactly this. For example:
>>> k  # an arbitrary unit vector k is not array. k is must be numpy class. np.array
np.array([ 0.59500984,  0.09655469, -0.79789754])

To obtain the 1st one:
>>> x = np.random.randn(3)  # take a random vector
>>> x -= x.dot(k) * k       # make it orthogonal to k
>>> x /= np.linalg.norm(x)  # normalize it

To obtain the 2nd one:
>>> y = np.cross(k, x)      # cross product with k

and to verify:
>>> np.linalg.norm(x), np.linalg.norm(y)
(1.0, 1.0)
>>> np.cross(x, y)          # same as k
array([ 0.59500984,  0.09655469, -0.79789754])
>>> np.dot(x, y)            # and they are orthogonal
-1.3877787807814457e-17
>>> np.dot(x, k)
-1.1102230246251565e-16
>>> np.dot(y, k)
0.0

",numpy
numpy check if float array contains whole numbers,"In Python, it is possible to check if a float contains an integer value using n.is_integer(), based on this QA: How to check if a float value is a whole number.
Does numpy have a similar operation that can be applied to arrays? Something that would allow the following:
>>> x = np.array([1.0 2.1 3.0 3.9])
>>> mask = np.is_integer(x)
>>> mask
array([True, False, True, False], dtype=bool)

It is possible to do something like
>>> mask = (x == np.floor(x))

or
>>> mask = (x == np.round(x))

but they involve calling extra methods and creating a bunch of temp arrays that could be potentially avoided.
Does numpy have a vectorized function that checks for fractional parts of floats in a way similar to Python's float.is_integer?
","From what I can tell, there is no such function that returns a boolean array indicating whether floats have a fractional part or not. The closest I can find is np.modf which returns the fractional and integer parts, but that creates two float arrays (at least temporarily), so it might not be best memory-wise.
If you're happy working in place, you can try something like:
>>> np.mod(x, 1, out=x)
>>> mask = (x == 0)

This should save memory versus using round or floor (where you have to keep x around), but of course you lose the original x.
The other option is to ask for it to be implemented in Numpy, or implement it yourself.
",numpy
what role does min value and max value play in reducing memory usage,"I am learning the code from this github Intrusion Detection (CIC-IDS2017)
Here is the code and the result that the authors use to reduce the memory, but I don't know why the author made adjustments based on the maximum and minimum values?
old_memory_usage = data.memory_usage().sum() / 1024 ** 2
print(f'Initial memory usage: {old_memory_usage:.2f} MB')
for col in data.columns:
    col_type = data[col].dtype
    if col_type != object:
        c_min = data[col].min()  # <-- retrieve c_min
        c_max = data[col].max()
        # Downcasting float64 to float32
        if str(col_type).find('float') >= 0 and c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:
            data[col] = data[col].astype(np.float32)

        # Downcasting int64 to int32
        elif str(col_type).find('int') >= 0 and c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:
            data[col] = data[col].astype(np.int32)

new_memory_usage = data.memory_usage().sum() / 1024 ** 2
print(f""Final memory usage: {new_memory_usage:.2f} MB"")

reference: Intrusion Detection (CIC-IDS2017)
I have tried to print the c_min and np.finfo(np.float32).min to find some roles.
Why exist numbers (np.finfo(np.float32).min) smaller than the minimum value (c_min) I have in the dataframe column?
Format:
column names ; c_min ; np.finfo(np.float32).min
Initial memory usage: 798.63 MB
Fwd Packet Length Mean 0.0 -3.4028235e+38
Fwd Packet Length Std 0.0 -3.4028235e+38
Bwd Packet Length Mean 0.0 -3.4028235e+38
Bwd Packet Length Std 0.0 -3.4028235e+38
Flow Bytes/s -261000000.0 -3.4028235e+38
Flow Packets/s -2000000.0 -3.4028235e+38
Flow IAT Mean -13.0 -3.4028235e+38
Flow IAT Std 0.0 -3.4028235e+38
Fwd IAT Mean 0.0 -3.4028235e+38
Fwd IAT Std 0.0 -3.4028235e+38
Bwd IAT Mean 0.0 -3.4028235e+38
Bwd IAT Std 0.0 -3.4028235e+38
Fwd Packets/s 0.0 -3.4028235e+38
Bwd Packets/s 0.0 -3.4028235e+38
Packet Length Mean 0.0 -3.4028235e+38
Packet Length Std 0.0 -3.4028235e+38
Packet Length Variance 0.0 -3.4028235e+38
Average Packet Size 0.0 -3.4028235e+38
Avg Fwd Segment Size 0.0 -3.4028235e+38
Avg Bwd Segment Size 0.0 -3.4028235e+38
Active Mean 0.0 -3.4028235e+38
Active Std 0.0 -3.4028235e+38
Idle Mean 0.0 -3.4028235e+38
Idle Std 0.0 -3.4028235e+38
Final memory usage: 798.63 MB

And search what is np.finfo()
Official introduction: Machine limits for floating point types
","A small toy example:
import numpy as np
import pandas as pd

# A value we need float64 for
max_32 = np.finfo(np.float32).max
needs_64 = max_32 + 1000

# Downcasting would be problematic
print(needs_64.astype(np.float32))
# Output:
# <stdin>:1: RuntimeWarning: overflow encountered in cast
# inf

If you have a dataframe with such a large values you cannot cast everything to float32. The script you posed makes sure that you cast only the columns that do not cause inf values.
# Dataframe with two float64 columns
df = pd.DataFrame([max_32, needs_64]).T
print(df.memory_usage().sum()) # 148

# Cast all columns to float32; you loose some values
df_float32 = df.astype(np.float32)
print(df_float32)
#              0    1
#0  3.402823e+38  inf

Note that all values in a column share the same data type, which you can check with:
df.dtypes
0    float64
1    float64

So you should only cast to float32 if your column-wise max/min values c_min/c_max are within the float32 min and max range
df_compressed = df.copy()
# cast only columns that are safe with your code
# ..., i.e. update:
df[0] = df[0].astype(np.float32)

print(df_compressed.memory_usage().sum())
# 144

print(df_compressed.dtypes)
# 0    float32
# 1    float64

# No inf value present
print(df_compressed)
#              0             1
#0  3.402823e+38  3.402823e+38

",numpy
how to create a numpy array of lists,"I want to create a numpy array in which each element must be a list, so later I can append new elements to each.
I have looked on google and here on stack overflow already, yet it seems nowhere to be found.
Main issue is that numpy assumes your list must become an array, but that is not what I am looking for.
","As you discovered, np.array tries to create a 2d array when given something like
 A = np.array([[1,2],[3,4]],dtype=object)

You have apply some tricks to get around this default behavior.
One is to make the sublists variable in length.  It can't make a 2d array from these, so it resorts to the object array:
In [43]: A=np.array([[1,2],[],[1,2,3,4]])
In [44]: A
Out[44]: array([[1, 2], [], [1, 2, 3, 4]], dtype=object)

And you can then append values to each of those lists:
In [45]: for i in A: i.append(34)
In [46]: A
Out[46]: array([[1, 2, 34], [34], [1, 2, 3, 4, 34]], dtype=object)

np.empty also creates an object array:
In [47]: A=np.empty((3,),dtype=object)
In [48]: A
Out[48]: array([None, None, None], dtype=object)

But you then have to be careful how you change the elements to lists.  np.fill is tempting, but has problems:
In [49]: A.fill([])
In [50]: A
Out[50]: array([[], [], []], dtype=object)
In [51]: for i in A: i.append(34)
In [52]: A
Out[52]: array([[34, 34, 34], [34, 34, 34], [34, 34, 34]], dtype=object)

It turns out that fill puts the same list in all slots, so modifying one modifies all the others.  You can get the same problem with a list of lists:
In [53]: B=[[]]*3
In [54]: B
Out[54]: [[], [], []]
In [55]: for i in B: i.append(34)
In [56]: B
Out[56]: [[34, 34, 34], [34, 34, 34], [34, 34, 34]]

The proper way to initial the empty A is with an iteration, e.g.
In [65]: A=np.empty((3,),dtype=object)
In [66]: for i,v in enumerate(A): A[i]=[v,i]
In [67]: A
Out[67]: array([[None, 0], [None, 1], [None, 2]], dtype=object)
In [68]: for v in A: v.append(34)
In [69]: A
Out[69]: array([[None, 0, 34], [None, 1, 34], [None, 2, 34]], dtype=object)

It's a little unclear from the question and comments whether you want to append to the lists, or append lists to the array.  I've just demonstrated appending to the lists.  
There is an np.append function, which new users often misuse.  It isn't a substitute for list append.  It is a front end to np.concatenate.  It is not an in-place operation; it returns a new array.
Also defining a list to add with it can be tricky:
In [72]: np.append(A,[[1,23]])
Out[72]: array([[None, 0, 34], [None, 1, 34], [None, 2, 34], 1, 23],     dtype=object)

You need to construct another object array to concatenate to the original, e.g.
In [76]: np.append(A,np.empty((1,),dtype=object))
Out[76]: array([[None, 0, 34], [None, 1, 34], [None, 2, 34], None], dtype=object)

In all of this, an array of lists is harder to construct than a list of lists, and no easier, or faster, to manipulate.  You have to make it a 2d array of lists to derive some benefit.
In [78]: A[:,None]
Out[78]: 
array([[[None, 0, 34]],
       [[None, 1, 34]],
       [[None, 2, 34]]], dtype=object)

You can reshape, transpose, etc an object array, where as creating and manipulating a list of lists of lists gets more complicated.
In [79]: A[:,None].tolist()
Out[79]: [[[None, 0, 34]], [[None, 1, 34]], [[None, 2, 34]]]

===
As shown in https://stackoverflow.com/a/57364472/901925, np.frompyfunc is a good tool for creating an array of objects.
np.frompyfunc(list, 0, 1)(np.empty((3,2), dtype=object))  

",numpy
can i create a multidimensional array containing a unit matrix without nested loops,"Suppose I have a Numpy array n indices, where the first n-2 represents some counting indices and the last 2 indices represent a square MxM matrix. I want to initialize this structure so it will contain copies of the unit matrix.
Example (here N=3, M=2):
A = numpy.zeros((3,2,2))
for k in range(3):
     A[k,:,:] = numpy.eye(2)

Another Example (here N=4, M=5):
B = numpy.zeros((3,4,5,5))
for k1 in range(3):
     for k2 in range(4):
         B[k1,k2,:,:] = numpy.eye(5)

Is there a way to do this without relying on nested loops?
","You can repeat:
A = np.repeat(np.eye(2)[None], 3, axis=0)

For more complex cases, combined with reshape:
extra = (3, 4)
M = 5
B = np.repeat(np.eye(M)[None], np.prod(extra), axis=0).reshape(extra+(M, M))

Or with tile:
extra = (3, 4)
B = np.tile(np.eye(5), extra+(1, 1))

Or, from numpy.zeros using indexing:
B = np.zeros((3, 4, 5, 5))
x = np.arange(B.shape[-1])
B[..., x, x] = 1

",numpy
how to understand the matmul function when matrix a is twodimensional and matrix b is threedimensional,"a=np.arange(8).reshape(2,2,2)
b=np.arange(4).reshape(2,2)
print(np.matmul(a,b))

the Result is:
[[[ 2  3]
  [ 6 11]]

 [[10 19]
  [14 27]]]

I don't understand this result, can someone please explain it?
","Short answer: it ""broadcasts"" the second 2d matrix to a 3d matrix, and then performs a ""mapping"" so, it maps the elementwise submatrices to new submatrices in the result.
As the documentation on np.matmul [numpy-doc] says:

numpy.matmul(a, b, out=None)
Matrix product of two arrays.
The behavior depends on the arguments in the following way.

If both arguments are 2-D they are multiplied like conventional matrices.
If either argument is N-D, N > 2, it is treated as a stack of matrices residing in the last two indexes and broadcast
  accordingly.
If the first argument is 1-D, it is promoted to a matrix by prepending a 1 to its dimensions. After matrix multiplication the
  prepended 1 is removed.
If the second argument is 1-D, it is promoted to a matrix by appending a 1 to its dimensions. After matrix multiplication the
  appended 1 is removed.


So here the second item is applicable. So first the second matrix is ""broadcasted"" to the 3d variant as well, so that means that we multiple:
array([[[0, 1],
        [2, 3]],

       [[4, 5],
        [6, 7]]])

with:
array([[[0, 1],
        [2, 3]],

       [[0, 1],
        [2, 3]]])

and we see these as stacked matrices. So first we multiply:
array([[0, 1],      array([[0, 1],
       [2, 3]])  x        [2, 3]])

which gives us:
array([[ 2,  3],
       [ 6, 11]])

and then the elementwise second submatrices:
array([[4, 5],      array([[0, 1],
       [6, 7]])  x        [2, 3]])

an this gives us:
array([[10, 19],
       [14, 27]])

we thus stack these together into the result, and obtain:
>>> np.matmul(a, b)
array([[[ 2,  3],
        [ 6, 11]],

       [[10, 19],
        [14, 27]]])

Although the behavior is thus perfectly defined, it might be better to use this feature carefully, since there are other ""sensical"" definitions of what a ""matrix product"" on 3d matrices with 2d matrices might look like, and these are thus not used here.
",numpy
how to convert a pytorch tensor into a numpy array,"How do I convert a torch tensor to numpy?
","copied from pytorch doc:
a = torch.ones(5)
print(a)


tensor([1., 1., 1., 1., 1.])

b = a.numpy()
print(b)


[1. 1. 1. 1. 1.]


Following from the below discussion with @John:
In case the tensor is (or can be) on GPU, or in case it (or it can) require grad, one can use
t.detach().cpu().numpy()

I recommend to uglify your code only as much as required.
",numpy
how to convert the column with lists into one hot encoded columns,"Assume, there is one DataFrame such as following
import pandas as pd 
import numpy as np 

df = pd.DataFrame({'id':range(1,4), 
                   'items':[['A', 'B'], ['A', 'B', 'C'], ['A', 'C']]})
df
        id  items
        1   [A, B]
        2   [A, B, C]
        3   [A, C]

Is there an efficient way to convert above DataFrame into the following (one-hot encoded columns)? Many Thanks in advance!
   id   items       A   B   C
    1   [A, B]      1   1   0
    2   [A, B, C]   1   1   1
    3   [A, C]      1   0   1

","SOLUTION 1
A possible solution, whose steps are:

First, the explode function is used to transform each item of a list-like to a row, replicating the index values.

Then, the to_numpy method converts the resulting dataframe to a numpy array, and .T transposes this array.

The crosstab function computes a simple cross-tabulation of factors, which, in this case, are the transposed columns of the exploded dataframe.

The reset_index method is used to reset the index of the dataframe, turning the index into a column named id.

Finally, the original dataframe df is merged with this transformed dataframe using the merge function.


df.merge(
    pd.crosstab(*df.explode('items').to_numpy().T)
    .reset_index(names='id'))


SOLUTION 2
Another possible solution, whose steps are:

First, the explode function is used to transform each item of a list-like to a row, replicating the index values.

Then, the pivot_table function is applied to reshape the data based on the unique values in the items column, aggregating the count of each id for every item. The fill_value=0 ensures that any missing combinations are filled with zeros.

The rename_axis method is used to remove the axis name for the columns.

Finally, reset_index is called to reset the index of the dataframe, turning the index into a column.

The original dataframe df is then merged with this transformed dataframe using the merge function.


df.merge(
    df.explode('items')
    .pivot_table(index='id', columns='items', values='id', aggfunc=len, 
                 fill_value=0)
    .rename_axis(None, axis=1).reset_index())


Output:
   id      items  A  B  C
0   1     [A, B]  1  1  0
1   2  [A, B, C]  1  1  1
2   3     [A, C]  1  0  1

",numpy
manipulation of a pandas dataframe most time and memoryefficiently,"Please imagine I have a dataframe like this:
df = pd.DataFrame(index=pd.Index(['1', '1', '2', '2'], name='from'), columns=['to'], data= ['2', '2', '4', '5'])

df:

Now, I would like to calculate a matrix comprising of the percentage of times each value in the index ""from"" transitions to each value in column 'to', which is known as a transition matrix. I can achieve this by creating an empty transition matrix first and then populating it with the percentages using a for loop:
#Create an empty matrix to populate later (using sparse dtype to save memory):
matrix = pd.DataFrame(index=df.index.unique(), columns=df.to.unique(), data=0, dtype=pd.SparseDtype(dtype=np.float16, fill_value=0)) 

matrix:

for i in range(len(df)):
    from_, to = df.index[i], df.to.iloc[i]     
    matrix[to] = matrix[to].sparse.to_dense() # Convert to dense format because sparse dtype does not allow value assignment with .loc in the next line:  
    matrix.loc[from_, to] += 1     # Do a normal insertion with .loc[]
    matrix[to] = matrix[to].astype(pd.SparseDtype(dtype=np.float16, fill_value=0)) # Back to the original sparse format

matrix = (matrix.div(matrix.sum(axis=1), axis=0)*100) # converting counts to percentages

matrix:

This works. For example, index ""1"" only transitioned to ""2"" (100% of the time) and index ""2"" transitioned to ""4"" 50% of the time and to ""5"" the other 50% of the time, as can be verified in df.
Issue: The actual matrix is about 500K by 500K and the for loop takes a really long time to finish. So, is there a vectorized or other efficient way of calculating matrix from df
Note: I would get MemoryError without using the whole Sparse dtype thing even with dtype=float16 in pd.DataFrame() so I prefer to keep that if possible. It would be great if the 500K by 500K matrix will not take up more than 10-12Gb of RAM. Also, if it matters, these percentages will always have a 0-100 range, obviously.
","Option 1: pd.crosstab

Use pd.crosstab with normalize='index'.
Add df.mul and df.rename_axis for formatting.

out = (pd.crosstab(index=df.index, 
                   columns=df['to'], 
                   normalize='index'
                   )
       .mul(100)
       .rename_axis(index='from', columns=None)
       )

Output:
          2     4     5
from                   
1     100.0   0.0   0.0
2       0.0  50.0  50.0

Option 2: df.pivot_table

Use df.reset_index to use df.pivot_table with aggfunc='size' and fill_value=0.
Chain df.div to divide by df.value_counts along axis=0.
Again, add df.mul and df.rename_axis.

out2 = (df.reset_index()
        .pivot_table(index='from', columns='to', 
                     values='to', aggfunc='size', fill_value=0)
        .div(df.index.value_counts(), axis=0)
        .mul(100)
        .rename_axis(columns=None)
        )

out2.equals(out)
# True

Option 3: df.groupby

Use df.groupby with df.index and get groupby.value_counts with normalize=True.
Apply df.unstack with fill_value=0 to move level='to' (i.e. -1) to columns.
Again, add df.mul + df.rename_axis.

out3 = (df.groupby(df.index)
        .value_counts(normalize=True)
        .unstack(-1, fill_value=0)
        .mul(100)
        .rename_axis(columns=None)
        )

out3.equals(out)
# True


Edit:
With a very sizeable df, I would consider creating a csr_matrix to avoid the MemoryError. In this case, we only use df.groupby + value_counts and use Index.factorize to pass compatible row_ind and col_ind values.
from scipy.sparse import csr_matrix

g = df.groupby(df.index).value_counts(normalize=True).mul(100)

idx_codes, idx_unique = g.index.get_level_values(0).factorize()
col_codes, col_unique = g.index.get_level_values(1).factorize() 

m = csr_matrix((g.values, (idx_codes, col_codes)))

If you want, you can turn that back into a df with df.sparse.from_spmatrix:
df_m = pd.DataFrame.sparse.from_spmatrix(m, 
                                         index=idx_unique, 
                                         columns=col_unique)

Output:
       2     4     5
1  100.0   0.0   0.0
2    0.0  50.0  50.0

But understanding this, you can also continue with m and still use idx_unique and col_unique for slicing. E.g.:
bool_idx = col_unique.isin(['4','5'])

m[:, bool_idx].sum(axis=1)

matrix([[  0.],
        [100.]])

Compare:
df_m.loc[:, bool_idx].sum(axis=1)

1        0
2    100.0
dtype: Sparse[float64, 0]

",numpy
numpy matrix tiling and multiplication combination,"I'm looking for a function capable of taking a m x n array, which repeats each row n times over a identity-like grid of m size.
For demo:
input = [[a1, b1, c1],
         [a2, b2, c2]]

output = [[a1, b1, c1,  0,  0,  0],
          [a1, b1, c1,  0,  0,  0],
          [a1, b1, c1,  0,  0,  0],
          [ 0,  0,  0, a2, b2, c2],
          [ 0,  0,  0, a2, b2, c2],
          [ 0,  0,  0, a2, b2, c2]]

Last time I asked something similar I was told of the Kronecker product, is there some similar function?
","While looking similar, it appears to me that the given problem cannot be solved using a Kronecker product: with the latter, you could only manage to get repetitions of your complete input matrix as blocks of the result matrix.
I stand corrected: For a solution that employs the Kronecker product, see @ThomasIsCoding's answer.
In any case, what you need is the individual rows, repeated, as blocks of the result matrix. So that is what the code below does: it constructs a block matrix from the rows, using scipy.linalg.block_diag(), then repeats them as required, using numpy.repeat(). Note that block_diag() expects individual blocks as individual arguments, which is why a is unpacked with the * prefix.
import numpy as np
import scipy

a = np.asarray([[11, 21, 31], [21, 22, 32], [31, 32, 33]])
print(a)
# [[11 21 31]
#  [21 22 32]
#  [31 32 33]]

blocked = scipy.linalg.block_diag(*a)
print(blocked)
# [[11 21 31  0  0  0  0  0  0]
#  [ 0  0  0 21 22 32  0  0  0]
#  [ 0  0  0  0  0  0 31 32 33]]

result = np.repeat(blocked, a.shape[1], axis=0)
print(result)
# [[11 21 31  0  0  0  0  0  0]
#  [11 21 31  0  0  0  0  0  0]
#  [11 21 31  0  0  0  0  0  0]
#  [ 0  0  0 21 22 32  0  0  0]
#  [ 0  0  0 21 22 32  0  0  0]
#  [ 0  0  0 21 22 32  0  0  0]
#  [ 0  0  0  0  0  0 31 32 33]
#  [ 0  0  0  0  0  0 31 32 33]
#  [ 0  0  0  0  0  0 31 32 33]]

",numpy
removing numpy meshgrid points outside of a shapely polygon,"I have a 10 x 10 grid that I would like to remove points outside of a shapely Polygon:
import numpy as np
from shapely.geometry import Polygon, Point
from descartes import PolygonPatch

gridX, gridY = np.mgrid[0.0:10.0, 0.0:10.0]
poly = Polygon([[1,1],[1,7],[7,7],[7,1]])

#plot original figure
fig = plt.figure()
ax = fig.add_subplot(111)
polyp = PolygonPatch(poly)
ax.add_patch(polyp)
ax.scatter(gridX,gridY)
plt.show()

Here is the resulting figure:

And what I want the end result to look like:

I know that I can reshape the array to a 100 x 2 array of grid points:
stacked = np.dstack([gridX,gridY])
reshaped = stacked.reshape(100,2)

I can see if the point lies within the polygon easily:
for i in reshaped:
    if Point(i).within(poly):
         print True

But I am having trouble taking this information and modifying the original grid
","You're pretty close already; instead of printing True, you could just append the points to a list.
output = []
for i in reshaped:
    if Point(i).within(poly):
        output.append(i)

output = np.array(output)
x, y = output[:, 0], output[:, 1]

It seems that Point.within doesn't consider points that lie on the edge of the polygon to be ""within"" it though.
",numpy
is this python code a kind of typecast and if so what is going on under the hood,"I'm editing some Python code containing the line:
arr = (c_double * len(self.static_mapped[0, :]))(*self.static_mapped[0, :])

where field self.static_mapped is created using numpy.zeros and where c_double is imported from ctypes.
Based on the context, I'm assuming this is typecasting from numpy doubles to C-type doubles, but this syntax doesn't appear in any of the Python guides I've consulted, and even my IDE is complaining that ""'int' object is not callable"".
Am I correct in my interpretation of its meaning? What sequence of function calls is going on under the hood? Is this a well-known code idiom in Python or is there a more standard/straightforward way of accomplishing this?
","Strictly from a syntactic perspective, your code should be equivalent to
t = c_double * len(self.static_mapped[0, :])
arr = t(*self.static_mapped[0, :])

This implies that c_double.__mul__ is overridden to produce some other type object that gets called to produce an array. No type-casting, just dynamic type generation.
Specifically, multiplying type by an integer n creates an array of n values:
>>> c_double * 2
<class '__main__.c_double_Array_2'>
>>> c_double * 3
<class '__main__.c_double_Array_3'>
>>> c_double * 7
<class '__main__.c_double_Array_7'>

Note that ""type multiplication"" is not associative:
# An array of arrays
>>> c_double * 7 * 2
<class '__main__.c_double_Array_7_Array_2'>

# A single array
>>> c_double * (7 * 2)
<class '__main__.c_double_Array_14'>

",numpy
numpy how to get a max from an argmax result,"I have a numpy array of arbitrary shape, e.g.:
a = array([[[ 1,  2],
            [ 3,  4],
            [ 8,  6]],

          [[ 7,  8],
           [ 9,  8],
           [ 3, 12]]])
a.shape = (2, 3, 2)

and a result of argmax over the last axis:
np.argmax(a, axis=-1) = array([[1, 1, 0],
                               [1, 0, 1]])

I'd like to get max:
np.max(a, axis=-1) = array([[ 2,  4,  8],
                            [ 8,  9, 12]])

But without recalculating everything. I've tried:
a[np.arange(len(a)), np.argmax(a, axis=-1)]

But got:
IndexError: shape mismatch: indexing arrays could not be broadcast together with shapes (2,) (2,3) 

How to do it? Similar question for 2-d: numpy 2d array max/argmax
","You can use advanced indexing -
In [17]: a
Out[17]: 
array([[[ 1,  2],
        [ 3,  4],
        [ 8,  6]],

       [[ 7,  8],
        [ 9,  8],
        [ 3, 12]]])

In [18]: idx = a.argmax(axis=-1)

In [19]: m,n = a.shape[:2]

In [20]: a[np.arange(m)[:,None],np.arange(n),idx]
Out[20]: 
array([[ 2,  4,  8],
       [ 8,  9, 12]])

For a generic ndarray case of any number of dimensions, as stated in the comments by @hpaulj, we could use np.ix_, like so -
shp = np.array(a.shape)
dim_idx = list(np.ix_(*[np.arange(i) for i in shp[:-1]]))
dim_idx.append(idx)
out = a[dim_idx]

",numpy
numpydtype size changed may indicate binary incompatibility expected 96 from c header got 88 from pyobject,"I want to call my Python module from the Matlab. I received the error:
Error using numpy_ops>init thinc.backends.numpy_ops

Python Error:
 ValueError: numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject.

The Python script is as follows
import spacy
def text_recognizer(model_path, text):
try:
    # Load the trained model
    nlp = spacy.load(model_path)
    print(""Model loaded successfully."")
    
    # Process the given text
    doc = nlp(text)
    ent_labels = [(ent.text, ent.label_) for ent in doc.ents]
        return ent_labels

The Matlab script is as follows
% Set up the Python environment
pe = pyenv;
py.importlib.import_module('final_output');

% Add the directory containing the Python script to the Python path
path_add = fileparts(which('final_output.py'));
if count(py.sys.path, path_add) == 0
    insert(py.sys.path, int64(0), path_add);
end
% Define model path and text to process
model_path = 'D:\trained_model\\output\\model-best';
text = 'Roses are red';
% Call the Python function
pyOut = py.final_output.text_recognizer(model_path, text);
% Convert the output to a MATLAB cell array
entity_labels = cell(pyOut);
disp(entity_labels);

I found one solution to update Numpy, what I did, but nothing changed. I am using Python 3.9 and Numpy version 2.0.0
The error was received when I tried to call the Python module using a Matlab script.
How can I fix the issue?
","The reason is that pandas defines its numpy dependency freely as ""anything newer than certain version of numpy"".
The problem occured, when numpy==2.0.0 has been released on June 16th 2024, because it is no longer compatible with your pandas version.
The solution is to pin down the numpy version to any before the 2.0.0. Today it could be (this is the most recent numpy 1 release):
numpy==1.26.4

To be added in your requirements or to the pip command you use (but together with installing pandas).
Nowadays pip is very flexible and can handle the issue flawesly. You just need to ask it to install both pandas and numpy of given versions in the same pip install invocation.
",numpy
slice a numpy 2d array using another 2d array,"I have a 2D array of (4,5) and another 2D array of (4,2) shape. The second array contains the start and end indices that I need to filter out from first array i.e., I want to slice the first  array using second array.
np.random.seed(0)
a = np.random.randint(0,999,(4,5))
a
array([[684, 559, 629, 192, 835],
       [763, 707, 359,   9, 723],
       [277, 754, 804, 599,  70],
       [472, 600, 396, 314, 705]])

idx = np.array([[2,4],
                [0,3],
                [2,3],
                [1,3]
               ])

Expected output - can be either of following two formats. Only reason for padding with zeros is that variable length 2d arrays are not supported.
[[629, 192, 835, 0, 0],
 [763, 707, 359, 9, 0],
 [804, 599, 0, 0, 0],
 [600, 396, 314, 0, 0]
]

[[0, 0, 629, 192, 835],
 [763, 707, 359, 9, 0],
 [0, 0, 804, 599, 0],
 [0, 600, 396, 314, 0]
]

","Another possible solution, which uses:

np.arange to create a range of column indices based on the number of columns in a.

A boolean mask m is created using logical operations to check if each column index falls within the range specified by idx. The np.newaxis is used to align dimensions for broadcasting.

np.where is used to create a_mask, where elements in a are replaced with 0 if the corresponding value in m is False.

np.argsort is used to get the indices that would sort each row of m (negated) in ascending order.

np.take_along_axis is used to rearrange the elements of a_mask based on the sorted indices.


cols = np.arange(a.shape[1])
m = (cols >= idx[:, 0, np.newaxis]) & (cols <= idx[:, 1, np.newaxis])

a_mask = np.where(m, a, 0)
sort_idx = np.argsort(~m, axis=1)
np.take_along_axis(a_mask, sort_idx, axis=1)

NB: Notice that a_mask contains the unsorted version of the solution (that is essentially the approach followed by @mozway).

Output:
array([[629, 192, 835,   0,   0],
       [763, 707, 359,   9,   0],
       [804, 599,   0,   0,   0],
       [600, 396, 314,   0,   0]])

# a_mask
array([[  0,   0, 629, 192, 835],
       [763, 707, 359,   9,   0],
       [  0,   0, 804, 599,   0],
       [  0, 600, 396, 314,   0]])

",numpy
demystify numpy indexingslicing,"could you please help demystify the following numpy indexing/slicing behaviours? Thanks!
arr = np.arange(60).reshape(3,4,5)

print(arr[2, :, 4])     #1

print(arr[[2], :, 4])   #2
print(arr[2, :, [4]])   #3
print(arr[[2], :, [4]]) #4

[44 49 54 59]
[[44 49 54 59]]
[[44 49 54 59]]
[[44 49 54 59]]

#1 is comprehensible whereas #2,#3,#4 are really confusing for me when it comes to the shape of results ((1,4) arrays). More specifically, when would inner [] impact dimensions of resulting array?
A trickier example:
arr = np.arange(120).reshape(4,6,5)
arr[[1,3], :3, [4,2]]

array([[ 34,  39,  44],
       [ 92,  97, 102]])

","In [118]: arr = np.arange(60).reshape(3,4,5)

Your first example is the straightforward basic indexing, with a 2 scalar indices and slice.  The result is a view, and the shape is that of the 2nd dimension, (4,):
In [119]: arr[2, :, 4]
Out[119]: array([44, 49, 54, 59])

Same thing, but a copy, when using an array/list instead of the slice:
In [120]: arr[2, [0,1,2,3], 4]
Out[120]: array([44, 49, 54, 59])

If I provide size 1 lists (arrays) for all indices, the result is (1,):
In [121]: arr[[2], [0], [4]]
Out[121]: array([44])

Same if one or more is a scalar:
In [122]: arr[[2], [0], 4]
Out[122]: array([44])

With size 1 lists instead of the scalars, the same (4,) shape - because (1,) broadcasts with (4,):
In [123]: arr[[2], [0,1,2,3], [4]]
Out[123]: array([44, 49, 54, 59])

But if there is slice in the middle, the shape is (1,4):
In [124]: arr[[2], :, [4]]
Out[124]: array([[44, 49, 54, 59]])

same if one those is a scalar:
In [125]: arr[2, :, [4]]
Out[125]: array([[44, 49, 54, 59]])

If I move the [2] out, I get a (4,1) array:
In [126]: arr[2][ :, [4]]
Out[126]: 
array([[44],
       [49],
       [54],
       [59]])

The 2 selects the first plane, the 4 comes from the slice, and 1 from the last dimesion.
Generalizing [125] so the last dimension is 2, the result is (2,4).
In [127]: arr[2, :, [1,4]]
Out[127]: 
array([[41, 46, 51, 56],
       [44, 49, 54, 59]])

Both this and [125] are examples where the slice is in the middle, and its dimension is tacked on the end, after the dimensions produced by advanced indexing.
As I commented this has come up periodically for many years.
Without a 'slice in the middle', we get the expected shape - (3,4) from the slices, (1,) from the advanced index:
In [130]: arr[:, :, [4]].shape
Out[130]: (3, 4, 1)

This is a copy of arr, but it is actually a view, a transpose, of a (1,3,4) base:
In [131]: arr[:, :, [4]].base.shape
Out[131]: (1, 3, 4)

As in the 'slice in the middle' cases, the advanced indexing dimension is first, and the slices are 'tacked' on.  But in this case it can transpose it to the desired shape.  It's an implementation detail that usually is ignored.
",numpy
python how to get values of a multidimensional array at certain indexed positions,"If I have a multi-dimensional numpy array like:
a = np.array([[0, 1, 2], 
              [3, 4, 5], 
              [6, 7, 8]])

How can I get the values at certain index positions in one step? For example, if were to define pairs of indices like:
indices = [[0,0], [1,1], [2,2]]

I would like:
a[indices] = [0, 4, 8]

Note that this does work for one-dimensional arrays (Python: How to get values of an array at certain index positions?), but I cannot see how to get this to work in more than one dimension. I am using Python 3.7.
","As in the one-dimensional answer you linked, you can do this elegantly in 2 dimensions with numpy:
a = np.array([[0, 1, 2], [3, 4, 5], [6, 7, 8]])
rows, columns = zip([0, 0], [1, 1], [2, 2])
print(a[rows, columns])

The output of the print will be:
array([0, 4, 8])

",numpy
generalizing a gaussian mix to take any number of arguments with numpyvectorize causes performance issues,"I am optimizing a gaussian mix using maximum likelyhood estimation. Originally I used the following model:
def normal(x, mu, sigma):
    """"""
    Gaussian (normal) probability density function.
    
    Args:
        x (np.ndarray): Data points.
        mu (float): Mean of the distribution.
        sigma (float): Standard deviation of the distribution.
    
    Returns:
        np.ndarray: Probability density values.
    """"""
    return (1 / (np.sqrt(2 * np.pi) * sigma)) * np.exp(-0.5 * ((x - mu) / sigma) ** 2)

def model(x, a, mu1, s1, mu2, s2):
    return a*normal(x, mu1, s1) + (1-a)*normal(x, mu2, s2)

This works great and finds a good fit in under a second.
I now wanted to dynamically generate such a function for any number of peaks.
def generate_gaussian_mix(n):
    def gaussian_mix(x, *params):
    
        if len(params) != 3 * n - 1:
            print(params)
            raise ValueError(f""Expected {3 * n - 1} parameters, but got {len(params)}."")

        params = np.asarray(params)
        mu = params[0::3]  # Means
        sigma = params[1::3]  # Standard deviations
        a = params[2::3]  # Weights
        a = np.hstack((a, 1 - np.sum(a)))

        return np.sum((a / (np.sqrt(2 * np.pi) * sigma))*np.exp(-0.5 * ((x - mu) / sigma) ** 2))

    return np.vectorize(gaussian_mix)

This model takes over three minutes to compute on my laptop with the same number of peaks and events. What are optimization steps I could take to reduce the magnitude of this second function? Is there a good way to avoid vectorization? Do you have any ideas to avoid the repeated slicing?
for completeness, this is the optimization function:
def neg_log_event_likelyhood(model, event, theta):
    x = -np.log(model(event, *theta))
    return x

def fit_distribution_anneal(model, events, bounds, data_range = None, **kwargs):
    def total_log_likelyhood(theta, model, events):
        return np.sum(neg_log_event_likelyhood(model, events, theta))

    if data_range is not None:
        events = np.copy(events)
        events = events[np.logical_and(events > data_range[0], events < data_range[1])]
    
    result = dual_annealing(total_log_likelyhood, bounds, args=(model, events), **kwargs)
    params = result.x

    return params

The annealing is required as opposed to minimize due to the non convex nature of the problem.
","As suspected the primary problem was np.vectorize. By using np.transpose I can abuse the matrix multiplication to calculate the normal distribution element wise and sum appropriate axis of the array. The following is the optimized code:
def generate_gaussian_mix(n):

    """"""
    Dynamically generates a function for the superposition of `n` Gaussian functions.
    
    Args:
        n (int): Number of Gaussian functions to include in the superposition.
    
    Returns:
        function: A callable function `f(x, params)` where `params` is a flat array of weights, means, 
                  and standard deviations for each Gaussian component, of size 3*n.
    """"""

    def gaussian_mix(x, *params):
        
        if len(params) != 3 * n - 1:
            print(params)
            raise ValueError(f""Expected {3 * n - 1} parameters, but got {len(params)}."")

        params = np.asarray(params)
        mu = params[0::3]  # Means
        sigma = params[1::3]  # Standard deviations
        a = params[2::3]  # Weights

        return  np.sum(normal(np.transpose([x]), mu[:-1], sigma[:-1], a), axis = 1) + normal(np.transpose([x]), mu[-1], sigma[-1], 1-np.sum(a))[:,0]

    return gaussian_mix

",numpy
how to mask inputs with variable size in transformer model when the batches needs to be masked differently,"I'm making a transformer using tensorflow.keras and having issues understanding how the attention_mask works for a MultiHeadAttention layer.
My input is 3-dimensional data. For example, let's assume my whole dataset has 10 elements, each one with length no more than 4:
# whole data
[
  # first item
  [
    [     1,      2,      3],
    [     1,      2,      3],
    [np.nan, np.nan, np.nan],
    [np.nan, np.nan, np.nan],
  ],
  # second item
  [
    [     1,      2,      3],
    [     5,      8,      2],
    [     3,      7,      8],
    [     4,      6,      2],
  ],
  ... # 8 more items
]

So, my mask looks like:
# assume this is a numpy array
mask = [
  [
    [1, 1, 1],
    [1, 1, 1],
    [0, 0, 0],
    [0, 0, 0],
  ],
  [
    [1, 1, 1],
    [1, 1, 1],
    [1, 1, 1],
    [1, 1, 1],
  ],
  ...
]

So the shape of the mask til now is [10, 4, 3]. Let's say I use batch_size = 5. Now, according documentation, attention_mask shape should be [B, T, S] (batch_size, query_size, key_size). In the example case should be [5, 4, 4]?
Question
If the mask is calculated only once, what 5 items should I give as a mask? This sounds counterintuitive to me. How should I build the mask?
According this answer, head_size should be also taken in account, so they also do:
mask = mask[:, tf.newaxis, tf.newaxis, :]

What I've tested
The only time I manage to run the transformer successfully using the attention_mask is when I do:
mask = np.ones((batch_size, data.shape[1], data.shape[2]))
mask = mask[:, tf.newaxis, tf.newaxis, :]

Obviously that mask makes no sense, because it is all ones, but it was just to test if it had the correct shape.
The model
I'm using practically the same code from the keras example transformer for time series classification
def transformer_encoder(inputs, head_size, num_heads, ff_dim, dropout=0.0, mask=None):
    # Normalization and Attention
    x = layers.LayerNormalization(epsilon=1e-6)(inputs)
    x = layers.MultiHeadAttention(
        key_dim=head_size, num_heads=num_heads, dropout=dropout
    )(x, x, attention_mask=mask)
    x = layers.Dropout(dropout)(x)
    res = x + inputs

    # Feed Forward Part
    x = layers.LayerNormalization(epsilon=1e-6)(res)
    x = layers.Conv1D(filters=ff_dim, kernel_size=1, activation=""relu"")(x)
    x = layers.Dropout(dropout)(x)
    x = layers.Conv1D(filters=inputs.shape[-1], kernel_size=1)(x)
    return x + res


def build_model(
    n_classes,
    input_shape,
    head_size,
    num_heads,
    ff_dim,
    num_transformer_blocks,
    mlp_units,
    dropout=0.0,
    mlp_dropout=0.0,
    input_mask=None,
) -> keras.Model:
    inputs = keras.Input(shape=input_shape)
    x = inputs
    for _ in range(num_transformer_blocks):
        x = transformer_encoder(x, head_size, num_heads, ff_dim, dropout, input_mask)

    x = layers.GlobalAveragePooling2D(data_format=""channels_first"")(x)
    for dim in mlp_units:
        x = layers.Dense(dim, activation=""relu"")(x)
        x = layers.Dropout(mlp_dropout)(x)
    outputs = layers.Dense(n_classes, activation=""softmax"")(x)
    return keras.Model(inputs, outputs)

","After a little research and seeing several transformer model examples this is what solved the problem for me.

Create a custom TransformerBlock layer that supports masking
Add a mask parameter in the call method of the TransformerBlock and reshape it there.
Add a Masking layer before the TransformerBlock

Code:
class TransformerBlock(layers.Layer):
    def __init__(self, head_size, num_heads, ff_dim, ff_dim2, rate=0.1):
        super().__init__()
        self.att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=head_size)
        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)
        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)
        self.dropout1 = layers.Dropout(rate)
        self.dropout2 = layers.Dropout(rate)
        self.conv1 = layers.Conv1D(filters=ff_dim, kernel_size=1, activation=""relu"")
        self.conv2 = layers.Conv1D(filters=ff_dim2, kernel_size=1)
        self.supports_masking = True

    def call(self, inputs, training, mask=None):
        padding_mask = None
        if mask is not None:
            padding_mask = tf.cast(mask[:, tf.newaxis, tf.newaxis, :], dtype=""int32"")

        out_norm1 = self.layernorm1(inputs, training=training)
        out_att = self.att(
            out_norm1, out_norm1, training=training, attention_mask=padding_mask
        )
        out_drop1 = self.dropout1(out_att, training=training)
        res = out_drop1 + inputs
        out_norm2 = self.layernorm2(res, training=training)
        out_conv1 = self.conv1(out_norm2, training=training)
        out_drop2 = self.dropout2(out_conv1, training=training)
        out_conv2 = self.conv2(out_drop2, training=training)
        return out_conv2 + res

def build_model(
    n_classes,
    input_shape,
    head_size,
    num_heads,
    ff_dim,
    num_transformer_blocks,
    mlp_units,
    dropout=0.0,
    mlp_dropout=0.0,
    mask=None,
) -> keras.Model:
    inputs = keras.Input(shape=input_shape)
    _x = inputs
    if mask is not None:
        _x = layers.Masking(mask_value=mask)(_x)
    for _ in range(num_transformer_blocks):
        _x = TransformerBlock(
            head_size,
            num_heads,
            ff_dim,
            inputs.shape[-1],
            dropout,
        )(_x)

    _x = layers.GlobalAveragePooling2D(data_format=""channels_first"")(_x)
    for dim in mlp_units:
        _x = layers.Dense(dim, activation=""relu"")(_x)
        _x = layers.Dropout(mlp_dropout)(_x)
    outputs = layers.Dense(n_classes, activation=""softmax"")(_x)
    return keras.Model(inputs, outputs)

",numpy
what39s wrong with my code to convert a matrix to and from a byte string,"I have this function to convert a binary 2d array to a byte array:
def flatten_and_pad_to_multiple_of_8(binary_matrix):
    # Step 1: Calculate the size of the original flattened array
    rows, cols = binary_matrix.shape
    current_length = rows * cols
    
    # Step 2: Calculate the required length that is a multiple of 8
    padded_length = ((current_length + 7) // 8) * 8
    
    # Step 3: Initialize flat_bits with the required padded length
    flat_bits = np.zeros(padded_length, dtype=np.uint8)
    
    # Step 4: Fill flat_bits with values from the binary matrix
    idx = 0
    for i in range(rows):
        for j in range(cols):
            flat_bits[idx] = binary_matrix[i, j]
            idx += 1
    
    return flat_bits

def matrix_to_ascii(matrix):
    flat_bits = flatten_and_pad_to_multiple_of_8(matrix)    
    # Convert the flattened bits into bytes
    ascii_string = """"
    for i in range(0, len(flat_bits), 8):
        byte = 0
        for j in range(8):
            byte = (byte << 1) | flat_bits[i + j]
        ascii_char = chr(byte)
        ascii_string += ascii_char
    return ascii_string

If
matrix = np.array([[0, 1, 1, 1, 1],
[1, 0, 1, 1, 1],
[1, 1, 0, 1, 1],
[1, 1, 1, 0, 1],
[1, 1, 1, 1, 0]], dtype=uint8)

then matrix_to_ascii(matrix) is '}÷ß\x00' although it is a string. I then have to do matrix_to_ascii(matrix).encode(). My problem is in converting it back to a matrix.
I will first convert the string to a byte array to save space. I need to save space in my code. Here is the broken code to convert it back to a matrix:
def ascii_to_matrix(byte_array, original_shape):
    """"""
    ascii_string must be a bytestring before it is passed in.
    """"""
    # Initialize the binary matrix with the original shape
    rows, cols = original_shape
    binary_matrix = np.zeros((rows, cols), dtype=np.uint8)
    
    # Fill the binary matrix with bits from the byte array
    bit_idx = 0
    for byte in byte_array:
        for j in range(8):
            if bit_idx < rows * cols:
                binary_matrix[bit_idx // cols, bit_idx % cols] = (byte >> (7 - j)) & 1
                bit_idx += 1
            else:
                break
    return binary_matrix

Unfortunately, it gives the wrong output:
ascii_to_matrix(matrix_to_ascii(matrix).encode(), (5, 5))

array([[0, 1, 1, 1, 1],
       [1, 0, 1, 1, 1],
       [0, 0, 0, 0, 1],
       [1, 1, 0, 1, 1],
       [0, 1, 1, 1, 1]], dtype=uint8)

What am I doing wrong?
(I am not using any fancier numpy functions as I will want to speed this all up with numba. In particular, I can't use packbits or tobytes as they are not supported by numba. I also can't use bytes or bytearray.)
","Your ascii_to_matrix seems to work fine if you encode your string to a 1-byte-per-character encoding; .encode() defaults to UTF-8, and some of the characters in your matrix's ""text"" representation will end up being two bytes in UTF-8.
print(x := matrix_to_ascii(matrix))
print(f""{x.encode()=!r}"")
print(f""{x.encode('iso-8859-1')=!r}"")
print(z := ascii_to_matrix(x.encode(""iso-8859-1""), matrix.shape))
assert np.array_equal(matrix, z)

prints out
}÷ß
x.encode()=b'}\xc3\xb7\xc3\x9f\x00'
x.encode('iso-8859-1')=b'}\xf7\xdf\x00'
[[0 1 1 1 1]
 [1 0 1 1 1]
 [1 1 0 1 1]
 [1 1 1 0 1]
 [1 1 1 1 0]]

– note how the default encoding is not the same as the single-byte encoding.
",numpy
shortest syntax to use numpy 1darray as sklearn x,"I often have two numpy 1d arrays, x and y, and would like to perform some quick sklearn fitting + prediction using them.
 import numpy as np
 from sklearn import linear_model

 # This is an example for the 1d aspect - it's obtained from something else.
 x = np.array([1, 3, 2, ...]) 
 y = np.array([12, 32, 4, ...])

Now I'd like to do something like
 linear_model.LinearRegression().fit(x, y)...

The problem is that it expects an X which is a 2d column array. For this reason, I usually feed it 
 x.reshape((len(x), 1))

which I find cumbersome and hard to read. 
Is there some shorter way to transform a 1d array to a 2d column array (or, alternatively, get sklearn to accept 1d arrays)?
","You can slice your array, creating a newaxis:
x[:, None]

This:
>>> x = np.arange(5)
>>> x[:, None]
array([[0],
       [1],
       [2],
       [3],
       [4]])

Is equivalent to:
>>> x.reshape(len(x), 1)
array([[0],
       [1],
       [2],
       [3],
       [4]])

If you find it more readable, you can use a transposed matrix:
np.matrix(x).T

If you want an array:
np.matrix(x).T.A

",numpy
summarize higher dimensions in numpy,"I have a numpy array that holds board game states for all possible moves, and I want to summarize some of those moves. I'm struggling to vectorize that code and avoid a for loop when I choose which moves I want to summarize.
Here's a simplified example of what I'm trying to do. I create a 3x3x3x3 array that represents the spaces that could be attacked by a queen at each board square of a 3x3 chess board. In other words, the first two dimensions are the coordinates of a queen on the board, and the last two dimensions are boolean flags of whether the queen could attack that square.
Then I select some squares, and count up how many of those squares could attack each square of the board. That counting step is what I'm trying to do without a Python for loop.
Here's the example code:
import numpy as np

size = 3
patterns = np.zeros((size, size, size, size), dtype=bool)
for i in range(size):
    for j in range(size):
        patterns[i, j, i, :] = True
        patterns[i, j, :, j] = True
        for i2 in range(size):
            shift = i2 - i
            j2 = j + shift
            if 0 <= j2 < size:
                patterns[i, j, i2, j2] = True
            j3 = j - shift
            if 0 <= j3 < size:
                patterns[i, j, i2, j3] = True

active_positions = np.array([[0, 1, 0],
                             [1, 0, 0],
                             [0, 0, 0]], dtype=bool)

# This is the part I want to vectorize:
counts = np.zeros((size, size))
for i in range(size):
    for j in range(size):
        if active_positions[i, j]:
            counts += patterns[i, j]

print(counts)

Is there a way to do that counting without using a for loop?
","Use active_positions to mask patterns and sum along the zeroth (first) axis (that will be an element-wise sum of each field, which in this case is two 3x3 arrays).
counts = patterns[active_positions].sum(0)

Consider the intermediate array patterns[active_positions]; this has shape (2,3,3). Summing along the zeroth axis adds the two 3x3 arrays in an element-wise manner to produce the final (3,3) array that you're looking for.
",numpy
sum up column values by special logic,"Say we have an array like:
a = np.array([
    [k11, k12, k13, k14, k15, k16, k17, k18],
    [k21, k22, k23, k24, k25, k26, k27, k28],
    [k31, k32, k33, k34, k35, k36, k37, k38],
    [k41, k42, k43, k44, k45, k46, k47, k48]
])

const = C

I need to create a vector from this array like this (runge kutta 4):
result = np.array([
    const * (k11 + 2*k21 + 2*k31 + k41),
    const * (k12 + 2*k22 + 2*k32 + k42),
    const * (k13 + 2*k23 + 2*k33 + k43),
        ....
    const * (k18 + 2*k28 + 2*k38 + k48)
])

I am able to do this in cycle, but I am pretty sure numpy methods allow this in vectorised form.
","np.einsum solution:
result = const * np.einsum('ij,i->j', a, [1, 2, 2, 1])

ij,i are the dimensions of a and the coefficients. The result, j is missing i, which means that that dimension is multiplied and summed across the arrays.
This solution is nice because it is very explicit about dimensions without requiring any reshaping or transposition. For larger matrices or longer multiplication chains, the order of operations will be optimized for speed.
",numpy
as a newbie how do i install numpy as a module please,"I have returned to python after a long break.
I have the following error in my code to understand and resolve: ModuleNotFoundError: No module named 'numpy'.
I am using a Mac and working in VSstudio - how do I install numpy as a module please?
I have tried searching extensions for the numpy module, and working through messages on this platform.
","To resolve the issue:

Open Terminal on your Mac.

Install NumPy by running the following command:
pip3 install numpy


Verify the installation with this command:
python3 -c ""import numpy; print(numpy.__version__)""


In VS Code, open the Command Palette by pressing Cmd + Shift + P. Search for Python: Select Interpreter and select the Python environment where NumPy is installed.


",numpy
numpy array of array into array of length vectorization,"I have a numpy array of arrays, where each sub-array represents a group of elements. I want to compute the length of each sub-array and store the result as a new numpy array.
Currently, I use a standard for loop, but I would like to vectorize this operation for better performance using only numpy.
Example
import numpy as np

indices = np.array([np.array([1, 2, 3]), np.array([4, 5]), np.array([6, 7, 8, 9])], dtype=object)
group_lengths = np.array([len(group) for group in indices])

print(group_lengths)

Expected output
[3 2 4]
I am looking for a fully vectorized solution that avoids the explicit loop. How can this be achieved with numpy?
Update
Thank you all for your insights and clarifications! I now understand that fully vectorizing my use case isn't possible. I'll stick with my current solution, as it seems to be the most efficient approach given the circumstances.
","Currently the best option is to use np.fromiter. This solution was proposed in the comments by Jérôme Richard:
# generate big dataset
indices = np.array([np.random.randint(0,10,size=np.random.randint(4,500))
                    for _ in range(100000)
                    ], dtype=object)

Use map function:
group_lengths = np.array(list(map(len, indices)))
# 6.49 ms ± 57.5 μs per loop

Use list generator with len function
group_lengths = np.array([len(group) for group in indices])
# 8.19 ms ± 60.5 μs per loop

Using numpy vectorized:
vfunc = np.vectorize(len)
group_lengths = vfunc(indices)
# 4.86 ms ± 23.2 μs per loop

Apply the function via np.fromiter
group_lengths = np.fromiter(map(len, indices), dtype=object)
# 3.53 ms ± 36.2 μs per loop 

",numpy
ambiguity in pandas dataframe  numpy array quotaxisquot definition,"I've been very confused about how python axes are defined, and whether they refer to a DataFrame's rows or columns. Consider the code below:
>>> df = pd.DataFrame([[1, 1, 1, 1], [2, 2, 2, 2], [3, 3, 3, 3]], columns=[""col1"", ""col2"", ""col3"", ""col4""])
>>> df
   col1  col2  col3  col4
0     1     1     1     1
1     2     2     2     2
2     3     3     3     3

So if we call df.mean(axis=1), we'll get a mean across the rows:
>>> df.mean(axis=1)
0    1
1    2
2    3

However, if we call df.drop(name, axis=1), we actually drop a column, not a row:
>>> df.drop(""col4"", axis=1)
   col1  col2  col3
0     1     1     1
1     2     2     2
2     3     3     3

Can someone help me understand what is meant by an ""axis"" in pandas/numpy/scipy?
A side note, DataFrame.mean just might be defined wrong. It says in the documentation for DataFrame.mean that axis=1 is supposed to mean a mean over the columns, not the rows...
","It's perhaps simplest to remember it as 0=down and 1=across. 
This means:

Use axis=0 to apply a method down each column, or to the row labels (the index).
Use axis=1 to apply a method across each row, or to the column labels.

Here's a picture to show the parts of a DataFrame that each axis refers to:

It's also useful to remember that Pandas follows NumPy's use of the word axis. The usage is explained in NumPy's glossary of terms:

Axes are defined for arrays with more than one dimension. A 2-dimensional array has two corresponding axes: the first running vertically downwards across rows (axis 0), and the second running horizontally across columns (axis 1). [my emphasis] 

So, concerning the method in the question, df.mean(axis=1), seems to be correctly defined. It takes the mean of entries horizontally across columns, that is, along each individual row. On the other hand, df.mean(axis=0) would be an operation acting vertically downwards across rows.
Similarly, df.drop(name, axis=1) refers to an action on column labels, because they intuitively go across the horizontal axis. Specifying axis=0 would make the method act on rows instead.
",numpy
problem with mismatched length when using a mask,"I'm writing a code and I have a function that calculates the values that are not fulfilling a condition with the values that are fulfilling the condition, but I'm having a lot of trouble with managing the shape of the arrays.
I have a similar function, but with other logical structure that does this (MWE for the function that works)
import numpy as np
def f_est(f, fd, mask):
    """"""
    This function returns the estimated value given a function f and its derivative fd 
    for the points selected by the mask, estimating forward.
    
    Parameters:
    - f: Array of function values.
    - fd: Array of derivative values of the function.
    - mask: Boolean mask to select points in f and fd.
    """"""
    h = 0.000001
    
    # Find the last index that satisfies the mask
    last_index = np.max(np.where(mask)[0])
    
    # Create shifted masks (inspired by f_cal), but centered on the last index
    mask_current = mask[:last_index + 1]  # Mask for current positions up to the last index
    mask_prev = mask[:last_index]          # Mask for previous positions (for fd_prev_slice)
    mask_prev2 = mask[:last_index - 1]     # Mask for previous positions (for fd_prev2_slice)
    
    # Apply masks to f and fd (with shifts), centered on the last index
    f_slice = f[:last_index + 1][mask_current]  # Note: adjusted to align with mask_current
    fd_slice = fd[:last_index + 1][mask_current] 
    fd_prev_slice = fd[:last_index][mask_prev]  
    fd_prev2_slice = fd[:last_index - 1][mask_prev2]  
    
    # Perform the calculations with consistent slices, estimating forward
    # Use the last value of f_slice, fd_slice, fd_prev_slice, and fd_prev2_slice for estimation
    last_f = f_slice[-1]
    last_fd = fd_slice[-1]
    last_fd_prev = fd_prev_slice[-1] if len(fd_prev_slice) > 0 else 0
    last_fd_prev2 = fd_prev2_slice[-1] if len(fd_prev2_slice) > 0 else 0
    
    estimated_next_value = (
        last_f 
        + h * last_fd 
        + 1 / 2 * (h * last_fd - h * last_fd_prev) 
        + 5 / 12 * ((h * last_fd - h * last_fd_prev) - (h * last_fd_prev - h * last_fd_prev2))
    )
    
    return estimated_next_value

f = np.array([1, 2, 3, 4, 5, 6, 7])
fd = f 
mask = np.array([True, True, True, False, False, False, False])
print(""Original Array:"", f)
print(""Length of Original Array:"", len(f))
print(""Masked Array:"", f[~mask])
print(""Length of Masked Array:"", len(f[~mask]))
f[~mask] = f_est(f, fd, mask)

print(""Final Array:"", f)
print(""Length of Final Array:"", len(f))

But with this function (MWE that doesn't work):
import numpy as np
def f_cal(var, dvar, mask):
    """"""
    Calculates next value using trapezoidal method with masks, estimating forward.
    
    Parameters:
        var: array of current values
        dvar: array of derivatives 
        mask: boolean array indicating which positions to calculate
    """"""
    h = 0.0000001
    # Encontrar el último índice que verifica la máscara
    last_index = np.max(np.where(mask)[0])
    
    # Crear máscaras para posiciones actuales y la siguiente
    mask_current = mask[:last_index]  
    mask_next =  mask[1:last_index+1] # Marcar como True el índice siguiente
    
    # Ajustar los arreglos para alinear con las máscaras
    var_current = var[:last_index+1][mask_current]
    dvar_current = dvar[:last_index+1][mask_current]
    dvar_next = dvar[:last_index+2][mask_next][:1]  # Solo el valor siguiente
    
    # Calculate using trapezoidal method with masks, estimating forward
    result = var_current + h * dvar_next - 1/2*(h*dvar_next-h*dvar_current[-1])
    
    return result

f = np.array([1, 2, 3, 4, 5,6,7])
fd = f 
mask = np.array([True, True, True, False, False, False, False])
print(""Original Array:"", f)
print(""Length of Original Array:"", len(f))
print(""Masked Array:"", f[~mask])
print(""Length of Masked Array:"", len(f[~mask]))
f[~mask] = f_cal(f, fd, mask)

print(""Final Array:"", f)
print(""Length of Final Array:"", len(f))

I'm having a lot of trouble keeping the length of the array to match the number of elements that are not satisfying the condition
","I believe you're trying to use the Taylor series to approximate the function at a position beyond the last known value. The function values are stored in the var array, and the first derivatives are in the dvar array. It seems like you're using mask to identify the last known function value, and h represents the argument step.
From your code, it looks like you're using terms up to the second derivative, which you approximate as the difference between adjacent first derivatives divided by the argument step. This can be expressed as:

f(x+h) = f(x) + h⋅f′(x) + ½⋅h²⋅f″(x) + … ≈
       ≈ f(x) + h⋅f′(x) + ½⋅h²⋅(f′(x+h)-f′(x))/h

At least, this resonates with the expression you return as the result (which I tweaked slightly):

var_current + h*dvar_next - 1/2*(h*dvar_next-h*dvar_current)

We can transform the above line step by step as follows:

   var_current + h*dvar_next - 1/2*h*dvar_next + 1/2*h*dvar_current
== var_current + 1/2*h*dvar_next + 1/2*h*dvar_current
== var_current + h*dvar_current + 1/2*h*dvar_next - 1/2*h*dvar_current
== var_current + h*dvar_current + 1/2*h*(dvar_next - dvar_current)
== var_current + h*dvar_current + 1/2*(h**2)*(dvar_next - dvar_current)/h

The last line reminded me of the Taylor series, which is why I suspect that the mention of the Trapezoidal method in your function description might be a mistake.
If that's the case, the function could look like this:
def f_cal(var, dvar, mask, h=1e-7):
    index = np.asarray(mask).nonzero()[0]
    assert index.size > 0, 'Nowhere to start from'
    assert index[-1] < len(mask)-1, 'Nowhere to extrapolate to'
    pos = index[-1]
    f = var[pos]
    df = dvar[pos]
    df_next = dvar[pos+1]
    return f + h*df + h*(df_next - df)/2

However, I might have misunderstood your intent. For example, if your reference to the trapezoidal method is deliberate, or you're using a different convention, you may need to clarify your question.

P.S. Answer to the original question about mismatching lengths when using a mask: you seem to have missed handling array dimensions in some places. Here's an alternative implementation to compare with (note that the changes are made based on the assumption that you approximate the function using Taylor series):
def f_cal(var, dvar, mask):
    h = 0.0000001
    last_index = np.max(np.where(mask)[0])
    mask_current = mask[:last_index+1]  
    mask_next =  mask[:last_index+2]
    var_current = var[:last_index+1][mask_current][-1]
    dvar_current = dvar[:last_index+1][mask_current][-1]
    dvar_next = dvar[:last_index+2][mask_next][-1]
    return var_current + h*dvar_next - 1/2*h*(dvar_next-dvar_current)

",numpy
numpythonic way of float to signed integer normalization,"What is the faster numpythonic way of this normalization:
def normalize_vector(x, b, axis):
    """"""
    Normalize real vector x and outputs an integer vector y.

    Parameters:
        x (numpy.ndarray): Input real vector. (batch_size, seq_len)
        b (int): Unsigned integer defining the scaling factor.
        axis (int/None): if None, perform flatenned version, if axis=-1, perform relative normalization across batch.

    Returns:
        numpy.ndarray: Integer vector y.
    """"""
    # Find the maximum absolute value in x
    m = np.max(np.abs(x))

    # Process each element in x
    y = []
    for xi in x:
        if xi > 0:
            y.append(int((2**b - 1) * xi / m))
        elif xi < 0:
            y.append(int(2**b * xi / m))
        else:
            y.append(0)

    return np.array(y)

Can np.digitize make it faster?
I have similar question, but it's not about NumPy.
I'm also expecting it supports axis parameter for batch vector.
","there is np.piecewise to transform data based on multiple conditions.
def normalize_vector2(x, b, axis):
    # Step 1: Find the maximum absolute value in `x`
    m = np.max(np.abs(x), axis=axis)


    y = np.piecewise(x, [x > 0, x < 0],
                     [
                         lambda xi: ((2**b - 1) * xi / m), 
                         lambda xi: (2**b * xi / m)
                     ])

    return y.astype(int)

if your paths are close then you can just simplify it with multiplies.
def normalize_vector3(x, b, axis):
    # Step 1: Find the maximum absolute value in x
    m = np.max(np.abs(x), axis=axis, keepdims=True)
    m[m==0] = 1

    y = (2**b - 1 * (x > 0)) * x / m
    return y.astype(int)

comparison:
import numpy as np
import time

def normalize_vector2(x, b):
    # Step 1: Find the maximum absolute value in `x`
    m = np.max(np.abs(x))


    y = np.piecewise(x, [x > 0, x < 0],
                     [
                         lambda xi: ((2**b - 1) * xi / m),
                         lambda xi: (2**b * xi / m)
                     ])

    return y.astype(int)

def normalize_vector3(x, b, axis):
    # Step 1: Find the maximum absolute value in x
    m = np.max(np.abs(x), axis=axis, keepdims=True)
    m[m==0] = 1

    y = (2**b - 1 * (x > 0)) * x / m
    return y.astype(int)

def normalize_vector(x, b):
    # Find the maximum absolute value in x
    m = np.max(np.abs(x))

    # Process each element in x
    y = []
    for xi in x:
        if xi > 0:
            y.append(int((2**b - 1) * xi / m))
        elif xi < 0:
            y.append(int(-2**b * xi / m))
        else:
            y.append(0)

    return np.array(y)

for elements in [10, 100, 1000, 10000]:
    iterations = int(100000 / elements)
    x = np.random.random(elements) * 256-128

    t1 = time.time()
    for i in range(iterations):
        normalize_vector(x,7)
    t2 = time.time()

    for i in range(iterations):
        normalize_vector2(x, 7)

    t3 = time.time()

    for i in range(iterations):
        normalize_vector3(x, 7, 0)
    t4 = time.time()

    print(f""{(t2-t1)/iterations:.7f}, {elements} elements python"")
    print(f""{(t3-t2)/iterations:.7f}, {elements} elements numpy"")
    print(f""{(t4-t3)/iterations:.7f}, {elements} elements numpy maths"")

0.0000109, 10 elements python
0.0000331, 10 elements numpy
0.0000158, 10 elements numpy maths
0.0000589, 100 elements python
0.0000399, 100 elements numpy
0.0000168, 100 elements numpy maths
0.0005812, 1000 elements python
0.0000515, 1000 elements numpy
0.0000255, 1000 elements numpy maths
0.0045110, 10000 elements python
0.0003255, 10000 elements numpy
0.0001083, 10000 elements numpy maths

numpy is slower than pure python for small lists (mostly < 50 elements).
",numpy
python ctypes and nparrayctypesdata_as unexpected behavior when indexing,"Using Python ctypes and numpy library, I pass data to a shared library and encounter a very weird behavior
C function :
#include <stdio.h>
typedef struct {
    double *a;
    double *b;
} s_gate;
void printdouble(s_gate*, int);

void printdouble(s_gate *gate, int n) {
    for (int i =0; i < n; i++) {
        printf(""gate->a[%d] = %f\n"", i, gate->a[i]);
    }
    for (int i =0; i < n; i++) {
        printf(""gate->b[%d] = %f\n"", i, gate->b[i]);
    }
}

Python code :
import ctypes
import numpy as np
class s_gate(ctypes.Structure):
    _fields_ = [('a', ctypes.POINTER(ctypes.c_double)),
                ('b', ctypes.POINTER(ctypes.c_double))]

    def __init__(self, mydict:dict):
        mask = [True, False, True, True, True, True, False, False, False, True]
        a = np.ascontiguousarray(mydict['a'], dtype=np.double)
        b = np.ascontiguousarray(mydict['b'], dtype=np.double)
        setattr(self, 'a', a[0,:].ctypes.data_as(ctypes.POINTER(ctypes.c_double)))
        setattr(self, 'b', b[0,:].ctypes.data_as(ctypes.POINTER(ctypes.c_double)))
        self.size = 10 

if __name__ == ""__main__"":

    a = np.array([[1,2,3,4,5,6,7,8,9,10], [10,9,8,7,6,5,4,3,2,1]], dtype=np.double).T
    b = a + 100
    
    data = {'a': a, 'b': b}

    mylib = ctypes.CDLL('./mwe.so')
    mylib.printdouble.argstype = [ctypes.POINTER(s_gate), ctypes.c_int]
    mylib.printdouble.restype = ctypes.c_void_p
    print(f'Sending \n{a} and \n{b}')
    gate = s_gate(data)

    mylib.printdouble(ctypes.byref(gate), gate.size)

Running this code, I got the expected result, which is :
[[ 1.  2.  3.  4.  5.  6.  7.  8.  9. 10.]
 [10.  9.  8.  7.  6.  5.  4.  3.  2.  1.]] and 
[[101. 102. 103. 104. 105. 106. 107. 108. 109. 110.]
 [110. 109. 108. 107. 106. 105. 104. 103. 102. 101.]]
gate->a[0] = 1.000000
gate->a[1] = 2.000000
gate->a[2] = 3.000000
gate->a[3] = 4.000000
gate->a[4] = 5.000000
gate->a[5] = 6.000000
...
gate->b[0] = 101.000000
gate->b[1] = 102.000000
gate->b[2] = 103.000000
gate->b[3] = 104.000000
gate->b[4] = 105.000000
gate->b[5] = 106.000000
...

Now, let's use the mask variable in the __init__ method of the s_gate class. So let's replace lines 11 and 12 with :
setattr(self, 'a', a[0,mask].ctypes.data_as(ctypes.POINTER(ctypes.c_double)))
setattr(self, 'b', b[0,mask].ctypes.data_as(ctypes.POINTER(ctypes.c_double)))
self.size = sum(mask)

The results is now :
gate->a[0] = 0.000000
gate->a[1] = 0.000000
gate->a[2] = 0.000000
gate->a[3] = 0.000000
gate->a[4] = 0.000000
gate->a[5] = 0.000000
gate->b[0] = 101.000000
gate->b[1] = 103.000000
gate->b[2] = 104.000000
gate->b[3] = 105.000000
gate->b[4] = 106.000000
gate->b[5] = 110.000000

All gate.a data is zeroed ! The expected result is of course [1, 3, 4, 5, 6, 10] for gate.a
What I tried so far :
Use np.require, np.ascontiguousarray to ensure C contiguous array, perform integer indexing instead of logical indexing, perform 2D logical indexing (array[slices_xy] instead of array[slice_x, slice_y]).
Use various copy, deepcopy methods and create an intermediate data with the first slicing...
Nothing worked, as soon as the slice that maskdescribe in this code appears (instead of :), this behavior appears. What goes wrong ?
","The memory is being freed for the masked arrays creating undefined behavior.  Likely the a pointer's memory is reused but the b pointer happens to still be the same.  Both are freed though.
Create the masked arrays and hold a reference to them in the s_gate object, then it works:
import ctypes as ct
import numpy as np

PDOUBLE = ct.POINTER(ct.c_double)

class s_gate(ct.Structure):
    _fields_ = [('a', PDOUBLE),
                ('b', PDOUBLE)]

    def __init__(self, a, b):
        mask = [True, False, True, True, True, True, False, False, False, True]
        self.tmpa = a[0,mask]  # hold a reference to the arrays
        self.tmpb = b[0,mask]
        self.a = self.tmpa.ctypes.data_as(PDOUBLE)  # get pointers to the arrays
        self.b = self.tmpb.ctypes.data_as(PDOUBLE)
        self.size = sum(mask)

a = np.array([[1,2,3,4,5,6,7,8,9,10], [10,9,8,7,6,5,4,3,2,1]], dtype=np.double)
b = a + 100

mylib = ct.CDLL('./test')
mylib.printdouble.argstype = ct.POINTER(s_gate), ct.c_int
mylib.printdouble.restype = ct.c_void_p
print(f'Sending \n{a} and \n{b}')
gate = s_gate(a, b)

mylib.printdouble(ct.byref(gate), gate.size)

Output:
[[ 1.  2.  3.  4.  5.  6.  7.  8.  9. 10.]
 [10.  9.  8.  7.  6.  5.  4.  3.  2.  1.]] and
[[101. 102. 103. 104. 105. 106. 107. 108. 109. 110.]
 [110. 109. 108. 107. 106. 105. 104. 103. 102. 101.]]
gate->a[0] = 1.000000
gate->a[1] = 3.000000
gate->a[2] = 4.000000
gate->a[3] = 5.000000
gate->a[4] = 6.000000
gate->a[5] = 10.000000
gate->b[0] = 101.000000
gate->b[1] = 103.000000
gate->b[2] = 104.000000
gate->b[3] = 105.000000
gate->b[4] = 106.000000
gate->b[5] = 110.000000

",numpy
efficiently custom array creation routines in jax,"I'm still getting a handle of best practices in jax. My broad question is the following:
What are best practices for the implementation of custom array creation routines in jax?
For instance, I want to implement a function that creates a matrix with zeros everywhere except with ones in a given column. I went for this (Jupyter notebook):
import numpy as np
import jax.numpy as jnp

def ones_at_col(shape_mat, idx):
    idxs = jnp.arange(shape_mat[1])[None,:]
    mat = jnp.where(idx==idxs, 1, 0)
    mat = jnp.repeat(mat, shape_mat[0], axis=0)
    return mat

shape_mat = (5,10)

print(ones_at_col(shape_mat, 5))

%timeit np.zeros(shape_mat)

%timeit jnp.zeros(shape_mat)

%timeit ones_at_col(shape_mat, 5)

The output is
[[0 0 0 0 0 1 0 0 0 0]
 [0 0 0 0 0 1 0 0 0 0]
 [0 0 0 0 0 1 0 0 0 0]
 [0 0 0 0 0 1 0 0 0 0]
 [0 0 0 0 0 1 0 0 0 0]]
127 ns ± 0.717 ns per loop (mean ± std. dev. of 7 runs, 10,000,000 loops each)
31.3 µs ± 331 ns per loop (mean ± std. dev. of 7 runs, 10,000 loops each)
123 µs ± 1.79 µs per loop (mean ± std. dev. of 7 runs, 10,000 loops each)

My function is a factor of 4 slower than the jnp.zeros() routine, which is not too bad. This tells me that what I'm doing is not crazy.
But then both jax routines are much slower than the equivalent numpy routines. These functions cannot be jitted because they take the shape as an argument, and so cannot be traced. I presume this is why they are inherently slower? I guess that if either of them appeared within the scope of another jitted function, they could be traced and sped up?
Is there something better I can do or am I pushing the limits of what is possible in jax?
","The best way to do this is probably something like this:
mat = jnp.zeros(shape_mat).at[:, 5].set(1)

Regarding timing comparisons with NumPy, relevant reading is JAX FAQ: is JAX faster than NumPy? The summary is that for this particular case (creating a simple array) you would not expect JAX to match NumPy performance-wise, due to JAX's per-operation dispatch overhead.
If you wish for faster performance in JAX, you should always use jax.jit to just-in-time compile your function. For example, this version of the function should be pretty optimal (though again, not nearly as fast as NumPy for the reasons discussed at the FAQ link):
@partial(jax.jit, static_argnames=['shape_mat', 'idx'])
def ones_at_col(shape_mat, idx):
  return jnp.zeros(shape_mat).at[:, idx].set(1)

You could leave idx non-static if you'll be calling this function multiple times with different index values, and if you're creating these arrays within another function, you should just put the code inline and JIT-compile that outer function.
Another side-note: your microbenchmarks may not be measuring what you think they're measuring: for tips on this see JAX FAQ: benchmarking JAX code. In particular, be careful of compilation time and asynchronous dispatch effects.
",numpy
how to create a column of increasing value base on other column with same value,"all
I have a data frame as below
df = pd.DataFrame.from_dict({'A':[1,1,2], 'B':[10,20,14], ""C"":[30,40,20]})


How to create a new column for example newCol which depends on column A. if value at A remains same, then newCol will fill with a increasing integer. if there is a diff value in A, here 2, then the value of newCol will reset and start increasing again.
Thank you.

","Code
grp = df['A'].ne(df['A'].shift()).cumsum()
df['newCol'] = df['A'].groupby(grp).cumcount().add(1)

df:
   A   B   C  newCol
0  1  10  30       1
1  1  20  40       2
2  2  14  20       1

",numpy
why is loginf  inf j equal to inf  0785398 j in cpythonnumpy,"I've been finding a strange behaviour of log functions in C++ and numpy about the behaviour of log function handling complex infinite numbers. Specifically, log(inf + inf * 1j) equals (inf + 0.785398j) when I expect it to be (inf + nan * 1j).
When taking the log of a complex number, the real part is the log of the absolute value of the input and the imaginary part is the phase of the input. Returning 0.785398 as the imaginary part of log(inf + inf * 1j) means it assumes the infs in the real and the imaginary part have the same length.
This assumption does not seem to be consistent with other calculation, for example, inf - inf == nan, inf / inf == nan which assumes 2 infs do not necessarily have the same values.
Why is the assumption for log(inf + inf * 1j) different?
Reproducing C++ code:
#include <complex>
#include <limits>
#include <iostream>
int main() {
    double inf = std::numeric_limits<double>::infinity();
    std::complex<double> b(inf, inf);
    std::complex<double> c = std::log(b);
    std::cout << c << ""\n"";
}

Reproducing Python code (numpy):
import numpy as np

a = complex(float('inf'), float('inf'))
print(np.log(a))

EDIT: Thank you for everyone who's involved in the discussion about the historical reason and the mathematical reason. All of you turn this naive question into a really interesting discussion. The provided answers are all of high quality and I wish I can accept more than 1 answers. However, I've decided to accept @simon's answer as it explains in more detail the mathematical reason and provided a link to the document explaining the logic (although I can't fully understand it).
","See Edit 2 at the bottom of the answer for a mathematical motivation (or rather, at least, the reference to one).
The value of 0.785398 (actually pi/4) is consistent with at least some other functions: as you said, the imaginary part of the logarithm of a complex number is identical with the phase angle of the number. This can be reformulated to a question of its own: what is the phase angle of inf + j * inf?
We can calculate the phase angle of a complex number z by atan2(Im(z), Re(z)). With the given number, this boils down to calculating atan2(inf, inf), which is also 0.785398 (or pi/4), both for Numpy and C/C++. So now a similar question could be asked: why is atan2(inf, inf) == 0.785398?
I do not have an answer to the latter (except for ""the C/C++ specifications say so"", as others already answered), I only have a guess: as atan2(y, x) == atan(y / x) for x > 0, probably someone made the decision in this context to not interpret inf / inf as ""undefined"" but instead as ""a very large number divided by the same very large number"". The result of this ratio would be 1, and atan(1) == pi/4 by the mathematical definition of atan.
Probably this is not a satisfying answer, but at least I could hopefully show that the log definition in the given edge case is not completely inconsistent with similar edge cases of related function definitions.
Edit: As I said, consistent with some other functions: it is also consistent with np.angle(complex(np.inf, np.inf)) == 0.785398, for example.
Edit 2: Looking at the source code of an actual atan2 implementation brought up the following code comment:

note that the non obvious cases are y and x both infinite or both zero. for more information, see Branch Cuts for Complex Elementary Functions, or Much Ado About Nothing's Sign Bit, by W. Kahan

I dug up the referenced document, you can find a copy here. In Chapter 8 of this reference, called ""Complex zeros and infinities"", William Kahan (who is both mathematician and computer scientist and, according to Wikipedia, the ""Father of Floating Point"") covers the zero and infinity edge cases of complex numbers and arrives at pi/4 for feeding inf + j * inf into the arg function (arg being the function that calculates the phase angle of a complex number, just like np.angle above). You will find this result on page 17 in the linked PDF. I am not mathematician enough for being able to summarize Kahan's rationale (which is to say: I don't really understand it), but maybe someone else can.
",numpy
numpy nanargmin select indice of 0 if row contains all nan,"Given the following matrix:
matrix = np.array([[0,np.nan,1],[np.nan,np.nan,np.nan],[1,2,3]])

I would like to obtain an array of min row values. In the case that a row contains all nan values, that indice for that row of all nan values should be 0. The reslting array should be.
array([0,0,0])

If I try to use np.argmin(matrix,axis=1) then the min indice is where np.nan occurs e.g:
array([1, 0, 0])

This is not desired, and if I use np.nanargmin(matrix,axis=1) I get raise ValueError(""All-NaN slice encountered"")
","Fill the NaNs with infinity using numpy.nan_to_num, then get the argmin:
np.argmin(np.nan_to_num(matrix, nan=float('inf')), axis=1)

output: array([0, 0, 0])
",numpy
numpy float64 vs python float,"I'm battling some floating point problems in Pandas read_csv function.  In my investigation, I found this:
In [15]: a = 5.9975

In [16]: a
Out[16]: 5.9975

In [17]: np.float64(a)
Out[17]: 5.9974999999999996

Why is builtin float of Python and the np.float64 type from Python giving different results?  I thought they were both C++ doubles?
",">>> numpy.float64(5.9975).hex()
'0x1.7fd70a3d70a3dp+2'
>>> (5.9975).hex()
'0x1.7fd70a3d70a3dp+2'

They are the same number. What differs is their representation; the Python native type uses a ""sane"" representation, and the NumPy type uses an accurate representation.
",numpy
increase image brightness without overflow,"I got a problem when trying to increase image brightness.
Here is the origin image:

The image I wanted to get is like this:

Now to increase the brightness with the following code:
    image = cv2.imread(""/home/wni/vbshare/tmp/a4_index2.png"",0)

    if sum(image[0])/len(image[0])<200:
        new = np.where((255-image)<image,255,image*2)
    else:
        new = image
    return new

And, I got the following image:

So, seems brightness of some points overflowed.
And I tried to change the threshold from 200 to some other number, e.g. 125, 100, 140 .etc
However, the image brightness stays either almost same dark or overflow.
Env:
Python: 2.7.10
Opencv: 3.2.0
Any suggestion for this is appreciated.
Thanks.
","Here's my shot at a simple algorithm for cleaning up that particular image. Feel free to play with it and tweak it further to get the desired result.
NB: The code shown should work both with the 2.4.x and 3.x branches of OpenCV.
Step 0
Load the input image as grayscale.
img = cv2.imread('paper.jpg',0)

Step 1
Dilate the image, in order to get rid of the text.
This step somewhat helps to preserve the bar code.
dilated_img = cv2.dilate(img, np.ones((7,7), np.uint8)) 


Step 2
Median blur the result with a decent sized kernel to further suppress any text.
This should get us a fairly good background image that contains all the shadows and/or discoloration.
bg_img = cv2.medianBlur(dilated_img, 21)


Step 3
Calculate the difference between the original and the background we just obtained. The bits that are identical will be black (close to 0 difference), the text will be white (large difference).
Since we want black on white, we invert the result.
diff_img = 255 - cv2.absdiff(img, bg_img)


Step 4
Normalize the image, so that we use the full dynamic range.
norm_img = diff_img.copy() # Needed for 3.x compatibility
cv2.normalize(diff_img, norm_img, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX, dtype=cv2.CV_8UC1)


Step 5
At this point we still have the paper somewhat gray. We can truncate that away, and re-normalize the image.
_, thr_img = cv2.threshold(norm_img, 230, 0, cv2.THRESH_TRUNC)
cv2.normalize(thr_img, thr_img, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX, dtype=cv2.CV_8UC1)


Done...
Well, at least for me ;) You will probably want to crop it, and do whatever other post-processing you desire.

Note: It might be worth switching to higher precision (16+ bit int or float) after you get the difference image, in order to minimize accumulating rounding errors in the repeated normalizations.
",numpy
how can i return a numpy array using fastapi,"I have a TensorFlow Keras deep learning model in the form of an h5 file.
How can I upload an image and return a NumPy array in FastAPI?
import numpy as np
import cv2
from fastapi import FastAPI, File, UploadFile
import numpy as np
from tensorflow.keras.models import load_model
import tensorflow as tf

model=load_model(""complete_model.h5"")
app = FastAPI()

def prepare(image):
    IMG_SIZE = 224
    new_array = cv2.resize(image, (IMG_SIZE, IMG_SIZE))
    return new_array.reshape(-1, IMG_SIZE,IMG_SIZE,3)

@app.post(""/"")
async def root(file: UploadFile = File(...)):
    global model
    content = await file.read()
    nparr = np.fromstring(content, np.uint8)
    img = cv2.imdecode(nparr, cv2.IMREAD_COLOR).astype(np.float32)
    prediction = model.predict(prepare(img))
    return prediction

When uploading the image using Swagger UI, I get the following error:
line 137, in jsonable_encoder
data = dict(obj)
TypeError: 'numpy.float32' object is not iterable

Working code without FastAPI:
import numpy as np
import numpy as np
from tensorflow.keras.models import load_model
import tensorflow as tf
import cv2

model=load_model(""complete_model.h5"")

def prepare(image):
    IMG_SIZE = 224
    new_array = cv2.resize(image, (IMG_SIZE, IMG_SIZE))
    return new_array.reshape(-1, IMG_SIZE,IMG_SIZE,3)

img = cv2.imread(""./test.jpeg"").astype(np.float32)
prediction = model.predict(prepare(img))
print(prediction)

Result in the terminal:
[[0.25442022 0.74557984]]

How can I get the same result while using FastAPI?
","The error is thrown when returning the response (i.e., prediction in your case) from your endpoint. It looks like FastAPI is trying to convert the NumPy array into a dict, using the jsonable_encoder, which is used internally by FastAPI when returning a value from an endpoint, and which seems to call Python's vars() method, as shown in the error you provided here (have a look at the discussion here, as well as the documentation). Thus, what you could do is to convert the NumPy array into a Python list, then serialize it into a JSON string and return it:
return json.dumps(prediction.tolist())

Note that instead of returning the JSON string in the way it is shown above, which would casue FastAPI to serialize it again behind the scenes, you might consider returning a custom Response directly, as demonstrated here, as well as here and here.
On Swagger UI /docs, you should still be able to see the expected result. However, if you needed to convert it back to a NumPy array, you could parse the JSON string in Python, as shown below.
arr = np.asarray(json.loads(resp.json()))  # resp.json() if using Python requests

If you would like to return the NumPy array as raw bytes and display the image in the browser or download it, have a look at this answer.
",numpy
how do i create an empty array and then append to it in numpy,"I want to create an empty array and append items to it, one  at a time.
xs = []
for item in data:
    xs.append(item)

Can I use this list-style notation with NumPy arrays?
","That is the wrong mental model for using NumPy efficiently. NumPy arrays are stored in contiguous blocks of memory. To append rows or columns to an existing array, the entire array needs to be copied to a new block of memory, creating gaps for the new elements to be stored. This is very inefficient if done repeatedly.
Instead of appending rows, allocate a suitably sized array, and then assign to it row-by-row:
>>> import numpy as np

>>> a = np.zeros(shape=(3, 2))
>>> a
array([[ 0.,  0.],
       [ 0.,  0.],
       [ 0.,  0.]])

>>> a[0] = [1, 2]
>>> a[1] = [3, 4]
>>> a[2] = [5, 6]

>>> a
array([[ 1.,  2.],
       [ 3.,  4.],
       [ 5.,  6.]])

",numpy
what39s the difference between numpy39s structured arrays vs xarray xray,"What's the difference between Numpy Structured Arrays named fields vs xarray (xray) N-D labeled arrays ?
","From the numpy docs on structured arrays:

Structured datatypes [i.e. structured numpy arrays] are designed to be able to mimic ‘structs’ in the C language, and share a similar memory layout. They are meant for interfacing with C code and for low-level manipulation of structured buffers, for example for interpreting binary blobs. For these purposes they support specialized features such as subarrays, nested datatypes, and unions, and allow control over the memory layout of the structure.
Users looking to manipulate tabular data, such as stored in csv files, may find other pydata projects more suitable, such as xarray, pandas, or DataArray. These provide a high-level interface for tabular data analysis and are better optimized for that use. For instance, the C-struct-like memory layout of structured arrays in numpy can lead to poor cache behavior in comparison.

",numpy
mean and median of distribution given numpy histogram,"Suppose you have a numpy histogram computed from some data (which you don't have access to), so you only know bins and counts. Is there an efficient way of computing the mean and median of the distribution described by the histogram?
","No you can't. After aggregation as histogram, the initial information is partially lost. You cannot exactly compute the mean/median of the original population.
As a demonstration here are two different arrays (with different means/medians) that give the same counts and bins:
a1 = np.array([10, 20, 100, 300, 310])
np.mean(a1), np.median(a1)
# (148.0, 100.0)

a2 = np.array([10, 10, 130, 300, 310])
np.mean(a2), np.median(a2)
# (152.0, 130.0)

np.histogram(a1, bins=2)
# (array([3, 2]), array([ 10., 160., 310.])

np.histogram(a2, bins=2)
# (array([3, 2]), array([ 10., 160., 310.])

Approximation
You can however determine the limits of the mean:
low = np.average(bins[:-1], weights=cnt)
high = np.average(bins[1:], weights=cnt)
print(f'The average is in the {low}-{high} range.')
# The average is in the 70.0-220.0 range.

And for the median:
cnt_cumsum = np.add.accumulate(cnt)
idx = np.searchsorted(cnt_cumsum, half)
low = bins[idx]
high = bins[idx+1]
print(f'The median is in the {low}-{high} range.')
# The median is in the 10.0-160.0 range.

Example with 1000 random values and 20 bins:
True data mean: 0.496, median: 0.481
The average is in the 0.471-0.521 range.
The median is in the 0.45-0.5 range.

",numpy
how can i convert the datatype of a numpy array sourced from an awkward array,"I have a numpy array I converted from awkward array by to_numpy() function, and the resulting array has the datatype: dtype=[('phi', '<f8'), ('eta', '<f8')]). I want to make it a regular tuple of (float32, float32) because otherwise this does not convert into a tensorflow tensor
I tried the regular asdtype functions but all I get is errors
>>> array = ak.Array([{""phi"": 1.1, ""eta"": 2.2}, {""phi"": 3.3, ""eta"": 4.4}])
>>> ak.to_numpy(array)
array([(1.1, 2.2), (3.3, 4.4)], dtype=[('phi', '<f8'), ('eta', '<f8')])

","I believe your problem is equivalent to this: you have some Awkward Array with record structure,
>>> array = ak.Array([{""phi"": 1.1, ""eta"": 2.2}, {""phi"": 3.3, ""eta"": 4.4}])

and when you convert that with ak.to_numpy, it turns the record fields into NumPy structured array fields:
>>> ak.to_numpy(array)
array([(1.1, 2.2), (3.3, 4.4)], dtype=[('phi', '<f8'), ('eta', '<f8')])

ML libraries like TensorFlow and PyTorch want the feature vectors to not have fields with names, but instead be 2D arrays in which the second dimension ranges over all of the features. If all of the NumPy structured array dtypes are identical, as they're all <f8 in this example, you could view it:
>>> ak.to_numpy(array).view(""<f8"").reshape(len(array), -1)
array([[1.1, 2.2],
       [3.3, 4.4]])

But this is unsafe. If, for example, some of your fields are 32-bit and others are 64-bit, or some are integers and others are floating-point, view will just reinterpret the memory, losing the meaning of the numbers:
>>> bad = np.array([(1, 2, 3.3), (4, 5, 6.6)], dtype=[(""x"", ""<i4""), (""y"", ""<i4""), (""z"", ""<f8"")])
>>> bad.view(""<f8"").reshape(len(bad), -1)
array([[4.24399158e-314, 3.30000000e+000],
       [1.06099790e-313, 6.60000000e+000]])

(z's 3.3 and 6.6 are preserved, but x and y get merged into a single field and the raw memory gets interpreted as floats.)
Instead, we should make the structure appropriate in Awkward, which has the tools to do exactly this sort of thing, and afterward convert it to NumPy (and from there to TensorFlow or PyTorch).
So, we're starting with an array of records with named fields:
>>> array
<Array [{phi: 1.1, eta: 2.2}, {...}] type='2 * {phi: float64, eta: float64}'>

We want the named fields to go away and make these individual arrays. That's ak.unzip.
>>> ak.unzip(array)
(<Array [1.1, 3.3] type='2 * float64'>, <Array [2.2, 4.4] type='2 * float64'>)

(The first in the tuple is from phi, the second is from eta.)
We want to get values for each field together into the same input vector for the ML model. That is, 1.1 and 2.2 should be in a vector [1.1, 2.2] and 3.3 and 4.4 should be in a vector [3.3, 4.4]. That's a concatenation of the arrays in this tuple, but not an axis=0 concatenation that would make [1.1, 3.3, 2.2, 4.4]; it has to be a concatenation in a higher axis=1. That axis doesn't exist yet, but we can always make length-1 axes with np.newaxis.
>>> ak.unzip(array[:, np.newaxis])
(<Array [[1.1], [3.3]] type='2 * 1 * float64'>, <Array [[2.2], [4.4]] type='2 * 1 * float64'>)

Now ak.concatenate with axis=1 will concatenate [1.1] and [2.2] into [1.1, 2.2], etc.
>>> ak.concatenate(ak.unzip(array[:, np.newaxis]), axis=1)
<Array [[1.1, 2.2], [3.3, 4.4]] type='2 * 2 * float64'>

So in the end, here's a one-liner that you can pass to TensorFlow that will work even if your record fields have different dtypes:
>>> ak.to_numpy(ak.concatenate(ak.unzip(array[:, np.newaxis]), axis=1))
array([[1.1, 2.2],
       [3.3, 4.4]])

Or, actually, maybe you can skip the ak.to_numpy and go straight to ak.to_tensorflow.
",numpy
explain this 4d numpy array indexing intuitively,"x = np.random.randn(4, 3, 3, 2)
print(x[1,1])

output:
[[ 1.68158825 -0.03701415]
[ 1.0907524  -1.94530359]
[ 0.25659178  0.00475093]]

I am python newbie. I can't really understand 4-D array index like above. What does x[1,1] mean?
For example, for vector
a = [[2][3][8][9]], a[0] = 2, a[3] = 9. 

I get this but I don't know what x[1,1] refers to.
Please explain in detail. Thank you.
","A 2D array is a matrix : an array of arrays.
A 4D array is basically a matrix of matrices:

Specifying one index gives you an array of matrices:
>>> x[1]
array([[[-0.37387191, -0.19582887],
        [-2.88810217, -0.8249608 ],
        [-0.46763329,  1.18628611]],

       [[-1.52766397, -0.2922034 ],
        [ 0.27643125, -0.87816021],
        [-0.49936658,  0.84011388]],

       [[ 0.41885001,  0.16037164],
        [ 1.21510322,  0.01923682],
        [ 0.96039904, -0.22761806]]])


Specifying two indices gives you a matrix:
>>> x[1, 1]
array([[-1.52766397, -0.2922034 ],
       [ 0.27643125, -0.87816021],
       [-0.49936658,  0.84011388]])


Specifying three indices gives you an array:
>>> x[1, 1, 1]
array([ 0.27643125, -0.87816021])


Specifying four indices gives you a single element:
>>> x[1, 1, 1, 1]
-0.87816021212791107


x[1,1] gives you the small matrix that was saved in the 2nd column of the 2nd row of the large matrix.
",numpy
calculate relative difference of elements in a 1d numpy array,"Say I have a 1D numpy-array given by np.array([1,2,3]).
Is there a built-in command for calculating the relative difference between each element and display it in a 2D-array? The result would then be given by
np.array([[0,-50,-100*2/3], [100,0,-100*1/3], [200,50,0]])
Alternatively I would have to use a for-loop.
","Use numpy broadcasting:
a = np.array([1,2,3])

out = (a[:, None]-a)/a*100

Output:
array([[  0.        , -50.        , -66.66666667],
       [100.        ,   0.        , -33.33333333],
       [200.        ,  50.        ,   0.        ]])

",numpy
how to efficiently compute and process 3x3x3 voxel neighborhoods in a 3d numpy array,"I am working on a function to process 3D images voxel-by-voxel. For each voxel, I compute the difference between the voxel value and its 3x3x3 neighborhood, apply distance-based scaling, and determine a label based on the maximum scaled difference. However, my current implementation is slow, and I'm looking for ways to optimize it.
import numpy as np 
from tqdm import tqdm import itertools

def create_label(image_input): 
    # Define neighborhood offsets and distances 
    locations = np.array([tuple([dx, dy, dz]) for dx, dy, dz in itertools.product(range(-1, 2), repeat=3)]) 
    euclidian_distances = np.linalg.norm(locations, axis=1) 
    euclidian_distances_inverse = np.divide(1, euclidian_distances, out=np.zeros_like(euclidian_distances), where=euclidian_distances != 0)
    
    image_input = image_input.astype(np.float32)
    label = np.zeros_like(image_input)
    image_shape = image_input.shape
    
    # Process each voxel (excluding the edges)
    for i in tqdm(range(1, image_shape[0] - 1)):
        for j in range(1, image_shape[1] - 1):
            for k in range(1, image_shape[2] - 1):
                # Extract 3x3x3 neighborhood values
                all_values_neighborhood = [image_input[loc[0] + i, loc[1] + j, loc[2] + k] for loc in locations]
    
                # Compute differences and scale by distances
                centre_minus_rest = image_input[i, j, k, None] - np.array(all_values_neighborhood)
                if np.all(centre_minus_rest < 0):  
                    label[i, j, k] = 13
                else:
                    centre_minus_rest_divided = centre_minus_rest * euclidian_distances_inverse
                    centre_minus_rest_divided[13] = -100  # Ignore the center value
                    class_label = np.argmax(centre_minus_rest_divided)
                    label[i, j, k] = class_label

return label

# example 3d array
image_input = np.random.rand(10, 10, 10).astype(np.float32)
# run the function
label = create_label(image_input) 
print(label)

","You could use a sliding_window_view, and vectorize the for loops.
One attempt
def create_label2(image_input):
    # Define neighborhood offsets and distances 
    locations = np.array([tuple([dx, dy, dz]) for dx, dy, dz in itertools.product(range(-1, 2), repeat=3)])
    euclidian_distances = np.linalg.norm(locations, axis=1)
    euclidian_distances_inverse = np.divide(1, euclidian_distances, out=np.zeros_like(euclidian_distances), where=euclidian_distances != 0)

    image_input = image_input.astype(np.float32)
    label = np.zeros_like(image_input)
    image_shape = image_input.shape

    neighborhood = np.lib.stride_tricks.sliding_window_view(image_input, (3,3,3))

    # Process each voxel (excluding the edges)
    centre_minus_rest = image_input[1:-1, 1:-1, 1:-1, None,None,None] - neighborhood
    centre_minus_rest_divided = centre_minus_rest * euclidian_distances_inverse.reshape(1,1,1,3,3,3)
    centre_minus_rest_divided[:,:,:,1,1,1] = -100
    class_label = np.argmax(centre_minus_rest_divided.reshape(image_shape[0]-2, image_shape[1]-2, image_shape[2]-2,27), axis=3)
    label[1:-1,1:-1,1:-1] = np.where((centre_minus_rest<0).all(axis=(3,4,5)), 13, class_label)

    return label

On my machine, that is a ×70 gain
What it does is building a (P-2)×(H-2)×(W-2)×3×3×3 array of all 3x3x3 cubes around each z,y,x points in 1..P-1, 1..H-1, 1..W-1
From there, it is then possible to computes in single vectorized operations what you did in for loops
I said ""attempt"", which may not seem very confident, because that is quite memory consuming. Not sliding_window_view itself, that costs nothing: it is just a view, with no own data in memory (the memory is the one of image_input. So neighborhood[12,13,14,1,1,1] is the same as neighborhood[13,14,15,0,0,0]. Not just the same value: the same data, the same memory place. If you neighborhood[12,13,14,1,1,1]=999, and then print(neighborhood[13,14,15,0,0,0]) or print(neighborhood[13,12,13,0,2,2]), it prints your 999 back.
So neighborhood cost nothing. And its shape (that amounts to, roughly, 27 times the volume of initial image) is ""virtual"".
But, still, even if it is not using actual memory, that is an array of almost 27 times more values than in the image (each being repeated 27 times — ""almost"" part being because of edges). So, as soon as you do some computation on it, you get such an array, with this times all values in its own place in memory.
In other words, if you start from a 100x100x100 image, that uses 1000000 memory places (4000000 bytes, since those are np.float32), neighborhood cost nothing: it is the same 4000000 bytes that makes its almost 27000000 values.
But centre_minus_rest, and then the subsequent computation do occupy ""almost"" 108000000 bytes in memory.
So, depending on the size of your image (and the memory available on your machine), we may have replaced a cpu-time problem by a memory problem by doing that.
Compromises are possible tho.
you could keep your first first loop (for i in range...), and then vectorize the computation for all j and k. Anyway, inner loops are the one that cost the more, and need to be vectorized. Not vectorizing outer loop is not that serious. In my 100x100x100 example, that means that I have 100 non-vectorized iterations and 1010000 vectorized ones... it is not a huge regression for the previous 1010100 vectorized for iterations
for i in range(1, image_shape[0]-1):
        centre_minus_rest = image_input[i, 1:-1, 1:-1, None,None,None] - neighborhood[i-1]
        centre_minus_rest_divided = centre_minus_rest * euclidian_distances_inverse.reshape(1,1,3,3,3)
        centre_minus_rest_divided[:,:,1,1,1] = -100
        class_label = np.argmax(centre_minus_rest_divided.reshape(image_shape[1]-2, image_shape[2]-2,27), axis=2)
        label[i,1:-1,1:-1] = np.where((centre_minus_rest<0).all(axis=(2,3,4)), 13, class_label)

That is as fast on my machine. Still have the same ×70 gain, with no measurable decrease. And yet, intermediates values (centre_minus_rest, ...) are 3 times smaller in memory.
Note that this is still not ideal code. I should pick size once for all between having 3x3x3 subarrays of values, euclideans distances, ..., or having a 1D array of size 27.
(But sliding_window_view needs a 3x3x3 array, otherwise it can do its magic => reshaping into (P-2,H-2,W-2,27) would necessitate a full copy ; and on the contrary, .argmax can't work along more than 1 axis, and needs length-27 arrays)
Also, the itertools trick is very pythonic, but not at all numpythonic.
Something like np.arange(3)[:,None,None]**2+np.arange(3)[None,:,None]**2+np.arange(3)[None,None,:]...
But that is anyway outside the costly loop
It must also be noted that one inconvenient of vectorization is that it sometimes forces you to compute more things that you need (but if you compute twice too much things, but 100 times faster, that is a x50 time gain).
Here, my np.where((centre_minus_rest<0).all(axis=(2,3,4)), 13, class_label) means that for each value for which answer to that is 13, I have computed class_label in vain. Which your for for for if wasn't.
So take it more as a first lead on how to do it, more than as a definitive solution.
But, still, ×70 is a good start.
",numpy
why is my matrix multiplication using numpy so slow,"I am trying to multiply two matrices in numpy with rather large dimensionality.
See the 3 methods below. I realise the 3 matrices randomly to show my problem. The first matrix, namely Y1[:,:,0] is part of a bigger 3d-array at first. The second is a .copy() of this matrix and the third is its own matrix.
Why is the first multiplication so much slower than the second two?
import numpy as np
from time import time

Y1 = np.random.uniform(-1, 1, (5000, 1093, 201))
Y2 = Y1[:,:,0].copy()
Y3 = np.random.uniform(-1, 1, (5000, 1093))

W = np.random.uniform(-1, 1, (1093, 30))

# method 1
START = time()
Y1[:,:,0].dot(W)
END = time()
print(f""Method 1 : {END - START}"")

# method 2
START = time()
Y2.dot(W)
END = time()
print(f""Method 2 : {END - START}"")

# method 3
START = time()
Y3.dot(W)
END = time()
print(f""Method 3 : {END - START}"")

The output times are roughly 34, 0.06, 0.06 seconds respectively.
I see the difference: Whereas the last two matrices are ""real"" 2d-arrays, the first one is a slice of my bigger 3d-array.
Is the subsetting Y1[:,:,0] what makes it so slow? Also, I noted that creating the copy of Y1 for the matrix Y2 is also quite slow.
After all, I am given this 3d-array and have to repeatedly calculate the matrix product of the slices of Y1 with a (potentially different) matrix W. Is there a better / faster way to do this?
Thanks in advance!
","This is a cache problem. If you study the cost difference compared to the size of the third axis, you'll a linear relation at first (k=1 => no difference, k=2, method 1 cost twice much, k=3, method 1 cost three times more, etc.), capped by a maximum (for k=20 or k=30 doesn't really change the situation)
That maximum cap is dependent on the size of other axis
The thing is, the matrix multiplication (and, basically, any operation on arrays) operate often iteratively. So data in memory are read one after the other.
The first data read cost a bit, because memory is slow. But when you access data in memory, a whole line (something like 64 or 128 bytes) is read and stored into cache. If next operation uses next number in the matrix, and this number happens to be just next to the previous one in memory, it, likely, belongs to the same cache line. And it won't be necessary to read it in memory, we have it in (way faster) cache memory.
It is a bit oversimplified. And not that obvious to see how it applies to matrix multiplication, because a matrix multiplication is not that sequential.
But, basically, the more you use data that are close to each other in memory, the faster. And people often overlook this, thinking that this is kind of hacker optimization to grab some extra nanosecond. But effect can be huge.
For very small amount of data, that fit entirely in cache (some kilobytes), and complex enough algorithm that read it more than once (even just a matrix multiplication qualifies), it doesn't really shows, because every data will end up in cache after a few computation steps.
But if everything doesn't fit in cache, the wider the space between your data, the less you can reuse a cache line. And the more you will have to reread data in memory. To the point that reading memory is the leading cost.
So, your problem is that in
Y1=np.random.uniform(-1, 1, (5000, 1093, 201))

each data of Y1[:,:,0] is separated by at least 201x8 = 1608 bytes. So, cache (for Y1 — it is still used for W, but all method are equal for this) is useless: no chance to have a fast access to a value of Y1[:,:,0] thanks to to fact that we have already read a value close to it in memory: they are all far to each other.
Another way to convince you that this is your problem, and maybe the solution, if needed
Just look at what would have happened if
Y1=np.random.uniform(-1, 1, (201, 5000, 1093)).transpose(1,2,0)

Y1 is exactly the same shape as yours. Same 5000x1093x201 array. And you keep the same subdata Y1[:,:,0] of shape 5000x1093.
The only difference between my Y1 and yours, is invisible from a pure ""maths"" point of view; the only difference is where exactly, in physical memory the data are stored.
In my your Y1, Y1[i,j,0] is far from Y1[i+1,j,0], and far from Y1[i,j+1,0] (but close to Y1[i,j,1], but that won't help in your case).
You can see it by watching Y1.strides
Y1.strides
# (1757544, 1608, 8)

which tells you how many bytes separate tow consecutive values along each axis. You see that it is bigger than typical cache size along all axis but the last one, which is the once you don't use
While my Y1
(8744, 8, 43720000)

Of course, the problem is, when you reduce your problem to the single part that is slow, you may conclude that you should code Y1 as I did.
But I suppose that your code doesn't allocate 201 numbers, and just never use the 200 others. Said otherwise, some other, unshown, parts of your code probably use that 3rd axis.
So, maybe the boost you gain by ordering Y1 in the optimal order for this part of the code would have to be paid in other part of the code by slower computation.
Last remark: when doing this kind of computation, it is important to avoid running thing only once. Because, also, of cache. The first algorithm is biases against because it has to read W, while the two other may find it already waiting in cache (probably not in your case, because your data is too bing. But for smaller data, you would have concluded that first method is slower, whatever the first is, just because it is the one that paid the cost of loading data into cache
",numpy
index numpy array by other array as indices,"I'm trying to take array 
a = [1,5,4,5,7,8,9,8,4,13,43,42]

and array 
b = [3,5,6,2,7]

And I want b to be the indexes in a, e.g. a new array that is
[a[b[0]], a[b[1]], a[b[2]], a[b[3]] ...]

So the values in b are indexes into a.
And there are 500k entries in a and 500k in b (approximately).
Is there a fast way to kick in all cores in numpy to do this?
I already do it just fine in for loops and it is sloooooooowwwwww.
Edit to clarify.  The solution has to work for 2D and 3D arrays.
so maybe
b = [(2,3), (5,4), (1,2), (1,0)]

and we want
c = [a[b[0], a[b[1], ...]

","I solved this by writing a C extension to numpy called Tensor Weighted Interpolative Transfer, in order to get speed and multi-threading. In pure python it is 3 seconds per 200x100x3 image scale and fade across, and in multi-threaded C with 8 cores is 0.5 milliseconds for the same operation.
The core C code ended up being like
t2[dstidxs2[i2] + doff1] += t1[srcidxs2[i2] + soff1] * w1 * ws2[i2];

Where the doff1 is the offset in the destination array etc. The w1 and ws2 are the interpolated weights.
All the code is ultra optimized in C for speed. (not code size or maintainability)
All code is available on https://github.com/RMKeene/twit and on PyPI.
I expect further optimization in the future such as special cases if all weights are 1.0.
---   Dec 2024 note: I dropped Twit because it is not what I need for my AI research any more.
",numpy
how do i check if a numpy dtype is integral,"How do I check if a numpy dtype is integral?  I tried:
issubclass(np.int64, numbers.Integral)

but it gives False.

Update: it now gives True.
","Numpy has a hierarchy of dtypes similar to a class hierarchy (the scalar types actually have a bona fide class hierarchy that mirrors the dtype hierarchy).
In Numpy>2.0, you should use np.isdtype: np.isdtype(np.void, 'integral').
Otherwise,  you can use np.issubdtype(some_dtype, np.integer) to test if a dtype is an integer dtype. Note that like most dtype-consuming functions, np.issubdtype() will convert its arguments to dtypes, so anything that can make a dtype via the np.dtype() constructor can be used.
http://docs.scipy.org/doc/numpy/reference/arrays.dtypes.html#specifying-and-constructing-data-types
>>> import numpy as np
>>> np.issubdtype(np.int32, np.integer)
True
>>> np.issubdtype(np.float32, np.integer)
False
>>> np.issubdtype(np.complex64, np.integer)
False
>>> np.issubdtype(np.uint8, np.integer)
True
>>> np.issubdtype(np.bool, np.integer)
False
>>> np.issubdtype(np.void, np.integer)
False

Alternatively, the scalar types are now registered with the appropriate numbers ABCs.
",numpy
pandas finding local max and min,"I have a pandas data frame with two columns one is temperature the other is time. 
I would like to make third and fourth columns called min and max. Each of these columns would be filled with nan's except where there is a local min or max, then it would have the value of that extrema.  
Here is a sample of what the data looks like, essentially I am trying to identify all the peaks and low points in the figure. 

Are there any built in tools with pandas that can accomplish this?
","Assuming that the column of interest is labelled data, one solution would be
df['min'] = df.data[(df.data.shift(1) > df.data) & (df.data.shift(-1) > df.data)]
df['max'] = df.data[(df.data.shift(1) < df.data) & (df.data.shift(-1) < df.data)]

For example:
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd

# Generate a noisy AR(1) sample
np.random.seed(0)
rs = np.random.randn(200)
xs = [0]
for r in rs:
    xs.append(xs[-1]*0.9 + r)
df = pd.DataFrame(xs, columns=['data'])

# Find local peaks
df['min'] = df.data[(df.data.shift(1) > df.data) & (df.data.shift(-1) > df.data)]
df['max'] = df.data[(df.data.shift(1) < df.data) & (df.data.shift(-1) < df.data)]

# Plot results
plt.scatter(df.index, df['min'], c='r')
plt.scatter(df.index, df['max'], c='g')
df.data.plot()


",numpy
numpythonic way of the inverse of sliding_window_view,"I have original array test:
from numpy.lib.stride_tricks import sliding_window_view

test = np.array([1,2,3,4,5,6,7,8,9,10,11,12,13,14]).reshape(-1,7) # (batch_size, seq_len) -> (2,7)
slided = sliding_window_view(test, window_shape=(3,), axis=-1).copy()
print(test, test.shape)
print(slided, slided.shape)

Outputting:
[[ 1  2  3  4  5  6  7]
 [ 8  9 10 11 12 13 14]] (2, 7)

[[[ 1  2  3]
  [ 2  3  4]
  [ 3  4  5]
  [ 4  5  6]
  [ 5  6  7]]

 [[ 8  9 10]
  [ 9 10 11]
  [10 11 12]
  [11 12 13]
  [12 13 14]]] (2, 5, 3)

Given copied of slided array that was computed by sliding_window_view returning shape (batch_size, num_win, win_len), how do I reconstruct back into original array test with the shape of (batch_size, seq_len)?
","Since you have a copy instead of the original sliding window view, you can't just reconstruct the shape and strides to reconstruct the original data. The copy has the wrong memory layout for that to work - the strides would have to change midway through a dimension.
But two views of your array put together can reconstruct the original data, so if we copy two such views into a new array, we get an array equivalent to the original:
import numpy

def unslide(arr):
    shape = arr.shape[:-2] + (arr.shape[-2] + arr.shape[-1] - 1,)
    view_1 = arr[..., 0]
    view_2 = arr[..., -1, 1:]

    res = numpy.empty(shape=shape, dtype=arr.dtype)
    res[..., :view_1.shape[-1]] = view_1
    res[..., view_1.shape[-1]:] = view_2

    return res

Looking at one ""layer"" of your array:
[[[ 1  2  3]
  [ 2  3  4]
  [ 3  4  5]
  [ 4  5  6]
  [ 5  6  7]]

we're effectively putting together the first column and last row of that layer to reconstruct that part of the original array, and doing that for every layer. view_1 handles the first column (of every layer), and view_2 handles the last row.
",numpy
coding a bifurcation diagram,"I was wondering what was wrong with my code as I keep getting a weird but similar result to the actual bifurcation diagram. I am using the iterative equation xn+1 = xn * r(1-xn).
Here is my code:
import numpy as np 
import matplotlib.pyplot as plt
from functools import lru_cache

@lru_cache(maxsize = 1000)
def bifunc():
    R_val = []
    X_val = []
    R = np.linspace(0.5,4,1000)
    for r in R:
        x = 0.5
        for iterations in range(1001):
            x = x*r*(1-x)
            R_val.append(r)
            X_val.append(x)

    plt.plot(R_val, X_val, ls = '', marker = ',')
    plt.show()

bifunc()

Here is the image that keeps coming up:

Any help would be appreciated. Thank you.
","As the first few iterations don't follow the pattern, they can be left out via an if-test. Best also remove the lru_cache as it has no real function here and can interfere with testing.
import numpy as np 
import matplotlib.pyplot as plt

def bifunc():
    R_val = []
    X_val = []
    R = np.linspace(0.5, 4, 1000)
    for r in R:
        x = 0.5
        for iterations in range(1001):
            x = x*r*(1-x)
            if iterations > 100:
                R_val.append(r)
                X_val.append(x)
    plt.plot(R_val, X_val, ls='', marker=',')
    plt.show()

bifunc()


This results in a plot similar to many found on the web. Usually the plots don't start before R=1 to avoid displaying the flat part between 0.5 and 1.
",numpy
error in keyword argument for matrix_rank,"I want to compute the rank of a matrix, and introduce a certain relative tolerance to take into account the numerical errors. I use the following code :
    A = np.random.randn(n,n)
    r = np.linalg.matrix_rank(A, rtol = 1e-5)

but i get the following error : 'TypeError: matrix_rank() got an unexpected keyword argument 'rtol''
As written here : https://numpy.org/doc/stable/reference/generated/numpy.linalg.matrix_rank.html, I don't understand why it doesn't work.
","your version of NumPy does not support the rtol keyword in the np.linalg.matrix_rank so you can upgrade it or use this code
import numpy as np

A = np.random.randn(4, 4)
tol = 1e-5
u, s, vh = np.linalg.svd(A) 
rank = np.sum(s > tol * np.max(s)) 
print(""Rank:"", rank)

",numpy
java equivalent for the numpy multidimensional object,"After using it for a while, I really like the Numpy multi-dimensional array. It's helpful to write algorithms with a concise yet readable and fairly general code. I wish to have the same thing in Java. Before coding a multi-dimensional array with a Numpy-like API myself, is there such a thing already ?
[PS] I searched a bit, did not see
","So the closest match seems to be Colt ! http://acs.lbl.gov/software/colt/
It features a multi-dimensional array object, views over an array and your usual linear algebra ! And it's seems to be rather efficient.
",numpy
extending numpy random via pyx throws cython error in converting python object to bitgen_t,"I am trying to use the Numpy C-API to extend random numbers through BitGenerator as in the numpy example in the documentation, and I am running into a PyObject cannot be convert to bitgen_t error.
I have copied the first code into a file named extension_rand.pyx as follows:
extension_rand.pyx
from libc.stdint cimport uint32_t
from cpython.pycapsule import PyCapsule_IsValid, PyCapsule_GetPointer

import numpy as np
cimport numpy as np
cimport cython

from numpy.random cimport bitgen_t
from numpy.random import PCG64

np.import_array()

@cython.boundscheck(False)
@cython.wraparound(False)
def uniforms(Py_ssize_t n):
    """"""
    Create an array of `n` uniformly distributed doubles.
    A 'real' distribution would want to process the values into
    some non-uniform distribution
    """"""
    cdef Py_ssize_t i
    cdef bitgen_t *rng
    cdef const char *capsule_name = ""BitGenerator""
    cdef double[::1] random_values
    cdef np.ndarray randoms

    x = PCG64()
    capsule = x.capsule
    # Optional check that the capsule if from a BitGenerator
    if not PyCapsule_IsValid(capsule, capsule_name):
        raise ValueError(""Invalid pointer to anon_func_state"")
    # Cast the pointer
    rng = PyCapsule_GetPointer(capsule, capsule_name)
    random_values = np.empty(n, dtype='float64')
    with x.lock, nogil:
        for i in range(n):
            # Call the function
            random_values[i] = rng.next_double(rng.state)
    randoms = np.asarray(random_values)

    return randoms

setup.py
My setup.py file is as follows:
# setup.py

from distutils.core import setup
from Cython.Build import cythonize
from setuptools.extension import Extension
import numpy as np
import os

lib_path = os.path.join(np.get_include(), '..', '..', 'random', 'lib')


extending = Extension(name=""extension_rand"", 
    sources=['extension_rand.pyx'],
    include_dirs=[np.get_include()],
    library_dirs=[lib_path],
    libraries=['npyrandom']
    )


setup(
    ext_modules=cythonize(
        extending, language_level=""3"", annotate=""True""
    )
)


Compile command
When I run
python setup.py build_ext --inplace

I get the following error:
Error log
Compiling extension_rand.pyx because it changed.
[1/1] Cythonizing extension_rand.pyx

Error compiling Cython file:
------------------------------------------------------------
...
    capsule = x.capsule
    # Optional check that the capsule if from a BitGenerator
    if not PyCapsule_IsValid(capsule, capsule_name):
        raise ValueError(""Invalid pointer to anon_func_state"")
    # Cast the pointer
    rng = PyCapsule_GetPointer(capsule, capsule_name)
                              ^
------------------------------------------------------------

extension_rand.pyx:33:30: Cannot convert Python object to 'bitgen_t *'

Error compiling Cython file:
------------------------------------------------------------
...
    capsule = x.capsule
    # Optional check that the capsule if from a BitGenerator
    if not PyCapsule_IsValid(capsule, capsule_name):
        raise ValueError(""Invalid pointer to anon_func_state"")
    # Cast the pointer
    rng = PyCapsule_GetPointer(capsule, capsule_name)
                              ^
------------------------------------------------------------

extension_rand.pyx:33:30: Storing unsafe C derivative of temporary Python reference
Traceback (most recent call last):
  File ""/Users/sayantanghosh/work/cython_test/setup.py"", line 21, in <module>
    ext_modules=cythonize(
                ^^^^^^^^^^
  File ""/Users/sayantanghosh/anaconda3/envs/nonlin/lib/python3.11/site-packages/Cython/Build/Dependencies.py"", line 1154, in cythonize
    cythonize_one(*args)
  File ""/Users/sayantanghosh/anaconda3/envs/nonlin/lib/python3.11/site-packages/Cython/Build/Dependencies.py"", line 1321, in cythonize_one
    raise CompileError(None, pyx_file)
Cython.Compiler.Errors.CompileError: extension_rand.pyx

System information / runtime environment
The output of my import numpy as np; np.show_runtime() is
[{'numpy_version': '1.26.4',
  'python': '3.11.10 | packaged by conda-forge | (main, Sep 30 2024, 17:57:16) '
            '[Clang 17.0.6 ]',
  'uname': uname_result(system='Darwin', node='calypso.local', release='22.6.0', version='Darwin Kernel Version 22.6.0: Thu Sep  5 20:48:48 PDT 2024; root:xnu-8796.141.3.708.1~1/RELEASE_X86_64', machine='x86_64')},
 {'simd_extensions': {'baseline': ['SSE', 'SSE2', 'SSE3', 'SSSE3'],
                      'found': ['SSE41',
                                'POPCNT',
                                'SSE42',
                                'AVX',
                                'F16C',
                                'FMA3',
                                'AVX2'],
                      'not_found': ['AVX512F',
                                    'AVX512CD',
                                    'AVX512_KNL',
                                    'AVX512_SKX',
                                    'AVX512_CLX',
                                    'AVX512_CNL',
                                    'AVX512_ICL']}},
 {'architecture': 'Haswell',
  'filepath': '/Users/sayantanghosh/anaconda3/envs/nonlin/lib/libopenblasp-r0.3.21.dylib',
  'internal_api': 'openblas',
  'num_threads': 2,
  'prefix': 'libopenblas',
  'threading_layer': 'pthreads',
  'user_api': 'blas',
  'version': '0.3.21'}]

My setuptools version is '75.1.0', while cython version is '3.0.10'.
I am not sure if this is a cython or numpy C-API related issue.
I will be grateful for any help.
","I think there's two issues:

from cpython.pycapsule import PyCapsule_IsValid, PyCapsule_GetPointer needs to be from cpython.pycapsule cimport PyCapsule_IsValid, PyCapsule_GetPointer. (cimport rather than import so the type is known at Cython compile time. Otherwise it assumes that the functions are arbitrary Python functions returning arbitrary Python objects).

PyCapsule_GetPointer returns a void* and you need to cast that to a bitgen_t* (i.e. rng = <bitgen_t*>PyCapsule_GetPointer(capsule, capsule_name))


",numpy
concatenate several np arrays in python,"I have several bumpy arrays and I want to concatenate them. I am using np.concatenate((array1,array2),axis=1). My problem now is that I want to make the number of arrays parametrizable, I wrote this function
x1=np.array([1,0,1])
x2=np.array([0,0,1])
x3=np.array([1,1,1])  

def conc_func(*args):
    xt=[]
    for a in args:
        xt=np.concatenate(a,axis=1)
    print xt
    return xt

xt=conc_func(x1,x2,x3)

this function returns ([1,1,1]), I want it to return ([1,0,1,0,0,1,1,1,1]). I tried to add the for loop inside the np.concatenate as such 
xt =np.concatenate((for a in args: a),axis=1)

but I am getting a syntax error. I can't used neither append nor extend because I have to deal with numpy arrays and not lists. Can somebody help?
Thanks in advance
","concatenate can accept a sequence of array-likes, such as args:
In [11]: args = (x1, x2, x3)

In [12]: xt = np.concatenate(args)

In [13]: xt
Out[13]: array([1, 0, 1, 0, 0, 1, 1, 1, 1])

By the way, although axis=1 works, the inputs are all 1-dimensional arrays (so they only have a 0-axis). So it makes more sense to use axis=0 or omit axis entirely since the default is axis=0.
",numpy
pandas group consecutive and label the length,"I want get consecutive length labeled data

a 
---
1   
0 
1 
0 
1 
1 
1 
0 
1
1

I want :
a | c 
--------
1   1
0   0
1   2
1   2
0   0
1   3
1   3
1   3
0   0
1   2
1   2


then I can calculate the mean of ""b"" column by group ""c"". tried with shift and cumsum and cumcount all not work.
","Use GroupBy.transform by consecutive groups and then set 0 if not 1 in a column:
df['c1'] = (df.groupby(df.a.ne(df.a.shift()).cumsum())['a']
              .transform('size')
              .where(df.a.eq(1), 0))
print (df)
    a  b  c  c1
0   1  1  1   1
1   0  2  0   0
2   1  3  2   2
3   1  2  2   2
4   0  1  0   0
5   1  3  3   3
6   1  1  3   3
7   1  3  3   3
8   0  2  0   0
9   1  2  2   2
10  1  1  2   2

If there are only 0, 1 values is possible multiple by a:
df['c1'] = (df.groupby(df.a.ne(df.a.shift()).cumsum())['a']
              .transform('size')
              .mul(df.a))
print (df)
    a  b  c  c1
0   1  1  1   1
1   0  2  0   0
2   1  3  2   2
3   1  2  2   2
4   0  1  0   0
5   1  3  3   3
6   1  1  3   3
7   1  3  3   3
8   0  2  0   0
9   1  2  2   2
10  1  1  2   2

",numpy
no enum for numpy uintp,"I am trying to wrap a C pointer array of type size_t with a numpy ndarray via Cython using the following:
cimport numpy as cnp
from libcpp.vector cimport vector
cnp.import_array()

cdef size_t num_layers = 10
cdef vector[size_t] steps_taken_vec = vector[size_t]()
steps_taken_vec.resize(3 * num_layers)
cdef size_t* steps_taken_ptr = steps_taken_vec.data()

cdef cnp.npy_intp[2] shape = [3, num_layers]
cdef cnp.npy_intp ndim     = 2
self.shooting_method_steps_taken_array = cnp.PyArray_SimpleNewFromData(
    ndim,
    &shape[0],
    cnp.NPY_UINTP,    # <-- This is the problem
    steps_taken_ptr)  # steps_taken_ptr is a size_t*

The above produces the error ""cimported module has no attribute 'NPY_UINTP'"". According to numpy's documentation there should be an enum that directs numpy to create an array using size_t: https://numpy.org/devdocs/reference/c-api/dtype.html#c.NPY_TYPES.NPY_UINTP
The PyArray_SimpleNewFromData API requires a enum that defines the type used to create the ndarray.
However, the actual init.pxd does not appear to have that enum. It does set the type correctly, see line 25 here but no enum in this list.
Those links, and my code, is using numpy 1.26.4. I looked ahead at 2.0+ and see that there was some definitional changes to this type, but the enum still appears to be missing (see here)
As a workaround, I am using cnp.NPY_UINT64 which works but I am not sure if that is guranteed to be the same size as size_t across platforms and into the future.
Am I missing something here?
","The easiest solution is probably to declare NPY_UINTP yourself.
def extern from *:
    cdef enum:
        NPY_UINTP

This just assumes Cython that some enum-like constant exists called NPY_UINTP. You can pass that to PyArray_SimpleNewFromData. It doesn't matter that it's not from the ""official"" Numpy pxd file.

As I said in a comment, do report the missing constant to Numpy (or better yet, contribute a fix to Numpy). If they don't know then they can't fix it.
",numpy
what is npuint8,"Is np.uint9 possible? Why use it?
red_lower = np.array([136, 87, 111], np.uint9)

","https://numpy.org/doc/stable/reference/arrays.scalars.html#unsigned-integer-types
class numpy.ubyte[source]
Unsigned integer type, compatible with C unsigned char.

Character code
'B'

Alias on this platform (Linux x86_64)
numpy.uint8: 8-bit unsigned integer (0 to 255).

Most often this is used for arrays representing images, with the 3 color channels having small integer values (0 to 255).
",numpy
efficient masked argsort in numpy,"I have a numpy array such as this one:
arr = np.array([
    [1, 2, 3],
    [4, -5, 6],
    [-1, -1, -1]
])

And I would like to argsort it, but with a arr <= 0 mask. The output should be:
array([[0, 1, 2],
       [0, 2],       # (Note that the indices are still relative to original un-masked array)
       []])

However, the output I get using np.ma.argsort() is:
array([[0, 1, 2],
       [0, 2, 1],
       [0, 1, 2]])

The approach needs to be very efficient because the real array has millions of columns. I am thinking this needs to be a synthesis of a few operations, but I don't know which ones.
","The np.where approach:
Input array
arr = np.array([
    [1, 2, 3],
    [4, -5, 6],
    [-1, -1, -1]
])

Mask of valid elements
mask = arr > 0

Preallocate result as an object array to hold variable-length indices
result = np.empty(arr.shape[0], dtype=object)

Efficient masked argsort for each row
for i in range(arr.shape[0]):
    valid_indices = np.where(mask[i])[0]  # Get indices of valid (masked) elements
    result[i] = valid_indices[np.argsort(arr[i, valid_indices])]  # Sort valid indices by their values

Output:
[array([0, 1, 2]) array([0, 2]) array([], dtype=int64)]

The np.flatnonzero approach:
A more optimised approach using vectorised operations:
def optimized_masked_argsort(arr, mask):
    result = np.empty(arr.shape[0], dtype=object)
    for i in range(arr.shape[0]):
        row = arr[i]
        valid_indices = np.flatnonzero(mask[i])  # Faster than np.where(mask[i])[0]
        valid_values = row[valid_indices]
        sorted_order = np.argsort(valid_values)
        result[i] = valid_indices[sorted_order]
    return result

Comparison:
Timings for given example:
np.where Time: 0.000034 seconds
np.flatnonzero Time: 0.000017 seconds

Timings for larger array (1000 rows):
np.where Time: 0.001856 seconds
np.flatnonzero Time: 0.001754 seconds

I tried a few other methods but they fell short in efficiency.
",numpy
how to mark repeated entries as true starting from the second occurrence using numpy,"Problem
I have a NumPy array and need to identify repeated elements, marking the second occurrence and beyond as True, while keeping the first occurrence as False.
For example, given the following array:
np.random.seed(100)
a = np.random.randint(0, 5, 10)
# Output: [0 0 3 0 2 4 2 2 2 2]

I want to get the following output:
[False True False True False False True True True True]

How can I achieve this using NumPy functions only, without using any loops or extra libraries?
What did you try and what were you expecting?
I was able to get it working with a loop, but I wanted to solve it using only NumPy functions. I tried implementing np.cumsum with masks, but I couldn’t make much progress.
Here's the solution I came up with using one loop:
np.random.seed(100)
a = np.random.randint(0, 5, 10)
print(a)
uniques, first_indices = np.unique(a, return_index=True)
all_occurrences = np.zeros_like(a, dtype=bool)
for i in range(len(a)):
    all_occurrences[i] = np.any(a[:i] == a[i])

all_occurrences[first_indices] = False
print(all_occurrences)

","To reveal that the problem is actually less complicated than it may seem at first glance, the question could be rephrased as follows: Mark all first occurrences of values with False.
This leads to a bit of a simplified version of EuanG's answer¹:
def find_repeated(a):
    mask = np.ones_like(a, dtype=bool)
    mask[np.unique(a, return_index=True)[-1]] = False
    return mask

Steps: (1) Initialize the result mask as an all-True array of appropriate shape. (2) Find the indices of the first occurrences in the given array a. (3) Only set these indices to False in the result mask.
To make the code also work with n-dimensional arrays, we need to add an extra step of unraveling the result of np.unique(), as it returns the indices into the flattened given array a:
def find_repeated(a):
    mask = np.ones_like(a, dtype=bool)
    mask[np.unravel_index(np.unique(a, return_index=True)[-1], a.shape)] = False
    return mask

In either case:

We can directly use the indices (np.unique(…, return_index=True)[-1]) for indexing the mask array.
No need for catching the empty-array case here, as it is handled implicitly.

¹) Yes, I find EuanG's answer perfectly acceptable as well. No, I did not downvote it.
",numpy
count first consecutive matches on a group,"I am quite new to Pandas, I am trying to count the total of the first consecutive instances of color from this DataFrame
    car   color
0   audi  black
1   audi  black
2   audi   blue
3   audi  black
4    bmw   blue
5    bmw  green
6    bmw   blue
7    bmw   blue
8   fiat  green
9   fiat  green
10  fiat  green
11  fiat   blue

Thanks to jezrael I have it so it counts the cumulative number of times the first color appears with this:
import pandas as pd

df = pd.DataFrame(data={
  'car': ['audi', 'audi', 'audi', 'audi', 'bmw', 'bmw', 'bmw', 'bmw', 'fiat', 'fiat', 'fiat', 'fiat'],'color': ['black', 'black', 'blue', 'black', 'blue', 'green', 'blue', 'blue', 'green', 'green', 'green', 'blue']
})

df1 = (df.groupby('car')['color']
          .transform('first')
          .eq(df['color'])
          .view('i1')
          .groupby(df['car'])
          .sum()
          .reset_index(name='colour_cars'))

print(df1)


And it works well for counting the total
    car  colour_cars
0  audi            3
1   bmw            3
2  fiat            3

But it turns out what I really need is to count the first consecutive sum, so it should be
    car  colour_cars
0  audi            2
1   bmw            1
2  fiat            3


I have tried to use an apply function to stop the series .sum() if a False is encounter by .eq, any help to find a way to break the count once a False is returned from the .eq would be greatly appreciated.
","Use:
df = (df.groupby(['car', df.color.ne(df.color.shift()).cumsum()])
        .size()
        .reset_index(level=1, drop=True)
        .reset_index(name='colour_cars')
        .drop_duplicates('car'))

print (df)
    car  colour_cars
0  audi            2
3   bmw            1
6  fiat            3

Details:
Create helper consecutive Series for test consecutive values of color column, pass to GroupBy.size, remove first level created from helper function by DataFrame.reset_index, convert index to columns by second reset_index and last get first rows per cars by DataFrame.drop_duplicates:
print (df.color.ne(df.color.shift()).cumsum())
0     1
1     1
2     2
3     3
4     4
5     5
6     6
7     6
8     7
9     7
10    7
11    8
Name: color, dtype: int32

",numpy
triangle wave shaped array in python,"What is the most efficient way to produce an array of 100 numbers that form the shape of the triangle wave below, with a max/min amplitude of 0.5?
Triangle waveform in mind:

","Use a generator:
def triangle(length, amplitude):
     section = length // 4
     for direction in (1, -1):
         for i in range(section):
             yield i * (amplitude / section) * direction
         for i in range(section):
             yield (amplitude - (i * (amplitude / section))) * direction

This'll work fine for a length divisible by 4, you may miss up to 3 values for other lengths.
>>> list(triangle(100, 0.5))
[0.0, 0.02, 0.04, 0.06, 0.08, 0.1, 0.12, 0.14, 0.16, 0.18, 0.2, 0.22, 0.24, 0.26, 0.28, 0.3, 0.32, 0.34, 0.36, 0.38, 0.4, 0.42, 0.44, 0.46, 0.48, 0.5, 0.48, 0.46, 0.44, 0.42, 0.4, 0.38, 0.36, 0.33999999999999997, 0.32, 0.3, 0.28, 0.26, 0.24, 0.21999999999999997, 0.2, 0.18, 0.15999999999999998, 0.14, 0.12, 0.09999999999999998, 0.08000000000000002, 0.06, 0.03999999999999998, 0.020000000000000018, -0.0, -0.02, -0.04, -0.06, -0.08, -0.1, -0.12, -0.14, -0.16, -0.18, -0.2, -0.22, -0.24, -0.26, -0.28, -0.3, -0.32, -0.34, -0.36, -0.38, -0.4, -0.42, -0.44, -0.46, -0.48, -0.5, -0.48, -0.46, -0.44, -0.42, -0.4, -0.38, -0.36, -0.33999999999999997, -0.32, -0.3, -0.28, -0.26, -0.24, -0.21999999999999997, -0.2, -0.18, -0.15999999999999998, -0.14, -0.12, -0.09999999999999998, -0.08000000000000002, -0.06, -0.03999999999999998, -0.020000000000000018]

",numpy
bottleneck using npwhere in term of computing ressources ie memory and speed,"In the current example, I spent (many) hours in order to find a way to sort the M matrix so that it corresponds exactly to the target one.

The 2 first columns are used to reorganize M
The matrixOrder indicates how to ""proceed"" (couples of values are important)
note the 2 couples [5, 15] and [5, 11] which must be tackled differently
at the final stage, the third column is extracted

For big arrays (hundreds thousands of lines, even millions), the amount of memory is huge and this step time is consuming (bottleneck).
# -*- coding: utf-8 -*-
import numpy as np

M = np.array([ [ 5  , 15 , 6 ],
               [ 14 , 15 , 14 ],
               [ 5  , 11 , 350 ],
               [ 5  , 11, 352 ],
               [ 5  , 11 , 351 ],
               [ 5  , 11 , 350 ],
               [ 9  , 11 , 351 ],
               [ 9  , 11 , 95 ],
               [ 9  , 11 , 353 ],
               [ 9  , 11 , 354 ],
               [ 28 , 15 , 28 ],
               [ 2  , 8 , 46 ],
               [ 2  , 8 , 353 ],
               [ 2  , 8 , 45 ],
               [ 21 , 15 , 21 ],
               [ 31 , 20 , 355 ],
               [ 31 , 20 , 358 ]])


matrixOrder = np.array([ [14, 15],
                         [2 , 8],
                         [31, 20],
                         [5 , 11],
                         [21, 15],
                         [9, 11],
                         [5, 15],
                         [28, 15] ])
                                           
Target = np.array([ [ 14 , 15 , 14 ],
                    [ 2 , 8 , 46 ],
                    [ 2 , 8 , 353 ],
                    [ 2 , 8 , 45 ],
                    [ 31 , 20 , 355 ],
                    [ 31 , 20 , 358 ],
                    [ 5 , 11 , 350 ],
                    [ 5 , 11 , 352 ],
                    [ 5 , 11 , 351 ],
                    [ 5 , 11 , 350 ],
                    [ 21 , 15 , 21 ],
                    [ 9 , 11 , 351 ],
                    [ 9 , 11 , 95 ],
                    [ 9 , 11 , 353 ],
                    [ 9 , 11 , 354 ],
                    [ 5 , 15 , 6 ],
                    [ 28 , 15 , 28 ] ])
                                        
index = np.where( (matrixOrder[:, 0].reshape(-1, 1) == M[:, 0].tolist()) &
                  (matrixOrder[:, 1].reshape(-1, 1) == M[:, 1].tolist()) )

T = M[index[1], :]
check = np.array_equal(Target, T)

","Try doing a dictionary lookup instead of using np.where
something like -
def sort_mat(M, ord):
    # Convert order pairs to tuples
    k = [tuple(r) for r in ord]
    
    # Make lookup dictionary
    d = {k: i for i,k in enumerate(k)}
    
    # Get sorted indices
    idx = sorted(range(len(M)), key=lambda x: d.get((M[x,0], 
    M[x,1]), len(k)))
    
    #return the sorted matrix
    return M[idx]

",numpy
mediapipe gives different results in two cases image file path and numpy array input,"As you may know, Mediapipe provides landmark locations based on the aligned output image rather than the input image.
Objective:
I intend to perform landmark detection on multiple images. Below, I’ve included code that uses PoseLandmarkerOptions to identify 33 body landmarks. After locating these landmarks, I plan to classify the face angle as either 0 degrees, 90 degrees, 180 degrees, or 270 degrees.
Data:
I have included sample images from the MARS dataset, as I was unable to use my original images due to issues—They have higher resolution and dimensions compared to the MARS dataset.









all images as a compressed file:
Code:
I have provided the main code to detect landmarks in the images.
import sys
import cv2
import numpy as np
import glob
import os
import base64
import mediapipe as mp
from mediapipe.tasks import python
from mediapipe.tasks.python import vision
from typing import Dict


base_options = python.BaseOptions(
    model_asset_path=""./models/pose_landmarker.task"",
    delegate=python.BaseOptions.Delegate.GPU,
)

options = vision.PoseLandmarkerOptions(
    base_options=base_options,
    output_segmentation_masks=True,
    min_pose_detection_confidence=0.5,
    min_pose_presence_confidence=0.5,
    min_tracking_confidence=0.5,
)
detector = vision.PoseLandmarker.create_from_options(options)


def check_landmarks(detection_result, img, address):
    file_name = address.split(""/"")[-1]
    w, h, _ = img.shape
    for each_person_pose in detection_result.pose_landmarks:
        for each_key_point in each_person_pose:
            if each_key_point.presence > 0.5 and each_key_point.visibility > 0.5:
                x_px = int(each_key_point.x * h)
                y_px = int(each_key_point.y * w)
                cv2.circle(img, (x_px, y_px), 3, (255, 0, 0), 2)
    cv2.imwrite(""./landmarks/"" + file_name, img)


def rectifier(detector, image, address):
    try:
        srgb_image = mp.Image.create_from_file(address)
        detection_result = detector.detect(srgb_image)
        check_landmarks(detection_result, srgb_image.numpy_view(), address)
    except Exception as e:
        print(f""error {e}"")


def rectify_image(rectify_image_request):
    image = cv2.imdecode(
        np.frombuffer(base64.b64decode(rectify_image_request[""image""]), np.byte),
        cv2.IMREAD_COLOR,
    )
    rectifier(detector, image, rectify_image_request[""address""])


def read_image_for_rectify(address: str) -> Dict:
    face_object = dict()
    img = cv2.imread(address)
    _, buffer = cv2.imencode("".jpg"", img)
    img = base64.b64encode(buffer).decode()
    face_object[""image""] = img
    face_object[""address""] = address
    return face_object


folder_path = ""./png2jpg""
file_paths = glob.glob(os.path.join(folder_path, ""*.jpg""), recursive=True)
for id_file, file in enumerate(file_paths):
    print(id_file, file)
    rectify_image(read_image_for_rectify(file))

Problem:
Initially, I used image addresses to feed images directly to Mediapipe, and the results indicated acceptable performance.









However, I now need to receive images as dictionaries with the images encoded in base64. I have modified the data input accordingly, but upon reviewing the output in this scenario, Mediapipe fails to detect landmarks in many of the images. So I feed images as numpy array into mediapipe by changing this line from
srgb_image = mp.Image.create_from_file(address)

into
srgb_image = mp.Image(image_format=mp.ImageFormat.SRGB, data=image)

output in the second scenario:









How can I achieve consistent output in both scenarios?
","Thanks to Christoph Rackwitz's suggestion, swapping the image channels in MediaPipe yields the same results as in the first case.
The rectifier function should be rewritten as follows:
def rectifier(detector, image, address):
    try:
        # srgb_image = mp.Image.create_from_file(address)
        srgb_image = mp.Image(image_format=mp.ImageFormat.SRGB, data=cv2.cvtColor(image, cv2.COLOR_BGR2RGB))
        detection_result = detector.detect(srgb_image)
        check_landmarks(detection_result, srgb_image.numpy_view(), address)
    except Exception as e:
        print(f""error {e}"")

Additionally, channel swapping should also be implemented in the check_landmarks function where the image is written:
def check_landmarks(detection_result, img, address):
    file_name = address.split(""/"")[-1]
    w, h, _ = img.shape
    for each_person_pose in detection_result.pose_landmarks:
        for each_key_point in each_person_pose:
            if each_key_point.presence > 0.5 and each_key_point.visibility > 0.5:
                x_px = int(each_key_point.x * h)
                y_px = int(each_key_point.y * w)
                cv2.circle(img, (x_px, y_px), 3, (255, 0, 0), 2)
    cv2.imwrite(""/home/nvs/landmarks/"" + file_name,    cv2.cvtColor(img, cv2.COLOR_RGB2BGR))

The following parameters have been set for Mediapipe:
min_pose_detection_confidence=0.5,
min_pose_presence_confidence=0.5,

However, it has not been able to detect landmarks for some images, such as the one shown below:

This is acceptable, as it results in a lower false positive rate in this state.
",numpy
form elementwise list from scalar and matrix,"I have a zero-dimensional numpy scalar s and a two-dimensional numpy matrix m. I want to form a matrix of vectors in which all the elements of m are paired with s as in the following example:
import numpy as np

s = np.asarray(5)

m = np.asarray([[1,2],[3,4]])

# Result should be as follows

array([[[5, 1],
        [5, 2]],

       [[5, 3],
        [5, 4]]])

In other words, I want to vectorize the operation np.asarray([s, m]) element-wise at the lowest level of m. Is there an obvious way to do that for any multidimensional array m within numpy?
I'm sure this is somewhere, but I have trouble expressing it in words and cannot find it. If you can find it, please feel free to redirect me there.
","A possible solution, which uses broadcast_to and stack functions to combine two arrays, s and m, into a single array along a new axis. The steps are:

First, np.broadcast_to(s, m.shape) expands the shape of array s to match that of array m without copying data.

Then, np.stack([np.broadcast_to(s, m.shape), m], axis=-1) joins the broadcasted s and m along a new last axis


np.stack([np.broadcast_to(s, m.shape), m], axis=-1)

Output:
array([[[5, 1],
        [5, 2]],

       [[5, 3],
        [5, 4]]])

",numpy
wave simulation with python,"I want to simulate a propagating wave with absorption and reflection on some bodies in three dimensional space. I want to do it with python. Should I use numpy? Are there some special libraries I should use?
How can I simulate the wave? Can I use the wave equation? But what if I have a reflection?
Is there a better method? Should I do it with vectors? But when the ray diverge the intensity gets lower.
","If you do any computationally intensive numerical simulation in Python, you should definitely use NumPy.
The most general algorithm to simulate an electromagnetic wave in arbitrarily-shaped materials is the finite-difference time domain method (FDTD). It solves the wave equation, one time-step at a time, on a 3-D lattice. It is quite complicated to program yourself, though, and you are probably better off using a dedicated package such as Meep.
There are books on how to write your own FDTD simulations: here's one, here's a document with some code for 1-D FDTD and explanations on more than 1 dimension, and Googling ""writing FDTD"" will find you more of the same.
You could also approach the problem by assuming all your waves are plane waves, then you could use vectors and the Fresnel equations. Or if you want to model Gaussian beams being transmitted and reflected from flat or curved surfaces, you could use the ABCD matrix formalism (also known as ray transfer matrices). This takes into account the divergence of beams.
",numpy
statistical learning confusion table variable,"I am getting an extra variable in my confusion table, not sure where it's coming from.
The Dataset 'Default' has the following columns: default, student, income, balance
The variable 'default' has two values: 'Yes' and 'No'
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import statsmodels.api as sm
from ISLP import load_data
from ISLP.models import (ModelSpec as MS,
                         summarize,
                         poly)
from ISLP import confusion_table



Default = load_data('Default')
vars = Default.columns.drop(['default'])
y = Default['default'] == 'Yes'
design = MS(vars)
X = design.fit_transform(Default)
glm = sm.GLM(y,
             X,
             family = sm.families.Binomial())
results = glm.fit()
summarize(results)
probs = results.predict()
labels = np.array(['No']*10000)
labels[probs>0.5] = 'Yes'
confusion_table(labels,Default.default)

In the output, I get a 3x3 table with the variables 'No', 'Yes' and 'Ye'
I want the confusion table values to be only 'Yes' and 'No'. Somehow, the numpy.array 'labels' is set to 'Ye' instead of 'Yes'.
","Numpy may be inferring the datatype to be 2 characters for labels = np.array(['No']*10000) since all elements of the array have two characters.
Try labels = np.array(['No']*10000, dtype='<U3')
",numpy
unexpected 39__mul__39 call during dot product,"So, I have been trying to implement a basic Autograd and Neural Network from scratch using some numpy. This is the part of code of my AD which matters for this question which has been greatly shorten down for MRE. This is grad.py
from typing import Self
import numpy as np


class Variable:
   def __init__(self, value: np.ndarray=None):

      self.value = value if isinstance(value, np.ndarray) else np.asarray(value)
      self.prev = None

   def _variablify(self, x) -> Self:
      if not isinstance(x, Variable):
         x = Variable(x)

      return x
   
   def __add__(self, x) -> Self:      
      x = self._variablify(x)
      y = Variable(self.value + x.value)
      return y
   
   def __mul__(self, x) -> Self:
      x = self._variablify(x)
      y = Variable(self.value * x.value)
      return y
   
   __radd__ = __add__
   __rmul__ = __mul__

   def dot(self, x):
      x = self._variablify(x)
      y = Variable(self.value.dot(x.value))
      return y
   
   def __lt__(self, other):
      return self.value < other
   
   def __gt__(self, other):
      return self.value > other


def dot(a: Variable, b: Variable):
   return a.dot(b)

In the other file, main.py I try to implement a neural net
from typing import Self
import numpy as np

from grad import Variable
import grad

class Layer:
   def __init__(self, neurons: int):
      self.n_size = neurons
      self.activation = Variable(0)
   
   def previous(self, layer: Self):
      self.previous_layer = layer
      self.previous_layer.next_layer = self

   def next(self, layer: Self):
      self.next_layer = layer
      self.next_layer.previous_layer = self

   def initialise(self):
      self.weight_matrix = Variable(np.random.normal(0, 0.01, (self.n_size, self.next_layer.n_size)))
      self.bias_vector = Variable(np.random.normal(0, 0.01, (1, self.next_layer.n_size)))

      self.next_layer.x = grad.dot(self.activation, self.weight_matrix) + self.bias_vector
      self.next_layer.activation = np.where(self.next_layer.x > 0, self.next_layer.x, 0.01*self.next_layer.x)  #  Using LeakyReLU


if __name__ == ""__main__"":
   input_layer = Layer(5)
   input_layer.activation = Variable(np.random.randint(1, 5, (1,5)))

   h1 = Layer(3)
   h1.previous(input_layer)

   output = Layer(2)
   output.previous(h1)

   input_layer.initialise()
   h1.initialise()

   print(input_layer.activation, h1.activation, output.activation)

So, as you can see in the grad.py I had implemented the code for dot product wrapper. But now, here comes the error upon running the main.py file-
Traceback (most recent call last):
  File "".../main.py"", line 62, in <module>
    h1.initialise()
  File "".../main.py"", line 40, in initialise
    self.next_layer.x = grad.dot(self.activation, self.weight_matrix) + self.bias_vector
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "".../grad.py"", line 191, in dot
    return a.dot(b)
           ^^^^^^^^
  File "".../grad.py"", line 49, in __mul__
    y = Variable(self.value * x.value)
                 ~~~~~~~~~~~^~~~~~~~~
ValueError: operands could not be broadcast together with shapes (1,3) (3,2) 

Now to me, this is very strange. Because the error seems to tell us that a.dot(b) somehow called __mul__ which...it never did. I have absolutely no idea what is going on here. Any help would be greatly appreciated.
Thanks.
","I see you are using type annotations. But apparently you aren't using a tool to enforce them like mypy. That's why they give you a sense of false security. It is true that def dot() is annotated to accept two Variables but you are not passing in two Variables. One of the parameters is a numpy array.
A call to mypy shows that this is the culprit:
self.next_layer.activation = np.where(self.next_layer.x > 0, self.next_layer.x, 0.01*self.next_layer.x)

You assign a numpy array to activation, but it is supposed and assumed to be a Variable
This explains the weird traceback. a.dot(b) is not calling your implementation, but numpy's instead. And numpy uses __mul__ on the array elements.
So you need to figure out how to fix the above line. And you really need to run mypy or something similar if you are using type hints.
",numpy
with python how to apply vector operations to a neighborhood in an nd image,"I have a 3D image with vector components (i.e., a mapping from R3 to R3). My goal is to replace each vector with the vector of maximum norm within its 3x3x3 neighborhood.
This task is proving to be unexpectedly challenging. I attempted to use scipy.ndimage.generic_filter, but despite its name, this filter only handles scalar inputs and outputs. I also briefly explored skimage and numpy's sliding_window_view, but neither seemed to provide a straightforward solution.
What would be the correct way to implement this?
Here's what I ended up writing. It's not very elegant and pretty slow, but should help understand what I'm trying to do.
import numpy as np
import matplotlib.pyplot as plt

def max_norm_vector(data):
    """"""Return the vector with the maximum norm.""""""
    data = data.reshape(-1, 3)
    norms = np.linalg.norm(data, axis=-1)
    idx_max = np.argmax(norms)
    return data[idx_max]

if __name__ == '__main__':
    # Load the image
    range_ = np.linspace(-5, 5, 30)
    x, y, z = np.meshgrid(range_, range_, range_, indexing='ij')
    data = 1 - (x ** 2)

    # Compute the gradient
    grad = np.gradient(data)
    grad = np.stack(grad, axis=-1)  # Stack gradients along a new last axis


    # grad = grad[:5, :5, :5, :]  # Crop the gradient for testing
    max_grad = np.zeros_like(grad)
    for i in range(1,grad.shape[0]-1):
        for j in range(1,grad.shape[1]-1):
            for k in range(2,grad.shape[2]-1):
                max_grad[i, j, k] = max_norm_vector(grad[i-1:i+2, j-1:j+2, k-1:k+2,:])

    # Visualization
    fig = plt.figure(figsize=(12, 6))

    # Plot original data
    ax1 = fig.add_subplot(121, projection='3d')
    ax1.scatter(x.ravel(), y.ravel(), z.ravel(), c=data.ravel(), cmap='viridis', alpha=0.5)
    ax1.set_title('Original Data')

    # Plot maximum gradient vectors
    ax2 = fig.add_subplot(122, projection='3d')

    # Downsample for better performance
    step = 3
    x_down = x[::step, ::step, ::step]
    y_down = y[::step, ::step, ::step]
    z_down = z[::step, ::step, ::step]
    max_grad_down = max_grad[::step, ::step, ::step]

    ax2.quiver(x_down.ravel(), y_down.ravel(), z_down.ravel(),
               max_grad_down[:, :, :, 0].ravel(), max_grad_down[:, :, :, 1].ravel(), max_grad_down[:, :, :, 2].ravel(),
               length=0.1, color='red', alpha=0.7)
    ax2.set_title('Maximum Gradient Vectors')

    plt.tight_layout()
    plt.show()

","DIPlib has this function implemented: dip.SelectionFilter().
This is how you'd use it:
grad = ...  # OP's grad array
norm = dip.Norm(grad)
out = dip.SelectionFilter(grad, norm, dip.Kernel(3, ""rectangular""), mode=""maximum"")

You can cast the dip.Image object out to a NumPy array with np.asarray(out) (no copy of the data will be made). NumPy functions will accept the dip.Image object as input, but many functions in scikit-image expect the input array to have a .shape method or similar, which will fail if you don't do the cast explicitly.
Install the package with pip install diplib.
Disclaimer: I'm an author of DIPlib, but I didn't implement this function.
",numpy
understanding unusual yolo label formats and their impact on training,"I’m working on a dataset of stationary objects, where the data is divided into train, test, and validation folders with corresponding images and labels. The labels are in text files with the following format:
2 0.3832013609375 0 0 0.19411217812499998 0 0.614612228125 0.1995640296875 1 0.619265075 1 1 0.8055533171875 1 0.386728209375 0.798922646875 0 0.3832013609375 0

I’m confused because I expected each bounding box to have just 5 numbers:
class_id, x_center, y_center, width, height.

But here, I see significantly more numbers. Could it be that this format represents something else? Are there additional possibilities for YOLO label formats that I’m unaware of?
Additional Context
The data was sourced from this website, but I couldn’t find clear documentation about this label format.
Here’s the part I don’t understand: when I pass this dataset to YOLO for training using the following code, the training process works without any issues:
def train_yolo(weight_name):
    weight_path = os.path.join(weights_folder, weight_name)

    model = YOLO(weight_path)

    # Train model and save new weights
    results = model.train(data=data_yaml, epochs=100, imgsz=640, batch=16, name=f""yolo_{weight_name.split('.')[0]}"", save=True)

    return results

My data.yaml file contains:
train: ../train/images
val: ../valid/images
test: ../test/images

nc: 4
names: ['pencil', 'rubber', 'ruler', 'sharpner']

roboflow:
  workspace: waqas-hussain
  project: stationary-object-detector
  version: 8
  license: CC BY 4.0
  url: https://universe.roboflow.com/waqas-hussain/stationary-object-detector/dataset/8

There’s no direct reference to bounding box formats in this YAML file, yet YOLO processes the data correctly during training.
Questions:

How does YOLO handle these unusual label formats?
Could it be that
my training was incorrect due to this strange bounding box format?
Is there a way to confirm what this format represents and how it’s
parsed by YOLO?

Any insights or pointers would be greatly appreciated!
","From the picture on the website, I see that some of the annotations are not bounding boxes. They are polygons. A common way to encode a polygon is as a list of x/y pairs.
So I would guess that the format is
class_id x1 y1 x2 y2 x3 y3

etc.
To check this, I downloaded one of the pictures and its associated label. (Specifically, CamScanner-10-15-2023-14-29_86_jpg.rf.1042acb34a88542b82bbefa27b86569e.jpg I wrote a program which parsed this label and plotted it.
Code:
import numpy as np
import matplotlib.pyplot as plt


label_text = """"""1 0.3855721390625 0.17391304375 0.26533996718749997 0.1273291921875 0.10779436093749999 0.273291925 0.25290215625 0.3354037265625 0.3855721390625 0.17391304375
0 0.9618573796875 0.381987578125 0.8872305140625001 0.3540372671875 0.327529021875 0.9782608703125 0.45190713125000004 1 0.9618573796875 0.381987578125
2 0.970149253125 0.034161490625 0.8084577109375 0 0.0165837484375 0.9254658390625 0.0414593703125 0.9937888203125 0.178275290625 1 0.970149253125 0.034161490625""""""


lines = label_text.split('\n')
for line in lines:
    line = line.split(' ')
    class_id = line[0]
    label_without_id = np.array([float(s) for s in line[1:]])
    label_x = label_without_id[::2]
    label_y = label_without_id[1::2]
    plt.plot(label_x, label_y, label=class_id)
    # The convention when working with image coordinates is that Y-axis gets bigger as you move down the image
    plt.gca().invert_yaxis()
plt.legend()
plt.show()

Output:

That looks reasonably plausible, given the input. The aspect ratio is wrong, but they're likely expecting you to rescale the x/y coordinates by the image width/height. You can also compare this to the image labels on roboflow.
",numpy
numpydistutils was deprecated how to install libtiff,"I have a series of tiff images and need to read them using the following command:
from libtiff import TIFF
tif = TIFF.open(tiff_image, mode = ""r"")

I cannot use Image.open as I'd like to get object attribute in the next step (tif.iter_images).
Given this context, I do need to install libtiff but it fails due to dependency with numpy.distutils which has been deprecated. My Python version is 3.12.7.
Any suggestions?
Thank you!
","numpy.distutils is removed in python 3.12. Here is the migration advice:
https://numpy.org/doc/stable/reference/distutils_status_migration.html#distutils-status-migration

To install libtiff you can use python 3.11 or 3.10
libtiff's latest version was Released: Dec 21, 2017
https://pypi.org/project/libtiff/#history

Alternatively, you can use tifffile as @Mark commented
tifffile is supported for python 3.10, 3.11, 3.12, 3.13.
https://pypi.org/project/tifffile/
",numpy
error failed building wheel for numpy  error could not build wheels for numpy which is required to install pyprojecttomlbased projects,"I`m using python poetry(https://python-poetry.org/) for dependency management in my project.
Though when I`m running poetry install, its giving me below error.
ERROR: Failed building wheel for numpy
  Failed to build numpy
  ERROR: Could not build wheels for numpy, which is required to install pyproject.toml-based projects


I`m having python 3.9 installed in my laptop.
I installed numpy 1.21.5 using pip install numpy, I even tried to down version it to 1.19.5.
Though I`m getting the same error.
I found out many people are getting ERROR: Failed building wheel for numpy this error in python 3.10, they solved it by down versioning python to 3.9, though that didnt working for me.
","I solved it by doing the following steps:-

I updated the pyproject.toml(This file contains all the library/dependency/dev dependency)with the numpy version that I installed using pip install numpy command.


Run poetry lock to update poetry.lock file(contains details information about the library)


Run poetry install again, & it should work fine.



In a nutshell, you just have to install the correct version of numpy Click me to check the compatibility
And then install the required version using pip install numpy==version.
Example:
To install NumPy version 1.23.5, use the following-
pip install numpy==1.23.5

If you are having any problems, you can comment.
I`ll try to answer it.
",numpy
fastest way to get hamming distance for integer array,"Let a and b be vectors of the same size with 8-bit integers (0-255). I want to compute the number of bits where those vectors differs i.e. a Hamming distance between vectors formed by concatenation of binary representations of those numbers. For example:
a = [127,255]
b= [127,240]

Using numpy library
np.bitwise_xor(a,b)
# Output: array([ 0, 15])

What I need is now to binary represent each element of the above array and count number of 1s in all the elements of the array. The above example will give hamming distance of 0+4 = 4. Any fast and elegant solution for this in Python?
","Approach #1 : We could broadcast them into binary bits & count number of different bits, like so -
def hamming_distance(a, b):
    r = (1 << np.arange(8))[:,None]
    return np.count_nonzero( (a & r) != (b & r) )

Sample run -
In [144]: a = [127,255]
     ...: b = [127,240]
     ...: 

In [145]: hamming_distance(a, b)
Out[145]: 4

Approach #2 : Using bitwise-xor operation, we can find out the number of different binary bits between a and b -
def hamming_distance_v2(a, b):
    r = (1 << np.arange(8))[:,None]
    return np.count_nonzero((np.bitwise_xor(a,b) & r) != 0)

",numpy
how to fix attributeerror 39series39 object has no attribute 39append39,"Hi I'm still learning pandas and numpy in python
I learn from e-learning that you could combine 2 series with append but when I tried it It give me error
students_classes = pd.Series({'Alice': 'Physics',
                   'Jack': 'Chemistry',
                   'Molly': 'English',
                   'Sam': 'History'})
students_classes

kelly_classes = pd.Series(['Philosophy', 'Arts', 'Math'], index=['Kelly', 'Kelly', 'Kelly'])
kelly_classes

all_students_classes = students_classes.append(kelly_classes)

all_students_classes

and It give me error like this
AttributeError                            Traceback (most recent call last)
Cell In\[35\], line 3
1 # Finally, we can append all of the data in this new Series to the first using the .append()
2 # function.
\----\> 3 all_students_classes.str = students_classes.append(kelly_classes)
5 # This creates a series which has our original people in it as well as all of Kelly's courses
6 all_students_classes

File \~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\pandas\\core\\generic.py:5989, in NDFrame.__getattr__(self, name)
5982 if (
5983     name not in self.\_internal_names_set
5984     and name not in self.\_metadata
5985     and name not in self.\_accessors
5986     and self.\_info_axis.\_can_hold_identifiers_and_holds_name(name)
5987 ):
5988     return self\[name\]
\-\> 5989 return object.__getattribute__(self, name)

AttributeError: 'Series' object has no attribute 'append'

I expect something like this from the e-learning
","In relatively recent pandas version, 1.5.2, append works, but gives a warning.
In pd 2.0, append has been removed
https://pandas.pydata.org/docs/dev/whatsnew/v2.0.0.html#deprecations
In [14]: students_classes = pd.Series({'Alice': 'Physics',
    ...:                    'Jack': 'Chemistry',
    ...:                    'Molly': 'English',
    ...:                    'Sam': 'History'})
    ...: kelly_classes = pd.Series(['Philosophy', 'Arts', 'Math'], index=['Kelly', 'Kelly', 'Kelly'])

In [15]: students_classes.append(kelly_classes)
C:\Users\paul\AppData\Local\Temp\ipykernel_6072\990183765.py:1: FutureWarning: The series.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.
  students_classes.append(kelly_classes)
Out[15]: 
Alice       Physics
Jack      Chemistry
Molly       English
Sam         History
Kelly    Philosophy
Kelly          Arts
Kelly          Math
dtype: object

Under the covers, append method uses _append, which works without raising the warning:
In [16]: students_classes._append(kelly_classes)
Out[16]: 
Alice       Physics
Jack      Chemistry
Molly       English
Sam         History
Kelly    Philosophy
Kelly          Arts
Kelly          Math
dtype: object

And using the recommended concat:
In [18]: pd.concat([students_classes,kelly_classes])
Out[18]: 
Alice       Physics
Jack      Chemistry
Molly       English
Sam         History
Kelly    Philosophy
Kelly          Arts
Kelly          Math
dtype: object

Python lists have an efficient append method.  numpy has a np.append function which is a poorly named cover for calling np.concatenate, and is often misused (it shouldn't be used iteratively).  pandas may be trying to avoid similar problems by getting rid of the append method.  With pd.concat you can join many Series (or frames) at once, and aren't (as) tempted to use it in a loop.
Looking up the code for _append (which is still in 2.0), I see it ends up using pd.concat.  So there's no value in using this 'work-around'.  Use concat as recommended.
",numpy
pytorch tensor to numpy array,"I have a pytorch Tensor of shape [4, 3, 966, 1296]. I want to convert it to numpy array using the following code:
imgs = imgs.numpy()[:, ::-1, :, :]

How does that code work?
","There are 4 dimensions of the tensor you want to convert.
[:, ::-1, :, :] 

: means that the first dimension should be copied as it is and converted, same goes for the third and fourth dimension.
::-1 means that for the second axes it reverses the the axes
",numpy
replace nan with the average of the last 5 values  pandas,"I want to know how can I replace the NaN in my dataset with the last average of 5 last values.




Column A
Column B




1
2


2
5


3
5


4
2


5
2


NaN
2


NaN
2


1
2


1
2


1
2


1
NaN


1
2


1
2




For example, in this case the first NaN will be the average of (1,2,3,4,5) and second NaN will be the average of (2,3,4,5, The value of the other NaN).
I have tried
df.fillna(df.mean())

","As mentioned, it has been answered here, but the updated version for the latest pandas version is as follow:

data = {
    'col1': [1, 2, 3, 4, 5, np.nan, np.nan, 1, 1, 1, 1, 1, 1],
    'col2': [2, 5, 5, 2, 2, 2, 2, 2, 2, 2, np.nan, 2, 2]
}
df = pd.DataFrame(data)

window_size = 5
df = df.fillna(df.rolling(window_size + 1, min_periods = 1).mean())

outputs:
    col1  col2
0    1.0   2.0
1    2.0   5.0
2    3.0   5.0
3    4.0   2.0
4    5.0   2.0
5    3.0   2.0
6    3.5   2.0
7    1.0   2.0
8    1.0   2.0
9    1.0   2.0
10   1.0   2.0
11   1.0   2.0
12   1.0   2.0

",numpy
numpy adding two vectors with different sizes,"If I have two numpy arrays of different sizes, how can I superimpose them.
a = numpy([0, 10, 20, 30])
b = numpy([20, 30, 40, 50, 60, 70])

What is the cleanest way to add these two vectors to produce a new vector (20, 40, 60, 80, 60, 70)?
This is my generic question. For background, I am specifically applying a Green's transform function and need to superimpose the results for each time step in the evaulation unto the responses previously accumulated.
","This could be what you are looking for
if len(a) < len(b):
    c = b.copy()
    c[:len(a)] += a
else:
    c = a.copy()
    c[:len(b)] += b

basically you copy the longer one and then add in-place the shorter one
",numpy
why is tznaive timestamp converted to integer while tzaware is kept as timestamp,"Understandable and expected (tz-aware):
import datetime
import numpy as np
import pandas as pd

aware = pd.DatetimeIndex([""2024-11-21"", ""2024-11-21 12:00""], tz=""UTC"")
eod = datetime.datetime.combine(aware[-1].date(), datetime.time.max, aware.tz)
aware, eod, np.concat([aware, [eod]])

returns
(DatetimeIndex(['2024-11-21 00:00:00+00:00', '2024-11-21 12:00:00+00:00'],
               dtype='datetime64[ns, UTC]', freq=None),
 datetime.datetime(2024, 11, 21, 23, 59, 59, 999999,
                   tzinfo=datetime.timezone.utc),
 array([Timestamp('2024-11-21 00:00:00+0000', tz='UTC'),
        Timestamp('2024-11-21 12:00:00+0000', tz='UTC'),
        datetime.datetime(2024, 11, 21, 23, 59, 59, 999999,
                          tzinfo=datetime.timezone.utc)],
       dtype=object))

note Timestamps (and a datetime) in the return value of np.concat.
Unexpected (tz-naive):
naive = pd.DatetimeIndex([""2024-11-21"", ""2024-11-21 12:00""])
eod = datetime.datetime.combine(naive[-1].date(), datetime.time.max, aware.tz)
naive, eod, np.concat([naive, [eod]])

returns
(DatetimeIndex(['2024-11-21 00:00:00', '2024-11-21 12:00:00'],
               dtype='datetime64[ns]', freq=None),
 datetime.datetime(2024, 11, 21, 23, 59, 59, 999999),
 array([1732147200000000000, 1732190400000000000,
        datetime.datetime(2024, 11, 21, 23, 59, 59, 999999)], dtype=object))

note intergers (and a datetime) in the return value of np.concat.

why do I get integers in the concatenated array for a tz-naive index?
how do I avoid it? I.e., how do I append EOD to a tz-naive DatetimeIndex?

PS. Interestingly enough, at the numpy level the indexes are identical:
np.testing.assert_array_equal(aware.values, naive.values)

","From Data type promotion in NumPy

When mixing two different data types, NumPy has to determine the appropriate dtype for the result of the operation. This step is referred to as promotion or finding the common dtype.
In typical cases, the user does not need to worry about the details of promotion, since the promotion step usually ensures that the result will either match or exceed the precision of the input.

np.concat() accepts a casting keyword argument (casting=""same_kind"" default).
If using casting='no' fails
naive_no = np.concat([naive, [eod]], casting='no')

TypeError: Cannot cast array data from dtype('<M8[ns]') to dtype('O') according to the rule 'no'

See Array-protocol type strings.
In both cases the type is object
naive_sk = np.concat([naive, [eod]], casting='same_kind')
print(naive_sk.dtype, naive_sk)

Result
object [1732147200000000000 1732190400000000000
 datetime.datetime(2024, 11, 21, 23, 59, 59, 999999, tzinfo=<DstTzInfo 'America/New_York' LMT-1 day, 19:04:00 STD>)]

python 3.9
pandas 2.2.2
",numpy
julia mapslices function equivalent in python,"I'm trying to find if there's a convenient equivalent to mapslices function from the Julia language to Python. Here's the line I'm trying to convert:
line_center = float64(mapslices(x->sum(x.^4.*[1:length(x)])./sum(x.^4),no_background,1)[:])

","For anyone wondering, here's an answer.
def exp_func(x):
    return np.dot(np.arange(len(x)), np.power(x, 4))/(np.sum(np.power(x, 4)))

result = np.apply_along_axis(exp_func,axis = 0,array)

exp_func takes 1-D arrays.
",numpy
what makes printnphalf5002 differs from printfquotnphalf5002quot,"everyone. I've been learning floating-point truncation errors recently. But I found  print(np.half(500.2)) and print(f""{np.half(500.2)}"") yield different results. Here are the logs I got in IPython.
In [11]: np.half(500.2)
Out[11]: np.float16(500.2)

In [12]: print(np.half(500.2))
500.2

In [13]: print(f""{np.half(500.2)}"")
500.25


I use half.hpp in c++ to compare results with numpy. It seems that 500.2 should be truncated into 500.25 instead of itself.
In binary formats, 500.0 is 0b0_01000_1111010000. So the next float16 number should be 0b_01000_1111010001, which is 500.25 in deximal format.

So what makes print(np.half(500.2)) differs from print(f""{np.half(500.2)}"")? Hope to see your answers.
","print calls __str__, while an f-string calls __format__. __format__ with an empty format spec is usually equivalent to __str__, but not all types implement it that way, and numpy.half is one of the types that implements different behavior:
In [1]: import numpy

In [2]: x = numpy.half(500.2)

In [3]: str(x)
Out[3]: '500.2'

In [4]: format(x, '')
Out[4]: '500.25'

",numpy
get a single series of classes instead of one series for each class with pandas in python,"I have a DataFrame with 3 column of zeroes and ones corresponding to 3 different classes. I want to get a single series of zeroes, ones, and twos depending of the class of the entry (0 for the first class, 1 for the second one and 2 for the third one):
>>> results.head()
    HOME_WINS  DRAW  AWAY_WINS
ID                            
0           0     0          1
1           0     1          0
2           0     0          1
3           1     0          0
4           0     1          0

What I want :
>>> results.head()
    SCORE
ID                            
0       2
1       1
2       2
3       0
4       1

","Multiply by a dictionary, sum and convert to_frame:
d = {'HOME_WINS': 0, 'DRAW': 1, 'AWAY_WINS': 2}

out = df.mul(d).sum(axis=1).to_frame(name='SCORE')

Or using a dot product:
d = {'HOME_WINS': 0, 'DRAW': 1, 'AWAY_WINS': 2}

out = df.dot(pd.Series(d)).to_frame(name='SCORE')

Or, if there is exactly one 1 per row, with from_dummies:
d = {'HOME_WINS': 0, 'DRAW': 1, 'AWAY_WINS': 2}

out = pd.from_dummies(df)[''].map(d).to_frame(name='SCORE')

Output:
    SCORE
ID       
0       2
1       1
2       2
3       0
4       1

",numpy
how do ndarrayflags39owndata39 ndarraybase idndarray and ndarray__array_interface__39data39 differ,"I seem to have had an XY problem in this question regarding how to tell if arrays share the same memory.  The ways I was checking were wrong and I'm not sure why.
Let's take a few examples
test = np.ones((3,3))
test2 = np.array(test, ndmin = 2, copy = False)
test3 = np.array(test, ndmin = 3, copy = False)

First, let's check if they're sharing memory using .base
test2.base is test
False

test3.base is test
True

So it seems like test3 is sharing data with test but test2 isn't.  In fact test2.base is None => True, which I thought meant that it is seperate memory (i.e. a copy).
This impression is reinforced when I check with .flags
test2.flags['OWNDATA']
True

test3.flags['OWNDATA']
False

It seems again like only test3 is sharing data, and test2 is a copy.
But if I check with the python builtin id(...)
id(test)
248896912

id(test2)
248896912

id(test3)
248897352

Now the id (which is supposedly the adress of the object in memory) of test and test2 match but test3 does not, which gives the exact opposite impression from the above methods.
And of course, both of those impressions are wrong because:
test.__array_interface__['data']
(209580688, False)

test2.__array_interface__['data']
(209580688, False)

test3.__array_interface__['data']
(209580688, False)

The actual buffer addresses all match.  Indeed:
test[0,0] = 2

test, test2, test3

(array([[ 2.,  1.,  1.],
        [ 1.,  1.,  1.],
        [ 1.,  1.,  1.]]),  
array([[ 2.,  1.,  1.],
        [ 1.,  1.,  1.],
        [ 1.,  1.,  1.]]), 
array([[[ 2.,  1.,  1.],
         [ 1.,  1.,  1.],
         [ 1.,  1.,  1.]]]))

So if ndarray.base, ndarray.flags['OWNDATA'], and id(ndarray) don't actually tell me if memory is shared, what are they telling me?
","Most of your confusion comes from the fact that test2 is test and test3 is a view of test.
ndarray.base
If an array is created as a view of another object, its base will point to the other object... or NumPy will follow that object's base chain and use the ""root base"". Sometimes one option, sometimes the other. There were a number of bugs and backward compatibility issues.
test2 is not a view of test. test2 is test. Its base is None because it's not a view.
test3 is a view of test, and its base is set to test.
ndarray.flags['OWNDATA']
This flag is set if the array is not a view. As previously stated, test3 is a view and test2 isn't, because test2 is test.
id
id gives a numeric identifier for an object, guaranteed not to be the same as the identifier for any object whose lifetime overlaps with the first. This function does not care about NumPy and views; two different arrays with overlapping lifetimes will have different IDs, regardless of any view relationships.
test2 is test, so of course it has the same ID as itself. test3 is a different object with an overlapping lifetime, so it has a different ID.
ndarray.__array_interface__['data']
This is a tuple whose first element is an integer representing the memory address of the array's first element. (The second element is unimportant for the question.) test, test2, and test3 all use the same data buffer, with their first elements at the same offset, so they give the same value here. However, two arrays that share memory won't always have the same value for this, because they might not have the same first element:
>>> import numpy
>>> x = numpy.arange(5)
>>> y = x[1:]
>>> z = x[:-1]
>>> x.__array_interface__['data']
(39692208L, False)
>>> y.__array_interface__['data']
(39692212L, False)
>>> z.__array_interface__['data']
(39692208L, False)

",numpy
randomly flip exactly n elements in multiple 2d matrices,"I need to flip exactly randomly n (n>2) chosen elements (uniformly distributed) in total in m 2d numpy matrices with varying sizes (see initialization of my code), which I have stored in a dictionary, every time in my simulation (approx. 10^(7-8) times in total).
Edit: Example: Let's say I have m=3 arrays (different sizes). What I desire is to flip n numbers inside of those arrays in total randomly. E.g. n=10: 6 elements flipped in the first, 3 in the second and 1 in the last array (all elements of any array must have the same probability to get flipped). Not n per array, not less, nor more per run. Over the whole simulation the number n must be uniformly distributed.

The following code works in itself and the loop over N here is for testing the speed. I would be delighted if someone could show me a way to do this faster. <3
Further notes: I also tried to convert the indices of my temporary 1d matrix first into 2d ones (numpy.unravel_index) and then just change my entries, but that's even slower in high dimensions.
# Initialization
import numpy as np
import time
rng = np.random.default_rng(seed=1)
sizes = [700, 27, 48, 20]# code must work for any numbers, usually in the range of~1-1000
num_layers = len(sizes)
params = np.sum([sizes[i]*sizes[i+1] for i in range(num_layers-1)])
w = dict((key, rng.choice([-1, 1], size=(y, x))) for key, (x, y) in enumerate(zip(sizes[:-1], sizes[1:])))# e.g. 27x700, 48x27, 20x48


# The problematic part...
N = 1001
start = time.time()
for i in range(N):# just for testing the speed a little
    # 1) Generate n random numbers idx (1d) (replace is a must, since we need exactly n), then store the old matrices away for later usage.
    n = rng.integers(2, high=params+1)
    idx = rng.choice(np.arange(params), size=(n,), replace=False)
    w_old = dict((key, np.copy(w)) for key, w in sorted(w.items()))
    # 2) Initialize matrices with ones.
    w_flip = dict((key, np.ones((sizes[i]*sizes[i+1]), dtype=np.int32)) for key, i in enumerate(range(num_layers-1)))
    # 3) Flip the spins of this temporary matrix and reshape it afterwards into a 2d one
    left = 0
    for i in range(num_layers-1):
        right = left + sizes[i]*sizes[i+1] 
        w_flip[i][idx[np.where(np.logical_and(idx>=left, idx<right))]-left] *= -1
        w_flip[i] = np.reshape(w_flip[i], (sizes[i+1], sizes[i]))
        left = right
    # 4) Flip the n entries of the m matrices
    for (_,w_i), (__,w_flip_i) in zip(w.items(), w_flip.items()):
        w_i *= w_flip_i
    # ... here I do other stuff with the changed matrices, but that's irrelevant to the topic at hand.
end = time.time()
print(end-start)


# Test if algorithm works (just for me)
for (_,w_i), (__,w_flip_i) in zip(w.items(), w_flip.items()):
    w_i *= w_flip_i
diff = 0
for i in range(len(w)):
    diff = np.sum(w[i] != w_old[i])
if diff != 0:
    print(""Something wrong with the algorithm, diff = {0} != 0."".format(diff))

","Update: I could speed up the algorithm by around 20% so far for high dimensions, but that's it. With inspiration from James to flatten my array instead I came up with the following code.
I will leave it at that, but if someone knows any points to improve (regarding speed), I would love to hear them. <3
rng = np.random.default_rng(seed=1)
sizes = [700, 27, 48, 10]# code must work for any numbers, usually in the range of~1-1000
num_layers = len(sizes)
params_w = [sizes[i]*sizes[i+1] for i in range(num_layers-1)]
params = np.sum(params_w)
params_arr = np.arange(params)
params_split = np.cumsum(np.array([params_w[i] for i in range(num_layers-2)]))
w = dict((key, rng.choice([-1, 1], size=(y, x))) for key, (x, y) in enumerate(zip(sizes[:-1], sizes[1:])))# e.g. 27x700, 48x27, 20x48

# The problematic part...
N = 1001
start = time.time()
for j in range(N):# just for testing the speed a little
    # 1) Generate n random numbers idx (1d) (replace is a must, since we need exactly n), then store the old matrices away for later usage.
    n = rng.integers(2, high=params+1)
    idx = rng.choice(params_arr, size=(n,), replace=False)
    w_old = dict((key, np.copy(w)) for key, w in sorted(w.items()))
    # 2) Initialize flip matrix (1d) and new dictionary
    w_flip = np.ones(params, dtype=np.int32)
    w_flip[idx] = -1    
    w_new = dict((key, None) for key, i in enumerate(range(num_layers-1)))
    # 3) Flatten the old matrices from 2d to 1d and concatenate them together
    for i, arr in enumerate(w.values()):
        w[i] = np.reshape(arr, (params_w[i]))
    w_new = np.concatenate(list(w.values()))
    # 4) Flip the n entries of the m matrices 
    w_new *= w_flip
    # 5) Split them apart again and store in dictionary
    temp = np.split(w_new, params_split)
    for key in w.keys(): 
        w[key] = np.reshape(temp[key], (sizes[key+1],sizes[key]))
    # ... here I do other stuff with the changed matrices, but that's irrelevant to the topic at hand.
end = time.time()
print(end-start)

# Just to check if n entries have been flipped (and one can easily verify that they have been uniformly flipped too).
n_done = 0
for i, (arr, arr_old) in enumerate(zip(w.values(), w_old.values())):
    temp = np.reshape(arr, (sizes[i+1]*sizes[i]))
    temp_old = np.reshape(arr_old, (sizes[i+1]*sizes[i]))
    n_done += np.sum(temp != temp_old)
print(""Performed flips minus amount of flips we wanted:"", n_done - n)

",numpy
how to average in a specific dimension with numpymean,"I have a matrix called POS which has form (10,132) and I need to average those first 10 elements in such a way that my averaged matrix has the form of (1,132)
I have tried doing 
means = pos.mean (axis = 1)
or
menas = np.mean(pos)
but the result in the first case is a matrix of (10,) and in the second it is a simple number
i expect the ouput a matrix of shape (1,132)
","The solution is to specify the correct axis and use keepdims=True which is noted by several commenters.
This can be done with either pos.mean(axis = 0,keepdims=True) or np.mean(pos,axis=0,keepdims=True)
",numpy
how can i partially fill a numpy array with a value given a range,"Given a numpy array, a range, and a value, how can I fill the array with the value inside the range?
Is there a way that is faster than filling it in manually with a loop one by one?
Edit:
myArray = np.zeros(10)
What I want:
[0, 0, 0, 0, 1, 1, 1, 0, 0, 0]
","arr = np.zeros(10)
arr[4:7] = 1
print(arr)

Output:
array([0., 0., 0., 0., 1., 1., 1., 0., 0., 0.])

",numpy
mark true from conditions satisfy on two consecutive values till another two consecutive values,"I have a float column in a dataframe. And I want to add another boolean column which will be True if condition satisfies on two consecutive values till another condition satisfies on next two consecutive values.
For Example I have a data-frame which look like this:




index
Values %




0
0


1
5


2
11


3
9


4
14


5
18


6
30


7
54


8
73


9
100


10
100


11
100


12
100


13
100




Now I want to mark True from where two consecutive values satisfies the condition df['Values %'] >= 10 till next two consecutive values satisfies the next condition i.e. df[Values %] == 100.
So the final result will look like something this:




index
Values %
Flag




0
0
False


1
5
False


2
11
False


3
9
False


4
14
False


5
18
True


6
30
True


7
54
True


8
73
True


9
100
True


10
100
True


11
100
False


12
100
False


13
100
False



","You can use:
# give 2 consecutive values ≥ 10
m1 = df['Values %'].ge(10).rolling(2).min().eq(1)
# give 2 consecutive 100
m2 = df['Values %'].eq(100).rolling(2).min().eq(1)
# get values before (including) first m2 True
m3 = (~m2).shift(fill_value=True).cummin()

# get values between the first m1 True 
df['Flag'] = m1.cummax() & m3

Output:

    index  Values %   Flag
0       0         0  False
1       1         5  False
2       2        11  False
3       3         9  False
4       4        14  False
5       5        18   True
6       6        30   True
7       7        54   True
8       8        73   True
9       9       100   True
10     10       100   True
11     11       100  False
12     12       100  False
13     13       100  False

If you need the logic repeated on several groups:
# give 2 consecutive values ≥ 10
m1 = df['Values %'].ge(10).rolling(2).min().eq(1)
# give 2 consecutive 100
m2 = df['Values %'].eq(100).rolling(2).min().eq(1)

# set up grouper
group = (m2.shift()&~m2).cumsum()

# get values before (including) first m2 True
m3 = ((~m2).groupby(group, group_keys=False)
           .apply(lambda x: x.shift(fill_value=True).cummin())
     )

# get values between the first m1 True 
df['Flag'] = m1.groupby(group).cummax() & m3

Output:
    index   Values %   Flag
0        0         0  False
1        1         5  False
2        2        11  False
3        3         9  False
4        4        14  False
5        5        18   True
6        6        30   True
7        7        54   True
8        8        73   True
9        9       100   True
10      10       100   True
11      11       100  False
12      12       100  False
13      13       100  False
14       0         0  False
15       1         5  False
16       2        11  False
17       3         9  False
18       4        14  False
19       5        18   True
20       6        30   True
21       7        54   True
22       8        73   True
23       9       100   True
24      10       100   True
25      11       100  False
26      12       100  False
27      13       100  False

Intermediates:
    index   Values %   Flag     m1     m2  group     m3
0        0         0  False  False  False      0   True
1        1         5  False  False  False      0   True
2        2        11  False  False  False      0   True
3        3         9  False  False  False      0   True
4        4        14  False  False  False      0   True
5        5        18   True   True  False      0   True
6        6        30   True   True  False      0   True
7        7        54   True   True  False      0   True
8        8        73   True   True  False      0   True
9        9       100   True   True  False      0   True
10      10       100   True   True   True      0   True
11      11       100  False   True   True      0  False
12      12       100  False   True   True      0  False
13      13       100  False   True   True      0  False
14       0         0  False  False  False      1   True
15       1         5  False  False  False      1   True
16       2        11  False  False  False      1   True
17       3         9  False  False  False      1   True
18       4        14  False  False  False      1   True
19       5        18   True   True  False      1   True
20       6        30   True   True  False      1   True
21       7        54   True   True  False      1   True
22       8        73   True   True  False      1   True
23       9       100   True   True  False      1   True
24      10       100   True   True   True      1   True
25      11       100  False   True   True      1  False
26      12       100  False   True   True      1  False
27      13       100  False   True   True      1  False

",numpy
setting rgb value for a numpy array using boolean indexing,"I have an array with shape (100, 80, 3) which is an rgb image.
I have a boolean mask with shape (100, 80).
I want each pixel where the mask is True to have value of pix_val = np.array([0.1, 0.2, 0.3]).
cols = 100
rows = 80
img = np.random.rand(rows, cols, 3)
mask = np.random.randint(2, size=(rows, cols), dtype=np.bool_)
px = np.array([0.1, 0.2, 0.3])

for ch in range(3):
  img[:, :, ch][mask] = px[ch]

I thought broadcasting:
img[mask[:, :, None]] = px

would work. But it did not.
I am looking for a vectorized (efficient) way to implement it.
","I'll attempt to explain why your indexing attempt didn't work.
Make a smaller 3d array, and 2d mask:
In [1]: import numpy as np

In [2]: img = np.arange(24).reshape(2,3,4)

In [3]: mask = np.array([[1,0,1],[0,1,1]],bool);mask
Out[3]: 
array([[ True, False,  True],
       [False,  True,  True]])

Using @mozway's indexing, produces a (4,4) array.  The first first 4 is the number of True values in the mask, the second is the trailing dimension:
In [4]: img[mask]
Out[4]: 
array([[ 0,  1,  2,  3],
       [ 8,  9, 10, 11],
       [16, 17, 18, 19],
       [20, 21, 22, 23]])

With your indexing attempt, we get an error.  (You really should have shown the error message):
In [5]: img[mask[:,:,None]]
---------------------------------------------------------------------------
IndexError                                Traceback (most recent call last)
Cell In[5], line 1
----> 1 img[mask[:,:,None]]

IndexError: boolean index did not match indexed array 
along dimension 2; dimension is 4 but corresponding 
boolean dimension is 1

With this None, mask dimension is (2,3,1).  That last 1 doesn't match the 4 of img.  broadcasting doesn't apply in this context.
Now if we attempt to use mask in a multiplication, the (2,3,4) and (2,3) don't broadcast together:
In [6]: img*mask
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
Cell In[6], line 1
----> 1 img*mask

ValueError: operands could not be broadcast together 
with shapes (2,3,4) (2,3) 

But (2,3,1) does broadcast with (2,3,4), producing a select number of 0 rows:
In [7]: img*mask[:,:,None]
Out[7]: 
array([[[ 0,  1,  2,  3],
        [ 0,  0,  0,  0],
        [ 8,  9, 10, 11]],

       [[ 0,  0,  0,  0],
        [16, 17, 18, 19],
        [20, 21, 22, 23]]])

As I commented, using a boolean mask is equivalent to indexing with nonzero arrays:
In [13]: I,J = np.nonzero(mask); I,J
Out[13]: (array([0, 0, 1, 1], dtype=int64), array([0, 2, 1, 2], dtype=int64))

In [14]: img[I,J,:]
Out[14]: 
array([[ 0,  1,  2,  3],
       [ 8,  9, 10, 11],
       [16, 17, 18, 19],
       [20, 21, 22, 23]])

In the assignment expresion, a size (4,) value can broadcast to the (n,4) indexed img[mask].  Now if we were attempting to mask other dimensions we might need to make a px[:,None,:] or something like that.
",numpy
futurewarning 39dataframeswapaxes39 is deprecated and will be removed in a future version,"Looks like numpy is using deprecated function DataFrame.swapaxes in fromnumeric.py.
Anaconda3\lib\site-packages\numpy\core\fromnumeric.py:59: FutureWarning: 'DataFrame.swapaxes' is deprecated and will be removed in a future version. Please use 'DataFrame.transpose' instead.
  return bound(*args, **kwds)

I am getting this warning from the following line of code in Jupyter Notebook:
train, val, test = np.split(df.sample(frac=1), [int(0.8*len(df)), int(0.9*len(df))])

This is the structure of the dataframe I am using:

What exactly is raising this warning and what should I change in my code to get rid of this Warning?
I also found that this is currently an open issue of numpy in github. It will be great if anybody could help. Thanks in advance.
","According to the numpy issue on github, this ""bug"" will not be fixed in numpy. The official statement is that np.split should not be used to split pandas DataFrames anymore. Instead, iloc should be used to split DataFrames as it is described in this answer.
As it looks that you are splitting the DataFrame for machine learning reasons, please be aware that there are built-in methods in many machine learning libraries doing the test/train(/validation) split for you. Here the example of sklearn.
",numpy
builtin range or numpyarange which is more efficient,"When iterating over a large array with a range expression, should I use Python's built-in range function, or numpy's arange to get the best performance?
My reasoning so far:
range probably resorts to a native implementation and might be faster therefore. On the other hand, arange returns a full array, which occupies memory, so there might be an overhead. Python 3's range expression is a generator, which does not hold all the values in memory.
","For large arrays, a vectorised numpy operation is the fastest. If you must loop, prefer xrange/range and avoid using np.arange.
In numpy you should use combinations of vectorized calculations, ufuncs and indexing to solve your problems as it runs at C speed.
Looping over numpy arrays is inefficient compared to this.
(Something like the worst thing you could do would be to iterate over the array with an index created with range or np.arange as the first sentence in your question suggests, but I'm not sure if you really mean that.)
import numpy as np
import sys

sys.version
# out: '2.7.3rc2 (default, Mar 22 2012, 04:35:15) \n[GCC 4.6.3]'
np.version.version
# out: '1.6.2'

size = int(1E6)

%timeit for x in range(size): x ** 2
# out: 10 loops, best of 3: 136 ms per loop

%timeit for x in xrange(size): x ** 2
# out: 10 loops, best of 3: 88.9 ms per loop

# avoid this
%timeit for x in np.arange(size): x ** 2
#out: 1 loops, best of 3: 1.16 s per loop

# use this
%timeit np.arange(size) ** 2
#out: 100 loops, best of 3: 19.5 ms per loop

So for this case numpy is 4 times faster than using xrange if you do it right. Depending on your problem numpy can be much faster than a 4 or 5 times speed up.
The answers to this question explain some more advantages of using numpy arrays instead of python lists for large data sets.
",numpy
reorder numpy array by given index list,"I have an array of indexes:
test_idxs = np.array([4, 2, 7, 5])

I also have an array of values (which is longer):
test_vals = np.array([13, 19, 31, 6, 21, 45, 98, 131, 11])

So I want to get an array with the length of the array of indexes, but with values from the array of values in the order of the array of indexes. In other words, I want to get something like this:
array([21, 31, 131, 45])

I know how to do this in a loop, but I don't know how to achieve this using Numpy tools.
","This is actually extremely simple with numpy, just index your test_vals array with test_idx (integer array indexing):
out = test_vals[test_idxs]

Output:
array([ 21,  31, 131,  45])

Note that this requires the indices to be valid. If you have indices that could be too high you would need to handle them explicitly.
Example:
test_idxs = np.array([4, 2, 9, 5])
test_vals = np.array([13, 19, 31, 6, 21, 45, 98, 131, 11])

out = np.where(test_idxs < len(test_vals),
               test_vals[np.clip(test_idxs, 0, len(test_vals)-1)],
               np.nan)

Output:
array([21., 31., nan, 45.])

",numpy
how to generate an array which is a multiple of original,"I'm trying to upsize OpenCV images in Python in such a manner that the individual pixels are spread out by an integral factor; I use this to visually examine deep detail and individual pixel values can be seen (using cv2.imshow in this instance).
For example, an array:
    [[1,2],
     [3,4]]


And a factor of 2 means I'd get:
    [[1,1,2,2],
     [1,1,2,2],
     [3,3,4,4], 
     [3,3,4,4]]


I've done this by generating an array of size*factor using np.zeros, then iterating each point in the original array and copying it to the target array using (for example)
for y in range(src.shape[0]):
    for x in range(src.shape[1]):
        tgt[y*f:y*f+f, x*f:x*f+f, :] = src[y,x,:]

But as you can imagine, it's not the fastest approach, and I'm hoping I'm just not finding the right thing.
OpenCV (and PIL) do not have a resize capability that doesn't interpolate by one method or another, which seems weird all by itself.
I looked over & tried numpy broadcast*, numpy stride_trickks, opencv functions, PIL functions.
The semi-manual method works as long as I don't need interactivity, but I'm trying to adjust parameters to several opencv functions quickly so I can find the right combinations to solve my problem.  (Which is proprietary, so I can't share imagery...)  Waiting a significant time between results is counterproductive.
","You can use opencv's cv.resize with nearest-neighbor as interpolation method (cv.INTER_NEAREST) to achieve what you need:
import cv2 as cv
import numpy as np

src = np.array([[1,2], [3,4]])
dst = cv.resize(src, (4,4), interpolation=cv.INTER_NEAREST)
print(dst)

Output:
[[1 1 2 2]
 [1 1 2 2]
 [3 3 4 4]
 [3 3 4 4]]

Live demo
",numpy
calculating multi dimensional gaussian distribution by nump apply_along_axis,"I want to calculate multi dimensional Gaussian for multi-band image. For this purpose I need to consider each pixel with its value in different bands as vector to apply Gaussian which contains matrix multiplication of V^T*inv(covariance)*V.
import numpy as np
import numpy.linalg as lg
X,Y=np.meshgrid(np.arange(-10,11,1/10),np.arange(-10,11,1/10),indexing='xy')
Z=np.stack((X,Y))
cov=np.array([[10,-0.4],[-0.4,1]])
W=lg.inv(cov)
def ee(x):
    global W
    x=x.reshape((-1,1))
    
    return lg.matmul(lg.matmul(x.transpose(),W),x)[0,0]
tt=np.apply_along_axis(ee,0,Z)
p=np.exp(-0.5*tt)/(np.power(2*np.pi,cov.shape[0]/2)*np.power(lg.det(cov),0.5))
import matplotlib.pyplot as plt
plt.imshow(p)


enter image description here
I have found apply_along_axis as solution but my image size is (3,1000,1000) and apply_along_axis is so slow to apply function on pixels to find each pixel's probability. How can i speed it up fore real world applications.thank you
","Instead of looping along the axis, You could use the following:
a = Z.reshape(2,-1)
t = (W @ a * a).sum(0).reshape(*X.shape)

Check whether it produces the desired results:
np.allclose(tt, t)
True

",numpy
difference between quotsinglequot and quottablequot methods in pandasdataframequantile,"I hope someone can help me understand the difference between the ""single"" and ""table"" methods in pandas.DataFrame.quantile?

Whether to compute quantiles per-column (‘single’) or over all columns (‘table’).
https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.quantile.html

For example, the following code yields the same results.
import numpy as np
import pandas as pd

df = pd.DataFrame(np.array([[1, 1], [2, 10], [3, 100], [4, 100]]), columns=[""a"", ""b""])

print(df.quantile(method=""single"", interpolation=""nearest""))
print(df.quantile(method=""table"", interpolation=""nearest""))

a      3
b    100
Name: 0.5, dtype: int64
a      3
b    100
Name: 0.5, dtype: int64

","If you invert the order of the values in b the result will be different:
df['b'] = df['b'].values[::-1]

print(df.quantile(method='single', interpolation='nearest'))
print(df.quantile(method='table', interpolation='nearest'))

Output:
a      3
b    100
Name: 0.5, dtype: int64

a     3
b    10
Name: 0.5, dtype: int64

The exact difference in behavior is a bit tricky to understand.
This happens in core/frame.py:
        if method == ""single"":
            res = data._mgr.quantile(qs=q, interpolation=interpolation)
        elif method == ""table"":
            valid_interpolation = {""nearest"", ""lower"", ""higher""}
            if interpolation not in valid_interpolation:
                raise ValueError(
                    f""Invalid interpolation: {interpolation}. ""
                    f""Interpolation must be in {valid_interpolation}""
                )
            # handle degenerate case
            if len(data) == 0:
                if data.ndim == 2:
                    dtype = find_common_type(list(self.dtypes))
                else:
                    dtype = self.dtype
                return self._constructor([], index=q, columns=data.columns, dtype=dtype)

            q_idx = np.quantile(np.arange(len(data)), q, method=interpolation)

            by = data.columns
            if len(by) > 1:
                keys = [data._get_label_or_level_values(x) for x in by]
                indexer = lexsort_indexer(keys)
            else:
                k = data._get_label_or_level_values(by[0])
                indexer = nargsort(k)

            res = data._mgr.take(indexer[q_idx], verify=False)
            res.axes[1] = q

        result = self._constructor_from_mgr(res, axes=res.axes)
        return result.__finalize__(self, method=""quantile"")

In short, if you use method='single', this computes:
np.percentile(a, 50, axis=0, method='nearest')
# array([  3, 100])

With method='table', this computes the quantile on the indexer (np.arange(len(df))). The result with be an integer (q_idx) between 0 and len(df). The, this sorts the rows with lexsort_indexer, giving priority to the first column(s), and finally takes the (q_idx)th row:
data = df
q = 0.5
interpolation = 'nearest'

q_idx = np.quantile(np.arange(len(data)), q, method=interpolation)
# 2

indexer = lexsort_indexer([data._get_label_or_level_values(x) for x in data.columns])
# array([0, 1, 2, 3])

df.iloc[indexer]
#    a    b
# 0  1  100
# 1  2  100
# 2  3   10   # this row will be picked
# 3  4    1

data.iloc[indexer[q_idx]]
# a     3
# b    10
# Name: 2, dtype: int64

This means that the result is dependent on the order of the columns:
print(df.quantile(method='table', interpolation='nearest'))

a     3
b    10
Name: 0.5, dtype: int64

# now let's give b priority over a
print(df.iloc[:, ::-1].quantile(method='table', interpolation='nearest'))

b    100
a      1
Name: 0.5, dtype: int64

",numpy
performance optimal way to serialise python objects containing large pandas dataframes,"I am dealing with Python objects containing Pandas DataFrame and Numpy Series objects. These can be large, several millions of rows.
E.g.

@dataclass
class MyWorld:
     # A lot of DataFrames with millions of rows
     samples: pd.DataFrame 
     addresses: pd.DataFrame 
     # etc.

I need to cache these objects, and I am hoping to find an efficient and painless way to serialise them, instead of standard pickle.dump(). Are there any specialised Python serialisers for such objects that would pickle Series data with some efficient codec and compression automatically? Alternatively, I need to hand construct several Parquet files, but that requires a lot of more manual code to deal with this, and I'd rather avoid that if possible.
Performance here may mean

Speed
File size (can be related, as you need to read less from the disk/network)

I am aware of joblib.dump() which does some magic for these kind of objects, but based on the documentation I am not sure if this is relevant anymore.
","What about storing huge structures in parquet format while pickling it, this can be automated easily:
import io
from dataclasses import dataclass
import pickle
import numpy as np
import pandas as pd

@dataclass
class MyWorld:
    
    array: np.ndarray
    series: pd.Series
    frame: pd.DataFrame

@dataclass
class MyWorldParquet:
    
    array: np.ndarray
    series: pd.Series
    frame: pd.DataFrame
        
    def __getstate__(self):

        for key, value in self.__annotations__.items():
            
            if value is np.ndarray:
                self.__dict__[key] = pd.DataFrame({""_"": self.__dict__[key]})
            
            if value is pd.Series:
                self.__dict__[key] = self.__dict__[key].to_frame()
        
            stream = io.BytesIO()
            self.__dict__[key].to_parquet(stream)
            
            self.__dict__[key] = stream
        
        return self.__dict__

    def __setstate__(self, data):
        
        self.__dict__.update(data)
        
        for key, value in self.__annotations__.items():
        
            self.__dict__[key] = pd.read_parquet(self.__dict__[key])
            
            if value is np.ndarray:
                self.__dict__[key] = self.__dict__[key][""_""].values
            
            if value is pd.Series:
                self.__dict__[key] = self.__dict__[key][self.__dict__[key].columns[0]]

Off course we will have some trade off between performance and volumetry as reducing the second requires format translation and compression.
Lets create a toy dataset:
N = 5_000_000
data = {
    ""array"": np.random.normal(size=N),
    ""series"": pd.Series(np.random.uniform(size=N), name=""w""),
    ""frame"": pd.DataFrame({
        ""c"": np.random.choice([""label-1"", ""label-2"", ""label-3""], size=N),
        ""x"": np.random.uniform(size=N),
        ""y"": np.random.normal(size=N)
    })
}

We can compare the parquet conversion trade off (about 300 ms more):
%timeit -r 10 -n 1 pickle.dumps(MyWorld(**data))
# 1.57 s ± 162 ms per loop (mean ± std. dev. of 10 runs, 1 loop each)

%timeit -r 10 -n 1 pickle.dumps(MyWorldParquet(**data))
# 1.9 s ± 71.3 ms per loop (mean ± std. dev. of 10 runs, 1 loop each)

And the volumetry gain (about 40 Mb spared):
len(pickle.dumps(MyWorld(**data))) / 2 ** 20
# 200.28876972198486

len(pickle.dumps(MyWorldParquet(**data))) / 2 ** 20
# 159.13739013671875

Indeed those metrics will strongly depends on the actual dataset to be serialized.
",numpy
optimisation of a numpy script to create links between points in a specific format and order,"Starting from a situation like the one below (see image):

So, to start with, we have a series of points located on the x axis (the distance between the points can vary and the number of points can vary. In the example, we have 5 starting points). We perform a translation along y in several steps (in the example in 3 steps) to arrive at a final y value (in this case 3 units). The numbers in yellow correspond to their indexes.
My aim is to achieve the following:

In other words, to create links between the points that follow the translation.
In practice, and to keep things simple, I'd like to obtain a final array like this:
final : 
 [[ 0  6] [ 6 12] [12 18]
 [ 1  7] [ 7 13] [13 19]
 [ 2  8] [ 8 14] [14 20]
 [ 3  9] [ 9 15] [15 21]
 [ 4 10] [10 16] [16 22]
 [ 5 11] [11 17] [17 23]]

Here's the script I made that does the job:

import numpy as np
# u: number of points along the x axis
u = 6
# v: number of points along the y axis
v = 4

def x_links(u,v):
    matrice = np.arange(u*v).reshape((v, u))
    links =np.array([])
    for i in range(u):
        a = np.repeat(matrice[:,i],2)[1:-1] 
        links=np.append(links,a).astype(int)
    return links.reshape(int(len(links)/2),2)
    
example = x_links(u,v)
print(""final : \n"",example)

My questions are:

Can anyone think of a better way of achieving the same result? a much more pythonic script ...
In particular, would it be possible to do without using a loop?

Thank you very much.
","This should work:
matrix = np.arange(u*v).reshape((v,u))
np.array(np.stack((matrix, np.roll(matrix, v-1, axis = 0)), axis=-1))[:-1]

Output:
array([[[ 0,  6],
        [ 1,  7],
        [ 2,  8],
        [ 3,  9],
        [ 4, 10],
        [ 5, 11]],

       [[ 6, 12],
        [ 7, 13],
        [ 8, 14],
        [ 9, 15],
        [10, 16],
        [11, 17]],

       [[12, 18],
        [13, 19],
        [14, 20],
        [15, 21],
        [16, 22],
        [17, 23]]])

",numpy
numpy quotwherequot with multiple conditions,"I try to add a new column ""energy_class"" to a dataframe ""df_energy"" which contains the string ""high"" if the ""consumption_energy"" value > 400, ""medium"" if the ""consumption_energy"" value is between 200 and 400, and ""low"" if the ""consumption_energy"" value is under 200.
I try to use  np.where from numpy, but I see that numpy.where(condition[, x, y]) treat only two condition not 3 like in my case.
","You can use a ternary:
np.where(consumption_energy > 400, 'high', 
         (np.where(consumption_energy < 200, 'low', 'medium')))

",numpy
understanding scipy deconvolve,"I'm trying to understand scipy.signal.deconvolve.  
From the mathematical point of view a convolution is just the multiplication in fourier space so I would expect
that for two functions f and g:
Deconvolve(Convolve(f,g) , g) == f
In numpy/scipy this is either not the case or I'm missing an important point. 
Although there are some questions related to deconvolve on SO already (like here and here) they do not address this point, others remain unclear (this) or unanswered (here). There are also two questions on SignalProcessing SE (this and this) the answers to which are not helpful in understanding how scipy's deconvolve function works.
The question would be:

How do you reconstruct the original signal f from a convoluted signal, 
assuming you know the convolving function g.?
Or in other words: How does this pseudocode Deconvolve(Convolve(f,g) , g) == f translate into numpy / scipy?

Edit: Note that this question is not targeted at preventing numerical inaccuracies (although this is also an open question) but at understanding how convolve/deconvolve work together in scipy.
The following code tries to do that with a Heaviside function and a gaussian filter. 
As can be seen in the image, the result of the deconvolution of the convolution is not at 
all the original Heaviside function. I would be glad if someone could shed some light into this issue.
import numpy as np
import scipy.signal
import matplotlib.pyplot as plt

# Define heaviside function
H = lambda x: 0.5 * (np.sign(x) + 1.)
#define gaussian
gauss = lambda x, sig: np.exp(-( x/float(sig))**2 )

X = np.linspace(-5, 30, num=3501)
X2 = np.linspace(-5,5, num=1001)

# convolute a heaviside with a gaussian
H_c = np.convolve( H(X),  gauss(X2, 1),  mode=""same""  )
# deconvolute a the result
H_dc, er = scipy.signal.deconvolve(H_c, gauss(X2, 1) )


#### Plot #### 
fig , ax = plt.subplots(nrows=4, figsize=(6,7))
ax[0].plot( H(X),          color=""#907700"", label=""Heaviside"",    lw=3 ) 
ax[1].plot( gauss(X2, 1),  color=""#907700"", label=""Gauss filter"", lw=3 )
ax[2].plot( H_c/H_c.max(), color=""#325cab"", label=""convoluted"" ,  lw=3 ) 
ax[3].plot( H_dc,          color=""#ab4232"", label=""deconvoluted"", lw=3 ) 
for i in range(len(ax)):
    ax[i].set_xlim([0, len(X)])
    ax[i].set_ylim([-0.07, 1.2])
    ax[i].legend(loc=4)
plt.show()


Edit: Note that there is a matlab example, showing how to convolve/deconvolve a rectangular signal using
yc=conv(y,c,'full')./sum(c); 
ydc=deconv(yc,c).*sum(c); 

In the spirit of this question it would also help if someone was able to translate this example into python.
","After some trial and error I found out how to interprete the results of scipy.signal.deconvolve() and I post my findings as an answer.
Let's start with a working example code
import numpy as np
import scipy.signal
import matplotlib.pyplot as plt

# let the signal be box-like
signal = np.repeat([0., 1., 0.], 100)
# and use a gaussian filter
# the filter should be shorter than the signal
# the filter should be such that it's much bigger then zero everywhere
gauss = np.exp(-( (np.linspace(0,50)-25.)/float(12))**2 )
print gauss.min()  # = 0.013 >> 0

# calculate the convolution (np.convolve and scipy.signal.convolve identical)
# the keywordargument mode=""same"" ensures that the convolution spans the same
#   shape as the input array.
#filtered = scipy.signal.convolve(signal, gauss, mode='same') 
filtered = np.convolve(signal, gauss, mode='same') 

deconv,  _ = scipy.signal.deconvolve( filtered, gauss )
#the deconvolution has n = len(signal) - len(gauss) + 1 points
n = len(signal)-len(gauss)+1
# so we need to expand it by 
s = (len(signal)-n)/2
#on both sides.
deconv_res = np.zeros(len(signal))
deconv_res[s:len(signal)-s-1] = deconv
deconv = deconv_res
# now deconv contains the deconvolution 
# expanded to the original shape (filled with zeros) 


#### Plot #### 
fig , ax = plt.subplots(nrows=4, figsize=(6,7))

ax[0].plot(signal,            color=""#907700"", label=""original"",     lw=3 ) 
ax[1].plot(gauss,          color=""#68934e"", label=""gauss filter"", lw=3 )
# we need to divide by the sum of the filter window to get the convolution normalized to 1
ax[2].plot(filtered/np.sum(gauss), color=""#325cab"", label=""convoluted"" ,  lw=3 )
ax[3].plot(deconv,         color=""#ab4232"", label=""deconvoluted"", lw=3 ) 

for i in range(len(ax)):
    ax[i].set_xlim([0, len(signal)])
    ax[i].set_ylim([-0.07, 1.2])
    ax[i].legend(loc=1, fontsize=11)
    if i != len(ax)-1 :
        ax[i].set_xticklabels([])

plt.savefig(__file__ + "".png"")
plt.show()    

This code produces the following image, showing exactly what we want (Deconvolve(Convolve(signal,gauss) , gauss) == signal)

Some important findings are:

The filter should be shorter than the signal
The filter should be much bigger than zero everywhere (here > 0.013 is good enough)
Using the keyword argument mode = 'same' to the convolution ensures that it lives on the same array shape as the signal.
The deconvolution has n = len(signal) - len(gauss) + 1 points. 
So in order to let it also reside on the same original array shape we need to expand it by s = (len(signal)-n)/2 on both sides.

Of course, further findings, comments and suggestion to this question are still welcome.
",numpy
find time shift between two similar waveforms,"I have to compare two time-vs-voltage waveforms. Because of the peculiarity of the sources of these waveforms, one of them can be a time shifted version of the other. 
How can i find whether there is a time shift? and if yes, how much is it.
I am doing this in Python and wish to use numpy/scipy libraries.
","scipy provides a correlation function which will work fine for small input and also if you want non-circular correlation meaning that the signal will not wrap around. note that in mode='full' , the size of the array returned by signal.correlation is sum of the signal sizes minus one (i.e. len(a) + len(b) - 1), so the value from argmax is off by (signal size -1 = 20) from what you seem to expect.
from scipy import signal, fftpack
import numpy
a = numpy.array([0, 1, 2, 3, 4, 3, 2, 1, 0, 1, 2, 3, 4, 3, 2, 1, 0, 0, 0, 0, 0])
b = numpy.array([0, 0, 0, 0, 0, 1, 2, 3, 4, 3, 2, 1, 0, 1, 2, 3, 4, 3, 2, 1, 0])
numpy.argmax(signal.correlate(a,b)) -> 16
numpy.argmax(signal.correlate(b,a)) -> 24

The two different values correspond to whether the shift is in a or b.
If you want circular correlation and for big signal size, you can use the convolution/Fourier transform theorem with the caveat that correlation is very similar to but not identical to convolution.
A = fftpack.fft(a)
B = fftpack.fft(b)
Ar = -A.conjugate()
Br = -B.conjugate()
numpy.argmax(numpy.abs(fftpack.ifft(Ar*B))) -> 4
numpy.argmax(numpy.abs(fftpack.ifft(A*Br))) -> 17

again the two values correspond to whether your interpreting a shift in a or a shift in b. 
The negative conjugation is due to convolution flipping one of the functions, but in correlation there is no flipping. You can undo the flipping by either reversing one of the signals and then taking the FFT, or taking the FFT of the signal and then taking the negative conjugate. i.e. the following is true: Ar = -A.conjugate() = fft(a[::-1])
",numpy
indexerror index 7 is out of bounds for axis 0 with size 7,"I am trying to assess whether the lips of a person are moving too much while the mouth is closed (to conclude it is chewing).
The mouth closed part is done without any issue, but when I try to assess the lip movement through landmarks (dlib) there seems to be a problem with the last landmark of the mouth.
Inspired by the mouth example (https://github.com/mauckc/mouth-open/blob/master/detect_open_mouth.py#L17), I wrote the following function:
def lips_aspect_ratio(shape):
    # grab the indexes of the facial landmarks for the lip
    (mStart, mEnd) = (61, 68)
    lip = shape[mStart:mEnd]
    print(len(lip))
    # compute the euclidean distances between the two sets of
    # vertical lip landmarks (x, y)-coordinates
    # to reach landmark 68 I need to get lib[7] not lip[6] (while I get lip[7] I get IndexOutOfBoundError)
    A = dist.euclidean(lip[1], lip[6])  # 62, 68
    B = dist.euclidean(lip[3], lip[5])  # 64, 66

    # compute the euclidean distance between the horizontal
    # lip landmark (x, y)-coordinates
    C = dist.euclidean(lip[0], lip[4])  # 61, 65

    # compute the lip aspect ratio
    mar = (A + B) / (2.0 * C)

    # return the lip aspect ratio
    return mar

The landmark of the lips are (61, 68), when I extract the lip as lip = shape[61:68] and try to access the last landmark as lip[7] I get the following error:
IndexError: index 7 is out of bounds for axis 0 with size 7

Why is that? and How to get the last landmark of the lip/face
","lip = shape[61:68]

The slices exclude the end element. So you got 7 elements: 61,62,63,64, 65,66,67. And len(lip) == 7 confirms that.
If there truly are 8 points for the lip shape, and they include element 68, the slice should be:
lip = shape[61:69]
assert(len(range(61, 69) == 8)
assert(len(lip) == 8)

Note that if shape is sufficiently long, then len(shape[a:b:c]) == len(range(a, b, c)). That's because the range function acts like taking a slice out of an infinitely long list of integers (with caveats).
So, the problem is a classic off-by-one, not much to do with AI/image analysis :)
",numpy
difference between npint npint_ int and npint_t in cython,"I am a bit struggled with so many int data types in cython.
np.int, np.int_, np.int_t, int
I guess int in pure python is equivalent to np.int_, then where does np.int come from? I cannot find the document from numpy? Also, why does np.int_ exist given we do already have int?
In cython, I guess int becomes a C type when used as cdef int or ndarray[int], and when used as int() it stays as the python caster?
Is np.int_ equivalent to long in C? so cdef long is the identical to cdef np.int_?
Under what circumstances should I use np.int_t instead of np.int? e.g. cdef np.int_t, ndarray[np.int_t] ...
Can someone briefly explain how the wrong use of those types would affect the performance of compiled cython code?
","It's a bit complicated because the names have different meanings depending on the context.
int

In Python

The int is normally just a Python type, it's of arbitrary precision, meaning that you can store any conceivable integer inside it (as long as you have enough memory).
    >>> int(10**50)
    100000000000000000000000000000000000000000000000000


However, when you use it as dtype for a NumPy array it will be interpreted as np.int_ 1. Which is not of arbitrary precision, it will have the same size as C's long:
 >>> np.array(10**50, dtype=int)
 OverflowError: Python int too large to convert to C long



That also means the following two are equivalent:
    np.array([1,2,3], dtype=int)
    np.array([1,2,3], dtype=np.int_)


As Cython type identifier it has another meaning, here it stands for the c type int. It's of limited precision (typically 32bits). You can use it as Cython type, for example when defining variables with cdef:
 cdef int value = 100    # variable
 cdef int[:] arr = ...   # memoryview



As return value or argument value for cdef or cpdef functions:
    cdef int my_function(int argument1, int argument2):
        # ...

As ""generic"" for ndarray:
    cimport numpy as cnp
    cdef cnp.ndarray[int, ndim=1] val = ...

For type casting:
    avalue = <int>(another_value)

And probably many more.

In Cython but as Python type. You can still call int and you'll get a ""Python int"" (of arbitrary precision), or use it for isinstance or as dtype argument for np.array. Here the context is important, so converting to a Python int is different from converting to a C int:
 cdef object val = int(10)  # Python int
 cdef int val = <int>(10)   # C int



np.int
Actually this is very easy. It's just an alias for int:
>>> int is np.int
True

So everything from above applies to np.int as well. However you can't use it as a type-identifier except when you use it on the cimported package. In that case it represents the Python integer type.
cimport numpy as cnp

cpdef func(cnp.int obj):
    return obj

This will expect obj to be a Python integer not a NumPy type:
>>> func(np.int_(10))
TypeError: Argument 'obj' has incorrect type (expected int, got numpy.int32)
>>> func(10)
10

My advise regarding np.int: Avoid it whenever possible. In Python code it's equivalent to int and in Cython code it's also equivalent to Pythons int but if used as type-identifier it will probably confuse you and everyone who reads the code! It certainly confused me...
np.int_
Actually it only has one meaning: It's a Python type that represents a scalar NumPy type. You use it like Pythons int:
>>> np.int_(10)        # looks like a normal Python integer
10
>>> type(np.int_(10))  # but isn't (output may vary depending on your system!)
numpy.int32

Or you use it to specify the dtype, for example with np.array:
>>> np.array([1,2,3], dtype=np.int_)
array([1, 2, 3])

But you cannot use it as type-identifier in Cython.
cnp.int_t
It's the type-identifier version for np.int_. That means you can't use it as dtype argument. But you can use it as type for cdef declarations:
cimport numpy as cnp
import numpy as np

cdef cnp.int_t[:] arr = np.array([1,2,3], dtype=np.int_)
     |---TYPE---|                         |---DTYPE---|

This example (hopefully) shows that the type-identifier with the trailing _t actually represents the type of an array using the dtype without the trailing t. You can't interchange them in Cython code!
Notes
There are several more numeric types in NumPy I'll include a list containing the NumPy dtype and Cython type-identifier and the C type identifier that could also be used in Cython here. But it's basically taken from the NumPy documentation and the Cython NumPy pxd file:
NumPy dtype          Numpy Cython type         C Cython type identifier

np.bool_             None                      None
np.int_              cnp.int_t                 long
np.intc              None                      int       
np.intp              cnp.intp_t                ssize_t
np.int8              cnp.int8_t                signed char
np.int16             cnp.int16_t               signed short
np.int32             cnp.int32_t               signed int
np.int64             cnp.int64_t               signed long long
np.uint8             cnp.uint8_t               unsigned char
np.uint16            cnp.uint16_t              unsigned short
np.uint32            cnp.uint32_t              unsigned int
np.uint64            cnp.uint64_t              unsigned long
np.float_            cnp.float64_t             double
np.float32           cnp.float32_t             float
np.float64           cnp.float64_t             double
np.complex_          cnp.complex128_t          double complex
np.complex64         cnp.complex64_t           float complex
np.complex128        cnp.complex128_t          double complex

Actually there are Cython types for np.bool_: cnp.npy_bool and bint but both  they can't be used for NumPy arrays currently. For scalars cnp.npy_bool will just be an unsigned integer while bint will be a boolean. Not sure what's going on there...

1 Taken From the NumPy documentation ""Data type objects""

Built-in Python types


Several python types are equivalent to a corresponding array scalar when used to generate a dtype object:
int           np.int_
bool          np.bool_
float         np.float_
complex       np.cfloat
bytes         np.bytes_
str           np.bytes_ (Python2) or np.unicode_ (Python3)
unicode       np.unicode_
buffer        np.void
(all others)  np.object_


",numpy
attributeerror 39numpyuint839 object has no attribute 39bit_length39,"I want to find the number of bits necessary to represent an unsigned numpy integer (or each element in an array of integers) in binary, in the same way the python's int.bit_length() does, but it seems that numpy has no equivalent function.
For example:
>>> int(0b1000).bit_length()
4
>>> np.uint8(0b1000).bit_length()
AttributeError: 'numpy.uint8' object has no attribute 'bit_length'

Can anyone help me find the correct function? My current approach is to convert each array element to a python int to find the bit length, which seems like an awful option, for speed and clarity:
np.vectorize(lambda np_int: int(np_int).bit_length())

","You can take the ceiling of the log2 of your array.
import numpy as np

x = np.random.randint(0, 100, size=30)
x
# returns:
array([92,  7, 53, 24, 85, 53, 78, 52, 99, 91, 79, 40, 82, 34, 18, 26, 20,
        7, 47, 38, 78, 50, 15, 12, 54,  3, 91, 82, 22, 90])

np.ceil(np.log2(x)).astype(int)
# returns:
array([7, 3, 6, 5, 7, 6, 7, 6, 7, 7, 7, 6, 7, 6, 5, 5, 5, 3, 6, 6, 7, 6,
       4, 4, 6, 2, 7, 7, 5, 7])

",numpy
can39t figure out why numpylog10 outputs nan,"So I have an 500k array of floating values. When I am trying to:
np.log10(my_long_array)

270k numbers getting replaced to nan, and they are not that small. For example:
In [1]: import numpy as np

In [2]: t = -0.055488893531690543

In [3]: np.log10(t)
/home/aydar/anaconda3/bin/ipython:1: RuntimeWarning: invalid value encountered in log10
  #!/home/aydar/anaconda3/bin/python3
Out[3]: nan

In [4]: type(t)
Out[4]: float

What am I missing?
","the logarithm of a negative number is undefined, hence the nan
From the docs to numpy.log10:

Returns: y : ndarray
The logarithm to the base 10 of x, element-wise. NaNs are returned where x is negative.

",numpy
is there a difference between scipypi numpypi or mathpi,"In a project using SciPy and NumPy, when should one use scipy.pi vs numpy.pi vs just math.pi? Is there a difference between these values?
",">>> import math
>>> import numpy as np
>>> import scipy
>>> math.pi == np.pi == scipy.pi
True

So it doesn't matter, they are all the same value.
The only reason all three modules provide a pi value is so if you are using just one of the three modules, you can conveniently have access to pi without having to import another module. They're not providing different values for pi.
",numpy
prettyprint indicescoordinates of 2d numpy array,"Does Numpy provide built-in capabilities to print the indices/coordinates of a 2-d Numpy array at its borders?
What I mean is the following: Given, for example, the array
a = np.arange(6).reshape(2, 3).astype(float)

I would like to have a printout as follows:
     0    1    2
0  0.0  1.0  2.0
1  3.0  4.0  5.0

It needn't be exactly like that, but the corresponding information should be conveyed. That is, I want to have a ""header column"" that shows the row indices (here: 0, 1) and a ""header row"" that shows the column indices (here: 0, 1, 2).
At present, I simply use Pandas for this purpose (i.e. I convert the Numpy array to a Pandas DataFrame and then print the DataFrame). This is also suggested, for example, in this answer to a related question. However, this would always require Pandas, even if I don't use it for anything else in a given project.
So my question, again, is: can I achieve the same printout only with Numpy (thus without Pandas)? Or, if not, is there a lightweight solution (in the sense of a few lines of code without an additional library) to achieve a similar output?
","You can try:
import numpy as np

def pretty_print(arr, precision=2, sep=1):
    # Convert array contents to list of list of formatted strings
    rows = [[f""{v:.{precision}f}"" for v in row] for row in arr]
    # Calculate width of row index's column (maxl) and remaining columns (maxw)
    maxl = len(str(arr.shape[0] - 1))
    maxw = max(len(str(arr.shape[1] - 1)), *(len(v) for row in rows for v in row))
    # Print column indexes and content, pad string entries to appropriate width
    print("" "" * maxl, *(f""{i:>{maxw}}"" for i in range(0, arr.shape[1])), sep="" "" * sep)
    for i, row in enumerate(rows):
        print(f""{i:>{maxl}}"", *(f""{v:>{maxw}}"" for v in row), sep="" "" * sep)

a = np.arange(12).reshape(3, 4).astype(float)
a[0, 0] = np.inf
a[1, 1] = -np.inf
a[2, 3] *= -1
pretty_print(a, precision=2, sep=1)
pretty_print(np.ones(shape=(3, 14)), precision=0)

Prints:
       0      1      2      3
0    inf   1.00   2.00   3.00
1   4.00   -inf   6.00   7.00
2   8.00   9.00  10.00 -11.00

and
   0  1  2  3  4  5  6  7  8  9 10 11 12 13
0  1  1  1  1  1  1  1  1  1  1  1  1  1  1
1  1  1  1  1  1  1  1  1  1  1  1  1  1  1
2  1  1  1  1  1  1  1  1  1  1  1  1  1  1

",numpy
what39s the difference between  and  with python matrix multiplication,"I know one does one kind of matrix multiplication and the other does another kind but can never remember the difference.
Doing
>>> import numpy as np
>>> a = np.matrix([[1, 2],[3,4]])

>>> print(a * a)
[[ 7 10]
 [15 22]]

>>> print(a @ a)
[[ 7 10]
 [15 22]]

appears to give the same answer which confuses me.
","a * b is a multiplication operator - it will return elements in a multiplied by elements in b.
When a and b are both matrices (specifically defined by np.matrix) the result will be the same as the @ operator.
a @ b is matrix multiplication (dot product when used with vectors). If you haven't specified that a is a matrix and have used an array instead, a * a would return every element in a squared.
",numpy
interpolation over an irregular grid,"So, I have three numpy arrays which store latitude, longitude, and some property value on a grid -- that is, I have LAT(y,x), LON(y,x), and, say temperature T(y,x), for some limits of x and y.  The grid isn't necessarily regular -- in fact, it's tripolar.
I then want to interpolate these property (temperature) values onto a bunch of different lat/lon points (stored as lat1(t), lon1(t), for about 10,000 t...) which do not fall on the actual grid points.  I've tried matplotlib.mlab.griddata, but that takes far too long (it's not really designed for what I'm doing, after all).  I've also tried scipy.interpolate.interp2d, but I get a MemoryError (my grids are about 400x400).
Is there any sort of slick, preferably fast way of doing this?  I can't help but think the answer is something obvious...  Thanks!!
","Try the combination of inverse-distance weighting and
scipy.spatial.KDTree
described in SO
inverse-distance-weighted-idw-interpolation-with-python.
Kd-trees
work nicely in 2d 3d ..., inverse-distance weighting is smooth and local,
and the k= number of nearest neighbours can be varied to tradeoff speed / accuracy.
",numpy
change 139s to 0 and 039s to 1 in numpy array without looping,"Let's say I have a numpy array where I would like to swap all the 1's to 0 and all the 0's to 1 (the array will have other values, and there is nothing special about the 0's and 1's). Of course, I can loop through the array and change the values one by one.
Is there an efficient method you can recommend using? Does the np.where() method have an option for this operation?
","A very simple way which does not require the use of any special method such as np.where() is to get the indices for the conditions of the variables in your numpy array, and accordingly assign the required value (in your case 0 for 1s and 1 for 0s) to the respective positional items in the array. This works for values other than 0s and 1s too. Also, you don't require any temporary variable to swap the values.
import numpy as np
arr = np.array([1, 0, 2, 3, 6, 1, 0])
indices_one = arr == 1
indices_zero = arr == 0
arr[indices_one] = 0 # replacing 1s with 0s
arr[indices_zero] = 1 # replacing 0s with 1s

Output: array([0, 1, 2, 3, 6, 0, 1])

",numpy
numpy elementwise addition with multiple arrays,"I'd like to know if there is a more efficient/pythonic way to add multiple numpy arrays (2D) rather than:
def sum_multiple_arrays(list_of_arrays):
   a = np.zeros(shape=list_of_arrays[0].shape) #initialize array of 0s
   for array in list_of_arrays:
      a += array
   return a 

Ps: I am aware of  np.add() but it works only with 2 arrays.
","np.sum(list_of_arrays, axis=0) 

should work. Or
np.add.reduce(list_of_arrays)

",numpy
runtimewarning overflow encountered in scalar negative,"I am getting this warning when I upgrade my numpy from 1.23.3 to 1.25.0
#For 1.23.3 version

import numpy as np
x = np.uint64(0xffffffff)

print(-x)
#output
18446744069414584321


#For 1.25.0 version

import numpy as np
x = np.uint64(0xffffffff)

print(-x)
#output
18446744069414584321
<ipython-input-4-adbfb5bb01c0>:1: RuntimeWarning: overflow encountered in scalar negative
  print(-x)

Anyone else is getting this warning message or its just me?
","One option is to just suppress the warning
import warnings
warnings.filterwarnings(""ignore"", ""^overflow encountered in scalar .*"", category=RuntimeWarning) 

I think this is safe enough because it only suppress warnings that specifically match that pattern.
",numpy
how to match pairs of values contained in two numpy arrays,"I have two sets of coordinates and want to find out which coordinates of the coo set are identical to any coordinate in the targets set. I want to know the indices in the coo set which means I'd like to get a list of indices or of bools.
import numpy as np

coo = np.array([[1,2],[1,6],[5,3],[3,6]]) # coordinates
targets = np.array([[5,3],[1,6]]) # coordinates of targets

print(np.isin(coo,targets))

[[ True False]
 [ True  True]
 [ True  True]
 [ True  True]]

The desired result would be one of the following two:
[False True True False] # bool list
[1,2] # list of concerning indices

My problem is, that ...

np.isin has no axis-attribute so that I could use axis=1.
even applying logical and to each row of the output would return True for the last element, which is wrong.

I am aware of loops and conditions but I am sure Python is equipped with ways for a more elegant solution.
","This solution will scale worse for large arrays, for such cases the other proposed answers will perform better.

Here's one way taking advantage of broadcasting:
(coo[:,None] == targets).all(2).any(1)
# array([False,  True,  True, False])


 Details 
Check for every row in coo whether or not it matches another in target by direct comparisson having added a first axis to coo so it becomes broadcastable against targets:
(coo[:,None] == targets)

array([[[False, False],
        [ True, False]],

       [[False, False],
        [ True,  True]],

       [[ True,  True],
        [False, False]],

       [[False, False],
        [False,  True]]])

Then check which ndarrays along the second axis have all values to True:
(coo[:,None] == targets).all(2)

array([[False, False],
       [False,  True],
       [ True, False],
       [False, False]])

And finally use any to check which rows have at least one True.
",numpy
how to create a dataframe from gcms data with retention time mass values and intensities in python with correct alignment,"I am processing gas chromatography - mass spectroscopy (GC-MS) data stored in a NetCDF file in Python, and I’m having trouble aligning the data correctly to create a structured DataFrame. My goal is to build a matrix where:
Rows: Retention times (from scan_acquisition_time).
Columns: Rounded mass values (from mass_values).
Cells: Intensity values (from intensity_values) corresponding to the retention time and mass.
The problem that I am facing now is that for the life of me, I cannot understand why some of the intensity values are not aligned/positioned in the correct order in the dataframe. I know that they are not positioned correctly because I have a reference of the same dataframe (output from another programming software which has been verified to be correct).
The placement of intensity values in the dataframe is very important for what I am trying to do; and I have tried different ways such us doing a pivot table or by chunking the intensity values according to the size of mass values, but all of the methods has failed so far.
this was how I extracted the raw data:
`    import xarray as xr
import pandas as pd
import numpy as np
 # Extract necessary data
 retention_time = data['scan_acquisition_time'].values.squeeze()
 intensity_values = data['intensity_values'].values.squeeze()
 mass_values = data['mass_values'].values.squeeze()
 scan_index = data['scan_index'].values.squeeze()
 rounded_mass = np.round(mass_values).astype(int)  # Round mass values to integers
 mass_min = np.round(min(mass_values)).astype(int)
 mass_max = np.round(max(mass_values)).astype(int)
 ideal_mass_values = list(range(mass_min, mass_max + 1))

`
the following is an example of my data structure:
Shapes
#retention times per scan
scan_acquisition_time: (4825,)
#Starting indices for scan's data
scan_index: (4825,)
#mass to charge ratio
mass_values: (2903174,)
#Intensities corresponding to each mass
intensity_values: (2903174,)
sample data
scan_acquisition_time = np.array([ 5.903  6.546  7.188  7.83   8.472  9.115  9.757 10.399 11.041 11.684  12.326 12.968 13.61  14.253 4.895 15.537 16.18  16.822 17.464 18.106 18.749 19.391 20.033 20.675 1.318 21.96  22.602 23.244 23.887 24.529 25.171 25.813 26.456 27.098 27.74  28.383 29.025 29.667 30.309 30.952 31.594 32.236 32.878 33.521 4.163 34.805 35.447 36.09  36.732 37.374])
scan_index = np.array([ 0 624 1249 1878 2492 3127 3742 4366 4982 5618 6246 6863 7469 8098 8715 9320 9936 10565 11191 11821 12449 13061 13680 14316 14931 15573 16208 16824 17448 18055 18680 19301 19928 20551 21186 21794 22419 23051 23674 24295 24919 25543 26164 26786 27414 28041 28664 29285 29904 30517])
mass_values = np.array([19.79999924 20.29999924 20.89999962 1.60000038 22.29999924 22.79999924 23.20000076 24.20000076 24.89999962 26.        27.20000076 28.10000038 29.         29.79999924 31.20000076 32.09999847 32.90000153 33.29999924
34.09999847 35.         35.40000153 36.         36.79999924 37.79999924 38.79999924 40.09999847 41.20000076 41.90000153
43.         44.         45.29999924 46.09999847 47.09999847 47.90000153 48.79999924 50.         50.29999924 51.         52.20000076 52.90000153 53.90000153 54.29999924 55.         56.09999847 56.79999924 57.09999847 58.09999847 59.09999847
60.20000076 60.79999924])
intensity_values = np.array([ 506. 574. 465. 411. 412. 562. 590.   598. 541. 1480. 762. 63016.   726. 576. 799. 23904.  98. 246. 471.   216. 291. 220.  222. 674.     887.  2035.  1982.   631.   501.  8382. 712.   469.   520.   743.   290.   585.   568.  1137.   907.   763.  236.   191.   811.   556.   340.   348.   425.   354.   327.   430.])
and then to build the dataframe, I did this:
 if 'point_count' in data.variables:
     point_count = data['point_count'].values.squeeze()
 else:
     raise ValueError(""The dataset does not have the 'point_count' variable,    necessary to map scans to intensity data"")

 # Repeat retention times for each point in each scan
 retention_time_repeated = np.repeat(retention_time, point_count)

 # Ensure lengths match
 assert len(retention_time_repeated) == len(intensity_values), ""Mismatch in  retention_time and intensity_values length""

 # Get unique retention times and define matrix dimensions
 unique_retention_times, inverse_indices = np.unique(retention_time_repeated, return_inverse=True)
 unique_masses = ideal_mass_values

 # Initialize a zero-filled intensity matrix
 intensity_matrix = np.zeros((len(unique_retention_times), len(unique_masses)))

 # Create mass index mapping
 mass_to_index = {mass: j for j, mass in enumerate(unique_masses)}

 # Get indices for the intensity matrix
 mass_indices = np.array([mass_to_index[mass] for mass in rounded_mass])

 # Populate the intensity matrix in a vectorized manner
 intensity_matrix[inverse_indices, mass_indices] += intensity_values

 # Convert the matrix to a DataFrame for easier inspection
 matrix_df = pd.DataFrame(
     intensity_matrix,
     index=unique_retention_times,
     columns=unique_masses
 )

 # Display part of the matrix for verification
 print(matrix_df.head())

`
My questions:

How can I ensure that intensities are correctly aligned with retention times and rounded mass values?
Should I incorporate scan_index to help with the alignment? If so, how should I approach it?
Are there alternative, better ways to structure or process this data in Python to avoid alignment issues?

Is there anything else I can do? Please help. Thank you :')
For reference, this is what I am trying to get:
Intensity matrix for sample.CDF:

I managed to get it out but like mentioned, some of the values came out differently.
","In your approach retention_time_repeated is created using np.repeat. This means that you are assuming a perfect match between retention times and the intensity values. But this is not the case. The actual mapping depends on scan_index which determines where each scan's data starts and ends. So, you need to calculate rounded_mass using np.round and ensure a consistent rounding method and have an explicit mapping mass_to_index. This is created to align each rounded mass value with a specific column in the intensity matrix. When populating the matrix use mass_to_index to place each intensity value into the correct column, ensuring alignment with the rounded masses.
THis can be done this way (here I created sample data since you didn't provide any):
import numpy as np
import pandas as pd

sample_data = {
    'scan_acquisition_time': np.array([1.0, 1.1, 1.2, 1.3, 1.4]), 
    'scan_index': np.array([0, 4, 9, 14, 18, 21]), 
    'mass_values': np.array([50.1, 51.2, 52.0, 53.3, 50.0, 51.1, 52.2, 54.1, 55.0, 50.2, 
                             51.5, 52.3, 53.8, 54.6, 50.3, 51.0, 52.1, 53.7, 50.5, 52.0, 53.0]),
    'intensity_values': np.array([100, 200, 150, 50, 120, 180, 160, 90, 80, 110, 
                                  210, 140, 70, 60, 130, 190, 170, 100, 125, 175, 155])
}

retention_time = sample_data['scan_acquisition_time']
scan_index = sample_data['scan_index']

rounded_mass = np.round(sample_data['mass_values']).astype(int)
intensity_values = sample_data['intensity_values']

mass_min = np.min(rounded_mass)
mass_max = np.max(rounded_mass)
ideal_mass_values = list(range(mass_min, mass_max + 1))

intensity_matrix = np.zeros((len(retention_time), len(ideal_mass_values)))

mass_to_index = {mass: i for i, mass in enumerate(ideal_mass_values)}

for i in range(len(scan_index) - 1):  
    start_idx = scan_index[i]
    end_idx = scan_index[i + 1]
    current_masses = rounded_mass[start_idx:end_idx]
    current_intensities = intensity_values[start_idx:end_idx]
    
    for m, intensity in zip(current_masses, current_intensities):
        col_idx = mass_to_index.get(m, None)
        if col_idx is not None:  
            intensity_matrix[i, col_idx] += intensity

last_start_idx = scan_index[-1]
current_masses = rounded_mass[last_start_idx:]
current_intensities = intensity_values[last_start_idx:]
for m, intensity in zip(current_masses, current_intensities):
    col_idx = mass_to_index.get(m, None)
    if col_idx is not None:
        intensity_matrix[-1, col_idx] += intensity

matrix_df = pd.DataFrame(
    intensity_matrix,
    index=retention_time,
    columns=ideal_mass_values
)

print(matrix_df.head())


which I believe returns what you would expect with your actual data:
        50     51     52     53     54    55
1.0  100.0  200.0  150.0   50.0    0.0   0.0
1.1  120.0  180.0  160.0    0.0   90.0  80.0
1.2  110.0    0.0  350.0    0.0   70.0  60.0
1.3  130.0  190.0  170.0    0.0  100.0   0.0
1.4  125.0    0.0  175.0  155.0    0.0   0.0

EDIT: SOLUTION WITH YOUR DATA:
import numpy as np
import pandas as pd

scan_acquisition_time = np.array([5.903, 6.546, 7.188, 7.83, 8.472, 9.115, 9.757, 10.399, 11.041, 11.684, 12.326, 12.968, 13.61, 14.253, 4.895, 15.537, 16.18, 16.822, 17.464, 18.106, 18.749, 19.391, 20.033, 20.675, 1.318, 21.96, 22.602, 23.244, 23.887, 24.529, 25.171, 25.813, 26.456, 27.098, 27.74, 28.383, 29.025, 29.667, 30.309, 30.952, 31.594, 32.236, 32.878, 33.521, 4.163, 34.805, 35.447, 36.09, 36.732, 37.374])

scan_index = np.array([0, 624, 1249, 1878, 2492, 3127, 3742, 4366, 4982, 5618, 6246, 6863, 7469, 8098, 8715, 9320, 9936, 10565, 11191, 11821, 12449, 13061, 13680, 14316, 14931, 15573, 16208, 16824, 17448, 18055, 18680, 19301, 19928, 20551, 21186, 21794, 22419, 23051, 23674, 24295, 24919, 25543, 26164, 26786, 27414, 28041, 28664, 29285, 29904, 30517])

mass_values = np.array([19.79999924, 20.29999924, 20.89999962, 1.60000038, 22.29999924, 22.79999924, 23.20000076, 24.20000076, 24.89999962, 26., 27.20000076, 28.10000038, 29., 29.79999924, 31.20000076, 32.09999847, 32.90000153, 33.29999924, 34.09999847, 35., 35.40000153, 36., 36.79999924, 37.79999924, 38.79999924, 40.09999847, 41.20000076, 41.90000153, 43., 44., 45.29999924, 46.09999847, 47.09999847, 47.90000153, 48.79999924, 50., 50.29999924, 51., 52.20000076, 52.90000153, 53.90000153, 54.29999924, 55., 56.09999847, 56.79999924, 57.09999847, 58.09999847, 59.09999847, 60.20000076, 60.79999924])

intensity_values = np.array([506., 574., 465., 411., 412., 562., 590., 598., 541., 1480., 762., 63016., 726., 576., 799., 23904., 98., 246., 471., 216., 291., 220., 222., 674., 887., 2035., 1982., 631., 501., 8382., 712., 469., 520., 743., 290., 585., 568., 1137., 907., 763., 236., 191., 811., 556., 340., 348., 425., 354., 327., 430.])

rounded_mass = np.round(mass_values).astype(int)
mass_min = np.min(rounded_mass)
mass_max = np.max(rounded_mass)
ideal_mass_values = list(range(mass_min, mass_max + 1))

intensity_matrix = np.zeros((len(scan_acquisition_time), len(ideal_mass_values)))
mass_to_index = {mass: i for i, mass in enumerate(ideal_mass_values)}

for i in range(len(scan_index) - 1):
    start_idx = scan_index[i]
    end_idx = scan_index[i + 1]
    current_masses = rounded_mass[start_idx:end_idx]
    current_intensities = intensity_values[start_idx:end_idx]
    
    for m, intensity in zip(current_masses, current_intensities):
        col_idx = mass_to_index.get(m, None)
        if col_idx is not None:
            intensity_matrix[i, col_idx] += intensity

last_start_idx = scan_index[-1]
current_masses = rounded_mass[last_start_idx:]
current_intensities = intensity_values[last_start_idx:]
for m, intensity in zip(current_masses, current_intensities):
    col_idx = mass_to_index.get(m, None)
    if col_idx is not None:
        intensity_matrix[-1, col_idx] += intensity

matrix_df = pd.DataFrame(
    intensity_matrix,
    index=scan_acquisition_time,
    columns=ideal_mass_values
)

which gives
          2    3    4    5    6    7    8    9    10   11  ...     52     53  \
5.903  411.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  907.0  763.0   
6.546    0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...    0.0    0.0   
7.188    0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...    0.0    0.0   
7.830    0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...    0.0    0.0   
8.472    0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...    0.0    0.0   

          54     55     56     57     58     59     60     61  
5.903  427.0  811.0  556.0  688.0  425.0  354.0  327.0  430.0  
6.546    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0  
7.188    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0  
7.830    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0  
8.472    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0  


",numpy
converting an array of floats into rgba values in an efficient way,"I am trying to create a system to take an array of floats which range from 0.0 to 1.0 and convert them into RGBA values based on a lookup table. The output should be an array that is one dimension larger that the input with the last dimension being size 4 and consisting of the RGBA values.
Currently I have only been able to do this via loops. Dose anyone know of any numpy indexing methods that could achieve this same result more efficiently.
import numpy as np
import matplotlib.pyplot as plt

cyan = np.array([(x*0,x*1,x*1,255) for x in range(256)])

input_array = np.arange(0,0.8,0.05).reshape(4,4)

input_array = input_array*256

colour_array = []
for x in range(input_array.shape[0]):
    for y in range(input_array.shape[1]):
        colour_array.append(cyan[int(input_array[x,y])])
        
colour_array = np.array(colour_array).reshape(4,4,4)

plt.imshow(colour_array) 


","Use the following:
shape = input_array.shape
index = input_array[*np.indices(shape).reshape(2, -1)].astype(int)
colour_array1 = cyan[index].reshape(4, *shape)


Confirm the two are equal:
np.allclose(colour_array, colour_array1,atol=0)
Out[62]: True


USE THE OTHER SOLUTION!!!
",numpy
how do i shuffle a specific 2d cell along the zaxis in a numpy 3d matrix,"Let's say I have a 3D numpy matrix M with shape (c, b, a). I want to shuffle a specific cell (x, y) in M along the z-axis --- that is, I want to shuffle the array [M[z][y][x] for z in range(c)] and assign it back (somehow, I have no idea how that would work --- maybe with M[:, y, x]?)
I can think of one way to do this: Flatten each 2D matrix, so M1 is a 2D matrix; transpose M1 to M2, shuffle M2[x+y*a], transpose M2 back to M1, then reform the 2D matrices.
However, this is clearly clunky. Is there a cleaner way to do this?
","I found a solution: np.random.shuffle(M[:, y, x]). Note that this modifies M (you can make a copy by first executing M1=copy.deecopy(M) before shuffling).
",numpy
how to get all array edges,"I have a n x n array, and want to receive its outline values. For example,
[4,5,6,7]
[2,2,6,3]
[4,4,9,4]
[8,1,6,1]
from this, i would get this
[4,5,6,7,3,4,1,6,1,8,4,2]

(see where bold)
So essentially, what is the most efficient way of getting a 1D array of all the values going around the edges of a 2D array?
I ask because I assume there is a numPy function that helps with this (which I haven't yet found!), instead of doing it manually with loops?
","In [1]: arr=np.arange(16).reshape(4,4)
In [2]: arr
Out[2]: 
array([[ 0,  1,  2,  3],
       [ 4,  5,  6,  7],
       [ 8,  9, 10, 11],
       [12, 13, 14, 15]])

A relatively straight forward way of doing this - in clockwise order is:
In [5]: alist=[arr[0,:-1], arr[:-1,-1], arr[-1,::-1], arr[-2:0:-1,0]]
In [6]: alist
Out[6]: [array([0, 1, 2]), array([ 3,  7, 11]), array([15, 14, 13, 12]), array([8, 4])]
In [7]: np.concatenate(alist)
Out[7]: array([ 0,  1,  2,  3,  7, 11, 15, 14, 13, 12,  8,  4])

In a sense it's a loop, in that I have to build 4 slices.  But if 4 is small compared to n, that's a small price.  It has to concatenate at some level.
If order doesn't matter we could simplify the slices some (e.g. forgetting the reverse order, etc).
alist=[arr[0,:], arr[1:,-1], arr[-1,:-1], arr[1:-1,0]]

If I didn't care about order, or double counting the corners I could use:
np.array([arr[[0,n],:], arr[:,[0,n]].T]).ravel()

eliminating the duplicate corners
In [18]: np.concatenate((arr[[0,n],:].ravel(), arr[1:-1,[0,n]].ravel()))
Out[18]: array([ 0,  1,  2,  3, 12, 13, 14, 15,  4,  7,  8, 11])

",numpy
convert a list of dictionaries to a table numpy matrix,"Let the given dictionaries are 
d = [{'a':1,'b':4},{'b':2}]

So basically I want a matrix like this
  | 'a' | 'b'  |
 _______________
  |  1  |  4   |
  |  na |  2   |

How can I efficiently achieve this ? 
","The Pandas DataFrame constructor will immediately give you the result you are looking for:
import pandas as pd
pd.DataFrame(d).values

The .values part on the end converts the result to a NumPy array, which is what you asked for.  Some people would just work directly with the DataFrame instead.
",numpy
how can i solve error quotmodule 39numpy39 has no attribute 39float39quot in python,"I am using NumPy 1.24.0.
On running this sample code line,
import numpy as np
num = np.float(3)

I am getting this error:
Traceback (most recent call last):   File ""<stdin>"", line 1, in <module>   File ""/home/ubuntu/.local/lib/python3.8/site-packages/numpy/__init__.py"", line 284, in __getattr__
    raise AttributeError(""module {!r} has no attribute "" AttributeError: module 'numpy' has no attribute 'float'

How can I fix it?
","The answer is already provided in the comments by @mattdmo and @tdelaney:

NumPy 1.20 (release notes) deprecated numpy.float, numpy.int, and similar aliases, causing them to issue a deprecation warning

NumPy 1.24 (release notes) removed these aliases altogether, causing an error when they are used


In many cases you can simply replace the deprecated NumPy types by the equivalent Python built-in type, e.g., numpy.float becomes a ""plain"" Python float.
For detailed guidelines on how to deal with various deprecated types, have a closer look at the table and guideline in the release notes for 1.20:

...
To give a clear guideline for the vast majority of cases, for the types bool, object, str (and unicode) using the plain version is shorter and clear, and generally a good replacement. For float and complex you can use float64 and complex128 if you wish to be more explicit about the precision.
For np.int a direct replacement with np.int_ or int is also good and will not change behavior, but the precision will continue to depend on the computer and operating system. If you want to be more explicit and review the current use, you have the following alternatives:

np.int64 or np.int32 to specify the precision exactly. This ensures that results cannot depend on the computer or operating system.
np.int_ or int (the default), but be aware that it depends on the computer and operating system.
The C types: np.cint (int), np.int_ (long), np.longlong.
np.intp which is 32bit on 32bit machines 64bit on 64bit machines. This can be the best type to use for indexing.

...

If you have dependencies that use the deprecated types, a quick workaround would be to roll back your NumPy  version to 1.24 or less (as suggested in some of the other answers), while waiting for the dependency to catch up. Alternatively, you could create a patch yourself and open a pull request, or monkey patch the dependency in your own code.
",numpy
calculate weighted average using pandas groupby and deal with nan,"I'm trying to calculate the weighted average using groupby. However, I'm dealing with empty value in my dataframe.
df = pd.DataFrame({
    'group': ['a', 'a', 'a', 'a'],
    'x': [10, 20, np.nan, 20],
    'weight_x': [10, 15, np.nan, 25],
    'y': [25, 35, 45, np.nan],
    'weight_y': [10, 20, 10, np.nan]
})

This is my groupby function:
summary = (
    df
    .groupby(['group'])
    .apply(
        lambda x: pd.Series([
            np.average(x['x'], weights=x['weight_x']),
            np.average(x['y'], weights=x['weight_y'])
        ], index=['wt_avg_x', 'wt_avg_y'])
    )
    .reset_index()
)

This one gives the following output:
  group  wt_avg_x  wt_avg_y
0     a       NaN       NaN

However, expected output should be the following:
  group  wt_avg_x  wt_avg_y
0     a      20.5        35

I've tried this solution:
summary = (
    df.dropna(subset=['x', 'y', 'weight_x', 'weight_y'])
    .groupby(['group'])
    .apply(
        lambda x: pd.Series([
            np.average(x['x'], weights=x['weight_x']),
            np.average(x['y'], weights=x['weight_y'])
        ], index=['wt_avg_x', 'wt_avg_y'])
    )
    .reset_index()
)

But dropna drops the whole line. How could I ignore nan values only in the np.average function?
","You could also use the following:
def nan_average(x, cols = ['x', 'y']):
    vals = x[cols]
    weights = x[[f'weight_{c}' for c in cols]].to_numpy()
    return np.sum(vals*weights,0)/weights.sum(0,where=~np.isnan(vals.to_numpy()))

df.groupby('group').apply(nan_average).reset_index()
  group     x     y
0     a  18.0  35.0


Another way:
a = df.rename(columns = {'x':'val_x', 'y':'val_y'}).reset_index()

(pd.wide_to_long(a, ['val','weight'], ['index', 'group'], '', '_', '\\w+')
    .groupby(level=[1,2]).apply(lambda x: np.nansum(x.val * x.weight)/
            x.weight.to_numpy().sum(where=~np.isnan(x.val.to_numpy())))
    .unstack().reset_index())

  group     x     y
0     a  18.0  35.0

",numpy
pandas rename multiple columns using regex pattern,"I have a dataframe like as shown below
ID,US-Test1,US-Test2,US-Test3
1,11,12,13
2,13,16,18
3,15,19,21

I would like to remove the keyword US -  from all my column names
I tried the below but there should be better way to do this
newNames = {
    'US-Test1':'Test1',
    'US-Test2':'Test2'
}
df.rename(columns=newNames,inplace=True)

But my real data has 70 plus columns and this is not efficient.
Any regex approach to rename columns based on regex to exclude the pattern and retain only what I want?
I expect my output to be like as shown below
ID,Test1,Test2,Test3
1,11,12,13
2,13,16,18
3,15,19,21

","You could use a regex that matches the ""US-"" at the beginning like this:
df.columns = df.columns.str.replace(""^US-"", """", regex=True)

It replaces the matching ""US-"" with an empty string.
Also, if you know the columns that you want to transform you could apply slicing on their names to remove the first 3 characters:
df.columns = df.columns.str.slice(3)

Of course, this will affect columns that do not match your condition (i.e. do not begin with ""US-"")
",numpy
huffman implementation reduce decompression time,"I'm trying to improve the computational speed of this huffman. For small input hex strings its fine but the bigger the input string is the time increments considerably with a large enough string speed (example below) goes up to x50 1ms vs 55ms+
import time
from concurrent.futures import ThreadPoolExecutor
from typing import List, Tuple, Optional
import numpy as np
from array import array
import ctypes
from line_profiler._line_profiler import byteorder

class Node:
    __slots__ = ['char', 'freq', 'left', 'right']

    def __init__(self, char: str, freq: int, left=None, right=None):
        self.char = char
        self.freq = freq
        self.left = left
        self.right = right




class HybridLookupTable:
    
""""""Hybrid approach combining direct lookup for short codes and binary search for long codes""""""
    
__slots__ = ['short_table', 'long_codes', 'max_short_bits']

    def __init__(self, max_short_bits: int = 8):
        self.max_short_bits = max_short_bits
        self.short_table = [(None, 0)] * (1 << max_short_bits)  # Changed to tuple list for safety
        self.long_codes = {}

    def add_code(self, code: str, char: str) -> None:
        code_int = int(code, 2)
        code_len = len(code)

        if code_len <= self.max_short_bits:
            # For short codes, use lookup table with limited prefix expansion
            prefix_mask = (1 << (self.max_short_bits - code_len)) - 1
            base_index = code_int << (self.max_short_bits - code_len)
            for i in range(prefix_mask + 1):
                self.short_table[base_index | i] = (char, code_len)
        else:
            # For long codes, store in dictionary
            self.long_codes[code_int] = (char, code_len)

    def lookup(self, bits: int, length: int) -> Optional[Tuple[str, int]]:
        
""""""Look up a bit pattern and return (character, code length) if found""""""
        
if length <= self.max_short_bits:
            return self.short_table[bits & ((1 << self.max_short_bits) - 1)]

        # Try matching long codes
        for code_bits, (char, code_len) in self.long_codes.items():
            if code_len <= length:
                mask = (1 << code_len) - 1
                if (bits >> (length - code_len)) == (code_bits & mask):
                    return (char, code_len)
        return None
class BitBuffer:
    
""""""Fast bit buffer implementation using ctypes""""""
    
__slots__ = ['buffer', 'bits_in_buffer']

    def __init__(self):
        self.buffer = ctypes.c_uint64(0)
        self.bits_in_buffer = 0
    def add_byte(self, byte: int) -> None:
        self.buffer.value = (self.buffer.value << 8) | byte
        self.bits_in_buffer += 8
    def peek_bits(self, num_bits: int) -> int:
        return (self.buffer.value >> (self.bits_in_buffer - num_bits)) & ((1 << num_bits) - 1)

    def consume_bits(self, num_bits: int) -> None:
        self.buffer.value &= (1 << (self.bits_in_buffer - num_bits)) - 1
        self.bits_in_buffer -= num_bits


class ChunkDecoder:
    
""""""Decoder for a chunk of compressed data""""""
    
__slots__ = ['lookup_table', 'tree', 'chunk_size']

    def __init__(self, lookup_table, tree, chunk_size=1024):
        self.lookup_table = lookup_table
        self.tree = tree
        self.chunk_size = chunk_size

    def decode_chunk(self, data: memoryview, start_bit: int, end_bit: int) -> Tuple[List[str], int]:
        
""""""Decode a chunk of bits and return (decoded_chars, bits_consumed)""""""
        
result = []
        pos = start_bit
        buffer = BitBuffer()
        bytes_processed = start_bit >> 3
        bit_offset = start_bit & 7
        # Pre-fill buffer
        for _ in range(8):
            if bytes_processed < len(data):
                buffer.add_byte(data[bytes_processed])
                bytes_processed += 1
        # Skip initial bit offset
        if bit_offset:
            buffer.consume_bits(bit_offset)

        while pos < end_bit and buffer.bits_in_buffer >= 8:
            # Try lookup table first (optimized for 8-bit codes)
            lookup_bits = buffer.peek_bits(8)
            char_info = self.lookup_table.lookup(lookup_bits, 8)

            if char_info:
                char, code_len = char_info
                buffer.consume_bits(code_len)
                result.append(char)
                pos += code_len
            else:
                # Fall back to tree traversal
                node = self.tree
                while node.left and node.right and buffer.bits_in_buffer > 0:
                    bit = buffer.peek_bits(1)
                    buffer.consume_bits(1)
                    node = node.right if bit else node.left
                    pos += 1
                if not (node.left or node.right):
                    result.append(node.char)

            # Refill buffer if needed
            while buffer.bits_in_buffer <= 56 and bytes_processed < len(data):
                buffer.add_byte(data[bytes_processed])
                bytes_processed += 1
        return result, pos - start_bit


class OptimizedHuffmanDecoder:
    def __init__(self, num_threads=4, chunk_size=1024):
        self.tree = None
        self.freqs = {}
        self.lookup_table = HybridLookupTable()
        self.num_threads = num_threads
        self.chunk_size = chunk_size
        self._setup_lookup_tables()

    def _setup_lookup_tables(self):
        # Pre-calculate bit manipulation tables
        self.bit_masks = array('Q', [(1 << i) - 1 for i in range(65)])
        self.bit_shifts = array('B', [x & 7 for x in range(8)])

    def _build_efficient_tree(self) -> None:
        # Use list-based heap instead of sorting
        nodes = [(freq, i, Node(char, freq)) for i, (char, freq) in enumerate(self.freqs.items())]

        # Convert to min-heap
        nodes.sort(reverse=True)  # Sort once at the beginning
        while len(nodes) > 1:
            freq1, _, node1 = nodes.pop()
            freq2, _, node2 = nodes.pop()

            # Create parent node
            parent = Node(node1.char + node2.char, freq1 + freq2, node1, node2)
            nodes.append((freq1 + freq2, len(nodes), parent))
            nodes.sort(reverse=True)

        self.tree = nodes[0][2] if nodes else None
        self._build_codes(self.tree)

    def _build_codes(self, node: Node, code: str = '') -> None:
        
""""""Build lookup table using depth-first traversal""""""
        
if not node:
            return
        if not node.left and not node.right:
            if code:  # Never store empty codes
                self.lookup_table.add_code(code, node.char)
            return
        if node.left:
            self._build_codes(node.left, code + '0')
        if node.right:
            self._build_codes(node.right, code + '1')

    def _parse_header_fast(self, data: memoryview) -> int:
        
""""""Optimized header parsing""""""
        
pos = 12  # Skip first 12 bytes (file_len, always0, chars_count)
        chars_count = int.from_bytes(data[8:12], byteorder)

        # Pre-allocate dictionary space
        self.freqs = {}
        self.freqs.clear()

        # Process all characters in a single loop
        for _ in range(chars_count):
            count = int.from_bytes(data[pos:pos + 4], byteorder)
            char = chr(data[pos + 4])  # Faster than decode
            self.freqs[char] = count
            pos += 8
        return pos

    def _decode_bits_parallel(self, data: memoryview, total_bits: int) -> str:
        
""""""Parallel decoding using multiple threads""""""
        
chunk_bits = (total_bits + self.num_threads - 1) // self.num_threads
        chunks = []

        # Create chunks ensuring they align with byte boundaries when possible
        for i in range(0, total_bits, chunk_bits):
            end_bit = min(i + chunk_bits, total_bits)
            if i > 0:
                # Align to byte boundary when possible
                while (i & 7) != 0 and i > 0:
                    i -= 1
            chunks.append((i, end_bit))

        # Create decoders for each thread
        decoders = [
            ChunkDecoder(self.lookup_table, self.tree, self.chunk_size)
            for _ in range(len(chunks))
        ]

        # Process chunks in parallel
        with ThreadPoolExecutor(max_workers=self.num_threads) as executor:
            futures = [
                executor.submit(decoder.decode_chunk, data, start, end)
                for decoder, (start, end) in zip(decoders, chunks)
            ]

            # Collect results
            results = []
            for future in futures:
                chunk_result, _ = future.result()
                results.extend(chunk_result)

        return ''.join(results)

    def _decode_bits_optimized(self, data: memoryview, total_bits: int) -> str:
        
""""""Optimized single-threaded decoding for small inputs""""""
        
if total_bits > self.chunk_size:
            return self._decode_bits_parallel(data, total_bits)

        result = []
        buffer = BitBuffer()
        pos = 0
        bytes_processed = 0
        # Pre-fill buffer
        while bytes_processed < min(8, len(data)):
            buffer.add_byte(data[bytes_processed])
            bytes_processed += 1
        while pos < total_bits:
            # Use lookup table for common patterns
            if buffer.bits_in_buffer >= 8:
                lookup_bits = buffer.peek_bits(8)
                char_info = self.lookup_table.lookup(lookup_bits, 8)

                if char_info:
                    char, code_len = char_info
                    buffer.consume_bits(code_len)
                    result.append(char)
                    pos += code_len
                else:
                    # Tree traversal for uncommon patterns
                    node = self.tree
                    while node.left and node.right and buffer.bits_in_buffer > 0:
                        bit = buffer.peek_bits(1)
                        buffer.consume_bits(1)
                        node = node.right if bit else node.left
                        pos += 1
                    if not (node.left or node.right):
                        result.append(node.char)

            # Refill buffer
            while buffer.bits_in_buffer <= 56 and bytes_processed < len(data):
                buffer.add_byte(data[bytes_processed])
                bytes_processed += 1
            if buffer.bits_in_buffer == 0:
                break
        return ''.join(result)

    def decode_hex(self, hex_string: str) -> str:
        # Use numpy for faster hex decoding
        clean_hex = hex_string.replace(' ', '')
        data = np.frombuffer(bytes.fromhex(clean_hex), dtype=np.uint8)
        return self.decode_bytes(data.tobytes())

    def decode_bytes(self, data: bytes) -> str:
        view = memoryview(data)
        pos = self._parse_header_fast(view)

        self._build_efficient_tree()

        # Get packed data info using numpy for faster parsing
        header = np.frombuffer(data[pos:pos + 12], dtype=np.uint32)
        packed_bits = int(header[0])
        packed_bytes = int(header[1])
        pos += 12
        # Choose decoding method based on size
        if packed_bits > self.chunk_size:
            return self._decode_bits_parallel(view[pos:pos + packed_bytes], packed_bits)
        else:
            return self._decode_bits_optimized(view[pos:pos + packed_bytes], packed_bits)

    def encode(self, text: str) -> bytes:
        
""""""Encode text using Huffman coding - for testing purposes""""""
        
# Count frequencies
        self.freqs = {}
        for char in text:
            self.freqs[char] = self.freqs.get(char, 0) + 1
        # Build tree and codes
        self._build_efficient_tree()

        # Convert text to bits
        bits = []
        for char in text:
            code = self.lookup_table.get_code(char)
            bits.extend(code)

        # Pack bits into bytes
        packed_bytes = []
        for i in range(0, len(bits), 8):
            byte = 0
            for j in range(min(8, len(bits) - i)):
                if bits[i + j]:
                    byte |= 1 << (7 - j)
            packed_bytes.append(byte)

        # Create header
        header = bytearray()
        header.extend(len(text).to_bytes(4, byteorder))
        header.extend(b'\x00' * 4)  # always0
        header.extend(len(self.freqs).to_bytes(4, byteorder))

        # Add frequency table
        for char, freq in self.freqs.items():
            header.extend(freq.to_bytes(4, byteorder))
            header.extend(char.encode('ascii'))
            header.extend(b'\x00' * 3)  # padding
        # Add packed data info
        header.extend(len(bits).to_bytes(4, byteorder))
        header.extend(len(packed_bytes).to_bytes(4, byteorder))
        header.extend(b'\x00' * 4)  # unpacked_bytes
        # Combine header and packed data
        return bytes(header + bytes(packed_bytes))

if __name__ == '__main__':
    # Create decoder with custom settings
    decoder = OptimizedHuffmanDecoder(
        num_threads=4,  # Number of threads for parallel processing
        chunk_size=1024  # Minimum size for parallel processing
    )

    test_hex = 'A7 64 00 00 00 00 00 00 0C 00 00 00 38 25 00 00 2D 00 00 00 08 69 00 00 30 00 00 00 2E 13 00 00 31 00 00 00 D4 13 00 00 32 00 00 00 0F 0D 00 00 33 00 00 00 78 08 00 00 34 00 00 00 A4 0A 00 00 35 00 00 00 63 0E 00 00 36 00 00 00 AC 09 00 00 37 00 00 00 D0 07 00 00 38 00 00 00 4D 09 00 00 39 00 00 00 68 0C 00 00 7C 00 00 00 73 21 03 00 2F 64 00 00 01 0B 01 00 C9 63 2A C7 21 77 40 77 25 8D AB E9 E5 E7 80 77'
    start_time = time.perf_counter()
    # Decode data
    result = decoder.decode_hex(test_hex)
    execution_time_ms = (time.perf_counter() - start_time) * 1000  # Convert to milliseconds
    print(f""\nTotal execution time: {execution_time_ms:.2f} milliseconds"")
    print(result)

expected output:
Total execution time: 1.04 milliseconds
19101-0-418-220000000|19102-0-371-530000000
But if you try with a bigger string it gets extremely slow, id like to improve the performance i tried cythoning it but didnt improve it by any mean, if anyone has any idea of what i can be doing wrong
With this second input hex it takes 55ms
bigger hex input example
text
I'm wondering if I'm doing anything bad and theres any way of speeding the process up, I tried for hours everything that come to my mind and I'm not sure how to improve further.
","
I'd like to improve the performance

For virtually every performance question, the answer is:
Profile Your Program
If you haven't profiled, you can't be sure where the slowness is coming from. If you don't know what's slow, you can only speculate about how to make it faster.
Python has profiling tools built in. Give them a try, and/or use timeit for micro benchmarks.
Also look at moving things you don't want to profile (eg. hex string conversion) outside the part you're timing.
Finally, you may well be able to get much better performance in C, or C++, or Rust or some other compiled language - but you'll need to learn how to profile those too, to get the best out of them.
",numpy
is there a faster way to solve time interval comparison in pandas,"i am trying to implement a solution that finds consequential alarms for an alarm within the given timeframe. i have created an intervalIndex from the dataframe and used np.vectorize to compare timestamps to the interval index but it is slow for large datasets (350000 rows). is there a way to make this faster, i am not sure how to truly vectorize the operation.
def find_top_consequential_alarms(
    ui: frontPage.Application, data: pd.DataFrame, fromAr: bool
):

    # Get selected alarm and time window based on the source
    selected_alarm = (
        ui.select_conc_alarm.currentText()
        if not fromAr
        else ui.ar_alarm_combo.currentText()
    )
    selected_time = (
        ui.select_conc_time_window.currentText()
        if not fromAr
        else ui.ar_timewindow_combo.currentText()
    )

    # Convert selected time to timedelta
    time_map = {
        ""5 Minutes"": timedelta(minutes=5),
        ""10 Minutes"": timedelta(minutes=10),
        ""15 Minutes"": timedelta(minutes=15),
        ""30 Minutes"": timedelta(minutes=30),
        ""1 Hour"": timedelta(hours=1),
        ""5 Hours"": timedelta(hours=5),
        ""24 Hours"": timedelta(hours=24),
    }
    time_window = time_map[selected_time]

    # Filter for occurrences of the selected alarm
    specific_alarms = data[data[""PtName""].str.strip() == selected_alarm].copy()

    # Create time intervals for each specific alarm occurrence
    specific_alarms[""window_start""] = specific_alarms[""TimestampUTC""]
    specific_alarms[""window_end""] = specific_alarms[""TimestampUTC""] + time_window
    intervals = pd.IntervalIndex.from_arrays(
        specific_alarms[""window_start""], specific_alarms[""window_end""], closed=""right""
    )

    # Filter for other alarms that fall within any of the intervals
    other_alarms = data[data[""PtName""] != selected_alarm].copy()
    in_interval = np.vectorize(lambda x: intervals.contains(x).any(), otypes=[bool])
    mask = in_interval(other_alarms[""TimestampUTC""])
    consequential_alarms = other_alarms[mask]

    # Count the occurrences of each alarm within the time windows and get the top 10
    consequential_alarm_counts = consequential_alarms[""PtName""].value_counts().head(10)

    title = f""Top 10 Consequential Alarms for {selected_alarm}\n(Time Window: {time_window})""
    return consequential_alarm_counts, title

the overall function is above
# Filter for other alarms that fall within any of the intervals
other_alarms = data[data[""PtName""] != selected_alarm].copy()
in_interval = np.vectorize(lambda x: intervals.contains(x).any(), otypes=[bool])
mask = in_interval(other_alarms[""TimestampUTC""])
consequential_alarms = other_alarms[mask]

this is the part where that needs improvements.
any help would be greatly appreciated.
","np.vectorize works row-by-row, making it slow for large datasets. Instead use pd.merge_asof, which performs highly efficient, vectorized operations for time-based data alignment.
import pandas as pd
import numpy as np
from datetime import timedelta
import time

np.random.seed(42)
num_rows = 350000

timestamps = pd.date_range(start=""2024-11-01"", end=""2024-11-10"", freq=""1min"")
random_timestamps = np.random.choice(timestamps, num_rows)
alarm_names = [f""Alarm{np.random.randint(1, 100)}"" for _ in range(num_rows)]

data = pd.DataFrame({""PtName"": alarm_names, ""TimestampUTC"": random_timestamps})
def find_top_consequential_alarms(ui, data, fromAr):
    selected_alarm = ui.select_conc_alarm.currentText() if not fromAr else ui.ar_alarm_combo.currentText()
    selected_time = ui.select_conc_time_window.currentText() if not fromAr else ui.ar_timewindow_combo.currentText()

    time_map = {
        ""5 Minutes"": timedelta(minutes=5),
        ""10 Minutes"": timedelta(minutes=10),
        ""15 Minutes"": timedelta(minutes=15),
        ""30 Minutes"": timedelta(minutes=30),
        ""1 Hour"": timedelta(hours=1),
        ""5 Hours"": timedelta(hours=5),
        ""24 Hours"": timedelta(hours=24),
    }
    time_window = time_map[selected_time]

    specific_alarms = data[data[""PtName""].str.strip() == selected_alarm].copy()
    specific_alarms[""window_start""] = specific_alarms[""TimestampUTC""]
    specific_alarms[""window_end""] = specific_alarms[""TimestampUTC""] + time_window
    other_alarms = data[data[""PtName""].str.strip() != selected_alarm].copy()
    specific_alarms = specific_alarms.sort_values(by=""window_start"")
    other_alarms = other_alarms.sort_values(by=""TimestampUTC"")

    start_time = time.time()
    matched_alarms = pd.merge_asof(
        other_alarms,
        specific_alarms,
        left_on=""TimestampUTC"",
        right_on=""window_start"",
        direction=""backward"",
        tolerance=time_window
    )
    execution_time = time.time() - start_time

    consequential_alarms = matched_alarms.dropna(subset=[""window_start""])
    consequential_alarm_counts = consequential_alarms[""PtName_x""].value_counts().head(10)
    title = f""Top 10 Consequential Alarms for {selected_alarm}\n(Time Window: {time_window})""
    return consequential_alarm_counts, title, execution_time

result_counts, result_title, execution_time = find_top_consequential_alarms(ui, data, fromAr=False)

print(result_counts)
print(result_title)
print(f""Execution Time: {execution_time} seconds"")




Which gives you
PtName_x
Alarm19    2954
Alarm36    2951
Alarm41    2951
Alarm31    2939
Alarm15    2933
Alarm75    2932
Alarm83    2932
Alarm40    2932
Alarm88    2927
Alarm54    2927
Name: count, dtype: int64
Top 10 Consequential Alarms for Alarm1
(Time Window: 0:05:00)
Execution Time: 0.022200584411621094 seconds

As you can see, the execution is quite low on a dataframe of 350 000 rows.
",numpy
numerically integrating signals with absolute value,"Suppose I have a numpy s array of acceleration values representing some signal sampled at a fixed rate dt. I want to compute the cumulative absolute velocity, i.e. np.trapz(np.abs(s), dx=dt).
This is great except if dt is ""large"" (e.g. 0.01) and the signal s is both long and crossing between positive and negative values frequently (an unfortunately common occurrence), an error is accumulated from the fact that taking |s| drops the information about the original sign of s. See the picture for a better idea of what this error actually looks like.
I have some custom code that can correctly account for this error by creating a modified trapezium rule with numba, but there are other very similar functions that I need to implement doing things like np.trapz(np.square(s), dx=dt). Is there an off the shelf solution for this sort of numerical integral that:

Can be reused for both integrals np.trapz(np.square(s), dx=dt) and np.trapz(np.abs(s), dx=dt), etc...
Is ideally vectorised so that integration can be done for tens of thousands of signals at once in a reasonable time?

For the record, the following parallel numba code is what I am using to integrate the signals
@numba.njit(parallel=True)
def cav_integrate(
    waveform: npt.NDArray[np.float32], dt: float
) -> npt.NDArray[np.float32]:
    """"""Compute the Cumulative Absolute Velocity (CAV) of a waveform.""""""
    cav = np.zeros((waveform.shape[0],), dtype=np.float32)
    for i in range(waveform.shape[0]):
        for j in range(waveform.shape[1] - 1):
            if np.sign(waveform[i, j]) * np.sign(waveform[i, j + 1]) >= 0:
                cav[i] += dt / 2 * (np.abs(waveform[i, j]) + np.abs(waveform[i, j + 1]))
            else:
                slope = (waveform[i, j + 1] - waveform[i, j]) / dt
                x0 = -waveform[i, j] / slope
                cav[i] += x0 / 2 * np.abs(waveform[i, j]) + (dt - x0) / 2 * np.abs(
                    waveform[i, j + 1]
                )
    return cav

Example Data
I have uploaded a small broadband ground motion simulation to a  dropbox link (approx. 91MiB) for testing. This data comes from a finite difference simulation of a recent earthquake near Wellington, New Zealand plus some empirically derived high-frequency noise.  The file is an HDF5 containing some station data (irrelevant for our purposes), and simulation waveforms in the ""waveforms"" key. The array has shape (number of stations, timesteps, components) = (1433, 5876, 3). The 1d numpy array waveform[i, :, j] is the simulated acceleration for the ith station in the jth component. We need to compute the cumulative absolute velocity (CAV) for each component and each station independently. The benchmark code to do this can be found below:
import time

import h5py
import numba
import numpy as np
import numpy.typing as npt

broadband_input_file = h5py.File(""broadband.h5"", ""r"")
# Load the entire dataset into memory so that the first method is not arbitrarily slowed down by file I/O
waveforms = np.array(broadband_input_file[""waveforms""])
dt = 0.01

start = time.process_time()
cav_naive = np.trapz(np.abs(waveforms), dx=dt, axis=1)
print(f""CAV naive time: {time.process_time() - start}"")


@numba.njit
def cav_integrate(
    waveform: npt.NDArray[np.float32], dt: float
) -> npt.NDArray[np.float32]:
    """"""Compute the Cumulative Absolute Velocity (CAV) of a waveform.""""""
    cav = np.zeros((waveform.shape[0], waveform.shape[-1]), dtype=np.float32)
    for c in range(waveform.shape[-1]):
        for i in range(waveform.shape[0]):
            for j in range(waveform.shape[1] - 1):
                if np.sign(waveform[i, j, c]) * np.sign(waveform[i, j + 1, c]) >= 0:
                    cav[i, c] += (
                        dt
                        / 2
                        * (np.abs(waveform[i, j, c]) + np.abs(waveform[i, j + 1, c]))
                    )
                else:
                    slope = (waveform[i, j + 1, c] - waveform[i, j, c]) / dt
                    x0 = -waveform[i, j, c] / slope
                    cav[i, c] += x0 / 2 * np.abs(waveform[i, j, c]) + (
                        dt - x0
                    ) / 2 * np.abs(waveform[i, j + 1, c])
    return cav


# Warm up the numba compilation cache
_ = cav_integrate(waveforms, dt)

start = time.process_time()
cav_bespoke = cav_integrate(waveforms, dt)
print(f""Custom CAV time: {time.process_time() - start}"")


print(
    f""Ratio naive CAV / custom CAV (0, 25, 50, 75, 100% quartiles): {np.percentile(cav_naive / cav_bespoke, [0, 25, 50, 75, 100])}""
)

Which gives the following output
CAV naive time: 0.14353649699999993
Custom CAV time: 0.11182449700000019
Ratio naive CAV / custom CAV (0, 25, 50, 75, 100% quartiles): [1.00607312 1.00999796 1.01163089 1.01318455 1.02221394]

These differences are reasonably small, better examples of larger differences are shown in the comments. Some of the observed waveforms have 20-40% differences between the methods. Even 2% differences might be important for some of the researchers I support. Note also that the CAV calculation is done on a single thread for comparison, but I would parallelise both methods in reality for the largest waveform arrays (having 6 or 7x the stations and 10-20x the timesteps depending on the temporal resolution of the simulation). Funnily enough the parallel overhead for this small file makes cav_integrate slower than the naive approach if enabled.
We actually do the CAV calculation for all linear combinations cos(theta) * waveform[i, :, 0] + sin(theta) * waveform[i, :, 1] where theta = 0, 1,...180° to obtain orientation independent measurements of CAV. This is part of the reason it needs to be fast.
","This answer focus more on the performance/vectorization aspects than numerical integration.

Faster implementation

Is ideally vectorised so that integration can be done for tens of thousands of signals at once in a reasonable time?

Technically, Numba code can run with njit (and without errors) are always vectorized based on the definition on Numpy (a vectorized function is basically a natively compiled function). However, it can be made faster. The first thing to do is to use multiple threads so the code can benefit from multiple CPU cores.

Funnily enough the parallel overhead for this small file makes cav_integrate slower than the naive approach if enabled.

This is because there is 2 issues:

process_time returns the sum of the system and user CPU time (i.e. amount of parallel work) of the current process. Its does not measures the wall clock time. Thus, the benchmark is biased. You should use time() to measure the wall clock time instead.
Numba don't automatically parallelize loops. It only parallelize some basic array operation but to parallelize loops, you need to use prange instead of range or otherwise the loop will be sequential (so the code of the question is not actually parallel).

To efficiently parallelize the code, we should swap the i-based and c-based loops.
Moreover, there are other things to consider when it comes to performance:

you should avoid updating cav[i, c] and accumulate values in a local variable instead (be careful to use it only within the parallel loop).
you should also be careful to avoid implicit 64-bit FP numbers conversions because they are often more expensive and certainly not needed here since you operate on 32-bit FP numbers. For example, dt is a 64-bit number so any operation involving it will results in a 64-bit number.
I think you can use a min-max check instead of a sign-product-based one (the former is more efficient and SIMD friendly)
you should use math tricks so to avoid some expensive mathematical computations like divisions (and avoiding repeated computation like multiplication by a constant for all terms of a sum)
adapt the code so to benefit from SIMD units and improve memory accesses (this can typically improve the scalability of the code): this is typically done by physically swapping the c and j axis (though this requires a different input layout).

Here is the modified code considering all points except the last one (about SIMD and the memory layout):
@numba.njit(parallel=True)
def cav_integrate_opt(
    waveform: npt.NDArray[np.float32], dt: float
) -> npt.NDArray[np.float32]:
    """"""Compute the Cumulative Absolute Velocity (CAV) of a waveform.""""""
    cav = np.zeros((waveform.shape[0], waveform.shape[-1]), dtype=np.float32)
    dtf = np.float32(dt)
    half = np.float32(0.5)
    for i in numba.prange(waveform.shape[0]):
        for c in range(waveform.shape[-1]):
            tmp = np.float32(0)
            for j in range(waveform.shape[1] - 1):
                v1 = waveform[i, j, c]
                v2 = waveform[i, j + 1, c]
                if min(v1, v2) >= 0 or max(v1, v2) <= 0:
                    tmp += dtf * (np.abs(v1) + np.abs(v2))
                else:
                    inv_slope = dtf / (v2 - v1)
                    x0 = -v1 * inv_slope
                    tmp += x0 * np.abs(v1) + (dtf - x0) * np.abs(v2)
            cav[i, c] = tmp * half
    return cav

Here are performance results on my AMD Ryzen 5700U CPU (with 8 cores):
naive trapz (seq):              315 ms
initial cav_integrate (seq):    244 ms
optimized cav_integrate (par):   10 ms   <-----

Th optimized implementation is 25 times faster than cav_integrate and 31 times faster than the naive approach.
For better performance, please consider the last optimization point (more precisely about SIMD). That being said, this can be a bit complex to perform here. It might requires the else branch to be rarely executed (i.e. <5%) so to be pretty efficient.

More generic integration

Can be reused for both integrals np.trapz(np.square(s), dx=dt) and np.trapz(np.abs(s), dx=dt), etc...

Here are some thoughts:
For np.trapz(np.abs(s), dx=dt), a solution consists in computing the minimum value of the signal, then subtract the minimum to the signal, compute np.trapz of the resulting adapted signal so to finally correct the result. This solution is more efficient than you current one because it can benefit from SIMD instructions. However, it does not work for np.square.
A generic solution is to add new points close to the problematic area (thanks to an interpolation function). This solution is not optimal because to increase the computational time and is not numerically exact either (though using a lot of point should give a pretty accurate solution). You do not need to interpolate all points nor to generate new array for the whole array : you can do that line by line or even on the fly (a bit more complicated). This can save a lot of RAM and computation time.
Another generic solution is to pass a generic function in parameter to the numba function for computing differently the case where the sign change. However, this solution should be significantly slower than your specialized solution because it does not benefit from SIMD instructions and add an expensive function call that can hardly be inlined.
You can mix the two last solution so to build a generic solution which should be still faster once running with multiple threads and optimized like above. The idea is to add just one point where the curve cross the line y=0 and and split the integration in two parts. A linear interpolation should give results similar to cav_integrate_opt (if not even equal). Here is an example:
@numba.njit(parallel=True)
def cav_integrate_opt_generic(waveform, dt, fun):
    cav = np.zeros((waveform.shape[0], waveform.shape[-1]), dtype=np.float32)
    dtf = np.float32(dt)
    half = np.float32(0.5)
    for i in numba.prange(waveform.shape[0]):
        for c in range(waveform.shape[-1]):
            tmp = np.float32(0)
            for j in range(waveform.shape[1] - 1):
                v1 = waveform[i, j, c]
                v2 = waveform[i, j + 1, c]
                if min(v1, v2) < 0 and max(v1, v2) > 0:
                    # Basic linear interp
                    # Consider passing another generic function in 
                    # parameter to find roots if needed (more expensive).
                    inv_slope = dtf / (v2 - v1)
                    x0 = -v1 * inv_slope
                    tmp += x0 * fun(v1) + (dtf - x0) * fun(v2)
                else:
                    tmp += dtf * (fun(v1) + fun(v2))
            cav[i, c] = tmp * half
    return cav

# Needs to be wrapped in a Numba function for sake of performance
# (so Numba can call it directly like a native C function)
@numba.njit
def numba_abs(y):
    return np.abs(y)

# Note `cav_integrate_opt_generic` is recompiled for each different provided function.
cav_bespoke = cav_integrate_opt_generic(waveforms, dt, numba_abs)

If you want to do that using a higher-order interpolation and integration then you certainly need to consider more points and a generic function to find roots (which is certainly much more expensive when it is even possible to find analytical solutions).
It turns out this more generic function is only 5~10% slower for np.abs on my machine. Result are the same for np.abs.
",numpy
39preparing metadata pyprojecttoml  error39 when installing numpy on vs code fantasy football data pros data munging with pandas,"So I was trying to install numpy 1.20.3, on VS Code, when it says:
  Preparing metadata (pyproject.toml) ... error
  error: subprocess-exited-with-error
  
  × Preparing metadata (pyproject.toml) did not run successfully.
  │ exit code: 1
  ╰─> [239 lines of output]
      setup.py:66: RuntimeWarning: NumPy 1.20.3 may not yet support Python 3.10.
        warnings.warn(
      Running from numpy source directory.
      setup.py:485: UserWarning: Unrecognized setuptools command, proceeding with generating Cython sources and expanding templates
        run_build = parse_setuppy_commands()
      C:\Users\_\AppData\Local\Temp\pip-install-12pl1k89\numpy_a61d254ad189429092d1fab3dbdca78f\tools\cythonize.py:67: DeprecationWarning: The distutils package is deprecated and slated for removal in Python 3.12. Use setuptools or check PEP 632 for potential alternatives
        from distutils.version import LooseVersion
      Processing numpy/random\_bounded_integers.pxd.in
      Processing numpy/random\bit_generator.pyx
      Processing numpy/random\mtrand.pyx
      Processing numpy/random\_bounded_integers.pyx.in
      Processing numpy/random\_common.pyx
      Processing numpy/random\_generator.pyx
      Processing numpy/random\_mt19937.pyx
      Processing numpy/random\_pcg64.pyx
      Processing numpy/random\_philox.pyx
      Processing numpy/random\_sfc64.pyx
      Cythonizing sources
      blas_opt_info:
      blas_mkl_info:
      No module named 'numpy.distutils._msvccompiler' in numpy.distutils; trying from distutils
      customize MSVCCompiler
        libraries mkl_rt not found in ['C:\\Users\\_\\OneDrive\\Desktop\\VOR-Models\\2021-VOR-Model\\venv\\lib', 'C:\\']
        NOT AVAILABLE

      blis_info:
        libraries blis not found in ['C:\\Users\\_\\OneDrive\\Desktop\\VOR-Models\\2021-VOR-Model\\venv\\lib', 'C:\\']
        NOT AVAILABLE

      openblas_info:
        libraries openblas not found in ['C:\\Users\\_\\OneDrive\\Desktop\\VOR-Models\\2021-VOR-Model\\venv\\lib', 'C:\\']
      get_default_fcompiler: matching types: '['gnu', 'intelv', 'absoft', 'compaqv', 'intelev', 'gnu95', 'g95', 'intelvem', 'intelem', 'flang']'
      customize GnuFCompiler
      Could not locate executable g77
      Could not locate executable f77
      customize IntelVisualFCompiler
      Could not locate executable ifort
      Could not locate executable ifl
      customize AbsoftFCompiler
      Could not locate executable f90
      customize CompaqVisualFCompiler
      Could not locate executable DF
      customize IntelItaniumVisualFCompiler
      Could not locate executable efl
      customize Gnu95FCompiler
      Could not locate executable gfortran
      Could not locate executable f95
      customize G95FCompiler
      Could not locate executable g95
      customize IntelEM64VisualFCompiler
      customize IntelEM64TFCompiler
      Could not locate executable efort
      Could not locate executable efc
      customize PGroupFlangCompiler
      Could not locate executable flang
      don't know how to compile Fortran code on platform 'nt'
        NOT AVAILABLE

      atlas_3_10_blas_threads_info:
      Setting PTATLAS=ATLAS
        libraries tatlas not found in ['C:\\Users\\_\\OneDrive\\Desktop\\VOR-Models\\2021-VOR-Model\\venv\\lib', 'C:\\']
        NOT AVAILABLE

      atlas_3_10_blas_info:
        libraries satlas not found in ['C:\\Users\\_\\OneDrive\\Desktop\\VOR-Models\\2021-VOR-Model\\venv\\lib', 'C:\\']
        NOT AVAILABLE

      atlas_blas_threads_info:
      Setting PTATLAS=ATLAS
        libraries ptf77blas,ptcblas,atlas not found in ['C:\\Users\\_\\OneDrive\\Desktop\\VOR-Models\\2021-VOR-Model\\venv\\lib', 'C:\\']
        NOT AVAILABLE

      atlas_blas_info:
        libraries f77blas,cblas,atlas not found in ['C:\\Users\\_\\OneDrive\\Desktop\\VOR-Models\\2021-VOR-Model\\venv\\lib', 'C:\\']
        NOT AVAILABLE

      C:\Users\_\AppData\Local\Temp\pip-install-12pl1k89\numpy_a61d254ad189429092d1fab3dbdca78f\numpy\distutils\system_info.py:1989: UserWarning:
          Optimized (vendor) Blas libraries are not found.
          Falls back to netlib Blas library which has worse performance.
          A better performance should be easily gained by switching
          Blas library.
        if self._calc_info(blas):
      blas_info:
        libraries blas not found in ['C:\\Users\\_\\OneDrive\\Desktop\\VOR-Models\\2021-VOR-Model\\venv\\lib', 'C:\\']
        NOT AVAILABLE

      C:\Users\_\AppData\Local\Temp\pip-install-12pl1k89\numpy_a61d254ad189429092d1fab3dbdca78f\numpy\distutils\system_info.py:1989: UserWarning:
          Blas (http://www.netlib.org/blas/) libraries not found.
          Directories to search for the libraries can be specified in the
          numpy/distutils/site.cfg file (section [blas]) or by setting
          the BLAS environment variable.
        if self._calc_info(blas):
      blas_src_info:
        NOT AVAILABLE

      C:\Users\_\AppData\Local\Temp\pip-install-12pl1k89\numpy_a61d254ad189429092d1fab3dbdca78f\numpy\distutils\system_info.py:1989: UserWarning:
          Blas (http://www.netlib.org/blas/) sources not found.
          Directories to search for the sources can be specified in the
          numpy/distutils/site.cfg file (section [blas_src]) or by setting
          the BLAS_SRC environment variable.
        if self._calc_info(blas):
        NOT AVAILABLE

      non-existing path in 'numpy\\distutils': 'site.cfg'
      lapack_opt_info:
      lapack_mkl_info:
        libraries mkl_rt not found in ['C:\\Users\\_\\OneDrive\\Desktop\\VOR-Models\\2021-VOR-Model\\venv\\lib', 'C:\\']
        NOT AVAILABLE

      openblas_lapack_info:
        libraries openblas not found in ['C:\\Users\\_\\OneDrive\\Desktop\\VOR-Models\\2021-VOR-Model\\venv\\lib', 'C:\\']
        NOT AVAILABLE

      openblas_clapack_info:
        libraries openblas,lapack not found in ['C:\\Users\\_\\OneDrive\\Desktop\\VOR-Models\\2021-VOR-Model\\venv\\lib', 'C:\\']
        NOT AVAILABLE

      flame_info:
        libraries flame not found in ['C:\\Users\\_\\OneDrive\\Desktop\\VOR-Models\\2021-VOR-Model\\venv\\lib', 'C:\\']
        NOT AVAILABLE

      atlas_3_10_threads_info:
      Setting PTATLAS=ATLAS
        libraries lapack_atlas not found in C:\Users\_\OneDrive\Desktop\VOR-Models\2021-VOR-Model\venv\lib
        libraries tatlas,tatlas not found in C:\Users\_\OneDrive\Desktop\VOR-Models\2021-VOR-Model\venv\lib
        libraries lapack_atlas not found in C:\
        libraries tatlas,tatlas not found in C:\
      <class 'numpy.distutils.system_info.atlas_3_10_threads_info'>
        NOT AVAILABLE

      atlas_3_10_info:
        libraries lapack_atlas not found in C:\Users\_\OneDrive\Desktop\VOR-Models\2021-VOR-Model\venv\lib
        libraries satlas,satlas not found in C:\Users\_\OneDrive\Desktop\VOR-Models\2021-VOR-Model\venv\lib
        libraries lapack_atlas not found in C:\
        libraries satlas,satlas not found in C:\
      <class 'numpy.distutils.system_info.atlas_3_10_info'>
        NOT AVAILABLE

      atlas_threads_info:
      Setting PTATLAS=ATLAS
        libraries lapack_atlas not found in C:\Users\_\OneDrive\Desktop\VOR-Models\2021-VOR-Model\venv\lib
        libraries ptf77blas,ptcblas,atlas not found in C:\Users\_\OneDrive\Desktop\VOR-Models\2021-VOR-Model\venv\lib
        libraries lapack_atlas not found in C:\
        libraries ptf77blas,ptcblas,atlas not found in C:\
      <class 'numpy.distutils.system_info.atlas_threads_info'>
        NOT AVAILABLE

      atlas_info:
        libraries lapack_atlas not found in C:\Users\_\OneDrive\Desktop\VOR-Models\2021-VOR-Model\venv\lib
        libraries f77blas,cblas,atlas not found in C:\Users\_\OneDrive\Desktop\VOR-Models\2021-VOR-Model\venv\lib
        libraries lapack_atlas not found in C:\
        libraries f77blas,cblas,atlas not found in C:\
      <class 'numpy.distutils.system_info.atlas_info'>
        NOT AVAILABLE

      lapack_info:
        libraries lapack not found in ['C:\\Users\\_\\OneDrive\\Desktop\\VOR-Models\\2021-VOR-Model\\venv\\lib', 'C:\\']
        NOT AVAILABLE

      C:\Users\_\AppData\Local\Temp\pip-install-12pl1k89\numpy_a61d254ad189429092d1fab3dbdca78f\numpy\distutils\system_info.py:1849: UserWarning:
          Lapack (http://www.netlib.org/lapack/) libraries not found.
          Directories to search for the libraries can be specified in the
          numpy/distutils/site.cfg file (section [lapack]) or by setting
          the LAPACK environment variable.
        return getattr(self, '_calc_info_{}'.format(name))()
      lapack_src_info:
        NOT AVAILABLE

      C:\Users\_\AppData\Local\Temp\pip-install-12pl1k89\numpy_a61d254ad189429092d1fab3dbdca78f\numpy\distutils\system_info.py:1849: UserWarning:
          Lapack (http://www.netlib.org/lapack/) sources not found.
          Directories to search for the sources can be specified in the
          numpy/distutils/site.cfg file (section [lapack_src]) or by setting
          the LAPACK_SRC environment variable.
        return getattr(self, '_calc_info_{}'.format(name))()
        NOT AVAILABLE

      numpy_linalg_lapack_lite:
        FOUND:
          language = c
          define_macros = [('HAVE_BLAS_ILP64', None), ('BLAS_SYMBOL_SUFFIX', '64_')]

      C:\Program Files\WindowsApps\PythonSoftwareFoundation.Python.3.10_3.10.752.0_x64__qbz5n2kfra8p0\lib\distutils\dist.py:274: UserWarning: Unknown distribution option: 'define_macros'
        warnings.warn(msg)
      running dist_info
      running build_src
      build_src
      building py_modules sources
      creating build
      creating build\src.win-amd64-3.10
      creating build\src.win-amd64-3.10\numpy
      creating build\src.win-amd64-3.10\numpy\distutils
      building library ""npymath"" sources
      LINK : fatal error LNK1104: cannot open file 'kernel32.lib'
      Traceback (most recent call last):
        File ""C:\Users\_\OneDrive\Desktop\VOR-Models\2021-VOR-Model\venv\lib\site-packages\pip\_vendor\pep517\in_process\_in_process.py"", line 363, in <module>
          main()
        File ""C:\Users\_\OneDrive\Desktop\VOR-Models\2021-VOR-Model\venv\lib\site-packages\pip\_vendor\pep517\in_process\_in_process.py"", line 345, in main
          json_out['return_val'] = hook(**hook_input['kwargs'])
        File ""C:\Users\_\OneDrive\Desktop\VOR-Models\2021-VOR-Model\venv\lib\site-packages\pip\_vendor\pep517\in_process\_in_process.py"", line 164, in prepare_metadata_for_build_wheel
          return hook(metadata_directory, config_settings)
        File ""C:\Users\_\AppData\Local\Temp\pip-build-env-fmnw10id\overlay\Lib\site-packages\setuptools\build_meta.py"", line 157, in prepare_metadata_for_build_wheel
          self.run_setup()
        File ""C:\Users\_\AppData\Local\Temp\pip-build-env-fmnw10id\overlay\Lib\site-packages\setuptools\build_meta.py"", line 248, in run_setup
          super(_BuildMetaLegacyBackend,
        File ""C:\Users\_\AppData\Local\Temp\pip-build-env-fmnw10id\overlay\Lib\site-packages\setuptools\build_meta.py"", line 142, in run_setup
          exec(compile(code, __file__, 'exec'), locals())
        File ""setup.py"", line 513, in <module>
          setup_package()
        File ""setup.py"", line 505, in setup_package
          setup(**metadata)
        File ""C:\Users\_\AppData\Local\Temp\pip-install-12pl1k89\numpy_a61d254ad189429092d1fab3dbdca78f\numpy\distutils\core.py"", line 169, in setup
          return old_setup(**new_attr)
        File ""C:\Users\_\AppData\Local\Temp\pip-build-env-fmnw10id\overlay\Lib\site-packages\setuptools\__init__.py"", line 165, in setup
          return distutils.core.setup(**attrs)
        File ""C:\Program Files\WindowsApps\PythonSoftwareFoundation.Python.3.10_3.10.752.0_x64__qbz5n2kfra8p0\lib\distutils\core.py"", line 148, in setup
          dist.run_commands()
        File ""C:\Program Files\WindowsApps\PythonSoftwareFoundation.Python.3.10_3.10.752.0_x64__qbz5n2kfra8p0\lib\distutils\dist.py"", line 966, in run_commands
          self.run_command(cmd)
        File ""C:\Program Files\WindowsApps\PythonSoftwareFoundation.Python.3.10_3.10.752.0_x64__qbz5n2kfra8p0\lib\distutils\dist.py"", line 985, in run_command
          cmd_obj.run()
        File ""C:\Users\_\AppData\Local\Temp\pip-build-env-fmnw10id\overlay\Lib\site-packages\setuptools\command\dist_info.py"", line 31, in run
          egg_info.run()
        File ""C:\Users\\AppData\Local\Temp\pip-install-12pl1k89\numpy_a61d254ad189429092d1fab3dbdca78f\numpy\distutils\command\egg_info.py"", line 24, in run
          self.run_command(""build_src"")
        File ""C:\Program Files\WindowsApps\PythonSoftwareFoundation.Python.3.10_3.10.752.0_x64__qbz5n2kfra8p0\lib\distutils\cmd.py"", line 313, in run_command
          self.distribution.run_command(command)
        File ""C:\Program Files\WindowsApps\PythonSoftwareFoundation.Python.3.10_3.10.752.0_x64__qbz5n2kfra8p0\lib\distutils\dist.py"", line 985, in run_command
          cmd_obj.run()
        File ""C:\Users\_\AppData\Local\Temp\pip-install-12pl1k89\numpy_a61d254ad189429092d1fab3dbdca78f\numpy\distutils\command\build_src.py"", line 144, in run
          self.build_sources()
        File ""C:\Users\_\AppData\Local\Temp\pip-install-12pl1k89\numpy_a61d254ad189429092d1fab3dbdca78f\numpy\distutils\command\build_src.py"", line 155, in build_sources
          self.build_library_sources(*libname_info)
        File ""C:\Users\_\AppData\Local\Temp\pip-install-12pl1k89\numpy_a61d254ad189429092d1fab3dbdca78f\numpy\distutils\command\build_src.py"", line 288, in build_library_sources
          sources = self.generate_sources(sources, (lib_name, build_info))
        File ""C:\Users\_\AppData\Local\Temp\pip-install-12pl1k89\numpy_a61d254ad189429092d1fab3dbdca78f\numpy\distutils\command\build_src.py"", line 378, in generate_sources
          source = func(extension, build_dir)
        File ""numpy\core\setup.py"", line 676, in get_mathlib_info
          raise RuntimeError(""Broken toolchain: cannot link a simple C program"")
      RuntimeError: Broken toolchain: cannot link a simple C program
      [end of output]

  note: This error originates from a subprocess, and is likely not a problem with pip.
error: metadata-generation-failed

× Encountered error while generating package metadata.
╰─> See above for output.

note: This is an issue with the package mentioned above, not pip.
hint: See above for details.

I am installing it like this:
(venv) PS C:\Users_\OneDrive\Desktop\VOR-Models\2021-VOR-Model> pip install -r requirements.txt
Because I am so lost, I will show you requirements.txt. This function literally installs everything before numpy, then it fails.
clearbeautifulsoup4==4.9.3
certifi==2021.5.30
chardet==4.0.0
idna==2.10
numpy==1.20.3
pandas==1.2.4
python-dateutil==2.8.1
pytz==2021.1
requests==2.25.1
six==1.16.0
soupsieve==2.2.1
urllib3==1.26.5

Any help is appreciated. The reason I am doing this is because I took a course online (https://www.fantasyfootballdatapros.com/), and am trying to do data munging with pandas. I need NumPy for that. I also tried looking at other questions, but they didn't give me the answers I needed, or didn't have an answer at all. Please help.
","It says it in the error message.

RuntimeWarning: NumPy 1.20.3 may not yet support Python 3.10.

Two quick trys:

Without looking through each packages, try removing numpy from requirements, and run ""python -m pip install -r requirements.txt"" again.  Then in command line, try typing this to see if it installed because the other packages (ex. Tensorflow install numpy automatically, with the version it depends on).


python -m numpy


try a lower numpy version.  inside requirments.txt try typing  or something under 1.20.3


numpy==1.19.2

",numpy
interpolating battery capacity data in logarithmic scale with python,"I'm working on interpolating battery capacity data based on the relationships between hour_rates, capacities and currents. Here’s a sample of my data:
import numpy as np
import pandas as pd
from scipy.interpolate import interp1d
import matplotlib.pyplot as plt

# Data from Rolls S-480 flooded battery
capacity_data = [
    [1, 135, 135], [2, 191, 95.63], [3, 221, 73.75],
    [4, 244, 60.94], [5, 263, 52.5], [6, 278, 46.25],
    [8, 300, 37.5], [10, 319, 31.88], [12, 334, 27.81],
    [15, 352, 23.45], [20, 375, 18.75], [24, 386, 16.09],
    [50, 438, 8.76], [72, 459, 6.38], [100, 486, 4.86]
]
capacity = pd.DataFrame(capacity_data, columns=['hour_rates', 'capacities_o', 'currents'])
capacity['capacities'] = np.around(capacity['currents'] * capacity['hour_rates'], 3)

The columns relate as follows:

hour_rates (h) = capacities (Ah) / currents (A)
capacities (Ah) = hour_rates (h) * currents (A)
currents (A) = capacities (Ah) / hour_rates (h)

Objective: I want to interpolate capacities and hour_rates for a range of currents values using logarithmic scaling for better accuracy.
Code
Custom interpolation class and function to achieve this. Here’s the code:
from typing import Union

class interpolate1d(interp1d):
    """"""Extend scipy interp1d to interpolate/extrapolate per axis in log space""""""
    
    def __init__(self, x, y, *args, xspace='linear', yspace='linear', **kwargs):
        self.xspace = xspace
        self.yspace = yspace
        if self.xspace == 'log': x = np.log10(x)
        if self.yspace == 'log': y = np.log10(y)
        super().__init__(x, y, *args, **kwargs)
        
    def __call__(self, x, *args, **kwargs):
        if self.xspace == 'log': x = np.log10(x)
        if self.yspace == 'log':
            return 10**super().__call__(x, *args, **kwargs)
        else:
            return super().__call__(x, *args, **kwargs)


def interpolate_cap_by_current(df: list,
                               current_values: list,
                               kind: Union[str, int] = 'linear',
                               hr_limit: int = 600
                               ):
    """"""
    Interpolate Battery Capacity Values From Current list values
    """"""
    result = 0
    if isinstance(np_data, np.ndarray):
        # Create interpolation functions for hour rates and capacities
        # Setting kind='cubic' for better fitting to nonlinear data
        hour_rate_interp_func = interpolate1d(
            df['currents'],
            df['hour_rates'],
            xspace='log',
            yspace='log',
            fill_value=""extrapolate"",
            kind=kind
        )
        capacity_interp_func = interpolate1d(
            df['currents'],
            df['capacities'],
            xspace='log',
            yspace='log',
            fill_value=""extrapolate"",
            kind=kind
        ) # , kind='cubic'

        # Calculate interpolated values for new currents
        hour_rate_interpolated = hour_rate_interp_func(current_values)
        capacity_interpolated = capacity_interp_func(current_values)

        # Create a DataFrame for the results
        calc_cap = np.around(current_values * hour_rate_interpolated, 3)
        calc_hr = np.around(capacity_interpolated / current_values, 3)
        diff_cap = np.around(capacity_interpolated - calc_cap, 3)
        diff_hr = np.around(hour_rate_interpolated - calc_hr, 3)
        real_hr = np.around(hour_rate_interpolated - diff_hr, 3)
        real_cap = np.around(current_values * real_hr, 3)
        real_current = np.around(real_cap / real_hr, 3)
        result = pd.DataFrame({
            'currents': current_values,
            'hour_rates': hour_rate_interpolated,
            'capacities': capacity_interpolated,
            'calc_cap': calc_cap,
            'real_cap': real_cap,
            'diff_cap': diff_cap,
            'calc_hr': calc_hr,
            'real_hr': real_hr,
            'diff_hr': diff_hr,
            'real_current': real_current,
            'diff_current': np.around(current_values - real_current, 3),
        })
        
        result = result[result['hour_rates'] < hr_limit]
    return result

def plot_grid(major_ticks: list,
              minor_ticks: list,
              ):
    """"""Set X Grid ticks""""""
    ax=plt.gca()
    ax.grid(True)
    ax.set_xticks(major_ticks)
    ax.set_xticks(minor_ticks, minor=True)
    ax.grid(which='minor', alpha=0.2)
    ax.grid(which='major', alpha=0.5)

Visualisation:
currents_list = np.array([
    0.1, 0.2, 0.4, 0.5, 0.6, 0.8, 1, 1.5, 1.7, 2, 2.2, 2.5,
    3, 4, 5, 6, 7, 8, 9, 10, 11, 15, 17, 20, 22, 25, 27, 30, 32,
    35, 37, 40, 60, 80, 120, 150, 180, 220, 250
])
capacities = interpolate_cap_by_current(
    df=capacity,
    current_values=currents_list,
    kind='quadratic'
)
rel_current = np.around(capacity['capacities']/capacity['hour_rates'], 3)
#  linear, nearest, nearest-up, zero, slinear, quadratic, cubic, previous, or next. zero, slinear, quadratic and cubic
plt.figure(figsize=(18, 15))
plt.subplot(3, 1, 1)
plt.plot(capacities['real_hr'], capacities['capacities'], label='Interpolated Capacitiy')
plt.plot(capacities['real_hr'], capacities['calc_cap'], label='Calculated Capacitiy')
plt.plot(capacities['real_hr'], capacities['real_cap'], label='Real Capacitiy')
plt.plot(capacity['hour_rates'], capacity['capacities'], label='Capacitiy')
plt.ylabel('Capacity (A/h)')
plt.xlabel('Hour Rate (h)')
plt.title('Battery Hour Rate / Capacity relationship')
plt.legend()
max_tick = capacities['hour_rates'].max() + 10
plot_grid(
    major_ticks=np.arange(0, max_tick, 20),
    minor_ticks=np.arange(0, max_tick, 5)
)
plt.subplot(3, 1, 2)
plt.plot(capacities['real_hr'], capacities['currents'], label='Interpolated Current (A)')
plt.plot(capacities['real_hr'], capacities['real_current'], label='Real Current (A)')
plt.plot(capacity['hour_rates'], rel_current, label='Calculated Original Current Relation (A)')
plt.plot(capacity['hour_rates'], capacity['currents'], label='Current (A)')
plt.ylabel('Current (A)')
plt.xlabel('Hour Rate (h)')
plt.title('Battery Hour Rate / Current relationship')
plt.legend()
plot_grid(
    major_ticks=np.arange(0, max_tick, 20),
    minor_ticks=np.arange(0, max_tick, 5)
)
plt.subplot(3, 1, 3)
plt.plot(capacities['currents'], capacities['capacities'], label='Interpolated capacity / current')
plt.plot(capacities['currents'], capacities['calc_cap'], label='Calculated capacity / current')
plt.plot(capacity['currents'], capacity['capacities'], label='capacity / current')
plt.ylabel('Capacity (A/h)')
plt.xlabel('Current (A)')
plt.title('Battery Current / Capacity relationship')
plt.xscale('linear')
plt.yscale('linear')
plt.legend()
max_tick = capacities['currents'].max() + 10
plot_grid(
    major_ticks=np.arange(0, max_tick, 20),
    minor_ticks=np.arange(0, max_tick, 5)
)


Problem
Even though I've configured the interpolation in logarithmic space, the interpolated values still don’t match the calculated values when verified against the relationships provided. I’ve illustrated this discrepancy in the plots below, where I calculate the difference by applying the original relationships to the interpolated results.
plt.figure(figsize=(18, 15))
plt.subplot(3, 1, 1)
plt.plot(capacities['hour_rates'], capacities['diff_cap'], label='Diff Capacity')
plt.plot(capacities['hour_rates'], capacities['diff_hr'], label='Diff Hour Rate')
plt.ylabel('Diff Interpolated / Calculated')
plt.xlabel('Hour Rate (h)')
plt.title('Interpolation Data Relationship By Hour Rate')
plt.legend()
max_tick = capacities['hour_rates'].max() + 10
plot_grid(
    major_ticks=np.arange(0, max_tick, 20),
    minor_ticks=np.arange(0, max_tick, 5)
)
plt.subplot(3, 1, 2)
plt.plot(capacities['capacities'], capacities['diff_cap'], label='Diff Capacity')
plt.plot(capacities['capacities'], capacities['diff_hr'], label='Diff Hour Rate')
plt.ylabel('Diff Interpolated / Calculated')
plt.xlabel('Capacity (A/h)')
plt.title('Interpolation Data Relationship By Capacity')
plt.legend()
max_tick = capacities['capacities'].max() + 10
plot_grid(
    major_ticks=np.arange(0, max_tick, 20),
    minor_ticks=np.arange(0, max_tick, 5)
)
plt.subplot(3, 1, 3)
plt.plot(capacities['currents'], capacities['diff_cap'], label='Diff Capacity')
plt.plot(capacities['currents'], capacities['diff_hr'], label='Diff Hour Rate')
plt.ylabel('Diff Interpolated / Calculated')
plt.xlabel('Current (A)')
plt.title('Interpolation Data Relationship By Current')
plt.legend()
max_tick = capacities['currents'].max() + 10
plot_grid(
    major_ticks=np.arange(0, max_tick, 20),
    minor_ticks=np.arange(0, max_tick, 5)
)


Is there a way to improve the accuracy of the interpolation on a logarithmic scale for this type of data relationship? I understand that current values outside the range of (4.86 A, 135 A) may lead to inaccurate results due to extrapolation.
Edit
I’ve updated the code above to improve interpolation accuracy:

The original capacity values appeared to be rounded in the source data. These values are now corrected prior to interpolation to enhance precision.
Added a second graph to evaluate the accuracy of the relationship for the calculated current values.

plt.figure(figsize=(18, 15))
plt.subplot(3, 1, 1)
plt.plot(capacities['real_hr'], capacities['diff_current'], label='Diff Current')
plt.plot(capacity['hour_rates'], capacity['currents'] - rel_current, label='Diff Original Current Relation')
plt.ylabel('Diff Interpolated / Calculated')
plt.xlabel('Hour Rate (h)')
plt.title('Interpolation Data Relationship By Hour Rate')
plt.legend()
max_tick = capacities['hour_rates'].max() + 10
plot_grid(
    major_ticks=np.arange(0, max_tick, 20),
    minor_ticks=np.arange(0, max_tick, 5)
)
plt.subplot(3, 1, 2)
plt.plot(capacities['real_cap'], capacities['diff_current'], label='Diff Current')
plt.plot(capacity['capacities'], capacity['currents'] - rel_current, label='Diff Original Current Relation')
plt.ylabel('Diff Interpolated / Calculated')
plt.xlabel('Capacity (A/h)')
plt.title('Interpolation Data Relationship By Capacity')
plt.legend()
max_tick = capacities['capacities'].max() + 10
plot_grid(
    major_ticks=np.arange(0, max_tick, 20),
    minor_ticks=np.arange(0, max_tick, 5)
)
plt.subplot(3, 1, 3)
plt.plot(capacities['currents'], capacities['diff_current'], label='Diff Current')
plt.plot(capacity['currents'], capacity['currents'] - rel_current, label='Diff Original Current Relation')
plt.ylabel('Diff Interpolated / Calculated')
plt.xlabel('Current (A)')
plt.title('Interpolation Data Relationship By Current')
plt.legend()
max_tick = capacities['currents'].max() + 10
plot_grid(
    major_ticks=np.arange(0, max_tick, 20),
    minor_ticks=np.arange(0, max_tick, 5)
)


Edit 2
I’ve made additional updates to the code to further improve interpolation accuracy:
- Rounded all values to 3 decimal places to minimize insignificant errors.
- Observing the updated graphs,
`hour_rate` interpolation values are more accurate than `capacity` interpolation values.
I’ve adjusted the code to interpolate only `hour_rate` and then calculate `capacity` using the relationship `capacity = hour_rate * current`.

Below are the updated graphs:
Data Visualization

Difference Between Interpolated and Calculated Capacity and Hour Rate

Difference Between Interpolated and Calculated Current

","Looking on your currency data described relations:
hour_rates (h) = capacities (Ah) / currents (A)
capacities (Ah) = hour_rates (h) * currents (A)
currents (A) = capacities (Ah) / hour_rates (h)

These are not met explicitly in the data you presented. I've created the data which are exactly like the presented results:
capacity_data_corr = capacity[['hour_rates', 'capacities']]
capacity_data_corr['currents'] = capacity_data_corr['capacities']/capacity_data_corr['hour_rates']

Interpolation is almost ideal


This means, that the interpolation obtained can be good, but the data does not meet assumed relations. If these relations are only approximate, in such long horizon error like this should not be as bad as it looks.
",numpy
how to replace values in numpy array at the same time,"I have numpy array with random numbers. For example like this
[7 1 2 0 2 3 4 0 5]
and I want to replace every number at the same time if number from this array = 7, I want to replace it with 2 , also if number = 2, replace it with 3. So it will be like [2 1 3 0 3 3 4 0 5] . I have tried it with np.where but can't change any of them.
","It's better to use np.select if you've multiple conditions:
a = np.array([7, 1, 2, 0, 2, 3, 4, 0, 5])
a = np.select([a == 7, a == 2], [2, 3], a)

OUTPUT:
[2 1 3 0 3 3 4 0 5]

",numpy
how to represent inf or inf in cython with numpy,"I am building an array with cython element by element. I'd like to store the constant np.inf (or -1 * np.inf) in some entries. However, this will require the overhead of going back into Python to look up inf. Is there a libc.math equivalent of this constant? Or some other value that could easily be used that's equivalent to (-1*np.inf) and can be used from Cython without overhead?
EDIT example, you have:
cdef double value = 0
for k in xrange(...):
  # use -inf here -- how to avoid referring to np.inf and calling back to python?
  value = -1 * np.inf

","There's no literal for it, but float can parse it from a string:
>>> float('inf')
inf
>>> np.inf == float('inf')
True

Alternatively, math.h may (almost certainly will) declare a macro that evaluates to inf, in which case you can just use that:
cdef extern from ""math.h"":
    float INFINITY


(There's no clean way to check if INFINITY is defined in pure Cython, so if you want to cover all your bases you'll need to get hacky. One way of doing it is to create a small C header, say fallbackinf.h:
#ifndef INFINITY
#define INFINITY 0
#endif

And then in your .pyx file:
cdef extern from ""math.h"":
    float INFINITY

cdef extern from ""fallbackinf.h"":
    pass

inf = INFINITY if INFINITY != 0 else float('inf')

(You can't assign to INFINITY, because it's an rvalue. You could do away with the ternary operator if you #defined INFINITY as 1.0/0.0 in your header, but that might raise SIGFPE, depending on your compiler.)
This is definitely in the realm of cargo cult optimisation, though.)
",numpy
how to apply a custom function to xarraydataarraycoarsenreduce,"I have a (2x2) NumPy array:
ar = np.array([[2, 0],[3, 0]])

and the same one in the form of xarray.DataArray:
da = xr.DataArray(ar, dims=['x', 'y'], coords=[[0, 1], [0, 1]])

I am trying to downsample the 2d array spatially using a custom function to find the mode (i.e., the most frequently occurring value):
def find_mode(window):
    # find the mode over all axes 
    uniq = np.unique(window, return_counts=True)
    return uniq[0][np.argmax(uniq[1])]

The find_mode() works well for ar as find_mode(ar) gives 0.
However, it doesn't work for da (i.e., da.coarsen(x=2, y=2).reduce(find_mode)), with an error:
TypeError: find_mode() got an unexpected keyword argument 'axis'

Thank you so much for your attention and participation.
","The signature for functions passed to DatasetCoarsen.reduce must include axis and kwargs. A good example is np.sum. So your function would need to look something like:
def find_mode(window, axis=None, **kwargs):
    # find the mode over all axes 
    uniq = np.unique(window, return_counts=True)
    
    ret = uniq[0][np.argmax(uniq[1])]
    ret = np.atleast_2d(ret)

    return ret

Depending on your application, you may want to use the axis argument (tuple of integers) in place of your [0] and [1] indexing steps.
Note: I've added np.atleast_2d here to make sure the return array is 2D. This is a bit ugly so I recommend thinking a bit more about this part in your application. The key thing to understand is that the return array needs to have the same number of dimensions as the input array.
",numpy
is it possible to map a discontiuous data on disk to an array with python,"I want to map a big fortran record (12G) on hard disk to a numpy array. (Mapping instead of loading for saving memory.)
The data stored in fortran record is not continuous as it is divided by record markers. The record structure is as ""marker, data, marker, data,..., data, marker"". The length of data regions and markers are known. 
The length of data between markers is not multiple of 4 bytes, otherwise I can map each data region to an array.
The first marker can be skipped by setting offset in memmap, is it possible to skip other markers and map the data to an array?
Apology for possible ambiguous expression and thanks for any solution or suggestion.

Edited May 15
These are fortran unformatted files. The data stored in record is a (1024^3)*3 float32 array (12Gb). 
The record layout of variable-length records that are greater than 2 gigabytes is shown below:

(For details see here -> the section [Record Types] -> [Variable-Length Records].)
In my case, except the last one, each subrecord has a length  of 2147483639 bytes and separated by 8 bytes (as you see in the figure above, a end marker of the previous subrecord and a begin marker of the following one, 8 bytes in total ) .
We can see the first subrecord ends with the first 3 bytes of certain float number and the second subrecord begins with the rest 1 byte as 2147483639 mod 4 =3.
","It is possible using numpy.memmap:
offset = 0
data1 = np.memmap('tmp', dtype='i', mode='r+', order='F',
                  offset=0, shape=(size1))
offset += size1*byte_size
data2 = np.memmap('tmp', dtype='i', mode='r+', order='F',
                  offset=offset, shape=(size2))
offset += size1*byte_size
data3 = np.memmap('tmp', dtype='i', mode='r+', order='F',
                  offset=offset, shape=(size3))

You need to set the byte_size according to the data type. For example:

int32 requires byte_size=32/8
int16 byte_size=16/8
and so forth...

If the data type is constant for the entire array, you can load the data in a 2D array like:
shape = (total_length/size,size)
data = np.memmap('tmp', dtype='i', mode='r+', order='F', shape=shape)

You can change the memmap object as long as you want. It is even possible to make arrays sharing the same elements, in which case the changes made in the shared elements are perceived by all corresponding arrays.
Other references:

Working with big data in python and numpy, not enough ram, how to save partial results on disc?

numpy.memmap documentation here.


",numpy
how can i print a matrix using numpy without ellipsis in the middle,"I made a matrix 12x8 to use it as a gameboard with numpy but when I try to print it the matrix is printed incomplete (with ellipsis in the middle).
My real question is how can I print it correctly.
Thanks.
","Use np.set_printoptions() to control summarization when printing.
",numpy
selecting a column vector from a matrix in python,"I would like to index a column vector in a matrix in Python/numpy and have it returned as a column vector and not a 1D array.
x = np.array([[1,2],[3,4]])
x[:,1]
>array([2, 4])

np.transpose(x[:,1]) is not a solution. Following the numpy.transpose documentation, this will return a row vector (1-D array).
","Few options -
x[:,[1]]
x[:,None,1]
x[:,1,None]
x[:,1][:,None]
x[:,1].reshape(-1,1)
x[None,:,1].T

",numpy
why can39t i stack 3d arrays with npconcatenate in numpy while 1d and 2d arrays work,"I'm working with 3D numpy arrays and having trouble stacking two of them. Here’s what I’m trying to do:
import numpy as np

grid = np.arange(16).reshape((1, 4, 4))
grid2 = grid[:, :-1, ::-1].copy()

I expected to be able to stack grid and grid2 with np.concatenate on axis=0, like this:
np.concatenate((grid, grid2), axis=0)

But it doesn’t work, nor do np.vstack or np.dstack (I observed that vstack only works on 1D and 2D arrays). I’ve checked the shapes, and I thought they’d align since they’re both derived from grid, but it’s not cooperating.
The goal here is not to reverse all rows and columns, but to only use the first 3 rows of the original grid array, with just the columns reversed. That's why I used grid[:, :-1, ::-1].copy() to keep the rows intact while only switching the columns.
Attempting to concatenate grid and grid2 along axis=0 fails with a ValueError:
ValueError: all the input array dimensions except for the concatenation axis must match exactly, but along dimension 1, the array at index 0 has size 4 and the array at index 1 has size 3
Here's an example of a simpler concatenation that does work as expected:
x = np.arange(1, 9).reshape((2, 4))
y = np.arange(4)
np.vstack((x, y))  # Works perfectly

Why does stacking grid and grid2 fail, while simpler cases like x and y work fine? Is there a specific rule or limitation in numpy that I’m missing here?
","In this particular case, you could use hstack:
np.hstack([grid, grid2])

For a generic case you should concatenate on axis=1 (all other dimensions must be equal):
np.concatenate([grid, grid2], axis=1)

Output:
array([[[ 0,  1,  2,  3],
        [ 4,  5,  6,  7],
        [ 8,  9, 10, 11],
        [12, 13, 14, 15],
        [ 3,  2,  1,  0],
        [ 7,  6,  5,  4],
        [11, 10,  9,  8]]])

Equality of dimensions for concatenate:
grid.shape    # (1, 4, 4)
grid2.shape   # (1, 3, 4)
              #     └──── only possible dimension for concatenate


NB. as noted in comments, if your input is really:
grid = np.arange(16).reshape((1, 4, 4))
grid2 = grid[:, ::-1, ::-1].copy()

Then concatenate on axis=0 can work since the other dimensions are now equal:
np.concatenate([grid, grid2], axis=0)

array([[[ 0,  1,  2,  3],
        [ 4,  5,  6,  7],
        [ 8,  9, 10, 11],
        [12, 13, 14, 15]],

       [[15, 14, 13, 12],
        [11, 10,  9,  8],
        [ 7,  6,  5,  4],
        [ 3,  2,  1,  0]]])

grid.shape    # (1, 4, 4)
grid2.shape   # (1, 4, 4)  # any axis can be used to concatenate

",numpy
how to merge dataframes over multiple columns and split rows,"I have two datafames:
df1 = pd.DataFrame({
    'from': [0, 2, 8, 26, 35, 46],
    'to': [2, 8, 26, 35, 46, 48],
    'int': [2, 6, 18, 9, 11, 2]})

df2 = pd.DataFrame({
    'from': [0, 2, 8, 17, 34],
    'to': [2, 8, 17, 34, 49],
    'int': [2, 6, 9, 17, 15]})

I want to create a new dataframe that looks like this:
df = pd.DataFrame({
    'from': [0, 2, 8, 17, 26, 34, 35, 46, 48],
    'to': [2, 8, 17, 26, 34, 35, 46, 48, 49],
    'int': [2, 6, 9, 9, 8, 1, 11, 2, 1]})

I have tried standard merging scripts but have not been able to split the rows containing higher 'from' and 'to' numbers in either df1 or df2 into smaller ones.
How can I merge my dataframes over multiple columns and split rows?
","Frirst, combine all unique from and to values from both df1 and df2 to create a set of breakpoints:
breakpoints = set(df1['from']).union(df1['to']).union(df2['from']).union(df2['to'])
breakpoints = sorted(breakpoints)

In the example, this is [0, 2, 8, 17, 26, 34, 35, 46, 48, 49]. Now, create a new dataframe with these from and to values, then compute the intervals:
new_df = pd.DataFrame({'from': breakpoints[:-1], 'to': breakpoints[1:]})
new_df['int'] = new_df['to'] - new_df['from']

Result:
  from  to  int
0    0   2    2
1    2   8    6
2    8  17    9
3   17  26    9
4   26  34    8
5   34  35    1
6   35  46   11
7   46  48    2
8   48  49    1

",pandas
with pandascut how do i get integer bins and avoid getting a negative lowest bound,"My dataframe has zero as the lowest value. I am trying to use the precision and include_lowest parameters of pandas.cut(), but I can't get the intervals consist of integers rather than floats with one decimal. I can also not get the left most interval to stop at zero.
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

sns.set(style='white', font_scale=1.3)

df = pd.DataFrame(range(0,389,8)[:-1], columns=['value'])
df['binned_df_pd'] = pd.cut(df.value, bins=7, precision=0, include_lowest=True)
sns.pointplot(x='binned_df_pd', y='value', data=df)
plt.xticks(rotation=30, ha='right')


I have tried setting precision to -1, 0 and 1, but they all output one decimal floats. The pandas.cut() help does mention that the x-min and x-max values are extended with 0.1 % of the x-range, but I thought maybe include_lowest could suppress this behaviour somehow. My current workaround involves importing numpy:
import numpy as np

bin_counts, edges = np.histogram(df.value, bins=7)
edges = [int(x) for x in edges]
df['binned_df_np'] = pd.cut(df.value, bins=edges, include_lowest=True)

sns.pointplot(x='binned_df_np', y='value', data=df)
plt.xticks(rotation=30, ha='right')


Is there a way to obtain non-negative integers as the interval boundaries directly with pandas.cut() without using numpy?
Edit: I just noticed that specifying right=False makes the lowest interval shift to 0 rather than -0.4. It seems to take precedence over include_lowest, as changing the latter does not have any visible effect in combination with right=False. The following intervals are still specified with one decimal point.

","None of the other answers (including OP's np.histogram workaround) seem to work anymore. They have upvotes, so I'm not sure if something has changed over the years.
IntervalIndex requires all intervals to be closed identically, so [0, 53] cannot coexist with (322, 376].

Here are two working solutions based on the relabeling approach:

Without numpy, reuse pd.cut edges as pd.cut labels
bins = 7

_, edges = pd.cut(df.value, bins=bins, retbins=True)
labels = [f'({abs(edges[i]):.0f}, {edges[i+1]:.0f}]' for i in range(bins)]

df['bin'] = pd.cut(df.value, bins=bins, labels=labels)

#     value         bin
# 1       8     (0, 53]
# 2      16     (0, 53]
# ..    ...         ...
# 45    360  (322, 376]
# 46    368  (322, 376]


With numpy, convert np.linspace edges into pd.cut labels
bins = 7

edges = np.linspace(df.value.min(), df.value.max(), bins+1).astype(int)
labels = [f'({edges[i]}, {edges[i+1]}]' for i in range(bins)]

df['bin'] = pd.cut(df.value, bins=bins, labels=labels)

#     value         bin
# 1       8     (0, 53]
# 2      16     (0, 53]
# ..    ...         ...
# 45    360  (322, 376]
# 46    368  (322, 376]



Note: Only the labels are changed, so the underlying binning will still occur with 0.1% margins.

pointplot() output (as of pandas 1.2.4):
sns.pointplot(x='bin', y='value', data=df)
plt.xticks(rotation=30, ha='right')


",pandas
how to solve an overflowerror when exporting pandas dataframe to json,"In Jupyter, I have a dataframe of 400 000 objects that I can't entirely export to a JSON file without facing the following error.
The export is working great as long as I limit the exportation to the first 141 000 objects , whatever the order of these first objects.
Should I be aware of any size limitation dealing with large JSON files ?
Thank you.
OverflowError                             Traceback (most recent call last)
<ipython-input-254-b59373f1eeb2> in <module>
----> 1 df4.to_json('test.json', orient = 'records')

~/anaconda3/lib/python3.7/site-packages/pandas/core/generic.py in to_json(self, path_or_buf, orient, date_format, double_precision, force_ascii, date_unit, default_handler, lines, compression, index)
   1889                             default_handler=default_handler,
   1890                             lines=lines, compression=compression,
-> 1891                             index=index)
   1892 
   1893     def to_hdf(self, path_or_buf, key, **kwargs):

~/anaconda3/lib/python3.7/site-packages/pandas/io/json/json.py in to_json(path_or_buf, obj, orient, date_format, double_precision, force_ascii, date_unit, default_handler, lines, compression, index)
     56         double_precision=double_precision, ensure_ascii=force_ascii,
     57         date_unit=date_unit, default_handler=default_handler,
---> 58         index=index).write()
     59 
     60     if lines:

~/anaconda3/lib/python3.7/site-packages/pandas/io/json/json.py in write(self)
     99         return self._write(self.obj, self.orient, self.double_precision,
    100                            self.ensure_ascii, self.date_unit,
--> 101                            self.date_format == 'iso', self.default_handler)
    102 
    103     def _write(self, obj, orient, double_precision, ensure_ascii,

~/anaconda3/lib/python3.7/site-packages/pandas/io/json/json.py in _write(self, obj, orient, double_precision, ensure_ascii, date_unit, iso_dates, default_handler)
    154                                                double_precision,
    155                                                ensure_ascii, date_unit,
--> 156                                                iso_dates, default_handler)
    157 
    158 

~/anaconda3/lib/python3.7/site-packages/pandas/io/json/json.py in _write(self, obj, orient, double_precision, ensure_ascii, date_unit, iso_dates, default_handler)
    110             date_unit=date_unit,
    111             iso_dates=iso_dates,
--> 112             default_handler=default_handler
    113         )
    114 

OverflowError: int too big to convert

","There is no inherent limitation on data size in JSON, so this isn't your problem: the message suggests some difficulty with a particular integer value.
This underlines the difficulty of working with such large files, since you now have to isolate the particular record that's causing the problems in the middle of the to_json call.
Since you know roughly where the problem is you could try converting subsets of your data frame in a bisection technique to home in on the row that's causing the issues.
",pandas
scraping dynamic data table with no easy references,"I'm trying to get the data from a simple table from the following website (https://bvmf.bmfbovespa.com.br/clube-de-investimento/clube-de-investimento.aspx?Idioma=pt-br). I was able to get the data from the first page, but as we can see the pagination it's not linked to the URL and I couldn't get it, even though i could find the buttons at the bottom of the page  ""ProximoPaginacao"" and ""MeioPaginacao"", but i couldn't handle this implementation. Any ideas?
import requests
from bs4 import BeautifulSoup
import pandas as pd

def extract_table_data(url, table_id):

try:
    response = requests.get(url,verify=False)
    response.raise_for_status()
    html_content = response.text
    soup = BeautifulSoup(html_content, 'html.parser')
    table = soup.find('table', id=table_id)
    if not table:
        print(f""Table with ID '{table_id}' not found."")
        return None

    # Extract header row
    header_row = [th.get_text(strip=True) for th in table.find_all('th')]

    # Extract data rows
    data_rows = []
    for row in table.find('tbody').find_all('tr'):
        data_rows.append([td.get_text(strip=True) for td in row.find_all('td')])

    # Create DataFrame
    df = pd.DataFrame(data_rows, columns=header_row)
    return df
except requests.exceptions.RequestException as e:
    print(f""Error during requests: {e}"")
    return None
except Exception as e:
    print(f""An error occurred: {e}"")
    return None

# Example usage
url = ""https://bvmf.bmfbovespa.com.br/clube-de-investimento/clube-de-investimento.aspx? 
Idioma=pt-br""  # Replace with the actual URL
table_id = ""ctl00_contentPlaceHolderConteudo_grdAtivo_ctl01""  # Replace with the actual 
table ID
table_data = extract_table_data(url, table_id)

if table_data is not None:
   print(table_data)

","You can get all the pages using requests & BeautifulSoup, without selenium:
import requests
from bs4 import BeautifulSoup
import pandas as pd


def extract_all_tables(url):
    requests.packages.urllib3.disable_warnings()
    headers = {
        'Content-Type': 'application/x-www-form-urlencoded',
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/132.0.0.0 Safari/537.36',
    }
    data = {}

    tables = []
    while True:
        print(f'Scraping table #{len(tables) + 1}')
        response = requests.post(url, headers=headers, data=data, verify=False)
        response.raise_for_status()

        soup = BeautifulSoup(response.text, 'html.parser')
        table = soup.select_one('table#ctl00_contentPlaceHolderConteudo_grdAtivo_ctl01')
        header_row = [th.get_text(strip=True) for th in table.thead.select('th')]
        data_rows = [[td.get_text(strip=True) for td in tr.select('td')] for tr in table.tbody.select('tr')]
        
        df = pd.DataFrame(data_rows, columns=header_row)
        tables.append(df)

        next_button = table.tfoot.select_one('td.ProximoPaginacao > input')
        if not next_button:
            break

        data['__VIEWSTATE'] = soup.select_one('input#__VIEWSTATE').get('value')
        data['__EVENTTARGET'] = next_button.get('name').replace('$', ':')

    return tables


url = 'https://bvmf.bmfbovespa.com.br/clube-de-investimento/clube-de-investimento.aspx?idioma=pt-br'
tables = extract_all_tables(url)
print(f'{len(tables) = }')

",pandas
pandas dataframe update with filter_func,"I have two dataframes with identical shape and want to update df1 with df2 if some conditions are met
import pandas as pd
from typing import Any


df1 = pd.DataFrame({""A"": [1, 2, 3], ""B"": [4, 5, 6]})
print(df1, ""\n"")
df2 = pd.DataFrame({""A"": [7, 8, 9], ""B"": [10, 3, 12]})
print(df2, ""\n"")

# Define a condition function
def condition(x: Any) -> bool:
    """"""condition function to update only cells matching the conditions""""""
    return True if x in [2, 7, 9] else False

df1.update(df2)
print(df1)

but if I use filter_func df1.update(df2,filter_func=condition) it fails with ValueError: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all() 
Unfortunately the docu is not very verbose.
How to update a dataframe with conditions correctly?
","Your function will receive a 1D numpy array (per column), it should be vectorized and return a boolean 1D array (callable(1d-array) -> bool 1d-array).
Use numpy.isin to test membership:
def condition(x):
    """"""condition function to update only cells matching the conditions""""""
    return np.isin(x, [2, 7, 9])

df1.update(df2, filter_func=condition)

with a lambda:
df1.update(df2, filter_func=lambda x: np.isin(x, [2, 7, 9]))

Alternatively, if you really can't vectorize with pure numpy functions (this should not be done here!), decorate with numpy.vectorize:
@np.vectorize
def condition(x: Any) -> bool:
    """"""condition function to update only cells matching the conditions""""""
    return True if x in [2, 7, 9] else False

df1.update(df2, filter_func=condition)

Updated df1:
   A  B
0  1  4
1  8  5
2  3  6

",pandas
merge two dataframes by index,"I have the following dataframes:
> df1
  id  begin conditional confidence discoveryTechnique  
0 278    56       false        0.0                  1   
1 421    18       false        0.0                  1 

> df2
   concept 
0  A  
1  B

How do I merge on the indices to get:
  id  begin conditional confidence discoveryTechnique concept 
0 278    56       false        0.0                  1       A 
1 421    18       false        0.0                  1       B

I ask because it is my understanding that merge() i.e. df1.merge(df2) uses columns to do the matching. In fact, doing this I get:
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/usr/local/lib/python2.7/dist-packages/pandas/core/frame.py"", line 4618, in merge
    copy=copy, indicator=indicator)
  File ""/usr/local/lib/python2.7/dist-packages/pandas/tools/merge.py"", line 58, in merge
    copy=copy, indicator=indicator)
  File ""/usr/local/lib/python2.7/dist-packages/pandas/tools/merge.py"", line 491, in __init__
    self._validate_specification()
  File ""/usr/local/lib/python2.7/dist-packages/pandas/tools/merge.py"", line 812, in _validate_specification
    raise MergeError('No common columns to perform merge on')
pandas.tools.merge.MergeError: No common columns to perform merge on

Is it bad practice to merge on index? Is it impossible? If so, how can I shift the index into a new column called ""index""?
","Use merge, which is an inner join by default:
pd.merge(df1, df2, left_index=True, right_index=True)

Or join, which is a left join by default:
df1.join(df2)

Or concat, which is an outer join by default:
pd.concat([df1, df2], axis=1)

Samples:
df1 = pd.DataFrame({'a':range(6),
                    'b':[5,3,6,9,2,4]}, index=list('abcdef'))

print (df1)
   a  b
a  0  5
b  1  3
c  2  6
d  3  9
e  4  2
f  5  4

df2 = pd.DataFrame({'c':range(4),
                    'd':[10,20,30, 40]}, index=list('abhi'))

print (df2)
   c   d
a  0  10
b  1  20
h  2  30
i  3  40


# Default inner join
df3 = pd.merge(df1, df2, left_index=True, right_index=True)
print (df3)
   a  b  c   d
a  0  5  0  10
b  1  3  1  20

# Default left join
df4 = df1.join(df2)
print (df4)
   a  b    c     d
a  0  5  0.0  10.0
b  1  3  1.0  20.0
c  2  6  NaN   NaN
d  3  9  NaN   NaN
e  4  2  NaN   NaN
f  5  4  NaN   NaN

# Default outer join
df5 = pd.concat([df1, df2], axis=1)
print (df5)
     a    b    c     d
a  0.0  5.0  0.0  10.0
b  1.0  3.0  1.0  20.0
c  2.0  6.0  NaN   NaN
d  3.0  9.0  NaN   NaN
e  4.0  2.0  NaN   NaN
f  5.0  4.0  NaN   NaN
h  NaN  NaN  2.0  30.0
i  NaN  NaN  3.0  40.0

",pandas
how to merge two data frames based on particular column in pandas python,"I have to merge two dataframes:
df1
company,standard
tata,A1
cts,A2
dell,A3

df2
company,return
tata,71
dell,78
cts,27
hcl,23

I have to unify both dataframes to one dataframe. I need output like:
company,standard,return
tata,A1,71
cts,A2,27
dell,A3,78

","Use merge:
print (pd.merge(df1, df2, on='company'))

Sample:
print (df1)
  company standard
0    tata       A1
1     cts       A2
2    dell       A3

print (df2)
  company  return
0    tata      71
1    dell      78
2     cts      27
3     hcl      23

print (pd.merge(df1, df2, on='company'))
  company standard  return
0    tata       A1      71
1     cts       A2      27
2    dell       A3      78

",pandas
row by row update  change of values based on 2nd dataframe with conditions,"i have got a Condition Dataframe like this which has about 300 rows
pd.DataFrame({""PERSONALNR"":[""000009461"",""000009461""],""PERIODE"":[""202401"",""202402""],""MANDANT"":[""LB"",""LB""],""DA"":[""01"",""01""]})

where ""PERSONALNR"" and ""PERIODE"" are the conditions i need to meet and the values ""MANDANT"" and ""DA"" need to be replaced in the second Dataframe
The Dataframe i want to replace values in looks similar to this one, which has about 110k rows
pd.DataFrame({""PERSONALNR"":[""000009461"",""000009461""],""PERIODE"":[""202401"",""202402""],""MANDANT"":[""LB"",""LB""],""DA"":[""01"",""01""], ""KSTBEZ"":[""Springer pool"",""bla bla""]})

and the solution i came up with is the following:
for row in POOL.itertuples():
    LA.loc[(LA.PERSONALNR==row.PERSONALNR)&(LA.PERIODE==row.PERIODE)&(LA.DA==""01"")&(LA.KSTBEZ.str.contains(""pool"")),[""MANDANT"",""DA""]]=[row.MANDANT,row.DA]

My solution works for the Dataframe above quite ok - takes about 10 seconds or so to finish, but i need to do the same operation in a dataframe with 1 Million rows - there it takes about 10 minutes...
can anyone come up with a better solution?
","Assuming you don't have duplicated combinations of PERSONALNR/PERIODE in POOL.
You can use a merge and boolean indexing:
mask = LA['DA'].eq('01') & LA['KSTBEZ'].str.contains('pool')
tmp = (LA[['PERSONALNR', 'PERIODE']].reset_index()
       .merge(POOL, on=['PERSONALNR', 'PERIODE'], how='left')
       .set_index('index')
      )

LA.loc[mask, ['MANDANT', 'DA']] = tmp.loc[mask, ['MANDANT', 'DA']]

Output (using a trailing x in the MANDANT/DA values from POOL for the demo):
  PERSONALNR PERIODE MANDANT   DA         KSTBEZ
0  000009461  202401     LBx  01x  Springer pool
1  000009461  202402      LB   01        bla bla

Why do we need reset_index/set_index?
After the merge, the index is lost, which wouldn't allow correct alignment with the mask or for the assignment to the input LA.
Example:
LA = pd.DataFrame({'PERSONALNR': ['000009461', '000009461'],
                   'PERIODE': ['202401', '202402'],
                   'MANDANT': ['LB', 'LB'],
                   'DA': ['01', '01'],
                   'KSTBEZ': ['Springer pool', 'bla bla']},
                  index=[2, 0]) # Note the non-range index
POOL = pd.DataFrame({'PERSONALNR': ['000009461', '000009461'],
                     'PERIODE': ['202401', '202402'],
                     'MANDANT': ['LBx', 'LBx'],
                     'DA': ['01x', '01x']})

mask = LA['DA'].eq('01') & LA['KSTBEZ'].str.contains('pool')
tmp = (LA[['PERSONALNR', 'PERIODE']]#.reset_index()
       .merge(POOL, on=['PERSONALNR', 'PERIODE'], how='left')
       #.set_index('index')
      )

LA.loc[mask, ['MANDANT', 'DA']] = tmp.loc[mask, ['MANDANT', 'DA']]

Output:
IndexingError: Unalignable boolean Series provided as indexer (index of the boolean Series and of the indexed object do not match).

",pandas
how to use vectorized calculations in pandas to find out where a value or category is changing with corrected first row,"With a dataset with millions of records, I have items with various categories and measurements, and I'm trying to figure out how many of the records have changed, in particular when the category or measurement goes to NaN (or NULL from the database query) during the sequence.
In SQL, I'd use some PARTITION style OLAP functions to do this, but seems like it should fairly straightforward in Python with Pandas, but I can't quite wrap my head around the vectorized notation.
I've tried various df.groupby clauses and lambda functions but nothing quite gets it in the required format - basically, the df.groupby('item')['measure'] in this example, the first row of the grouped subset of item & measure always returns True, where I'd like to it to be False or NaN. Simply put, they are false positives. I understand from pandas' perspective, it's a change since the first x.shift() would be NaN, but I can't figure out how to filter that or handle it in the lambda function.
Sample Code:
import pandas as pd
import numpy as np

test_df = pd.DataFrame({'item': [20, 20, 20, 20, 20, 20, 20, 20, 30, 30, 30, 30, 30, 30, 30, 30, 40, 40, 40, 40, 40, 40, 40, 40 ],
                        'measure': [1, 1, 1, 3, 3, 3, 3, 3, 6, 6, 6, 6, 6, 7, 7, 7, 10, 10, 10, 10, 10, 10, 10, 10 ],
                        'cat': ['a', 'a', 'a', 'b', 'b', 'b', 'b', 'b', 'c', 'c', 'c', 'c', 'c', 'd', 'd', 'd', 'e', 'e', 'e', 'e', 'e', 'e', 'e', 'e']})

test_df['measure_change'] = test_df.groupby('item')['measure'].transform(lambda x: x.shift() != x)
test_df['cat_change'] = test_df.groupby('item')['cat'].transform(lambda x: x.shift() != x)

In the output below, as an example, rows 0, 8, and 16, the measure_change should be False. So all of item 40 would have measure_change == False and that would indicate no changes with that item. Any & all suggestions are appreciated.
(cat_change set up the same way)



#
item
measure
measure_change




0
20
1
True


1
20
1
False


2
20
1
False


3
20
3
True


4
20
3
False


5
20
3
False


6
20
3
False


7
20
3
False


8
30
6
True


9
30
6
False


10
30
6
False


11
30
6
False


12
30
6
False


13
30
7
True


14
30
7
False


15
30
7
False


16
40
10
True


17
40
10
False


18
40
10
False


19
40
10
False


20
40
10
False


21
40
10
False


22
40
10
False


23
40
10
False



","You can use a combination of groupby.diff and fillna to achieve this. We compare the row difference with 0 to find any rows where measure changed:
test_df['measure_change'] = test_df.groupby('item')['measure'].diff().fillna(0) != 0

Result:
    item  measure  measure_change
0     20        1           False
1     20        1           False
2     20        1           False
3     20        3            True
4     20        3           False
5     20        3           False
6     20        3           False
7     20        3           False
8     30        6           False
9     30        6           False
10    30        6           False
11    30        6           False
12    30        6           False
13    30        7            True
14    30        7           False
15    30        7           False
16    40       10           False
17    40       10           False
18    40       10           False
19    40       10           False
20    40       10           False
21    40       10           False
22    40       10           False
23    40       10           False


Alternativly, if you have strings to compare as well you can add a secondary condition checking the shift value for nans: x.shift().notna().
test_df['measure_change'] = test_df.groupby('item')['measure'].transform(lambda x: (x != x.shift()) & (x.shift().notna()))

",pandas
attributeerror 39styler39 object has no attribute 39style39,"This is my DataFrame:
import pandas as pd
df = pd.DataFrame(
    {
        'a': [2, 2, 2, -4, 4, 4, 4, -3, 2, -2, -6],
        'b': [2, 2, 2, 4, 4, 4, 4, 3, 2, 2, 6]
    }
)

I use a function to highlight cells in a when I use to_excel:
def highlight_cells(s):
    if s.name=='a':
        conds = [s > 0, s < 0, s == 0]
        labels = ['background-color: lime', 'background-color: pink', 'background-color: gold']
        array = np.select(conds, labels, default='')
        return array

    else:
        return ['']*s.shape[0]

Now I want to add one more feature by adding plus sign if a value in a is positive. For example 1 becomes +1. I want this feauture only for column a.
This is my attempt but it does not work. It gives me the error that is the title of the post.
df.style.apply(highlight_cells).style.format({'a': '{:+g}'}).to_excel('df.xlsx', sheet_name='xx', index=False)

","style.apply already returns a Styler object, you only need further do operation on this, i.e.
df.style.apply(highlight_cells).format(...)
#                               ^
#                               |
#                               No need .style again

",pandas
converting all columns in spark df from decimal to float for pandas conversion,"I want to create a function to transform the datatype of all spark dataframe columns from decimal to float.
I do not know my column names in advance, nor if and how many columns of the type of decimal are included. This excludes explicit casting of columns to prevent scaling limitations.
Other data type columns should not be affected.
NULLS might occur.
Reason behind all this madness: I need to convert the spark dataframe to pandas, to then be able to write an xlsx file. The transformation to pandas of decimal however results in an object type, which is stored in the xlsx file as text, not as a number.
Sample code:
df = spark.sql(""select 'text' as txt, 1.1111 as one, 2.22222 as two, CAST(3.333333333333 AS FLOAT) as three"")
df.printSchema()

>>
root
 |-- txt: string (nullable = false)
 |-- one: decimal(5,4) (nullable = false)
 |-- two: decimal(6,5) (nullable = false)
 |-- three: float (nullable = false)

Transform to Pandas:
df_pd = df.toPandas()
print(df_pd.dtypes)

>>
txt       object
one       object
two       object
three    float32
dtype: object

I need all of the decimal types to be of float type in df_pd.
Ideally I have something like this:
df = spark.sql(""select 'text' as txt, 1.1111 as one, 2.22222 as two, 3.333333333333 as three"")

insert magic

df.printSchema()

>>
root
 |-- txt: string (nullable = false)
 |-- one: float (nullable = false)
 |-- two: float (nullable = false)
 |-- three: float (nullable = false)

Thanks
","To resolve your issue, please follow below code. For the sample, I use the above four columns as a data frame and convert it into a Temp table.
Code:
from pyspark.sql.functions import col
from pyspark.sql.types import DecimalType, FloatType

df1 = spark.sql(""""""
    SELECT 'text' AS txt, 
           CAST(1.1111 AS DECIMAL(5,4)) AS one, 
           CAST(2.22222 AS DECIMAL(6,5)) AS two, 
           CAST(3.333333333333 AS FLOAT) AS three
"""""")

df1.createOrReplaceTempView(""deci_table"")

def convert_decimal_to_float_from_table(table_name):
    df12 = spark.sql(f""SELECT * FROM {table_name}"")

    # check the decimal columns values
    decimal_columns = [field.name for field in df12.schema.fields if isinstance(field.dataType, DecimalType)]

    # using below for loop you can convert decimal columns to float 
    for col_name in decimal_columns:
        df12 = df12.withColumn(col_name, col(col_name).cast(FloatType()))

    return df12

df1.printSchema()
df12_conv = convert_decimal_to_float_from_table(""deci_table"")

df12_conv.printSchema()

df_pd = df12_conv.toPandas()
print(df_pd.dtypes)

Output:

",pandas
python error  indexerror single positional indexer is outofbounds,"I'm trying to write a Python code to download NSE stocks data for all the listed companies. There are just over 1700 listed companies in NSE India. I want data for only last 120 days. I have writen the below code but its throwing the below error, tried several ways to fix this but its still not working. Any guidance to resolve this issue would be much appreciated. Note that I have declared all the necessary variables that are appearing in my code.
Error:
IndexError: single positional indexer is out-of-bounds
My Python version - 3.13
def getHistoricalData(symInfo):
    res = None
    try:
        parseInstrument = urllib.parse.quote(symInfo.instrument_key)
        fromDate = (datetime.now(TIME_ZONE) - timedelta(days=120)) . strftime(""%Y-%m-%d"")
        todate = datetime.now(TIME_ZONE).strftime(""%Y-%m-%d"")        
        url = f'https://api.upstox.com/v2/historical-candle/{parseInstrument}/day/{todate}/{fromDate}'        
        res = requests.get(url, headers={'accept': 'application/json', }, params={}, timeout=5.0)
        candleRes = res.json()

        if 'data' in candleRes and 'candles' in candleRes['data'] and candleRes['data']['candles']:
            candleData = pd.DataFrame(candleRes['data']['candles'])
            candleData.columns = ['date', 'open', 'high', 'low', 'close', 'vol', 'oi']
            candleData = candleData[['date', 'open', 'high', 'low', 'close', 'vol', 'oi']]
            candleData['date'] = pd.to_datetime(candleData['date']).dt.tz_convert('Asia/Kolkata')
            candleData['symbol'] = symInfo.tradingsymbol            
            return candleData
        else:
            print('No data', symInfo.instrument_key, candleRes)
    except:
        print(f'Error in data fetch for (symInfo.instrument_key) {res} {e}')

candledfList = []
for i in symboldf.index[:-1]:
    candledfList = []
    res = getHistoricalData(symboldf.iloc[i])
    if res is not None: candledfList.append(res)
    finalDataDf = pd.concat(candledfList, ignore_index=True)

    isCsv = True
    for symData in candledfList[:-1]:
        try:
            filename = symData.iloc[0]['symbol']
            if isCsv:
                filename = f'{filename}.csv'
                symData.to_csv(folder + filename, index=False)
                del candledfList
            else:
                filename = f'{filename}.parquet'
                symData.to_parquet(folder_parquet + filename, engine='pyarrow')
                del candledfList
        except Exception as e:
            print(f'Error {e}')

","I have managed to resolve this issue. The reason for the error is very simple -

I had to remove i from res = getHistoricalData(symboldf.**i**loc[i]) to make it res = getHistoricalData(symboldf.loc[i]). This will solve the issue.

    candledfList = []
    for i in symboldf.index[:-1]:
        candledfList = []
        res = getHistoricalData(symboldf.iloc[i])
        if res is not None: candledfList.append(res)
        finalDataDf = pd.concat(candledfList, ignore_index=True)


The for loop below is to generate single/separate file for each company stock data. We can remove this and move the logic to for loop above this and generate one single CSV file to capture all companies data. But the problem with this approach is we will lose data for several companies, not sure why. I lost 36 companies data when tried to capture all companies data into one single CSV file.

isCsv = True
    for symData in candledfList[:-1]:
        try:
            filename = symData.iloc[0]['symbol']
            if isCsv:
                filename = f'{filename}.csv'
                symData.to_csv(folder + filename, index=False)
                del candledfList
            else:
                filename = f'{filename}.parquet'
                symData.to_parquet(folder_parquet + filename, engine='pyarrow')
                del candledfList
        except Exception as e:
            print(f'Error {e}')

",pandas
vs code problem quotimport quotpandasquot could not be resolved from sourcequot,"I am running a virtual environment for my project in VS code, and installed pandas and PyMuPDF but I cannot fix the flagged problems below. Please note that I am using a WSL command terminal with a Windows OS.

""Import ""pandas"" could not be resolved from source Pylance(reportMissingImports)""
""Import ""pymupdf"" could not be resolved Pylance(reportMissingImports)""

I've looked at prior stackoverflow threads, but the suggestions have not been able to fix this issue. I made sure that my packages are installed properly (see screenshot). My installed panadas version is 2.2.3 and my python version is 3.12.3 in the virtual environment.

This leads me to believe that I do not have the correct Python interpreter selected, but when I browse my file system to select a different interpreter in the venv/bin folder, it's empty so I cannot select a python interpreter. However, when I run ""ls venv/vin"", it shows that there are several Python interpreters installed (see screenshot).

I tried manually adding a path to my settings.json file to the Python interpreter, but that did not work either.
I do not know what to do from this point. If someone could please help, that would be much appreciated!
","The simplest way to solve this is to create a new virtual environment, VScode usually recognizes that a new venv has been created and allows you to switch to that new environment.
in your vscode terminal, run rm -r {your env folder}, then python venv .venv (the name should be "".venv"", sometimes it helps vscode recognize that this is a venv). You should receive a small modal at the bottom right of the screen for switching to the new environment. If you don't receive it, use source ./.venv/Scripts/Activate.
If all of this doesn't work, try running wsl code . to open vscode using wsl. It's very much possible that vscode is looking for a windows venv and fails to find a linux venv.
",pandas
how to slice a pandas dataframe by position,"I have a Pandas Data Frame object that has 1000 rows and 10 columns. I would simply like to slice the Data Frame and take the first 10 rows. How can I do this? I've been trying to use this:
>>> df.shape
(1000,10)
>>> my_slice = df.ix[10,:]
>>> my_slice.shape
(10,)

Shouldn't my_slice be the first ten rows, ie. a 10 x 10 Data Frame? How can I get the first ten rows, such that my_slice is a 10x10 Data Frame object? Thanks.
","http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.head.html?highlight=head#pandas.DataFrame.head
df2 = df.head(10)

should do the trick
",pandas
changing a pandas dataframe format into another format,"The given dataframe looks like this:

                     sensorA  sensorB  deviceA  deviceB  inputA  inputB  machineA  machineB  flagA  flagB  mainA
Time                                                                                                            
2021-11-26 20:20:00    379.0      0.0      0.0    489.0    0.77    35.0       0.0      51.0  -13.0  230.0    1.6
2021-11-26 20:30:00    344.0      0.0      0.0    143.0    0.76    31.0       0.0      50.0  -11.0  230.0    1.8

I want to map this to a the following format separating the individual columns into a combination of field and attribute.



Time
Type
attribute
Value




2021-11-26 20:20:00
sensor
a
999



I have tried multiple directions to approach this using multi indexing, groupby etc but cant seem to get around on how to exactly implement this ?
Any help would be appreciated!!
","Edit
If your column names contain '_' as separator, you can use:
df.columns = df.columns.str.split('_', expand=True).rename(['Type', 'Tag'])
out = df.unstack().rename('Value').reset_index(level=['Type', 'Tag']).sort_index()


Extract type/tag from column names with a regular expression:
types = ['sensor', 'device', 'input', 'machine', 'flag', 'main']
pat = fr""({'|'.join(types)})(.*)""

df.columns = pd.MultiIndex.from_frame(df.columns.str.extract(pat), 
                                      names=['Type', 'Tag'])
out = df.unstack().rename('Value').reset_index(level=['Type', 'Tag']).sort_index()

Output:
>>> out
                        Type Tag   Value
Time                                    
2021-11-26 20:20:00   sensor   A  379.00
2021-11-26 20:20:00     flag   B  230.00
2021-11-26 20:20:00     flag   A  -13.00
2021-11-26 20:20:00  machine   B   51.00
2021-11-26 20:20:00  machine   A    0.00
2021-11-26 20:20:00     main   A    1.60
2021-11-26 20:20:00    input   A    0.77
2021-11-26 20:20:00    input   B   35.00
2021-11-26 20:20:00   device   B  489.00
2021-11-26 20:20:00   device   A    0.00
2021-11-26 20:20:00   sensor   B    0.00
2021-11-26 20:30:00    input   A    0.76
2021-11-26 20:30:00   device   A    0.00
2021-11-26 20:30:00    input   B   31.00
2021-11-26 20:30:00  machine   A    0.00
2021-11-26 20:30:00   sensor   B    0.00
2021-11-26 20:30:00  machine   B   50.00
2021-11-26 20:30:00     flag   A  -11.00
2021-11-26 20:30:00   sensor   A  344.00
2021-11-26 20:30:00     flag   B  230.00
2021-11-26 20:30:00   device   B  143.00
2021-11-26 20:30:00     main   A    1.80

",pandas
extract row with maximum value in a group pandas dataframe,"A similar question is asked here:
Python : Getting the Row which has the max value in groups using groupby
However, I just need one record per group even if there are more than one record with maximum value in that group. 
In the example below, I need one record for ""s2"". For me it doesn't matter which one. 
>>> df = DataFrame({'Sp':['a','b','c','d','e','f'], 'Mt':['s1', 's1', 's2','s2','s2','s3'], 'Value':[1,2,3,4,5,6], 'count':[3,2,5,10,10,6]})
>>> df
   Mt Sp  Value  count
0  s1  a      1      3
1  s1  b      2      2
2  s2  c      3      5
3  s2  d      4     10
4  s2  e      5     10
5  s3  f      6      6
>>> idx = df.groupby(['Mt'])['count'].transform(max) == df['count']
>>> df[idx]
   Mt Sp  Value  count
0  s1  a      1      3
3  s2  d      4     10
4  s2  e      5     10
5  s3  f      6      6
>>> 

","You can use first
In [14]: df.groupby('Mt').first()
Out[14]: 
   Sp  Value  count
Mt                 
s1  a      1      3
s2  c      3      5
s3  f      6      6

Update
Set as_index=False to achieve your goal
In [28]: df.groupby('Mt', as_index=False).first()
Out[28]: 
   Mt Sp  Value  count
0  s1  a      1      3
1  s2  c      3      5
2  s3  f      6      6 

Update Again
Sorry for misunderstanding what you mean. You can sort it first if you want the one with max count in a group
In [196]: df.sort('count', ascending=False).groupby('Mt', as_index=False).first()
Out[196]: 
   Mt Sp  Value  count
0  s1  a      1      3
1  s2  e      5     10
2  s3  f      6      6

",pandas
select rows with highest value from groupby,"I have a dataframe that contains some information about users. There is a column for user, column for type, and column for count, like this:
name         type     count
robert       x        123
robert       y        456
robert       z        5123
charlie      x        442123
charlie      y        0 
charlie      z        42

I'm trying to figure out which type has the highest count per name, so for this case, I would want to select this:
name         type    count
robert       z       5123
charlie      x       442123

I know I can do something like this to get the max count per name, but I'm not sure how I can include the ""type"" column, which is actually the most important
df.sort_values('count', ascending=False).drop_duplicates('name').sort_index()

","What if you have two maxes for a name with different types:
print(df)

      name type   count
0   robert    x     123
1   robert    y     456
2   robert    z    5123
3   robert    a    5123
4  charlie    x  442123
5  charlie    y       0
6  charlie    z      42

Use boolean indexing:
df[df['count'] == df.groupby('name')['count'].transform('max')]

Output:
      name type   count
2   robert    z    5123
3   robert    a    5123
4  charlie    x  442123

",pandas
how can i get rows which have the max value of the group to which they belong,"I reword my question. I'm searching solution for the following problem:
I have a dataFrame like:
   Sp   Mt   Value  count
4  MM2  S4   bg     10
5  MM2  S4   dgd    1
6  MM4  S2   rd     2
7  MM4  S2   cb     8
8  MM4  S2   uyi    8

My objective is to get ALL the rows where count equal max in each group e.g. :
MM4  S4   bg     10
MM4  S2   cb     8
MM4  S2   uyi    8

I group by ['Sp','Mt']
Somebody knows how can I do it in pandas or in python?
",">>> print d
     Sp  Mt Value  Count
ID                      
4   MM2  S4    bg     10
5   MM2  S4   dgd      1
6   MM4  S2    rd      2
7   MM4  S2    cb      8
8   MM4  S2   uyi      8

>>> d.groupby('Sp').apply(lambda t: t[t.Count==t.Count.max()])
         Sp  Mt Value  Count
Sp  ID                      
MM2 4   MM2  S4    bg     10
MM4 7   MM4  S2    cb      8
    8   MM4  S2   uyi      8

",pandas
how to set a column which suffix name is based on a value in another column,"#Column X contains the suffix of one of V* columns. Need to put set column V{X} to 9 if X > 1.  
#But my code created a new column 'VX' instead of updating one of the V* columns
    
import pandas as pd

df = pd.DataFrame({'EMPLID': [12, 13, 14, 15, 16, 17, 18],
    'V1': [2,3,4,50,6,7,8],
    'V2': [3,3,3,3,3,3,3],
    'V3': [7,15,8,9,10,11,12],
    'X': [2,3,1,3,3,1,2]
})

# Expected output:
     
#    EMPLID  V1  V2  V3  X  
#    12       2   9   7  2  
#    13       3   3   9  3 
#    14       4   3   8  1
#    15      50   3   9  3 
#    16       6   3   9  3
#    17       7   3  11  1  
#    18       8   9  12  2 

My code created a new column 'VX' instead of updating one of the V* columns:
df.loc[(df['X'] > 1), f""V{'X'}""] = 9

Any suggestion is appreciated. Thank you.
","# your dataframe
df = pd.DataFrame({'EMPLID': [12, 13, 14, 15, 16, 17, 18],
    'V1': [2,3,4,50,6,7,8],
    'V2': [3,3,3,3,3,3,3],
    'V3': [7,15,8,9,10,11,12],
    'X': [2,3,1,3,3,1,2]
})

First, we get the columns names that we want to change and their indices on the original dataframe.
# column name
x = df['X'][(df['X'] > 1)]
# column names mapped to your scenario
columns = [f'V{v}' for v in x]
# desired indexes
positions = x.index.values
#Then we convert the column names to indices and use these indices to update the positions matching the conditions.
column_indices = [df.columns.get_loc(col) for col in columns]

Now, we can use two approaches here.
A vectorized approach
Convert the dataframe to numpy array, change the desired positions all at once and change the result back to a dataframe.

import numpy as np

# the original dataframe column names
column_names = df.columns
# convert the dataframe to numpy (this will 'remove' the column names)
df_array = df.values
# put the columns back (axis=0 will stack the columns at the top of the array)
df_array = np.concatenate([[column_names], df_array], axis=0)
# position+1 because when using pandas to get the row index, we ignore the columns (which would have index 0)
df_array[positions+1, column_indices] = 9
# convert the result back to a dataframe
df = pd.DataFrame(df_array[1:], columns=column_names)

print(df)

Output:
 EMPLID  V1  V2  V3  X
0      12   2   9   7  2
1      13   3   3   9  3
2      14   4   3   8  1
3      15  50   3   9  3
4      16   6   3   9  3
5      17   7   3  11  1
6      18   8   9  12  2

A loop approach
The easiest way would be just to loop over the rows and columns, changing one value at a time.
for row, column in zip(positions, column_indices):
    df.iat[row,column] = 9

If your dataframe is small, the vectorized approach won't have as much of an advantage over the for loop.
",pandas
how to improve code performance  using google translate api ,"import time
start = time.time()
import pandas as pd
from deep_translator import GoogleTranslator
    
data = pd.read_excel(r""latestdata.xlsx"")
translatedata = data['column']. fillna('novalue')
    
list = []
for i in translatedata:
      finaldata = GoogleTranslator(source='auto', target='english').translate(i)
      print(finaldata)
      list.append(finaldata)
    
df = pd.DataFrame(list, columns=['Translated_values'])
df.to_csv(r""jobdone.csv"", sep= ';')
    
end = time.time()

print(f""Runtime of the program is {end - start}"")

I have data of 220k points and trying to translate a column data At first I tried to use pool method parallel program but got an error that I can not access API several time at once. My question is if there is other way to improve performance of code that I have right now.
# 4066.826668739319     with just 10000 data all together.
# 3809.4675991535187    computation time when I run in 2 batch's of 5000

","
Q :"" ... is ( there ) other way to improve performance of code ...? ""

A :Yes, there are a few ways,yet do not expect anything magical, as you have already reported the API-provider's throttling/blocking somewhat higher levels of concurrent API-call from being served
There still might be some positive effects from latency-masking tricks from a just-[CONCURRENT] orchestration of several API-calls, as the End-to-End latencies are principally ""long"" as going many-times across the over-the-""network""-horizons and having also some remarkable server-side TAT-latency on translation-matching engines.
Details matter, a lot...
A performance boosting code-template to start with( avoiding 220k+ repeated local-side overheads' add-on costs ) :
import time
import pandas as pd
from   deep_translator import GoogleTranslator as gXLTe
    
xltDF = pd.read_excel( r""latestdata.xlsx"" )['column'].fillna( 'novalue' )
resDF = xltDF.copy( deep = True )

PROC_ns_START = time.perf_counter_ns()
#________________________________________________________ CRITICAL SECTION: start
for                  i in range( len( xltDF ) ):
         resDF.iloc( i ) = gXLTe( source = 'auto',
                                  target = 'english'
                                  ).translate( xltDF.iloc( i ) )

#________________________________________________________ CRITICAL SECTION: end
PROC_ns_END = time.perf_counter_ns()

resDF.to_csv( r""jobdone.csv"",
              sep = ';'
              )

print( f""Runtime was {0:} [ns]"".format( PROC_ns_END - PROC_ns_START ) )

Tips for performance boosting :

if Google API-policy permits, we may increase thread-count, that participate on CRITICAL SECTION,
as the Python-interpreter threads are ""inside"" the same address-space and still are GIL-lock MUTEX-blocked, we may operate all just-[CONCURRENT] accesses to the same DataFrame-objects, best using non-overlapping, separate (thread-private) block-iterators over disjunct halves ( for a pair of threads ) over disjunct thirds ( for 3 threads ) etc...
as the Google API-policy is limiting attempts to overly concurrent access to the API-service, you shall build-in some, even naive-robustness

def thread_hosted_blockCRAWLer( i_start, i_end ):
    for i in range( i_start, i_end ):
        while True:
              try:
                  resDF.iloc( i ) = gXLTe( source = 'auto',
                                           target = 'english'
                                           ).translate( xltDF.iloc( i ) )
                  # SUCCEDED
                  break
              except:
                  # FAILED
                  print( ""EXC: _blockCRAWLer() on index "", i )
                  time.sleep( ... )
                  # be careful here, not to get on API-provider's BLACK-LIST
                  continue


if more time-related details per thread, may reuse this

Do not hesitate to go tuning & tweaking - and anyway, keep us posted how fast you managed to get, that's fair, isn't it?
",pandas
trying to remove all rows without a numeric value in a column using python pandas,"Consider:
       age
0       55
1       45
2       58
4      N/A

I need to remove all the rows that doesn't contain numeric values in column 'age' above, given the dataframe example.
The expected output is given below
       age
0       55
1       45
2       58

","Try this
import pandas as pd
import numpy as np
data = {
""age"": [0, 55, 1,55,4,'N/A',5]

}
df = pd.DataFrame(data)
df=df[df['age'].apply(lambda x: type(x) in [int, np.int64, 
float, np.float64])]

print(df) 

",pandas
why do i get notimplementederror trying to create a dataframe from a database table,"Using Pandas I can't read from an SQLite table:
import pandas as pd
import sqlite3 as sq

with sq.connect(""master.db"") as con:
    table = ""personal""
    df = pd.read_sql_table(table, con)

Database file and code are in the same directory. Python raises ""NotImplementedError"".
","For the entire table you can use pd.read_sql_query() and a SELECT * FROM table query:
import pandas as pd
import sqlite3 as sq

with sq.connect(""master.db"") as con:
   yourParams = [""personal""]
   df = pd.read_sql_query(""SELECT * FROM ?"", con, params = yourParams)

Note the parametrized query to prevent SQL-injection. You can also use pd.read_sql_table() (its parameters are the table name and database file, not the connection):
import pandas as pd
import sqlite3 as sq

df = pd.read_sql_table('personal','master.db')

",pandas
typeerror indexreindex got an unexpected keyword argument fill_value39,"I am trying to get the indices of the missing date by comparing it to a list of un-missed dates, as the following:
a = pd.DatetimeIndex([""2000"", ""2001"", ""2002"", ""2003"",
                      ""2004"", ""2005"", ""2009"", ""2010""])
b = pd.DatetimeIndex([""2000"", ""2001"", ""2002"", ""2003"",
                      ""2004"", ""2005"", ""2006"", ""2007"",
                      ""2008"", ""2009"", ""2010""])
a.reindex(b)

I got the following
(DatetimeIndex(['2000-01-01', '2001-01-01', '2002-01-01', '2003-01-01',
                '2004-01-01', '2005-01-01', '2006-01-01', '2007-01-01',
                '2008-01-01', '2009-01-01', '2010-01-01'],
               dtype='datetime64[ns]', freq=None),
 array([ 0,  1,  2,  3,  4,  5, -1, -1, -1,  6,  7]))

I tried to replace all missing value which is -1 to Nan, by using a.reindex(b, fill_value=np.NAN) but I got the following error TypeError: Index.reindex() got an unexpected keyword argument ‘fill_value’
According the pandas documentation fill_vaue is among the parameters of reindex.
Any ideas?
","First of all, you have to do:
newIndex, indexer = a.reindex(b)

reindex returns two things. You need/want to get only the indexer.
So now you can get what you want:
indexerWithNan = np.where(indexer == -1, np.nan, indexer)

Which is:
[ 0.  1.  2.  3.  4.  5. nan nan nan  6.  7.]

Why was your initial code wrong?  The reindex() method does not support the fill_value parameter for pandas.Index objects as it does for pandas.Series or pandas.DataFrame.
",pandas
how can i iterate over rows in a pandas dataframe,"I have a pandas dataframe, df:
   c1   c2
0  10  100
1  11  110
2  12  120

How do I iterate over the rows of this dataframe? For every row, I want to access its elements (values in cells) by the name of the columns. For example:
for row in df.rows:
    print(row['c1'], row['c2'])


I found a similar question, which suggests using either of these:


for date, row in df.T.iteritems():



for row in df.iterrows():



But I do not understand what the row object is and how I can work with it.
","DataFrame.iterrows is a generator which yields both the index and row (as a Series):
import pandas as pd

df = pd.DataFrame({'c1': [10, 11, 12], 'c2': [100, 110, 120]})
df = df.reset_index()  # make sure indexes pair with number of rows

for index, row in df.iterrows():
    print(row['c1'], row['c2'])

10 100
11 110
12 120


Obligatory disclaimer from the documentation

Iterating through pandas objects is generally slow. In many cases, iterating manually over the rows is not needed and can be avoided with one of the following approaches:

Look for a vectorized solution: many operations can be performed using built-in methods or NumPy functions, (boolean) indexing, …
When you have a function that cannot work on the full DataFrame/Series at once, it is better to use apply() instead of iterating over the values. See the docs on function application.
If you need to do iterative manipulations on the values but performance is important, consider writing the inner loop with cython or numba. See the enhancing performance section for some examples of this approach.


Other answers in this thread delve into greater depth on alternatives to iter* functions if you are interested to learn more.
",pandas
reading a table from sql server into a dataframe using sqlalchemy,"I have a table named ""products"" on SQL Server. I would like to read the table into a DataFrame in Python using SQLAlchemy.
The pandas.read_sql function has a ""sql"" parameter that accepts two types of SQLAlchemy ""selectable"" objects:

select object
text object

I succeeded in using the text object but failed in using the select object. What corrections are needed?
successful code using text object:
import pandas as pd

from sqlalchemy import text
from sqlalchemy.engine import URL
from sqlalchemy import create_engine


url_object = URL.create(
    ""mssql+pyodbc"",
    host=""abgsql.xx.xx.ac.uk"",
    database=""ABG"",
    query={
        ""driver"": ""ODBC Driver 18 for SQL Server"",
        ""TrustServerCertificate"": ""yes"",
    },
)
engine = create_engine(url_object)
stmt = text(""SELECT * FROM products"")
df = pd.read_sql(sql=stmt, con=engine)

not successful code using select object:
import pandas as pd

from sqlalchemy import select
from sqlalchemy.engine import URL
from sqlalchemy import create_engine

from sqlalchemy import Table, MetaData


url_object = URL.create(
    ""mssql+pyodbc"",
    host=""abgsql.xx.xx.ac.uk"",
    database=""ABG"",
    query={
        ""driver"": ""ODBC Driver 18 for SQL Server"",
        ""TrustServerCertificate"": ""yes"",
    },
)
engine = create_engine(url_object)
products = Table(""products"", MetaData())
stmt = select(products)
df = pd.read_sql(sql=stmt, con=engine)

sqlalchemy.exc.ProgrammingError: (pyodbc.ProgrammingError) ('42000', ""[42000] [Microsoft][ODBC Driver 18 for SQL Server][SQL Server]Incorrect syntax near the keyword 'FROM'. (156) (SQLExecDirectW)"")
[SQL: SELECT
FROM products]
(Background on this error at: https://sqlalche.me/e/20/f405)

","As @AlwaysLearning pointed out, the option to load some or all table information from an existing database is called reflection (https://docs.sqlalchemy.org/en/20/tutorial/metadata.html#table-reflection).
from sqlalchemy import select, Table, MetaData, create_engine, URL
import pandas as pd

metadata_obj = MetaData()
url_object = URL.create(
    ""mssql+pyodbc"",
    host=""abgsql.xx.xx.ac.uk"",
    database=""ABG"",
    query={
        ""driver"": ""ODBC Driver 18 for SQL Server"",
        ""TrustServerCertificate"": ""yes"",
    },
)
engine = create_engine(url_object)
products = Table(""products"", metadata_obj, autoload_with=engine)
stmt = select(products)
df = pd.read_sql(sql=stmt, con=engine)

",pandas
pandas dataframe add a mark columns by two other columns condition,"There is a dataframe like this:
import numpy as np
import pandas as pd

df = pd.DataFrame({'x':np.arange(1,29),'y':[5.69, 6.03, 6.03, 6.03, 6.03, 6.03, 6.03, 5.38, 5.21, 5.4 , 5.24,
       5.4 , 5.36, 5.47, 5.58, 5.5 , 5.61, 5.53, 5.4 , 5.51, 5.47, 5.44,5.39, 5.27, 5.38, 5.35, 5.32, 5.09],
          'valley':[1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1],
          'peak':[0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,0, 0, 0, 0, 0, 0]})

>>> df
     x     y  valley  peak
0    1  5.69       1     0
1    2  6.03       0     0
2    3  6.03       0     1
3    4  6.03       0     0
4    5  6.03       0     0
5    6  6.03       0     0
6    7  6.03       0     0
7    8  5.38       0     0
8    9  5.21       1     0
9   10  5.40       0     0
10  11  5.24       0     0
11  12  5.40       0     0
12  13  5.36       0     0
13  14  5.47       0     0
14  15  5.58       0     0
15  16  5.50       0     0
16  17  5.61       0     1
17  18  5.53       0     0
18  19  5.40       0     0
19  20  5.51       0     0
20  21  5.47       0     0
21  22  5.44       0     0
22  23  5.39       0     0
23  24  5.27       0     0
24  25  5.38       0     0
25  26  5.35       0     0
26  27  5.32       0     0
27  28  5.09       1     0

I hope to add a new column 'grp' to this dataframe, with the requirement that for each row that starts with ""1"" in the valley column and ends with ""1"" in the peak column, the value in the added column is ""A"", and conversely, for each row that starts with ""1"" in the peak column and ends with ""1"" in the valley column, the value in the added column is 'B'.
The desire result is:
>>> out
     x     y  valley  peak  grp
0    1  5.69       1     0  A
1    2  6.03       0     0  A
2    3  6.03       0     1  B
3    4  6.03       0     0  B
4    5  6.03       0     0  B
5    6  6.03       0     0  B
6    7  6.03       0     0  B
7    8  5.38       0     0  B
8    9  5.21       1     0  A
9   10  5.40       0     0  A
10  11  5.24       0     0  A
11  12  5.40       0     0  A
12  13  5.36       0     0  A
13  14  5.47       0     0  A
14  15  5.58       0     0  A
15  16  5.50       0     0  A
16  17  5.61       0     1  B
17  18  5.53       0     0  B
18  19  5.40       0     0  B
19  20  5.51       0     0  B
20  21  5.47       0     0  B
21  22  5.44       0     0  B
22  23  5.39       0     0  B
23  24  5.27       0     0  B
24  25  5.38       0     0  B
25  26  5.35       0     0  B
26  27  5.32       0     0  B
27  28  5.09       1     0  A

If we don't use apply with a function and for-loops, is there a native way to achieve by use pandas?
","Given your description, you could use np.select and ffill. This way you'll ensure that even if you have multiple peaks before a valley or conversely this will keep the order:
m1 = df['valley'].eq(1)
m2 = df['peak'].eq(1)
df['grp'] = pd.Series(np.select([m1, m2], ['A', 'B'], pd.NA),
                      index=df.index).ffill()

Variant with case_when:
m1 = df['valley'].eq(1)
m2 = df['peak'].eq(1)
df['grp'] = df['valley'].case_when([(m1, 'A'), (m2, 'B'), (~(m1|m2), pd.NA)]
                                   ).ffill()

Or with from_dummies after adding a new column:
df['grp'] = (pd.from_dummies(df[['valley', 'peak']]
                             .assign(other=df[['valley', 'peak']]
                                     .sum(axis=1).rsub(1)))
               .squeeze().map({'valley': 'A', 'peak': 'B'}).ffill()
            )

Or with reshaping:
df['grp'] = (df[['valley', 'peak']]
 .rename_axis(columns='col')
 .replace(0, pd.NA).stack().reset_index(-1, name='val')
 .replace({'col': {'valley': 'A', 'peak': 'B'}})['col']
 .reindex(df.index).ffill()
)

Alternatively, if there is always a valley then peak then valley... you could use cumsum+mod and map the group after identify which one of peak/valley is the first:
df['grp'] = (df[['valley', 'peak']].max(axis=1).cumsum()
            .add(df[['valley', 'peak']].idxmax().idxmin() == 'peak')
            .mod(2).map({0: 'B', 1: 'A'})
           )

Output:
     x     y  valley  peak grp
0    1  5.69       1     0   A
1    2  6.03       0     0   A
2    3  6.03       0     1   B
3    4  6.03       0     0   B
4    5  6.03       0     0   B
5    6  6.03       0     0   B
6    7  6.03       0     0   B
7    8  5.38       0     0   B
8    9  5.21       1     0   A
9   10  5.40       0     0   A
10  11  5.24       0     0   A
11  12  5.40       0     0   A
12  13  5.36       0     0   A
13  14  5.47       0     0   A
14  15  5.58       0     0   A
15  16  5.50       0     0   A
16  17  5.61       0     1   B
17  18  5.53       0     0   B
18  19  5.40       0     0   B
19  20  5.51       0     0   B
20  21  5.47       0     0   B
21  22  5.44       0     0   B
22  23  5.39       0     0   B
23  24  5.27       0     0   B
24  25  5.38       0     0   B
25  26  5.35       0     0   B
26  27  5.32       0     0   B
27  28  5.09       1     0   A

",pandas
why am i getting quotraise sourceerrorquotmultiple repeatquot reerror multiple repeat at position 2quot when trying to save data frames to csv files,"The code is attached below. It works fine until it gets to ai: df_ai in the database dict.
data = pd.read_csv('survey_results_public.csv')

df_demographics = data[['ResponseId', 'MainBranch', 'Age', 'Employment', 'EdLevel', 'YearsCode', 'Country']]

df_learn_code = data[['ResponseId', 'LearnCode']]

df_language = data[['ResponseId', 'LanguageAdmired']]

df_ai = data[['ResponseId', 'AISelect', 'AISent', 'AIAcc', 'AIComplex', 'AIThreat', 'AIBen', 'AIToolCurrently Using']]

database = {'demographics': df_demographics, 'learn_code': df_learn_code, 'language': df_language, 'ai': df_ai}

def find_semicolons(dataframe):
    result = []

    firstFifty = dataframe.head(50)

    for column in firstFifty.columns:
        if firstFifty[column].apply(lambda x: ';' in str(x)).any():
            result.append(column)

    return result


def transform_dataframe(dataframe):
    result = find_semicolons(dataframe)

    for column in result:
        values = [str(x).split(';') for x in dataframe[column].unique().tolist()]
        flat_values = []
        for x in values:
            flat_values.extend(x)
        flat_values = set(flat_values)
        for x in flat_values:
            dataframe[x] = dataframe[column].str.contains(x, na=False).astype(int)



for x in database:
    transform_dataframe(database.get(x))
    database.get(x).to_csv(x + '.csv')


Here's the traceback
Traceback (most recent call last):
  File ""/Users/shalim/PycharmProjects/work/stackoverflow.py"", line 45, in <module>
    transform_dataframe(database.get(x))
  File ""/Users/shalim/PycharmProjects/work/stackoverflow.py"", line 40, in transform_dataframe
    dataframe[x] = dataframe[column].str.contains(x, na=False).astype(int)
  File ""/Users/shalim/PycharmProjects/work/venv/lib/python3.9/site-packages/pandas/core/strings/accessor.py"", line 137, in wrapper
    return func(self, *args, **kwargs)
  File ""/Users/shalim/PycharmProjects/work/venv/lib/python3.9/site-packages/pandas/core/strings/accessor.py"", line 1327, in contains
    if regex and re.compile(pat).groups:
  File ""/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/re.py"", line 252, in compile
    return _compile(pattern, flags)
  File ""/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/re.py"", line 304, in _compile
    p = sre_compile.compile(pattern, flags)
  File ""/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/sre_compile.py"", line 764, in compile
    p = sre_parse.parse(p, flags)
  File ""/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/sre_parse.py"", line 948, in parse
    p = _parse_sub(source, state, flags & SRE_FLAG_VERBOSE, 0)
  File ""/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/sre_parse.py"", line 443, in _parse_sub
    itemsappend(_parse(source, state, verbose, nested + 1,
  File ""/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/sre_parse.py"", line 671, in _parse
    raise source.error(""multiple repeat"",
re.error: multiple repeat at position 2

","Pandas .str.contains performs a regex search rather than a substring search, by default. That means that characters like * or + get treated as regex metacharacters instead of a literal asterisk or plus sign.
It looks like you're trying to perform a substring search, not a regex search. Your x isn't a valid regex, and even if it was, it wouldn't mean what you want. You need to specify regex=False:
dataframe[x] = dataframe[column].str.contains(x, na=False, regex=False).astype(int)

",pandas
best way to turn every cell in a dataframe into its own row in a new dataframe,"Suppose I have a dataframe Old with columns A, B, and C. I want a new dataframe New where two columns D and E. For each cell in Old, I want a corresponding row in the D column in New where the value in E is the name of the column the cell was in.
I know that straight up iterating over a dataframe is bad, but that's how I did it. Here, I only cared about some column names in the Old dataframe, so if the cell wasn't under a column I cared about, I just assigned it the value other. But the principle is the same.
for column in df.columns:
    for entry in df[column]:
        entries.append(entry)
        labels.append(column_labels.get(column, ""other""))  # Assign label based on column

My question is what are some better ways to do this? Running this will become exceedingly slow as the dataset grows.
","You must be looking for stack():
    df = pd.DataFrame(np.arange(12).reshape((4,3)), columns=list(""ABC""))
    
       A   B   C
    0  0   1   2
    1  3   4   5
    2  6   7   8
    3  9  10  11
    
    res = (df.stack()
             .reset_index(level=1)
             .sort_values(by=""level_1"")
             .reset_index(drop=True)
             .rename(columns={""level_1"":""labels"", 0:""entries""})
    )
    
       labels  entries
    0       A        0
    1       A        3
    2       A        6
    3       A        9
    4       B        1
    5       B        4
    6       B        7
    7       B       10
    8       C        2
    9       C        5
    10      C        8
    11      C       11

",pandas
fastest way to shingle from pandas column,"I need the fastest possible way to shingle strings from a data frame and then create a master list.
Given the following data frame:
import pandas as pd
d=['Hello', 'Helloworld']
f=pd.DataFrame({'strings':d})
f
    strings
0   Hello
1   Helloworld

I'd like to generate a list shingled strings (of length 3) like this:
(All possible 3-letter combinations are included.)
[['Hel', 'ell', 'llo'],['Hel', 'ell', 'llo', 'low', 'owo', 'wor', 'orl', 'rld']]

... and a master list of all unique values like this:
['wor', 'Hel', 'ell', 'owo', 'llo', 'rld', 'orl', 'low']

I can do it as follows, but I suspect there is a much faster way:
#Shingle into strings of exactly 3
def shingle(word):
    r = [word[i:i + 3] for i in range(len(word) - 3 + 1)]
    return [''.join(t) for t in r]
#Shingle (i.e. ""hello"" -> ""hel"",""ell"",'llo')
r=[shingle(w) for w in f['strings']]
#Get all elements into one list:
import itertools
colsunq=list(itertools.chain.from_iterable(r))
#Remove duplicates:
colsunq=list(set(colsunq))
colsunq

['wor', 'Hel', 'ell', 'owo', 'llo', 'rld', 'orl', 'low']

Thanks in advance!
","I'm 4 years late, but here's an answer. I don't think that it's possible to determine the ""fastest"" way since that's heavily dependent on hardware and algorithms. (It might fall under something similar to Kolmogorov complexity.)
But, I needed to shingle over 11 million files. I put each of the words in a numpy array and ran the following code.
shingles = set()

for i in range(words.shape[0] - w + 1):
    a = words[i:i + w]
    shingles.add(tuple(a))

This code processed 27.2 billion words in about 6 hours.
",pandas
panderaerrorsbackendnotfounderror with pandas dataframe,"pandera: 0.18.3
pandas: 2.2.2
python: 3.9/3.11

Hi,
I am unable to setup the pandera for pandas dataframe as it complains:
File ""/anaconda/envs/data_quality_env/lib/python3.9/site-packages/pandera/api/base/schema.py"",
line 96, in get_backend
        raise BackendNotFoundError(
    pandera.errors.BackendNotFoundError: Backend not found for backend, class: (<class 'data_validation.schemas.case.CaseSchema'>,
<class 'pandas.core.frame.DataFrame'>). Looked up the following base
classes: (<class 'pandas.core.frame.DataFrame'>, <class 'pandas.core.generic.NDFrame'>, <class 'pandas.core.base.PandasObject'>, <class 'pandas.core.accessor.DirNamesMixin'>, <class 'pandas.core.indexing.IndexingMixin'>, <class 'pandas.core.arraylike.OpsMixin'>, <class 'object'>)

My folder structure is:
project/
    data_validation/
        schema/
            case.py
        validation/
            validations.py
    pipeline.py

case.py:
import pandas as pd
import pandera as pa

class CaseSchema(pa.DataFrameSchema):
    case_id = pa.Column(pa.Int)

validations.py
import pandas as pd
from data_validation.schemas.case import CaseSchema

def validate_case_data(df: pd.DataFrame) -> pd.DataFrame:
    """"""Validate a DataFrame against the PersonSchema.""""""
    schema = CaseSchema()
    return schema.validate(df)

pipeline.py
import pandas as pd
from data_validation.validation.validations import validate_case_data

def validate_df(df: pd.DataFrame) -> pd.DataFrame:
    """"""Process data, validating it against the PersonSchema.""""""
    validated_df = validate_case_data(df)
    return validated_df

df = pd.DataFrame({
    ""case_id"": [1, 2, 3]
})

processed_df = validate_df(df)

","This can be solved by including a get_backend method in CaseSchema:
import pandas as pd
import pandera as pa
from pandera.backends.pandas.container import DataFrameSchemaBackend

class CaseSchema(pa.DataFrameSchema):
    case_id = pa.Column(pa.Int)

    @classmethod
    def get_backend(cls, check_obj=None, check_type=None):
        if check_obj is not None:
            check_obj_cls = type(check_obj)
        elif check_type is not None:
            check_obj_cls = check_type
        else:
            raise ValueError(""Must pass in one of `check_obj` or `check_type`."")

        cls.register_default_backends(check_obj_cls)
        return DataFrameSchemaBackend()

",pandas
pandas subtotal similar to excel,"I have the following dataframe df:
     A   B      C
0    21  Blue   100 
1    33  Yellow 100 
2    17  White  250 
3    A2  Grey   40
4    65  Green  500 
5    33  Red    80 
6    17  Purple -50
7    A2  Orange 600

Column B is basically irrelevant information IRT the code itself but still needs to be included in the output.
I have sorted the dataframe by column A and gotten around the issue that col A has contains both int and str:
df['A'] = df['A'].astype(str)
df_sorted = df.sort_values(by=['A'])

So now df_sorted looks like this:
     A   B      C
2    17  White  250
6    17  Purple -50
0    21  Blue   100
1    33  Yellow 100
5    33  Red    80
4    65  Green  500
3    A2  Grey   40
7    A2  Orange 600

My question is: How can I then make subtotals for each change in col A by summarizing col C similar to Excel's subtotal function?
The final output of the dataframe should look like this:
     A        B      C
2    17       White  250
6    17       Purple -50
     Subtotal        200  
0    21       Blue   100
     Subtotal        100
1    33       Yellow 100
5    33       Red    80
     Subtotal        180
4    65       Green  500
     Subtotal        500
3    A2       Grey   40
7    A2       Orange 600
     Subtotal        640

","you can concat your original df and the groupby subtotal.
df1 =  pd.concat([df,
             df.groupby(['A'],as_index=False)['C'].sum()]).sort_values('A')

df1.loc[df1['B'].isnull(), 'A'] = 'Subtotal'

print(df1.fillna(''))

          A       B    C
2        17   White  250
6        17  Purple  -50
0  Subtotal          200
0        21    Blue  100
1  Subtotal          100
1        33  Yellow  100
5        33     Red   80
2  Subtotal          180
4        65   Green  500
3  Subtotal          500
3        A2    Grey   40
7        A2  Orange  600
4  Subtotal          640

",pandas
prettyprint an entire pandas series  dataframe,"I work with Series and DataFrames on the terminal a lot. The default __repr__ for a Series returns a reduced sample, with some head and tail values, but the rest missing.
Is there a builtin way to pretty-print the entire Series / DataFrame?  Ideally, it would support proper alignment, perhaps borders between columns, and maybe even color-coding for the different columns.
","You can also use the option_context, with one or more options:
with pd.option_context('display.max_rows', None, 'display.max_columns', None):  # more options can be specified also
    print(df)

This will automatically return the options to their previous values.
If you are working on jupyter-notebook, using display(df) instead of print(df) will use jupyter rich display logic (like so).
",pandas
pandas dataframe table vertical scrollbars,"I have a large (vertically) pandas dataframe that I would like to display as a nice table with scrollbars.  I can get it to display the table with all the rows but I cannot get the scrollbars to display. 
def data(x):
    strData = strData[['Data1','Data2','Data3']]
    display(strData)

output: No vertical scrollbars

","Not sure if this is what you mean, but I guess you need to set the max_rows option to None, so that pandas doesn't put a limit on the number of rows displayed:
pd.set_option(""display.max_rows"", None)



Update:
In [27]: 
##javascript
IPython.OutputArea.auto_scroll_threshold = 10;

In[28]:
def display_():    
    pd.set_option(""display.max_rows"", None)
    from IPython.core.display import display 
    display(df) #df must be defined up there


",pandas
python pandas distance matrix using jaccard similarity,"I have implemented a function to construct a distance matrix using the jaccard similarity:
import pandas as pd
entries = [
    {'id':'1', 'category1':'100', 'category2': '0', 'category3':'100'},
    {'id':'2', 'category1':'100', 'category2': '0', 'category3':'100'},
    {'id':'3', 'category1':'0', 'category2': '100', 'category3':'100'},
    {'id':'4', 'category1':'100', 'category2': '100', 'category3':'100'},
    {'id':'5', 'category1':'100', 'category2': '0', 'category3':'100'}
           ]
df = pd.DataFrame(entries)

and the distance matrix with scipy
from scipy.spatial.distance import squareform
from scipy.spatial.distance import pdist, jaccard

res = pdist(df[['category1','category2','category3']], 'jaccard')
squareform(res)
distance = pd.DataFrame(squareform(res), index=df.index, columns= df.index)

The problem is that my result looks like this which seems to be false:

What am i missing? The similarity of 0 and 1 have to be maximum for example and the other values seem wrong too
","Looking at the docs, the implementation of jaccard in scipy.spatial.distance is jaccard dissimilarity, not similarity. This is the usual way in which distance is computed when using jaccard as a metric.  The reason for this is because in order to be a metric, the distance between the identical points must be zero.
In your code, the dissimilarity between 0 and 1 should be minimized, which it is.  The other values look correct in the context of dissimilarity as well.
If you want similarity instead of dissimilarity, just subtract the dissimilarity from 1.
res = 1 - pdist(df[['category1','category2','category3']], 'jaccard')

",pandas
how to subtract pddataframegroupbyobjects from each other,"I have the following pd.DataFrame
match_id    player_id   round   points  A   B   C   D   E
5890    3750    1   10  0   0   0   3   1
5890    3750    2   10  0   0   0   1   0
5890    3750    3   10  0   8   0   0   1
5890    2366    1   9   0   0   0   5   0
5890    2366    2   9   0   0   0   5   0
5890    2366    3   9   0   0   0   2   0

I want to subtract the values of A, B, C, D and E of the two players and create two new columns that represent the number of points of the two players.
My desired output looks as follows:
match_id    round   points_home points_away A   B   C   D   E
5890    1   10  9   0   0   0   -2  1
5890    2   10  9   0   0   0   -4  0
5890    3   10  9   0   8   0   -2  1

Please advice
","Use GroupBy.first with GroupBy.last first, subtract necessary columns with add points columns in concat:
g = df.groupby(['match_id','round'])
df1 = g.first()
df2 = g.last()

cols = ['A','B','C','D','E']
out = pd.concat([df1['points'].rename('points_home'),
                df2['points'].rename('points_away'), 
                df1[cols].sub(df2[cols])], axis=1).reset_index()
print (out)
   match_id  round  points_home  points_away  A  B  C  D  E
0      5890      1           10            9  0  0  0 -2  1
1      5890      2           10            9  0  0  0 -4  0
2      5890      3           10            9  0  8  0 -2  1

Alternative with MultiIndex with GroupBy.agg:
df3 = (df.groupby(['match_id','round'])
         .agg(['first','last'])
         .rename(columns={'first':'home', 'last':'away'}))

cols = ['A','B','C','D','E']

out = pd.concat([df3['points'].add_prefix('points_'), 
                 df3.xs('home', axis=1, level=1)[cols]
                    .sub(df3.xs('away', axis=1, level=1)[cols])], axis=1).reset_index()
print (out)
   match_id  round  points_home  points_away  A  B  C  D  E
0      5890      1           10            9  0  0  0 -2  1
1      5890      2           10            9  0  0  0 -4  0
2      5890      3           10            9  0  8  0 -2  1

",pandas
how to label or rename bin ranges in a series output from value count,"In a series or df column, I want to count the number of values that fit within predefined bins (easy) and meaningfully label the bin values (problem).
import pandas as pd

data = [{'A': 1, 'B': ""Jim""}, {'A': 5, 'B': ""Jim""}, {'A': 2, 'B': ""Bob""}, {'A': 3, 'B': ""Bob""}]
df = pd.DataFrame(data)

mBins = [-1, 2, 4, 6]
mLabels = [""0-2"", ""3-4"", ""5-6""]

simple_VC = df[""A""].value_counts(bins=mBins)

Out[25]:   # ugly bin values
(-1.001, 2.0]    2
(2.0, 4.0]       1
(4.0, 6.0]       1

# Wanted more meaningful bin values:
0-2    2
3-4    1
5-6    1

I've tried using pd.cut, which allows me to label the bins, but I'm not sure how to use this in a value count. I've also tried to rename, but I don't know how to specify values like (4.0, 6.0], which are neither text or non-text.
How do I label the binned value counts - if possible during the value count, and how to rename bin ranges?
","The bins parameter is actually just a convenience that calls cut internally.
You can therefor cut the values manually and use the labels parameter before value_counts:
out = pd.cut(df['A'], mBins, labels=mLabels).value_counts()

If you don't want to provide the labels, you could also infer them from the intervals and rename:
from math import ceil

out = (df['A'].value_counts(bins=mBins)
       .rename(lambda x: f'{ceil(x.left+1)}-{ceil(x.right)}')
      )

NB. this is an example for integer values/bounds.
Output:
A
0-2    2
3-4    1
5-6    1
Name: count, dtype: int64

",pandas
infercnvpy cnv_score function attributeerror 39series39 object has no attribute 39nonzero39,"I was analysing some scRNA-seq data from GSE214966 when I encountered a problem in executing cnv_score() function from infercnvpy package. It consistenly raises a AttributeError: 'Series' object has no attribute 'nonzero'.
I also tried reproducing the example from their website tutorial (https://infercnvpy.readthedocs.io/en/latest/notebooks/tutorial_3k.html) but it raises the same error. That probably mean that the problem it's on my end but I can't figure out what's wrong.
I tried reinstalling all packages and python as well but nothing worked out. The error suggests that the problem is that Pandas.Series doesn't have a attribute such as nonzero, which it doesn't. But then I don't know what's wrong or how to solve this problem since it is a function from a package.
Can anybody help me with this?
Error description
AttributeError                            Traceback (most recent call last)
~\AppData\Local\Temp\ipykernel_9248\2255080222.py in ?()
      1 import infercnvpy as cnv
----> 2 cnv.tl.cnv_score(adata, groupby=""leiden"")

d:\Datos de usuario\Desktop\Single Cell Analysis\Glioblastoma (GSE214966)\Glioblastoma-GSE214966\.venv\Lib\site-packages\infercnvpy\tl\_scores.py in ?(adata, groupby, use_rep, key_added, inplace, obs_key)
     61         groupby = obs_key
     62 
     63     if groupby not in adata.obs.columns and groupby == ""cnv_leiden"":
     64         raise ValueError(""`cnv_leiden` not found in `adata.obs`. Did you run `tl.leiden`?"")
---> 65     cluster_score = {
     66         cluster: np.mean(np.abs(adata.obsm[f""X_{use_rep}""][adata.obs[groupby] == cluster, :]))
     67         for cluster in adata.obs[groupby].unique()
     68     }

d:\Datos de usuario\Desktop\Single Cell Analysis\Glioblastoma (GSE214966)\Glioblastoma-GSE214966\.venv\Lib\site-packages\scipy\sparse\_index.py in ?(self, key)
     29     def __getitem__(self, key):
---> 30         index, new_shape = self._validate_indices(key)
     31 
     32         # 1D array
     33         if len(index) == 1:

d:\Datos de usuario\Desktop\Single Cell Analysis\Glioblastoma (GSE214966)\Glioblastoma-GSE214966\.venv\Lib\site-packages\scipy\sparse\_index.py in ?(self, key)
    265                 if ix.shape != mid_shape:
    266                     raise IndexError(
    267                         f""bool index {i} has shape {mid_shape} instead of {ix.shape}""
    268                     )
--> 269                 index.extend(ix.nonzero())
    270                 array_indices.extend(range(index_ndim, tmp_ndim))
    271                 index_ndim = tmp_ndim
    272             else:  # dense array

d:\Datos de usuario\Desktop\Single Cell Analysis\Glioblastoma (GSE214966)\Glioblastoma-GSE214966\.venv\Lib\site-packages\pandas\core\generic.py in ?(self, name)
   6295             and name not in self._accessors
   6296             and self._info_axis._can_hold_identifiers_and_holds_name(name)
   6297         ):
   6298             return self[name]
-> 6299         return object.__getattribute__(self, name)

AttributeError: 'Series' object has no attribute 'nonzero'

Session_info
-----
anndata             0.11.3
infercnvpy          0.5.0
matplotlib          3.10.0
numpy               1.26.4
openpyxl            3.1.5
pandas              2.2.3
scanpy              1.10.4
scipy               1.15.1
session_info        1.0.0
sklearn             1.6.1
-----
PIL                 11.1.0
asttokens           NA
attr                24.3.0
attrs               24.3.0
cairo               1.27.0
cattr               NA
cattrs              NA
certifi             2024.12.14
charset_normalizer  3.4.1
colorama            0.4.6
comm                0.2.2
cycler              0.12.1
...
Python 3.12.6 (tags/v3.12.6:a4a2d2b, Sep  6 2024, 20:11:23) [MSC v.1940 64 bit (AMD64)]
Windows-10-10.0.19045-SP0
-----
Session information updated at 2025-01-16 16:27

","A similar issue was raised on https://github.com/chanzuckerberg/single-cell-curation/issues/1165. Seems to be a bug with scipy version >= 1.15.0. The user stated that rolling back to scipy version 1.14.1 fixes the issue. It's still an open bug issue on the repository.
",pandas
antijoin pandas,"I have two tables and I would like to append them so that only all the data in table A is retained and data from table B is only added if its key is unique (Key values are unique in table A and B however in some cases a Key will occur in both table A and B). 
I think the way to do this will involve some sort of filtering join (anti-join) to get values in table B that do not occur in table A then append the two tables. 
I am familiar with R and this is the code I would use to do this in R.
library(""dplyr"")

## Filtering join to remove values already in ""TableA"" from ""TableB""
FilteredTableB <- anti_join(TableB,TableA, by = ""Key"")

## Append ""FilteredTableB"" to ""TableA""
CombinedTable <- bind_rows(TableA,FilteredTableB)

How would I achieve this in python?
","Consider the following dataframes
TableA = pd.DataFrame(np.random.rand(4, 3),
                      pd.Index(list('abcd'), name='Key'),
                      ['A', 'B', 'C']).reset_index()
TableB = pd.DataFrame(np.random.rand(4, 3),
                      pd.Index(list('aecf'), name='Key'),
                      ['A', 'B', 'C']).reset_index()


TableA



TableB


This is one way to do what you want
Method 1
# Identify what values are in TableB and not in TableA
key_diff = set(TableB.Key).difference(TableA.Key)
where_diff = TableB.Key.isin(key_diff)

# Slice TableB accordingly and append to TableA
TableA.append(TableB[where_diff], ignore_index=True)


Method 2
rows = []
for i, row in TableB.iterrows():
    if row.Key not in TableA.Key.values:
        rows.append(row)

pd.concat([TableA.T] + rows, axis=1).T


Timing
4 rows with 2 overlap
Method 1 is much quicker 

10,000 rows 5,000 overlap
loops are bad

",pandas
convert multiindex column to single column in dataframe,"import pandas as pd

columns = pd.MultiIndex.from_tuples(
    [('A', 'one'), ('A', 'two'), ('B', 'one'), ('B', 'two'), ('C', '')],
    names=[None, 'number'])

df = pd.DataFrame([[1, 2, 3, 4, 'X'], [5, 6, 7, 8, 'Y']], columns=columns)

         A       B      C
number one two one two   
0        1   2   3   4  X
1        5   6   7   8  Y

I'd like to remove the multi-index by making number a column:
A   B   C   number
1   3   X   one
5   7   Y   one
2   4   X   two
6   8   Y   two

I tried extracting the values with df[[('number', ('A','one')]] so that I can assign them to individual columns, but it doesn't work.
","Set C as the index then .stack('number').
Then to make it look like you want, reset the index and sort by number.
(
    df.set_index('C')
    .stack('number')
    .reset_index()
    .sort_values('number')
)

   C number  A  B
0  X    one  1  3
2  Y    one  5  7
1  X    two  2  4
3  Y    two  6  8

Note: To sort by number more correctly:
    .sort_values('number', key=lambda s: s.map({'one': 1, 'two': 2}))

",pandas
get last row that satisfies a condition using pandas groupby,"I have a DataFrame like this:




date_from
date_to
item_id
VALUE_NEW
VALUE_OLD
cost_var




1/1/1900  00:00:00
11/3/2022  15:31:18
452953
5366,46
4024,71
33.34%


11/3/2022  15:31:18
1/1/2200  00:00:00
452953
9122,57
5366,46
69.99%


1/1/1900  00:00:00
11/3/2022  15:31:18
452954
5366,46
4024,71
33.34%


11/3/2022  15:31:18
1/1/2200  00:00:00
452954
9122,57
5366,46
69.99%


1/1/1900  00:00:00
21/7/2021  16:30:46
452961
6170,98
4024,71
53.33%


21/7/2021  16:30:46
11/3/2022  15:31:09
452961
5312
6170,98
13.92%


11/3/2022  15:31:09
1/1/2200  00:00:00
452961
9122,57
5312
71.74%


1/1/1900  00:00:00
13/10/2021  14:39:55
801286
4052,1
1332,8
204.03%


13/10/2021  14:39:55
13/10/2021  14:43:09
801286
4,4732
4052,1
99.89%


13/10/2021  14:43:09
3/2/2022  17:16:23
801286
4473,2
4,4732
99900.00%


3/2/2022  17:16:23
1/1/2200  00:00:00
801286
4946,8
4473,2
10.59%




I need to check each item_id, and get the last row where cost_var is >60%. If it's the last row, that's ok, but if there is a next one, and it is <60%, I have to drop the last row>60%. Output should look like this:




date_from
date_to
item_id
VALUE_NEW
VALUE_OLD
cost_var




11/3/2022  15:31:18
1/1/2200  00:00:00
452953
9122,57
5366,46
69.99%


11/3/2022  15:31:18
1/1/2200  00:00:00
452954
9122,57
5366,46
69.99%


11/3/2022  15:31:09
1/1/2200  00:00:00
452961
9122,57
5312
71.74%




Item 802186 returned no value, because last row>60% (99900.00%) has a next row and cost_var<60% (10.59%)... Is it possible to do? I couldn't find a way to solve it.
","We can select the last row of each item_id using groupby and only select ones where cost_var > 60% using query.
df.groupby('item_id', as_index=False).last().query(""cost_var.str.rstrip('%').astype('float')>60"")


",pandas
pandas equivalent of group by having in sql,"What is the most efficient way to use groupby and in parallel apply a filter in pandas?
Basically I am asking for the equivalent in SQL of
select *
...
group by col_name
having condition

I think there are many uses cases ranging from conditional means, sums, conditional probabilities, etc. which would make such a command very powerful.
I need a very good performance, so ideally such a command would not be the result of several layered operations done in python.
","As mentioned in unutbu's comment, groupby's filter is the equivalent of SQL'S HAVING:
In [11]: df = pd.DataFrame([[1, 2], [1, 3], [5, 6]], columns=['A', 'B'])

In [12]: df
Out[12]:
   A  B
0  1  2
1  1  3
2  5  6

In [13]: g = df.groupby('A')  #  GROUP BY A

In [14]: g.filter(lambda x: len(x) > 1)  #  HAVING COUNT(*) > 1
Out[14]:
   A  B
0  1  2
1  1  3

You can write more complicated functions (these are applied to each group), provided they return a plain ol' bool:
In [15]: g.filter(lambda x: x['B'].sum() == 5)
Out[15]:
   A  B
0  1  2
1  1  3

Note: potentially there is a bug where you can't write you function to act on the columns you've used to groupby... a workaround is the groupby the columns manually i.e. g = df.groupby(df['A'])).
",pandas
filtering multiple items in a multiindex pandas dataframe,"I have the following table:
                               Area
NSRCODE  PBL_AWI          
CM       BONS             44705.492941
         BTNN            253854.591990
         FONG             41625.590370
         FONS             16814.159680
         Lake             57124.819333
         River             1603.906642
         SONS            583958.444751
         STNN             45603.837177
         clearcut        106139.013930
         disturbed       127719.865675
         lowland         118795.578059
         upland         2701289.270193
LBH      BFNN            289207.169650
         BONS           9140084.716743
         BTNI             33713.160390
         BTNN          19748004.789040
         FONG           1687122.469691
         FONS           5169959.591270
         FTNI            317251.976160
         FTNN           6536472.869395
         Lake            258046.508310
         River            44262.807900
         SONS           4379097.677405
         burn regen      744773.210860
         clearcut         54066.756790
         disturbed       597561.471686
         lowland       12591619.141842
         upland        23843453.638117

Note: Both NSRCODE and PBL_AWI are indices.
How do I search for values in column PBL_AWI? For example I want to keep the values ['Lake', 'River', 'Upland'].
","You can get_level_values in conjunction with Boolean slicing.
In [50]:

print df[np.in1d(df.index.get_level_values(1), ['Lake', 'River', 'Upland'])]
                          Area
NSRCODE PBL_AWI               
CM      Lake      57124.819333
        River      1603.906642
LBH     Lake     258046.508310
        River     44262.807900

The same idea can be expressed in many different ways, such as df[df.index.get_level_values('PBL_AWI').isin(['Lake', 'River', 'Upland'])]
Note that you have 'upland' in your data instead of 'Upland'
",pandas
pandas change values of a dataframe based on an override,"I have a pandas dataframe which looks something like this.
orig |  dest |  type | class |  BKT   | BKT_order | value   | fc_Cap | sc_Cap
-----+-------+-------+-------+--------+-----------+---------+--------+---------
 AMD |  TRY  |   SA  | fc    |  MA    |   1       |   12.04 |   20   |   50
 AMD |  TRY  |   SA  | fc    |  TY    |   2       |   11.5  |   20   |   50
 AMD |  TRY  |   SA  | fc    |  NY    |   3       |   17.7  |   20   |   50
 AMD |  TRY  |   SA  | fc    |  MU    |   4       |   09.7  |   20   |   50
 AMD |  TRY  |   PE  | fc    |  RE    |   1       |   09.7  |   20   |   50
 AMD |  TRY  |   PE  | sc    |  EW    |   5       |   07.7  |   20   |   50
 NCL |  MNK  |   PE  | sc    |  PO    |   2       |   08.7  |   20   |   50
 NCL |  MNK  |   PE  | sc    |  TU    |   3       |   12.5  |   20   |   50
 NCL |  MNK  |   PE  | sc    |  MA    |   1       |   16.7  |   20   |   50

Also i have an override Dataframe which may look something like this:
orig |  dest |  type |  max_BKT 
-----+-------+-------+-----------
 AMD |  TRY  |   SA  |  TY
 NCL |  MNK  |   PE  |  PO
 NCL |  AGZ  |   PE  |  PO

what i want to do is modify the original dataframe such that after comparison of  orig dest type & BKT ( with max_BKT)  values, the value column for any rows which have the BKT_order higher than or equal to the max_BKT in override DF is set to either fc_Cap or sc_Cap depending on the class value.
For Example in above scenario,
Since the Override DF sets max_BKT as TY for AMD |  TRY  |   SA and the bucket order for TY is 2 in original Df, i need to set the value column equal to fc_Cap or sc_Cap
depending on the value of class for all rows where BKT_order >= 2
So basically:

filter the rows for orig dest type combination
Get the BKT_order of max_BKT from the Original DF
for each row that matches the above criteria

if class == fc update value column with fc_Cap
if class == sc update value column with sc_Cap



So our original DF looks something like this:
orig |  dest |  type | class |  BKT   | BKT_order | value   | fc_Cap | sc_Cap
-----+-------+-------+-------+--------+-----------+---------+--------+---------
 AMD |  TRY  |   SA  | fc    |  MA    |   1       |   12.04 |   20   |   50
 AMD |  TRY  |   SA  | fc    |  TY    |   2       |   20    |   20   |   50
 AMD |  TRY  |   SA  | fc    |  NY    |   3       |   20    |   20   |   50
 AMD |  TRY  |   SA  | fc    |  MU    |   4       |   20    |   20   |   50
 AMD |  TRY  |   PE  | fc    |  RE    |   1       |   09.7  |   20   |   50
 AMD |  TRY  |   PE  | sc    |  EW    |   5       |   07.7  |   20   |   50
 NCL |  MNK  |   PE  | sc    |  PO    |   2       |   50    |   20   |   50
 NCL |  MNK  |   PE  | sc    |  TU    |   3       |   50    |   20   |   50
 NCL |  MNK  |   PE  | sc    |  MA    |   1       |   16.7  |   20   |   50

I have tried an approach to iterate over the override df and try to handle 1 row at a time but, i get stuck when i need to do a reverse lookup to get the BKT_order  of the max_BKT from original df.
Hope that makes sense... i am fairly new to pandas.
","That's a fairly complex task. The individual steps are straightforward though.
You need:

indexing lookup to find the cap values based on class
merge to match the max_BKT
mask+groupby.transform to identify the rows to mask

idx, cols = pd.factorize(df['class']+'_Cap')

group = ['orig', 'dest', 'type']

out = (
    df.merge(override, on=group, how='left')
    .assign(
        value=lambda x: x['value'].mask(
            x['BKT_order'].ge(
                x['BKT_order']
                .where(x['BKT'].eq(x['max_BKT']))
                .groupby([x[c] for c in group])
                .transform('first')
            ),
            x.reindex(cols, axis=1).to_numpy()[np.arange(len(x)), idx],
        )
    )
    .reindex(columns=df.columns)
)

Output:
  orig dest type class BKT  BKT_order  value  fc_Cap  sc_Cap
0  AMD  TRY   SA    fc  MA          1  12.04      20      50
1  AMD  TRY   SA    fc  TY          2  20.00      20      50
2  AMD  TRY   SA    fc  NY          3  20.00      20      50
3  AMD  TRY   SA    fc  MU          4  20.00      20      50
4  AMD  TRY   PE    fc  RE          1   9.70      20      50
5  AMD  TRY   PE    sc  EW          5   7.70      20      50
6  NCL  MNK   PE    sc  PO          2  50.00      20      50
7  NCL  MNK   PE    sc  TU          3  50.00      20      50
8  NCL  MNK   PE    sc  MA          1  16.70      20      50

Intermediates:
tmp = df.merge(override, on=group, how='left')
tmp['cap'] = tmp.reindex(cols, axis=1).to_numpy()[np.arange(len(tmp)), idx]
tmp['mask'] = tmp['BKT'].eq(tmp['max_BKT'])
tmp['masked_BKT'] = tmp['BKT_order'].where(tmp['mask'])
tmp['ref_BKT'] = tmp.groupby(group)['masked_BKT'].transform('first')
tmp['>= ref_BKT'] = tmp['BKT_order'].ge(tmp['ref_BKT'])

  orig dest type class BKT  BKT_order  value  fc_Cap  sc_Cap max_BKT  cap   mask  masked_BKT  ref_BKT  >= ref_BKT
0  AMD  TRY   SA    fc  MA          1  12.04      20      50      TY   20  False         NaN      2.0       False
1  AMD  TRY   SA    fc  TY          2  11.50      20      50      TY   20   True         2.0      2.0        True
2  AMD  TRY   SA    fc  NY          3  17.70      20      50      TY   20  False         NaN      2.0        True
3  AMD  TRY   SA    fc  MU          4   9.70      20      50      TY   20  False         NaN      2.0        True
4  AMD  TRY   PE    fc  RE          1   9.70      20      50     NaN   20  False         NaN      NaN       False
5  AMD  TRY   PE    sc  EW          5   7.70      20      50     NaN   50  False         NaN      NaN       False
6  NCL  MNK   PE    sc  PO          2   8.70      20      50      PO   50   True         2.0      2.0        True
7  NCL  MNK   PE    sc  TU          3  12.50      20      50      PO   50  False         NaN      2.0        True
8  NCL  MNK   PE    sc  MA          1  16.70      20      50      PO   50  False         NaN      2.0       False

",pandas
how to flatten a hierarchical index in columns,"I have a data frame with a hierarchical index in axis 1 (columns) (from a groupby.agg operation):
     USAF   WBAN  year  month  day  s_PC  s_CL  s_CD  s_CNT  tempf       
                                     sum   sum   sum    sum   amax   amin
0  702730  26451  1993      1    1     1     0    12     13  30.92  24.98
1  702730  26451  1993      1    2     0     0    13     13  32.00  24.98
2  702730  26451  1993      1    3     1    10     2     13  23.00   6.98
3  702730  26451  1993      1    4     1     0    12     13  10.04   3.92
4  702730  26451  1993      1    5     3     0    10     13  19.94  10.94

I want to flatten it, so that it looks like this (names aren't critical - I could rename):
     USAF   WBAN  year  month  day  s_PC  s_CL  s_CD  s_CNT  tempf_amax  tmpf_amin   
0  702730  26451  1993      1    1     1     0    12     13  30.92          24.98
1  702730  26451  1993      1    2     0     0    13     13  32.00          24.98
2  702730  26451  1993      1    3     1    10     2     13  23.00          6.98
3  702730  26451  1993      1    4     1     0    12     13  10.04          3.92
4  702730  26451  1993      1    5     3     0    10     13  19.94          10.94

How do I do this? (I've tried a lot, to no avail.) 
Per a suggestion, here is the head in dict form
{('USAF', ''): {0: '702730',
  1: '702730',
  2: '702730',
  3: '702730',
  4: '702730'},
 ('WBAN', ''): {0: '26451', 1: '26451', 2: '26451', 3: '26451', 4: '26451'},
 ('day', ''): {0: 1, 1: 2, 2: 3, 3: 4, 4: 5},
 ('month', ''): {0: 1, 1: 1, 2: 1, 3: 1, 4: 1},
 ('s_CD', 'sum'): {0: 12.0, 1: 13.0, 2: 2.0, 3: 12.0, 4: 10.0},
 ('s_CL', 'sum'): {0: 0.0, 1: 0.0, 2: 10.0, 3: 0.0, 4: 0.0},
 ('s_CNT', 'sum'): {0: 13.0, 1: 13.0, 2: 13.0, 3: 13.0, 4: 13.0},
 ('s_PC', 'sum'): {0: 1.0, 1: 0.0, 2: 1.0, 3: 1.0, 4: 3.0},
 ('tempf', 'amax'): {0: 30.920000000000002,
  1: 32.0,
  2: 23.0,
  3: 10.039999999999999,
  4: 19.939999999999998},
 ('tempf', 'amin'): {0: 24.98,
  1: 24.98,
  2: 6.9799999999999969,
  3: 3.9199999999999982,
  4: 10.940000000000001},
 ('year', ''): {0: 1993, 1: 1993, 2: 1993, 3: 1993, 4: 1993}}

","I think the easiest way to do this would be to set the columns to the top level:
df.columns = df.columns.get_level_values(0)

Note: if the to level has a name you can also access it by this, rather than 0.
.
If you want to combine/join your MultiIndex into one Index (assuming you have just string entries in your columns) you could:
df.columns = [' '.join(col).strip() for col in df.columns.values]

Note: we must strip the whitespace for when there is no second index.
In [11]: [' '.join(col).strip() for col in df.columns.values]
Out[11]: 
['USAF',
 'WBAN',
 'day',
 'month',
 's_CD sum',
 's_CL sum',
 's_CNT sum',
 's_PC sum',
 'tempf amax',
 'tempf amin',
 'year']

",pandas
how to predict list elements outside the bounds of a py dataframe,"I built a model based on LSTM and trained it to predict stock price changes during the day, where the unit of time is one second. The test data gives a result comparable to the real one, but I need to get a forecast for the future outside the existing range. What am I doing wrong?

        close = df['close']
        values = close.values
        values = values.reshape(-1, 1)

        training_scaler = MinMaxScaler(feature_range=(0, 1))

        testing_input = values
        testing_input = training_scaler.fit_transform(testing_input)
        testing = []
        for i in range(50, len(testing_input) + 50):
            testing.append(testing_input[i - 50:i][0])

        testing = np.array(testing)
        testing = np.reshape(testing, (testing.shape[0], testing.shape[1], 1))
        predict = model.predict(testing)
        predict = training_scaler.inverse_transform(predict)

        plt.plot(values, color='blue', label='Stock Price')
        plt.plot(predict, color='red', label='Predicted Stock Price')
        plt.title('Changes')
        plt.xlabel('Timeline')
        plt.ylabel('Stock Price')
        plt.legend()
        plt.show()

My results
It turns out that the model predicts data that I already know. How can I predict future data?
","I found a solution. The problem was that I had trained the model incorrectly and it was unable to predict data outside the set. The code below works correctly:
def learn(self, dataset_path: str) -> Sequential:
    df = pd.read_csv(dataset_path)
    y = df['close'].fillna(method='ffill')
    y = y.values.reshape(-1, 1)

    scaler = MinMaxScaler(feature_range=(0, 1))
    scaler = scaler.fit(y)
    y = scaler.transform(y)

    n_lookback = int(len(y) * 0.24)
    n_forecast = int(len(y) * 0.12)

    X = []
    Y = []

    for i in range(n_lookback, len(y) - n_forecast + 1):
        X.append(y[i - n_lookback: i])
        Y.append(y[i: i + n_forecast])

    X = np.array(X, dtype=np.float16)
    Y = np.array(Y, dtype=np.float16)

    model = Sequential()
    model.add(LSTM(units=50, return_sequences=True, input_shape=(n_lookback, 1)))
    model.add(LSTM(units=50))
    model.add(Dense(n_forecast))

    model.compile(loss='mean_squared_error', optimizer='adam')
    model.fit(X, Y, epochs=30, batch_size=128)

    return model

Call predict method:
def predict(self, model: Sequential, df: pd.DataFrame) -> pd.DataFrame:
    y = df['close'].fillna(method='ffill')
    y = y.values.reshape(-1, 1)

    scaler = MinMaxScaler(feature_range=(0, 1))
    scaler = scaler.fit(y)
    y = scaler.transform(y)

    n_lookback = int(len(y) * 0.24)
    n_forecast = int(len(y) * 0.12)

    X_ = y[- n_lookback:]
    X_ = X_.reshape(1, n_lookback, 1)

    Y_ = model.predict(X_).reshape(-1, 1)
    Y_ = scaler.inverse_transform(Y_)

    timestamp_step = 1_000_000

    df_future = pd.DataFrame(columns=['unix', 'Forecast'])
    unix_range = np.array(
        range(int(df['unix'].iloc[0] / timestamp_step), int(df['unix'].iloc[-1] / timestamp_step) + 1)
    )
    df_future['unix'] = np.array(range(unix_range[-1], (unix_range[-1] + n_forecast) - 1))
    df_future['Forecast'] = pd.Series(Y_.flatten())

    return df_future[df_future['Forecast'].notna()]

",pandas
creating reusable and composable filters for pandas dataframes,"I am working with multiple Pandas DataFrames with a similar structure and would like to create reusable filters that I can define once and then apply or combine as needed.
The only working solution I came up with so far feels clunky to me and makes it hard to combine filters with OR:
import pandas as pd
df = pd.DataFrame({""A"":[1,1,2],""B"":[1,2,3]})

def filter_A(df):
    return df.loc[df[""A""]==1]

def filter_B(df):
    return df.loc[df[""B""]==2]

print(filter_A(filter_B(df)).head())

I am hoping for something along the lines of
filter_A = (df[""A""]==1)
filter_B = (df[""B""]==2)

print(df.loc[(filter_A) & (filter_B)])

but reusable after changing the df and also applicable to other DataFrames with the same columns. Is there any cleaner or more readable way to do this?
","You can use the .eval() method, which allows for the evaluation of a string describing operations on dataframe columns:

Evaluate these string expressions on the dataframe df.

Combine the results of these evaluations using the bitwise AND operator (&), which performs element-wise logical AND operation.

Use the .loc accessor to filter the dataframe based on the combined condition.


filter_A = 'A == 1'
filter_B = 'B == 2'
df.loc[df.eval(filter_A) & df.eval(filter_B)]

Output:
   A  B
1  1  2

",pandas
the easiest way for getting feature names after running selectkbest in scikit learn,"I'm trying to conduct a supervised machine-learning experiment using the SelectKBest feature of scikit-learn, but I'm not sure how to create a new dataframe after finding the best features:
Let's assume I would like to conduct the experiment selecting 5 best features:
from sklearn.feature_selection import SelectKBest, f_classif

select_k_best_classifier = SelectKBest(score_func=f_classif, k=5).fit_transform(features_dataframe, targeted_class)

Now, if I add the line:
import pandas as pd

dataframe = pd.DataFrame(select_k_best_classifier)

I receive a new dataframe without feature names (only index starting from 0 to 4), but I want to create a dataframe with the new selected features, in a way like this:
dataframe = pd.DataFrame(fit_transofrmed_features, columns=features_names)

My question is how to create the features_names list?
I know that I should use:
 select_k_best_classifier.get_support()

Which returns an array of boolean values, where true values indices represent the column that should be selected in the original dataframe.
How should I use this boolean array with the array of all features names I can get via the method feature_names = list(features_dataframe.columns.values) ?
","You can do the following :
mask = select_k_best_classifier.get_support() #list of booleans
new_features = [] # The list of your K best features

for bool_val, feature in zip(mask, feature_names):
    if bool_val:
        new_features.append(feature)

Then change the name of your features:
dataframe = pd.DataFrame(fit_transformed_features, columns=new_features)

",pandas
vectorizing three nested loops that calculate the daily mean of hourly data,"Is there a way to vectorize the following three-nested loop that calcuate the daily mean of hourly data? The function below loops first over the year, then months, and finally over days. It also check for the last month and day to ensure that the loop does not go beyond the last month or day of the data.
def hourly2daily(my_var,my_periods):
 
    import pandas as pd 
    import numpy as np
    import sys
    
    print('######### Daily2monthly function ##################')
    Frs_year   =my_periods[0].year
    Frs_month  =my_periods[0].month
    Frs_day    =my_periods[0].day
    Frs_hour   =my_periods[0].hour
    
    Last_year  =my_periods[-1].year
    Last_month =my_periods[-1].month
    Last_day   =my_periods[-1].day
    Last_hour  =my_periods[-1].hour
    

    print('First year   is '+str(Frs_year) +'\n'+\
          'First months is '+str(Frs_month)+'\n'+\
          'First day    is '+str(Frs_day)+'\n'+\
          'First hour   is '+str(Frs_hour))
    print('        ')
    
    print('Last  year   is '+str(Last_year)+'\n'+\
          'Last  months is '+str(Last_month)+'\n'+\
          'Last  day    is '+str(Last_day)+'\n'+\
          'Last  hour   is '+str(Last_hour))
    
    
    Frs = str(Frs_year)+'/'+str(Frs_month)+'/'+str(Frs_day)+' '+str(Frs_hour)+"":00""
    
    Lst = str(Last_year)+'/'+str(Last_month)+'/'+str(Last_day)+' '+str(Last_hour)+"":00""
    
    my_daily_time=pd.date_range(Frs,Lst,freq='D')
    
    ## END of the data_range tricks ###########

    nt_days=len(my_daily_time)
    nd=np.ndim(my_var)

        
    if (nd == 1): # only time series
        var_mean=np.full((nt_days),np.nan)
    
    if (nd == 2): # e.g., time, lat or lon or lev
        n1=np.shape(my_var)[1]
        var_mean=np.full((nt_days,n1),np.nan)
    
    if (nd == 3): #  e.g., time, lat, lon 
        n1=np.shape(my_var)[1]
        n2=np.shape(my_var)[2]
        var_mean=np.full((nt_days,n1,n2),np.nan)
    
    if (nd == 4): # e.g., time, lat , lon, lev 
        n1=np.shape(my_var)[1]
        n2=np.shape(my_var)[2]
        n3=np.shape(my_var)[3]
        var_mean=np.full((nt_days,n1,n2,n3),np.nan)
    
    end_mm=12
    k=0
    ####### loop over years ################
    for yy in np.arange(Frs_year,Last_year+1):
        print('working on the '+str(yy))
        # in case the last month is NOT 12
        if (yy == Last_year):
            end_mm=Last_month 
            print('The last month is '+str(end_mm))
        ## Loop over months ################
        for mm in np.arange(1,end_mm+1):
            end_day=pd.Period(str(yy)+'-'+str(mm)).days_in_month
            # in case the last day is not at the end of the month.
            if ((yy == Last_year) & (mm == Last_month)):
                end_day=Last_day 
            #### loop over days ###############
            for dd in np.arange(1,end_day+1):
                print(str(yy)+'-'+str(mm)+'-'+str(dd))
                #list all days of the month and year.
                I=np.where((my_periods.year ==  yy) &\
                           (my_periods.month == mm) &\
                           (my_periods.day == dd  ))[0]
                
                print(I)
                # if there is a discontinuity in time.
                # I will be empty and then you have to quit. 
                # you have first to reindex the data. 
                if len(I) == 0 :
                    print('Warning time shift here >>')
                    print('Check the continuity of your time sequence')
                    sys.exit()
                
                var_mean[k,...]=np.nanmean(my_var[I,...],0)
                k=k+1
            
        
    return var_mean,my_daily_time

Here is, perhaps, easy and quick way to call this function.
Note that you may be asked to install Pooch
import numpy as np
import xarray as xr

x = xr.tutorial.load_dataset(""air_temperature"")
time      = x['time']      # reading the time
period=time.to_index().to_period('h')
bb0,bb1=hourly2daily(x['air'],period)

I am aware that there is another way to implement this; for example, I can do the previous calculation in one single loop as shown below, but it won’t help for data with discontinues in time.
daily_tem2m = np.full((int(len_time/24),len_lat,len_lon),np.nan,float)

counter=0
timemm=[]
for i in np.arange(0,len_time,24):
    print(period[i])
    timemm.append(period[i])
    daily_tem2m[counter,:,:]=np.nanmean(cleaned_tem2m_celsius.data[i:i+24,:,:],0)
    counter=counter+1

","Per this documentation, to group the data in daily buckets, you can use similar syntax to the pandas option:
import numpy as np
import xarray as xr

x = (
    xr
    .tutorial
    .load_dataset(""air_temperature"")
    .groupby(""time.day"")
    .mean()
)

Edit
Since you want two things, one to see if there are any non-contiguous dates and then to do the analysis, here's how that can be accomplished:
import pandas as pd
import xarray as xr

x = (
    xr
    .tutorial
    .load_dataset(""air_temperature"")
    .resample(time='D')
)

idx = pd.Series(x.groups).index

# Check that all of the range is contiguous, raise an error otherwise
# dropna because the first element will always be NaT
if not (idx.diff().dropna() == '1 days').all():
    raise ValueError(""Got a non-contiguous date range!"")

# do the calculation
x.mean()

And to show that the index check will fail on a non-contiguous date range:
import pandas as pd 

a = pd.date_range('2023-01-01', periods=5)
b = pd.date_range('2024-01-01', periods=5)

if not (a.union(b).diff().dropna() == '1 days').all():
    raise Exception(""Broken date range!"")

Exception                                 Traceback (most recent call last)
Cell In[8], line 5
      2 b = pd.date_range('2024-01-01', periods=5)
      4 if not (a.union(b).diff().dropna() == '1 days').all():
----> 5     raise Exception(""Broken date range!"")

Exception: Broken date range!

",pandas
how can i display full nontruncated dataframe information in html when converting from pandas dataframe to html,"I converted a Pandas dataframe to an HTML output using the DataFrame.to_html function. When I save this to a separate HTML file, the file shows truncated output.
For example, in my TEXT column,
df.head(1) will show
The film was an excellent effort...
instead of
The film was an excellent effort in deconstructing the complex social sentiments that prevailed during this period.
This rendition is fine in the case of a screen-friendly format of a massive Pandas dataframe, but I need an HTML file that will show complete tabular data contained in the dataframe, that is, something that will show the latter text element rather than the former text snippet.
How would I be able to show the complete, non-truncated text data for each element in my TEXT column in the HTML version of the information? I would imagine that the HTML table would have to display long cells to show the complete data, but as far as I understand, only column-width parameters can be passed into the DataFrame.to_html function.
","Set the display.max_colwidth option to None (or -1 before version 1.0):
pd.set_option('display.max_colwidth', None)

set_option documentation
For example, in IPython, we see that the information is truncated to 50 characters. Anything in excess is ellipsized:

If you set the display.max_colwidth option, the information will be displayed fully:

",pandas
regex exact match,"I have the following sentence:
""The size of the lunch box is around 1.5l or 1500ml""
How can I change this to:
""The size of the lunch box is around 1.5 liter or 1500 milliliter""
In some cases, the value might also be present as ""1.5 l or 1500 ml"" with a space.
I am not be able to capture the ""l"" or ""ml"" when I am trying to build a function, or it is giving me an escape error.
I tried:
def stnd(text):

text = re.sub('^l%',' liter', text) 
text = re.sub('^ml%',' milliliter', text) 

text = re.sub('^\d+\.\d+\s*l$','^\d+\.\d+\s*liter$', text) 
text = re.sub('^^\d+\.\d+\s*ml$%','^\d+\.\d+\s*milliliter$', text) 

return text

","You could use a dict to list all the units as the key, and use a pattern to find a digit followed by either ml or l which you could then use as the key for the dict to get the value.
(?<=\d)m?l\b

The pattern matches:

(?<=\d) Positive lookbehind, assert a digit to the left
m?l\b Match an optional m followed by b and a word boundary

See a regex demo.
Example
s = ""The size of the lunch box is around 1.5l or 1500ml""
pattern = r""(?<=\d)m?l\b""
dct = {
    ""ml"": ""milliliter"",
    ""l"": ""liter""
}
result = re.sub(pattern, lambda x: "" "" + dct[x.group()] if x.group() in dct else x, s)
print(result)

Output
The size of the lunch box is around 1.5 liter or 1500 milliliter

",pandas
how can i make pandas dataframe column headers all lowercase,"I want to make all column headers in my pandas data frame lower case
Example
If I have:
data =

  country country isocode  year     XRAT          tcgdp
0  Canada             CAN  2001  1.54876   924909.44207
1  Canada             CAN  2002  1.56932   957299.91586
2  Canada             CAN  2003  1.40105  1016902.00180
....

I would like to change XRAT to xrat by doing something like:
data.headers.lowercase()

So that I get:
  country country isocode  year     xrat          tcgdp
0  Canada             CAN  2001  1.54876   924909.44207
1  Canada             CAN  2002  1.56932   957299.91586
2  Canada             CAN  2003  1.40105  1016902.00180
3  Canada             CAN  2004  1.30102  1096000.35500
....

I will not know the names of each column header ahead of time.
","You can do it like this:
data.columns = map(str.lower, data.columns)

or
data.columns = [x.lower() for x in data.columns]

example:
>>> data = pd.DataFrame({'A':range(3), 'B':range(3,0,-1), 'C':list('abc')})
>>> data
   A  B  C
0  0  3  a
1  1  2  b
2  2  1  c
>>> data.columns = map(str.lower, data.columns)
>>> data
   a  b  c
0  0  3  a
1  1  2  b
2  2  1  c

",pandas
how to vectorize pandas datetimeindex,"I'm trying to avoid a for loop with DateTimeIndex. I have a function get_latest that looks up the most recent wage index value. When I step through the dates of pay days, the lookup works fine. When I attempt to vectorize the operation, I'm advised TypeError: 'numpy.ndarray' object is not callable. I've tried all manners of dt, date, to_pydatetime, etc. to no avail.
# payroll.py
# %%
import pandas as pd
import datetime

# %%
def get_latest(date, series):
    return series.loc[max([x for x in series.index if x <= date])]

# %%
start_rate = 1000.0
start = '2025-08-01'
end = '2026-02-01'

raise_series = pd.Series ({
    datetime.date(2024, 10, 1) : 0.06,
    datetime.date(2025, 10, 1) : 0.05,
    datetime.date(2026, 1, 1) : 1.055,
    datetime.date(2026, 10, 1) : 0.04,
    datetime.date(2027, 10, 1) : 0.04,
    datetime.date(2028, 10, 1) : 0.04,
    datetime.date(2029, 10, 1) : 0.04
})

# %%
initial_index = pd.Series ({
    datetime.date(1, 1, 1): 0.00
})

index_series = pd.concat([initial_index, raise_series], axis=0)
index_series += 1
index_series = index_series.cumprod()

# %%
pay_days = pd.date_range(start=start, end=end, freq='2W')

# %%
gross = []
for row in pay_days:
    gross.append(get_latest(row.date(), index_series) * start_rate) 
pay_days_gross1 = pd.Series(gross)
# %%
pay_days_gross2 = get_latest(pay_days.date(), index_series) * start_rate
# %%

","Here's one approach:
pay_days_gross = (raise_series.add(1)
                  .cumprod()
                  .reindex(pay_days, 
                           method='ffill', 
                           fill_value=1)
                  .mul(start_rate)
                  )

Output:
2025-08-03    1060.000
2025-08-17    1060.000
2025-08-31    1060.000
2025-09-14    1060.000
2025-09-28    1060.000
2025-10-12    1113.000
2025-10-26    1113.000
2025-11-09    1113.000
2025-11-23    1113.000
2025-12-07    1113.000
2025-12-21    1113.000
2026-01-04    2287.215
2026-01-18    2287.215
2026-02-01    2287.215
Freq: 2W-SUN, dtype: float64


Equality check OP's method:
pay_days_gross1.set_axis(pay_days_gross.index).equals(pay_days_gross)

# True


Explanation / Intermediates

Use Series.add to add 1, turning percentages into multipliers, and Series.cumprod to calculate their cumulative effect over time.

raise_series.add(1).cumprod()

2024-10-01    1.060000
2025-10-01    1.113000
2026-01-01    2.287215
2026-10-01    2.378704
2027-10-01    2.473852
2028-10-01    2.572806
2029-10-01    2.675718
dtype: float64


Use Series.reindex with pay_days, using method='ffill' to forward-fill values and fill_value=1 to set the multiplier to 1 if start precedes the first date in raise_series.
Multiply by start_rate with Series.mul.

",pandas
quick way to create hundreds of new columns in pandas dataframe,"I have a Pandas dataframe that looks like:
Group_ID   feature1   feature2   label
1          3          2          0
1          5          7          0
1          2          4          1
1          9          9          1
1          2          0          1
2          4          1          1
2          8          8          0
2          5          5          0
3          0          9          1
3          4          7          1
3          2          3          0
3          7          2          0

and for each feature i, I would like to create a new feature called featurei_rel for i=1,2 using the following logic: it is given by feature i for that row divided by the mean of the smallest two feature i in the same group.
So for example, for feature1 row 1, the smallest two values in Group 1 are 2 and 2 (row 3 and 5 respectively), hence feature1_rel for row 1 is given by 3/((2+2)/2) = 3/2 = 1.5 and the desired result looks like
Group_ID   feature1   feature2   feature1_rel   feature2_rel   label
1          3          2          3/((2+2)/2)    2/((0+2)/2)    0
1          5          7          5/((2+2)/2)    7/((0+2)/2)    0
1          2          4          2//((2+2)/2)   4/((0+2)/2)    1
1          9          9          9//((2+2)/2)   9/((0+2)/2)    1
1          2          0          2/((2+2)/2)    0/((0+2)/2)    1
2          4          1          4/((4+5)/2)    1/((1+5)/2)    1
2          8          8          8/((4+5)/2)    8/((1+5)/2)    0
2          5          5          5/((4+5)/2)    5/((1+5)/2)    0
3          0          9          0/((0+2)/2)    9/((2+3)/2)    1
3          4          7          4/((0+2)/2)    7/((2+3)/2)    1
3          2          3          2/((0+2)/2)    3/((2+3)/2)    0
3          7          2          7/((0+2)/2)    2/((2+3)/2)    0

So here is what I have tried:
# create columns to find the smallest element in a group
df['feature1min'] = df.groupby('Group_ID')['feature1'].transform('min')
# create columns to find the second smallest element in a group
df['feature1min2'] = df.groupby('Group_ID')['feature1'].nsmallest(2)

df['feature1_rel'] = df['feature1']/((df['feature1min'] + df['feature1min2']) / 2)

However, in my actual dataset, I have hundreds of features and millions of rows, so I was wondering is there any fast way to do it, thank you so much in advance.
","IIUC, you can try:
col = pd.Index(['feature1', 'feature2'])
df[col+'_rel']= df.groupby('Group_ID')[col].transform(lambda x: x/x.nsmallest(2).mean())

Output:
    Group_ID  feature1  feature2  label  feature1_rel  feature2_rel
0          1         3         2      0      1.500000      2.000000
1          1         5         7      0      2.500000      7.000000
2          1         2         4      1      1.000000      4.000000
3          1         9         9      1      4.500000      9.000000
4          1         2         0      1      1.000000      0.000000
5          2         4         1      1      0.888889      0.333333
6          2         8         8      0      1.777778      2.666667
7          2         5         5      0      1.111111      1.666667
8          3         0         9      1      0.000000      3.600000
9          3         4         7      1      4.000000      2.800000
10         3         2         3      0      2.000000      1.200000
11         3         7         2      0      7.000000      0.800000

",pandas
reshape dictionary to make violin plot,"I have some data that is saved in a dictionary of dataframes. The real data is much bigger with index up to 3000 and more columns.
In the end I want to make a violinplot of two of the columns in the dataframes but for multiple dictionary entries. The dictionary has a tuple as a key and I want to gather all entries which first number is the same.

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

data_dict = {
    (5, 1): pd.DataFrame({""Data_1"": [0.235954, 0.739301, 0.443639],
                          ""Data_2"": [0.069884, 0.236283, 0.458250],
                          ""Data_3"": [0.170902, 0.496346, 0.399278],
                          ""Data_4"": [0.888658, 0.591893, 0.381895]}),
    (5, 2): pd.DataFrame({""Data_1"": [0.806812, 0.224321, 0.504660],
                          ""Data_2"": [0.070355, 0.943047, 0.579285],
                          ""Data_3"": [0.526866, 0.251339, 0.600688],
                          ""Data_4"": [0.283107, 0.409486, 0.307315]}),
    (7, 3): pd.DataFrame({""Data_1"": [0.415159, 0.834547, 0.170972],
                          ""Data_2"": [0.125926, 0.401789, 0.759203],
                          ""Data_3"": [0.398494, 0.587857, 0.130558],
                          ""Data_4"": [0.202393, 0.395692, 0.035602]}),
    (7, 4): pd.DataFrame({""Data_1"": [0.923432, 0.622174, 0.185039],
                          ""Data_2"": [0.759154, 0.126699, 0.783596],
                          ""Data_3"": [0.075643, 0.287721, 0.939428],
                          ""Data_4"": [0.983739, 0.738550, 0.108639]})
}

My idea was that I could re-arrange it into a different dictionary and then plot the violinplot. Say that 'Data_1' and 'Data_4' are of interest. So then I loop over the keys in dict as below.
new_dict = {}
for col in ['Data_1','Data_4']:
    df = pd.DataFrame()
    for i in [5,7]:
        temp = []   
        for key, value in dict.items():
            if key[0]==i:
                temp.extend(value[col])
        df[i] = temp
    new_dict[col] = df

This then make the following dict.
new_dict = 
{'Data_1':           5         7
 0  0.235954  0.415159
 1  0.739301  0.834547
 2  0.443639  0.170972
 3  0.806812  0.923432
 4  0.224321  0.622174
 5  0.504660  0.185039,
 'Data_4':           5         7
 0  0.888658  0.202393
 1  0.591893  0.395692
 2  0.381895  0.035602
 3  0.283107  0.983739
 4  0.409486  0.738550
 5  0.307315  0.108639}

Which I then loop over to make the violin plots for Data_1and Data_4.
for key, value in new_dict.items():
    fig, ax = plt.subplots()
    ax.violinplot(value, showmeans= True)
    ax.set(title = key, xlabel = 'Section', ylabel = 'Value')
    ax.set_xticks(np.arange(1,3), labels=['5','7'])

While I get the desired result it's very cumbersome to re-arrange the dictionary. Could this be done in a faster way? Since it's the same column I want for each dictionary entry I feel that it should.
","You could minimize the reshaping by using concat+melt and a higher level plotting library like seaborn:
import seaborn as sns

sns.catplot(data=pd.concat(data_dict, names=['section', None])
                    [['Data_1', 'Data_4']]
                   .melt(ignore_index=False, var_name='dataset')
                   .reset_index(),
            row='dataset',
            x='section', y='value',
            kind='violin',
           )

Output:

Another approach to reshape:
tmp = (pd
   .concat(data_dict, names=['section', None])
                    [['Data_1', 'Data_4']]
   .pipe(lambda x: x.set_axis(pd.MultiIndex.from_arrays([x.index.get_level_values('section'),
                                                         x.groupby('section').cumcount()])))
   .T.stack()
)

# then access the datasets
tmp.loc['Data_1']
# section         5         7
# 0        0.235954  0.415159
# 1        0.739301  0.834547
# 2        0.443639  0.170972
# 3        0.806812  0.923432
# 4        0.224321  0.622174
# 5        0.504660  0.185039

",pandas
how can i scrape a table from baseball reference using pandas and beautiful soup,"I am trying to scrape the pitching stats on this url and then save the dataframe to a csv file.
https://www.baseball-reference.com/boxes/ARI/ARI202204070.shtml
My  current code is below (Python 3.9.7)
_URL = ""https://www.baseball-reference.com/boxes/ARI/ARI202204070.shtml""
data = pd.read_html(_URL,attrs={'id': 'ArizonaDiamondbackspitching'},header=1)[0]
data.to_csv('boxscore.csv', index='False')
return data

When I run this code I get the following error:
Traceback (most recent call last):
  File ""d:\BaseballAlgo\Baseball_WhoWins.py"", line 205, in <module>
    getBoxScore('ARI','2022-04-07')
  File ""d:\BaseballAlgo\Baseball_WhoWins.py"", line 99, in getBoxScore
    data = pd.read_html(_URL,attrs={'id': 'ArizonaDiamondbackspitching'},header=1)[0]
  File ""D:\BaseballAlgo\.venv\lib\site-packages\pandas\io\html.py"", line 1240, in   read_html
    return _parse(
  File ""D:\BaseballAlgo\.venv\lib\site-packages\pandas\io\html.py"", line 1003, in _parse
    raise retained
  File ""D:\BaseballAlgo\.venv\lib\site-packages\pandas\io\html.py"", line 983, in   _parse
    tables = p.parse_tables()
  File ""D:\BaseballAlgo\.venv\lib\site-packages\pandas\io\html.py"", line 249, in parse_tables
    tables = self._parse_tables(self._build_doc(), self.match, self.attrs)
  File ""D:\BaseballAlgo\.venv\lib\site-packages\pandas\io\html.py"", line 598, in   _parse_tables
    raise ValueError(""No tables found"")
ValueError: No tables found

Past iterations of code:
session = BRefSession()
_URL = ""https://www.baseball-reference.com/boxes/ARI/ARI202204070.shtml""
content =session.get(_URL).content
soup = BeautifulSoup(content, ""html.parser"")
table = soup.find_all('table', id=""ArizonaDiamondbackspitching"")
print (table)
data = pd.read_html(StringIO(str(table)))[0]

This code runs and when it prints the table the output is ""[]"". The same traceback above is also outputted as a result of the last line.
I understand what the error is saying but I simply do not understand how that possible. It seems as if the soup.findall function is not able to find the specific table I need but I am not sure why. How can I fix this issue?
","Main issue here is that the table is hidden in the comments, so you have to bring it up first, before BeautifulSoup respectively pandas, that use it under the hood, could find it - simplest solution in my opinion is to replace the specific characters in this case:
.replace('<!--','').replace('-->','')

Example with requests and pandas:
import requests
import pandas as pd

df = pd.read_html(
    requests.get(
        'https://www.baseball-reference.com/boxes/ARI/ARI202204070.shtml').text.replace('<!--','').replace('-->',''), 
    attrs={'id':'ArizonaDiamondbackspitching'}
    )[0]
df

Check also Special Strings in BeautifulSoup docs:

Tag, NavigableString, and BeautifulSoup cover almost everything you’ll
see in an HTML or XML file, but there are a few leftover bits. The
main one you’ll probably encounter is the Comment.

",pandas
pandas drop first columns after csv read,"Is there a way to reference an object within the line of the instantiation ?
See the following example :
I wanted to drop the first column (by index) of a csv file just after reading it (usually pd.to_csv outputs the index as first col) :
df = pd.read_csv(csvfile).drop(self.columns[[0]], axis=1)

I understand self should be placed in the object context but it here describes what I intent to do.
(Of course, doing this operation in two separate lines works perfectly.)
","One way is to use pd.DataFrame.iloc:
import pandas as pd
from io import StringIO

mystr = StringIO(""""""col1,col2,col3
a,b,c
d,e,f
g,h,i
"""""")

df = pd.read_csv(mystr).iloc[:, 1:]

#   col2 col3
# 0    b    c
# 1    e    f
# 2    h    i

",pandas
how to drop rows from pandas data frame that contains a particular string in a particular column,"I have a very large data frame in python and I want to drop all rows that have a particular string inside a particular column.
For example, I want to drop all rows which have the string ""XYZ"" as a substring in the column C of the data frame.
Can this be implemented in an efficient way using .drop() method?
","pandas has vectorized string operations, so you can just filter out the rows that contain the string you don't want:
In [91]: df = pd.DataFrame(dict(A=[5,3,5,6], C=[""foo"",""bar"",""fooXYZbar"", ""bat""]))

In [92]: df
Out[92]:
   A          C
0  5        foo
1  3        bar
2  5  fooXYZbar
3  6        bat

In [93]: df[~df.C.str.contains(""XYZ"")]
Out[93]:
   A    C
0  5  foo
1  3  bar
3  6  bat

",pandas
use a list of values to select rows from a pandas dataframe,"Let’s say I have the following Pandas dataframe:
df = DataFrame({'A': [5,6,3,4], 'B': [1,2,3,5]})
df

     A   B
0    5   1
1    6   2
2    3   3
3    4   5

I can subset based on a specific value:
x = df[df['A'] == 3]
x

     A   B
2    3   3

But how can I subset based on a list of values? - something like this:
list_of_values = [3, 6]

y = df[df['A'] in list_of_values]

To get:
     A    B
1    6    2
2    3    3

","You can use the isin method:
In [1]: df = pd.DataFrame({'A': [5,6,3,4], 'B': [1,2,3,5]})

In [2]: df
Out[2]:
   A  B
0  5  1
1  6  2
2  3  3
3  4  5

In [3]: df[df['A'].isin([3, 6])]
Out[3]:
   A  B
1  6  2
2  3  3

And to get the opposite use ~:
In [4]: df[~df['A'].isin([3, 6])]
Out[4]:
   A  B
0  5  1
3  4  5

",pandas
getting into the definition of the pandasseries superreindex,"I am interested in seeing the source code of the pandas.Series.reindex,  so I jumped to the source code using the link in the documentation page, yet I found a return to other function super().reindex, which I could not yet find out where this function was introduced.
How can I get into the definition of this function  pandas.Series.reindex?
","super().reindex calls the reindex method from Series' parent class. Series class definition is
class Series(base.IndexOpsMixin, NDFrame):

So there are 2 parent classes. In the import at the top of the file you can see that base is imported from pandas.core and NDFrame from pandas.core.generic
In the base.py there is no reindex function but in the class NDFrame there is. You can find the reindex() function there:
https://github.com/pandas-dev/pandas/blob/v2.2.3/pandas/core/generic.py#L5347 (permalink)
",pandas
pandas holiday package black friday offset,"I am using the holidays package to build a calendar of my calendar holidays and I made an adjustment to account for ""Black Friday"" (Day after US Thanksgiving. Always a Friday) with a relative delta for the 4th week in November, which works for 2018 and 2020, but this year, it would be the 5th week in November, which would make my setup ineffective
Is there a better way to ensure that this value always falls on the Friday after Thanksgiving? I'm not sure whether it is best to use the holidays package list of holidays and use some pandas magic to offset based on those values and set the holiday or if there is a better date manipulation method to achieve this.
Here is my method:
self.append({datetime.date(datetime(year, 11, 1) + relativedelta(weekday=FR(+4))) : ""Black Friday""}) # Adding Black Friday

Here is the full code:
import pandas as pd
import numpy as np
import calendar as cal
from datetime import *
from dateutil.relativedelta import *
import holidays

class CorporateHolidays(holidays.UnitedStates):
    def _populate(self, year):
        print(self)
        # Populate the holiday list with the default US holidays
        holidays.UnitedStates._populate(self, year)
        # Remove Holiday Date(s)
        self.pop(datetime.date(datetime(year, 10, 1) + relativedelta(weekday=MO(+2))), None) # Removing Columbus Day
        # Add Holiday Date(s)
        self.append({datetime.date(datetime(year, 12, 24)) : ""Christmas Eve""})
        self.append({datetime.date(datetime(year, 12, 31)) : ""New Years Eve""})
        self.append({datetime.date(datetime(year, 11, 1) + relativedelta(weekday=FR(+4))) : ""Black Friday""}) # Adding Black Friday

Where print(self) renders a list as such:
{datetime.date(2018, 1, 1): ""New Year's Day"", datetime.date(2018, 1, 15): 'Martin Luther King, Jr. Day', datetime.date(2018, 2, 19): ""Washington's Birthday"", datetime.date(2018, 5, 28): 'Memorial Day', datetime.date(2018, 7, 4): 'Independence Day', datetime.date(2018, 9, 3): 'Labor Day', datetime.date(2018, 11, 11): 'Veterans Day', datetime.date(2018, 11, 12): 'Veterans Day (Observed)', datetime.date(2018, 11, 22): 'Thanksgiving', datetime.date(2018, 12, 25): 'Christmas Day', datetime.date(2018, 12, 24): 'Christmas Eve', datetime.date(2018, 12, 31): 'New Years Eve', datetime.date(2018, 11, 23): 'Black Friday'}....

","Thanksgiving is (according to Wikipedia) always the forth Thursday in November. This causes a problem for you if the month starts on a Friday. So rather than using the forth Friday try to use the forth Thursday and add a final day to make it Friday. Something like 
datetime.date(datetime(year, 11, 1) + relativedelta(weekday=TH(+4)) + timedelta(days=1))

In [5]: datetime.date(datetime(2018, 11, 1) + relativedelta(weekday=TH(+4)) + timedelta(days=1))                                                                                                            
Out[5]: datetime.date(2018, 11, 23)

In [6]: datetime.date(datetime(2019, 11, 1) + relativedelta(weekday=TH(+4)) + timedelta(days=1))                                                                                                            
Out[6]: datetime.date(2019, 11, 29)


should do the trick
",pandas
resample multiple columns with pandas,"I want to resample daily stock data into monthly stock data.
data = yf.download(['AAPL', 'TSLA', 'FB'], '2018-01-01', '2019-01-01')['Close']

for column in data:
    data[column].resample('M').last()
    print(data[column])

print(data)

My data:
                  AAPL          FB        TSLA
Date                                          
2018-01-02  172.259995  181.419998  320.529999
2018-01-03  172.229996  184.669998  317.250000
2018-01-04  173.029999  184.330002  314.619995
2018-01-05  175.000000  186.850006  316.579987
2018-01-08  174.350006  188.279999  336.410001

","You can just apply the resample call to the entire DataFrame:
data = yf.download(['AAPL', 'TSLA', 'FB'], '2018-01-01', '2019-01-01')['Close']

data_resampled = data.resample('M').last()

print(data)

See also: DataFrame.agg()
",pandas
how to download dictionary from url,"I would like to download the dictionary from the following url: https://data.sec.gov/api/xbrl/companyfacts/CIK0000320193.json
The reason for that is because I would like to extract some datas from the dict into a pandas df. It should look like this:
filed_date   filed_periode   form    accn
2020-11-01   Q4              10-K    0001193125-15-153166
2020-08-01   Q3              10-Q    0001193125-15-153112

I could extract a dict from another SEC-link by using following code:
import pandas as pd
import urllib
import json

url1 = 'https://www.sec.gov/files/company_tickers_exchange.json'

sec_dict = urllib.request.urlopen(url1)
for line in sec_dict:
    decoded_line = line.decode(""utf-8"")
company_dict = json.loads(decoded_line)

If used the above code for the 1st url, I'll get the following error:
HTTPError: HTTP Error 403: Forbidden

I have tried another following approach but get the same error:
import urllib.request

user_agent = 'Mozilla/5.0 (Windows; U; Windows NT 5.1; en-US; rv:1.9.0.7) Gecko/2009021910 Firefox/3.0.7'

url = ""https://data.sec.gov/api/xbrl/companyfacts/CIK0000320193.json""
headers={'User-Agent':user_agent,} 

request=urllib.request.Request(url,None,headers)
response = urllib.request.urlopen(request)
data = response.read()

Thank you in advance for any pointer :-)
","The SEC has an unusual requirement for user agent strings. They want it to be in the format Sample Company Name AdminContact@<sample company domain>.com
So for me, a compliant user agent would be:
user_agent = 'Dan Monego <myemail>@<emailservice>'

Change the user agent to include your name and email.
",pandas
how do i multiply the values of an array contained in a column in a dataframe by the value in another column,"I have tried to do this in order to create a new column, with each row being an array containing the values of column b multiplied by column a.
data = {'a': [3, 2], 'b': [[4], [7, 2]]}
df = pd.DataFrame(data)
df['c'] = df.apply(lambda row: [row['a'] * x for x in row['b']])

The final result should look like this



a
b
c




3
[4]
[12]


2
[7, 2]
[14, 4]



","Your approach would have been correct with axis=1 (= row-wise, the default apply is column-wise):
df['c'] = df.apply(lambda row: [row['a'] * x for x in row['b']], axis=1)

Using apply is however quite slow since pandas creates an intermediate Series for each row. It will be more efficient to use pure python: a list comprehension is well suited.
df['c'] = [[a * x for x in b] for a, b in zip(df['a'], df['b'])]

Output:
   a       b        c
0  3     [4]     [12]
1  2  [7, 2]  [14, 4]

Comparison of timings
(on 200k rows)
# list comprehension
# df['c'] = [[a * x for x in b] for a, b in zip(df['a'], df['b'])]
98.3 ms ± 3.47 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)

# conversion to numpy arrays
# df['c'] = df['a'] * df['b'].map(np.array)
371 ms ± 75.4 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)

# apply with axis=1
# df['c'] = df.apply(lambda row: [row['a'] * x for x in row['b']], axis=1)
1.65 s ± 65.2 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)

",pandas
how to perform string operation on an entire pandas multiindex,"I have a pandas dataframe with a two-level column index.  It's read in from a spreadsheet where the author used a lot of whitespace to accomplish things like alignment (for example, one column is called 'Tank                             #').
I've been able to remove the whitespace on the levels individually...
level0 = df.columns.levels[0].str.replace('\s', '', regex=True)
level1 = df.columns.levels[1].str.replace('\s', '', regex=True)
df.columns.set_levels([level0, level1], inplace=True)

...but I'm curious if there is a way to do it without having to change each individual level one at a time.
I tried
df.columns.set_levels(df.columns.str.replace('\s', '', regex=True))

but got AttributeError: Can only use .str accessor with Index, not MultiIndex.
Here is a small sample subset of the data.
  Run Info                                      Run Data              
     run # Tank                             # Step A\npH concentration
0     6931                                  5       5.29         33.14
1     6932                                  1       5.28         33.13
2     6933                                  2       5.32          33.4
3     6934                                  3       5.19         32.98
4     6935                                  4       5.28          32.7

>>> df.to_dict('list')
{('Run Info', 'run #'): [6931, 6932, 6933, 6934, 6935],
 ('Run Info', 'Tank                             #'): [5, 1, 2, 3, 4],
 ('Run Data', 'Step A\npH'): [5.29, 5.28, 5.32, 5.19, 5.28],
 ('Run Data', 'concentration'): [33.14, 33.13, 33.4, 32.98, 32.7]}

","How about rename:
import re

df.rename(columns=lambda x: re.sub('\s+', ' ', x.strip()), inplace=True)

If you don't want to keep any of the spaces, you can just replace ' ' with ''.
",pandas
how to explode a column,"Not able to use explode from pandas df.
I want to ""explode"" a named column with named sub columns in a data frame on Jupyter Notebook.
Here is the data frame :
   State or territory  Census population[8][9][a]               
   State or territory         July 1, 2024 (est.)  April 1, 2020
0          California                  39431263.0       39538223
1               Texas                  31290831.0       29145505
2             Florida                  23372215.0       21538187
3            New York                  19867248.0       20201249
4        Pennsylvania                  13078751.0       13002700

I want to explode Census population and then delete April 1 2020 leaving ""State or territory"" and ""July 1, 2024 (est.)""
import pandas as pd

tables1 = pd.read_html(""https://en.wikipedia.org/wiki/Fortune_500"")
tables2 = pd.read_html(
    ""https://en.wikipedia.org/wiki/List_of_U.S._states_and_territories_by_population"")

df1 = tables[1]
df2 = tables2[0]

df1copy = df1.drop([""Rank""], axis=1)
df2copy = df2.drop(
    [""Change, 2010–2020[9][a]"",
     ""House seats[b]"",
     ""Pop.  per elec. vote (2020)[c]"",
     ""Pop. per seat (2020)[a]"",
     ""% US (2020)"",
     ""% EC (2020)""],
    axis=1)
print(df1copy.head())
print(df2copy.head())
df2.drop([""July 1, 2024 (est.)""], axis=1)
print(df2.head())

Here is the result:
KeyError  Traceback (most recent call last)
  File ~/Library/Python/3.9/lib/python/site-packages/pandas/core/indexes/base.py:3805, in Index.get_loc(self, key)
    3804 try:
 -> 3805     return self._engine.get_loc(casted_key)
    3806 except KeyError as err:
KeyError: 'July 1, 2024 (est.)'

","You need to specify the level of the column index that you want to drop.
Since the date column you want to drop is in level 1 you need to mention it explicitly.
df2.drop(columns=[""July 1, 2024 (est.)""], level=1, axis=1)

",pandas
how to on a rolling window pass two column vectors instead of one,"I'm computing technical indicators on a rolling basis to avoid any look-ahead bias, for example, for model training and back-testing. To that end I would like to compute the indicator ForceIndexIndicator using the TA Python project. However this needs two inputs instead of one: close and volume, and I can't get hold of both on my rolling - apply pipeline:
import pandas as pd
import ta

...
df.columns = ['close', 'volume']
df['force_index_close'] = (
    df.rolling(window=window)
    .apply(
        lambda x: ta.volume.ForceIndexIndicator(
            close=x['close'],
            volume=x['volume'],
            window=13,
            fillna=True)
        .force_index().iloc[-1]))

I get the error KeyError: 'close' because apply gets one column at a time and not both simultaneously as needed.
","I found two ways to do it, but one of them using numba and rolling method='table' doesn't work because numba is a bit obscure and doesn't understand the outside context of the callback function.
However, the solution based on this answer works perfectly:
df['force_index_close'] = df['close'].rolling(window=window).\
    apply(args=(df['volume'],), 
          func=lambda close, dfv: ta.volume.ForceIndexIndicator(close=close, volume=dfv.loc[close.index], window=13, fillna=True).force_index().iloc[-1])
print(df['force_index_close'])
df['force_index_close'].plot()

this is what's happening:

I perform the rolling on a single column close, otherwise the apply is computed twice, i.e. once per column
apply gets an additional context args with the series made of the other column volume, if your use-case would require additional columns then they could be injected here into the apply
in the apply func I simply narrow the context volume series to align to the close index

",pandas
sum columns in dataframe with pandas,"I have a dataframe df_F1
df_F1.info()
   <class 'pandas.core.frame.DataFrame'>
    Int64Index: 2 entries, 0 to 1
    Data columns (total 7 columns):
    class_energy              2 non-null object
    ACT_TIME_AERATEUR_1_F1    2 non-null float64
    ACT_TIME_AERATEUR_1_F3    2 non-null float64
    dtypes: float64(6), object(1)
    memory usage: 128.0+ bytes

df_F1.head()
 class_energy ACT_TIME_AERATEUR_1_F1 ACT_TIME_AERATEUR_1_F3   
 low 5.875550 431 
 medium 856.666667 856

I try to create a dataframe Ratio which contain for each class_energy the value of energy of each ACT_TIME_AERATEUR_1_Fx divided by the sum of energy of all class_energy for each ACT_TIME_AERATEUR_1_Fx. For example:

ACT_TIME_AERATEUR_1_F1 ACT_TIME_AERATEUR_1_F3 
low        5.875550/(5.875550 + 856.666667) 431/(431+856) 
medium 856.666667/(5.875550+856.666667) 856/(431+856)


Can you help me please to resolve it?
","you can do this:
In [20]: df.set_index('class_energy').apply(lambda x: x/x.sum()).reset_index()
Out[20]:
  class_energy  ACT_TIME_AERATEUR_1_F1  ACT_TIME_AERATEUR_1_F3
0          low                0.006812                0.334887
1       medium                0.993188                0.665113

",pandas
get only the first and last rows of each group with pandas,"I have huge a dataframe with millions of rows and id. My data looks like this:
Time    ID  X   Y
8:00    A   23  100
9:00    B   24  110
10:00   B   25  120
11:00   C   26  130
12:00   C   27  140
13:00   A   28  150
14:00   A   29  160
15:00   D   30  170
16:00   C   31  180
17:00   B   32  190
18:00   A   33  200
19:00   C   34  210
20:00   A   35  220
21:00   B   36  230
22:00   C   37  240
23:00   B   38  250

When I sort the data on id and time, the result looks like this:
Time    ID  X   Y
8:00    A   23  100
13:00   A   28  150
14:00   A   29  160
18:00   A   33  200
20:00   A   35  220
9:00    B   24  110
10:00   B   25  120
17:00   B   32  190
21:00   B   36  230
23:00   B   38  250
11:00   C   26  130
12:00   C   27  140
16:00   C   31  180
19:00   C   34  210
22:00   C   37  240
15:00   D   30  170

From here, I want to pick only ""The first and the last"" of the id and eliminate the rest. The expected result looks like this:
Time    ID  X   Y
8:00    A   23  100
20:00   A   35  220
9:00    B   24  110
23:00   B   38  250
11:00   C   26  130
22:00   C   37  240
15:00   D   30  170

How to do it in pandas?
","Use groupby, find the head and tail for each group, and concat the two.
g = df.groupby('ID')

(pd.concat([g.head(1), g.tail(1)])
   .drop_duplicates()
   .sort_values('ID')
   .reset_index(drop=True))

    Time ID   X    Y
0   8:00  A  23  100
1  20:00  A  35  220
2   9:00  B  24  110
3  23:00  B  38  250
4  11:00  C  26  130
5  22:00  C  37  240
6  15:00  D  30  170

If you can guarantee each ID group has at least two rows, the drop_duplicates call is not needed.

Details
g.head(1)

    Time ID   X    Y
0   8:00  A  23  100
1   9:00  B  24  110
3  11:00  C  26  130
7  15:00  D  30  170

g.tail(1)

     Time ID   X    Y
7   15:00  D  30  170
12  20:00  A  35  220
14  22:00  C  37  240
15  23:00  B  38  250

pd.concat([g.head(1), g.tail(1)])

     Time ID   X    Y
0    8:00  A  23  100
1    9:00  B  24  110
3   11:00  C  26  130
7   15:00  D  30  170
7   15:00  D  30  170
12  20:00  A  35  220
14  22:00  C  37  240
15  23:00  B  38  250

",pandas
python plotly  customdata saving all columns at the first index and separating them by commas,"Whenever I add more than one column to my customdata, it saves the data at the first index (customdata[0]) and separates them by commas, instead of spreading them out across different indexes. Any reference to the other indexes shows the literal text (ex. calling %{customdata[1]} will just show %{customdata[1]} itself.)
Also, whenever I try to format the data (ex. $%{customdata[0]:,.2f}) when customdata has multiple inputs, it turns the data into NaN.
How can I make sure the data gets spread to the other indexes?
Is this even the correct approach/syntax to passing my data to the hover text?
Context:
I'm making a donut chart with plotly express, and using Pandas to pass it data from my spreadsheet. I want to display additional information when I hover over each part of the graph by using hover text. I'm doing this by passing the spreadsheet's columns as customdata under the update_traces function, and referencing them in the hovertemplate.
    exceldf = pd.read_excel('spreadsheetname.xlsx', sheet_name='Transaction', usecols='B:H', skiprows=4)
    exceldf = exceldf.dropna(how='any')

    donutchart = px.pie(
        exceldf, 
        names='Ticker', 
        values='Weight', 
        hole=0.5,
    )
    
    donutchart.update_traces(
    customdata=exceldf[['Live Price', 'Purchase Price', 'Quantity']].to_numpy(),
    hovertemplate='Customdata[0]: %{customdata[0]}<br>Customdata[1]: %{customdata[1]}<br>Customdata[2]: %{customdata[2]}<extra></extra>'
    )

I've tried to correct this by:
-Changing customdata to a numpy array (no change)
-Checking whether the columns' data types were correct (these 3 are floats)
-Using hover_data instead of hovertemplate (gives me an ambiguous input error)
","Since your data is unknown, add multiple column names to custom_data in list form based on this reference example. Specify the location of that list for the hover template specification and add the numeric display format.
import plotly.express as px
df = px.data.gapminder().query(""year == 2007"").query(""continent == 'Asia'"")
df = df.sort_values('pop', ascending=False).head(10)

fig = px.pie(df,
             values='pop',
             names='country',
             custom_data=['lifeExp', 'gdpPercap'],
             title='Top 10 population of Asia continent',
             hole=0.5,)
    
fig.update_traces(
    hovertemplate=
    ""LifeExp: %{customdata[0][0]:.02f}<br>"" +
    ""GDPperCap: %{customdata[0][1]:.02f}"" +
    ""<extra></extra>""
)
fig.show()


",pandas
how to stream dataframe using fastapi without saving the data to csv file,"I would like to know how to stream a DataFrame using FastAPI without having to save the DataFrame to a csv file on disk. Currently, what I managed to do is to stream data from the csv file, but the speed was not very fast compared to returning a FileResponse. The /option7 below is what I'm trying to do.
My goal is to stream data from FastAPI backend without saving the DataFrame to a csv file.
Thank you.
from fastapi import FastAPI, Response,Query
from fastapi.responses import FileResponse,HTMLResponse,StreamingResponse
app = FastAPI()

df = pd.read_csv(""data.csv"")

@app.get(""/option4"")
def load_questions():
    return FileResponse(path=""C:Downloads/data.csv"", filename=""data.csv"")

@app.get(""/option5"")
def load_questions():
    def iterfile():  # 
        with open('data.csv', mode=""rb"") as file_like:  # 
            yield from file_like  # 

    return StreamingResponse(iterfile(), media_type=""text/csv"")

@app.get(""/option7"")
def load_questions():
    def iterfile():  # 
        #with open(df, mode=""rb"") as file_like:  # 
        yield from df  # 

    return StreamingResponse(iterfile(), media_type=""application/json"")



","Approach 1
As mentioned in this answer, as well as here and here, when the entire data (a DataFrame in your case) is already loaded into memory, there is no need to use StreamingResponse. StreamingResponse makes sense when you want to transfer real-time data and when you don't know the size of your output ahead of time, and you don't want to wait to collect it all to find out before you start sending it to the client, as well as when a file that you would like to return is too large to fit into memory—for instance, if you have 8GB of RAM, you can't load a 50GB file—and hence, you would rather load the file into memory in chunks.
In your case, as the DataFrame is already loaded into memory, you should instead return a custom Response directly, after using .to_json() method to convert the  DataFrame into a JSON string, as described in this answer (see related posts here and here as well). Example:
from fastapi import Response

@app.get(""/"")
def main():
    return Response(df.to_json(orient=""records""), media_type=""application/json"")

If you find the browser taking a while to display the data, you may want to have the data downloaded as a .json file to the user's device (which would be completed much faster), rather than waiting for the browser to display a large amount of data. You can do that by setting the Content-Disposition header in the Response using the attachment parameter (see this answer for more details):
@app.get(""/"")
def main():
    headers = {'Content-Disposition': 'attachment; filename=""data.json""'}
    return Response(df.to_json(orient=""records""), headers=headers, media_type='application/json')

You could also return the data as a .csv file, using the .to_csv() method without specifying the path parameter. Since using return df.to_csv() would result in displaying the data in the browser with \r\n characters included, you might find it better to put the csv data in a Response instead, and specify the Content-Disposition header, so that the data will be downloaded as a .csv file. Example:
@app.get(""/"")
def main():
    headers = {'Content-Disposition': 'attachment; filename=""data.csv""'}
    return Response(df.to_csv(), headers=headers, media_type=""text/csv"")

Approach 2
To use a StreamingResponse, you would need to iterate over the rows in a DataFrame, convert each row into a dictionary and subsequently into a JSON string, using either the standard json library, or other faster JSON encoders, as described in this answer (the JSON string will be later encoded into byte format internally by FastAPI/Starlette, as shown in the source code here). Example:
@app.get(""/"")
def main():
    def iter_df():
        for _, row in df.iterrows():
            yield json.dumps(row.to_dict()) + '\n'

    return StreamingResponse(iter_df(), media_type=""application/json"")

Iterating through Pandas objects is generally slow and not recommended. As described in this answer:

Iteration in Pandas is an anti-pattern and is something you should
only do when you have exhausted every other option. You should
not use any function with ""iter"" in its name for more than a few
thousand rows or you will have to get used to a lot of waiting.

Update
As @Panagiotis Kanavos noted in the comments section below, using either .to_json() or .to_csv() (as demonstrated in Approach 1 earlier) on a DataFrame that is already loaded into memory, would result in allocating the entire output string in memory as well, thus doubling the RAM usage or even worse. Hence, in the case of having such a huge amount of data that could likely cause your system to slow down or crash (because of running out of memory) if used either of the DataFrame methods described above, you should rather use a StreamingResponse instead, as described earlier in Approach 2. You might find faster alernative methods to iterrows() in this post, as well as faster JSON encoders, such as orjson and ujson, as described in this answer and this answer.
Alternatively, you could save the data to disk (a NamedTemporaryFile could be used as well, as described here, here and here, which might further increase the performance of the app), then delete the DataFrame to release the memory—you could even manually trigger the garbage collection using gc.collect(), as shown in this answer; however, frequent calls to garbage collection is discouraged, as it is a costly operation and may affect performance—and return a FileResponse (or a StreamingResponse, see this answer and this answer), and finally, have a BackgroundTask to delete the file from disk after returning the response. Example:
from fastapi import BackgroundTasks
from fastapi.responses import FileResponse 
import uuid
import os

@app.get(""/"")
def main(background_tasks: BackgroundTasks):
    filename = str(uuid.uuid4()) + "".csv""
    df.to_csv(filename)
    del df  # release the memory
    background_tasks.add_task(os.remove, filename) 
    return FileResponse(filename, filename=""data.csv"", media_type=""text/csv"")
    # or return StreamingResponse - see the linked answers above

The solution you may choose should be based on your application's requirements (e.g., the number of users you expect to serve simultaneously, the size of data, the response time, etc.), as well as your system's specifications (e.g., available memory for allocation). Additionally, since all calls to DataFrame's methods are synchronous, you should remember to define your endpoint with a normal def, so that it is run in an external threadpool; otherwise, it would block the server. Alternatively, you could use Starlette's run_in_threadpool() from the concurrency module, which would run the to_csv() or to_json() function in a separate thread to ensure that the main thread (where coroutines are run) does not get blocked. Please have a look at this answer and this answer for more details on def vs async def, as well as solutions when one has to run synchronous blocking operations inside async def endpoints.
",pandas
efficiently filtering comma separated strings in pandasdask,"I have some data which has the following shape (with header)
Name,Signal,Date
MyName,""1,2,3,4,5,6,7,8,9,10"",19-04-2024
MyName,""1,2,3,4,5,6,7,8,9,10"",19-04-2024

I'm interested in filtering the rows based on the sum of the array in ""Signal"". So I tried the following :
df = read_csv(""my_csv.csv"", dtype={""Signal"" : ""string""}, parse_dates=True)

for i in df[""Signal""]:
   t = np.array([int(x) for x in i.split("","")])
   if t.sum() == 100:
       #etc

This approach raises some issues though :

How can I then record the index of the current row to then filter/drop it from my dataframe
Can this operation be sped up/done more efficiently? I was thinking of allocating a 2d numpy array and then parsing in the numbers to only allocate once but not sure this would make a difference
When using dask, which lacks a global row index, is there a more efficient way to filter the rows without allocating all the data into numpy arrays?

","Having read in your dataframe, you can then convert the Signal values into a list of integers using this code:
df['Signal'] = df['Signal'].apply(lambda s:list(map(int, s.split(','))))

Output:
     Name                           Signal        Date
0  MyName  [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]  19-04-2024
1  MyName  [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]  19-04-2024

Then you can play with your values to filter the dataframe. For example, to filter on sums:
sums = df['Signal'].apply(sum)
# 0    55
# 1    55
# Name: Signal, dtype: int64

mask = sums == 100
# 0    False
# 1    False
# Name: Signal, dtype: bool

df_filtered = df[mask]

If you want to use the sums more than once, it's probably best to save them in the dataframe e.g.
df['sum'] = df['Signal'].apply(sum)

",pandas
how can i get a value from a cell of a dataframe,"I have constructed a condition that extracts exactly one row from my dataframe:
d2 = df[(df['l_ext']==l_ext) & (df['item']==item) & (df['wn']==wn) & (df['wd']==1)]

Now I would like to take a value from a particular column:
val = d2['col_name']

But as a result, I get a dataframe that contains one row and one column (i.e., one cell). It is not what I need. I need one value (one float number). How can I do it in pandas?
","If you have a DataFrame with only one row, then access the first (only) row as a Series using iloc, and then the value using the column name:
In [3]: sub_df
Out[3]:
          A         B
2 -0.133653 -0.030854

In [4]: sub_df.iloc[0]
Out[4]:
A   -0.133653
B   -0.030854
Name: 2, dtype: float64

In [5]: sub_df.iloc[0]['A']
Out[5]: -0.13365288513107493

",pandas
constructing dataframe from values in variables yields quotvalueerror if using all scalar values you must pass an indexquot,"I have two variables as follows.
a = 2
b = 3

I want to construct a DataFrame from this:
df2 = pd.DataFrame({'A':a, 'B':b})

This generates an error:
ValueError: If using all scalar values, you must pass an index

I tried this also:
df2 = (pd.DataFrame({'a':a, 'b':b})).reset_index()

This gives the same error message. How do I do what I want?
","The error message says that if you're passing scalar values, you have to pass an index.  So you can either not use scalar values for the columns -- e.g. use a list:
>>> df = pd.DataFrame({'A': [a], 'B': [b]})
>>> df
   A  B
0  2  3

or use scalar values and pass an index:
>>> df = pd.DataFrame({'A': a, 'B': b}, index=[0, 3])
>>> df
   A  B
0  2  3
3  2  3

",pandas
excel file format cannot be determined you must specify an engine manually,"I am not sure why I am getting this error although sometimes my code works fine!
Excel file format cannot be determined, you must specify an engine manually.
Here below is my code with steps:
1- list of columns of customers Id:
customer_id = [""ID"",""customer_id"",""consumer_number"",""cus_id"",""client_ID""]

2- The code to find all xlsx files in a folder and read them:
l = [] #use a list and concat later, faster than append in the loop
for f in glob.glob(""./*.xlsx""):
    df = pd.read_excel(f).reindex(columns=customer_id).dropna(how='all', axis=1)
    df.columns = [""ID""] # to have only one column once concat
    l.append(df)
all_data  = pd.concat(l, ignore_index=True) # concat all data

I added the engine openpyxl
df = pd.read_excel(f, engine=""openpyxl"").reindex(columns = customer_id).dropna(how='all', axis=1)
Now I got a different error:
BadZipFile: File is not a zip file

pandas version: 1.3.0
python version: python3.9
os: MacOS
is there a better way to read all xlsx files from a folder ?
","Found it. When an excel file is opened for example by MS excel a hidden temporary file is created in the same directory:
~$datasheet.xlsx

So, when I run the code to read all the files from the folder it gives me the error:
Excel file format cannot be determined, you must specify an engine manually.

When all files are closed and no hidden temporary files ~$filename.xlsx in the same directory the code works perfectly.
",pandas
how do i make a simple multilevel sankey diagram with plotly,"I have a DataFrame like this that I'm trying to describe with a Sankey diagram:
import pandas as pd

pd.DataFrame({
    'animal': ['dog', 'cat', 'cat', 'dog', 'cat'],
    'sex': ['male', 'female', 'female', 'male', 'male'],
    'status': ['wild', 'domesticated', 'domesticated', 'wild', 'domesticated'],
    'count': [8, 10, 11, 14, 6]
})

    animal  sex     status          count
0   dog     male    wild            8
1   cat     female  domesticated    10
2   cat     female  domesticated    11
3   dog     male    wild            14
4   cat     male    domesticated    6

I'm trying to follow the steps in the documentation but I can't make it work - I can't understand what branches where. Here's the example code:
import plotly.graph_objects as go

fig = go.Figure(data=[go.Sankey(
    node = dict(
      pad = 15,
      thickness = 20,
      line = dict(color = ""black"", width = 0.5),
      label = [""A1"", ""A2"", ""B1"", ""B2"", ""C1"", ""C2""],
      color = ""blue""
    ),
    link = dict(
      source = [0, 1, 0, 2, 3, 3], 
      target = [2, 3, 3, 4, 4, 5],
      value = [8, 4, 2, 8, 4, 2]
  ))])

fig.update_layout(title_text=""Basic Sankey Diagram"", font_size=10)
fig.show()

Here's what I'm trying to achieve:

","You can create with Plotly a Sankey diagram in the following way:
import pandas as pd
import plotly.graph_objects as go

label_list = ['cat', 'dog', 'domesticated', 'female', 'male', 'wild']
# cat: 0, dog: 1, domesticated: 2, female: 3, male: 4, wild: 5
source = [0, 0, 1, 3, 4, 4]
target = [3, 4, 4, 2, 2, 5]
count = [21, 6, 22, 21, 6, 22]

fig = go.Figure(data=[go.Sankey(
    node = {""label"": label_list},
    link = {""source"": source, ""target"": target, ""value"": count}
    )])
fig.show()


How does it work: The lists source, target and count have all length 6 and the Sankey diagram has 6 arrows. The elements of source and target are the indexes of label_list. So the the first element of source is 0 which means ""cat"". The first element of target is 3 which means ""female"". The first element of count is 21. Therefore, the first arrow of the diagram goes from cat to female and has size 21. Correspondingly, the second elements of the lists source, target and count define the second arrow, etc.

Possibly you want to create a bigger Sankey diagram as in this example. Defining the source, target and count list manually then becomes very tedious.
So here's a code which creates these lists from a dataframe of your format.
import pandas as pd
import numpy as np

df = pd.DataFrame({
    'animal': ['dog', 'cat', 'cat', 'dog', 'cat'],
    'sex': ['male', 'female', 'female', 'male', 'male'],
    'status': ['wild', 'domesticated', 'domesticated', 'wild', 'domesticated'],
    'count': [8, 10, 11, 14, 6]
})

categories = ['animal', 'sex', 'status']

newDf = pd.DataFrame()
for i in range(len(categories)-1):
    tempDf = df[[categories[i],categories[i+1],'count']]
    tempDf.columns = ['source','target','count']
    newDf = pd.concat([newDf,tempDf])    
newDf = newDf.groupby(['source','target']).agg({'count':'sum'}).reset_index()

label_list = list(np.unique(df[categories].values))
source = newDf['source'].apply(lambda x: label_list.index(x))
target = newDf['target'].apply(lambda x: label_list.index(x))
count = newDf['count']

",pandas
how to get coefficients of multinomial logistic regression,"I need to calculate coefficients of a multiple logistic regression using sklearn:
X =
x1          x2          x3   x4         x5    x6
0.300000    0.100000    0.0  0.0000     0.5   0.0
0.000000    0.006000    0.0  0.0000     0.2   0.0
0.010000    0.678000    0.0  0.0000     2.0   0.0
0.000000    0.333000    1.0  12.3966    0.1   4.0
0.200000    0.005000    1.0  0.4050     1.0   0.0
0.000000    0.340000    1.0  15.7025    0.5   0.0
0.000000    0.440000    1.0  8.2645     0.0   4.0
0.500000    0.055000    1.0  18.1818    0.0   4.0

The values of y are categorical in range [1; 4].
y =
1
2
1
3
4
1
2
3

This is what I do:
import pandas as pd
from sklearn import linear_modelion
from sklearn.metrics import mean_squared_error, r2_score
import numpy as np

h = .02

logreg = linear_model.LogisticRegression(C=1e5)

logreg.fit(X, y)

# print the coefficients
print(logreg.intercept_)
print(logreg.coef_)

However, I get 6 columns in the output of logreg.intercept_ and 6 columns in the output of logreg.coef_ How can I get 1 coefficient per feature, e.g. a - f values?
y = a*x1 + b*x2 + c*x3 + d*x4 + e*x5 + f*x6

Also, probably I am doing something wrong, because y_pred = logreg.predict(X) gives me the value of 1 for all rows.
","Check the online documentation:

coef_ : array, shape (1, n_features) or (n_classes, n_features)
Coefficient of the features in the decision function.
coef_ is of shape (1, n_features) when the given problem is binary.

As @Xochipilli has already mentioned in comments you are going to have (n_classes, n_features) or in your case (4,6) coefficients and 4 intercepts (one for each class)

Probably I am doing something wrong, because y_pred =
  logreg.predict(X) gives me the value of 1 for all rows.

yes, you shouldn't try to use data that you've used for training your model for  prediction. Split your data into training and test data sets, train your model using train data set and check it's accuracy using test data set. 
",pandas
pandas read_csv and keep only certain rows python,"I am aware of the skiprows that allows you to pass a list with the indices of the rows to skip. However, I have the index of the rows I want to keep.
Say that my cvs file looks like this for millions of rows:
  A B
0 1 2
1 3 4
2 5 6
3 7 8
4 9 0

The list of indices i would like to load are only 2,3, so
index_list = [2,3]

The input for the skiprows function would be [0,1,4]. However, I only have available [2,3].
I am trying something like:
pd.read_csv(path, skiprows = ~index_list)

but no luck.. any suggestions?
thank and I appreciate all the help,
","I think you would need to find the number of lines first, like this.
num_lines = sum(1 for line in open('myfile.txt'))

Then you would need to delete the indices of index_list:
to_exclude = [i for i in range(num_lines) if i not in index_list]

and then load your data:
pd.read_csv(path, skiprows = to_exclude)

",pandas
creating a link id between two related columns,"I have a file with some family relationship data and I would like to create a family id column based on id and sibling_id. My data looks like the following:
import pandas as pd
import numpy as np

df = pd.DataFrame({
    'id': [1, 1, 2, 2, 3, 3, 4, 5, 6, 7],
    'field_a': list('AABBCCDEFG'),
    'sibling_id': [2, 3, 1, 3, 1, 2, np.nan, np.nan, 7, 6],
    'sibling_field_a': ['B', 'C', 'A', 'C' , 'A', 'B', np.nan, np.nan, 'G', 'F']
})

df['sibling_id'] = df['sibling_id'].astype('Int64')

   id field_a  sibling_id sibling_field_a
0   1       A           2               B
1   1       A           3               C
2   2       B           1               A
3   2       B           3               C
4   3       C           1               A
5   3       C           2               B
6   4       D        <NA>             NaN
7   5       E        <NA>             NaN
8   6       F           7               G
9   7       G           6               F

My expected output is
   id field_a  sibling_id sibling_field_a  family_id
0   1       A           2               B          0
1   1       A           3               C          0
2   2       B           1               A          0
3   2       B           3               C          0
4   3       C           1               A          0
5   3       C           2               B          0
6   4       D        <NA>             NaN          1
7   5       E        <NA>             NaN          2
8   6       F           7               G          3
9   7       G           6               F          3

Thank you for the help in advance.
","This is a graph problem, it cannot be solved easily with pandas only if you have complex relationships (e.g. half-sibling of half-siblings).
You can use networkx's connected_components after converting the DataFrame to graph:
# pip install networkx
import networkx as nx

# convert to graph
G = nx.from_pandas_edgelist(df,
                            source='field_a',
                            target='sibling_field_a')
# ensure NaNs are not a node
G.remove_nodes_from([np.nan])

# compute the groups/families
groups = {n: i for i, c in enumerate(nx.connected_components(G)) for n in c}

# map the groups to the ids
df['family_id'] = df['field_a'].map(groups)

Output:
   id field_a  sibling_id sibling_field_a  family_id
0   1       A           2               B          0
1   1       A           3               C          0
2   2       B           1               A          0
3   2       B           3               C          0
4   3       C           1               A          0
5   3       C           2               B          0
6   4       D        <NA>             NaN          1
7   5       E        <NA>             NaN          2
8   6       F           7               G          3
9   7       G           6               F          3

Graph:

",pandas
converting a column to date in pandas,"I'm having difficulty with Pandas when trying to convert this column to a date. The table doesn't include a year, so I think that's making the conversion difficult.
    28 JUL  Unnamed: 0        Alura *Alura - 7/12     68,00
0   28 JUL         NaN  Passei Direto S/A. - 3/12     19,90
1   31 JUL         NaN          Drogarias Pacheco     25,99
2   31 JUL         NaN     Mundo Verde - Rj - Sho      5,90
3   31 JUL         NaN              Paypal *99app      4,25
4   04 AGO         NaN            Saldo em atraso  1.091,17
5   04 AGO         NaN          Crédito de atraso  1.091,17
6   06 AGO         NaN             Apple.Com/Bill     34,90
7   07 AGO         NaN        Pagamento em 07 AGO  1.091,17
8   07 AGO         NaN            Juros de atraso     16,86
9   07 AGO         NaN              IOF de atraso      4,43
10  07 AGO         NaN            Multa de atraso     21,91
11  08 AGO         NaN             Apple.Com/Bill     21,90
12  09 AGO         NaN      Google Youtubepremium     20,90
13  10 AGO         NaN              Amazon.Com.Br     41,32
14  12 AGO         NaN           Uber *Uber *Trip     17,91
15  12 AGO         NaN           Uber *Uber *Trip     16,94
16  12 AGO         NaN                Mia Cookies     47,50
17  13 AGO         NaN           Uber *Uber *Trip     16,96
18  13 AGO         NaN           Uber *Uber *Trip     19,98
19  16 AGO         NaN           Uber *Uber *Trip     11,93
20  16 AGO         NaN           Uber *Uber *Trip      9,97
21  18 AGO         NaN           Uber *Uber *Trip      9,91
22  22 AGO         NaN           Uber *Uber *Trip      9,96
23  23 AGO         NaN              Amazonprimebr     14,90
24  27 AGO         NaN        Paypal *Sacocheiotv     15,00
25  27 AGO         NaN        Pag*Easymarketpleno      6,50

I tried to transform it using this code, but it's not working:
df[""Data""] = pd.to_datetime(df[""Data""], format=""%d %b"", errors=""coerce"")

Incorrect output:
Data                      Local  Valor
0  1900-07-28        Alura *Alura - 7/12  68,00
1  1900-07-28  Passei Direto S/A. - 3/12  19,90
2  1900-07-31          Drogarias Pacheco  25,99
3  1900-07-31     Mundo Verde - Rj - Sho   5,90
4  1900-07-31              Paypal *99app   4,25
7         NaT             Apple.Com/Bill  34,90
9         NaT            Juros de atraso  16,86
10        NaT              IOF de atraso   4,43
11        NaT            Multa de atraso  21,91
12        NaT             Apple.Com/Bill  21,90
13        NaT      Google Youtubepremium  20,90
14        NaT              Amazon.Com.Br  41,32
15        NaT           Uber *Uber *Trip  17,91
16        NaT           Uber *Uber *Trip  16,94
17        NaT                Mia Cookies  47,50
18        NaT           Uber *Uber *Trip  16,96
19        NaT           Uber *Uber *Trip  19,98
20        NaT           Uber *Uber *Trip  11,93
21        NaT           Uber *Uber *Trip   9,97
22        NaT           Uber *Uber *Trip   9,91
23        NaT           Uber *Uber *Trip   9,96
24        NaT              Amazonprimebr  14,90
25        NaT        Paypal *Sacocheiotv  15,00
26        NaT        Pag*Easymarketpleno   6,50

Could someone help me with this?
","This looks like Brazilian Portuguese, you should install the pt_BR locale on your machine, then run:
import locale
locale.setlocale(locale.LC_ALL, 'pt_BR.UTF-8')
df['Data_converted'] = pd.to_datetime(df['Data'], format='%d %b',
                                      errors='coerce')

Output:
     Data Data_converted
0  28 JUL     1900-07-28
1  04 AGO     1900-08-04

And, if you want to force the year:
df['Data_converted'] = pd.to_datetime('2025 ' + df['Data'],
                                      format='%Y %d %b', errors='coerce')

Output:
     Data Data_converted
0  28 JUL     2025-07-28
1  04 AGO     2025-08-04

",pandas
unable to accurately detect top 7 prominent peaks in data using pythons find_peaks function,"I hope to identify the peaks in a segment of data (selecting the top 7 points with the highest prominences), which are clearly visible to the naked eye. However, I am unable to successfully obtain the results using the find_peaks function.
The data is accessible in this gist.
Error Result: If I directly use find_peaks:
find_peaks(series, prominence=np.max(series) * 0.1, distance=48)

and then select the top 7 points with the highest prominences, I end up with some undesired points.
Clumsy Method: I can first smooth the data:
percentile_80 = series.rolling(
    window=61, center=True, min_periods=1
    ).apply(lambda x: np.percentile(x, 80))
smoothed_series = series - percentile_80

Then, use find_peaks(smoothed_series, prominence=np.max(smoothed_series) * 0.1, distance=48), and select the top 7 points with the highest prominences, which yields the expected results.
However, this approach is much slower.

Edit on 2025.1.9:
Thanks mozway, this is a good method.
And I found another way to speed up: first find all peaks, then compare peaks with neighboring peaks,find the prominence peaks with neighbor. is this a good method?
    def find_significant_peaks(x, prominence_diff_ratio=0.1, initial_distance=3):
        # Step 1: Get all candidate peaks and their properties
        peaks, properties = find_peaks(
            x, distance=initial_distance, prominence=np.max(x) * 0.01
        )
    
        if len(peaks) == 0:
            return peaks, properties
    
        # Get prominences of all peaks
        prominences = properties[""prominences""]
    
        # Calculate prominence differences using vectorized operations
        diffs = np.abs(np.subtract.outer(prominences, prominences))
        threshold_values = prominences * prominence_diff_ratio
    
        valid_peaks_mask = np.ones(len(peaks), dtype=bool)
    
        # For each peak, check prominence difference with neighbors and local maximality
        compare_num = 10
        for i in range(len(peaks)):
            # Get local window range
            start_idx = max(0, i - compare_num)
            end_idx = min(len(peaks), i + 1 + compare_num)
    
            # Get prominence values within local window
            local_prominences = prominences[start_idx:end_idx]
            current_prominence = prominences[i]
    
            # Condition 1: Check if it's a local maximum
            if current_prominence < np.max(local_prominences):
                valid_peaks_mask[i] = False
                continue
    
            # Condition 2: Get prominence differences with neighbors
            neighbor_diffs = diffs[i, start_idx:end_idx]
            neighbor_diffs = neighbor_diffs[neighbor_diffs != 0]  # Remove self-difference
    
            # Check if all neighboring differences are greater than threshold
            if np.any(neighbor_diffs <= threshold_values[i]):
                valid_peaks_mask[i] = False
    
        # Filter valid peaks and properties
        valid_peaks = peaks[valid_peaks_mask]
    
        # Update all properties in the properties dictionary
        valid_properties = {}
        for key in properties:
            valid_properties[key] = properties[key][valid_peaks_mask]
    
        return valid_peaks, valid_properties

","The issue with your approach is that you rely on the prominence, which is the local height of the peaks, and not a good fit with your type of data.
From your total dataset, it looks indeed clear to the naked eye that there are high ""peaks"" relative to the top of the large blue area, but this is no longer obvious once we consider the exact local data:

NB. the scale of the insets' Y-axis is the same.
Also, let's compute the prominence of all peaks (see how the middle peak has a much greater prominence):

As you can see, there are peaks everywhere and what you would define as a peak in the left inset is actually a relatively small peak compared to peaks that you would not want to detect in the right inset.
What you want is a peak that is higher than the surrounding peaks, and you want to fully ignore the baseline, thus your approach of using a smoothing function to get the local trend is good.
Since your issue seems to be about speed, you can greatly improve it by using the native rolling.quantile over a custom rolling.apply with np.percentile:
from scipy.signal import find_peaks

percentile_80 = series.rolling(window=61, center=True, min_periods=1).quantile(0.8)
smoothed_series = series.sub(percentile_80).clip(lower=0)

peaks, peak_data = find_peaks(smoothed_series, prominence=np.max(smoothed_series) * 0.1, distance=48)

series.plot()
series.loc[smoothed_series.iloc[peaks].nlargest(7).index].plot(ls='', marker='o')

This runs in just a few milliseconds compared to more than one second for the custom apply:
# series.rolling(window=61, center=True, min_periods=1).apply(lambda x: np.percentile(x, 80))
1.47 s ± 25 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)

# series.rolling(window=61, center=True, min_periods=1).quantile(0.8)
3.9 ms ± 56.4 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)

Output:

I also added a clip step after smoothing to get the following intermediate:

",pandas
drop_duplicates not working in pandas,"The purpose of my code is to import 2 Excel files, compare them, and print out the differences to a new Excel file.
However, after concatenating all the data, and using the drop_duplicates function, the code is accepted by the console. But, when printed to the new excel file, duplicates still remain within the day.
Am I missing something? Is something nullifying the drop_duplicates function?
My code is as follows:
import datetime
import xlrd
import pandas as pd
#identify excel file paths
filepath = r""excel filepath""
filepath2 = r""excel filepath2""
#read relevant columns from the excel files
df1 = pd.read_excel(filepath, sheetname=""Sheet1"", parse_cols= ""B, D, G, O"")
df2 = pd.read_excel(filepath2, sheetname=""Sheet1"", parse_cols= ""B, D, F, J"")
#merge the columns from both excel files into one column each respectively
df4 = df1[""Exchange Code""] + df1[""Product Type""] + df1[""Product Description""] + df1[""Quantity""].apply(str)
df5 = df2[""Exchange""] + df2[""Product Type""] + df2[""Product Description""] + df2[""Quantity""].apply(str)
#concatenate both columns from each excel file, to make one big column containing all the data
df = pd.concat([df4, df5])
#remove all whitespace from each row of the column of data
df=df.str.strip()
df=["""".join(x.split()) for x in df] 
#convert the data to a dataframe from a series
df = pd.DataFrame({'Value': df}) 
#remove any duplicates
df.drop_duplicates(subset=None, keep=""first"", inplace=False)
#print to the console just as a visual aid
print(df)
#print the erroneous entries to an excel file
df.to_excel(""Comparison19.xls"") 

","You've got inplace=False so you're not modifying df. You want either
 df.drop_duplicates(subset=None, keep=""first"", inplace=True)

or
 df = df.drop_duplicates(subset=None, keep=""first"", inplace=False)

",pandas
databricks issue while creating spark data frame from pandas,"I have a pandas data frame which I want to convert into spark data frame. Usually, I use the below code to create spark data frame from pandas but all of sudden I started to get the below error, I am aware that pandas has removed iteritems() but my current pandas version is 2.0.0 and also I tried to install lesser version and tried to created spark df but I still get the same error. The error invokes inside the spark function. What is the solution for this? which pandas version should I install in order to create spark df. I also tried to change the runtime of cluster databricks and tried re running but I still get the same error.
import pandas as pd
spark.createDataFrame(pd.DataFrame({'i':[1,2,3],'j':[1,2,3]}))

error:-
UserWarning: createDataFrame attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:
  'DataFrame' object has no attribute 'iteritems'
Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.
  warn(msg)
AttributeError: 'DataFrame' object has no attribute 'iteritems'

","It's related to the Databricks Runtime (DBR) version used - the Spark versions in up to DBR 12.2 rely on .iteritems function to construct a Spark DataFrame from Pandas DataFrame.  This issue was fixed in the Spark 3.4 that is available as DBR 13.x.
If you can't upgrade to DBR 13.x, then you need to downgrade the Pandas to latest 1.x version (1.5.3 right now) by using %pip install -U pandas==1.5.3 command in your notebook. Although it's just better to use Pandas version shipped with your DBR - it was tested for compatibility with other packages in DBR.
",pandas
how can i import pandas_datareader,"
I just downloaded Visual Studio Code, and I´m having troubles with importing the library in the newest version of the software.
I hope someone can give me an instruction step by step.
NOTE: I'm using python as my coding language

","
The following instructions are for versions of packages current as of this writing.
pandas-datareader documentation
The module is not part of pandas and must be installed separately
Use the directions at Python in Visual Studio Code to properly configure VS Code to work with your python interpreter.

pip

As per the documentation at pydata / pandas-datareader

pip install pandas-datareader

conda

Use this method if your python was installed as part of the Anaconda distribution.
anaconda / packages / pandas-datareader

# at the anaconda prompt
conda install -c anaconda pandas-datareader

Usage
import pandas_datareader as pdr

Note

For older versions of ipython (e.g. 3.x, which is from before 2015-08), !pip install pandas-datareader or %pip ... may work (mentioned in a comment).

",pandas
how to iterate over columns of a pandas dataframe,"I have this code using Pandas in Python:
all_data = {}
for ticker in ['FIUIX', 'FSAIX', 'FSAVX', 'FSTMX']:
    all_data[ticker] = web.get_data_yahoo(ticker, '1/1/2010', '1/1/2015')

prices = DataFrame({tic: data['Adj Close'] for tic, data in all_data.iteritems()})  
returns = prices.pct_change()

I know I can run a regression like this:
regs = sm.OLS(returns.FIUIX,returns.FSTMX).fit()

but how can I do this for each column in the dataframe? Specifically, how can I iterate over columns, in order to run the regression on each?
Specifically, I want to regress each other ticker symbol (FIUIX, FSAIX and FSAVX) on FSTMX, and store the residuals for each regression.
I've tried various versions of the following, but nothing I've tried gives the desired result:
resids = {}
for k in returns.keys():
    reg = sm.OLS(returns[k],returns.FSTMX).fit()
    resids[k] = reg.resid

Is there something wrong with the returns[k] part of the code? How can I use the k value to access a column? Or else is there a simpler approach?
","Old answer:
for column in df:
    print(df[column])

The previous answer still works, but was added around the time of pandas 0.16.0. Better versions are available.
Now you can do:
for series_name, series in df.items():
    print(series_name)
    print(series)

",pandas
why am i getting only nan values in all columns in jupyter notebook,"This is how the datset_1 looks like,

But when I am making pivot table out of it as show below I am only getting NaN as the volues in all the columns. 
monthly_sales_data = pd.pivot_table(dataset_1, values = ""Weekly_Sales"", columns = ""Year"", index = ""Month"")
monthly_sales_data = monthly_sales_data.reindex(index = ['Jan','Feb','Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec'])
monthly_sales_data

I am only getting NaN values for the whole DataFrame:

","Change this:
monthly_sales_data = monthly_sales_data.reindex(index = ['Jan','Feb','Mar',
'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec'])

to this:
monthly_sales_data = monthly_sales_data.reindex(index = [1,2,3,4,5,6,7,8,9,10,11,12])

",pandas
python beautifulsoup scraping yield data,"I am trying to scrape Yield tables for several countries and several maturities from a website.
So far I only get empty tables:

while it should rather look like:

So far I have been doing the following:
import time 
import datetime as dt
import pandas as pd
from bs4 import BeautifulSoup
from dateutil.relativedelta import relativedelta
import requests
import re 
import os

path = os.getcwd()

def ZCCWord(Date,country): 

    # Site URL
    url=""http://www.worldgovernmentbonds.com/country/""+country

    html_content = requests.get(url).text
    soup = BeautifulSoup(html_content, ""lxml"")
    #gdp = soup.find_all(""table"", attrs={""class"": ""w3-table w3-white table-padding-custom w3 small font-family-arial table-valign-middle""})
    gdp = soup.find_all(""table"") # , attrs={""class"": ""w3-table money pd44 -f15""})
    table1 = gdp[0]
    body = table1.find_all(""tr"")
    body_rows = body[1:] 
    all_rows = [] # will be a list for list for all rows
    for row_num in range(len(body_rows)): # A row at a time
        row = [] # this will old entries for one row
        for row_item in body_rows[row_num].find_all(""td""): #loop through all row entries
            aa = re.sub(""(\xa0)|(\n)|,"","""",row_item.text)
            #append aa to row - note one row entry is being appended
            row.append(aa)
        # append one row to all_rows
        all_rows.append(row)

    AAA           = pd.DataFrame(all_rows)
    ZCC           = pd.DataFrame()
    ZCC           = AAA[1].str.extract('([^a-zA-Z]+)([a-zA-Z]+)', expand=True).dropna().reset_index(drop=True)
    ZCC.columns   = ['TENOR', 'PERIOD'] 
    ZCC['TENOR'] = ZCC['TENOR'].str.strip().str.isdigit()  # Remove leading/trailing spaces
    #ZCC = ZCC[ZCC['TENOR'].str.isdigit()] 
    ZCC['TENOR']  = ZCC['TENOR'].astype(int)
    ZCC['RATES']  = AAA[2].str.extract(r'([0-9.]+)', expand=True).dropna().reset_index(drop=True).astype(float)
    ZCC['RATES']  = ZCC['RATES']/100

    row2      = []
    for i in range(len(ZCC)): 
        if ZCC['PERIOD'][i]=='month' or  ZCC['PERIOD'][i]=='months':
            b  = ZCC['TENOR'][i]
            bb = Date + relativedelta(months = b)
            row2.append(bb)
        else: 
            b  = ZCC['TENOR'][i]
            bb = Date + relativedelta(years = b)
            row2.append(bb)

    ZCC['DATES'] = pd.DataFrame(row2)
    ZCC = ZCC.reindex(['TENOR','PERIOD','DATES','RATES'], axis=1)
    return ZCC



LitsCountries   =  ['spain','portugal','latvia','ireland','united-kingdom',
                'germany', 'france','italy','sweden','finland','greece',
                'poland','romania','hungary','netherlands']

todays_date     = path+'\\WorldYields' +str(dt.datetime.now().strftime(""%Y-%m-%d-%H-%M"") )+ '.xlsx'   
writer          = pd.ExcelWriter(todays_date, engine='xlsxwriter',engine_kwargs={'options':{'strings_to_urls': False}})
dictYield       = {}

for i in range(len(LitsCountries)): 
        country         = LitsCountries[i]
        Date            = pd.to_datetime('today').date()
        country         = LitsCountries[i] 
        ZCC             = ZCCWord(Date,country)  
        dictYield[i]    = ZCC
        ZCC.to_excel(writer, sheet_name=country)     
       
writer.close()    
time.sleep(60) # wait one minute

I would be fine also with other websites, solutions or methods which provide similar outputs.
Any idea?
thanks in advance!
","To get the table data you need to use the wp-json endpoint combined with the country id. You can retrieve the country id from the website you are already requesting by finding it in the raw text response.
Next you need to request the wp-json endpoint. There you will recieve a json object including the table html.
def request_table(country_id: str):
    url = ""https://www.worldgovernmentbonds.com/wp-json/country/v1/main""

    payload = {
    ""GLOBALVAR"":
        {
            ""JS_VARIABLE"": ""jsGlobalVars"",
            ""FUNCTION"": ""Country"",
            ""DOMESTIC"": True,
            ""ENDPOINT"": ""https://www.worldgovernmentbonds.com/wp-json/country/v1/historical"",
            ""DATE_RIF"": ""2099-12-31"",
            ""OBJ"": None,
            ""COUNTRY1"":
                {
                    ""SYMBOL"": country_id
                },
            ""COUNTRY2"": None,
            ""OBJ1"": None,
            ""OBJ2"":None
        }
    }
    headers = {
      'accept': '*/*',
      'content-type': 'application/json; charset=UTF-8',
      'origin': 'https://www.worldgovernmentbonds.com',
    }
    response = requests.request(""POST"", url, headers=headers, data=json.dumps(payload))

    data = response.json()
    return data.get(""mainTable"")



def ZCCWord(Date, country):
    # Site URL
    url = ""http://www.worldgovernmentbonds.com/country/"" + country
    html_content = requests.get(url).text

    # extract country id
    start_index = html_content.find(""\""SYMBOL\"":\"""")
    end_index = html_content[start_index + 10:].find(""\"","")
    country_id = html_content[start_index + 10:start_index + 10 + end_index]
    # request table
    table_html = request_table(country_id)

    soup = BeautifulSoup(table_html, ""lxml"")
    # gdp = soup.find_all(""table"", attrs={""class"": ""w3-table w3-white table-padding-custom w3 small font-family-arial table-valign-middle""})
    gdp = soup.find_all(""table"")  # , attrs={""class"": ""w3-table money pd44 -f15""})
    table1 = gdp[0]

    # ... Rest of your code ...

",pandas
panda iterate rows and multiply nth row values to nextn1 row value,"I am trying to iterate multiple column rows and multiply nth row to n+1 row after that add columns.
I tried below code and it's working fine.
Is there any other simply way to achieve the subtraction and multiplication part together?
import pandas as pd

df = pd.DataFrame({'C': [""Spark"",""PySpark"",""Python"",""pandas"",""Java""],
                    'F' : [2,4,3,5,4],
                    'D':[3,4,6,5,5]})

df1 = pd.DataFrame({'C': [""Spark"",""PySpark"",""Python"",""pandas"",""Java""],
                    'F': [1,2,1,2,1],
                    'D':[1,2,2,2,1]})

df = pd.merge(df, df1, on=""C"")

df['F_x-F_y'] = df['F_x'] - df['F_y']
df['D_x-D_y'] = df['D_x'] - df['D_y']

for index, row in df.iterrows():
    df['F_mul'] = df['F_x-F_y'].mul(df['F_x-F_y'].shift())
    df['D_mul'] = df['D_x-D_y'].mul(df['D_x-D_y'].shift())

df['F+D'] = df['F_mul'] + df['D_mul']

Output -
         C  F_x  D_x  F_y  D_y  F_x-F_y  D_x-D_y  F_mul  D_mul   F+D
0    Spark    2    3    1    1        1        2    NaN    NaN   NaN
1  PySpark    4    4    2    2        2        2    2.0    4.0   6.0
2   Python    3    6    1    2        2        4    4.0    8.0  12.0
3   pandas    5    5    2    2        3        3    6.0   12.0  18.0
4     Java    4    5    1    1        3        4    9.0   12.0  21.0

","First remove iterating by iterrows, then is possible simplify a generalize solution by:
cols = ['F','D']

for col in cols:
    s = df[f'{col}_x'].sub(df[f'{col}_y'])
    df[f'{col}_mul'] = s.mul(s.shift())

df['+'.join(cols)] = df.filter(like='mul').sum(axis=1, min_count=1)
print (df)
         C  F_x  D_x  F_y  D_y  F_mul  D_mul   F+D
0    Spark    2    3    1    1    NaN    NaN   NaN
1  PySpark    4    4    2    2    2.0    4.0   6.0
2   Python    3    6    1    2    4.0    8.0  12.0
3   pandas    5    5    2    2    6.0   12.0  18.0
4     Java    4    5    1    1    9.0   12.0  21.0

Another idea is processing all columns together - advantage is dont need specify columns for processing:
df1 = (df.filter(like='x').rename(columns=lambda x: x.replace('x','mul'))
         .sub(df.filter(like='y').rename(columns=lambda x: x.replace('y','mul'))))

df2 = df1.mul(df1.shift())
df = df.join(df2)

df['+'.join(x.replace('_mul','') for x in df2.columns)] = df2.sum(axis=1, min_count=1)
print (df)
         C  F_x  D_x  F_y  D_y  F_mul  D_mul   F+D
0    Spark    2    3    1    1    NaN    NaN   NaN
1  PySpark    4    4    2    2    2.0    4.0   6.0
2   Python    3    6    1    2    4.0    8.0  12.0
3   pandas    5    5    2    2    6.0   12.0  18.0
4     Java    4    5    1    1    9.0   12.0  21.0

",pandas
pandas multiindex dataframe  selecting max from one index within multiindex,"I've got a dataframe with a multi index of Year and Month like the following
     |     |Value
Year |Month|  
     |  1  |  3
1992 |  2  |  5
     |  3  |  8
     | ... | ...
1993 |  1  |  2
     | ... | ...

I'm trying to select the maximum Value for each year and put that in a DF like this: 
     | Max
Year |  
1992 |  5
1993 |  2
     | ... 

There's not much info on multi-indexes, should I simply do a group by and apply or something similar to make it more simple?
","Exactly right:
df.groupby(level=0).apply(max)

In my sample DataFrame:

                     0
Caps Lower            
A    a     0  0.246490
           1 -1.265711
           2 -0.477415
           3 -0.355812
           4 -0.724521
     b     0 -0.409198
           1 -0.062552
           2 -0.731789
           3  1.131616
           4  0.085248
B    a     0  0.193948
           1  2.010710
           2  0.289300
           3  0.305373
           4  1.376965
     b     0  0.210522
           1  1.431279
           2 -0.247171
           3  0.899074
           4  0.639926

Result:

             0
Caps          
A     1.131616
B     2.010710

This is how I created the DataFrame, by the way:
df = pd.DataFrame(np.random.randn(5,4), columns = l)
df.columns = pd.MultiIndex.from_tuples(df.columns, names=['Caps','Lower'])
df = pd.DataFrame(df.unstack())

",pandas
how to transform dataframe that contains list in every row of each column,"I have the following dataframe which is one of the output from for loop.
df = pd.DataFrame()

df['Score'] = [['0-0','1-1','2-2'],['0-0','1-1','2-2']]
df ['value'] =[[0.08,0.1,0.15],[0.07,0.12,0.06]]
df ['Team'] = ['A','B']

I want to transform each element of list of each row to each element of a column. The following is the expected output:

How can I transform it?
","You can try of unstacking index, once after applying pd.Series on each list of dataframe
df = pd.DataFrame()

df['Score'] = [['0-0','1-1','2-2'],['0-0','1-1','2-2']]
df ['value'] =[[0.08,0.1,0.15],[0.07,0.12,0.06]]    

df.stack().apply(pd.Series).ffill(1).unstack(level=0).T.reset_index(drop=True)

Out:
    Score   value   Team
0   0-0     0.08    A
1   0-0     0.07    B
2   1-1     0.1     A
3   1-1     0.12    B
4   2-2     0.15    A
5   2-2     0.06    B

",pandas
pandas check if row exists with certain values,"I have a two dimensional (or more) pandas DataFrame like this:
>>> import pandas as pd
>>> df = pd.DataFrame([[0,1],[2,3],[4,5]], columns=['A', 'B'])
>>> df
   A  B
0  0  1
1  2  3
2  4  5

Now suppose I have a numpy array like np.array([2,3]) and want to check if there is any row in df that matches with the contents of my array. Here the answer should obviously true but eg. np.array([1,2]) should return false as there is no row with both 1 in column A and 2 in column B.
Sure this is easy but don't see it right now.
","Turns out it is really easy, the following does the job here:
>>> ((df['A'] == 2) & (df['B'] == 3)).any()
True
>>> ((df['A'] == 1) & (df['B'] == 2)).any()
False

Maybe somebody comes up with a better solution which allows directly passing in the array and the list of columns to match.
Note that the parenthesis around df['A'] == 2 are not optional since the & operator binds just as strong as the == operator.
",pandas
drop a specific row in pandas,"I tried drop method of pandas but I didn't use it. How do I remove a specific row in pandas with Python?
e.g.: My specific row is => Name: Bertug  Grade: A  Age: 15
","df = pd.DataFrame([['Jhon',15,'A'],['Anna',19,'B'],['Paul',25,'D']])
df. columns = ['Name','Age','Grade']

df
Out[472]: 
   Name  Age Grade
0  Jhon   15     A
1  Anna   19     B
2  Paul   25     D

You can get the index of your row:
i = df[((df.Name == 'jhon') &( df.Age == 15) & (df.Grade == 'A'))].index

and then drop it:
df.drop(i)
Out[474]: 
   Name  Age Grade
1  Anna   19     B
2  Paul   25     D

As @jezrael pointed our, you can also just negate all three:
df[((df.Name != 'jhon') &( df.Age != 15) & (df.Grade != 'A'))]
Out[477]: 
   Name  Age Grade
1  Anna   19     B
2  Paul   25     D

",pandas
how to merge two dataframes based on using the substring of one column,"I'm trying to merge two dataframes based on a column. Ideally I would like to use startswith() as they won't always be exact matches.
df1:
       id    symbol
0      123      CCH
1      456     IAC1
2      789    MTCH1
3      987     CVLG

df2
        id       symbol
0       23434     CCHCP
1       35564    IAC1XP
2       76764     MTCH1
3       87877    CVLGPX
4       98765    CVLGPX
5       13234     CCHCP

and my desired output
         id      symbol    matched_id
0       23434     CCHCP       123
1       35564    IAC1XP       456
2       76764     MTCH1       789
3       87877    CVLGPX       987
4       98765    CVLGPXR      987
5       13234     CCH         123    

So far I have used
df2 = pd.merge(df2, df1, on='symbol')

which works but only when there is a full match and fails on partials.
Any help advice would be much appreciated.
","You can try:
merged_df = (
    df2.assign(
        temp=df2.symbol.str.extract(pat=f""({'|'.join(df1.symbol)})""))
    .merge(
        df1,
        how='left',
        left_on='temp',
        right_on='symbol',
        suffixes=['', '_y'])
).rename(columns={'id_y': 'matched_id'}).drop(['temp', 'symbol_y'], axis=1)

OUTPUT:
      id  symbol  matched_id
0  23434   CCHCP         123
1  35564  IAC1XP         456
2  76764   MTCH1         789
3  87877  CVLGPX         987
4  98765  CVLGPX         987
5  13234   CCHCP         123

",pandas
collapse values from multiple rows of a column into an array when all other columns values are same,"I have a table with 7 columns where for every few rows, 6 columns remain same and only the 7th changes. I would like to merge all these rows into one row, and combine the value of the 7th column into a list.
So if I have this dataframe:
   A  B  C
0  a  1  2
1  b  3  4
2  c  5  6
3  c  7  6

I would like to convert it to this:
   A       B  C
0  a       1  2
1  b       3  4
2  c  [5, 7]  6

Since the values of column A and C were same in row 2 and 3, they would get collapsed into a single row and the values of B will be combined into a list.
Melt, explode, and pivot don't seem to have such functionality. How can achieve this using Pandas?
","Use GroupBy.agg with custom lambda function, last add DataFrame.reindex for same order of columns by original:
f = lambda x: x.tolist() if len(x) > 1 else x
df = df.groupby(['A','C'])['B'].agg(f).reset_index().reindex(df.columns, axis=1)

You can also create columns names dynamic like:
changes = ['B']
cols = df.columns.difference(changes).tolist()

f = lambda x: x.tolist() if len(x) > 1 else x
df = df.groupby(cols)[changes].agg(f).reset_index().reindex(df.columns, axis=1)
print (df)
   A       B  C
0  a       1  2
1  b       3  4
2  c  [5, 7]  6

For all lists in column solution is simplier:
changes = ['B']
cols = df.columns.difference(changes).tolist()

df = df.groupby(cols)[changes].agg(list).reset_index().reindex(df.columns, axis=1)
print (df)
   A       B  C
0  a     [1]  2
1  b     [3]  4
2  c  [5, 7]  6

",pandas
how to mark 2 rows before and after a value change in a specific column,"I want to create a new column 'mark' in the dataframe.
I already set the value of 'mark' on True when a change of value occurs in column 'Numbers'.
I would like to set the values on 'True' two row before and after a value change in column 'Numbers'.
I would like to see:




Numbers
mark




10
False


10
False


10
False


10
False


10
True


10
True


20
True


20
True


20
False


20
False


20
True


20
True


30
True


40
True


40
True


40
False


40
False


40
False




What in found so far:
import pandas as pd
data = [10,10,10,10,10,10,20,20,20,20,20,20,20,30,40,40,40,40,40] 
df = pd.DataFrame(data, columns=['Numbers'])
check=10
def detect_changes (row):
    global check 
    if row['Numbers'] == check :           
        return False
    else :
        check=check+10
        return True
df['mark']=df.apply (lambda row: detect_changes(row), axis=1)

","You can use a rolling max:
# better than the used function
df['mark'] = df['Numbers'].diff().eq(10)

N = 2 # number of rows before/after 
df['mark2'] = df['mark'].rolling(2*N+1, min_periods=1, center=True).max().astype(bool)

output:
    Numbers   mark  mark2
0        10  False  False
1        10  False  False
2        10  False  False
3        10  False  False
4        10  False   True
5        10  False   True
6        20   True   True
7        20  False   True
8        20  False   True
9        20  False  False
10       20  False  False
11       20  False   True
12       20  False   True
13       30   True   True
14       40   True   True
15       40  False   True
16       40  False   True
17       40  False  False
18       40  False  False

1 before, 2 after
# better than the used function
df['mark'] = df['Numbers'].diff().eq(10)

# 1 before
m1 = df.loc[::-1, 'mark'].rolling(1+1, min_periods=1).max().astype(bool)

# 2 after
m2 = df['mark'].rolling(2+1, min_periods=1).max().astype(bool)

df['mark2'] = m1|m2

output:
    Numbers   mark  mark2
0        10  False  False
1        10  False  False
2        10  False  False
3        10  False  False
4        10  False  False
5        10  False   True
6        20   True   True
7        20  False   True
8        20  False   True
9        20  False  False
10       20  False  False
11       20  False   True
12       30   True   True
13       40   True   True
14       40  False   True
15       40  False   True
16       40  False  False
17       40  False  False

",pandas
how can i scrape event links and contact information from a website with python,"I am trying to scrape event links and contact information from the RaceRoster website (https://raceroster.com/search?q=5k&t=upcoming) using Python, requests, Pandas, and BeautifulSoup. The goal is to extract the Event Name, Event URL, Contact Name, and Email Address for each event and save the data into an Excel file so we can reach out to these events for business development purposes.
However, the script consistently reports that no event links are found on the search results page, despite the links being visible when inspecting the HTML in the browser. Here’s the relevant HTML for the event links from the search results page:
<a href=""https://raceroster.com/events/2025/98542/13th-annual-delaware-tech-chocolate-run-5k"" 
   target=""_blank"" 
   rel=""noopener noreferrer"" 
   class=""search-results__card-event-name"">
    13th Annual Delaware Tech Chocolate Run 5k
</a>

Steps Taken:

Verified the correct selector for event links:

soup.select(""a.search-results__card-event-name"")


Checked the response content from the requests.get() call using soup.prettify(). The HTML appears to lack the event links that are visible in the browser, suggesting the content may be loaded dynamically via JavaScript.

Attempted to scrape the data using BeautifulSoup but consistently get:


Found 0 events on the page.
Scraped 0 events.
No contacts were scraped.

What I Need Help With:

How can I handle this JavaScript-loaded content? Is there a way to scrape it directly, or do I need to use a tool like Selenium?
If Selenium is required, how do I properly integrate it with BeautifulSoup for parsing the rendered HTML?

Current Script:
import requests
from bs4 import BeautifulSoup
import pandas as pd

def scrape_event_contacts(base_url, search_url):
    headers = {
        ""User-Agent"": ""Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36""
    }
    event_contacts = []

    # Fetch the main search page
    print(f""Scraping page: {search_url}"")
    response = requests.get(search_url, headers=headers)

    if response.status_code != 200:
        print(f""Failed to fetch page: {search_url}, status code: {response.status_code}"")
        return event_contacts

    soup = BeautifulSoup(response.content, ""html.parser"")
    # Select event links
    event_links = soup.select(""a.search-results__card-event-name"")


    print(f""Found {len(event_links)} events on the page."")

    for link in event_links:
        event_url = link['href']
        event_name = link.text.strip()  # Extract Event Name

        try:
            print(f""Scraping event: {event_url}"")
            event_response = requests.get(event_url, headers=headers)
            if event_response.status_code != 200:
                print(f""Failed to fetch event page: {event_url}, status code: {event_response.status_code}"")
                continue

            event_soup = BeautifulSoup(event_response.content, ""html.parser"")

            # Extract contact name and email
            contact_name = event_soup.find(""dd"", class_=""event-details__contact-list-definition"")
            email = event_soup.find(""a"", href=lambda href: href and ""mailto:"" in href)

            contact_name_text = contact_name.text.strip() if contact_name else ""N/A""
            email_address = email['href'].split(""mailto:"")[1].split(""?"")[0] if email else ""N/A""

            if contact_name or email:
                print(f""Found contact: {contact_name_text}, email: {email_address}"")
                event_contacts.append({
                    ""Event Name"": event_name,
                    ""Event URL"": event_url,
                    ""Event Contact"": contact_name_text,
                    ""Email"": email_address
                })
            else:
                print(f""No contact information found for {event_url}"")
        except Exception as e:
            print(f""Error scraping event {event_url}: {e}"")

    print(f""Scraped {len(event_contacts)} events."")
    return event_contacts

def save_to_spreadsheet(data, output_file):
    if not data:
        print(""No data to save."")
        return
    df = pd.DataFrame(data)
    df.to_excel(output_file, index=False)
    print(f""Data saved to {output_file}"")

if __name__ == ""__main__"":
    base_url = ""https://raceroster.com""
    search_url = ""https://raceroster.com/search?q=5k&t=upcoming""
    output_file = ""/Users/my_name/Documents/event_contacts.xlsx""

    contact_data = scrape_event_contacts(base_url, search_url)
    if contact_data:
        save_to_spreadsheet(contact_data, output_file)
    else:
        print(""No contacts were scraped."")

Expected Outcome:

Extract all event links from the search results page.
Navigate to each event’s detail page.
Scrape the contact name () and email () from the detail page.
Save the results to an Excel file.

","Use the API endpoint to get the data on upcoming events.
Here's how:
import requests
from tabulate import tabulate
import pandas as pd

url = 'https://search.raceroster.com/search?q=5k&t=upcoming'

headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/103.0.0.0 Safari/537.36',
}

events = requests.get(url,headers=headers).json()['data']

loc_keys = [""address"", ""city"", ""country""]

table = [
    [
        event[""name""],
        event[""url""],
        "" "".join([event[""location""][key] for key in loc_keys if key in event[""location""]])
    ] for event in events
]

columns = [""Name"", ""URL"", ""Location""]
print(tabulate(table, headers=columns))

df = pd.DataFrame(table, columns=columns)
df.to_csv('5k_events.csv', index=False, header=True)

This should print:
Name                                         URL                                                                                         Location
-------------------------------------------  ------------------------------------------------------------------------------------------  ----------------------------------------------------------------------------------------------------------------------------
Credit Union Cherry Blossom                  https://raceroster.com/events/2025/72646/credit-union-cherry-blossom                        Washington, D.C. Washington United States
Big Cork Wine Run 5k                         https://raceroster.com/events/2025/98998/big-cork-wine-run-5k                               Big Cork Vineyards, 4236 Main Street, Rohrersville, MD 21779, U.S. Rohrersville United States
3rd Annual #OptOutside Black Friday Fun Run  https://raceroster.com/events/2025/98146/3rd-annual-number-optoutside-black-friday-fun-run  Grain H2O, Summit Harbour Place, Bear, DE, USA Bear United States
Ryan's Race 5K walk Run                      https://raceroster.com/events/2025/97852/ryans-race-5k-walk-run                             Odessa High School, Tony Marchio Drive, Townsend, DE Townsend United States
13th Annual Delaware  Tech Chocolate Run 5k  https://raceroster.com/events/2025/98542/13th-annual-delaware-tech-chocolate-run-5k         Delaware Technical Community College - Charles L. Terry Jr. Campus - Dover, Campus Drive, Dover, DE, USA Dover United States
Builders Dash 5k                             https://raceroster.com/events/2025/99146/builders-dash-5k                                   Rail Haus - Beer Garden, North West Street, Dover, DE Dover United States
The Ivy Scholarship 5k                       https://raceroster.com/events/2025/96874/the-ivy-scholarship-5k                             Hare Pavilion, River Place, Wilmington, DE Wilmington United States
39th Firecracker 5k Run Walk                 https://raceroster.com/events/2025/96907/39th-firecracker-5k-run-walk                       Rockford Tower, Lookout Drive, Wilmington, DE Wilmington United States
24th Annual John D Kelly Logan House 5k      https://raceroster.com/events/2025/97364/24th-annual-john-d-kelly-logan-house-5k            Kelly's Logan House, Delaware Avenue, Wilmington, DE, USA Wilmington United States
2nd Annual Scott Trot 5K                     https://raceroster.com/events/2025/96904/2nd-annual-scott-trot-5k                           American Legion Post 17, American Legion Road, Lewes, DE Lewes United States

Bonus:

 To get more events data, just paginate the API with these parameters: l=10&p=1. For example, https://search.raceroster.com/search?q=5k&l=10&p=1&t=upcoming Also, note there's a field in meta -> hits that holds the number of found events. For your query that's 1465.

",pandas
why does pandas return timestamps instead of datetime objects when calling pdto_datetime,"According to the manual, pd.to_datetime() should create a datetime object.
Instead, when I call pd.to_datetime(""2012-05-14""), I get a timestamp object! Calling to_datetime() on that object finally gives me a datetime object.
In [1]: pd.to_datetime(""2012-05-14"")
Out[1]: Timestamp('2012-05-14 00:00:00', tz=None)

In [2]: t = pd.to_datetime(""2012-05-14"")
In [3]: t.to_datetime()
Out[2]: datetime.datetime(2012, 5, 14, 0, 0)

Is there an explanation for this unexpected behaviour?
","A Timestamp object is the way pandas works with datetimes, so it is a datetime object in pandas. But you expected a datetime.datetime object.
Normally you should not care about this (it is just a matter of a different repr). As long as you are working with pandas, the Timestamp is OK. And even if you really want a datetime.datetime, most things will work (eg all methods), and otherwise you can use to_pydatetime to retrieve the datetime.datetime object.
The longer story:

pandas stores datetimes as data with type datetime64 in index/columns (this are not datetime.datetime objects). This is the standard numpy type for datetimes and is more performant than using datetime.datetime objects:
 In [15]: df = pd.DataFrame({'A':[dt.datetime(2012,1,1), dt.datetime(2012,1,2)]})

 In [16]: df.dtypes
 Out[16]:
 A    datetime64[ns]
 dtype: object

 In [17]: df.loc[0,'A']
 Out[17]: Timestamp('2012-01-01 00:00:00', tz=None)

when retrieving one value of such a datetime column/index, you will see a Timestamp object. This is a more convenient object to work with the datetimes (more methods, better representation, etc than the datetime64), and this is a subclass of datetime.datetime, and so has all methods of it.

",pandas
disable scientific notation and offset in pandas plot function,"I have a dataframe df with 2 columns I want to plot together and days as index:
           | col1  | col2   | col3 | ...
2020-01-01 | 1     | 300000 | ...
2020-01-02 | 1000  | 600000 | ...
2020-01-03 | 3000  | 50000  | ...

Plotting col1 + col2 via
df[[""col1"", ""col2""]].plot()

shows values from 0 till 1.0 and at the top ""1e6"" like in this example: https://i.sstatic.net/tJjgX.png
I want the full value range on the y-axis and no scientific notation. How can I do this via pandas .plot() or matplotlib?
","You have several options:
Option One: With Matplotlib
axes=fig.add_axes([0,0,1,1])
axes.set_xticks() # with list or range() inside
axes.set_yticks() # with list or range() inside

#You can also label the ticks with your desired values
axes.set_xticklabels() # with list or range() inside
axes.set_yticklabels() # with list or range() inside

Option Two: Change Pandas settings
pd.set_option('display.float_format', lambda x: '%.3f' % x)

or
pd.options.display.float_format = '{:.2f}'.format

I believe option one is better since you need it only for the graph and don't necessarily want to modify your dataframe columns.
Cheers!
",pandas
how to convert unix time to datetime when using pandasdataframefrom_dict,"I am reading from a json data file and loading it into a dictionary. It as key:value like below.
""1707195600000"":1,""1707282000000"":18,""1707368400000"":1,""1707454800000"":13,""1707714000000"":18,""1707800400000"":12,""1707886800000"":155,""1707973200000"":1""

Code Snippet:
with open('data.json', 'r') as json_file:
    data_pairs = json.load(json_file)

dataframe = pd.DataFrame.from_dict(data_pairs, orient='index')

Can it be done with Pandas.DataFrame.from_dict? Or I should convert it all the keys in the dictionary before using from_dict?
","You can convert the keys first.  They seem to be in milliseconds:
import pandas as pd
import datetime as dt

data = {""1707195600000"":1,""1707282000000"":18,""1707368400000"":1,""1707454800000"":13,""1707714000000"":18,""1707800400000"":12,""1707886800000"":155,""1707973200000"":1}
cvt_data = {dt.datetime.fromtimestamp(int(k) / 1000):v for k, v in data.items()}
df = pd.DataFrame.from_dict(cvt_data, orient='index')
print(df)

Output:
                       0
2024-02-05 21:00:00    1
2024-02-06 21:00:00   18
2024-02-07 21:00:00    1
2024-02-08 21:00:00   13
2024-02-11 21:00:00   18
2024-02-12 21:00:00   12
2024-02-13 21:00:00  155
2024-02-14 21:00:00    1

The above converts the timestamps to time zone-unaware local time which was Pacific Standard Time(UTC-8) in my case.  Below converts to to time zone-aware datetimes in UTC.  Note the +00:00 indicating the zone offset in the output:
cvt_data = {dt.datetime.fromtimestamp(int(k) / 1000, tz=dt.UTC):v for k, v in data.items()}

It is much faster to use pd.to_datetime (about 6-7x in my testing), and defaults to a tz-unaware UTC conversion for this data.  Use utc=True to make it tz-aware:
df = pd.DataFrame.from_dict(data, orient='index')
df.index = pd.to_datetime(df.index.astype(dtype='int64'), utc=True, unit='ms')

Output (both UTC versions):
                             0
2024-02-06 05:00:00+00:00    1
2024-02-07 05:00:00+00:00   18
2024-02-08 05:00:00+00:00    1
2024-02-09 05:00:00+00:00   13
2024-02-12 05:00:00+00:00   18
2024-02-13 05:00:00+00:00   12
2024-02-14 05:00:00+00:00  155
2024-02-15 05:00:00+00:00    1


",pandas
pandas grouped list aggregation using transform fails with key error,"How can I apply a list function to a grouped pandas dataframe which is not aggregated using apply but rather transform?
For me the following fails with: KeyError: ""None of [Index(['v1', 'v2'], dtype='object')] are in the [index]""
import pandas as pd
df = pd.DataFrame({'key':[1,1,1,2,3,2], 'v1': [1,4,6,7,4,9], 'v2':[0.3, 0.6, 0.4, .1, .2, .8]})
display(df)

def list_function(x):
    #display(x)
    all_values = x[['v1','v2']].drop_duplicates()
    #display(all_values)
    #result = all_values.to_json()
    result = all_values.values
    return result


display(df.groupby(['key']).apply(list_function))
df['list_result'] = df.groupby(['key']).transform(list_function)
df


NOTICE: I know that a join would be possible with the aggregated data, but in this particular case I would prefer not having to do the JOIN afterwards.

","It is not possible, in pandas GroupBy.transform and also GroupBy.agg working with each column separately, so cannot select by multiple columns names like you need.
It is possible only by GroupBy.apply.
So instead transform is possible use Series.map if match one column, for multiple column use DataFrame.join:
df['list_result'] = df['key'].map(df.groupby(['key']).apply(list_function))
print (df)

   key  v1   v2                           list_result
0    1   1  0.3  [[1.0, 0.3], [4.0, 0.6], [6.0, 0.4]]
1    1   4  0.6  [[1.0, 0.3], [4.0, 0.6], [6.0, 0.4]]
2    1   6  0.4  [[1.0, 0.3], [4.0, 0.6], [6.0, 0.4]]
3    2   7  0.1              [[7.0, 0.1], [9.0, 0.8]]
4    3   4  0.2                          [[4.0, 0.2]]
5    2   9  0.8              [[7.0, 0.1], [9.0, 0.8]]


#added one column for match by 2 columns sample
df['new'] = 1

s = df.groupby(['key', 'new']).apply(list_function)
df = df.join(s.rename('list_result'), on=['key','new'])
print (df)
   key  v1   v2  new                           list_result
0    1   1  0.3    1  [[1.0, 0.3], [4.0, 0.6], [6.0, 0.4]]
1    1   4  0.6    1  [[1.0, 0.3], [4.0, 0.6], [6.0, 0.4]]
2    1   6  0.4    1  [[1.0, 0.3], [4.0, 0.6], [6.0, 0.4]]
3    2   7  0.1    1              [[7.0, 0.1], [9.0, 0.8]]
4    3   4  0.2    1                          [[4.0, 0.2]]
5    2   9  0.8    1              [[7.0, 0.1], [9.0, 0.8]]

",pandas
pandas value_counts with a little tolerance,"I have a set of data and want to get its value_counts result:
df = pd.DataFrame(
    [5.01, 5.01, 5.08, 6.1, 5.54, 6.3, 5.56, 5.55, 6.7],
    columns=['val'])

>>> df
    val
0  5.01
1  5.01
2  5.08
3  6.10
4  5.54
5  6.30
6  5.56
7  5.55
8  6.70
>>> df.val.value_counts()
5.01    2
5.08    1
6.10    1
5.54    1
6.30    1
5.56    1
5.55    1
6.70    1
Name: val, dtype: int64

Is there a way to allow a certain tolerance when using value_counts, such as plus or minus 0.01, so that 5.54, 5.55, and 5.56 in the series are calculated as a group? The result I hope is:
[5.54,5.56,5.55] 3
[5.01] 2
[5.08] 1
[6.10] 1
...

","try this:
mask = df['val'].sort_values().diff().gt(0.01)
result_df = df.groupby(mask.cumsum())['val'].agg([set, 'count'])
print(result_df)

                    set  count
val                           
0                {5.01}      2
1                {5.08}      1
2    {5.55, 5.54, 5.56}      3
3                 {6.1}      1
4                 {6.3}      1
5                 {6.7}      1

",pandas
how do i subdividerefine a dimension in an xarray dataset,"Summary: I have a dataset that is collected in such a way that the dimensions are not initially available. I would like to take what is essentially a big block of undifferentiated data and add dimensions to it so that it can be queried, subsetted, etc. That is the core of the following question.
Here is an xarray DataSet that I have:
<xarray.Dataset>
Dimensions:  (chain: 1, draw: 2000, rows: 24000)
Coordinates:
  * chain    (chain) int64 0
  * draw     (draw) int64 0 1 2 3 4 5 6 7 ... 1993 1994 1995 1996 1997 1998 1999
  * rows     (rows) int64 0 1 2 3 4 5 6 ... 23994 23995 23996 23997 23998 23999
Data variables:
    obs      (chain, draw, rows) float64 4.304 3.985 4.612 ... 6.343 5.538 6.475
Attributes:
    created_at:                 2019-12-27T17:16:13.847972
    inference_library:          pymc3
    inference_library_version:  3.8

The rows dimension here corresponds to a number of subdimensions that I need to restore to the data.  In particular, the 24,000 rows correspond to 100 samples each from 240 conditions (these 100 samples are in contiguous blocks).  These conditions are combinations of gate, input, growth medium, and od.
I would like to end up with something like this:
<xarray.Dataset>
Dimensions:  (chain: 1, draw: 2000, gate: 1, input: 4, growth_medium: 3, sample: 100, rows: 24000)
Coordinates:
  * chain    (chain) int64 0
  * draw     (draw) int64 0 1 2 3 4 5 6 7 ... 1993 1994 1995 1996 1997 1998 1999
  * rows     *MultiIndex*
  * gate     (gate) int64 'AND'
  * input    (input) int64 '00', '01', '10', '11'
  * growth_medium (growth_medium) 'standard', 'rich', 'slow'
  * sample   (sample) int64 0 1 2 3 4 5 6 7 ... 95 96 97 98 99
Data variables:
    obs      (chain, draw, gate, input, growth_medium, samples) float64 4.304 3.985 4.612 ... 6.343 5.538 6.475
Attributes:
    created_at:                 2019-12-27T17:16:13.847972
    inference_library:          pymc3
    inference_library_version:  3.8

I have a pandas dataframe that specifies the values of gate, input, and growth medium -- each row gives a set of values of gate, input, and growth medium, and an index that specifies where (in the rows) the corresponding set of 100 samples appears.  The intent is that this data frame is a guide for labeling the Dataset.
I looked at the xarray docs on ""Reshaping and Reorganizing Data"", but I don't see how to combine those operations to do what I need.  I suspect somehow I need to combine these with GroupBy, but I don't get how. Thanks!
Later: I have a solution to this problem, but it is so disgusting that I am hoping someone will explain how wrong I am, and what a more elegant approach is possible.
So, first, I extracted all the data in the original Dataset into raw numpy form:
foo = qm.idata.posterior_predictive['obs'].squeeze('chain').values.T
foo.shape # (24000, 2000)

Then I reshaped it as needed:
bar = np.reshape(foo, (240, 100, 2000))

This gives me roughly the shape I want: there are 240 different experimental conditions, each has 100 variants, and for each of these variants, I have 2000 Monte Carlo samples in my data set.
Now, I extract the information about the 240 experimental conditions from the Pandas DataFrame:
import pandas as pd
# qdf is the original dataframe with the experimental conditions and some
# extraneous information in other columns
new_df = qdf[['gate', 'input', 'output', 'media', 'od_lb', 'od_ub', 'temperature']]
idx = pd.MultiIndex.from_frame(new_df)

Finally, I reassembled a DataArray from the numpy array and the pandas MultiIndex:
xr.DataArray(bar, name='obs', dims=['regions', 'conditions', 'draws'],
             coords={'regions': idx, 'conditions': range(100), 'draws': range(2000)})

The resulting DataArray has these coordinates, as I wished:
Coordinates:
  * regions      (regions) MultiIndex
  - gate         (regions) object 'AND' 'AND' 'AND' 'AND' ... 'AND' 'AND' 'AND'
  - input        (regions) object '00' '10' '10' '10' ... '01' '01' '11' '11'
  - output       (regions) object '0' '0' '0' '0' '0' ... '0' '0' '0' '1' '1'
  - media        (regions) object 'standard_media' ... 'high_osm_media_five_percent'
  - od_lb        (regions) float64 0.0 0.001 0.001 ... 0.0001 0.0051 0.0051
  - od_ub        (regions) float64 0.0001 0.0051 0.0051 2.0 ... 0.0003 2.0 2.0
  - temperature  (regions) int64 30 30 37 30 37 30 37 ... 37 30 37 30 37 30 37
  * conditions   (conditions) int64 0 1 2 3 4 5 6 7 ... 92 93 94 95 96 97 98 99
  * draws        (draws) int64 0 1 2 3 4 5 6 ... 1994 1995 1996 1997 1998 1999

That was pretty horrible, though, and it seems wrong that I had to punch through all the nice layers of xarray abstraction to get to this point.  Especially since this does not seem like an unusual piece of a scientific workflow: getting a relatively raw data set together with a spreadsheet of metadata that needs to be combined with the data.  So what am I doing wrong? What's the more elegant solution?
","Given the starting Dataset, similar to:
<xarray.Dataset>
Dimensions:  (draw: 2, row: 24)
Coordinates:
  * draw     (draw) int32 0 1
  * row      (row) int32 0 1 2 3 4 5 6 7 8 9 ... 14 15 16 17 18 19 20 21 22 23
Data variables:
    obs      (draw, row) int32 0 1 2 3 4 5 6 7 8 ... 39 40 41 42 43 44 45 46 47

You can concatenate several pure xarray commands to subdivide the dimensions (get the data in the same shape but using a multiindex) or even reshape the Dataset. To subdivide the dimensions, the following code can be used:
multiindex_ds = ds.assign_coords(
    dim_0=[""a"", ""b"", ""c""], dim_1=[0,1], dim_2=range(4)
).stack(
    stacked_dim=(""dim_0"", ""dim_1"", ""dim_2"")
).reset_index(
    ""row"", drop=True
).rename(
    row=""stacked_dim""
)
multiindex_ds

whose output is:
<xarray.Dataset>
Dimensions:  (stacked_dim: 24, draw: 2)
Coordinates:
  * draw     (draw) int32 0 1
  * stacked_dim      (stacked_dim) MultiIndex
  - dim_0    (stacked_dim) object 'a' 'a' 'a' 'a' 'a' 'a' ... 'c' 'c' 'c' 'c' 'c' 'c'
  - dim_1    (stacked_dim) int64 0 0 0 0 1 1 1 1 0 0 0 0 1 1 1 1 0 0 0 0 1 1 1 1
  - dim_2    (stacked_dim) int64 0 1 2 3 0 1 2 3 0 1 2 3 0 1 2 3 0 1 2 3 0 1 2 3
Data variables:
    obs      (draw, stacked_dim) int32 0 1 2 3 4 5 6 7 8 ... 39 40 41 42 43 44 45 46 47

Moreover, the multiindex can then be unstacked, effectively reshaping the Dataset:
reshaped_ds = multiindex_ds.unstack(""stacked_dim"")
reshaped_ds

with output:
<xarray.Dataset>
Dimensions:  (dim_0: 3, dim_1: 2, dim_2: 4, draw: 2)
Coordinates:
  * draw     (draw) int32 0 1
  * dim_0    (dim_0) object 'a' 'b' 'c'
  * dim_1    (dim_1) int64 0 1
  * dim_2    (dim_2) int64 0 1 2 3
Data variables:
    obs      (draw, dim_0, dim_1, dim_2) int32 0 1 2 3 4 5 ... 42 43 44 45 46 47

I think that this alone does not completely cover your needs because you want to convert a dimension into two dimensions, one of which is to be a multiindex. All the building blocks are here though.
For example, you can follow this steps (including unstacking) with regions and conditions and then follow this steps (no unstacking now) to convert regions to multiindex. Another option would be to use all dimensions from the start, unstack them and then stack them again leaving conditions outside of the final multiindex.

Detailed answer
The answer combines several quite unrelated commands, and it might be tricky to see what each of them is doing.
assign_coords
The first step is to create new dimensions and coordinates and add them to the Dataset. This is necessary because the next methods need the dimensions and coordinates to already be present in the Dataset.
Stopping right after assign_coords yields the following Dataset:
<xarray.Dataset>
Dimensions:  (dim_0: 3, dim_1: 2, dim_2: 4, draw: 2, row: 24)
Coordinates:
  * draw     (draw) int32 0 1
  * row      (row) int32 0 1 2 3 4 5 6 7 8 9 ... 14 15 16 17 18 19 20 21 22 23
  * dim_0    (dim_0) <U1 'a' 'b' 'c'
  * dim_1    (dim_1) int32 0 1
  * dim_2    (dim_2) int32 0 1 2 3
Data variables:
    obs      (draw, row) int32 0 1 2 3 4 5 6 7 8 ... 39 40 41 42 43 44 45 46 47

stack
The Dataset now contains 3 dimensions that add up to 24 elements, however, as the data is currently flat with respect to these 24 elements, we have to stack them into a single 24 element multiindex to make their shapes compatible.
I find the assign_coords followed by stack the most natural solution, however, another possibility would be to generate a multiindex similarly to how it is done above and directly call assign_coords with the multiindex, rendering the stack unnecessary.
This step combines all 3 new dimensions into a single one:
<xarray.Dataset>
Dimensions:  (stacked_dim: 24, draw: 2, row: 24)
Coordinates:
  * draw     (draw) int32 0 1
  * row      (row) int32 0 1 2 3 4 5 6 7 8 9 ... 14 15 16 17 18 19 20 21 22 23
  * stacked_dim      (stacked_dim) MultiIndex
  - dim_0    (stacked_dim) object 'a' 'a' 'a' 'a' 'a' 'a' ... 'c' 'c' 'c' 'c' 'c' 'c'
  - dim_1    (stacked_dim) int64 0 0 0 0 1 1 1 1 0 0 0 0 1 1 1 1 0 0 0 0 1 1 1 1
  - dim_2    (stacked_dim) int64 0 1 2 3 0 1 2 3 0 1 2 3 0 1 2 3 0 1 2 3 0 1 2 3
Data variables:
    obs      (draw, row) int32 0 1 2 3 4 5 6 7 8 ... 39 40 41 42 43 44 45 46 47

Note that as desired now we have 2 dimensions with size 24 as desired.
reset_index
Now we have our final dimension present in the Dataset as a coordinate, and we want this new coordinate to be the one used to index the variable obs. set_index seems like the correct choice, however, each of our coordinates indexes itself (unlike the example in set_index docs where x indexes both x and a coordinates) which means that set_index cannot be used in this particular case. The method to use is reset_index to remove the coordinate row without removing the dimension row.
In the following output it can be seen how now row is a dimension without coordinates:
<xarray.Dataset>
Dimensions:  (stacked_dim: 24, draw: 2, row: 24)
Coordinates:
  * draw     (draw) int32 0 1
  * stacked_dim      (stacked_dim) MultiIndex
  - dim_0    (stacked_dim) object 'a' 'a' 'a' 'a' 'a' 'a' ... 'c' 'c' 'c' 'c' 'c' 'c'
  - dim_1    (stacked_dim) int64 0 0 0 0 1 1 1 1 0 0 0 0 1 1 1 1 0 0 0 0 1 1 1 1
  - dim_2    (stacked_dim) int64 0 1 2 3 0 1 2 3 0 1 2 3 0 1 2 3 0 1 2 3 0 1 2 3
Dimensions without coordinates: row
Data variables:
    obs      (draw, row) int32 0 1 2 3 4 5 6 7 8 ... 39 40 41 42 43 44 45 46 47

rename
The current Dataset is nearly the final one, the only issue is that the obs variable still has the row dimension instead of the desired one: stacked_dim. It does not really look like this is intended usage of rename but it can be used to get stacked_dim to absorb row, yielding the desired final result (called multiindex_ds above).
Here again, set_index seems to be the method to choose, however, if instead of rename(row=""stacked_dim""), set_index(row=""stacked_dim"") is used, the multiindex is collapsed into an index made of tuples:
<xarray.Dataset>
Dimensions:  (draw: 2, row: 24)
Coordinates:
  * draw     (draw) int32 0 1
  * row      (row) object ('a', 0, 0) ('a', 0, 1) ... ('c', 1, 2) ('c', 1, 3)
Data variables:
    obs      (draw, row) int32 0 1 2 3 4 5 6 7 8 ... 39 40 41 42 43 44 45 46 47

",pandas
parsing multiindex pandas data frame for tuple list appendage,"Problem/Task: create a function that inputs a pandas data frame represented by the markdown in Fig 1 and converts/outputs it to a list with the structure represented in Fig 2.
I look forward to any feedback/support anyone might have!
Fig 1: Pandas Data Frame (Function Input) as Markdown



resources
('Widget A (idx = 0)', 't1')
('Widget A (idx = 0)', 't2')
('Widget A (idx = 0)', 't3')
('Widget A (idx = 0)', 't4')
('Widget A (idx = 0)', 't5')
('Widget A (idx = 0)', 't6')
('Widget A (idx = 0)', 't7')
('Widget A (idx = 0)', 't8')
('Widget A (idx = 0)', 't9')
('Widget A (idx = 0)', 't10')
('Widget A (idx = 0)', 't11')
('Widget A (idx = 0)', 't12')
('Widget A (idx = 0)', 't13')
('Widget A (idx = 0)', 't14')
('Widget A (idx = 0)', 't15')
('Widget B (idx = 1)', 't1')
('Widget B (idx = 1)', 't2')
('Widget B (idx = 1)', 't3')
('Widget B (idx = 1)', 't4')
('Widget B (idx = 1)', 't5')
('Widget B (idx = 1)', 't6')
('Widget B (idx = 1)', 't7')
('Widget B (idx = 1)', 't8')
('Widget B (idx = 1)', 't9')
('Widget B (idx = 1)', 't10')
('Widget B (idx = 1)', 't11')
('Widget B (idx = 1)', 't12')
('Widget B (idx = 1)', 't13')
('Widget B (idx = 1)', 't14')
('Widget B (idx = 1)', 't15')
('Widget C (idx =2)', 't1')
('Widget C (idx =2)', 't2')
('Widget C (idx =2)', 't3')
('Widget C (idx =2)', 't4')
('Widget C (idx =2)', 't5')
('Widget C (idx =2)', 't6')
('Widget C (idx =2)', 't7')
('Widget C (idx =2)', 't8')
('Widget C (idx =2)', 't9')
('Widget C (idx =2)', 't10')
('Widget C (idx =2)', 't11')




m_1
10
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
23
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
17
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan


m_2
nan
nan
15
nan
nan
nan
17
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
30
nan
nan
nan
23
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
24
nan
nan
nan
nan
nan
nan
nan
nan


m_3
nan
nan
23
nan
nan
nan
15
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
26
nan
nan
nan
21
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
22
nan
nan
nan
nan
nan
nan
nan
nan


m_4
nan
nan
27
nan
nan
nan
19
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
22
nan
nan
nan
18
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
29
nan
nan
nan
nan
nan
nan
nan
nan


m_5
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
15
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
21
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
23
nan
nan
nan
nan


m_6
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
16
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
16
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
25
nan
nan
nan
nan


m_7
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
23
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
14
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
30
nan
nan
nan
nan


m_8
nan
nan
nan
nan
10
nan
nan
nan
10
nan
nan
nan
10
nan
nan
nan
nan
nan
nan
15
nan
nan
nan
15
nan
nan
nan
15
nan
nan
nan
nan
nan
nan
13
nan
nan
nan
13
nan
nan


m_9
nan
nan
nan
nan
10
nan
nan
nan
10
nan
nan
nan
10
nan
nan
nan
nan
nan
nan
15
nan
nan
nan
15
nan
nan
nan
15
nan
nan
nan
nan
nan
nan
13
nan
nan
nan
13
nan
nan


m_10
nan
nan
nan
nan
10
nan
nan
nan
10
nan
nan
nan
10
nan
nan
nan
nan
nan
nan
15
nan
nan
nan
15
nan
nan
nan
15
nan
nan
nan
nan
nan
nan
13
nan
nan
nan
13
nan
nan


m_11
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
14
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
12
nan
nan
nan
nan
nan
nan
nan
nan
nan
nan
10


m_12
nan
1
nan
1
nan
1
nan
1
nan
1
nan
1
nan
1
nan
nan
1
nan
1
nan
1
nan
1
nan
1
nan
1
nan
1
nan
nan
1
nan
1
nan
1
nan
1
nan
1
nan



Fig 2: Example of Target Data Structure (Function Output) for List
`
components = [ 

# widget A -> [task_0...task_i] -> [(machine_id_0, dur_0)...machine_id_i, dur_i]
[   
    [(1, 10)], #t1
    [(12, 1)], #t2
    [(2, 15), (3, 23), (4,27)], #t3
    [(12, 1)], #t4
    [(8,10), (9,10), (10,10)], #t5
    [(12, 1)], #t6
    [(2, 17), (3, 15), (4,19)], #t7
    [(12, 1)], #t8
    [(8,10), (9,10), (10,10)], #t9
    [(12, 1)], #t10
    [(5, 15), (6, 16), (7,23)], #t11
    [(12, 1)], #t12
    [(8,10), (9,10), (10,10)], #t13
    [(12, 1)], #t14
    [(11,14)], #t15

],

# widget B -> [task_0...task_i] -> [(machine_id_0, dur_0)...machine_id_i, dur_i]
[   
    [(1, 23)], #t1
    [(12, 1)], #t2
    [(2, 30), (3, 26), (4,22)], #t2
    [(12, 1)], #t2
    [(8,15), (9,15), (10,15)], #t3
    [(12, 1)], #t2
    [(2, 23), (3, 21), (4,18)], #t4
    [(12, 1)], #t2
    [(8,15), (9,15), (10,15)], #t5
    [(12, 1)], #t2
    [(5, 21), (6, 16), (7,14)], #t6
    [(12, 1)], #t2
    [(8,15), (9,15), (10,15)], #t7
    [(12, 1)], #t2
    [(11,12)], #t8

],

# widget C -> [task_0...task_i] -> [(machine_id_0, dur_0)...machine_id_i, dur_i]
[   
    [(1, 17)], #t1
    [(12, 1)], #t2
    [(2, 24), (3, 22), (4,29)], #t3
    [(12, 1)], #t4
    [(8,13), (9,13), (10,13)], #t5
    [(12, 1)], #t6
    [(2, 23), (3, 25), (4,30)], #t7
    [(12, 1)], #t8
    [(8,13), (9,13), (10,13)], #t9
    [(12, 1)], #t10
    [(11,10)], #t11

],] 

`
","Here's one approach:
Minimal Reproducible Example
import pandas as pd
import numpy as np

data = [[1, np.nan, np.nan],
        [np.nan, 2, 2],
        [np.nan, 3, np.nan]]
        
m_idx = pd.MultiIndex.from_tuples(
    [('A', 't1'),
     ('A', 't2'),
     ('B', 't1')]
    )

idx = pd.Index([f'm_{i}' for i in range(1, 4)], name='resources')

df = pd.DataFrame(data, columns=m_idx, index=idx)

             A         B
            t1   t2   t1
resources               
m_1        1.0  NaN  NaN
m_2        NaN  2.0  2.0
m_3        NaN  3.0  NaN

Desired output
components = [
    [ # A
        [(1, 1)], # t1
        [(2, 2), (3, 3)] # t2
    ],
    [ # B
        [(2, 2)] # t1
    ]
]

Code
components = (
    df.reset_index()
    .melt([('resources','')])
    .dropna(subset='value')
    .assign(
        tmp=lambda x: 
            list(
                zip(
                    x[('resources','')].str.split('_').str[1].astype(int), 
                    x['value'].astype(int))
                )
            )
    .groupby(['variable_0', 'variable_1'], sort=False)['tmp']
    .apply(list)
    .groupby('variable_0', sort=False)
    .apply(list)
    .to_list()
    )

Output:
components

[[[(1, 1)], [(2, 2), (3, 3)]], [[(2, 2)]]]

Explanation / Intermediates

Use df.reset_index to apply df.melt on the previous index (now: ('resources', '')) + df.dropna on 'value' column.

df.reset_index().melt([('resources','')]).dropna(subset='value')

  (resources, ) variable_0 variable_1  value
0           m_1          A         t1    1.0
4           m_2          A         t2    2.0
5           m_3          A         t2    3.0
7           m_2          B         t1    2.0


Use df.assign to add a column ('tmp') as a tuple (list + zip) containing the digits from 'resources' (via Series.str.split + Series.astype) and values from 'value'.

.assign(...)

  (resources, ) variable_0 variable_1  value     tmp
0           m_1          A         t1    1.0  (1, 1)
4           m_2          A         t2    2.0  (2, 2)
5           m_3          A         t2    3.0  (3, 3)
7           m_2          B         t1    2.0  (2, 2)


Now, use df.groupby with the variable columns (original pd.MultiIndex) with sort=False to preserve order, and get 'tmp' as list (groupby.apply).

.groupby(['variable_0', 'variable_1'])['tmp'].apply(list)

variable_0  variable_1
A           t1                    [(1, 1)]
            t2            [(2, 2), (3, 3)]
B           t1                    [(2, 2)]
Name: tmp, dtype: object


Chain another df.groupby, now solely with 'variable_0' (level 0 from original pd.MultIndex) and get list again.

.groupby('variable_0').apply(list)

variable_0
A    [[(1, 1)], [(2, 2), (3, 3)]]
B                      [[(2, 2)]]
Name: tmp, dtype: object


Finally, chain Series.to_list.

",pandas
apply style to pandas table and print with truncation jupyter labnotebook,"Is there a way to apply a style to a pandas table in Jupyter Lab and then print it while keeping the table truncation functionality?
I have large tables where both the columns and the rows are truncated automatically when printed to the output cells. I would like to retain this functionality and still use the formatting capability of a Styler object, e.g., use a formatter to show up to two decimal places.
I would not like to use head(), tail(), or truncate(). I would like keep the three dots that imply that the data shown is not the full table.
I also know that some pandas display options, such as ""chop"", can change decimal formatting, but there are other kinds of formatting that I would like to apply, like formatting the index.
Is this possible?
For example:
import numpy as np
import pandas as pd

n_rows = 100
n_cols = 50
rng = np.random.default_rng()
data = rng.uniform(low=0., high=100., size=(n_rows, n_cols))
df = pd.DataFrame(data)
df

I would like it to be truncated like this:

– while applying a formatter like this:
df.style.format(formatter='{:.2f}')


","I was able to come up with my own answer. The code is below.
Notes:

This only applies to number formatting but can easily be modified for
any Styler format.
pandas display options usually print the minimum number of rows and the maximum number of columns (no option display.min_columns).

I had some help from the thread StackOverflow: Merging Pandas styler object side by side.
from IPython.display import display, HTML

# Function
def style_with_truncation(df, formatter='{:.2f}', min_rows=10, max_columns=20):
    half_rows = min_rows // 2
    half_cols = max_columns // 2

    # Left half
    upper_left = df.iloc[:half_rows ,:half_cols].style.format(formatter=formatter)
    lower_left = df.iloc[-half_rows:,:half_cols].style.format(formatter=formatter)

    ellipsis_half_row_left = pd.DataFrame([['...'] * (half_cols)],
                                          index=['...'], columns=upper_left.data.columns)
    
    left_half = upper_left.concat(ellipsis_half_row_left.style).concat(lower_left)

    # Right half
    upper_right = df.iloc[:half_rows ,-half_cols:].style.format(formatter=formatter)
    lower_right = df.iloc[-half_rows:,-half_cols:].style.format(formatter=formatter)
    
    ellipsis_half_row_right = pd.DataFrame([['...'] * (half_cols)],
                                          index=['...'], columns=upper_right.data.columns)

    right_half = upper_right.concat(ellipsis_half_row_right.style).concat(lower_right)

    # Middle
    ellipsis_column = pd.DataFrame({'...' : ['...'] * (min_rows+1)}, columns=['...'])
    ellipsis_column = ellipsis_column.style
    
    # Set the Styler attribute to be shown side by side
    left_half.set_table_attributes(""style='display:inline'"")
    right_half.set_table_attributes(""style='display:inline'"")
    ellipsis_column.set_table_attributes(""style='display:inline'"")

    # Display the styler objects inline
    row_col_text = f""<p>{df.shape[0]:d} rows × {df.shape[1]:d} columns</p>""
    display(HTML(left_half._repr_html_() +
                 ellipsis_column.hide(axis=""index"")._repr_html_() +
                 right_half.hide(axis=""index"")._repr_html_()
                 + row_col_text))

# Example of a function call
min_rows = pd.options.display.min_rows
max_columns = pd.options.display.max_columns
style_with_truncation(df, formatter='{:.2f}', min_rows=min_rows, max_columns=max_columns)


Output:
Successful truncation of a stylized DataFrame
",pandas
access denied during geopandas read parquet from s3 bucket,"After moving the parquet file containing the locally created geodataframe to s3, I tried to read the file within AWS Glue as follows.
import geopandas as gpd
test_gdf = gpd.read_parquet(""s3://bucket_name/key/file.parquet"")

However, OS Error occurred as follows
OSError: When getting information for key 'key/file.parquet' in bucket 'bucket_name': AWS Error ACCESS_DENIED during HeadObject operation: No response body.

What I found strange was that when I run pandas.read_parquet, it runs successfully.
import pandas as pd
test_gdf = pd.read_parquet(""s3://bucket_name/key/file.parquet"")

However, I confirmed that reading a geodataframe by pandas and then converting it back to geodataframe takes a lot of time.
Therefore, I want to read the parquet file directly through geopandas.
Referring to other questions, there were issues with IAM Role or s3 bucket policy, so I checked them.
Policy at AWS Glue Role
{
...
            ""Action"": [
                ""s3:*""
            ],
            ""Effect"": ""Allow"",
            ""Resource"": [
                ""*""
            ]
...
}

S3 Bucket Policy
{
    ""Version"": ""2012-10-17"",
    ""Id"": ""PolicyForDatalakeBucket"",
    ""Statement"": [
        {
            ""Sid"": ""denyInsecureTransport"",
            ""Effect"": ""Deny"",
            ""Principal"": ""*"",
            ""Action"": ""s3:*"",
            ""Resource"": [
                ""arn:aws:s3:::bucket_name/*"",
                ""arn:aws:s3:::bucket_name""
            ],
            ""Condition"": {
                ""Bool"": {
                    ""aws:SecureTransport"": ""false""
                },
                ""ArnNotEquals"": {
                    ""aws:SourceArn"": ""arn:aws:iam::IAM_USER:role/GLUE_ROLE""
                }
            }
        },
        {
            ""Sid"": """",
            ""Effect"": ""Allow"",
            ""Principal"": {
                ""AWS"": [
                    ""arn:aws:iam::IAM_USER:role/GLUE_ROLE"",
                    ""arn:aws:iam::IAM_USER:root""
                ]
            },
            ""Action"": [
                ""s3:GetBucketAcl"",
                ""s3:ListBucket"",
                ""s3:GetObject"",
                ""s3:PutObject""
            ],
            ""Resource"": [
                ""arn:aws:s3:::BUCKET_NAME/*"",
                ""arn:aws:s3:::BUCKET_NAME""
            ]
        }
    ]
}

What needs to be resolved so that geopandas can successfully read parquet files from s3?
","The solution is like below,
import fsspec
import geopandas as gpd

with fsspec.open(feather_file) as f
  gdf = gpd.read_feather(f)

If you want to access feather file in s3 bucket, you need to open the file by fsspec and try to read file by geopandas.read_feather.
You can find more reference in https://geopandas.org/en/stable/docs/user_guide/io.html
",pandas
using one dataframe to find matching combinations in fixed sets,"



A
B
C
D
E




Key 1
1
-1





Key 2

1
-1




Key 3


1
-1



Key 4
-1
1





Key 5
1

-1




Key 6

1

-1



Key 7


1

-1


Key 8

1
-2
1




Final Result



A
B
C
D
E





1
-1





suppose we have the above dataframe where each key is an option used to create a combination to get to the desired Final Result. Suppose that you can also specify max number of combinations it can use to achieve the final result below, how would one iterate through the dataframe and when a set of combinations equals the final result, it prints all the keys that make up the combo as well as number of combos it took?
For example, let's say the maximum number of combinations is 3-key combo. Then the following combinations will satisfy both the Final Result and stay under or equal to the number of key combos allowed to achieve it
Key 2 (itself), combos 1
Key 4 + Key 5, combos 2
Key 3 + Key 8, combos 2
","Assuming:
df = pd.DataFrame({
    'A': [1, None, None, -1, 1, None, None, None],
    'B': [-1, 1, None, 1, None, 1, None, 1],
    'C': [None, -1, 1, None, -1, None, 1, -2],
    'D': [None, None, -1, None, None, -1, None, 1],
    'E': [None, None, None, None, None, None, -1, None]
}, index=['Key 1', 'Key 2', 'Key 3', 'Key 4', 'Key 5', 'Key 6', 'Key 7', 'Key 8'])

final = pd.Series([None, 1, -1, None, None], index=['A', 'B', 'C', 'D', 'E'])

You could use itertools to produce the combinations, and reindex+sum to compare to the expected output:
# helper function to produce the combinations
from itertools import combinations, chain
def powerset(iterable, max_set=None):
    s = list(iterable)
    if max_set is None:
        max_set = len(s)
    return chain.from_iterable(combinations(s, r) for r in range(1, max_set+1))

# loop over the combinations 
# reindex and sum
# compare to final with nulls as 0
MAX_N = 3
for c in powerset(df.index, MAX_N):
    if df.reindex(c).sum().eq(final, fill_value=0).all():
        print(c)

Output:
('Key 2',)
('Key 3', 'Key 8')
('Key 4', 'Key 5')
('Key 1', 'Key 2', 'Key 4')

Note that this produces Key1/Key2/Key4 as a valid combination since Key1/Key4 cancel themselves and Key2 alone is valid.
To avoid this, you could keep track of the produced combinations and only retain those that are not a superset of already seen valid combinations:
MAX_N = 3
seen = set()
for c in powerset(df.index, MAX_N):
    if df.reindex(c).sum().eq(final, fill_value=0).all():
        f = frozenset(c)
        if not any(f > s for s in seen):
            seen.add(f)
            print(c)

Output:
('Key 2',)
('Key 3', 'Key 8')
('Key 4', 'Key 5')

",pandas
pandas dfitertuples renaming dataframe columns when printing,"I know that normally pandas' itertuples() will return the values of each including the column names as follows:
ab=pd.DataFrame(np.random.random([3,3]),columns=['hi','low','med'])
for i in ab.itertuples():
    print(i)

and the output is as follows:
Pandas(Index=0, hi=0.05421443, low=0.2456833, med=0.491185)
Pandas(Index=1, hi=0.28670429, low=0.5828551, med=0.279305)
Pandas(Index=2, hi=0.53869406, low=0.3427290, med=0.750075)

However, I have no idea why it doesn't shows the columns as I expected for my another set of code as below:
            us qqq equity  us spy equity
date                                    
2017-06-19            0.0            1.0
2017-06-20            0.0           -1.0
2017-06-21            0.0            0.0
2017-06-22            0.0            0.0
2017-06-23            1.0            0.0
2017-06-26            0.0            0.0
2017-06-27           -1.0            0.0
2017-06-28            1.0            0.0
2017-06-29           -1.0            0.0
2017-06-30            0.0            0.0

the above is a Pandas Dataframe with Timestamp as index, float64 as the values in the list, and a list of string ['us qqq equity','us spy equity'] as the columns.
When I do this:
for row in data.itertuples():
    print (row)

It shows the columns as _1 and _2 as follows:
Pandas(Index=Timestamp('2017-06-19 00:00:00'), _1=0.0, _2=1.0)
Pandas(Index=Timestamp('2017-06-20 00:00:00'), _1=0.0, _2=-1.0)
Pandas(Index=Timestamp('2017-06-21 00:00:00'), _1=0.0, _2=0.0)
Pandas(Index=Timestamp('2017-06-22 00:00:00'), _1=0.0, _2=0.0)
Pandas(Index=Timestamp('2017-06-23 00:00:00'), _1=1.0, _2=0.0)
Pandas(Index=Timestamp('2017-06-26 00:00:00'), _1=0.0, _2=0.0)
Pandas(Index=Timestamp('2017-06-27 00:00:00'), _1=-1.0, _2=0.0)
Pandas(Index=Timestamp('2017-06-28 00:00:00'), _1=1.0, _2=0.0)
Pandas(Index=Timestamp('2017-06-29 00:00:00'), _1=-1.0, _2=0.0)
Pandas(Index=Timestamp('2017-06-30 00:00:00'), _1=0.0, _2=0.0)

Does anyone has any clue about what have I done wrong? Does it have to do with some variable referencing issue when creating the original dataframe? (Also, as a side question, I learnt from the community that the type of data generated from itertuples() should be tuples, but it seems (as shown above), the return type is  as I verified from the type statement?)
Thank you for all your patience as I am still trying to master the application of DataFrame. 
","This seems to be an issue with handling column names having spaces in them. If you replace the column names with different ones without spaces, it will work:
df.columns = ['us_qqq_equity', 'us_spy_equity'] 
# df.columns = df.columns.str.replace(r'\s+', '_', regex=True)  # Courtesy @MaxU  
for r in df.head().itertuples():
    print(r)

# Pandas(Index='2017-06-19', us_qqq_equity=0.0, us_spy_equity=1.0)
# Pandas(Index='2017-06-20', us_qqq_equity=0.0, us_spy_equity=-1.0)
# ...

Column names with spaces cannot effectively be represented in named tuples, so they are renamed automatically when printing.
",pandas
group a multiindexed pandas dataframe by one of its levels,"Is it possible to groupby a multi-index (2 levels) pandas dataframe  by one of the multi-index levels ?  
The only way I know of doing it is to reset_index on a multiindex and then set index again. I am sure there is a better way to do it, and I want to know how. 
","Yes, use the level parameter of the groupby method. Take a look here. Example:
In [26]: s

first  second  third
bar    doo     one      0.404705
               two      0.577046
baz    bee     one     -1.715002
               two     -1.039268
foo    bop     one     -0.370647
               two     -1.157892
qux    bop     one     -1.344312
               two      0.844885
dtype: float64

In [27]: s.groupby(level=['first','second']).sum()

first  second
bar    doo       0.981751
baz    bee      -2.754270
foo    bop      -1.528539
qux    bop      -0.499427
dtype: float64

",pandas
pandas groupby with multiindex columns,"My goal, the way I expected to achieve it, and what happens instead
I am trying to do a groupby on a DataFrame which has multiindex columns using a Series (without multiindex) as an input to group by. Specifically, given the below DataFrame
>>> df
            X        Y      
            A  B  C  A  B  C
2020-01-01  9  1  2  1  6  5
2020-01-02  5  7  8  0  6  9
2020-01-03  6  3  4  8  6  1
2020-01-06  0  0  9  0  5  1
2020-01-07  8  7  4  8  3  1

and the Series representing the groups
>>> groups
A    D
B    D
C    E
dtype: object

I try to run the following
>>> df.groupby(groups, axis=1, level=1).sum()

and expect to get
             X      Y   
             D  E   D  E
2020-01-01  10  2   7  5
2020-01-02  12  8   6  9
2020-01-03   9  4  14  1
2020-01-06   0  9   5  1
2020-01-07  15  4  11  1

Instead however I get the following error:
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/home/zak/anaconda3/envs/lib/python3.8/site-packages/pandas/core/frame.py"", line 6717, in groupby
    return DataFrameGroupBy(
  File ""/home/zak/anaconda3/envs/lib/python3.8/site-packages/pandas/core/groupby/groupby.py"", line 560, in __init__
    grouper, exclusions, obj = get_grouper(
  File ""/home/zak/anaconda3/envs/lib/python3.8/site-packages/pandas/core/groupby/grouper.py"", line 828, in get_grouper
    Grouping(
  File ""/home/zak/anaconda3/envs/lib/python3.8/site-packages/pandas/core/groupby/grouper.py"", line 485, in __init__
    ) = index._get_grouper_for_level(self.grouper, level)
  File ""/home/zak/anaconda3/envs/lib/python3.8/site-packages/pandas/core/indexes/multi.py"", line 1487, in _get_grouper_for_level
    grouper = level_values.map(mapper)
  File ""/home/zak/anaconda3/envs/lib/python3.8/site-packages/pandas/core/indexes/base.py"", line 5098, in map
    new_values = super()._map_values(mapper, na_action=na_action)
  File ""/home/zak/anaconda3/envs/lib/python3.8/site-packages/pandas/core/base.py"", line 937, in _map_values
    new_values = map_f(values, mapper)
  File ""pandas/_libs/lib.pyx"", line 2467, in pandas._libs.lib.map_infer
TypeError: 'numpy.ndarray' object is not callable

I'm using Python 3.8.8 and Pandas version 1.2.3.
A sub-optimal solution
One way I found to achieve the above is with the following code, but I'm specifically wondering whether there is a cleaner way to do so. If not, why not? To me the above attempt would be the expected behaviour of the groupby method, but it seems I'm misunderstanding the logic behind it.
>>> df, groups = df.align(groups, axis=1, level=1)
>>> df.groupby(groups, axis=1).apply(lambda x: x.sum(axis=1, level=0)).swaplevel(axis=1).sort_index(axis=1)
             X      Y   
             D  E   D  E
2020-01-01  10  2   7  5
2020-01-02  12  8   6  9
2020-01-03   9  4  14  1
2020-01-06   0  9   5  1
2020-01-07  15  4  11  1

","You can use rename by second level of MultiIndex and then aggregate by both levels:
df = df.rename(columns=groups, level=1).sum(axis=1, level=[0,1])

#working like
#df = df.rename(columns=groups, level=1).groupby(axis=1, level=[0,1]).sum()
print (df)
             X      Y   
             D  E   D  E
2020-01-01  10  2   7  5
2020-01-02  12  8   6  9
2020-01-03   9  4  14  1
2020-01-06   0  9   5  1
2020-01-07  15  4  11  1


Your solution should be changed by lambda function, but output is different:
df = df.groupby(lambda x: groups[x], axis=1, level=1).sum()
print (df)
             D   E
2020-01-01  17   7
2020-01-02  18  17
2020-01-03  23   5
2020-01-06   5  10
2020-01-07  26   5

",pandas
nan values in pandas are not being filled by the interpolate function when it39s applied to a full dataframe,"So, i'm a beginner at the Pandas Python and noticed the interpolate function is pretty interesting, but i have one problem when using the line:
result = df.interpolate(method='linear')

I found out that even though it did filled a lot of the NaN's in 'df', the four first NaN's of Ambient_temp and the first NaN on the Intake_temp column are not filled the way i wanted. Any hints on how to get this working? The interpolation worked very well with every other column besides those two.
Image of said dataframe
Example:
amb_temp = [np.nan, np.nan, 32, 32]
in_temp = [ 29, 27, 23, 22]
volts = [np.nan, 13, 11, 11]

dict = {'ambient_temperature': amb_temp, 'temperature_inside': in_temp, 'volts': volts} 

df = pd.DataFrame(dict)

(it's not exactly the same dataframe, but encapsulates the same problem. I got this one based off and example on 'geeksforgeeks' and used numpy.nan to simulate the absence of data.)
","This is it:
import numpy as np
import pandas as  pd
amb_temp = [np.nan, np.nan, 32, 32]
in_temp = [ 29, 27, 23, 22]
volts = [np.nan, 13, 11, 11]

dict1 = {'ambient_temperature': amb_temp, 'temperature_inside': in_temp, 'volts': volts} 
keys = list(dict1.keys())

for k in keys:
    data_array = np.array(dict1[k])
    print(""1   {}"".format(data_array))
    not_nan = ~np.isnan(data_array)
    indices = np.arange(len(data_array))
    dict1[k] = np.interp(indices, indices[not_nan], data_array[not_nan])
print(dict1)

",pandas
expanding pandas data frame with date range in columns,"I have a pandas dataframe with dates and strings similar to this:
Start        End           Note    Item
2016-10-22   2016-11-05    Z       A
2017-02-11   2017-02-25    W       B


I need to expand/transform it to the below, filling in weeks (W-SAT) in between the Start and End columns and forward filling the data in Note and Items:
Start        Note    Item
2016-10-22   Z       A
2016-10-29   Z       A
2016-11-05   Z       A
2017-02-11   W       B
2017-02-18   W       B
2017-02-25   W       B


What's the best way to do this with pandas? Some sort of multi-index apply?
","You can iterate over each row and create a new dataframe and then concatenate them together
pd.concat([pd.DataFrame({'Start': pd.date_range(row.Start, row.End, freq='W-SAT'),
               'Note': row.Note,
               'Item': row.Item}, columns=['Start', 'Note', 'Item']) 
           for i, row in df.iterrows()], ignore_index=True)

       Start Note Item
0 2016-10-22    Z    A
1 2016-10-29    Z    A
2 2016-11-05    Z    A
3 2017-02-11    W    B
4 2017-02-18    W    B
5 2017-02-25    W    B

",pandas
how to read a xlsx file using the pandas library in ipython,"I want to read a .xlsx file using the Pandas Library of python and port the data to a postgreSQL table. 
All I could do up until now is:
import pandas as pd
data = pd.ExcelFile(""*File Name*"")


Now I know that the step got executed successfully, but I want to know how i can parse the excel file that has been read so that I can understand how the data in the excel maps to the data in the variable data. 
I learnt that data is a Dataframe object if I'm not wrong. So How do i parse this dataframe object to extract each line row by row.
","I usually create a dictionary containing a DataFrame for every sheet:
xl_file = pd.ExcelFile(file_name)

dfs = {sheet_name: xl_file.parse(sheet_name) 
          for sheet_name in xl_file.sheet_names}


Update: In pandas version 0.21.0+ you will get this behavior more cleanly by passing sheet_name=None to read_excel:
dfs = pd.read_excel(file_name, sheet_name=None)


In 0.20 and prior, this was sheetname rather than sheet_name (this is now deprecated in favor of the above):
dfs = pd.read_excel(file_name, sheetname=None)

",pandas
merging multiple dataframes with non unique indexes,"Given two DFs with non unique indexes and multidimentional columns:
ars:
           arsenal   arsenal   arsenal   arsenal
NaN             B3        SK        BX        BY
2015-04-15     NaN       NaN       NaN      26.0
2015-04-14     NaN       NaN       NaN       NaN
2015-04-13    26.0      26.0      23.0       NaN
2015-04-13    22.0      21.0      19.0       NaN

che:
           chelsea   chelsea   chelsea   chelsea
NaN             B3        SK        BX        BY
2015-04-15     NaN       NaN       NaN      1.01
2015-04-14    1.02       NaN       NaN       NaN
2015-04-14     NaN      1.05       NaN       NaN

here in csv format
,arsenal,arsenal,arsenal,arsenal
,B3,SK,BX,BY
2015-04-15,,,,26.0
2015-04-14,,,,
2015-04-13,26.0,26.0,23.0,
2015-04-13,22.0,21.0,19.0,


,chelsea,chelsea,chelsea,chelsea
,B3,SK,BX,BY
2015-04-15,,,,1.01
2015-04-14,1.02,,,
2015-04-14,,1.05,,

I would like to join/merge them, sort of an outer join so that rows are not dropped. 
I would like the output to be:
            arsenal  arsenal   arsenal   arsenal chelsea   chelsea   chelsea   chelsea
NaN             B3        SK        BX        BY      B3        SK        BX        BY
2015-04-15     NaN       NaN       NaN      26.0     NaN       NaN       NaN      1.01
2015-04-14     NaN       NaN       NaN       NaN    1.02       NaN       NaN       NaN
2015-04-14     NaN       NaN       NaN       NaN     NaN      1.05       NaN       NaN
2015-04-13    26.0      26.0      23.0       NaN     NaN       NaN       NaN       NaN
2015-04-13    22.0      21.0      19.0       NaN     NaN       NaN       NaN       NaN

None of the pandas tools I know worked: merge, join, concat. merge's outer join gives a dot product which is not what I am looking for, while concat can't handle non unique indexes.  
Do you have any ideas how this can be achieved?
Note: the lengths of dataframes won't be idential.
","I've managed to sort it out using pandas' concat method.
First, we need to add a Multiindex level so that it becomes unique:
ars = pd.read_csv(""ars.csv"", index_col=[0], header=[0,1])
che = pd.read_csv(""che.csv"", index_col=[0], header=[0,1])

ars.index.name = ""date""
ars[""num""] = range(0, len(ars.index))
ars = ars.set_index(""num"", append=True)

che.index.name = ""date""
che[""num""] = range(0, len(che.index))
che = che.set_index(""num"", append=True)

Now we can use concat:
df = pd.concat([ars, che], axis=1)
df = df.reset_index()
df = df.sort_index(by=[""date"", ""num""], ascending=[False, True])
df = df.set_index([""date"", ""num""])
df.index = df.index.droplevel(1)

Output:
                arsenal             chelsea                
                B3  SK  BX  BY      B3    SK  BX    BY
date                                                  
2015-04-15     NaN NaN NaN  26     NaN   NaN NaN  1.01
2015-04-14     NaN NaN NaN NaN    1.02   NaN NaN   NaN
2015-04-14     NaN NaN NaN NaN     NaN  1.05 NaN   NaN
2015-04-13      26  26  23 NaN     NaN   NaN NaN   NaN
2015-04-13      22  21  19 NaN     NaN   NaN NaN   NaN

",pandas
how to reorder columns if the columns have the same part name,"I want to reorder columns name if the columns have the same part name. Sample as below:
import pandas as pd

df = pd.DataFrame({
    'Branch': ['Hanoi'],
    '20241201_Candy': [3], '20241202_Candy': [4], '20241203_Candy': [5],
    '20241201_Candle': [3], '20241202_Candle': [4], '20241203_Candle': [5],
    '20241201_Biscuit': [3], '20241202_Biscuit': [4], '20241203_Biscuit': [5]})

Below is my Expected Ouput:
df2 = pd.DataFrame({
    'Branch': ['Hanoi'],
    '20241201_Biscuit': [3], '20241201_Candle': [3], '20241201_Candy': [3],
    '20241202_Biscuit': [4], '20241202_Candle': [4], '20241202_Candy': [4],
    '20241203_Biscuit': [5], '20241203_Candle': [5], '20241203_Candy': [5]})

So I want to auto reorder dataframe if it has same date.
","You can use df.reindex, single out column 'Branch' and apply sorted to the remainder, df.columns[1:]:
out = df.reindex(['Branch'] + sorted(df.columns[1:]), axis=1)

out.equals(df2)
# True

Or directly:
out2 = df[['Branch'] + sorted(df.columns[1:])]

out2.equals(df2)
# True

",pandas
pandas nonnegative integers to nbits binary representation,"I have a pandas Series containing strictly non-negative integers like so:
1
2
3
4
5

I want to convert them into n-bits binary representation based on the largest value. For example, the largest value here is 5, so we would have 3 bits/3 columns, and the resulting series would be something like this
0 0 1
0 1 0
0 1 1
1 0 0
1 0 1

Thanks a lot in advance!
","If your values are less than 255, you could unpackbits:
s = pd.Series([1, 2, 3, 4, 5])

N = int(np.log2(s.max()))
powers = 2**np.arange(N, -1, -1)
out = pd.DataFrame(np.unpackbits(s.to_numpy(np.uint8)[:, None], axis=1)[:, -N-1:],
                   index=s.index, columns=powers)

If your have larger numbers, compute a mask with & and an array of powers of 2:
s = pd.Series([1, 2, 3, 4, 5])

powers = 2**np.arange(int(np.log2(s.max())), -1, -1)

out = pd.DataFrame((s.to_numpy()[:, None] & powers).astype(bool).astype(int),
                   index=s.index, columns=powers)

Output:
   4  2  1
0  0  0  1
1  0  1  0
2  0  1  1
3  1  0  0
4  1  0  1

",pandas
troubleshootingquot valueerror index date invalidquot,"I'm learning Python 3.7.4 using IDLE. I am receiving an error saying index date invalid.
I've had success using excel files that i've formatted with dates & data in individual col's (A-C) but this is a csv file with all the data in one col separated by commas. In the format with the data formatted i would use the below...
ANTM = pd.read_csv(r'C:\..\ANTM_stock'.csv', parse_dates= True, index_col=0)

I'm pretty sure the parse_dates=True, index_col=0 is indicating look at the first col & interpret them as dates. This works fine.
#Markowitz efficient frontier

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import pandas_datareader.data as web

companies = ['WMT','FB']
df = pd.read_csv(r'C:\...\Walmart_FB_2014_2017.csv', index_col='Date')

df.head()

I expected to see a table with some data instead i received the error below.
ValueError: Index Date invalid

Below is a sample of the data (It is all in Col A)
Data in CSV File
","@xzcvb33 Your code runs fine for me. What happens if you try and read the csv without specifying the index column df = pd.read_csv(""...2017.csv"") and then setting the index to be ""Date"" df.set_index(""Date"")? – Tom yesterday
",pandas
pandas append sum of multy dataframes separately to target dataframe,"I need to get sum of couple DataFrames and append each result to a target DataFrames(df_sum)
df_sum = pd.DataFrame(columns = ['Source', 'Column2_SUM', 'Column3_SUM'])

I have 4 dataframe as
import pandas as pd

data_A = {'Column1': ['2023-06-16','2023-08-24','2023-04-24'],
          'Column2': [4, 5, 6],
          'Column3': [7, 8, 9]}

data_B = {'Column1': ['2023-07-19','2023-08-24','2023-03-18'],
          'Column2': [4, 96, 6],
          'Column3': [12, 8, 9]}

data_C = {'Column1': ['2023-06-22','2023-04-20','2023-09-12'],
          'Column2': [4, 88, 6],
          'Column3': [7, 8, 12]}

data_D = {'Column1': ['2023-08-27','2023-11-24','2023-04-08'],
          'Column2': [4, 32, 6],
          'Column3': [66, 8, 80]}


df_A = pd.DataFrame(data_A)
df_B = pd.DataFrame(data_B)
df_C = pd.DataFrame(data_C)
df_D = pd.DataFrame(data_D)

Now what I need is to get something like, lo be loaded up to df_sum

","Create dictionary for specify names of DataFrames, use concat and aggregate sum, add DataFrame.add_suffix with DataFrame.rename_axis and DataFrame.reset_index for column SOURCE:
d = {'df_A':df_A, 'df_B':df_B, 'df_C':df_C, 'df_D':df_D}

df = (pd.concat(d)
        .groupby(level=0)[['Column2','Column3']].sum()
        .add_suffix('_SUM')
        .rename_axis('SOURCE')
        .reset_index())
print (df)
  SOURCE  Column2_SUM  Column3_SUM
0   df_A           15           24
1   df_B          106           29
2   df_C           98           27
3   df_D           42          154

",pandas
for loop in pandas allows applying some changes to the df whereas discarding others,"When having multiple df's to which similar changes need to be made I usually tend to use a for loop as a quick solution to avoid repetitive work.
I understand that the df change happening in the loop is being applied to a copy. So for some methods I can overcome that with the 'inplace' parameter where available. Unfortunately for things like filtering I don't seem to have such an option. Below are examples of a code where I try to filter on rows where the date is higher or equal then the variable 'now'. Regarding the same filtering outside of the loop it will work correctly so it's not the matter of incorrect dtype or lack of data to be filtered out. Below are my code examples and would like to learn if the incorrect result is due to my poor knowledge or there are specific limitations and if so how do more experience users tackle this:
original code:
now = pd.to_datetime(date.today(), format='%Y%m%d')

for df in df_list:
    df.dropna(how='all', axis=1, inplace=True)
    df['Valid From Converted'] = pd.to_datetime(df['Valid From'], format='%Y%m%d')
    df['to Converted'] = pd.to_datetime(df['to'], format='%Y%m%d')
    df = df[df['to Converted'] >= now]

enumerate approach:
now = pd.to_datetime(date.today(), format='%Y%m%d')

for i, df in enumerate(df_list):
    df.dropna(how='all', axis=1, inplace=True)
    df['Valid From Converted'] = pd.to_datetime(df['Valid From'], format='%Y%m%d')
    df['to Converted'] = pd.to_datetime(df['to'], format='%Y%m%d')
    df = df[df['to Converted'] >= now]
    df_list[i] = df

range approach:
for i in range(len(df_list)):
    df_list[i].dropna(how='all', axis=1, inplace=True)
    df_list[i]['Valid From Converted'] = pd.to_datetime(df_list[i]['Valid From'], format='%Y%m%d')
    df_list[i]['to Converted'] = pd.to_datetime(df_list[i]['to'], format='%Y%m%d')
    df_list[i] = df_list[i][df_list[i]['to Converted'] >= now]

","For code clarity, durability, and maintainability, avoid relying on side-effects but explicitly assign objects. One approach can be to define a method to handle the multiple step operations and call it in a list comprehension:
def proc_frame(df):
    df = df.dropna(how='all', axis=1)
    df['Valid From Converted'] = pd.to_datetime(df['Valid From'], format='%Y%m%d')
    df['to Converted'] = pd.to_datetime(df['to'], format='%Y%m%d')
    return df[df['to Converted'] >= now]

new_df_list = [proc_frame(df) for df in df_list]

Additionally, consider chaining lines in right hand side methods like assign and query to avoid the df calls.
def proc_frame(df):
    return (
        df.dropna(how='all', axis=1)
          .assign(**{
              'Valid From Converted': lambda x: pd.to_datetime(x['Valid From'], format='%Y%m%d'),
              'to Converted': lambda x: pd.to_datetime(x['to'], format='%Y%m%d')
          })
          .query('`to Converted` >= @now')
    )
new_df_list = [proc_frame(df) for df in df_list]

",pandas
how can i create the minimum size executable with pyinstaller,"I am on Windows 10, I have anaconda installed but I want to create an executable independently in a new, clean minimal environment using python 3.5. So I did some tests: 
TEST1: 
I created a python script test1.py in the folder testenv with only:
print('Hello World')

Then I created the environment, installed pyinstaller and created the executable
D:\testenv> python -m venv venv_test
...
D:\testenv\venv_test\Scripts>activate.bat
...
(venv_test) D:\testenv>pip install pyinstaller
(venv_test) D:\testenv>pyinstaller --clean -F test1.py

And it creates my test1.exe of about 6 Mb
TEST 2: I modified test1.py as follows:
import pandas as pd
print('Hello World')  

I installed pandas in the environment and created the new executable:
(venv_test) D:\testenv>pip install pandas
(venv_test) D:\testenv>pyinstaller --clean -F test1.py

Ant it creates my test1.exe which is now of 230 Mb!!! 
if I run the command 
(venv_test) D:\testenv>python -V
Python 3.5.2 :: Anaconda custom (64-bit)

when I am running pyinstaller I get some messages I do not understand, for example: 
INFO: site: retargeting to fake-dir 'c:\\users\\username\\appdata\\local\\continuum\\anaconda3\\lib\\site-packages\\PyInstaller\\fake-modules'

Also I am getting messages about matplotlib and other modules that have nothing to do with my code, for example: 
INFO:   Matplotlib backend ""pdf"": added
INFO:   Matplotlib backend ""pgf"": added
INFO:   Matplotlib backend ""ps"": added
INFO:   Matplotlib backend ""svg"": added

I know there are some related questions: 
Reducing size of pyinstaller exe, size of executable using pyinstaller and numpy
but I could not solve the problem and I am afraid I am doing something wrong with respect to anaconda.  
So my questions are:
what am I doing wrong? can I reduce the size of my executable?
","The problem is that you should not be using a virtual environment and especially not anaconda.  Please download default python 32 bit and use only necessary modules.  Then follow the steps provided in the links, this should definitely fix it.  
Although you created a virtual env, are you sure your spec file is not linking to old Anaconda entries?
If all this fails, then submit a bug as this is very strange.  
",pandas
pandas displaying the first row but not indexing it,"I have a large text file, with a header of 18 lines.
If I try to display the entire dataframe:
df = pd.read_csv('my_log')
print(df)

I get:
pandas.errors.ParserError: Error tokenizing data. C error: Expected 1 fields in line 19, saw 3
If I try to use exclude the header:
df = pd.read_csv('my_log', header=18)

I get the first row (line 19), then the second row (showing indexed at 0)
No matter which index number I use in: print(df.loc[[0]]), I always get that first row displayed (no index number) before the row that I want.
I've checked out the text file, and every row ends in a CR/LF. I've also completely removed line 19; but, the same behavior occurs.
Also, if I completely remove the header and print the entire dataframe, I still get the same behavior. The first row prints (without an index number) and the row count is 1 less than the true row count.
Any suggestions greatly appreciated!
","One approach is to skip the rows and define the columns
column_names = ['Column1', 'Column2', 'Column3']
df = pd.read_csv(file_path, skiprows=18, names=column_names)

",pandas
replace first row value with last row value,"I'm trying to take the value from the last row of a df col and replace it with the first value. I'm returning a value error.
import pandas as pd
df = pd.DataFrame({'name': ['tom','jon','sam','jane','bob'],
       'age': [24,25,18,26,17],
       'Notification': [np.nan,'2025-01-03 14:19:35','2025-01-03 14:19:39','2025-01-03 14:19:41','2025-01-03 14:19:54'],
       'sex':['male','male','male','female','male']})

df_test = df.copy()

df_test['Notification'] = pd.to_datetime(df_test['Notification'])

df_test['Notification'].iloc[0] = df_test['Notification'].tail(1)

Error:
ValueError: Could not convert object to NumPy datetime

","You need to edit this line:
df.loc[df.index[0], 'age'] = df.loc[df.index[-1], 'age']

Complete code:
import pandas as pd
import numpy as np

df = pd.DataFrame({'name': ['tom', 'jon', 'sam', 'jane', 'bob'],
               'age': [np.nan, 25, 18, 26, 17],
               'sex': ['male', 'male', 'male', 'female', 'male']})

df.loc[df.index[0], 'age'] = df.loc[df.index[-1], 'age']

Cheers!!!
",pandas
how to update pandas dataframe column using string concatenation in function,"I have a dataframe where I would like to add a full address column, which would be the combination of 4 other columns (street, city, county, postalcode) from that dataframe. Example output of the address column would be:
5 Test Street, Worthing, West Sussex, RH5 3BX

Or if the city was empty as an example:
5 Test Street, West Sussex, RH5 3BX

This is my code, which after testing I see I might need to use something like apply, but I can't workout how to do it.
def create_address(street: str, city: str, county: str, postalcode: str) -> str:
    
    list_address = []
    
    if street:
        list_address.append(street)
    if city:
        list_address.append(city)
    if county:
        list_address.append(county)
    if postalcode:
        list_address.append(postalcode)

    address = "", "".join(list_address).rstrip("", "")

    return address

df[""address""] = create_address(df[""Street""], df[""City""], df[""County""], df[""PostalCode""])

","you can use lambda and apply to get the concatenated of full address
Example input
EDIT : postalcode with None
data = {
'street': ['street1', 'street2', 'street3'],
'city': ['city1', '', 'city2'],
'county': ['county1', 'county2', 'county3'],
'postalcode': ['postalcode1', 'postalcode2', '']
}

Sample code
df['full_address'] = df.apply(
    lambda row: ', '.join(filter(None, [row['street'], row['city'], row['county'], row['postalcode']])),
    axis=1
)

None is used as a filter so that unavailable elements are removed.
Output
    street   city   county   postalcode                          full_address
0  street1  city1  county1  postalcode1  street1, city1, county1, postalcode1
1  street2         county2  postalcode2         street2, county2, postalcode2
2  street3  city2  county3                            street3, city2, county3

",pandas
tmdb movie dataset  data visualization eda,"I need help for my Dissertation Project. I am working on a Python project as part of my Masters Degree at my university in England, UK. The dataset I have gotten through the Kaggle platform which contains over one million movies in terms of their titles, budget, box-office revenue, genres, popularity, reviews, keywords etc. Here is the weblink for clarification (I got the latest update):
https://www.kaggle.com/datasets/asaniczka/tmdb-movies-dataset-2023-930k-movies
UPDATE: The cleaning of the data is completed but I need help in collaboration with data visualizations for my Exploratory Data Analysis (EDA) because as the dataset is larger for memory at 504mb it has created ridiculous visualizations from the data. I am very familiar with matplotlib and seaborn functions for Python.
The simple codes I used for count plots for part of this data was:
plt.figure(figsize=(20,16))
sns.countplot(x = 'Genres', data=df2)  
plt.xlabel('Genres', fontsize=14)
plt.ylabel('Frequency', fontsize=14)
plt.show()


Does anyone know how I can create clear and concise data visualizations for specific parts of the data in terms of Bivariate, Univariate and Multivariate analysis?
For example:
Most popular movie genres/movie production companies in terms of frequency
Top movies with biggest box office revenue/budgets, popularity etc.
Countries with biggest number of production/distribution movies made.
And much more. Anything would help. Thank you very much in advance.
","If df is the dataframe you got from pd.read_csv(), you can filter on the strings inside the column 'production_countries' by using str.contains().
Note that ""|"" means ""OR"", that na=False is to ignore the few missing data in this column and case=False to ignore the case(lower/upper) though it seems that case is not necessary for this dataset.
df = df[df['production_countries'].str.contains(""United States|United Kingdom"", na=False, case=False)]

",pandas
how do i read in a csv file from my desktop in python,"I am trying to read in a CSV file from my desktop:
My code is as follows:
import pandas as pd
import csv
from pathlib import Path

csv = r'C:\Users\nulli\OneDrive\Desktop\Work_Sample.csv'
df = pd.read_csv(csv, sep=',')

Error:
---> df = pd.read_csv(csv, sep=',')

FileNotFoundError: [Errno 2] No such file or directory: 'C:\\Users\\nulli\\OneDrive\\Desktop\\Work_Sample.csv'

","You shouldn't use backslashes for paths as Python has some special escape characters like \n that means newline or \t that means a tab. The easiest way is to always use standard slashes. You can also use:
r""some_path\with\backslashes""

to ignore escape characters and treat all backslashes as backslashes.
But the best way is to use a package which was designed to handle paths. pathlib is a great tool for that:
from pathlib import Path
my_csv = Path(""C:/Usersnulli/OneDrive/Desktop/Work_Sample.csv"")
df = pd.read_csv(my_csv.resolve(), sep=',')

resolve() returns a str from Path object.
And I think that it is often better to use relative paths - they will work for everyone without the need to update them. You can for example create a data folder in your workspace with the script and put the file there.
Also, you can check this out for more details regarding paths in Python.
",pandas
pythonpandas  breaking postal code into postal and extension,"I have a following column in pandas dataframe which has postal code -
postal code
56789-2345
45675
null
23445-445
1234-45
34567

I need to break it into postal code and ext like below -
Postal code     postal_ext
56789           2345
45675
null
23445           445
1234            45
34567

How can i do this
","Try with split
out = df['postal code'].str.split('-',expand=True)
out.columns = ['Postal code','postal_ext']

out
  Postal code postal_ext
0       56789       2345
1       45675       None
2         NaN        NaN
3       23445        445
4        1234         45
5       34567       None

",pandas
customize header pandas to html,"I created a web application to search content in CSV files.for now i only able to put all content into 1 column with comma as delimiter ,
I want to display the data as shown in the image.
My python code
selected_columns = [202, 9, 10, 135, 13, 127, 126, 26, 28,128,129,31,33]
df = df.iloc[:, selected_columns]
df.columns = ['CDRID', 'ANumber', 'To', 'SipStatus', 'ISrcIP', 'IDstIP', 'ISrcRTP', 'IDstRTP', 'OSrcIP', 'ODstIP', 'OSrcRTP', 'ODstRTP']]
df['Ingress'] = df[['ISrcIP', 'IDstIP', 'ISrcRTP', 'IDstRTP']].apply(lambda x: ', '.join(x.dropna().astype(str)), axis=1)
df['Egress'] = df[[ 'OSrcIP', 'ODstIP', 'OSrcRTP', 'ODstRTP']].apply(lambda x: ', '.join(x.dropna().astype(str)), axis=1)
df = df.drop(columns=['ISrcIP', 'IDstIP', 'ISrcRTP', 'IDstRTP', 'OSrcIP', 'ODstIP', 'OSrcRTP', 'ODstRTP'])    
df = df.drop(columns=['ISrcIP', 'IDstIP', 'ISrcRTP', 'IDstRTP', 'OSrcIP', 'ODstIP', 'OSrcRTP', 'ODstRTP'])
combined_df = pd.concat([combined_df, df], ignore_index=True)
results.append(('Combined Results', combined_df.to_html(index=False)))

html frontend code
        {% for file, table in results %}
    <div class=""container-fluid"">
    <div class=""table-responsive"">
        <table class=""table table-striped table-bordered table-hover"">
            {{ table|safe }}
        </table>
    </div>
{% endfor %}

current output

expected html header

","You can use multi-index to have several levels of column headers and you can use .style.set_table_styles to customize the display of the dataframe:
cols = ['CDRID', 'ANumber', 'To', 'SipStatus', 'ISrcIP', 'IDstIP', 'ISrcRTP', 'IDstRTP', 
        'OSrcIP', 'ODstIP', 'OSrcRTP', 'ODstRTP']
df = pd.DataFrame(np.round(100*np.random.random((3, 12))), columns=cols)

# Use multi-index
df.columns = pd.MultiIndex.from_arrays([4*[""""] + 4*[""Ingress""] + 4*[""Outgress""], df.columns])

# Add borders and hide the index
df_styled = df.style.set_table_styles([
     {""selector"": ""td"", ""props"": [(""border"", ""1px solid black"")]},  # cells borders
     {""selector"": ""th"", ""props"": [(""border"", ""1px solid black"")]},  # headers borders
     {""selector"": ""th"", ""props"": [(""text-align"", ""center"")]},  # text headers alignment
])
display(df_styled.hide(axis=""index""))
# print(df_styled.to_html())  # to get the HTML/CSS code


",pandas
seaborn linegraph with binned values,"I have the following code and graph:
bins = [0, 5, 15, 25, 50, 75, 100, 125, 150, 175, 200, 250, 300, 400, 500, 600, 700, 850, 1000, 5000, 100000]
df['articles_binned'] = pd.cut(df['ARTICLES'], bins)

export_df = df.groupby(['articles_binned', 'SUB_TYPE'])['CANCELLATION_FLAG'].mean().mul(100).reset_index()
export_df = export_df.rename(columns={'CANCELLATION_FLAG': 'Cancellation_Percentage'})
    
# Plot the bar chart
ax = sns.barplot(x='articles_binned', 
            y='Cancellation_Percentage', 
            hue='SUB_TYPE', 
            data=export_df)


But now I would actually like to see this information in a line graph. With a different color for every sub_type. Seaborn doesn't accept the bins, even though they are categorical.
How should I go about this?
","The example below plots the data as a line plot, with a colour per SUB_TYPE.
The articles_binned column is formatted as a string. If it doesn't work with your data, consider including some sample data in your question to help with debugging.
Update: OP confirms that converting the articles_binned column to strings resolved the problem.


    SUB_TYPE    articles_binned Cancellation_Percentage
0   Type A  (0, 5]  14
1   Type A  (5, 15] 21
...
14  Type D  (15, 25]    14
15  Type D  (25, 50]    13

Reproducible example
import pandas as pd

#
# Data for testing
#
data = {
    'SUB_TYPE': [
        'Type A', 'Type A', 'Type A', 'Type A',
        'Type B', 'Type B', 'Type B', 'Type B',
        'Type C', 'Type C', 'Type C', 'Type C',
        'Type D', 'Type D', 'Type D', 'Type D'
    ],
    'articles_binned': [
        '(0, 5]', '(5, 15]', '(15, 25]', '(25, 50]',
        '(0, 5]', '(5, 15]', '(15, 25]', '(25, 50]',
        '(0, 5]', '(5, 15]', '(15, 25]', '(25, 50]',
        '(0, 5]', '(5, 15]', '(15, 25]', '(25, 50]'
    ],
    'Cancellation_Percentage': [
        14, 21, 14, 13,
        16, 25, 18, 17,
        21, 21, 19, 12,
        15, 16, 14, 13
    ]
}

df = pd.DataFrame(data)
display(df)

#
# Plot
#
import seaborn as sns
import matplotlib.pyplot as plt

#Bar plot
ax = sns.barplot(
    df, x='articles_binned', y='Cancellation_Percentage',
    hue='SUB_TYPE', legend=False
)

sns.despine(ax.figure)
ax.figure.set_size_inches(5, 2.5)
plt.show()

#Line plot
ax = sns.lineplot(
    df, x='articles_binned', y='Cancellation_Percentage',
    hue='SUB_TYPE',
    marker='s', linewidth=1,
)
ax.figure.set_size_inches(5, 3)
sns.despine(ax.figure)
plt.show()

",pandas
how to find the first nonnan data before nan in one column in pandas,"For example,
I have some data like this:
column = pd.Series([1,2,3,np.nan,4,np.nan,7])
print column

Executing the command, the result looks like:
0    1.0
1    2.0
2    3.0
3    NaN
4    4.0
5    NaN
6    7.0

Now I want to know what is the first value before each NaN value, such as 3.0 which is before the first NaN. And 4.0 is the result before the second NaN value. Is there any built-in function in pandas which can complete this or shall I write a for loop to do this? 
","Same idea as @jezrael... numpyfied.
column[np.append(np.isnan(column.values)[1:], False)]

2    3.0
4    4.0
dtype: float64

Complete with pd.Series reconstruction
m = np.append(np.isnan(column.values)[1:], False)
pd.Series(column.values[m], column.index[m])

2    3.0
4    4.0
dtype: float64


Not nearly as quick but intuitive.  Group by the the cumsum of isnull and take the last value.  Of this result, get rid of last row.
column.groupby(column.isnull().cumsum()).last().iloc[:-1]

0    3.0
1    4.0
dtype: float64

",pandas
how to create combinations from dataframes for a specific combination size,"Say I have a dataframe with 2 columns, how would I create all possible combinations for a specific combination size? Each row of the df should be treated as 1 item in the combination rather than 2 unique separate items. I want the columns of the combinations to be appended to the right. The solution should ideally be efficient since it takes long to generate all the combinations with a large list.
For example, I want to create all possible combinations with a combination size of 3.
import pandas as pd

df = pd.DataFrame({'A':['a','b','c','d'], 'B':['1','2','3','4']})

How would I get my dataframe to look like this?
    A  B  A  B  A  B
0   a  1  b  2  c  3
1   a  1  b  2  d  4
2   a  1  c  3  d  4
3   b  2  c  3  d  4

","An approach is itertools to generate the combinations.

Define the combination size and generate all possible combinations of rows using itertools.combinations
Flatten each combination into a single list of values using itertools.chain.
combination_df is created from the flattened combinations and the columns are dynamically generated to repeat 'A' and 'B' for each combination

Sample
import itertools
combination_size = 3
combinations = list(itertools.combinations(df.values, combination_size))
combination_df = pd.DataFrame(
    [list(itertools.chain(*comb)) for comb in combinations],
    columns=[col for i in range(combination_size) for col in df.columns]
)
    )

EDIT : Optimisation as suggested by @ouroboros1
combination_df = pd.DataFrame( (chain.from_iterable(c) for c in combinations), columns=np.tile(df.columns, combination_size) )

Output
   A  B  A  B  A  B
0  a  1  b  2  c  3
1  a  1  b  2  d  4
2  a  1  c  3  d  4
3  b  2  c  3  d  4

",pandas
why does summing data grouped by dfiloc 0 also sum up the column names,"I have a DataFrame with a species column and four arbitrary data columns. I want to group it by species and sum up the four data columns for each one. I've tried to do this in two ways: once by grouping by df.columns[0] and once by grouping by df.iloc[:, 0].
data = {
    'species': ['a', 'b', 'c', 'd', 'e', 'rt', 'gh', 'ed', 'e', 'd', 'd', 'q', 'ws', 'f', 'fg', 'a', 'a', 'a', 'a', 'a'],
    's1': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20],
    's2': [9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9],
    's3': [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21],
    's4': [10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10]
}
df = pd.DataFrame(data)

grouped_df1 = df.groupby(df.columns[0], as_index=False).sum()
grouped_df2 = df.groupby(df.iloc[:, 0], as_index=False).sum()

Both methods correctly sum the data in the four rightmost columns. But for some reason, the second method also sums up the names of the species, concatenating them into one long, repeating string.
Here's the result from the first method, which is what I'm looking for:
print(grouped_df1)
   species  s1  s2  s3  s4
0        a  91  54  97  60
1        b   2   9   3  10
2        c   3   9   4  10
3        d  25  27  28  30
4        e  14  18  16  20
5       ed   8   9   9  10
6        f  14   9  15  10
7       fg  15   9  16  10
8       gh   7   9   8  10
9        q  12   9  13  10
10      rt   6   9   7  10
11      ws  13   9  14  10

And here's the result from the df.iloc method, which incorrectly sums up the species data:
print(grouped_df2)
   species  s1  s2  s3  s4
0   aaaaaa  91  54  97  60
1        b   2   9   3  10
2        c   3   9   4  10
3      ddd  25  27  28  30
4       ee  14  18  16  20
5       ed   8   9   9  10
6        f  14   9  15  10
7       fg  15   9  16  10
8       gh   7   9   8  10
9        q  12   9  13  10
10      rt   6   9   7  10
11      ws  13   9  14  10

Why is the second method summing up the species names as well as the numerical data?
","In groupby - column name is treated as an intrinsic grouping key, while a Series is treated as an external key.
Reference - https://pandas.pydata.org/docs/reference/groupby.html
When using df.iloc[:, 0]:
Pandas considers the string values in the species column as a separate grouping key independent of the DataFrame structure.
When using df.columns[0]:
Pandas directly uses the column 'species' within the DataFrame as the grouping key. This allows Pandas to manage the grouping and summation correctly.
Code COrrection
You should always reference the column name explicitly
grouped_df1 = df.groupby('species', as_index=False).sum()

Or this also works
grouped_df1 = df.groupby(df[df.columns[0]], as_index=False).sum()

",pandas
converting pandas dataframe from long to wide format,"I have a Pandas dataframe that records the performance of students in exams in different classes and it looks like:
Class_ID   Class_size   Student_Number   IQ   Hours   Score   Place
1          3            1                101  10      98      1
1          3            2                99   19      80      3
1          3            3                130  3       95      2
2          5            1                93   5       50      5
2          5            2                103  9       88      3
2          5            3                112  12      99      2
2          5            4                200  10      100     1
2          5            5                90   19      78      4
3          2            1                100  12      84      2
3          2            2                102  13      88      1

and  I would like to convert the above long format into wide format, using Student_Number as columns and Class_ID, Class_size as index, with the last column Top being the student who comes first in that class, so the desire outcome looks like:
Class_ID Class_size IQ_1 IQ_2 IQ_3 IQ_4 IQ_5 Hours_1 Hours_2 Hours_3 Hours_4 Hours_5 Score_1 Score_2 Score_3 Score_4 Score_5 Top
1        1          101  99   130  NaN  NaN  10      19      3       NaN     NaN     98      80      95      NaN     NaN     1
2        5          93   103  112  200  90   5       9       12      10      19      50      88      99      100     78      4
3        2          100  102  NaN  NaN  NaN  12      13      NaN     NaN     NaN     84      88      NaN     NaN     NaN     2

And here is what I have tried:
out = df.pivot_table(index=['Class_ID', 'Class_size'],
                     columns='Student_Number',
                     values=['IQ', 'Hours', 'Score'])
out.columns = [f'{x}_{y}' for x,y in out.columns]
out_dummy = out.reset_index()
df_wide = out_dummy

However, I have no idea how to create the last column Top. And also, the above code seems to be rather slow, and since my original dataframe is quite huge (~300,000rows), I would like to ask is there any quicker way to do that too. Thank you so much in advance.
","A simple method using apply:
out[""Top""] = out.apply(lambda row: 1 + np.argmax(row[""Score_1"":""Score_5""]) , axis=1)

",pandas
dataframe with all nat should be timedelta and not datetime,"I have a DataFrame with a column min_latency, which represents the minimum latency achieved by a predictor. If the predictor failed, there's no value, and therefore it returns min_latency=pd.NaT.
The dataframe is created from a dict, and if and only if all the rows have a pd.NaT value, the resulting column will have a datetime64[ns] dtype. It seems impossible to convert it to timedelta.
df = pd.DataFrame([{'id': i, 'min_latency': pd.NaT} for i in range(10)])
print(df['min_latency'].dtype) # datetime64[ns]
df['min_latency'].astype('timedelta64[ns]') # TypeError: Cannot cast DatetimeArray to dtype timedelta64[ns]

This problem doesn't happen if there's some timedelta in there:
df = pd.DataFrame([{'id': i, 'min_latency': pd.NaT} for i in range(10)] + [{'id': -1, 'min_latency': dt.timedelta(seconds=3)}])
print(df['min_latency'].dtype) # timedelta64[ns]

","Naturally, the best thing would be to adjust the return value, using np.timedelta64 instead of pd.NaT.
import numpy as np

df = pd.DataFrame([{'id': i, 'min_latency': np.timedelta64('NaT', 'ns')} 
                   for i in range(3)]
                  )

Output:
df['min_latency']

0   NaT
1   NaT
2   NaT
Name: min_latency, dtype: timedelta64[ns]

If that is not an option, you can check is_datetime64_dtype. If True, first use Series.values to return the column as ndarray and then apply np.ndarray.astype:
from pandas.api.types import is_datetime64_dtype

df = pd.DataFrame([{'id': i, 'min_latency': pd.NaT} 
                   for i in range(3)]
                  )

if is_datetime64_dtype(df['min_latency']):
    df['min_latency'] = df['min_latency'].values.astype('timedelta64[ns]')

Output:
df['min_latency']

0   NaT
1   NaT
2   NaT
Name: min_latency, dtype: timedelta64[ns]


If you want to rely solely on pandas, you will first need to change values of df['min_latency'] into values that can be understood as a duration. E.g., using pd.to_timedelta + Series.dt.nanosecond:
if is_datetime64_dtype(df['min_latency']):
    df['min_latency'] = pd.to_timedelta(df['min_latency'].dt.nanosecond, 
                                        unit='ns')

",pandas
how can i find the column containing the third nan value in each row of a dataframe in pandas,"I have been given a problem to solve for my assignment. Here's the description for my problem:
In the cell below, you have a DataFrame df that consists of 10 columns of floating-point numbers. Exactly 5 entries in each row are NaN values.
For each row of the DataFrame, find the column which contains the third NaN value.
You should return a Series of column labels: e, c, d, h, d
nan = np.nan

data = [[0.04,  nan,  nan, 0.25,  nan, 0.43, 0.71, 0.51,  nan,  nan],
        [ nan,  nan,  nan, 0.04, 0.76,  nan,  nan, 0.67, 0.76, 0.16],
        [ nan,  nan, 0.5 ,  nan, 0.31, 0.4 ,  nan,  nan, 0.24, 0.01],
        [0.49,  nan,  nan, 0.62, 0.73, 0.26, 0.85,  nan,  nan,  nan],
        [ nan,  nan, 0.41,  nan, 0.05,  nan, 0.61,  nan, 0.48, 0.68]]

columns = list('abcdefghij')

df = pd.DataFrame(data, columns=columns)

# write a solution to the question here

This is my solution:
result = df.apply(lambda row: row.isna().idxmax(), axis=1)
print(result)

My code returns b, a, a, a, b, a, whereas the desired output is e, c, d, h, d. My solution returns the index of the first NaN but according to the question I want the third NaN in each row. How can i do that? Or is there an alternate solution that gives the desired output.
","idxmax was a good approach, you can combine this with a mask that indicates the 3rd NaN, for this use cumsum:
m = df.isna()

out = (m & m.cumsum(axis=1).eq(3)).idxmax(axis=1)

since idxmax always returns the first value if there are several maxes, this could be simplified to:
out = df.isna().cumsum(axis=1).eq(3).idxmax(axis=1)

Output:
0    e
1    c
2    d
3    h
4    d
dtype: object

Intermediates:
# m
       a     b      c      d      e      f      g      h      i      j
0  False  True   True  False   True  False  False  False   True   True
1   True  True   True  False  False   True   True  False  False  False
2   True  True  False   True  False  False   True   True  False  False
3  False  True   True  False  False  False  False   True   True   True
4   True  True  False   True  False   True  False   True  False  False

# m.cumsum(axis=1)
   a  b  c  d  e  f  g  h  i  j
0  0  1  2  2  3  3  3  3  4  5
1  1  2  3  3  3  4  5  5  5  5
2  1  2  2  3  3  3  4  5  5  5
3  0  1  2  2  2  2  2  3  4  5
4  1  2  2  3  3  4  4  5  5  5

# m.cumsum(axis=1).eq(3)
       a      b      c      d      e      f      g      h      i      j
0  False  False  False  False   True   True   True   True  False  False
1  False  False   True   True   True  False  False  False  False  False
2  False  False  False   True   True   True  False  False  False  False
3  False  False  False  False  False  False  False   True  False  False
4  False  False  False   True   True  False  False  False  False  False

# m & m.cumsum(axis=1).eq(3)
       a      b      c      d      e      f      g      h      i      j
0  False  False  False  False   True  False  False  False  False  False
1  False  False   True  False  False  False  False  False  False  False
2  False  False  False   True  False  False  False  False  False  False
3  False  False  False  False  False  False  False   True  False  False
4  False  False  False   True  False  False  False  False  False  False

Alternatively, if you can't assume a specific number of NaNs in each row, using melt + groupby.nth:
N = 3
out = (df.melt(ignore_index=False)              # reshape to long
         .loc[lambda x: x['value'].isna()]      # only keep NaNs
         .groupby(level=0)['variable'].nth(N-1) # keep 3rd row per group
      )

Output:
1    c
2    d
4    d
0    e
3    h
Name: variable, dtype: object

",pandas
combine two pandas dataframes sidebyside with resulting length being maxdf1 df2,"Essentially, what I described in the title.
I am trying to combine two dataframes (i.e. df1 & df2) where they have different amounts of columns (df1=3, df2=8) with varying row lengths. (The varying row lengths stem from me having a script that breaks main two excel lists into blocks based on a date condition).
My goal is to combine the two length-varying dataframes into one dataframe, where they both start at index 0 instead of one after the other.
What is currently happening:
     A  B  C  D
0    1  2 nan nan
1    3  4 nan nan
2  nan nan 5  6
3  nan nan 7  8
4  nan nan 9  10

This is how I would like it to be:
     A  B  C  D
0    1  2  5  6
1    3  4  7  8
2  nan nan 9  10

I tried many things, but this is the last code that worked (but with wrong results):
import pandas as pd


hours_df = pd.read_excel(""hours.xlsx"").fillna("""")
hours_columns = hours_df.columns

material_df = pd.read_excel(""material.xlsx"").fillna("""")
material_df = material_df.rename(columns={'Date': 'Material Date'})
material_columns = material_df.columns


breaker = False

temp = []
combined_df = pd.DataFrame()

last_date = ""1999-01-01""


for _, row in hours_df.iterrows():
    if row[""Date""] != """":
        block_df = pd.DataFrame(temp, columns=hours_columns)

        if temp:
            cell_a1 = block_df.iloc[0,0]

            filtered_df = material_df.loc[
                    (material_df[""Material Date""] < cell_a1) &
                    (material_df[""Material Date""] >= last_date)]

            last_date = cell_a1

            combined_block = pd.concat([block_df, filtered_df], axis=1)

            combined_df = pd.concat([combined_df, combined_block], ignore_index=True)

        temp = []

    temp.append(row)

if temp:
    block_df = pd.DataFrame(temp, columns=hours_columns)
    combined_df = pd.concat([combined_df, block_df], ignore_index=True)

print(combined_df)

I am not getting any errors. Just stacked output -- like the one I showed above.
","Your issue arises because you are concatenating dataframes vertically rather than horizontally. To achieve the desired output, you need to align rows from df1 and df2 with the same index and then concatenate horizontally.
Here’s the updated code that would produce the output you want. I have added comments on the places where I've made the changes.
import pandas as pd

# Loading dataframes
hours_df = pd.read_excel(""hours.xlsx"").fillna("""")
material_df = pd.read_excel(""material.xlsx"").fillna("""")
material_df = material_df.rename(columns={'Date': 'Material Date'})

temp = []
combined_df = pd.DataFrame()
last_date = ""1999-01-01""

for _, row in hours_df.iterrows():
    if row[""Date""] != """":
        block_df = pd.DataFrame(temp, columns=hours_df.columns)

        if temp:
            # Filter material_df based on the date range
            first_date_in_block = block_df.iloc[0, 0]
            filtered_df = material_df.loc[
                (material_df[""Material Date""] < first_date_in_block) &
                (material_df[""Material Date""] >= last_date)
            ]
            last_date = first_date_in_block

            # Reset indices for horizontal alignment
            block_df.reset_index(drop=True, inplace=True)
            filtered_df.reset_index(drop=True, inplace=True)

            # Concatenate horizontally
            combined_block = pd.concat([block_df, filtered_df], axis=1)
            combined_df = pd.concat([combined_df, combined_block], ignore_index=True)

        temp = []

    temp.append(row)

# Handling the remaining block
if temp:
    block_df = pd.DataFrame(temp, columns=hours_df.columns)
    combined_df = pd.concat([combined_df, block_df], ignore_index=True)

print(combined_df)

",pandas
calculate average daily amount of time between two news posted by a unique source,"I have a pandas dataframe (I have simplified table with one date showing in example), I want to calculate the average daily amount of time between two news posted by a unique Source
Input
source          date           time     
Investing.com   2017-05-11     08:00:00     
Investing.com   2017-05-11     12:00:00
Investing.com   2017-05-11     16:00:00 
yahoo.com       2017-05-11     09:00:00 
yahoo.com       2017-05-11     12:00:00
yahoo.com       2017-05-11     15:00:00
yahoo.com       2017-05-12     06:00:00 
yahoo.com       2017-05-12     12:00:00
yahoo.com       2017-05-12     18:00:00  

Desired_output
source          date           Average_Daily_time   
Investing.com   2017-05-11     04:00:00      
yahoo.com       2017-05-11     03:00:00
yahoo.com       2017-05-12     06:00:00 

My Attempt
I merged the datetime in one timestamp and called it datetime
df.sort_values('datetime').groupby('source')['datetime'].apply(lambda x: x.diff().dt.seconds.mean()/60)

Issue
It calculates average time for all dates combined, not separate dates. How to show average time for separate dates?
","Convert the time column to timedelta, then group the dataframe by source and date and aggregate time using a lambda function to calculate the mean of diff between rows
df['time'] = pd.to_timedelta(df['time'])
(
    df.groupby(['source', 'date'])['time']
      .agg(lambda d: d.diff().mean()).reset_index(name='avg')
)


          source        date             avg
0  Investing.com  2017-05-11 0 days 04:00:00
1      yahoo.com  2017-05-11 0 days 03:00:00
2      yahoo.com  2017-05-12 0 days 06:00:00

",pandas
empty plotly candlestick chart with yfinancedownload,"I am trying to plot a simple Candlestick chart from OHLCV data retrieved by yfinance.
This is my code:
import yfinance as yf
import pandas as pd
import plotly.graph_objects as go
from datetime import datetime

tf = '1d'  # Time frame (daily)
asset = 'AAPL'  # Asset ticker (e.g., Apple)
start = '2019-01-01'  # Start date
end = datetime.now().strftime('%Y-%m-%d')  # End date is current date

df = yf.download(asset, start=start, end=end, interval=tf)

df['pct_chg'] = df['Close'].pct_change() * 100

df.index.name = 'timestamp'

# now plot the chart

hover_text = [f""Open: {open}<br>Close: {close}<br>Pct: {pct_chg:.2f}%"" for open, close, pct_chg in zip(df['Open'], df['Close'], df['pct_chg'])]

# Create a candlestick chart using Plotly
fig = go.Figure(data=[go.Candlestick(
        x=df.index,
        open=df['Open'],
        high=df['High'],
        low=df['Low'],
        close=df['Close'],
        hovertext=hover_text,
        hoverinfo='text'
    )])

# Update layout
fig.update_layout(
        title='Candlestick chart',
        xaxis_title='Date',
        yaxis_title='Price',
        xaxis_rangeslider_visible=False,
        template='plotly_dark')

# Show the plot
fig.show()

Data is correctly downloaded. However the graph does not show any candle.
","Actually Pandas will use MultiIndexing if you put data downloaded from yahoo finance into a dataframe.
display(df) gives:

To remove the Ticker column multi-index which is not needed, just drop it:
df.columns = df.columns.droplevel(1)

Then you get a regular dataframe and can plot it the way you did it:

",pandas
get list from pandas dataframe column or row,"I have a dataframe df imported from an Excel document like this:
cluster load_date   budget  actual  fixed_price
A   1/1/2014    1000    4000    Y
A   2/1/2014    12000   10000   Y
A   3/1/2014    36000   2000    Y
B   4/1/2014    15000   10000   N
B   4/1/2014    12000   11500   N
B   4/1/2014    90000   11000   N
C   7/1/2014    22000   18000   N
C   8/1/2014    30000   28960   N
C   9/1/2014    53000   51200   N

I want to be able to return the contents of column 1 df['cluster'] as a list, so I can run a for-loop over it, and create an Excel worksheet for every cluster.
Is it also possible to return the contents of a whole column or row to a list? e.g.
list = [], list[column1] or list[df.ix(row1)]

","Pandas DataFrame columns are Pandas Series when you pull them out, which you can then call x.tolist() on to turn them into a Python list. Alternatively you cast it with list(x).
import pandas as pd

data_dict = {'one': pd.Series([1, 2, 3], index=['a', 'b', 'c']),
             'two': pd.Series([1, 2, 3, 4], index=['a', 'b', 'c', 'd'])}

df = pd.DataFrame(data_dict)

print(f""DataFrame:\n{df}\n"")
print(f""column types:\n{df.dtypes}"")

col_one_list = df['one'].tolist()

col_one_arr = df['one'].to_numpy()

print(f""\ncol_one_list:\n{col_one_list}\ntype:{type(col_one_list)}"")
print(f""\ncol_one_arr:\n{col_one_arr}\ntype:{type(col_one_arr)}"")

Output:
DataFrame:
   one  two
a  1.0    1
b  2.0    2
c  3.0    3
d  NaN    4

column types:
one    float64
two      int64
dtype: object

col_one_list:
[1.0, 2.0, 3.0, nan]
type:<class 'list'>

col_one_arr:
[ 1.  2.  3. nan]
type:<class 'numpy.ndarray'>

",pandas
remove all rows in pandas dataframe with n or more consecutive nans,"corollary to this question: replace values in pandas column when N number of NaNs exist in another column
         a         b         c     d           e
2018-05-25  0.000381  0.264318     land    2018-05-25
2018-05-26  0.000000  0.264447     land    2018-05-26
2018-05-27  0.000000  0.264791     NaN           NaT
2018-05-28  0.000000  0.265253     NaN           NaT
2018-05-29  0.000000  0.265720     NaN           NaT
2018-05-30  0.000000  0.266066     land    2018-05-30
2018-05-31  0.000000  0.266150     NaN           NaT
2018-06-01  0.000000  0.265816     NaN           NaT
2018-06-02  0.000000  0.264892     land    2018-06-02
2018-06-03  0.000000  0.263191     NaN           NaT
2018-06-04  0.000000  0.260508     land    2018-06-04
2018-06-05  0.000000  0.256619     NaN           NaT
2018-06-06  0.000000  0.251286     NaN           NaT
2018-06-07  0.000000  0.244250     NaN           NaT
2018-06-08  0.000000  0.235231     NaN           NaT
2018-06-09  0.000000  0.223932     land    2018-06-09

I want to remove all rows where there is a NaN in the 4th column (d) 3 or more times. The output should be:
         a         b         c     d           e
2018-05-25  0.000381  0.264318     land    2018-05-25
2018-05-26  0.000000  0.264447     land    2018-05-26
2018-05-30  0.000000  0.266066     land    2018-05-30
2018-05-31  0.000000  0.266150     NaN           NaT
2018-06-01  0.000000  0.265816     NaN           NaT
2018-06-02  0.000000  0.264892     land    2018-06-02
2018-06-03  0.000000  0.263191     NaN           NaT
2018-06-04  0.000000  0.260508     land    2018-06-04
2018-06-09  0.000000  0.223932     land    2018-06-09

From that question, I tried this:
    threshold = 3
    mask = df.d.notna()
    df.loc[(~mask).groupby(mask.cumsum()).transform('cumsum') < threshold, 'c'] = np.nan
    df = df[np.isfinite(df['c'])]

but it does not work
","Create helper Series a by consecutive values and transform size, last filter by boolean indexing:
mask = df.d.notna()
a = mask.ne(mask.shift()).cumsum()

df = df[(a.groupby(a).transform('size') < 3) | mask]
print (df)
             a         b         c     d           e
0   2018-05-25  0.000381  0.264318  land  2018-05-25
1   2018-05-26  0.000000  0.264447  land  2018-05-26
5   2018-05-30  0.000000  0.266066  land  2018-05-30
6   2018-05-31  0.000000  0.266150   NaN         NaT
7   2018-06-01  0.000000  0.265816   NaN         NaT
8   2018-06-02  0.000000  0.264892  land  2018-06-02
9   2018-06-03  0.000000  0.263191   NaN         NaT
10  2018-06-04  0.000000  0.260508  land  2018-06-04
15  2018-06-09  0.000000  0.223932  land  2018-06-09

Detail:
print (a)
0     1
1     1
2     2
3     2
4     2
5     3
6     4
7     4
8     5
9     6
10    7
11    8
12    8
13    8
14    8
15    9
Name: d, dtype: int32


print (a.groupby(a).transform('size'))
0     2
1     2
2     3
3     3
4     3
5     1
6     2
7     2
8     1
9     1
10    1
11    4
12    4
13    4
14    4
15    1
Name: d, dtype: int64

",pandas
convert pandas series to dataframe,"I have a Pandas series sf:
email
email1@email.com    [1.0, 0.0, 0.0]
email2@email.com    [2.0, 0.0, 0.0]
email3@email.com    [1.0, 0.0, 0.0]
email4@email.com    [4.0, 0.0, 0.0]
email5@email.com    [1.0, 0.0, 3.0]
email6@email.com    [1.0, 5.0, 0.0]

And I would like to transform it to the following DataFrame:
index | email             | list
_____________________________________________
0     | email1@email.com  | [1.0, 0.0, 0.0]
1     | email2@email.com  | [2.0, 0.0, 0.0]
2     | email3@email.com  | [1.0, 0.0, 0.0]
3     | email4@email.com  | [4.0, 0.0, 0.0]
4     | email5@email.com  | [1.0, 0.0, 3.0]
5     | email6@email.com  | [1.0, 5.0, 0.0]

I found a way to do it, but I doubt it's the more efficient one:
df1 = pd.DataFrame(data=sf.index, columns=['email'])
df2 = pd.DataFrame(data=sf.values, columns=['list'])
df = pd.merge(df1, df2, left_index=True, right_index=True)

","Rather than create 2 temporary dfs you can just pass these as params within a dict using the DataFrame constructor:
pd.DataFrame({'email':sf.index, 'list':sf.values})

There are lots of ways to construct a df, see the docs
",pandas
getting each element in pandas crosstab,"I have a dataset given below:
a,b,c
1,1,1
1,1,1
1,1,2
2,1,2
2,1,1
2,2,1

I created crosstab with pandas:
 cross_tab = pd.crosstab(index=a, columns=[b, c], rownames=['a'], colnames=['b', 'c'])

my crosstab is given as an output:
b        1     2
c        1  2  1
a        
1        2  1  0
2        1  1  1

I want to iterate over this crosstab for given each a,b and c values. How can I get values such as cross_tab[a=1][b=1, c=1]? Thank you.
","You can use slicers:
a,b,c = 1,1,1
idx = pd.IndexSlice
print (cross_tab.loc[a, idx[b,c]])
2

You can also reshape df by DataFrame.unstack, reorder_levels and then use loc:
a = cross_tab.unstack().reorder_levels(('a','b','c'))
print (a)
a  b  c
1  1  1    2
2  1  1    1
1  1  2    1
2  1  2    1
1  2  1    0
2  2  1    1
dtype: int64

print (a.loc[1,1,1])
2

",pandas
multiindex quotget_level_valuesquotfunction for arbitrarily many levels,"Is there a way to construct a function that uses ""get_level_values"" an arbitrarily number of times and returns the sliced dataframe? An example can explain my need.
Multiindex:
arrays = [['bar', 'bar', 'bar', 'baz', 'baz', 'foo', 'foo','foo','qux', 'qux'],
          ['one', 'two', 'three', 'one', 'four', 'one', 'two', 'eight','one', 'two'],
          ['green', 'green', 'blue', 'blue', 'black', 'black', 'orange', 'green','blue', 'black']  ]
s = pd.DataFrame(np.random.randn(10), index=arrays)
s.index.names = ['p1','p2','p3']

s
                         0
p1  p2    p3              
bar one   green  -0.676472
    two   green  -0.030377
    three blue   -0.957517
baz one   blue    0.710764
    four  black   0.404377
foo one   black  -0.286358
    two   orange -1.620832
    eight green   0.316170
qux one   blue   -0.433310
    two   black   1.127754


Now, this is is the function I want to create: 
def my_func(df,levels, values):
    # Code using get_level_values
    return ret

# Example use
my_func(s, ['p1'],['bar'])

p1  p2    p3              
bar one   green  -0.676472
    two   green  -0.030377
    three blue   -0.957517

my_func(s, ['p1','p2'],['bar','one'])

p1  p2    p3              
bar one   green  -0.676472


Above my_func(['p1'],['bar']) returns s.loc[s.index.get_level_values('p1')=='bar'] and my_func(['p1','p2'],['bar','one']) returns s.loc[(s.index.get_level_values('p1')=='bar') & (s.index.get_level_values('p2')=='one')]
So, I want to put a list of arbitrarily many levels and a list of the same number of values and return the sliced dataframe.
Any help is much appreciated!
","Try this and see if it works for you. Since your multi-index has names, it is easier using query for your function:
def my_func(df,levels, values):
    # Code using query
    m = dict(zip(levels,values))
    #create expression to use in the query method
    expr = "" and "".join(f""{k}=={v!r}"" for k,v in m.items())
    ret = df.query(expr)
    return ret


#function application
my_func(s, ['p1'],['bar'])

                    0
p1  p2  p3  
bar one green   -0.087366
    two green   1.126620
  three blue    0.868515


my_func(s, ['p1','p2'],['bar','one'])

                    0
p1  p2  p3  
bar one green   -0.087366

",pandas
pandas series subtract pandas dataframe strange result,"I'm wondering why pandas Series subtract a pandas dataframe produce such a strange result.
df = pd.DataFrame(np.arange(10).reshape(2, 5), columns='a-b-c-d-e'.split('-'))
df.max(axis=1) - df[['b']]

What are the steps for pandas to produce the result?
    b   0   1
0 NaN NaN NaN
1 NaN NaN NaN

","By default an operation between a DataFrame and a Series is broadcasted on the DataFrame by column, over the rows. This makes it easy to perform operations combining a DataFrame and aggregation per column:
# let's subtract the DataFrame to its max per column
df.max(axis=0) - df[['b']]

    a  b   c   d   e
b NaN  5 NaN NaN NaN
1 NaN  0 NaN NaN NaN

Here, since you're aggregating per row, this is no longer possible. You should use rsub with the parameter axis=0:
df[['b']].rsub(df.max(axis=1), axis=0)

Output:
   b
0  3
1  3

Note that using two Series would also align the values:
df.max(axis=1) - df['b']

Output:
0    3
1    3
dtype: int64

Why 3 columns with df.max(axis=1) - df[['b']]?
First, let's have a look at each operand:
# df.max(axis=1)
0    4
1    9
dtype: int64

# df[['b']]
   b
0  1
1  6

Since df[['b']] is 2D (DataFrame), and df.max(axis=1) is 1D (Series), df.max(axis=1) will be used as if it was a ""wide"" DataFrame:
# df.max(axis=1).to_frame().T
   0  1
0  4  9

There are no columns in common, thus the output is only NaNs with the union of column names ({'b'}|{0, 1} -> {'b', 0, 1}).
If you replace the NaNs that are used in the operation by 0 this makes it obvious how the values are used:
df[['b']].rsub(df.max(axis=1).to_frame().T, fill_value=0)

     b    0    1
0 -1.0  4.0  9.0
1 -6.0  NaN  NaN

Now let's check a different example in which one of the row indices has the same name as one of the selected columns:
df = pd.DataFrame(np.arange(10).reshape(2, 5),
                  columns=['a', 'b', 'c', 'd', 'e'],
                  index=['b', 0]
                 )
df.max(axis=1) - df[['b']]

Now the output only has 2 columns, b the common indice and 1 the second index in the Series ({'b', 1}|{'b'} -> {'b', 1}):
    1  b
b NaN  3
1 NaN -2

",pandas
what does the term quotbroadcastingquot mean in pandas documentation,"I'm reading through the Pandas documentation, and the term ""broadcasting"" is used extensively, but never really defined or explained.
What does it mean?
","So the term broadcasting comes from numpy, simply put it explains the rules of the output that will result when you perform operations between n-dimensional arrays (could be panels, dataframes, series) or scalar values.
Broadcasting using a scalar value
So the simplest case is just multiplying by a scalar value:
In [4]:
s = pd.Series(np.arange(5))
s

Out[4]:
0    0
1    1
2    2
3    3
4    4
dtype: int32

In [5]:    
s * 10

Out[5]:
0     0
1    10
2    20
3    30
4    40
dtype: int32

and we get the same expected results with a dataframe:
In [6]:    
df = pd.DataFrame({'a':np.random.randn(4), 'b':np.random.randn(4)})
df

Out[6]:
          a         b
0  0.216920  0.652193
1  0.968969  0.033369
2  0.637784  0.856836
3 -2.303556  0.426238

In [7]:    
df * 10

Out[7]:
           a         b
0   2.169204  6.521925
1   9.689690  0.333695
2   6.377839  8.568362
3 -23.035557  4.262381

So what is technically happening here is that the scalar value has been broadcasted along the same dimensions of the Series and DataFrame above.
Broadcasting using a 1-D array
Say we have a 2-D dataframe of shape 4 x 3 (4 rows x 3 columns) we can perform an operation along the x-axis by using a 1-D Series that is the same length as the row-length:
In [8]:
df = pd.DataFrame({'a':np.random.randn(4), 'b':np.random.randn(4), 'c':np.random.randn(4)})
df

Out[8]:
          a         b         c
0  0.122073 -1.178127 -1.531254
1  0.011346 -0.747583 -1.967079
2 -0.019716 -0.235676  1.419547
3  0.215847  1.112350  0.659432

In [26]:    
df.iloc[0]

Out[26]:
a    0.122073
b   -1.178127
c   -1.531254
Name: 0, dtype: float64

In [27]:    
df + df.iloc[0]

Out[27]:
          a         b         c
0  0.244146 -2.356254 -3.062507
1  0.133419 -1.925710 -3.498333
2  0.102357 -1.413803 -0.111707
3  0.337920 -0.065777 -0.871822

the above looks funny at first until you understand what is happening, I took the first row of values and added this row-wise to the df, it can be visualised using this pic (sourced from scipy):

The general rule is this:

In order to broadcast, the size of the trailing axes for both arrays
  in an operation must either be the same size or one of them must be
  one.

So if I tried to add a 1-D array that didn't match in length, say one with 4 elements, unlike numpy which will raise a ValueError, in Pandas you'll get a df full of NaN values:
In [30]:
df + pd.Series(np.arange(4))

Out[30]:
    a   b   c   0   1   2   3
0 NaN NaN NaN NaN NaN NaN NaN
1 NaN NaN NaN NaN NaN NaN NaN
2 NaN NaN NaN NaN NaN NaN NaN
3 NaN NaN NaN NaN NaN NaN NaN

Now some of the great things about pandas is that it will try to align using existing column names and row labels, this can get in the way of trying to perform a fancier broadcasting like this:

In [55]:
df[['a']] + df.iloc[0]

Out[55]:
          a   b   c
0  0.244146 NaN NaN
1  0.133419 NaN NaN
2  0.102357 NaN NaN
3  0.337920 NaN NaN

In the above I use double subscripting to force the shape to be (4,1) but we see a problem when trying to broadcast using the first row as the column alignment only aligns on the first column. To get the same form of broadcasting to occur like the diagram above shows we have to decompose to numpy arrays which then become anonymous data:
In [56]:
df[['a']].values + df.iloc[0].values

Out[56]:
array([[ 0.24414608, -1.05605392, -1.4091805 ],
       [ 0.13341899, -1.166781  , -1.51990758],
       [ 0.10235701, -1.19784299, -1.55096957],
       [ 0.33792013, -0.96227987, -1.31540645]])

It's also possible to broadcast in 3-dimensions but I don't go near that stuff often but the numpy, scipy and pandas book have examples that show how that works.
Generally speaking the thing to remember is that aside from scalar values which are simple, for n-D arrays the minor/trailing axes length must match or one of them must be 1.
Update
it seems that the above now leads to ValueError: Unable to coerce to Series, length must be 1: given 3 in latest version of pandas 0.20.2
so you have to call .values on the df first:
In[42]:
df[['a']].values + df.iloc[0].values

Out[42]: 
array([[ 0.244146, -1.056054, -1.409181],
       [ 0.133419, -1.166781, -1.519908],
       [ 0.102357, -1.197843, -1.55097 ],
       [ 0.33792 , -0.96228 , -1.315407]])

To restore this back to the original df we can construct a df from the np array and pass the original columns in the args to the constructor:
In[43]:
pd.DataFrame(df[['a']].values + df.iloc[0].values, columns=df.columns)

Out[43]: 
          a         b         c
0  0.244146 -1.056054 -1.409181
1  0.133419 -1.166781 -1.519908
2  0.102357 -1.197843 -1.550970
3  0.337920 -0.962280 -1.315407

",pandas
what does axis in pandas mean,"Here is my code to generate a dataframe:
import pandas as pd
import numpy as np

dff = pd.DataFrame(np.random.randn(1, 2), columns=list('AB'))

then I got the dataframe:
          A        B
0  0.626386  1.52325

When I type the command dff.mean(axis=1), I get:
0    1.074821
dtype: float64

According to the reference of pandas, axis=1 stands for columns and I expect the result of the command to be
A    0.626386
B    1.523255
dtype: float64

So what does axis in pandas mean?
","It specifies the axis along which the means are computed. By default axis=0. This is consistent with the numpy.mean usage when axis is specified explicitly (in numpy.mean, axis==None by default, which computes the mean value over the flattened array) , in which axis=0 along the rows (namely, index in pandas), and axis=1 along the columns. For added clarity, one may choose to specify axis='index' (instead of axis=0) or axis='columns' (instead of axis=1).
          A        B
0  0.626386  1.52325  → → axis=1 → →
          ↓        ↓
          ↓ axis=0 ↓
          ↓        ↓

",pandas
is axis1 meaning rowwise a special case for the apply method in pandas,"Apparently, this code is supposed to execute the function row-wise for a given dataset. However, according to my knowledge, row-wise operations are done by axis=0, not axis=1.
df.apply(func, axis=1)

Does the apply method somehow operate differently and I just need to know that this is an exception?
","axis=0 means the functions works along the column and axis=1 means the functions works on the rows. For example
data = pd.DataFrame([[1,2], [3,4]])
data.sum(axis=0)

The result is
0    4
1    6

which is sum along the column
",pandas
should i manually patch the pandas dataframequery vulnerability or wait for an official update,"I'm currently addressing the Pandas DataFrame.query() Code Injection vulnerability, which allows arbitrary code execution if unsafe user input is processed by the .query() method. I understand this issue arises because the query() method can execute expressions within the context of the DataFrame, potentially leading to security risks.
My questions are as follows:

Should I manually patch the vulnerability?

For example, I can override the query() method in the Pandas source code to validate expressions using Python's ast module and block unsafe constructs. Is this a recommended approach, or does it pose potential risks (e.g., breaking functionality, maintaining the patch long-term)?

How frequently does the Pandas team release updates or patches for vulnerabilities like this?

Should I wait for an official update to address this issue? Are there any best practices for monitoring when a fix becomes available?

What is the general practice for addressing such library-level vulnerabilities in production environments?

Is it better to apply temporary mitigations (like validating input in my application code) and wait for the library maintainers, or should I fork/patch the library for immediate resolution?
Any insights, especially from those with experience in maintaining secure Python applications, would be greatly appreciated!
","
Should I wait for an official update to address this issue? Are there any best practices for monitoring when a fix becomes available?

The 'vulnerability' in question strikes me as basically unfixable, so I would not expect a fix to become available.
The method DataFrame.query() is designed to allow a user to run essentially arbitrary Python code to filter a DataFrame. Passing untrusted code to DataFrame.query() is exactly as dangerous as passing untrusted code to eval().
I asked about this on the Pandas issue tracker, and this was the response from one of Pandas's contributors:

Q: My question is about Pandas's security model. What security guarantees does Pandas make about DataFrame.query() with an attacker-controlled expr? My intuition about this is ""none, don't do that,"" but I'm wondering what the Pandas project thinks.
A: This is indeed my take, both query and eval should be used with string literals and not with strings provided by or derived from untrusted user input.

(Source.)


For example, I can override the query() method in the Pandas source code to validate expressions using Python's ast module and block unsafe constructs.

That strikes me as very difficult to do in the general case. If you look at this thread, you'll see example after example of people proposing a way to sandbox Python execution, and it turns out to not be a perfect sandbox because of a feature of Python that the answer doesn't take into account. For that reason, I think that disallowing unsafe constructs is a doomed approach, because it requires anticipating every unsafe construct.
Rather, I think you should come up with a list of safe constructs, and only allow those.
For example, you could compare the expression to a known-good list of expressions, and only allow those.
allowlist = [
    'num > 0',
    'num == 0',
    'num < 0',
]
if expr in allowlist:
    result = df.query(expr)
else:
    raise Exception('Illegal expr value')

This restricts the strings that can be passed to DataFrame.query() to one of these pre-approved values.


Is it better to apply temporary mitigations (like validating input in my application code) and wait for the library maintainers, or should I fork/patch the library for immediate resolution?

That's hard to answer in general. To me, I would think about three factors:

How hard is it to validate the input within my application?
How complex or intrusive is the change to the library?
How difficult will it be to keep this patch up to date?

In this specific case, I would suggest validating input within application code, unless #1 is really difficult.
",pandas
delete empty rows before column names start on excel spreadsheet using python,"My column names on a spreadsheet start on row 2, row 1 is completely blank. 
When I try to delete it, it also deletes column names on row 2 and only keeps first column name. 
Any idea of what I am doing wrong?
Original file
|          |          |
| Column A | Column B |
| Cell 1   | Cell 2   |
| Cell 3   | Cell 4   |

Expected output



Column A
Column B




Cell 1
Cell 2


Cell 3
Cell 4



Current output



Column A




Cell 1


Cell 3



I have tried: 
openpyxl
sheet.delete_rows(1)

pandas
df = df.drop(index=0)

","If you have the following data layout and want to delete the first row;

The following code shown for using Openpyxl and Pandas, is all that's needed.
from openpyxl import load_workbook
import pandas as pd

# Openpyxl
wb = load_workbook('foo.xlsx')

wb['Sheet1'].delete_rows(1)

wb.save('foo_openpyxl.xlsx')


# Pandas
df = pd.read_excel('foo.xlsx')
df.to_excel('foo_pandas.xlsx', header=False, index=False)


For Pandas since row 1 will be the default header, writing back to Excel without the header will remove row 1.
The output from both is the same;

",pandas
using external library in pyspark udf pickle error,"I'm trying the following code:
import pandas as pd
from pymorphy2 import MorphAnalyzer
from pyspark.sql import SparkSession
from pyspark.sql import types as T
from pyspark.sql import functions as F

spark = SparkSession.builder.appName(""udf"").getOrCreate()

def gender(s):
    m = MorphAnalyzer()
    return m.parse(s)[0].tag.gender

gen = F.udf(gender, T.StringType())

df = spark.createDataFrame(pd.DataFrame({""name"": [""кирилл"", ""вавила""]}))

df.select(gen(""name"").alias(""gender"")).show()

and more or less expectedly getting the following error message:
ERROR Executor: Exception in task 2.0 in stage 29.0 (TID 151)
net.razorvine.pickle.PickleException: expected zero arguments for construction of ClassDict (for pyspark.cloudpickle.cloudpickle._make_skeleton_class). This happens when an unsupported/unregistered class is being unpickled that requires construction arguments. Fix it by registering a custom IObjectConstructor for this class.
    at net.razorvine.pickle.objects.ClassDictConstructor.construct(ClassDictConstructor.java:23)
    at net.razorvine.pickle.Unpickler.load_reduce(Unpickler.java:759)
    at net.razorvine.pickle.Unpickler.dispatch(Unpickler.java:199)

What could be the easiest way to circumvent the error (if any) ?
","You could use pandas_udf which is vectorized (more efficient compared to a regular udf).
import pandas as pd
from pymorphy2 import MorphAnalyzer
from pyspark.sql import SparkSession, types as T, functions as F

spark = SparkSession.builder.appName(""udf"").getOrCreate()

@F.pandas_udf(T.StringType())
def gender(s: pd.Series) -> pd.Series:
    return s.apply(lambda x: MorphAnalyzer().parse(x)[0].tag.gender)

df = spark.createDataFrame(pd.DataFrame({""name"": [""кирилл"", ""вавила"", ""софия""]}))
df.withColumn(""gender"", gender(""name"")).show()
# +------+------+
# |  name|gender|
# +------+------+
# |кирилл|  masc|
# |вавила|  masc|
# | софия|  femn|
# +------+------+

In some systems, you may not need pyspark.sql.types library solely for pandas_udf, the function could be defined like this:
@F.pandas_udf('string')
def gender(s: pd.Series) -> pd.Series:
    return s.apply(lambda x: MorphAnalyzer().parse(x)[0].tag.gender)

Update: You could also use Iterator class in order to create MorphAnalyzer object once for every batch.
import pandas as pd
from pymorphy2 import MorphAnalyzer
from pyspark.sql import SparkSession, types as T, functions as F
from typing import Iterator

spark = SparkSession.builder.appName(""udf"").getOrCreate()

@F.pandas_udf(T.StringType())
def gender(s: Iterator[pd.Series]) -> Iterator[pd.Series]:
    m = MorphAnalyzer()
    for e in s:
        yield e.apply(lambda x: m.parse(x)[0].tag.gender)

",pandas
parsing units out of column,"I've got some data I'm reading into Python using Pandas and want to keep track of units with the Pint package.  The values have a range of scales, so have mixed units, e.g. lengths are mostly meters but some are centimeters.
For example the data:
what,length
foo,5.3 m
bar,72 cm

and I'd like to end up with the length column in some form that Pint understands.  Pint's Pandas integration suggests that it only supports the whole column having the same datatype, which seems reasonable.  I'm happy with some arbitrary unit being picked (e.g. the first, most common, or just SI base unit) and everything expressed in terms of that.
I was expecting some nice way of getting from the data I have to what's expected, but I don't see anything.
import pandas as pd
import pint_pandas

length = pd.Series(['5.3 m', ""72 cm""], dtype='pint[m]')

Doesn't do the correct thing at all, for example:
length * 2

outputs
0    5.3 m5.3 m
1    72 cm72 cm
dtype: pint[meter]

so it's just leaving things as strings.  Calling length.pint.convert_object_dtype() doesn't help and everything stays as strings.
","Going through the examples, it looks like pint_pandas is expecting numbers rather than strings. You can use apply to do the conversion:
from pint import UnitRegistry
ureg = UnitRegistry()

df[""length""].apply(lambda i: ureg(i)).astype(""pint[m]"")

However, why keep the column as Quantity objects instead of just plain float numbers?
",pandas
how to count the occurrences of a value in a data frame,"I have a data frame [df] that looks like this but much lagger:
title of the novel                author          publishing year   mentionned cities   
0   Beasts and creatures        Bruno Ivory             2021           New York 
0   Monsters                    Renata Mcniar           2023           New York 
0   At risk                     Charles Dobi            2020           London   
0   Manuela and Ricardo         Lucas Zacci             2022           Rio de Janeiro
0   War against the machine     Angelina Trotter        1999           Rio de Janeiro

I would like to add another column with the objective of counting all the occurences of the cities. The problem is that I want to maintain the year of that occurrence, as I work with history. In other words, it is important for me to be able to know when the city was mentionned.
The expected outcome would look like this:
title of the novel     author    publishing year    mentionned cities       Counter
0   Beasts and creatures        Bruno Ivory             2021           New York   1 
0   Monsters                    Renata Mcniar           2022           New York   2
0   At risk                     Charles Dobi            2020           London     1
0   Manuela and Ricardo         Lucas Zacci             2022           Rio de Janeiro 1
0   War against the machine     Angelina Trotter        1999           Rio de Janeiro 2

So far, I have just managed to count all the occurrences, but I could not relate it to the publishing years. The code I am using is:
df ['New York'] = df.eq('New York').sum().to_frame().T

Can someone help me, please?
edit:
I tried joining two dataframes and I got something interesting but not what I really wanted. The problem is that it does not keep the Publishing year on track.
    d[f'counter'] = d.fgroupby('mentionned cities')['mentionned cities'].transform('counter')

result = pd.concat([df['New York'], df], axis=1, join='inner')
    display(result)

Output:
title of the novel     author    publishing year    mentionned cities       Counter
    0   Beasts and creatures        Bruno Ivory             2021           New York   2 
    0   Monsters                    Renata Mcniar           2023           New York   2
    0   At risk                     Charles Dobi            2020           London     1
    0   Manuela and Ricardo         Lucas Zacci             2022           Rio de Janeiro 1
    0   War against the machine     Angelina Trotter        1999           Rio de Janeiro 2

The problem still lingers on
","Perhaps you could use a for loop to iterate through the 'mentioned cities' column, and use a dict to count the occurrences of cities:
city_count = {}
count_column = []
for city in df['mentioned cities']:
    city_count[city] = city_count.get(city, 0) + 1
    count_column.append(city_count[city])

df['Counter'] = count_column

",pandas
transposing multiple values columns in pandas,"I have the following pandas dataframe and I would like to transpose it.



YEAR
CD_PRD
VL_JAN
VL_FEB
VL_MAR
VL_APR




2022
CD76
10000
8000
170000
58000


2022
CD92
15000
7500
86000
1200


2023
CD76
8000
12000
15000
5000


2023
CD92
12000
4560
10480
15600


2024
CD76
400
8000
45800
120


2024
CD92
56000
10200
150000
1500



What I need is to pivot the values columns names e.g.(VL_JAN   |  VL_FEB   |  VL_MAR   | VL_APR ) into rows, and the rows (CD-PRD) into lines to have the following dataframe at the end:



YEAR
MONTHS
VL_CD76
VL_CD92




2022
JAN
10000
15000


2022
FEB
8000
7500


2022
MAR
170000
86000


2022
ABR
58000
1200


2023
JAN
8000
12000


2023
FEB
12000
4560


2023
MAR
15000
10480


2023
ABR
5000
15600


2024
JAN
400
56000


2024
FEB
8000
10200


2024
MAR
45800
150000


2024
ABR
120
1500



I did the following, but it seems I miss something and did not managed to get the right dataframe structure.
df3 = df2.pivot(index='YEAR', columns=['CD_PRD'], values=['VL_JAN', 'VL_FEB', 'VL_MAR', 'VL_APR']).reset_index()
print(df3)

Any help would be amazing. Thanks!
Marcio
","A possible solution, whose steps are:

The first step is to set the index of the dataframe df to the columns YEAR and CD_PRD using the set_index method.

Then, the stack method is used to pivot the columns into rows, creating a hierarchical index.

The unstack method is then applied to pivot the innermost level of the index back into columns.

The reset_index method is used to reset the index of the dataframe, turning the index back into columns.

The rename_axis method is used to remove the name of the axis.

The set_axis method is used to rename the columns of the dataframe to the list colnames.

Finally, the pipe method is used to apply a lambda function that uses the assign method to create a new column MONTH by removing the prefix VL_ from the values in the MONTH column using the str.replace method.


colnames = ['YEAR', 'MONTH', 'VL_CD76', 'VL_CD92']

(df.set_index(['YEAR', 'CD_PRD']).stack()
 .unstack(1).reset_index().rename_axis(None, axis=1)
 .set_axis(colnames, axis=1)
 .pipe(lambda x: x.assign(MONTH = x['MONTH'].str.replace('VL_', ''))))

Output:
    YEAR MONTH  VL_CD76  VL_CD92
0   2022   JAN    10000    15000
1   2022   FEB     8000     7500
2   2022   MAR   170000    86000
3   2022   APR    58000     1200
4   2023   JAN     8000    12000
5   2023   FEB    12000     4560
6   2023   MAR    15000    10480
7   2023   APR     5000    15600
8   2024   JAN      400    56000
9   2024   FEB     8000    10200
10  2024   MAR    45800   150000
11  2024   APR      120     1500

",pandas
the feature names should match those that were passed during fit,"Im trying to calculate the r squared value after the creation of a model using sklearn linear regression.
Im simply

importing a csv dataset
filtering the interesting columns
splitting the dataset in train and test
creating the model
making a prediction on the test
calculating the r squared in order to see how good is the model to fit the test dataset

the dataset is taken from https://www.kaggle.com/datasets/jeremylarcher/american-house-prices-and-demographics-of-top-cities
the code is as following
''' Lets verify if there s a correlation between price and beds number of bathroom'''

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression

df = pd.read_csv('data/American_Housing_Data_20231209.csv')

df_interesting_columns = df[['Beds', 'Baths', 'Price']]

independent_variables = df_interesting_columns[['Beds', 'Baths']]
dependent_variable = df_interesting_columns[['Price']]

X_train, X_test, y_train, y_test = train_test_split(independent_variables, dependent_variable, test_size=0.2)

model = LinearRegression()
model.fit(X_train, y_train)

prediction = model.predict(X_test)

print(model.score(y_test, prediction))

but i get the error
ValueError: The feature names should match those that were passed during fit.
Feature names unseen at fit time:

Price
Feature names seen at fit time, yet now missing:
Baths
Beds

what am I doing wrong?
","Your last line is wrong. You misunderstood the score method. score take X and y as parameter not the y_true and y_pred
Try:
from sklearn.metrics import r2_score

print(r2_score(y_test, prediction))
# 0.24499127100887863

Or with the score method:
print(model.score(X_test, y_test))
# 0.24499127100887863

",pandas
how can i solve this error valueerror the feature names should match those that were passed,"I am trying to traing and predict with a my model, so the current error is
ValueError: The feature names should match those that were passed during fit.
Feature names unseen at fit time:
this is my code
X_train = X_train.drop(columns=['InvoiceDate', ""BillingAddress"", ""BillingCity"", ""BillingState"", ""BillingCountry"", ""BillingPostalCode"", ""Rowversion_x"", ""Rowversion_y"", ""Rowversion"", ""Name"", ""Composer""], axis=1)


print(f'Tipo X_train: {type(X_train)} Tipo y_train: {type(y_train)}')
clf.fit(X_train, y_train)
y_pred = clf.predict(X_test)

accuracy = accuracy_score(y_test, y_pred)
print(""Accuracy:"", accuracy)

mining_data.to_csv('mining_table.csv', index=False)

i dont understand what is the error, i am excluding some columns because i was getting another error like this:
ValueError: could not convert string to float: 'Calle Lira, 198'
","Because you have dropped columns in your X_train but not in your X_test, the model is telling you it is seeing columns during testing that it hasn't encountered at training.
The number of dimensions/columns in your inputs, for both train and test should be the same. If the number of columns is already the same and you are encountering the issue, check that the columns you are sending are the right ones, and if thats so (maybe just a typo or your columns have different titles but the data is matching) pass the numpy version of your data by doing X_train.values
",pandas
how to select rows that display some type of pattern in python,"I am looking to extract rows from my dataset based on a pattern like condition. The condition I'm looking for is finding periods in a battery's charging history where it discharged from 100-0% without charging in between.
For example, in this dataset below I would be interested in a function that would only return timestamp of 7 to 12 as it contains a full discharge of the battery.



timestamp
Charge level (%)




1
50


2
55


3
40


4
60


5
80


6
100


7
100


8
85


9
60


10
55


11
40


12
0


13
20



The approach I have tried is to use the loc function in Pandas to look for rows with a charge level of 0% and then backtrack until I reach a row with a charge level of 100%.
But I am struggling with the backtracking part in this approach.
","[Code updated according to comments]
The idea I use is to keep only the rows with 0 and 100 and the final rows of interest will be the ones with 100 followed by 0. [after checking with .diff(1) that the values are monotonically decreasing]
I also updated your example to include some more difficult cases like when it start discharging and then start charging before it was fully discharged [or fully discharged].
with pd.option_context('display.max_columns', None):
    display(df.T)

    0   1   2   3   4   5   6   7   8   9   10  11  12  13  14  15  16  17  18  19  20  21  22  23  24
time    1   2   3   4   5   6   7   8   9   10  11  12  13  14  15  16  17  18  19  20  21  22  23  24  25
level   20  0   40  60  80  100 100 85  60  40  40  0   100 20  20  55  100 10  100 0   100 20  40  0   20

df[""diff1""] = df[""level""].diff(1)
df.set_index(""time"", inplace=True)
res = df.copy()

mask =((res[""level""] == 100) | (res[""level""] == 0)) 
res = res.loc[mask]

mask1 = (res[""level""] == 100) & (res[""level""].shift(-1) == 0)
mask2 = (res[""level""] == 0) & (res[""level""].shift(1) == 100)
res = res.loc[mask1 | mask2, [""level""]]

    level
time    
7   100
12  0
19  100
20  0
21  100
24  0

# Get the start and end of all the segments having levels [100, ..., 0]
t_start, t_end = res[res[""level""] == 100].index, res[res[""level""] == 0].index
 
# Among all the segments keep only the ones with level monotonically decreasing
idx = [(t_start[i], t_end[i]) for i in range(len(t_start)) 
       if (df.loc[t_start[i]+1:t_end[i], ""diff1""] <= 0).all()]
print(idx)  # All resulting groups with (start_time, end_time)

[(7, 12), (19, 20)]

# Example to get the sum of all the levels in result group n°0 (idx[0] ie (7, 12))
gp0 = df.loc[idx[0][0]:idx[0][1], [""level""]]
display(gp0)

    level
time    
7   100
8   85
9   60
10  40
11  40
12  0

print(f""The sum of levels is {sum(gp0['level'])}."")
The sum of levels is 325.

",pandas
i want to connect to an api and extract the data,"I'm doing my own python analytics project on compiling and analysing data from this open-data source:
(https://data.gov.ie/dataset?q=homeless&api=true&sort=score+desc%2C+metadata_created+desc&theme=Housing)
I've never worked with api's or json's before.. all the info on google or YouTube video's always have an API key.. but I don't know how to get it
so far I've done this:
import requests
import pandas as pd
import time

API_KEY = requests.get('https://data.gov.ie/dataset?q=homelessness&api=true&theme=Housing&sort=metadata_modified+desc')
API_KEY.status_code
# this returns 200 which from google means connected status correct


Then I write this:
#make api call
response = API_KEY.json()


and get back errors:
JSONDecodeError                           Traceback (most recent call last)
~/opt/anaconda3/lib/python3.9/site-packages/requests/models.py in json(self, **kwargs)
    970         try:
--> 971             return complexjson.loads(self.text, **kwargs)
    972         except JSONDecodeError as e:

~/opt/anaconda3/lib/python3.9/json/__init__.py in loads(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)
    345             parse_constant is None and object_pairs_hook is None and not kw):
--> 346         return _default_decoder.decode(s)
    347     if cls is None:
...
...
...

About The Project:
What I want it to connect to the data on homelessness in ireland (I would like the data to be easily updatable for future automatic updating but that's later on.. no idea how to do it yet).
Ireland is facing a serious crisis with homelessness and housing in general and I would like to see how it has worsened over the years and visualise it. Perhaps get some insight or the scale of the issue.
After I do some work with the project I will then import the data into tableau to do some more visualisation then publish my report to my LinkedIn.
Perhaps you can advise me if I should be trying to work with csv's or json's?
Any help as always would be sincerely appreciated.
","An API key is generally used to access private data and/or make some write operations. It's not the case here, all data are public.
What you need is to get all data from 'Homelessness Report'. You have to proceed in 3 steps:

Query the database
Process and filter the result
Download and merge data

import requests

# 1. Query the database
q_url = 'https://data.gov.ie/api/3/action/package_search?q=homelessness&rows=100'
packages = requests.get(q_url).json()['result']

# 2a. Process results
df = pd.json_normalize(packages['results'], 'resources', 'title')

# 2b. Filter results
mask = df['title'].str.startswith('Homelessness Report') & df['format'].eq('CSV')
df = df.loc[mask, ['title', 'url']]

# 3a. Download data
data = {}
for idx, row in df.iterrows():
    dt = pd.to_datetime(row.title.strip('Homelessness Report '))
    data[dt] = pd.read_csv(row.url)

# 4a. Merge data
out = (pd.concat(data, axis=0).droplevel(1).rename_axis('Date')
         .sort_index().reset_index())
out.to_excel('Homelessness Report.xlsx', index=False)

Output:
>>> out
          Date      Region Total Adults  ... Number of people with citizenship EEA/Uk Number of people with citizenship Non-EEA  Number of Child Dependants in Families
0   2019-01-01    Mid-East          254  ...                                      NaN                                       NaN                                     NaN
1   2019-01-01    Mid-West          338  ...                                      NaN                                       NaN                                     NaN
2   2019-01-01  North-East          162  ...                                      NaN                                       NaN                                     NaN
3   2019-01-01  North-West           52  ...                                      NaN                                       NaN                                     NaN
4   2019-01-01  South-East          316  ...                                      NaN                                       NaN                                     NaN
..         ...         ...          ...  ...                                      ...                                       ...                                     ...
436 2023-01-01    Mid-West          414  ...                                       45                                        32                                     NaN
437 2023-01-01    Midlands          148  ...                                       17                                        19                                     NaN
438 2023-01-01    Mid-East          437  ...                                       81                                        66                                     NaN
439 2023-01-01      Dublin         5946  ...                                     1506                                      1190                                     NaN
440 2023-01-01        West          321  ...                                       34                                        19                                     NaN

[441 rows x 21 columns]

",pandas
how to shift dates in a pandas dataframe add x months,"I have a dataframe with columns of dates.
I know how to shift dates by a fixed number of months (eg add 3 months to all the dates in column x); however, I cannot figure out how to shift dates by a number of months which is not fixed, but is another column of the dataframe.
Any ideas?
I have copied a minimal example below. The error I get is:
The truth value of a Series is ambiguous

Thanks a lot!
import pandas as pd
import numpy as np
import datetime

df = pd.DataFrame()
df['year'] = np.arange(2000,2010)
df['month'] = 3

df['mydate'] = pd.to_datetime(  (df.year * 10000 + df.month * 100 +1).apply(str), format='%Y%m%d')
df['month shift'] = np.arange(0,10)

# if I want to shift mydate by 3 months, I can convert it to DatetimeIndex and use dateOffset:
df['my date shifted by 3 months'] = pd.DatetimeIndex( df['mydate'] ) + pd.DateOffset(months = 3)

# however, how do I shift mydate by the number of months in the column 'month shift'?
#This does NOT work:
df['my date shifted'] = pd.DatetimeIndex( df['mydate'] ) + pd.DateOffset(months = df['month shift'])
print df

","IIUC you could use apply with axis=1:
In [23]: df.apply(lambda x: x['mydate'] + pd.DateOffset(months = x['month shift']), axis=1)
Out[23]:
0   2000-03-01
1   2001-04-01
2   2002-05-01
3   2003-06-01
4   2004-07-01
5   2005-08-01
6   2006-09-01
7   2007-10-01
8   2008-11-01
9   2009-12-01
dtype: datetime64[ns]

",pandas
how to hide or remove all headers from a pandas dataframe,"I want to delete to just column name (x,y,z), and display only the data.
In [68]: df
Out[68]: 
   x  y  z
0  1  0  1  
1  2  0  0 
2  2  1  1 
3  2  0  1 
4  2  1  0

I want to print result to same as below.
Out[68]: 

0  1  0  1  
1  2  0  0 
2  2  1  1 
3  2  0  1 
4  2  1  0

Is it possible? How can I do this?
","In pandas by default need column names.
But if really want 'remove' columns what is strongly not recommended, because get duplicated column names is possible assign empty strings:
df.columns = [''] * len(df.columns)


But if need write df to file without columns and index add parameter header=False and index=False to to_csv or to_excel.
df.to_csv('file.csv', header=False, index=False)

df.to_excel('file.xlsx', header=False, index=False)

",pandas
delete a column from a pandas dataframe,"To delete a column in a DataFrame, I can successfully use:
del df['column_name']

But why can't I use the following?
del df.column_name

Since it is possible to access the Series via df.column_name, I expected this to work.
","As you've guessed, the right syntax is 
del df['column_name']

It's difficult to make del df.column_name work simply as the result of syntactic limitations in Python. del df[name] gets translated to df.__delitem__(name) under the covers by Python.
",pandas
pandas dataframe filter to return true for all rows how,"Hi I have a filter 'm' set that is flexible enough to change by me. Sometimes, I want to filter by Car or x_acft_body , or any of the various other fields, etc. Sometime I want to have all of the rows returned by commenting and uncommenting the required lines. But without changing the subsequent code, after the filter 'm' line.
How can I have a filter that will return true for ALL rows, when I don't want the filters applied? For e.g. something like 1==1 but i know this doesn't work.
I don't want to set dfdata.somefield.notnull() etc.  as I will not be too sure if this field will be always not null or not. also I DO NOT want to change subsequent code to be like dfdata.groupby.     i.e. without the [m]
# set filter if needed
m = (   1==1 #& return true at all times
#         (dfdata.Car == 'PG') #&
#         (dfdata.x_acft_body == 'N')# &
#         (dfdata.Car.isin(['PG', 'VJ', 'VZ']))
)


dft1 = dfdata[m].groupby(['FLD1']).agg({'FLD2': 'count'})

","You can create bool constant and change final mask by it:
#True for return all rows
m = (dfdata.Car == 'PG') | True

And:
#False for apply filter
m = (dfdata.Car == 'PG') | False


First solutions:
m = [True] * len(df.index)


m = np.repeat(True, len(df.index))

",pandas
select dataframe column and replace values by indices if true,"Hard to find the right title...here is what I want:
I have a dataframe and a column col1 with values : val1, val2, val3
I want to select the rows with val2 or val3 values for this specific column and replace them with val4 value but not for all of them, just for a ""slice"" between idx x and y :
import pandas as pd
data = {'col1':[""val1"",""val3"",""val3"",""val2"",""val1"",""val2"",""val3"",""val1""],'col2':[""val3"",""val1"",""val2"",""val1"",""val2"",""val3"",""val2"",""val2""]}
df = pd.DataFrame(data)
df
   col1  col2
0  val1  val3
1  val3  val1
2  val3  val2
3  val2  val1
4  val1  val2
5  val2  val3
6  val3  val2
7  val1  val2

Select rows from col1 with val2 or val3 values :
(df['col1']==""val2"") | (df['col1']==""val3"")
0    False
1     True
2     True
3     True
4    False
5     True
6     True
7    False

Now I want to replace the first 4 True rows for col1 (rows with index 1 2 3 5) with val4 in order to obtain :
   col1  col2
0  val1  val3
1  val4  val1
2  val4  val2
3  val4  val1
4  val1  val2
5  val4  val3
6  val3  val2
7  val1  val2

I thought something like :
df[((df['col1']==""val2"") | (df['col1']==""val3""))==True][0:4] = ""val4""
but it doesn't work (not surprise...)
Thought I need to use something like .loc
Thanx for any clue
","You can get the rows based on the condition
condition = (df['col1'] == ""val2"") | (df['col1'] == ""val3"")

And then get indices of rows that match the condition
indices = df[condition].index[:4]

Finally use loc to replace the selected rows with val4
df.loc[indices, 'col1'] = 'val4'

Output
   col1  col2
0  val1  val3
1  val4  val1
2  val4  val2
3  val4  val1
4  val1  val2
5  val4  val3
6  val3  val2
7  val1  val2

",pandas
appending column values from one dataframe to another as a list,"I have dozens of very similar dataFrames. What I want is to combine all 'VALUE' column values from each into lists, and return a dataFrame where the 'VALUE' column is comprised of these lists. I only want to do this for rows where 'PV' contains a substring from a list of substrings.
I came up with one way I thought would work, but it's real nasty and doesn't work anyways (stopped it at 3m). There has to be a better way of doing this, does anyone here have any ideas? Thanks for any and all help.
import pandas as np
# Example dataFrames
df0 = pd.DataFrame(data={'PV': ['pv1', 'pv2', 'pv3', 'pv4'], 'VALUE': [1, 2, 3, 4]})
df1 = pd.DataFrame(data={'PV': ['pv1', 'pv2', 'pv3', 'pv4'], 'VALUE': [5, 6, 7, 8]})
df2 = pd.DataFrame(data={'PV': ['pv1', 'pv2', 'pv3', 'pv4'], 'VALUE': [10, 11, 12, 13]})

DATAFRAMES

df0 dataFrame          df1 dataFrame      df2 dataFrame

PV   VALUE             PV   VALUE         PV   VALUE
pv1    1               pv1    5           pv1    10
pv2    2               pv2    6           pv2    11
pv3    3               pv3    7           pv3    12
pv4    4               pv4    8           pv4    13

# Nasty code I thought might work
strings = ['v2', 'v4']
for i, row0 in df0.iterrows():
    for j, row1 in df1.iterrows():
        if (row0['PV']==row1['PV']) & any(substring in row0['PV'] for substring in strings):
            df0.at[i,'VALUE'] = [row0['VALUE'], row1['VALUE']]


Desired result:
PV   VALUE
pv1    1
pv2  [2,6]
pv3    3
pv4  [4,8]

@enke thank you for your help! I had to play with it a bit to figure out how to keep nested lists from occurring, and ended up using the following commented function/code/output:
def appendValues(df0, df1, pvStrings=['v2','v4']):
       # Turn values in VALUE column into list objects
       df0['VALUE'] = df0['VALUE'].apply(lambda x: x if isinstance(x,list) else [x])
       # For rows were PV string DOESN'T contain substring, set value to max()+1
       #    apply makes lists [x] empty if they were set to max()+1, else [x]
       df1['VALUE'] = (df1['VALUE']
                     .where(df1['PV'].str.contains('|'.join(pvStrings)), df1['VALUE'].max()+1)
                     .apply(lambda x: [x] if x <= df1['VALUE'].max() else []))
       # concatenate df1's VALUE column to df0
       #    set the indexing column to 'PV'
       #    sum all row values (axis=1) into one list
       data = (df0.merge(df1, on='PV')
                     .set_index('PV')
                     .sum(axis=1))
       # restore singleton lists to their original type, reset index moves current 'PV' index back to a column, and implements new sequential index
       data = data.mask(data.str.len().eq(1), data.str[0]).reset_index(name='VALUE')
       return data

data = appendValues(df0, df1, pvStrings=['v2','v4'])
data = appendValues(data, df2, pvStrings=['v1','v4'])
data

Output:
    PV    VALUE
0   pv1   [1,10]
1   pv2   [2,6]
2   pv3     3
3   pv4  [4,8,13]

","You could filter df1 for rows that contain strings; concatenate it with df0; then groupby + agg(list) can aggregate ""VALUE""s for each ""PV"".
Finally, you could use mask to take out the elements from the singleton lists.
out = (pd.concat([df0, df1[df1['PV'].str.contains('|'.join(strings))]])
       .groupby('PV', as_index=False)['VALUE'].agg(list))
out['VALUE'] = out['VALUE'].mask(out['VALUE'].str.len().eq(1), out['VALUE'].str[0])

Alternatively, we could make the values in the ""VALUE"" columns lists and merge + concatenate the lists:
df0['VALUE'] = df0['VALUE'].apply(lambda x: [x])
df1['VALUE'] = df1['VALUE'].where(df1['PV'].str.contains('|'.join(strings)), df1['VALUE'].max()+1).apply(lambda x: [x] if x <= df1['VALUE'].max() else [])
out = df0.merge(df1, on='PV').set_index('PV').sum(axis=1)
out = out.mask(out.str.len().eq(1), out.str[0]).reset_index(name='VALUE')

Output:
    PV   VALUE
0  pv1       1
1  pv2  [2, 6]
2  pv3       3
3  pv4  [4, 8]

If you don't want to filter out the rows that contain ""strings"" in ""PV"" but rather keep them as separate rows, then you could concat + groupby first; then filter + explode:
out = pd.concat([df0, df1]).groupby('PV', as_index=False)['VALUE'].agg(list)
msk = out['PV'].str.contains('|'.join(strings))
out = pd.concat((out[msk].explode('VALUE'), out[~msk])).sort_index()

Output:
    PV   VALUE
0  pv1  [1, 5]
1  pv2       2
1  pv2       6
2  pv3  [3, 7]
3  pv4       4
3  pv4       8

",pandas
pandas dataframe creating a new column and filling it with values according to 2 conditional statements on other columns,"I've written this script that create new columns based on a value meeting two conditions.
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
df=pd.DataFrame()

df['variable 1']= np.arange(0,1.1,0.1)
df['variable 2']= 0.2*df['variable 1']
df['variable 3']= 0.4 -0.2*df['variable 1']


# Create new columns 

slope = [2, 1.5, 1, 0.5]

for i in range(len(slope)):

    df['slope = ' + str(slope[i])]=''
    for j in range(len(df['variable 1'])):
    # Calculating Scl_disp_sd with equation 1
        curve = 0.5 - slope[i]*df['variable 1'][j]
        df['slope = ' + str(slope[i])][j]= np.where((curve>df['variable 2'][j]) & (curve<df['variable 3'][j]), curve,np.nan)

display(df)

plt.plot(df['variable 1'], df['variable 2'], 'o', label='variable 2')
plt.plot(df['variable 1'], df['variable 3'], 'o', label='variable 3')
plt.plot(df['variable 1'], df.filter(like='slope =', axis=1), marker='.')
plt.legend()


enter image description here
The script works, however, I get this message:
/var/folders/m0/_y1fs5x50xx99pjg2yf42y7r0000gp/T/ipykernel_1964/2618301266.py:11: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!
You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.
A typical example is when you are setting values in a column of a DataFrame, like:

df[""col""][row_indexer] = value

Use `df.loc[row_indexer, ""col""] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy

  df['slope = ' + str(slope[i])][j]= np.where((curve>df['variable 2'][j]) & (curve<df['variable 3'][j]),
/var/folders/m0/_y1fs5x50xx99pjg2yf42y7r0000gp/T/ipykernel_1964/2618301266.py:11: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  df['slope = ' + str(slope[i])][j]= np.where((curve>df['variable 2'][j]) & (curve<df['variable 3'][j]),
/var/folders/m0/_y1fs5x50xx99pjg2yf42y7r0000gp/T/ipykernel_1964/2618301266.py:11: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!
You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.
A typical example is when you are setting values in a column of a DataFrame, like:

df[""col""][row_indexer] = value

Use `df.loc[row_indexer, ""col""] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
...
A value is trying to be set on a copy of a slice from a DataFrame

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  df['slope = ' + str(slope[i])][j]= np.where((curve>df['variable 2'][j]) & (curve<df['variable 3'][j]),
Output is truncated. View as a scrollable element or open in a text editor. Adjust cell output settings...

I'd appreciate if someone would have another idea how to write this script in order to avoid the message
","No need for the nested loop. Just apply you operation as a vector:
slope = [2, 1.5, 1, 0.5]

for i in range(len(slope)):
    curve = 0.5 - slope[i]*df['variable 1']
    df['slope = ' + str(slope[i])] = np.where((curve>df['variable 2'])
                                               & (curve<df['variable 3']),
                                              curve,np.nan)

Or full vectorial with numpy:
curve = 0.5 - slope*df['variable 1'].to_numpy()[:, None]
cols = [f'slope = {c}' for c in slope]
df[cols] = np.where(  (curve > df[['variable 2']].to_numpy())
                    & (curve < df[['variable 3']].to_numpy()),
                    curve, np.nan)

Output:
    variable 1  variable 2  variable 3  slope = 2  slope = 1.5  slope = 1  slope = 0.5
0          0.0        0.00        0.40        NaN          NaN        NaN          NaN
1          0.1        0.02        0.38        0.3         0.35        NaN          NaN
2          0.2        0.04        0.36        0.1         0.20        0.3          NaN
3          0.3        0.06        0.34        NaN          NaN        0.2          NaN
4          0.4        0.08        0.32        NaN          NaN        0.1         0.30
5          0.5        0.10        0.30        NaN          NaN        NaN         0.25
6          0.6        0.12        0.28        NaN          NaN        NaN         0.20
7          0.7        0.14        0.26        NaN          NaN        NaN         0.15
8          0.8        0.16        0.24        NaN          NaN        NaN          NaN
9          0.9        0.18        0.22        NaN          NaN        NaN          NaN
10         1.0        0.20        0.20        NaN          NaN        NaN          NaN

",pandas
how to normalize json correctly by pandas,"I want to do is load a json file of forex historical price data by Pandas and do statistic with the data. I have go through many topics on Pandas and parsing json file. I want to pass a json file with extra value and nested list to a pandas dataframe.
I got a json file 'EUR_JPY_H8.json'
First I import the lib that required,
import pandas as pd
import json
from pandas.io.json import json_normalize

Then load the json file,
with open('EUR_JPY_H8.json') as data_file:    
data = json.load(data_file)

I got a list below:
[{u'complete': True,
u'mid': {u'c': u'119.743',
  u'h': u'119.891',
  u'l': u'119.249',
  u'o': u'119.341'},
u'time': u'1488319200.000000000',
u'volume': 14651},
{u'complete': True,
u'mid': {u'c': u'119.893',
  u'h': u'119.954',
  u'l': u'119.552',
  u'o': u'119.738'},
u'time': u'1488348000.000000000',
u'volume': 10738},
{u'complete': True,
u'mid': {u'c': u'119.946',
  u'h': u'120.221',
  u'l': u'119.840',
  u'o': u'119.888'},
u'time': u'1488376800.000000000',
u'volume': 10041}]

Then I pass the list to json_normalize. Try to get price which is in the nested list under 'mid'
result = json_normalize(data,'time',['time','volume','complete',['mid','h'],['mid','l'],['mid','c'],['mid','o']])

But I got such result,

The 'time' data got breakdown into each integer row by row.
I have checked related document. I have to pass a string or list object to the 2nd parameter of json_normalize. How can I pass the timestamp there without breaking down?
The columns of my expected output are:
  index  |  time  | volumn  |  completed  |  mid.h  |  mid.l  |  mid.c  |  mid.o 

","You could just pass data without any extra params.
df = pd.io.json.json_normalize(data)
df

   complete    mid.c    mid.h    mid.l    mid.o                  time  volume
0      True  119.743  119.891  119.249  119.341  1488319200.000000000   14651
1      True  119.893  119.954  119.552  119.738  1488348000.000000000   10738
2      True  119.946  120.221  119.840  119.888  1488376800.000000000   10041


If you want to change the column order, use df.reindex:
df = df.reindex(columns=['time', 'volume', 'complete', 'mid.h', 'mid.l', 'mid.c', 'mid.o'])
df

                   time  volume  complete    mid.h    mid.l    mid.c    mid.o
0  1488319200.000000000   14651      True  119.891  119.249  119.743  119.341
1  1488348000.000000000   10738      True  119.954  119.552  119.893  119.738
2  1488376800.000000000   10041      True  120.221  119.840  119.946  119.888

",pandas
json list to pandas dataframe,"I've a json list and I can't convert to Pandas dataframe (various rows and 19 columns)
Link to response : https://www.byma.com.ar/wp-admin/admin-ajax.php?action=get_historico_simbolo&simbolo=INAG&fecha=01-02-2018
json response:
[
    {""Apertura"":35,""Apertura_Homogeneo"":35,""Cantidad_Operaciones"":1,""Cierre"":35,""Cierre_Homogeneo"":35,""Denominacion"":""INSUMOS AGROQUIMICOS S.A."",""Fecha"":""02\/02\/2018"",""Maximo"":35,""Maximo_Homogeneo"":35,""Minimo"":35,""Minimo_Homogeneo"":35,""Monto_Operado_Pesos"":175,""Promedio"":35,""Promedio_Homogeneo"":35,""Simbolo"":""INAG"",""Variacion"":-5.15,""Variacion_Homogeneo"":0,""Vencimiento"":""48hs"",""Volumen_Nominal"":5},
    {""Apertura"":34.95,""Apertura_Homogeneo"":34.95,""Cantidad_Operaciones"":2,""Cierre"":34.95,""Cierre_Homogeneo"":34.95,""Denominacion"":""INSUMOS AGROQUIMICOS S.A."",""Fecha"":""05\/02\/2018"",""Maximo"":34.95,""Maximo_Homogeneo"":34.95,""Minimo"":34.95,""Minimo_Homogeneo"":34.95,""Monto_Operado_Pesos"":5243,""Promedio"":-79228162514264337593543950335,""Promedio_Homogeneo"":-79228162514264337593543950335,""Simbolo"":""INAG"",""Variacion"":-0.14,""Variacion_Homogeneo"":-0.14,""Vencimiento"":""48hs"",""Volumen_Nominal"":150},
    {""Apertura"":32.10,""Apertura_Homogeneo"":32.10,""Cantidad_Operaciones"":2,""Cierre"":32.10,""Cierre_Homogeneo"":32.10,""Denominacion"":""INSUMOS AGROQUIMICOS S.A."",""Fecha"":""07\/02\/2018"",""Maximo"":32.10,""Maximo_Homogeneo"":32.10,""Minimo"":32.10,""Minimo_Homogeneo"":32.10,""Monto_Operado_Pesos"":98756,""Promedio"":32.10,""Promedio_Homogeneo"":32.10,""Simbolo"":""INAG"",""Variacion"":-8.16,""Variacion_Homogeneo"":-8.88,""Vencimiento"":""48hs"",""Volumen_Nominal"":3076}
]

I use the next piece of code to convert this json to dataframe:
def getFinanceHistoricalStockFromByma(tickerList): 
     dataFrameHistorical = pd.DataFrame()  
     for item in tickerList:
         url = 'https://www.byma.com.ar/wp-admin/admin-ajax.php?action=get_historico_simbolo&simbolo=' + item + '&fecha=01-02-2018'
         response = requests.get(url)
         if response.content : print 'ok info Historical Stock'
         data = response.json()                
         dfItem = jsonToDataFrame(data)                
         dataFrameHistorical = dataFrameHistorical.append(dfItem, ignore_index=True)    
    return dataFrameHistorical

def jsonToDataFrame(jsonStr):    
     return json_normalize(jsonStr)    

The result of json_normalize is 1 row and a lot of columns. How can I convert this json response to 1 row per list?
","If you change this line in your function: dfItem = jsonToDataFrame(data) to:
dfItem = pd.DataFrame.from_records(data)
it should work. I tested your function with this line replaced, using ['INAG'] as a parameter passed to your getFinanceHistoricalStockFromByma function, and it returned a DataFrame.
",pandas
count number of characters in each row and drop if all are below a certain number,"I have a dataframe with many columns, all of which contain text data mixed with NaNs.
I want to count the number of characters in each cell in each column and then drop any rows where all the columns have less than 5 characters (if any cells have more than 5 characters, then the row is not dropped).
I was considering making a new column with str.len for each column and then filter out rows using that, but it seems very cumbersome.
Example:
>>> df
   column_1    column_2    column_3
0   werhi      dsfhjk       dh      ---> not filtered because some columns have more than 5 characters
1   sgds        fuo         g       ---> filtered
2   wqyuio      dsklh       fhkjfj
3   fhi         d           fgho    ---> filtered
4   sadfhkj     sdjfkhs     yyisdk

>>> df_filtered
   column_1    column_2    column_3
0   werhio      dsfhjk      dh
2   wqyuio      dsfjklh     fhkjfj
4   sadfhkj     sdjfkhs     yyisdk

","No need to create new columns.
OPTION 1: minimal total string length per row
just apply to all columns, aggregate the total sum and create a boolean Series for boolean indexing:
thresh = 10
out = df[df.apply(lambda x: x.str.len()).sum(axis=1).ge(thresh)]

Output:
  column_1 column_2 column_3
0   werhio   dsfhjk       dh
2   wqyuio  dsfjklh   fhkjfj
4  sadfhkj  sdjfkhs   yyisdk

OPTION 2: minimal string length for any strings in a row
If you want to ensure all values in a column have at least a given size, compare to the threshold before aggregating with any:
thresh = 5
df[df.apply(lambda x: x.str.len()).ge(thresh).any(axis=1)]

Output:
  column_1 column_2 column_3
0   werhio   dsfhjk       dh
2   wqyuio  dsfjklh   fhkjfj
4  sadfhkj  sdjfkhs   yyisdk

Intermediates
OPTION 1:
# df.apply(lambda x: x.str.len())
   column_1  column_2  column_3
0         6         6         2
1         4         3         1
2         6         7         6
3         3         1         4
4         7         7         6

# df.apply(lambda x: x.str.len()).sum(axis=1)
0    14
1     8
2    19
3     8
4    20
dtype: int64

# df.apply(lambda x: x.str.len()).sum(axis=1).ge(thresh)
0     True
1    False
2     True
3    False
4     True
dtype: bool

OPTION 2:
# df.apply(lambda x: x.str.len().ge(5))
   column_1  column_2  column_3
0      True      True     False
1     False     False     False
2      True      True      True
3     False     False     False
4      True      True      True

# df.apply(lambda x: x.str.len().ge(5)).any(axis=1)
0     True
1    False
2     True
3    False
4     True
dtype: bool

",pandas
how to chain operations in pandas entirely inline,"I often want to both manipulate and display a dataframe during a sequence of chained operations, for which I would use*:
df = (
  df

  #Modify the dataframe:
  .assign(new_column=...)

  #View result (without killing the chain)
  .pipe(lambda df_: display(df_) or df_)

  #...further chaining is possible
)

The code block above adds new_column to the dataframe, displays the new dataframe, and finally returns it. Chaining works here because display returns a falsy value (None).
My question is about scenarios where I want to replace display with plt.plot or some function that returns a truthy value. In such cases, df_ would no longer propagate through the chain.
Currently, my round this is to define an external function transparent_pipe that can run plt.plot or any other method(s), whilst also ensuring that the dataframe gets propagated:
def transparent_pipe(df, *funcs):
  [func(df) for func in funcs]
  return df

df = (
  df

  #Modify the dataframe:
  .assign(new_column=...)

  #Visualise a column from the modified df, without killing the chain
  .pipe(lambda df_: transparent_pipe(df_, plt.ecdf(df_.new_column), display(df_), ...)

  #...further chaining is possible
)

Question
Is there an entirely in-line way of doing this, without needing to define transparent_pipe?
Preferably just using pandas.

*Tip from Effective Pandas 2: Opinionated Patterns for Data Manipulation, M. Harrison, 2024.
","With pyjanitor, you could use also:
# pip install pyjanitor
import janitor

df = (pd.DataFrame({'col1': [1, 2, 3], 'col2': [4, 5, 6]})
        .also(display)
        .mul(10)
     )

Alternatively, with a wrapper function to hide the output of any function and replace it by its first parameter (=the DataFrame):
def hide(f):
    """"""The inner function should accept the DataFrame as first parameter""""""
    def inner(df, *args, **kwargs):
        f(df, *args, **kwargs)
        return df
    return inner

df = (pd.DataFrame({'col1': [1, 2, 3], 'col2': [4, 5, 6]})
        .pipe(hide(display))
        .mul(10)
     )

Or, going like the original approach with short-circuiting:
df = (pd.DataFrame({'col1': [1, 2, 3], 'col2': [4, 5, 6]})
        .pipe(lambda x: plt.ecdf(x['col1']) and False or x) # truthy output
        .pipe(lambda x: display(x['col1']) and False or x)  # falsy output
        .mul(10)
     )

Or forcing a truthy with a tuple:
df = (pd.DataFrame({'col1': [1, 2, 3], 'col2': [4, 5, 6]})
        # example 1
        .pipe(lambda x: (display(x),) and x)
        # example 2
        .pipe(lambda x: (display(x), plt.ecdf(x['col1'])) and x)
        .mul(10)
     )

",pandas
in a pandas dataframe with a multiindex how to conditionally fill missing values with group means,"Setup:
# create a MultiIndex
dfx = pd.MultiIndex.from_product([
    list('ab'),
    list('cd'),
    list('xyz'),
], names=['idx1', 'idx2', 'idx3'])

# create a dataframe that fits the index
df = pd.DataFrame([None, .9, -.08, -2.11, 1.09, .38, None, None, -.37, -.86, 1.51, -.49], columns=['random_data'])
df.set_index(dfx, inplace=True)

Output:
                        random_data
idx1    idx2    idx3    
a       c       x       NaN
                y       0.90
                z      -0.08

        d       x      -2.11
                y       1.09
                z       0.38

b       c       x       NaN
                y       NaN
                z      -0.37

        d       x      -0.86
                y       1.51
                z      -0.49

Within this index hierarchy,  I am trying to accomplish the following:

When a value is missing within [idx1, idx2, idx3], fill NaN with the group mean of [idx1, idx2]
When multiple values are missing within [idx1, idx2, idx3], fill NaN with the group mean of [idx1]

I have tried df.apply(lambda col: col.fillna(col.groupby(by='idx1').mean())) as a way to solve #2, but I haven't been able to get it to work.
UPDATE
OK, so I have this solved in parts, but still at a loss about how to apply these conditionally:
For case #1: 
df.unstack().apply(lambda col: col.fillna(col.mean()), axis=1).stack(). 
I verified that the correct value was filled by looking at this:
df.groupby(by=['idx1', 'idx2']).mean(), 
but it also replaces the missing values that I am trying to handle differently in case #2.
Similarly for #2: 
df.unstack().unstack().apply(lambda col: col.fillna(col.mean()), axis=1).stack().stack()
verified the values replaced were correct by looking at 
df.groupby(by=['idx1']).mean() 
but it also applies to case #1, which I don't want.
","OK, solved it. 
First, I made a dataframe containing counts by group of non-missing values:
truth_table = df.apply(lambda row: row.count(), axis = 1).groupby(by=['idx1', 'idx2']).sum()

>> truth_table

idx1  idx2
a     c       2
      d       3
b     c       1
      d       3
dtype: int64

Then set up a dataframe (one for each case I'm trying to resolve) containing the group means:
means_ab = x.groupby(by=['idx1']).mean()

>> means_ab

idx1    
a    0.0360
b   -0.0525

means_abcd = x.groupby(by=['idx1', 'idx2']).mean()

>> means_abcd

idx1    idx2    
a       c    0.410000
d           -0.213333
b       c   -0.370000
d            0.053333

Given the structure of my data, I know:

Case #1 is analogous to truth_table having exactly one missing value in a given index grouping of [idx1, idx2] (e.g., these are the NaN values I want to replace with values from means_abcd)
Case #2 is analogous to truth_table having more than one missing value in a given index grouping of [idx1, idx2] (e.g., these are the NaN values I want to replace with values from means_ab

fix_case_2 = df.combine_first(df[truth_table > 1].fillna(means_ab, axis=1))

>> fix_case_2

idx1    idx2    idx3    
a       c       x     NaN
                y     0.9000
                z    -0.0800
        d       x    -2.1100
                y     1.0900
                z     0.3800
b       c       x    -0.0525 *
                y    -0.0525 *
                z    -0.3700
        d       x    -0.8600
                y     1.5100
                z    -0.4900

df = fix_case_2.combine_first(df[truth_table == 1].fillna(means_abcd, axis=1))

>> df

idx1    idx2    idx3    
a       c       x     0.4100 *
                y     0.9000
                z    -0.0800
        d       x    -2.1100
                y     1.0900
                z     0.3800
b       c       x    -0.0525 *
                y    -0.0525 *
                z    -0.3700
        d       x    -0.8600
                y     1.5100
                z    -0.4900


",pandas
filtering dataframe by list of substrings,"Building off this answer, is there a way to filter a Pandas dataframe by a list of substrings?
Say I want to find all rows where df['menu_item'] contains fresh or spaghetti
Without something like this:
df[df['menu_item'].str.contains('fresh') | (df['menu_item'].str.contains('spaghetti')]
","The str.contains method you're using accepts regex, so use the regex | as or:
df[df['menu_item'].str.contains('fresh|spaghetti')]

Example Input:
          menu_item
0        fresh fish
1      fresher fish
2           lasagna
3     spaghetti o's
4  something edible

Example Output:
       menu_item
0     fresh fish
1   fresher fish
3  spaghetti o's

",pandas
select rows if a condition is met in any two columns,"Please help me filter my dataframe for a condition, that should be fulfilled in any two columns.
Imagine a list of students with grades in different sports. I want to filter the list so the new list passed_students shows only those who have scored a 4 or greater in at least two different sports.
Students = {
  ""Names"": [""Tom"", ""Rick"", ""Sally"",""Sarah""],
  ""Football"": [4, 5, 2,1],
  ""Basketball"": [4, 2, 4,2],
  ""Volleyball"": [6, 1, 6,1],
  ""Foosball"": [4, 3, 4,3],
}

The Code should return this:
passed_Students = {
  ""Names"": [""Tom"", ""Sally""],
  ""Football"": [4,2],
  ""Basketball"": [4,4],
  ""Volleyball"": [6,6],
  ""Foosball"": [4,4],
}

I can make it work if one grade above 4 is sufficient:
import numpy as np
import pandas as pd


Students = {
  ""Names"": [""Tom"", ""Rick"", ""Sally"",""Sarah""],
  ""Football"": [4, 5, 2,1],
  ""Basketball"": [4, 2, 4,2],
  ""Volleyball"": [6, 1, 6,1],
  ""Foosball"": [4, 3, 4,3],
}
Students = pd.DataFrame(Students)


passed_Students= Students[(Students[""Football""]>3) |(Students[""Basketball""]>3)|(Students[""Volleyball""]>3)|(Students[""Foosball""]>3) ]

print(passed_Students)


This returns:
Students = {
  ""Names"": [""Tom"", ""Rick"", ""Sally""],
  ""Football"": [4, 5, 2],
  ""Basketball"": [4, 2, 4],
  ""Volleyball"": [6, 1, 6],
  ""Foosball"": [4, 3, 4],
}

But how can I make it such, that any two grades of 4 or above qualify for passed_students, thereby returning only this?
passed_Students = {
  ""Names"": [""Tom"", ""Sally""],
  ""Football"": [4,2],
  ""Basketball"": [4,4],
  ""Volleyball"": [6,6],
  ""Foosball"": [4,4],
}

","drop the ""Names"", then compare to 4 with ge, sum to count the number of True per row and filter with boolean indexing:
passed_Students = Students[Students.drop(columns=['Names'])
                                   .ge(4).sum(axis=1).ge(2)]

Output:
   Names  Football  Basketball  Volleyball  Foosball
0    Tom         4           4           6         4
2  Sally         2           4           6         4

Intermediates:
# Students.drop(columns=['Names']).ge(4)
   Football  Basketball  Volleyball  Foosball
0      True        True        True      True
1      True       False       False     False
2     False        True        True      True
3     False       False       False     False

# Students.drop(columns=['Names']).ge(4).sum(axis=1)
0    4
1    1
2    3
3    0
dtype: int64

# Students.drop(columns=['Names']).ge(4).sum(axis=1).ge(2)
0     True
1    False
2     True
3    False
dtype: bool

",pandas
how to find if all the columns in a dataframe are object dtype,"Let's say I have a DataFrame as data, and I want to find if every single column in the data frame is an object and use it as an if condition.
example:
describe = data.describe
(if condition to find all the columns are 'object'):
  agg = data.agg(['a','b','c'])
  if not agg.empty:
    describe = pd.concat(describe,agg)
    describe = describe.round(2)

","Your question is not without ambiguity, but you probably want to combinedtypes, eq and all:
if df.dtypes.eq(object).all():
    # do something

Another option:
if df.select_dtypes(exclude='object').empty:
     # do something

",pandas
grouping on multiple columns then finding unique rows based on max,"I have a data frame with 3 columns, 'Supplier', 'Category', and 'Amount'. This df is the result of a grouping operation where I grouped by Supplier, Category, and summed the Amounts. There are still many instance where there are multiple rows per supplier where they used different categories and had different amounts per category. What I'm hoping to do is filter the df so that only the category with the maximum amount per supplier is remaining. I've tried implementing this as part of the groupby, but hasn't worked, I've also followed tutorials where it shows using the .max() operation, but that results in a single line that shows the highest amount in the entire df. Any assistance would be appreciated - I can't share the actual data, but here is an example of what the df looks like now:

And what I would like the resulting df to look like:

","You can directly obtain the index of the maximum rows and use this to select those rows:
df2 = df.iloc[df.groupby('SN')['Amount'].idxmax(),:].reset_index(drop = True)

which gives:
   SN Category  Amount
0   1     Cat2    3000
1   2    Cat22    5000

",pandas
create a dataframe from another dataframe and a list of formulas in pandas dataframe,"I have the following list and dataframe:
import pandas as pd

df = pd.DataFrame({
    'name': ['alice','bob','charlie'],
    'a0': [25,26,27],
    'b0': [10,11,12],
    'c0': [3,4,5],
})

formul=['a0+b0','a0-c0','b0*c0','a0+c0']

from this i want to build a new dataframe in which the first column is the original and the others are modified according to the operations in the list:
name    a0+b0 a0-c0 b0*c0 a0+c0
alice      35    22    30    28
bob        37    22    44    30
charlie    39    22    60    32

I have developed the formula in R, but now i want to translate it to python:
Formula<-strsplit(formul, split="","")[[1]]
df<-as.data.frame(cbind(as.numeric(df$Name),sapply(Formula, function(x) with(df, eval(parse(text = x))))))

regards
","You could combine assign and eval:
out = df.assign(**{s: lambda x: x.eval(s) for s in formul})

Output:
      name  a0  b0  c0  a0+b0  a0-c0  b0*c0  a0+c0
0    alice  25  10   3     28     28     28     28
1      bob  26  11   4     30     30     30     30
2  charlie  27  12   5     32     32     32     32

Or, for a new DataFrame:
tmp = df.set_index('name')
out = pd.DataFrame({s: tmp.eval(s) for s in formul})

# or

out = (pd.DataFrame({s: df.eval(s) for s in formul})
         .set_axis(df['name'])
      )

Output:
         a0+b0  a0-c0  b0*c0  a0+c0
name                               
alice       35     22     30     28
bob         37     22     44     30
charlie     39     22     60     32

",pandas
convert to float pandas string column with mixed thousand and decimal separators,"I have a pandas DataFrame with a column containing strings representing numbers. These strings have mixed formats. Some times numbers use comma as a decimal separator and sometimes a dot. When a dot is used as a decimal separator, that number can contain comma as a thousand separator.
For example:
import pandas as pd
data = {
    'NumberString': [
        '1,234.56',
        '789,012.34',
        '45,678',
        '9,876.54',
        '3,210.98',
        '1,000,000.01',
        '123.45',
        '42,000',
        'NaN'
    ]
}
df = pd.DataFrame(data)

I want to convert this column to numeric without losing some of the data due to inconsistent format (commas vs dots). However, using pd.to_numeric with parameter errors='coerce' will drop down some of the number
Python
Is there a way to format all the strings to numbers without loosing them due to format?
What I have tried so far:
>>> df['Number'] = pd.to_numeric(df['NumberString'].str.replace(',','.'), errors='coerce')

NumberString  Number
1,234.56      NaN
789,012.34    NaN
45,678        45.678
9,876.54      NaN
3,210.98      NaN
1,000,000.01  NaN
123.45        123.450
42,000        42.000
NaN           NaN

Desired output:
NumberString  Number
1,234.56      1234.56 
789,012.34    789012.34
45,678        45.678
9,876.54      9876.54
3,210.98      3210.98
1,000,000.01  1000000.01
123.45        123.450
42,000        42.000
NaN           NaN

","If you have mixed formats, you could first try to str.replace the commas by a dot and convert to_numeric with errors='coerce', then fillna with an attempt converting the commas to empty string:
df['Number'] = (pd.to_numeric(df['NumberString'].str.replace(',', '.'), errors='coerce')
                  .fillna(pd.to_numeric(df['NumberString'].str.replace(',', ''), errors='coerce'))
               )

Output:
   NumberString       Number
0      1,234.56     1234.560
1    789,012.34   789012.340
2        45,678       45.678
3      9,876.54     9876.540
4      3,210.98     3210.980
5  1,000,000.01  1000000.010
6        123.45      123.450
7        42,000       42.000
8           NaN          NaN

what about 1.000.000,01/1.000,01?
If this is desired, then you can use a custom function. Count the number of ./, and decide based on those. If a single . or , and the other has zero or more than 1, then remove the other character and convert. If one of each, use the last one as decimal separator. If more that 1 for each, return NaN (you could also use a try/except to catch all invalid strings).
def to_num(s):
    d = s.count('.')
    c = s.count(',')
    if d<=1 and c != 1:
        return float(s.replace(',', ''))
    if c<=1 and d != 1:
        return float(s.replace('.', '').replace(',', '.'))
    if c>1 and d>1:
        return float('nan')
    s2 = s[::-1]
    if s2.index('.') < s2.index(','):
        return float(s.replace(',', ''))
    else:
        return float(s.replace('.', '').replace(',', '.'))
        
df['Number'] = df['NumberString'].map(to_num)

Output:
    NumberString       Number
0       1,234.56     1234.560
1     789,012.34   789012.340
2         45,678       45.678
3       9,876.54     9876.540
4       3,210.98     3210.980
5   1,000,000.01  1000000.010
6         123.45      123.450
7         42,000       42.000
8            NaN          NaN
9   1.000.000,01  1000000.010
10     1,000,000  1000000.000
11     1.000.000  1000000.000
12      1,000.01     1000.010
13      1.000,01     1000.010
14     1.2.3,4,5          NaN

",pandas
how can i convert each pandas data frame row into an object including the column values as the attributes,"Suppose I have a DataFrame including following columns ""NAME"", ""SURNAME"", ""AGE"" and I would like to create one object for each row, including those column values as its variables.
person = ConvertRow2Object(frame.iloc[0,:])
print person.NAME //outputs Gary

How can I do it with a generic solution to any DataFrame with any kind of column names and data types?
","You can convert the whole thing to a numpy recarray, then each record in the array is attributed:
people = frame.to_records()
person = people[0]
print person.NAME  # etc...

Using a namedtuple also seems to work:
from collections import namedtuple

Person = namedtuple('Person', frame.dtypes.index.tolist())
person = Person(*frame.iloc[0,:])
print person.NAME  # etc...

",pandas
calculate percentage of flag grouped by another column,"I have the following dataframe:



Account ID
Subscription type
Cancellation flag




123
Basic
1


222
Basic
0


234
Hybrid
1


345
Hybrid
1



Now I would like to calculate the percentage of cancellations, but grouped by  the subscription type.
I would like to get it in a format so that I can easily create a bar chart out of the percentages grouped by the subscription type.
","Use a groupby.mean:
out = df.groupby('Subscription type')['Cancellation flag'].mean().mul(100)

Output:
Subscription type
Basic      50.0
Hybrid    100.0
Name: Cancellation flag, dtype: float64

Then plot.bar:
out.plot.bar()


Or directly with seaborn.barplot:
import seaborn as sns
sns.barplot(df, x='Subscription type', y='Cancellation flag',
            estimator='mean', errorbar=None)

Output:

",pandas
discordgateway warning quotshard id none heartbeat blocked for more than 10 secondsquot while using pandas,"So I've made a discord bot using discord.py in python and have been running it for some time. However, recently the bot has randomly started to die. So I added the logging library to my program to try and find out what was happening and I got this log this morning:
https://pastebin.com/s5yjQMs7
This error traceback goes on forever referencing multiple pandas files. My discord bot code:
# Import libraries
import asyncio
import random
import AO3
import pandas as pd
from discord.ext import commands
import logging


# Function to setup the dataframe
def dataframeSetup():
    # Create the dataframe
    df = pd.read_csv(
        ""https://docs.google.com/spreadsheets/d/16QtBJEtvV5a5DheR78x5AsoVA5b2DpXD1mq-x3lCFiA/export?format=csv"",
        names=[""NaN"", ""Title"", ""Author"", ""Ship(s)"", ""Type"", ""Series"", ""Status"", ""Smut"", ""No of words"", ""No of chapters"",
               ""Link""])
    # Remove first two lines
    df = df.iloc[2:]
    # Remove the first column
    df.drop(""NaN"", axis=1, inplace=True)
    # Create variable to store the index of the first empty row
    firstEmptyRow = 0
    # Iterate over every row
    for index, row in df.iterrows():
        # Test if every cell is empty
        if row.isnull().all():
            # Set firstEmptyRow to the index (it is minus 2 because the index of the dataframe starts at 2)
            firstEmptyRow = index - 2
            break
    # Return the final dataframe
    return df.iloc[0:firstEmptyRow]

# Function to make random quotes
def quoteMaker(df):
    # Grab a random fic
    randomFic = df.iloc[random.randint(2, len(df))]
    # Create AO3 session
    ao3Session = AO3.Session(""username"", ""password"")
    # Create work object
    work = AO3.Work(AO3.utils.workid_from_url(randomFic[""Link""]), ao3Session)
    # Get chapter amount
    chapterAmount = work.chapters
    # Get chapter text for a random chapter
    randomChapter = random.randint(1, chapterAmount)
    randomChapterText = work.get_chapter_text(randomChapter)
    # Convert the chapter text into a list
    textList = list(filter(None, randomChapterText.split(""\n"")))
    # Return random string
    return textList[random.randint(0, len(textList) - 1)], work, randomChapter, ao3Session

# Function to create trivia
def triviaMaker(triviaDone):
    # Test if all trivia questions have been done
    if len(triviaDone) == len(df1):
        # They've all been done, so clear the list and start again
        triviaDone.clear()
    # Generate a random index and use that to get a random trivia question
    randomIndex = random.randint(0, len(df1)) - 1
    randomTrivia = df1.iloc[randomIndex]
    # Test if the selected trivia question has been done before
    while randomIndex in triviaDone:
        # Trivia has already been done recently so try another one
        randomTrivia = df.iloc[random.randint(0, len(df1))]
    # Add the selected trivia question's index to the list
    triviaDone.append(randomIndex)
    # Return the formatted string as well as the correct index to allow for validation
    return f'''{randomTrivia[""Question""]}:
1. {randomTrivia[""Option 1""]}
2. {randomTrivia[""Option 2""]}
3. {randomTrivia[""Option 3""]}
4. {randomTrivia[""Option 4""]}''', randomTrivia, randomTrivia[""Correct Option""]


def record(work):
    # Create initial array to store results
    ficResults = []
    # Open file and write existing results to ficResults
    with open(""QuoteResults.txt"", ""r"") as file:
        for line in file.readlines():
            ficResults.append(line)
    # Test if fic already exists in the results
    found = False
    for count, fic in enumerate(ficResults):
        if str(work.workid) in fic:
            # Fic already exists
            found = True
            break
    # Assign the new result
    if found == True:
        # Increment the result
        ficResults[count] = f""22561831, {int(ficResults[count][-2:]) + 1}\n""
    else:
        # Create new result
        ficResults.append(f""{work.workid}, 1\n"")
    # Write to file
    with open(""QuoteResults.txt"", ""w"") as file:
        for result in ficResults:
            file.write(result)


def authorGrab(work, session):
    # Function to grab only the authors
    return session.request(work.url).findAll(""h3"", {""class"": ""byline heading""})[0].text.replace(""\n"", """")


# Initialise discord variables
token = ""discord token""
client = commands.Bot(command_prefix=""!"", case_insensitive=True)
# Initialise the dataframe
df = dataframeSetup()
# Initialise trivia variables
df1 = pd.read_csv(""Trivia.txt"", delimiter=""/"",
                  names=[""Question"", ""Option 1"", ""Option 2"", ""Option 3"", ""Option 4"", ""Correct Option""])
# Initialise asked trivia questions list
triviaDone = []
# Initialise channel ID variables using a file
with open(""IDs.txt"", ""r"") as file:
    channelIDs = file.read().splitlines()

# Initialise logging
logger = logging.getLogger(""discord"")
logger.setLevel(logging.DEBUG)
handler = logging.FileHandler(filename=""quoteBot.log"", encoding=""utf-8"", mode=""a"")
handler.setFormatter(logging.Formatter('%(asctime)s:%(levelname)s:%(name)s: %(message)s'))
logger.addHandler(handler)

# Register !quote command
@client.command()
@commands.cooldown(1, 10, commands.BucketType.default)
async def quote(ctx):
    if ctx.channel.id == int(channelIDs[0][10:]):
        quote = """"
        # Test whether the quote is longer than 10 words
        while len(quote.split()) < 10:
            # Grab quote and related attributes
            quote, work, randomChapter, session = quoteMaker(df)
        # Grab authors
        authors = authorGrab(work, session)
        # Print quote and attributes
        await ctx.channel.send(quote)
        await ctx.channel.send(f""-{work.title} chapter {randomChapter} by {authors}. Link {work.url}"")
        record(work)


# Register !trivia command
# This command can only be used once every 60 seconds server-wide
@client.command()
@commands.cooldown(1, 60, commands.BucketType.default)
async def trivia(ctx):
    shortenedIDString = channelIDs[1][11:]
    for id in shortenedIDString.split("", ""):
        if ctx.channel.id == int(id):
            # Display trivia question
            triviaString, randomTrivia, correctIndex = triviaMaker(triviaDone)
            await ctx.channel.send(triviaString)

            # Function to check if an answer is correct
            def check(message):
                # Check if answer is correct
                if ""!answer"" in message.content:
                    return message.content == f""!answer {randomTrivia.iloc[int(correctIndex)]}"" or message.content == f""!answer {int(correctIndex)}""

            # Try and except statement to catch timeout error
            try:
                # Wait for user response
                await client.wait_for(""message"", check=check, timeout=15)
                # User response is correct
                await ctx.channel.send(""Correct answer"")
            except asyncio.TimeoutError:
                # Time has run out
                await ctx.channel.send(""Times up, better luck next time"")


# Register empty !answer command
# This is only needed to stop an error being returned
@client.command()
async def answer(ctx):
    return None


# Register !cruzie command
@client.command()
@commands.cooldown(1, 5, commands.BucketType.default)
async def cruzie(ctx):
    # User has types !cruzie so do secret
    await ctx.channel.send(""https://giphy.com/gifs/midland-l4FsJgbbeKQC8MGBy"")


# Register !murica command
@client.command()
@commands.cooldown(1, 5, commands.BucketType.default)
async def murica(ctx):
    # User has typed !murica so play murica gif
    await ctx.channel.send(""https://tenor.com/view/merica-gif-9091003"")


# Register !gamer command
@client.command()
@commands.cooldown(1, 5, commands.BucketType.default)
async def gamer(ctx):
    # User has typed !gamer so play gamers gif
    await ctx.channel.send(""https://tenor.com/view/hello-gamers-hello-hi-howdy-whats-up-gif-12988393"")


# Register !stinky command
@client.command()
@commands.cooldown(1, 5, commands.BucketType.default)
async def stinky(ctx):
    # User has typed !stinky so play srinky gif
    await ctx.channel.send(""https://tenor.com/view/monke-uh-oh-stinky-uh-oh-stinky-monke-gif-18263597"")


# Run when discord bot has started
@client.event
async def on_ready():
    # Get channel ID for test channel
    channel = client.get_channel(""debug channel"")
    # Send message to user signalling that the bot is ready
    await channel.send(""Running"")


# Catch discord errors
@client.event
async def on_command_error(ctx, error):
    if isinstance(error, commands.CommandOnCooldown):
        # CommandOnCooldown error detected
        await ctx.channel.send(f""Command is on cooldown, try again in {round(error.retry_after, 2)} seconds"")


# Start discord bot
client.run(token)

If anyone can figure out why this error occurs, that'll be greatly appreciated.
","The warning essentially means that your code is blocking for more than x seconds, it blocks the event loop and triggers that warning (you can reproduce this with time.sleep(x)). To fix it, you have to run the blocking functions (the panda ones) in a non-blocking way:
import time # To reproduce the error
import typing # For typehinting 
import functools

def blocking_func(a, b, c=1):
    """"""A very blocking function""""""
    time.sleep(a + b + c)
    return ""some stuff""


async def run_blocking(blocking_func: typing.Callable, *args, **kwargs) -> typing.Any:
    """"""Runs a blocking function in a non-blocking way""""""
    func = functools.partial(blocking_func, *args, **kwargs) # `run_in_executor` doesn't support kwargs, `functools.partial` does
    return await client.loop.run_in_executor(None, func)


@client.command()
async def test(ctx):
    r = await run_blocking(blocking_func, 1, 2, c=3) # Pass the args and kwargs here
    print(r) # -> ""some stuff""
    await ctx.send(r) 

You should run all the blocking functions this way
Another (easier) way would be to simply create a decorator
import functools
import typing
import asyncio


def to_thread(func: typing.Callable) -> typing.Coroutine:
    @functools.wraps(func)
    async def wrapper(*args, **kwargs):
        return await asyncio.to_thread(func, *args, **kwargs)
    return wrapper


@to_thread
def blocking_func(a, b, c=1):
    time.sleep(a + b + c)
    return ""some stuff""


await blocking_func(1, 2, 3)

If you're using python <3.9 you should use loop.run_in_executor instead of asyncio.to_thread
def to_thread(func: typing.Callable) -> typing.Coroutine:
    @functools.wraps(func)
    async def wrapper(*args, **kwargs):
        loop = asyncio.get_event_loop()
        wrapped = functools.partial(func, *args, **kwargs)
        return await loop.run_in_executor(None, wrapper)
    return wrapper


@to_thread
def blocking_func(a, b, c=1):
    time.sleep(a + b + c)
    return ""some stuff""


await blocking_func(1, 2, 3)

",pandas
release hdf5 disk memory after table or node removal with pytables or pandas,"I'm using HDFStore with pandas / pytables.
After removing a table or object, hdf5 file size remains unaffected. It seems this space is reused afterwards when additional objects are added to store, but it can be an issue if large space is wasted.
I have not found any command in pandas nor pytables APIs that might be used to recover hdf5 memory.
Do you know of any mechanism to improve data management in hdf5 files?
","see here
you need to ptrepack it, which rewrites the file.
ptrepack --chunkshape=auto --propindexes --complevel=9 --complib=blosc in.h5 out.h5

as an example (this will also compress the file).
",pandas
hdf5 file grows in size after overwriting the pandas dataframe,"I'm trying to overwrite the pandas dataframe in hdf5 file. Each time I do this, the file size grows up while the stored frame content is the same. If I use mode='w' I lost all other records.
Is this a bug or am I missing something?
import pandas
df = pandas.read_csv('1.csv')
for i in range(100):
  store = pandas.HDFStore('tmp.h5')
  store.put('TMP', df)
  store.close()

The tmp.h5 grows in size.
","Read the big warning at the bottom of this section
This is how HDF5 works.
",pandas
rolling window selection with groupby in pandas,"I have the following pandas dataframe:
# Create the DataFrame
df = pd.DataFrame({
    'id': [1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2],
    'date': [1, 2, 3, 4, 5, 6, 7, 8, 5, 6, 7, 8, 9, 10, 11, 12],
    'value': [11, 12, 13, 14, 15, 16, 17, 18, 21, 22, 23, 24, 25, 26, 27, 28]
})
df

id  date    value
0   1   1   11
1   1   2   12
2   1   3   13
3   1   4   14
4   1   5   15
5   1   6   16
6   1   7   17
7   1   8   18
8   2   5   21
9   2   6   22
10  2   7   23
11  2   8   24
12  2   9   25
13  2   10  26
14  2   11  27
15  2   12  28

I want to query the above dataframe, in a rolling window manner, for both ids. The rolling window should be of size n.
So, if n==2, in the 1st iteration I would like to query this:
df.query('(id==1 and (date==1 or date==2)) or (id==2 and (date==5 or date==6))')
id  date    value
0   1   1   11
1   1   2   12
8   2   5   21
9   2   6   22

in the 2nd iteration I would like to query this:
df.query('(id==1 and (date==2 or date==3)) or (id==2 and (date==6 or date==7))')
id  date    value
1   1   2   12
2   1   3   13
9   2   6   22
10  2   7   23

in the 3rd iteration I would like to query this:
df.query('(id==1 and (date==3 or date==4)) or (id==2 and (date==7 or date==8))')
id  date    value
2   1   3   13
3   1   4   14
10  2   7   23
11  2   8   24

etc. How could I do that in pandas ? My data has around 500 ids
","The exact expected logic is not fully clear, but assuming you want to loop over the groups/rolls, you could combine groupby.nth with sliding_window_view. By reusing the DataFrameGroupBy object, will only need to compute the groups once:
import numpy as np
from numpy.lib.stride_tricks import sliding_window_view as swv

n = 2

max_size = df['id'].value_counts(sort=False).max()
g = df.sort_values(by=['id', 'date']).groupby('id', sort=False)

for idx in swv(np.arange(max_size), n):
    print(f'rows {idx}')
    print(g.nth(idx))

Output:
rows [0 1]
   id  date  value
0   1     1     11
1   1     2     12
8   2     5     21
9   2     6     22
rows [1 2]
    id  date  value
1    1     2     12
2    1     3     13
9    2     6     22
10   2     7     23
rows [2 3]
    id  date  value
2    1     3     13
3    1     4     14
10   2     7     23
11   2     8     24
rows [3 4]
    id  date  value
3    1     4     14
4    1     5     15
11   2     8     24
12   2     9     25
rows [4 5]
    id  date  value
4    1     5     15
5    1     6     16
12   2     9     25
13   2    10     26
rows [5 6]
    id  date  value
5    1     6     16
6    1     7     17
13   2    10     26
14   2    11     27
rows [6 7]
    id  date  value
6    1     7     17
7    1     8     18
14   2    11     27
15   2    12     28

Alternatively, and assuming groups with an identical size and sorted by id/date, using shifted indexing:
n = 2
ngroups = df['id'].nunique()
for idx in swv(np.arange(len(df)).reshape(-1, ngroups, order='F'), n, axis=0):
    print(f'indices: {idx.ravel()}')
    print(df.iloc[idx.flat])

Output:
indices: [0 1 8 9]
   id  date  value
0   1     1     11
1   1     2     12
8   2     5     21
9   2     6     22
indices: [ 1  2  9 10]
    id  date  value
1    1     2     12
2    1     3     13
9    2     6     22
10   2     7     23
indices: [ 2  3 10 11]
    id  date  value
2    1     3     13
3    1     4     14
10   2     7     23
11   2     8     24
indices: [ 3  4 11 12]
    id  date  value
3    1     4     14
4    1     5     15
11   2     8     24
12   2     9     25
indices: [ 4  5 12 13]
    id  date  value
4    1     5     15
5    1     6     16
12   2     9     25
13   2    10     26
indices: [ 5  6 13 14]
    id  date  value
5    1     6     16
6    1     7     17
13   2    10     26
14   2    11     27
indices: [ 6  7 14 15]
    id  date  value
6    1     7     17
7    1     8     18
14   2    11     27
15   2    12     28

",pandas
how to speed up pandas string function why is str slower than applylambda,"I am using the pandas vectorized str.split() method to extract the first element returned from a split on ""~"".  I also have also tried using df.apply() with a lambda and str.split() to produce equivalent results.  When using %timeit, I'm finding that df.apply() is performing faster than the vectorized version.
Everything that I have read about vectorization seems to indicate that the first version should have better performance.  Can someone please explain why I am getting these results? Example:
    id     facility      
0   3466   abc~24353  
1   4853   facility1~3.4.5.6   
2   4582   53434_Facility~34432~cde   
3   9972   facility2~FACILITY2~343
4   2356   Test~23 ~FAC1  

The above dataframe has about 500,000 rows and I have also tested at around 1 million with similar results. Here is some example input and output:
Vectorization
In [1]: %timeit df['facility'] = df['facility'].str.split('~').str[0]
1.1 s ± 54.4 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)

Lambda Apply
In [2]: %timeit df['facility'] = df['facility'].astype(str).apply(lambda s: s.split('~')[0])
650 ms ± 52.1 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)

Does anyone know why I am getting this behavior?
","Pandas string methods are only ""vectorized"" in the sense that you don't have to write the loop yourself. There isn't actually any parallelization going on, because string (especially regex problems) are inherently difficult (impossible?) to parallelize. If you really want speed, you actually should fall back to python here. 
%timeit df['facility'].str.split('~', n=1).str[0]
%timeit [x.split('~', 1)[0] for x in df['facility'].tolist()]

411 ms ± 10.2 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)
132 ms ± 302 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)

For more information on when loops are faster than pandas functions, take a look at For loops with pandas - When should I care?.
As for why apply is faster, I'm of the belief that the function apply is applying (i.e., str.split) is a lot more lightweight than the string splitting happening in the bowels of Series.str.split.
",pandas
pandas dataframe cannot use assign function  why,"I am encountering some odd behavior in pandas, and I am hoping someone could shed some light on specifics from the df.assign(...) function in a pandas dataframe. I am getting a ValueError when trying to assign to column, despite the function being valid.
def is_toc_row(row):
    m_sig = m_df.loc[m_df.signature == row.signature]
    pct = (~pd.isnull(m_sig.line_type)).sum() / m_sig.shape[0]
    return (not pd.isnull(row.line_type)) or (pct < .5)


m_df = m_df.assign(is_toc_row=is_toc_row)

Gives:
ValueError: The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().

But this works totally fine:
for ind, row in m_df.iterrows():
    m_df.at[ind, 'is_toc_row'] = is_toc_row(row)

Is there some issue with referencing the rest of the DataFrame in the function? All I see in the docs is that the subject df cannot change, which it does not.
Of course I am capable of building a workaround, I just want to understand why this does not work for future use.
A few rows of data



index
signature
line_type




0
WYcxXTjq27YAP4uJOcLeRLelyUixNJaOwFwf2qqfpM4
NaN


1
WYcxXTjq27YAP4uJOcLeRLelyUixNJaOwFwf2qqfpM4
NaN


2
WYcxXTjq27YAP4uJOcLeRLelyUixNJaOwFwf2qqfpM4
1


3
WYcxXTjq27YAP4uJOcLeRLelyUixNJaOwFwf2qqfpM4
2


4
WYcxXTjq27YAP4uJOcLeRLelyUixNJaOwFwf2qqfpM4
2.4



","Actually when assign is used with a custom function, the function doesn't receive the datafame row by row (like apply) but receives once the full dataframe. Let's take a toy example:
m_df = pd.DataFrame({'temp_b': [7.0, 5.0], 'temp_c': [17.0, 25.0]},
                     index=['Portland', 'Berkeley'])

def myfunc(x):
    print(x, ""*end*"")
    return  x.temp_c + x.temp_b

m_df = m_df.assign(is_toc_row=myfunc)
display(m_df)


",pandas
in pandas how to reference and use a value from a dictionary based on column and index values in a dataframe,"I've data about how my times people are sick in certain locations (location A and B) at certain times (index of dates). I need to divide each value by the population in that location (column) AND at that time (index), which references a separate dictionary.
Eg dataframe:
import pandas as pd
data = [{'A': 1, 'B': 3}, {'A': 2, 'B': 20}, {'A': ""Unk"", 'B': 50}]
df = pd.DataFrame(data, index=[pd.to_datetime(""2019-12-31"")
                               , pd.to_datetime(""2020-12-30"")
                               , pd.to_datetime(""2020-12-31"")])
Out:
            A   B
2019-12-31  1   3
2020-12-30  2   20
2021-12-31  Unk 50

Population dictionary (location_year):
dic = {""A_2019"": 100, ""B_2019"": 200, ""A_2020"": 120, ""B_2020"": 150}

While it's not necessary to have the output in the same df, the output I'm trying to achieve would be:
            A   B    A1     B1
2019-12-31  1   3   0.01    0.015
2020-12-30  2   20  0.017   0.133
2021-12-31  Unk 50  nan     0.333

I've tried lots of different approaches, but almost always get an unhashable type error.
for col in df.columns:
     df[col + ""1""] = df[col]/dic[col + ""_"" + df.index.strftime(""%Y"")]
Out: `TypeError: unhashable type: 'Index

I guess I don't understand how pandas is parsing the df.index value to the dictionary(?). Can this be fixed, or is another approach necessary?
","You can create a Series from your dictionary, then unstack to DataFrame, reindex/set_axis, perform your operation and join with add_suffix:
def split(k):
    x, y = k.split('_')
    return (int(y), x)

# ensure using NaNs for missing values, not strings
df = df.replace('Unk', pd.NA).convert_dtypes()

# reshape to match the original DataFrame
tmp = (pd.Series({split(k): v for k, v in dic.items()})
         .unstack()
         .reindex(df.index.year)  # match years in df.index
         .set_axis(df.index)      # restore full dates
      )

# divide, add_suffix, join
out = df.join(df.div(tmp).add_suffix('1'))
# or
# out = df.join(tmp.rdiv(df), rsuffix='1')

Output:
               A   B        A1        B1
2019-12-31     1   3      0.01     0.015
2020-12-30     2  20  0.016667  0.133333
2020-12-31  <NA>  50      <NA>  0.333333

Intermediate tmp:
              A    B
2019-12-31  100  200
2020-12-30  120  150
2020-12-31  120  150

Variant
Here we rather create a tmp with only the years, and use an intermediate rename step to perform the alignment:
def split(k):
    x, y = k.split('_')
    return (int(y), x)

df = df.replace('Unk', pd.NA).convert_dtypes()

tmp = (pd.Series({split(k): v for k, v in dic.items()})
         .unstack()
      )

out = df.join(df.rename(lambda x: x.year).div(tmp)
                .add_suffix('1').set_axis(df.index)
             )

Intermediate tmp:
        A    B
2019  100  200
2020  120  150

",pandas
geopandas checking whether point is inside polygon,"I have ocean geopandas which contains 1 multipolygon (source: naturalearthdata.com)
I also have another dataframe that contains at lot of longitude and latitude information
I want to add a new column that will be True if the Point is in the ocean (inside the multipolygon)
zipfile = ""ne_10m_ocean/ne_10m_ocean.shp""
ocean_gpd = geopandas.read_file(zipfile)

df = pd.DataFrame({
    'lon': [120.0,120.1,120.2,120.3,120.4],
    'lat': [10.0,10.1,10.2,10.3,10.4]
})

for index, row in df.iterrows():
    df.loc[index,'is_ocean'] = ocean_gpd.contains(Point(x['lon'],x['lat'])

but it is too slow, I tried to used lambda function like this
df = df.assign(is_ocean = lambda x: ocean_gpd.contains(Point(x['lon'],x['lat']))

but failed, the error is cannot convert the series to <class 'float'>
Is anyone know how to do better individual point checking like this in geopandas?
Note: I just realized that for polygon data I used 10m one (more detailed polygon), if I uses 110m it a lot better, but in the future maybe I need to use 10m
","You can use apply like this:
import geopandas
import pandas as pd
from shapely.geometry import Point

ocean_gpd = geopandas.read_file('ne_10m_ocean.shp')

df = pd.DataFrame({
    'lon': [120.0, 120.1, 120.2, 120.3, 120.4],
    'lat': [10.0, 10.1, 10.2, 10.3, 10.4]
})

def in_ocean(row):
    point = Point(row['lon'], row['lat'])
    return ocean_gpd.contains(point).any()

df['is_ocean'] = df.apply(in_ocean, axis=1)



which returns:
     lon   lat  is_ocean
0  120.0  10.0     False
1  120.1  10.1     False
2  120.2  10.2     False
3  120.3  10.3     False
4  120.4  10.4     False

",pandas
catelog sentences into 5 words that represent them,"I have dataframe with 1000 text rows. df['text']
I also have 5 words that I want to know for each one of them how much they represnt the text  (between 0 to 1)
every score will be in df[""word1""] ,df[""word2""] and etc
I will glad for recomendations how to do that
edit
represnt = the semantic distance between the word to the text.
for example -
lets say in row 1 the text is ""i want to eat""
and I have 2 words : food and house.
so in df[""food ""] it would be higher score than in df[""house""]
","You could use a pre-trained sentence transformer model from sentence_transformers:
import pandas as pd
from sentence_transformers import SentenceTransformer, util


class SemanticSimilarityCalculator:
  def __init__(self, model_name: str = 'all-MiniLM-L6-v2') -> None:
    self.model = SentenceTransformer(model_name)
    self.word_embeddings = None

  def encode_words(self, words: list[str]) -> None:
    self.word_embeddings = self.model.encode(words, convert_to_tensor=True)
    self.words = words

  def calculate_similarity(self, text: str) -> list[float]:
    if self.word_embeddings is None:
      raise ValueError('Words must be encoded before calculating similarity.')
    text_embedding = self.model.encode(text, convert_to_tensor=True)
    similarities = util.cos_sim(text_embedding, self.word_embeddings)[
      0
    ].tolist()
    return similarities

  def add_similarity_scores_to_df(
    self, df: pd.DataFrame, text_column: str
  ) -> pd.DataFrame:
    if self.words is None:
      raise ValueError(
        'Words must be encoded before adding scores to the DataFrame.'
      )
    similarity_columns = ['word_' + word for word in self.words]
    df[similarity_columns] = df[text_column].apply(
      lambda text: pd.Series(self.calculate_similarity(text))
    )
    return df


def main():
  data = {'text': ['I want to eat', 'The house is big', 'I need to sleep']}
  df = pd.DataFrame(data)
  words = ['food', 'house', 'sleep', 'drink', 'run']
  calculator = SemanticSimilarityCalculator()
  calculator.encode_words(words)
  df_with_scores = calculator.add_similarity_scores_to_df(
    df, text_column='text'
  )
  print(df_with_scores)


if __name__ == '__main__':
  main()

Output:
               text  word_food  word_house  word_sleep  word_drink  word_run
0     I want to eat   0.592410    0.215032    0.254065    0.370329  0.259350
1  The house is big   0.243262    0.672110    0.170785    0.213780  0.119716
2   I need to sleep   0.253703    0.222462    0.725105    0.358372  0.303838

",pandas
behavior of dfmap inside another dfapply,"I find this code very interesting. I modified the code a little to improve the question. Essentially, the code uses a DataFrame to format the style of another DataFrame using pd.style.
t1 = pd.DataFrame({'x':[300,200,700], 'y':[100,300,200]})
t2 = pd.DataFrame({'x':['A','B','C'], 'y':['C','B','D']})

def highlight_cell(val, props=''):
    return props if val > 200 else ''
    
t2.style.apply(lambda x: t1.map(highlight_cell, props='background-color:yellow'), axis=None)


But can anyone explain how the last line works? I couldn't find Pandas documentation that clarifies the behavior of df.map() inside another df.apply().
To me, the code reads like for each item in t1, apply highlight_cell() to the entire t2 at once, and then return the whole thing, as illustrated in this pseudocode.
for x in all_items_in_t1:
    yield [highlight_cell(y) for y in all_items_in_t2]

However, the output is saying for each item in t1, apply highlight_cell() only to the corresponding item in t2 that has the same (x, y) location as that item in t1, like this.
for x, y in zip(all_items_in_t1, all_items_in_t2):
    yield highlight_cell(y)

I'm still having trouble understanding this pattern because it seems a bit confusing. Can anyone explain it more clearly?
","DataFrame.style.apply is used here, not DataFrame.apply.
By using the parameter axis=None, the callable is applied once (not per cell) on the whole DataFrame. Since the callable is a lambda, this essentially means we run:
t1.map(highlight_cell, props='background-color:yellow')

and use the output as format.
                         x                        y
0  background-color:yellow                         
1                           background-color:yellow
2  background-color:yellow                         

Note that using DataFrame.map here is not needed (and inefficient), better go for a vectorial approach:
t2.style.apply(lambda x: np.where(t1>200, 'background-color:yellow', ''), axis=None)

",pandas
how to fix pandas v2 quotvalueerror cannot convert from timedelta64ns to timedelta64dquot,"When upgrading from pandas version 1 to 2.0.0, I suddenly get a ValueError in a script that worked fine before upgrading pandas to version 2:
ValueError: Cannot convert from timedelta64[ns] to timedelta64[D].
Supported resolutions are 's', 'ms', 'us', 'ns'

This is a minimally reproducible example:
import pandas as pd
df = pd.DataFrame({'designation_date': ['2021-01-01', '2021-01-02']})
df['recency'] = pd.to_datetime('today') - pd.to_datetime(df['designation_date'])
df['recency'] = df['recency'].astype('timedelta64[D]')

What do I need to replace df['recency'].astype('timedelta64[D]') with so that the code works with pandas v2?
Using astype('timedelta64[D]') is used quite a bit in answers across SO, e.g. here.
","Use the .dt.days accessor instead of astype('timedelta64[D]):
df['recency'] = df['recency'].dt.days

The change in behaviour from v1 to v2 is documented here in the Pandas changelog.
",pandas
copying columns in pandas,"I want to compare two or more columns and place the larger and smaller values ​​in a new column.
First, I copied the two columns and then changed the values ​​of the rows that met the conditions.
However, there is a problem where row values ​​that do not meet the conditions are changed to nan.

How not to change to nan
How to do this all at once (if possible)

sample:
import pandas as pd

a = [
    {'a': 5, 'b': 10, 'c': 5},
    {'a': 4, 'b': 0.5, 'c': 1},
    {'a': 3.5, 'b': 15, 'c': 9},
    {'a': 2.1, 'b': 5, 'c': 8},
    {'a': 0.1, 'b': 1, 'c': 5},
]

df = pd.DataFrame(a)
df.loc[:, ['Big', 'Small']] = df[['a', 'b']].values
print(df.head())
df.loc[df['a'] < df['b'], ['Big', 'Small']] = df[['b', 'a']]
print(df.head())
>>
     a     b  c  Big  Small
0  5.0  10.0  5  5.0   10.0
1  4.0   0.5  1  4.0    0.5
2  3.5  15.0  9  3.5   15.0
3  2.1   5.0  8  2.1    5.0
4  0.1   1.0  5  0.1    1.0

     a     b  c  Big  Small
0  5.0  10.0  5  NaN    NaN
1  4.0   0.5  1  4.0    0.5
2  3.5  15.0  9  NaN    NaN
3  2.1   5.0  8  NaN    NaN
4  0.1   1.0  5  NaN    NaN

i want:
>>
      a     b  c  Big  Small
*0  5.0  10.0  5 10.0    5.0*
 1  4.0   0.5  1  4.0    0.5
*2  3.5  15.0  9 15.0    3.5*
*3  2.1   5.0  8  5.0    2.1*
*4  0.1   1.0  5  1.0    0.1*

","You can use numpy to compare
df['Big'] = np.where(df['a'] >= df['b'], df['a'], df['b'])
df['Small'] = np.where(df['a'] < df['b'], df['a'], df['b'])

Output
     a     b  c   Big  Small
0  5.0  10.0  5  10.0    5.0
1  4.0   0.5  1   4.0    0.5
2  3.5  15.0  9  15.0    3.5
3  2.1   5.0  8   5.0    2.1
4  0.1   1.0  5   1.0    0.1

",pandas
pandas read_sql_query  date params,"I am trying to add two date parameters to a SQL query in Pandas. Following example query:
trades = """"""
Select
trade_date,
units
from
transactions
where
trade_date BETWEEN :startdate and :enddate
""""""

I am now defining the dates in MM/DD/YYYY format (as they are stored like that in the database)
today = pd.Timestamp.today().strftime('%-m/%-d/%Y')
yday = date.today() - timedelta(days=1)
yday = yday.strftime('%-m/%-d/%Y')

trades_df = pd.read_sql_query(trades, con, params = {""startdate"":today, ""enddate"":yday})

But when passing the parameters like above I am getting error message ORA-01843: not a valid month
What is wrong here?
Thanks much
","You mentioned about trying to change to %m/%d/%Y, which still failed.
We can try to first change it DD/MM/YYYY and then cast TO_DATE in select query
today = pd.Timestamp.today().strftime('%m/%d/%Y')
yday = (date.today() - timedelta(days=1)).strftime('%m/%d/%Y')

SELECT query
trades = """""" 
SELECT trade_date, 
units FROM transactions 
WHERE TO_DATE(trade_date, 'MM/DD/YYYY') -- try with or without casting in trade_date
BETWEEN TO_DATE(:startdate, 'MM/DD/YYYY') AND TO_DATE(:enddate, 
'MM/DD/YYYY') 
""""""

An example in Fiddle, seems to work OK
",pandas
pyspark topandas valueerror found nonunique column index,"I get the following error when I try to convert pyspark dataframe to pandas dataframe with the method toPandas. I don't understand the reason for the error:
 ---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
/tmp/ipykernel_64705/3870041712.py in <module>
----> 1 df_who.limit(10).toPandas()

/opt/miniforge/miniforge/envs/jupyterlab/lib/python3.7/site-packages/pyspark/sql/dataframe.py in toPandas(self)
   2130                     if len(batches) > 0:
   2131                         table = pyarrow.Table.from_batches(batches)
-> 2132                         pdf = table.to_pandas()
   2133                         pdf = _check_dataframe_convert_date(pdf, self.schema)
   2134                         return _check_dataframe_localize_timestamps(pdf, timezone)

/opt/miniforge/miniforge/envs/jupyterlab/lib/python3.7/site-packages/pyarrow/array.pxi in pyarrow.lib._PandasConvertible.to_pandas()

/opt/miniforge/miniforge/envs/jupyterlab/lib/python3.7/site-packages/pyarrow/table.pxi in pyarrow.lib.Table._to_pandas()

/opt/miniforge/miniforge/envs/jupyterlab/lib/python3.7/site-packages/pyarrow/pandas_compat.py in table_to_blockmanager(options, table, categories, ignore_metadata, types_mapper)
    786 
    787     _check_data_column_metadata_consistency(all_columns)
--> 788     columns = _deserialize_column_index(table, all_columns, column_indexes)
    789     blocks = _table_to_blocks(options, table, categories, ext_columns_dtypes)
    790 

/opt/miniforge/miniforge/envs/jupyterlab/lib/python3.7/site-packages/pyarrow/pandas_compat.py in _deserialize_column_index(block_table, all_columns, column_indexes)
    901 
    902     # ARROW-1751: flatten a single level column MultiIndex for pandas 0.21.0
--> 903     columns = _flatten_single_level_multiindex(columns)
    904 
    905     return columns

/opt/miniforge/miniforge/envs/jupyterlab/lib/python3.7/site-packages/pyarrow/pandas_compat.py in _flatten_single_level_multiindex(index)
   1142         # Cheaply check that we do not somehow have duplicate column names
   1143         if not index.is_unique:
-> 1144             raise ValueError('Found non-unique column index')
   1145 
   1146         return pd.Index(

ValueError: Found non-unique column index

","You can check columns of pyspark dataframe, There is repeat column name in your dataframe according to your error.
",pandas
groupby a df column based on more than 3 columns,"I have an df which has 3 columns: Region, Country and AREA_CODE.
Region      Country     AREA_CODE   AREA_SUB_CODE_1     AREA_SUB_CODE_2
===========================================================================
AMER       US             A1            A1_US_1           A1_US_2
AMER       CANADA         A1            A1_CA_1           A1_CA_2
AMER       US             B1            B1_US_1           B1_US_2
AMER       US             A1            A1_US_1           A1_US_2

Is there a way to get output list of both the AREA_SUB_CODE_1  and AREA_SUB_CODE_2 as a list under each of the previous column value. something like the below?
{
    ""AREA_SUB_CODE_1"": {
        ""AMER"": {
                ""US"": {
                    ""A1"": [""A1_US_1""],
                    ""B1"": [""B1_US_1""]
                },
                ""CANADA"": {
                    ""A1"": [""A1_CA_1""],
                }
            }
    },
    ""AREA_SUB_CODE_2"": {
        ""AMER"": {
                ""US"": {
                    ""A1"": {
                        ""A1_US_1"": [""A1_US_2""]
                    },
                    ""B1"": {
                        ""B1_US_1"": [""B1_US_2""]
                },
                ""CANADA"": {
                    ""A1"": {
                        ""A1_CA_1"": [""A1_CA_2""],
                        }
                }
            }
    },
}

So far i have tried to groupby on 3 columns it works which is,
for (k1, k2), v in df.groupby(['Region', 'Country'])['AREA_CODE']:
    tTmp.setdefault(k1, {})[k2] = sorted(v.unique())
 

But when i try to groupby 4 columns, it is throwing error

too many values to unpack (expected 2)

for (k1, k2), v in df.groupby(['Region', 'Country', 'AREA_CODE'])['AREA_SUB_CODE_1']:
    tTmp.setdefault(k1, {})[k2] = sorted(v.unique())

How to apply groupby for 4 columns and 5 columns? Or any other way to achieve this?
","I think we can achieve this with the following recursive function:
f = lambda s: ({k: f(s[k]) for k in s.index.levels[0]} 
               if s.index.nlevels > 1 
               else {k: s.loc[[k]].unique().tolist() 
                     for k in s.index.unique()})   

Here, s is expected to be a pandas.Series with hierarchical indexing. At each indexing level, we map the keys to the corresponding depth of the resulting dictionary. At the last level, we extract unique values into a list. The double square brackets in s.loc[[k]] ensure the output is a series, the following unique method returns a numpy.ndarray with unique values of the series, and tolist converts the array into a Python list.
If we know there's exactly one unique value at the final level, we can simplify the function:
f = lambda s: {k: f(s[k]) for k in s.index.levels[0]} \
              if s.index.nlevels > 1 \
              else s.to_dict()

In this case, we skip creating a list at the end. But if needed, we can insert additional mapping like s.map(lambda x: [x]).to_dict().
Before applying any of the function above, we have to transform the data into a properly indexed series:
inner = ['Region', 'Country', 'AREA_CODE']
values = df.melt(inner).set_index(['variable', *inner]).squeeze()

Here, 'variable' is the default name for the new column with the rest of column names excluding the inner list after melting. The final answer is f(values)
Let's see the example:
df = pd.DataFrame({
    'Region': ['AMER', 'AMER', 'AMER', 'AMER'],
    'Country': ['US', 'CANADA', 'US', 'US'],
    'AREA_CODE': ['A1', 'A1', 'B1', 'A1'],
    'AREA_SUB_CODE_1': ['A1_US_1x', 'A1_CA_1', 'B1_US_1', 'A1_US_1y'],
    'AREA_SUB_CODE_2': ['A1_US_2', 'A1_CA_2', 'B1_US_2', 'A1_US_2']})

f = lambda s: ({k: f(s[k]) for k in s.index.levels[0]} 
               if s.index.nlevels > 1 
               else {k: s.loc[[k]].unique().tolist() 
                     for k in s.index.unique()}) 
  
inner = ['Region', 'Country', 'AREA_CODE']
values = df.melt(inner, var_name='sub_code').set_index(['sub_code', *inner]).squeeze()
answer = f(values)

Note, that in this example, we have 2 different values for the key set ('AREA_SUB_CODE_1', 'AMER', 'US', 'A1') and 2 equal ones for the key set ('AREA_SUB_CODE_2', 'AMER', 'US', 'A1'), so the second case will end up as a list with one value in the final answer:
{'AREA_SUB_CODE_1': {'AMER': {'CANADA': {'A1': ['A1_CA_1']},
                              'US': {'A1': ['A1_US_1x', 'A1_US_1y'],
                                     'B1': ['B1_US_1']}}},
 'AREA_SUB_CODE_2': {'AMER': {'CANADA': {'A1': ['A1_CA_2']},
                              'US': {'A1': ['A1_US_2'], 'B1': ['B1_US_2']}}}}

If we drop the last record in the example data, then we can use the alternative function with s.to_dict() at the end.
",pandas
problem with importing pandas package due to pandas codependency being unavailable,"I am having an issue with being unable to import pandas due to a certain pandas package being missing for some reason. When I run the cell in my jupyter notebook I get the following error.
ModuleNotFoundError: No module named 'pandas._libs.parsers'

I am using python 3.13, and I have the main pandas package install through miniconda. I also have the latest version of scikit-learn, matplotlib, seaborn, and numpy installed. I tried uninstalling then reinstalling the pandas package as well as all my other packages, and nothing seems to work.
","Installing python 3.12.8 solved the issue. Before I was using 3.13
",pandas
fill pandas columns based on datetime condition,"Here is the sample code to generate a dataframe.
import pandas as pd
import numpy as np  

dates = pd.date_range(""20241218"", periods=9600,freq='1MIN')   
df = pd.DataFrame(np.random.randn(9600, 4), index=dates, columns=list(""ABCD"")) 

I want to fill all the columns with -1 for time between 1:35 to 1:45 for all the dates.
Similarly I want to fill all the columns with -2 for the exact time of 1:00 for all the dates.
For all other time values, the columns need to be filled with zeros.
Please suggest the way forward.
","Try:
df.loc[df.between_time('01:35', '01:45').index] = -1
df.loc[df.index.time == pd.Timestamp('01:00').time()] = -2

Output can be verified with the similar:
print(df.between_time('1:35', '1:45').head(15) )
print(df.loc[df.index.time == pd.Timestamp('01:00').time()])

",pandas
multiply the value determined for each section,"I am in a situation where I need to multiply several columns by the value corresponding to the interval.
I'm using Pandas, but I'm using a for loop to get the value as follows.
I think there is a better way than this, can someone give me some advice?
sample:
import pandas as pd

d = {
    5: 5,
    4: 7,
    3: 2,
    2: -4,
    1: -5,
}
def mul(value):
    if value < 1: return value * 0.01
    elif 5 < value: return value * 20
    for k, v in d.items():
        if value <= k:
            val = value * v
            break
    return val

a = [
    {'a': 5, 'b': 10, 'c': 2},
    {'a': 4, 'b': 0.5, 'c': 4},
    {'a': 3.5, 'b': 1.5, 'c': 5},
    {'a': 2.1, 'b': 5, 'c': 6},
    {'a': 0.1, 'b': 1, 'c': 7},
]

df = pd.DataFrame(a)
a = []
b = []
for i in df.values:
    a.append(mul(i[0]))
    b.append(mul(i[1]))
df['a1'] = a
df['b1'] = b
print(df.head())

""""""
     a     b     c      a1       b1
0  5.0  10.0   2.0  25.000  200.000
1  4.0   0.5   4.0  20.000    0.005
2  3.5   1.5   5.0  17.500    7.500
3  2.1   5.0   6.0  10.500   25.000
4  0.1   1.0   7.0   0.001    5.000
""""""

","Updated answer to the edited question
Since the question was edited, the logic changed a lot and you can't easily vectorize your operation. Mapping of the values strongly depends on the internal logic of the mul function and here is dependent on the order of the keys in the dictionary.
Without vectorizing, you could simplify your code to:
cols = ['a', 'b']
out = df.join(df[cols].map(mul).add_suffix('1'))

Output:
     a     b  c      a1       b1
0  5.0  10.0  2  25.000  200.000
1  4.0   0.5  4  20.000    0.005
2  3.5   1.5  5  17.500    7.500
3  2.1   5.0  6  10.500   25.000
4  0.1   1.0  7   0.001    5.000

Original answer to the initial question
This is exactly what numpy.searchsorted is doing, just combine it with integer array indexing:
import numpy as np

k = np.array(list(d))          # array([1, 2, 3, 4, 5])
v = np.array(list(d.values())) # array([-5, -4,  2,  7,  5])

out = df.mul(v[np.searchsorted(k, df)])

NB. the keys in d must be sorted. Also, this uses the default side='left' parameter of searchsorted.
Output:
      a     b
0  25.0  -8.0
1  28.0  -2.5
2  24.5  -6.0
3   4.2  25.0
4  -5.0  -5.0

To combine to the original DataFrame, join:
out = df.join(df.mul(v[np.searchsorted(k, df)]).add_suffix('1'))

Output:
     a    b    a1    b1
0  5.0  2.0  25.0  -8.0
1  4.0  0.5  28.0  -2.5
2  3.5  1.5  24.5  -6.0
3  2.1  5.0   4.2  25.0
4  1.0  1.0  -5.0  -5.0

Intermediate (indices of matching values in k):
# np.searchsorted(k, df)
array([[4, 1],
       [3, 0],
       [3, 1],
       [2, 4],
       [0, 0]])

",pandas
recasting values in pandas dataframe that meet specific requirements,"I have a Pandas dataframe that has been created by importing from an Excel spreadsheet. When using the .dtypes method, the column appears to be of data type object. However, within the single column, there are data of multiple types such as str, int and float. There may also be missing values (NaN). Some of the string values are actually string representations of integer values, specifically, in this case, 4-digit years. I would like to be able to recast string values that consist of 4 digits to integers but leave all the other values (and datatypes) unchanged.
As an example, a minimal dataframe might look like:
import numpy as np
import pandas as pd
import re

testdf = pd.DataFrame({'col1':['abc','2023',456,789,'2021',4.5,'123',np.nan]})
   col1
0   abc
1  2023
2   456
3   789
4  2021
5   4.5
6   123
7   NaN

with dtype:
col1    object
dtype: object

However, data types in individual cells vary:
testdf['col1_types'] = testdf['col1'].apply(type)

   col1       col1_types
0   abc    <class 'str'>
1  2023    <class 'str'>
2   456    <class 'int'>
3   789    <class 'int'>
4  2021    <class 'str'>
5   4.5  <class 'float'>
6   123    <class 'str'>
7   NaN  <class 'float'>

The solution I've come up with involves several steps as shown below (with outcomes of individual steps added as new columns in the dataframe for clarity) but the process seems extremely clunky. Intuitively, I think there should be a much easier way of doing it - perhaps even as a one-liner - but I haven't been able to get the syntax worked out. The steps I've used are as follows:
Step 1 - Create a mask indicating which cells contain strings
strmask = testdf['col1'].apply(type) == str
testdf['strmask'] = strmask

   col1       col1_types  strmask
0   abc    <class 'str'>     True
1  2023    <class 'str'>     True
2   456    <class 'int'>    False
3   789    <class 'int'>    False
4  2021    <class 'str'>     True
5   4.5  <class 'float'>    False
6   123    <class 'str'>     True
7   NaN  <class 'float'>    False

Step 2 - Test if cells containing strings match regex '20\d{2}'; if so, recast as int, otherwise leave cell unchanged
testdf['col2'] = testdf.loc[phjStrMask,'col1'].apply(lambda c: int(c) if re.match('20\d{2}',c) else c)

   col1       col1_types  strmask  col2
0   abc    <class 'str'>     True   abc
1  2023    <class 'str'>     True  2023
2   456    <class 'int'>    False   NaN
3   789    <class 'int'>    False   NaN
4  2021    <class 'str'>     True  2021
5   4.5  <class 'float'>    False   NaN
6   123    <class 'str'>     True   123
7   NaN  <class 'float'>    False   NaN

Step 3 - Cells which originally contained something other than a string are currently represented as NaN in col2. Create a mask indicating which cells in col2 contain NaN and then replace with original content (in col1)
nanmask = testdf['col2'].isnull()
testdf.loc[nanmask,'col2'] = testdf['col1']

   col1       col1_types  strmask  col2
0   abc    <class 'str'>     True   abc
1  2023    <class 'str'>     True  2023
2   456    <class 'int'>    False   456
3   789    <class 'int'>    False   789
4  2021    <class 'str'>     True  2021
5   4.5  <class 'float'>    False   4.5
6   123    <class 'str'>     True   123
7   NaN  <class 'float'>    False   NaN

Step 4 - Confirm that the data types in the new column are as expected
testdf['col2_types'] = testdf['col2'].apply(type)

   col1       col1_types  strmask  col2       col2_types
0   abc    <class 'str'>     True   abc    <class 'str'>
1  2023    <class 'str'>     True  2023    <class 'int'>
2   456    <class 'int'>    False   456    <class 'int'>
3   789    <class 'int'>    False   789    <class 'int'>
4  2021    <class 'str'>     True  2021    <class 'int'>
5   4.5  <class 'float'>    False   4.5  <class 'float'>
6   123    <class 'str'>     True   123    <class 'str'>
7   NaN  <class 'float'>    False   NaN  <class 'float'>

The above process seems to work but is long-winded and clunky. Is there a way to directly recast as int the string values in col1 that match the regex without having to go through a series of intermediate steps?
","Actually you were very close. Just replace c by str(c) in your apply and it is done in 1 line:
testdf = pd.DataFrame({'col1':['abc','2023',456,789,'2021',4.5,'123',np.nan]})
testdf['col2'] = testdf['col1'].apply(lambda c: int(c) if re.match('20\d{2}',str(c)) else c)
display(testdf['col2'].apply(type))

0      <class 'str'>
1      <class 'int'>
2      <class 'int'>
3      <class 'int'>
4      <class 'int'>
5    <class 'float'>
6      <class 'str'>
7    <class 'float'>

",pandas
python generating a list of dates between two dates,"I want to generate a list of dates between two dates and store them in a list in string format. This list is useful to compare with other dates I have. 
My code is given below: 
from datetime import date, timedelta

sdate = date(2019,3,22)   # start date
edate = date(2019,4,9)   # end date

def dates_bwn_twodates(start_date, end_date):
    for n in range(int ((end_date - start_date).days)):
        yield start_date + timedelta(n)
print(dates_bwn_twodates(sdate,edate))

My present output: 
<generator object dates_bwn_twodates at 0x000002A8E7929410>

My expected output: 
['2019-03-22',.....,'2019-04-08']

Something wrong in my code. 
","You can use pandas.date_range() for this:
import pandas
pandas.date_range(sdate,edate-timedelta(days=1),freq='d')


DatetimeIndex(['2019-03-22', '2019-03-23', '2019-03-24', '2019-03-25',
           '2019-03-26', '2019-03-27', '2019-03-28', '2019-03-29',
           '2019-03-30', '2019-03-31', '2019-04-01', '2019-04-02',
           '2019-04-03', '2019-04-04', '2019-04-05', '2019-04-06',
           '2019-04-07', '2019-04-08'],
          dtype='datetime64[ns]', freq='D')

",pandas
find average rate per group in specific years using groupby transform,"I'm trying to find a better/faster way to do this. I have a rather large dataset (~200M rows) with individual dates per row. I want to find the average yearly rate per group from 2018 to 2019. I know I could create a small df with the results and merge it back in but, I was trying to find a way to use transform. Not sure if it would just be faster to merge. Extra points for one-liners.
Sample data
rng = np.random.default_rng(seed=123)
df = pd.DataFrame({'group':rng.choice(list('ABCD'), 100),
                   'date':[(pd.to_datetime('2018')+pd.Timedelta(days=x)).normalize() for x in rng.integers(0, 365*5, 100)],
                   'foo':rng.integers(1, 100, 100),
                   'bar':rng.integers(50, 200, 100)})
df['year'] = df['date'].dt.year

This works
#find average 2018 and 2019 'foo' and 'bar'
for col in ['foo', 'bar']:
    for y in [2018, 2019]:
        df[col+'_'+str(y)+'_total'] = df.groupby('group')['year'].transform(lambda x: df.loc[x.where(x==y).dropna().index, col].sum())

#find 2018 and 2019 rates
for y in [2018, 2019]:
    df['rate_'+str(y)] =  df['foo_'+str(y)+'_total'].div(df['bar_'+str(y)+'_total'])

#find average rate
df['2018_2019_avg_rate'] = df[['rate_2018', 'rate_2019']].mean(axis=1)

Thing's I've tried that don't quite work (I'm using apply to test if it works before switching to transform)
#gives yearly totals for each year and each column, but further 'apply'ing to find rates then averaging isn't working after I switch to transform
df.groupby(['group', 'year'])['year'].apply(lambda x: df.loc[x.where(x.between(2018, 2019)).dropna().index, ['foo', 'bar']].sum())

#close but is averaging too early
df.groupby(['group', 'year'])['year'].apply(lambda x: df.loc[i, 'foo'].sum()/denom if (denom:=df.loc[i:=x.where(x.between(2018, 2019)).dropna().index, 'bar'].sum())>0 else np.nan)

","You can't perform multiple filtering/aggregations efficiently with a groupby.transform. You will have to loop.
A more efficient approach would be to combine a pivot_table + merge:
cols = ['foo', 'bar']
years = [2018, 2019]

tmp = (df[df['year'].isin(years)]
       .pivot_table(index='group', columns='year',
                    values=cols, aggfunc='sum')
       [cols]
       .pipe(lambda x: x.join(pd.concat({'rate': x['foo'].div(x['bar'])}, axis=1)))
      )

avg_rate = tmp['rate'].mean(axis=1)

tmp.columns = tmp.columns.map(lambda x: f'{x[0]}_{x[1]}_total')

tmp[f'{""_"".join(map(str, years))}_avg_rate'] = avg_rate

out = df.merge(tmp, left_on='group', right_index=True)

Output:
   group       date  foo  bar  year  foo_2018_total  foo_2019_total  bar_2018_total  bar_2019_total  rate_2018_total  rate_2019_total  2018_2019_avg_rate
0      A 2022-03-11   59   91  2022             343             270             972             875         0.352881         0.308571            0.330726
1      C 2018-08-22   56   52  2018             175             325             331             902         0.528701         0.360310            0.444506
2      C 2019-04-24   47   89  2019             175             325             331             902         0.528701         0.360310            0.444506
3      A 2019-04-16   43  102  2019             343             270             972             875         0.352881         0.308571            0.330726
4      D 2019-11-25    3   56  2019             126             222             224             696         0.562500         0.318966            0.440733
5      A 2018-01-06   86  148  2018             343             270             972             875         0.352881         0.308571            0.330726
...
99     B 2018-02-25   32   90  2018             253             204             703             400         0.359886         0.510000            0.434943

",pandas
add exception handler and continue code execution,"I have this Python code used to read files:
import pandas as pd

metadata_files = []
csv_df = pd.DataFrame()
meta_files_list = (pd.read_csv(file, encoding='utf-8', sep="";"") for file in metadata_files)
meta_files_df = pd.concat(meta_files_list, ignore_index=True)

Sometimes I get an exception because files are empty at the line pd.concat.
How I can properly process this exception and continue program execution?
I want to send a message somehow to process this issue.
","You could use try and except syntax of python:
metadata_files = []
csv_df = pd.DataFrame()
meta_files_list = (pd.read_csv(file, encoding='utf-8', sep="";"") for file in 
metadata_files)
try:
  meta_files_df = pd.concat(meta_files_list, ignore_index=True)
except:
  print(""could not concat"")

Simply add what you want to do in the case of concat not working.
",pandas
cannot compare tznaive and tzaware timestamps,"I'm finding the error below:
Cannot compare tz-naive and tz-aware timestamps
How can I convert dates to fix the issue? The error appear in the end of the syntax below.
from datetime import datetime, timedelta
import pytz
# Define start and end dates
start = datetime(2024, 6, 1)
end = datetime(2024, 12, 10)
current_date = start
# List to store valid dates
valid_dates = []

while current_date <= end:
    valid_dates.append(current_date.strftime('%Y-%m-%d'))
    current_date += timedelta(days=1)

df['OpenDate'] = pd.to_datetime(df['OpenDate'], utc=True)
df['CloseDate'] = pd.to_datetime(df['CloseDate'], errors='coerce', utc=True)

date_range = pd.date_range(start=start, end=end)

results = []

for index, row in df.iterrows():
    for current_date in date_range:
        if row['OpenDate'] < current_date and (pd.isnull(row['CloseDate']) or 
row['CloseDate'] > current_date) and row['CloseType'] in [0, 1, 3, 7, 8]:
            result_row = {
                'CurrentDate': current_date,
                'User_Id': row['User_Id'],
                'OpenDate': row['OpenDate'],
                'CloseDate': row['CloseDate'],
                'CloseType': row['CloseType']
            }
            results.append(result_row)

","try this to convert to tz-aware:
start = datetime(2024, 6, 1, tzinfo=pytz.UTC)
end = datetime(2024, 12, 10, tzinfo=pytz.UTC)

And to create a tz-aware time range:
date_range = pd.date_range(start=start, end=end, tz='UTC')

Complete code:
from datetime import datetime, timedelta
import pytz
import pandas as pd

# Define start and end dates (timezone-aware)
start = datetime(2024, 6, 1, tzinfo=pytz.UTC)
end = datetime(2024, 12, 10, tzinfo=pytz.UTC)
current_date = start

# List to store valid dates
valid_dates = []
while current_date <= end:
    valid_dates.append(current_date.strftime('%Y-%m-%d'))
    current_date += timedelta(days=1)

# Convert 'OpenDate' and 'CloseDate' to timezone-aware datetimes
df['OpenDate'] = pd.to_datetime(df['OpenDate'], utc=True)
df['CloseDate'] = pd.to_datetime(df['CloseDate'], errors='coerce', utc=True)

# Create a timezone-aware date range
date_range = pd.date_range(start=start, end=end, tz='UTC')

results = []
for index, row in df.iterrows():
    for current_date in date_range:
        if row['OpenDate'] < current_date and (pd.isnull(row['CloseDate']) or 
row['CloseDate'] > current_date) and row['CloseType'] in [0, 1, 3, 7, 8]:
            result_row = {
                'CurrentDate': current_date,
                'User_Id': row['User_Id'],
                'OpenDate': row['OpenDate'],
                'CloseDate': row['CloseDate'],
                'CloseType': row['CloseType']
            }
            results.append(result_row)

Hope this helps.
",pandas
pdread_csv not working with parse_dates,"I'm using the Netflix Movies and TV Shows dataset to better understand pandas.
The column date_added is in the format: ""September 21, 2024"" which, as I understand, would be parsed as ""%B-%d-%Y"". Simply Reading with pd.read_csv() returns this column as an object type.
Using parse_dates=['date_added'] doesn't seem to work, and neither specifying the format. Here is my code:
df = pd.read_csv('netflix_titles.csv', parse_dates=['date_added'], date_format='%B-%d-%Y')
df.info()

resulting in:
<class 'pandas.core.frame.DataFrame'>
RangeIndex: 8807 entries, 0 to 8806
Data columns (total 12 columns):
 #   Column        Non-Null Count  Dtype 
---  ------        --------------  ----- 
 0   show_id       8807 non-null   object
 1   type          8807 non-null   object
 2   title         8807 non-null   object
 3   director      6173 non-null   object
 4   cast          7982 non-null   object
 5   country       7976 non-null   object
 6   date_added    8797 non-null   object
 7   release_year  8807 non-null   int64 
 8   rating        8803 non-null   object
 9   duration      8804 non-null   object
 10  listed_in     8807 non-null   object
 11  description   8807 non-null   object
dtypes: int64(1), object(11)
memory usage: 825.8+ KB

What am I doing wrong?
","I see several problems here.

incorrect date_format (as wrote @iBeMeltin and @ayki)
nan values
spaces in values

How to check:
df = pd.read_csv('netflix_titles.csv')
for rec in df.to_dict('records'):
    try:
        datetime.strptime(rec['date_added'], '%B %d, %Y')
    except (TypeError, ValueError):
        print(rec)

# {'show_id': 's6067', 'type': 'TV Show', 'title': ""A Young Doctor's Notebook and Other Stories"", 'director': nan, 'cast': 'Daniel Radcliffe, Jon Hamm, Adam Godley, Christopher Godwin, Rosie Cavaliero, Vicki Pepperdine, Margaret Clunie, Tim Steed, Shaun Pye', 'country': 'United Kingdom', 'date_added': nan ...
# {'show_id': 's8756', 'type': 'TV Show', 'title': 'Women Behind Bars', 'director': nan, 'cast': nan, 'country': 'United States', 'date_added': ' November 1, 2016'...
...

How to fix:
df = pd.read_csv('netflix_titles.csv')
df['date_added'] = df['date_added'].fillna('January 1, 1970')           # default for nan. You can skip if you don't need all values ​​as datetime
df['date_added'] = df['date_added'].str.strip()                         # remove spaces
df['date_added'] = pd.to_datetime(df['date_added'], format='%B %d, %Y') # convert to dt

df.info()

Output:
<class 'pandas.core.frame.DataFrame'>
RangeIndex: 8807 entries, 0 to 8806
Data columns (total 12 columns):
 #   Column        Non-Null Count  Dtype         
---  ------        --------------  -----         
 0   show_id       8807 non-null   object        
 1   type          8807 non-null   object        
 2   title         8807 non-null   object        
 3   director      6173 non-null   object        
 4   cast          7982 non-null   object        
 5   country       7976 non-null   object        
 6   date_added    8807 non-null   datetime64[ns]
 7   release_year  8807 non-null   int64         
 8   rating        8803 non-null   object        
 9   duration      8804 non-null   object        
 10  listed_in     8807 non-null   object        
 11  description   8807 non-null   object        
dtypes: datetime64[ns](1), int64(1), object(10)
memory usage: 825.8+ KB

",pandas
drop all data in a pandas dataframe,"I would like to drop all data in a pandas dataframe, but am getting TypeError: drop() takes at least 2 arguments (3 given). I essentially want a blank dataframe with just my columns headers.
import pandas as pd

web_stats = {'Day': [1, 2, 3, 4, 2, 6],
             'Visitors': [43, 43, 34, 23, 43, 23],
             'Bounce_Rate': [3, 2, 4, 3, 5, 5]}
df = pd.DataFrame(web_stats)

df.drop(axis=0, inplace=True)
print df

","You need to pass the labels to be dropped.
df.drop(df.index, inplace=True)

By default, it operates on axis=0.
You can achieve the same with 
df.iloc[0:0]

which is much more efficient.
",pandas
counting based on criterias in pandas,"I have a pandas DataFrame like this:
d={'gen':['A','A','A','A','B','B','B','B','C','D','D','D','D','D','D','D','D','D','D'], 'diff':pd.Series([1,1,1,1,2,1,1,1,1,1,1,1,1,2,2,1,1,1], index=[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17])}
wk = pd.DataFrame(data=d, index=[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18])

my goal is to count how many occurences the gen have based on diff with some criterias:

Count if diff is 1, and
gen at index i equal to gen at index i+1, and
If there's a consecutive 1's, then the count will be like this: if (number of consecutive 1) %2 == 0: count = number of consecutive/2, if not: count = (number of consecutive - 1) /2

with this code, I can achieve what I want:
k=0
j=0
z={}
for i in range(wk.shape[0]):
    if wk['diff'][i] == 1:
        if wk['gen'][i] == wk['gen'][i+1]:
            if j == 0:
                j+=2
            if j%2==0:
                k+=1                
            if j>=2:
                j+=1
            z[wk['gen'][i]] = k
        if wk['gen'][i] != wk['gen'][i+1]:
            j=0
            k=0

and the result of dictionary z is: {'A': 2, 'B': 1, 'D': 4}
but when I used a bigger data (more than 410,000 records), the counter didn't always start from 0 when the gen at index i is not equal to gen at index i+1. What is wrong with my code?
","Count the consecutive 1s per group with groupby.count, perform a floordiv by 2 (equivalent to your x/2 if x%2==0 else (x-1)/2), and aggregate again with groupby.sum before converting to_dict:
group = wk['diff'].ne(wk.groupby('gen')['diff'].shift()).cumsum()
m = wk['diff'].eq(1)

out = (wk[m].groupby(['gen', group])      # keep only 1s and group
             ['diff'].count().floordiv(2) # count and floor division
            .groupby(level='gen').sum()   # sum per ""gen"" group
            .loc[lambda x: x>0].to_dict() # only counts > 0 and convert to dict
      )

Output:
{'A': 2, 'B': 1, 'D': 3}

Intermediates
group and m:
   gen  diff  group      m
0    A   1.0      1   True
1    A   1.0      1   True
2    A   1.0      1   True
3    A   1.0      1   True
4    B   2.0      2  False
5    B   1.0      3   True
6    B   1.0      3   True
7    B   1.0      3   True
8    C   1.0      4   True
9    D   1.0      5   True
10   D   1.0      5   True
11   D   1.0      5   True
12   D   1.0      5   True
13   D   2.0      6  False
14   D   2.0      6  False
15   D   1.0      7   True
16   D   1.0      7   True
17   D   1.0      7   True
18   D   NaN      8  False

",pandas
split a pandas column of lists with different lengths into multiple columns,"I have a Pandas DataFrame that looks like:
ID  result
1   [.1,.5]
2   [.4,-.2,-.3,.1,0]
3   [0,.1,.6]

How can split this column of lists into two columns?
Desired result:
ID  result_1 result_2 result_3 result_4 result_5
1   .1       .5       NaN      NaN      NaN
2   .4       -.2      -.3      .1       0
3   0        .1       .6       NaN      NaN

I have digged into it a little and found this: Split a Pandas column of lists into multiple columns
but this only seems to apply to list with a constant number of elements.
Thank you so much in advance.
","You can do this as suggested in linked post.
import pandas as pd

# your example code
data = {""ID"": [1, 2, 3], ""result"": [[0.1, 0.5], [0.4, -0.2, -0.3, 0.1, 0], [0, 0.1, 0.6]]}
df = pd.DataFrame(data)
print(df)

answer
out = df[['ID']].join(
    pd.DataFrame(df['result'].tolist())
    .rename(columns=lambda x: f'result_{x + 1}')
)

out:
   ID  result_1  result_2  result_3  result_4  result_5
0   1       0.1       0.5       NaN       NaN       NaN
1   2       0.4      -0.2      -0.3       0.1       0.0
2   3       0.0       0.1       0.6       NaN       NaN

",pandas
importerror cannot import name 39json_normalize39 from 39pandasiojson39,"python 3.9.2-3
pandas 2.0.0
pandas-io           0.0.1

Error: 
from pandas.io.json import json_normalize
ImportError: cannot import name 'json_normalize' from 'pandas.io.json' (/home/casaos/.local/lib/python3.9/site-packages/pandas/io/json/__init__.py)

Apparently this was a problem early on in the pre 1x days of pandas, but seems to have resurfaced.
Suggestions?
I'm running a script which was  functional previously, but migrating it to a new host.
It errors out on the line:
from pandas.io.json import json_normalize

and throws the error
ImportError: cannot import name 'json_normalize' from 'pandas.io.json' (/home/casaos/.local/lib/python3.9/site-packages/pandas/io/json/__init__.py)

I've attempted to reinstall pandas ('install' option), remove and reinstall, and 'install --force-reinstall' all performed as root so that the base install  of python3 has it installed as opposed to a single user
","This was indeed the solution simply removing the import line. I'd have liked an attribute to check to determine the pandas version installed easily, but have to settle for a try:except: to determine if the import is needed.
pandas.io.json.json_normalize was deprecated and removed in the newest version. Use pandas.json_normalize. Also, the tutorial you were following is most probably severely outdated. You are on your own now. –
Ξένη Γήινος
Apr 23 at 7:48
",pandas
sorting columns in pandas dataframe based on column name,"I have a dataframe with over 200 columns. The issue is as they were generated the order is
['Q1.3','Q6.1','Q1.2','Q1.1',......]

I need to sort the columns as follows:
['Q1.1','Q1.2','Q1.3',.....'Q6.1',......]

Is there some way for me to do this within Python?
","df = df.reindex(sorted(df.columns), axis=1)

This assumes that sorting the column names will give the order you want.  If your column names won't sort lexicographically (e.g., if you want column Q10.3 to appear after Q9.1), you'll need to sort differently, but that has nothing to do with pandas.
",pandas
removing index column in pandas when reading a csv,"I have the following code which imports a CSV file.  There are 3 columns and I want to set the first two of them to variables.  When I set the second column to the variable ""efficiency"" the index column is also tacked on.  How can I get rid of the index column?
df = pd.DataFrame.from_csv('Efficiency_Data.csv', header=0, parse_dates=False)
energy = df.index
efficiency = df.Efficiency
print efficiency

I tried using 
del df['index']

after I set 
energy = df.index

which I found in another post but that results in ""KeyError: 'index' ""
","DataFrames and Series always have an index. Although it displays alongside the column(s), it is not a column, which is why del df['index'] did not work.
If you want to replace the index with simple sequential numbers, use df.reset_index(). 
To get a sense for why the index is there and how it is used, see e.g. 10 minutes to Pandas. 
",pandas
calculate all missing values for specific data using pivot tables in pandas,"I am working on this dataset called titanic.csv Let's simplify the problem and include some data here:

I need to calculate all missing values for child, as you see it is a value under who column. This should be done using a pivot table.
I have tried this solution:
pd.pivot_table(df[df['who'] == 'child'], 
index='sex', 
aggfunc=lambda x: x.isnull().sum(), 
 margins=True) # to sum all missing values based on gender

But I get this output: 
in which as you also notice, ALL row doesn't sum all missing values per gender.
Where is the problem in my code? Should I use another way to create the pivot table?
","EDIT:
If you prefer to use a pivot table, just add the parameter dropna=False to get the result you want.

First answer:
If you want the number of missing values per features for only the child you can use isna/isnull directly after filtering:
data = {'survived': [0, 1, 1, 1, 0], 
        'pclass': [3, 1, None, 1, 3], 
        'sex': ['male', 'female', 'female', 'female', 'male'], 
        'age': [22, 38, None, None, 35], 
        'class_': ['Third', 'First', None, 'First', 'Third'], 
        'who': ['man', 'woman', 'child', 'child', 'man'], 
        'deck': [None, 'C', None, 'C', None], 
        'alive': ['no', 'yes', 'yes', 'yes', 'no'], 
        'alone': [False, False, True, False, True] } 
df = pd.DataFrame(data)

display(df[df[""who""] == ""child""].isna().sum())

survived    0
pclass      1
sex         0
age         2
class_      1
who         0
deck        1
alive       0
alone       0

",pandas
convert pandas multiindex series to json python,"Hi I have two pandas series similar to below
PnL 
           Product Name      Price
Company A  Orange            3000
Company B  Apple             2000
           Grapes            1000

Tax
           Product Name      Price
Company A  Orange            100
Company B  Apple             100
           Grapes            10

I would like to transform the pandas series into the following JSON format
{'PnL':{'Company A':{'productName':'Orange','price':3000},
        'Company B':[{'productName':'Apple','price':2000},
                     {'productName':'Grapes','price':1000}]
       },
 'Tax':{'Company A':{'productName':'Orange','price':100},
        'Company B':[{'productName':'Apple','price':100},
                     {'productName':'Grapes','price':10}]
       }
}

I have tried to use the code below
convertedJson = json.dumps([{'company': k[0], 'productName':k[1],'price': v} for k,v in df.items()])

but I cannot form the JSON which I want to produce.
Thank you for your help
","You can use concat for join DataFrames together and then groupby with to_dict for expected output:
df = pd.concat([s1, s2], keys=('PnL','Tax')).reset_index()
df.columns = ['type','company','productName','price']
print (df)
  type    company productName  price
0  PnL  Company A      Orange   3000
1  PnL  Company B       Apple   2000
2  PnL  Company B      Grapes   1000
3  Tax  Company A      Orange   3000
4  Tax  Company B       Apple   2000
5  Tax  Company B      Grapes   1000

d = (df.groupby(['type','company'])['productName','price']
       .apply(lambda x: x.to_dict('r'))
       .reset_index(name='data')
       .groupby('type')['company','data']
       .apply(lambda x: x.set_index('company')['data'].to_dict())
       .to_json()
       )


print (d)

{
    ""PnL"": {
        ""Company A"": [{
            ""productName"": ""Orange"",
            ""price"": 3000
        }],
        ""Company B"": [{
            ""productName"": ""Apple"",
            ""price"": 2000
        }, {
            ""productName"": ""Grapes"",
            ""price"": 1000
        }]
    },
    ""Tax"": {
        ""Company A"": [{
            ""productName"": ""Orange"",
            ""price"": 3000
        }],
        ""Company B"": [{
            ""productName"": ""Apple"",
            ""price"": 2000
        }, {
            ""productName"": ""Grapes"",
            ""price"": 1000
        }]
    }
}

",pandas
deep copy of pandas dataframes and dictionaries,"I'm creating a small Pandas dataframe:
df = pd.DataFrame(data={'colA': [[""a"", ""b"", ""c""]]})

I take a deepcopy of that df. I'm not using the Pandas method but general Python, right? 
import copy
df_copy = copy.deepcopy(df)

A df_copy.head() gives the following: 

Then I put these values into a dictionary: 
mydict = df_copy.to_dict()

That dictionary looks like this:

Finally, I remove one item of the list:
mydict['colA'][0].remove(""b"")

I'm surprized that the values in df_copy are updated. I'm very confused that the values in the original dataframe are updated too! Both dataframes look like this now:

I understand Pandas doesn't really do deepcopy, but this wasn't a Pandas method. My questions are:
1) how can I build a dictionary from a dataframe that doesn't update the dataframe?
2) how can I take a copy of a dataframe which would be completely independent?
thanks for your help!
Cheers,
Nicolas
","TLDR
To get deepcopy:
df_copy = pd.DataFrame(
    columns = df.columns, data = copy.deepcopy(df.values)
)

Disclaimer

Notice that putting mutable objects inside a DataFrame can be an antipattern so make sure you need it and understand what you are doing.


Why your copy is not independent

When applied on an object, copy.deepcopy is looked up for a _deepcopy_ method of that object, that is called in turn. It's added to avoid copying too much for objects. In the case of a DataFrame instance in version 0.20.0 and above - _deepcopy_ doesn`t work recursively.
Similarly, if you will use DataFrame.copy(deep=True) deep copy will copy the data, but will not do so recursively. .


How to solve the problem

To take a truly deep copy of a DataFrame containing a list(or other python objects), so that it will be independent - you can use one of the methods below.
df_copy = pd.DataFrame(
    columns = df.columns, data = copy.deepcopy(df.values)
)

For a dictionary, you may use same trick:
mydict = pd.DataFrame(
    columns = df.columns, data = copy.deepcopy(df_copy.values)
).to_dict()
mydict['colA'][0].remove(""b"")

There's also a standard hacky way of deep-copying python objects:
import pickle
df_copy = pickle.loads(pickle.dumps(df))  

Feel free to ask for any clarifications, if needed.
",pandas
merge a list of pandas dataframes,"I have a list of data frames and I need to merge them together using a unique column (date). Field names are different so concat is out.
I can manually use df[0].merge(df[1],on='Date').merge(df[3],on='Date) etc. to merge each df one by one, but the issue is that the number of data frames in the list differs with user input.
Is there any way to merge that just combines all data frames in a list in one go? Or perhaps some for loop that does that?
","You can use reduce function where dfList is your list of data frames:
import pandas as pd
from functools import reduce
reduce(lambda x, y: pd.merge(x, y, on = 'Date'), dfList)

As a demo:
df = pd.DataFrame({'Date': [1,2,3,4], 'Value': [2,3,3,4]})
dfList = [df, df, df]
dfList

# [   Date  Value
#  0     1      2
#  1     2      3
#  2     3      3
#  3     4      4,    Date  Value
#  0     1      2
#  1     2      3
#  2     3      3
#  3     4      4,    Date  Value
#  0     1      2
#  1     2      3
#  2     3      3
#  3     4      4]

reduce(lambda x, y: pd.merge(x, y, on = 'Date'), dfList)
#   Date  Value_x  Value_y  Value
# 0    1        2        2      2
# 1    2        3        3      3
# 2    3        3        3      3
# 3    4        4        4      4

",pandas
how to get rid of quotunnamed 0quot column in a pandas dataframe read in from csv file,"I have a situation wherein sometimes when I read a csv from df I get an unwanted index-like column named unnamed:0. 
file.csv

,A,B,C
0,1,2,3
1,4,5,6
2,7,8,9

The CSV is read with this:
pd.read_csv('file.csv')

   Unnamed: 0  A  B  C
0           0  1  2  3
1           1  4  5  6
2           2  7  8  9

This is very annoying! Does anyone have an idea on how to get rid of this?
","It's the index column, pass pd.to_csv(..., index=False) to not write out an unnamed index column in the first place, see the to_csv() docs.
Example:
In [37]:
df = pd.DataFrame(np.random.randn(5,3), columns=list('abc'))
pd.read_csv(io.StringIO(df.to_csv()))

Out[37]:
   Unnamed: 0         a         b         c
0           0  0.109066 -1.112704 -0.545209
1           1  0.447114  1.525341  0.317252
2           2  0.507495  0.137863  0.886283
3           3  1.452867  1.888363  1.168101
4           4  0.901371 -0.704805  0.088335

compare with:
In [38]:
pd.read_csv(io.StringIO(df.to_csv(index=False)))

Out[38]:
          a         b         c
0  0.109066 -1.112704 -0.545209
1  0.447114  1.525341  0.317252
2  0.507495  0.137863  0.886283
3  1.452867  1.888363  1.168101
4  0.901371 -0.704805  0.088335

You could also optionally tell read_csv that the first column is the index column by passing index_col=0:
In [40]:
pd.read_csv(io.StringIO(df.to_csv()), index_col=0)

Out[40]:
          a         b         c
0  0.109066 -1.112704 -0.545209
1  0.447114  1.525341  0.317252
2  0.507495  0.137863  0.886283
3  1.452867  1.888363  1.168101
4  0.901371 -0.704805  0.088335

",pandas
how to use lagged timeseries variables in a python pandas regression model,"I'm creating time-series econometric regression models. The data is stored in a Pandas data frame.
How can I do lagged time-series econometric analysis using Python? I have used Eviews in the past (which is a standalone econometric program i.e. not a Python package). To estimate an OLS equation using Eviews you can write something like:
equation eq1.ls log(usales) c log(usales(-1)) log(price(-1)) tv_spend radio_spend

Note the lagged dependent and lagged price terms. It's these lagged variables which seem to be difficult to handle using Python e.g. using scikit or statmodels (unless I've missed something).
Once I've created a model I'd like to perform tests and use the model to forecast.
I'm not interested in doing ARIMA, Exponential Smoothing, or Holt Winters time-series projections - I'm mainly interested in time-series OLS.
","pandas allows you to shift your data without moving the index. The shift function on a dataframe df allows creating leads and lags.
df.shift(-1)

will create a 1 index lead into the future and
df.shift(1)

will create a 1 period lag.
So if you have a daily time series, you could use df.shift(1) to create a 1 day lag in your values of price such as
df['lagprice'] = df['price'].shift(-1)

After that, if you want to do OLS you can look at the scipy module here :
http://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.stats.linregress.html
",pandas
interpolating time series data for step values,"I have time series data that looks like this (mm/dd hh:mm):
3.100   12/14 05:42
3.250   12/14 05:24
3.300   12/14 05:23
3.600   12/14 02:45
3.700   12/13 10:54
3.600   12/12 13:19
3.900   12/12 10:43

I need to interpolate it at 1 minute intervals. It will be a step chart, so the values should be the same until the new value.
","If your goal is to make a step plot, no need to interpolate, just use matplotlib.pyplot.step:
import matplotlib.pyplot as plt

s = pd.Series(['12/14 05:42', '12/14 05:24', '12/14 05:23', '12/14 02:45', '12/13 10:54', '12/12 13:19', '12/12 10:43'],
              index=[3.1, 3.25, 3.3, 3.6, 3.7, 3.6, 3.9])

plt.step(pd.to_datetime(s, format='%m/%d %H:%M'), s.index)

NB. assuming here the values are the index and the dates the series' values, which is a bit counterintuitive. Better use the date as index.
Output:

Also, be aware that without a year, the default will be to use 1900 during the conversion to datetime, which might be unwanted. Better be explicit and add the exact year.

If you really want to interpolate, use the date as index and asfreq:
s = pd.Series([3.1, 3.25, 3.3, 3.6, 3.7, 3.6, 3.9],
              index=['12/14 05:42', '12/14 05:24', '12/14 05:23', '12/14 02:45', '12/13 10:54', '12/12 13:19', '12/12 10:43'])

s.index = pd.to_datetime(s.index, format='%m/%d %H:%M')

out = s.asfreq('min', method='ffill')

Output:
1900-12-12 10:43:00    3.9
1900-12-12 10:44:00    3.6
1900-12-12 10:45:00    3.6
1900-12-12 10:46:00    3.6
1900-12-12 10:47:00    3.6
                      ... 
1900-12-14 05:38:00    3.1
1900-12-14 05:39:00    3.1
1900-12-14 05:40:00    3.1
1900-12-14 05:41:00    3.1
1900-12-14 05:42:00    3.1
Freq: T, Length: 2580, dtype: float64

",pandas
how to use numpywhere in a pipe function for pandas dataframe groupby,"Here is a script to simulate the issue I am facing:
import pandas as pd
import numpy as np

data = {
        'a':[1,2,1,1,2,1,1],
        'b':[10,40,20,10,40,10,20],
        'c':[0.3, 0.2, 0.6, 0.4, 0.5, 0.2, 0.8],
        'd':[3, 1, 5, 1, 7, 2., 2.],
        }

df = pd.DataFrame.from_dict(data)

# I apply some custom function to populate column 'e'.
# For demonstration, I am using a very simple function here.
df['e']=df.apply(lambda x: x['c']<=0.3, axis=1)

# This is the column I need to obtain using groupby and pipe/transform
df['f']=[2., 1., 0., 2., 1., 2., 0.]

print(df)

Output:
   a   b    c    d      e    f
0  1  10  0.3  3.0   True  2.0
1  2  40  0.2  1.0   True  1.0
2  1  20  0.6  5.0  False  0.0
3  1  10  0.4  1.0  False  2.0
4  2  40  0.5  7.0  False  1.0
5  1  10  0.2  2.0   True  2.0
6  1  20  0.8  2.0  False  0.0

The logic to be used to find column f is as follows:
For each group of df.groupby(['a', 'b']):

select entries with True value for e.
if there are any item in the selected array:

find entry with minimum d and return d (in real application, d needs to be manipulated in conjunction with other columns, and the result would be returned)


else:

return 0



What I have tried:
def func(x):
    print(type(x))
    print(x)
    print('-'*50)

    ind=np.where(x['e']) #<--- How can I implement this?

    if len(ind)>0:
        ind_min=np.argmin(x.iloc[ind]['d'])
        return x.iloc[ind[ind_min]]['d']
    else:
        return 0
    
df['g']=df.groupby(['a', 'b']).pipe(func)

Output:
<class 'pandas.core.groupby.generic.DataFrameGroupBy'>
<pandas.core.groupby.generic.DataFrameGroupBy object at 0x000001B348735550>
--------------------------------------------------
...
ValueError: setting an array element with a sequence. The requested array has an inhomogeneous shape after 2 dimensions. The detected shape was (3, 2) + inhomogeneous part.

I get the mentioned error on line: ind=np.where(x['e']) #<--- How can I implement this?
So, how can apply np.where on a pandas.core.groupby.generic.DataFrameGroupBy object?
","You're presenting the XY problem. Here's one approach:
cond = df['c'] <= 0.3

df['f'] = (
    df.assign(filtered_d=df['d'].where(cond))
    .groupby(['a', 'b'])['filtered_d']
    .transform('min')
    .fillna(0)
    )

Output:
   a   b    c    d      e    f
0  1  10  0.3  3.0   True  2.0
1  2  40  0.2  1.0   True  1.0
2  1  20  0.6  5.0  False  0.0
3  1  10  0.4  1.0  False  2.0
4  2  40  0.5  7.0  False  1.0
5  1  10  0.2  2.0   True  2.0
6  1  20  0.8  2.0  False  0.0

Explanation / Intermediate

First, apply Series.where to column 'd' based on the boolean series to keep only the values that we want to consider for min:

# df['d'].where(cond)

0    3.0
1    1.0
2    NaN
3    NaN
4    NaN
5    2.0
6    NaN
Name: d, dtype: float64


Use df.assign to include the result (here as filtered_d) and apply df.groupby + groupby.transform to get 'min'.
Finally, add Series.fillna and assign as a new column.


An alternative way to do this could be:
cond = df['c'] <= 0.3

df['f'] = (
    df.merge(
        df[cond]
        .groupby(['a', 'b'], as_index=False)
        .agg(f=('d', 'min')),
        on=['a', 'b'],
        how='left'
    ).assign(f=lambda x: x['f'].fillna(0))
)

Explanation

Use boolean indexing + df.groupby + groupby.agg + named aggregation.
Merge with df via df.merge + chain df.assign to apply Series.fillna.

",pandas
convert cardinal wind directions to degrees,"I have a Pandas dataframe like below with cardinal wind directions :
|  X       |
+----------+
|N         |
|NE        | 
|NNE       | 
|SSE       | 
|WSW       |
+----------+

Question is how can I convert the wind directions to degrees and store in a dataframe/excel ?
","Firstly create a dictionary:
d={'N':0, 'NNE':22.5,""NE"":45,""ENE"":67.5, 'E':90,'ESE':112.5, 'SE':135,'SSE':157.5, 'S':180,'SSW':202.5, 'SW':225,'WSW':247.5, 'W':270,'WNW':292.5,'NW':315,'NNW':337.5, 'N':0,'North':0,'East':90,'West':270,'South':180}

Finally use strip() method and map() method:
df['X']=df['X'].str.strip().map(d)

OR
use strip() and replace() method
df['X']=df['X'].str.strip().replace(d)

Output of df:
    X
0   0.0
1   45.0
2   22.5
3   157.5
4   247.5

",pandas
how to vectorize timezone offset calculation in pandas datetimeindex for improved performance,"I'm working with a Pandas DataFrame that utilizes a DatetimeIndex with timezone information. My objective is to compute the timezone offset (in hours) for each timestamp and store these offsets in a new column within the DataFrame.
Current Approach:
Currently, I'm using the .map() function combined with a lambda to extract the UTC offset from each timestamp:
import pandas as pd

# Sample DataFrame setup
timestamps = pd.date_range('2024-01-01 00:00:00', '2024-12-31 23:59:59', freq='5min', tz='Europe/Brussels')
df = pd.DataFrame({'value': range(len(timestamps))}, index=timestamps)

# Computing timezone_offset using .map() and lambda
df['timezone_offset'] = df.index.map(
    lambda x: x.utcoffset().total_seconds() / 3600 if x.utcoffset() else 0)

print(df['timezone_offset'][:5])

OUTPUT
2024-01-01 00:00:00+01:00    1.0
2024-01-01 00:05:00+01:00    1.0
2024-01-01 00:10:00+01:00    1.0
2024-01-01 00:15:00+01:00    1.0
2024-01-01 00:20:00+01:00    1.0

Issue:
While this method accurately populates the 'timezone_offset' column, it becomes significantly slow as the size of the DataFrame grows. Processing 100,000 entries takes a considerable amount of time, which is a bottleneck for larger datasets or real-time applications.
Objective:
I aim to vectorize the timezone offset calculation to enhance performance and reduce computation time. Ideally, I want to avoid using row-wise operations like .map() with lambda functions, which are known to be inefficient with large datasets.
","One fast solution:
df[""offset""] = df.index.tz_localize(None) - df.index.tz_convert('UTC').tz_localize(None)
df[""offset""] = df[""offset""].dt.total_seconds() / 3600


",pandas
how to store the result of a datafram grouping seperatly using pandas python,"I'm facing a challenge when I'm trying to group a dataframe, here's a fake datafram which similir to the real one :
**univertcity   country     sector  name    firstname   code**
Evergreen College   USA     URO    Isabel   Emily       694123
Evergreen College   USA     URO    Rami     David       63123
Evergreen College   Swiss   URO   Johnson   Parker      114196
Cascade University  CANADA  DIG   Anthony   Jessica     55177
Cascade University  CANADA  VIP   Michael   Thierry     124199
Horizon Institute   FRANCE  MP    Ben       Samuel      25896
Horizon Institute   FRANCE  MP    Benjamin  John        52366

what I'm looking for is to group by univercity col the country and finaly the sector, and I would like (somehow) to fetch each grouping sepratly..because i woulf like to display each grouping using html table..the is what i intend to get :

could you help please ?
","Just use groupby:
gp = df.groupby([""univertcity"", ""country"", ""sector""])
for key, item in gp:
    print(gp.get_group(key), ""\n\n"")


",pandas
pandas dataframe reshape with columns name,"I have a dataframe like this:
>>> df
  TYPE    A    B    C    D
0   IN  550  350  600  360
1  OUT  340  270  420  190

I want reshape it to this shape:
       AIN AOUT  BIN BOUT  CIN COUT  DIN DOUT
       550  340  350  270  600  420  360  190

So I use these codes to do it:
ds = df.melt().T.iloc[1:,2:]
ds.columns = ['AIN','AOUT','BIN','BOUT','CIN','COUT','DIN','DOUT']
>>> ds
       AIN AOUT  BIN BOUT  CIN COUT  DIN DOUT
value  550  340  350  270  600  420  360  190

It works, but it seems stupid, the columns name was manual inputed, I wonder if there's a better way more pythonic to do this. Any idea?
P.S. the ""value"" in output dataframe is insignificant.
","Code
Apply join function(python) with map function(pandas) to Multi-index.
out = df.assign(index=0).pivot(index='index', columns='TYPE')
out.columns = out.columns.map(''.join)

out:
       AIN  AOUT  BIN  BOUT  CIN  COUT  DIN  DOUT
index                                            
0      550   340  350   270  600   420  360   190

I chose pivot function because it is inconvenient when creating a 1-row dataframe because both melt and stack require T. (If I were creating a series or 1-column dataframe, I would have chosen melt or stack.)
Example Code
import pandas as pd
data = {'TYPE': ['IN', 'OUT'], 
        'A': [550, 340],
        'B': [350, 270],
        'C': [600, 420],
        'D': [360, 190]}
df = pd.DataFrame(data)

",pandas
pandas dataframe multiindex  calculate mean and add additional column to each level of the index,"Given the following dataframe:
Year     2024          2023          2022
Header   N Result SD   N Result SD   N Result SD
Vendor
A        5 20     3    5 22     4    1 21     3
B        4 25     2    4 25     3    4 26     5
C        9 22     3    9 27     1    3 23     3   
D        3 23     5    3 16     2    5 13     4 
E        5 27     2    5 21     3    3 19     5

I would like to calculate for each year the mean value of the results column and then create a column, where the relative deviation to the mean is displayed (e.g. Results Value / mean-value * 100). The N and SD column were just included for completeness and is not needed for the calculation.
Year     2024                    2023                      2022
Header   N Result SD Deviation   N Result SD Deviation     N Result SD Deviation
Vendor
A        5 20     3  85.5        5 22     4  99.1          1 21     3  ..
B        4 25     2  106         4 25     3  113           4 26     5  ..
C        9 22     3  ..          9 27     1  ..            3 23     3  .. 
D        3 23     5  ..          3 16     2  ..            5 13     4  ..
E        5 27     2  ..          5 21     3  ..            3 19     5  ..

How what i be able to achieve that?
Thanks a lot in advance!
","Use DataFrame.xs for select Result labels in MultiIndex, divide by mean and append to original in concat, last for correct position add DataFrame.sort_index with parameter sort_remaining=False:
df1 = df.xs('Result', axis=1, level=1, drop_level=False)

out = (pd.concat([df, 
                 df1.div(df1.mean()).mul(100)
                    .rename(columns={'Result':'Deviation'})], axis=1)
         .sort_index(axis=1, ascending=False, level=0, sort_remaining=False))
print (out)
  2024                       2023                       2022            \
     N Result SD   Deviation    N Result SD   Deviation    N Result SD   
A    5     20  3   85.470085    5     22  4   99.099099    1     21  3   
B    4     25  2  106.837607    4     25  3  112.612613    4     26  5   
C    9     22  3   94.017094    9     27  1  121.621622    3     23  3   
D    3     23  5   98.290598    3     16  2   72.072072    5     13  4   
E    5     27  2  115.384615    5     21  3   94.594595    3     19  5   

               
    Deviation  
A  102.941176  
B  127.450980  
C  112.745098  
D   63.725490  
E   93.137255  

Another loop idea:
for x in df.columns.levels[0]:
    df[(x, 'Deviation')] = df[(x, 'Result')].div(df[(x, 'Result')].mean()).mul(100)

out = df.sort_index(axis=1, ascending=False, level=0, sort_remaining=False)
print (out)
  2024                       2023                       2022            \
     N Result SD   Deviation    N Result SD   Deviation    N Result SD   
A    5     20  3   85.470085    5     22  4   99.099099    1     21  3   
B    4     25  2  106.837607    4     25  3  112.612613    4     26  5   
C    9     22  3   94.017094    9     27  1  121.621622    3     23  3   
D    3     23  5   98.290598    3     16  2   72.072072    5     13  4   
E    5     27  2  115.384615    5     21  3   94.594595    3     19  5   

               
    Deviation  
A  102.941176  
B  127.450980  
C  112.745098  
D   63.725490  
E   93.137255  

",pandas
pandas multi index subset selection,"import pandas as pd
import numpy as np

# Sample data
index = pd.MultiIndex.from_tuples([
    ('A', 'a1', 'x'),
    ('A', 'a1', 'y'),
    ('A', 'a2', 'x'),
    ('A', 'a2', 'y'),
    ('B', 'b1', 'x'),
    ('B', 'b1', 'y'),
    ('B', 'b2', 'x'),
    ('B', 'b2', 'y')
], names=['level_1', 'level_2', 'level_3'])

data = np.random.randn(len(index))
df = pd.DataFrame(data, index=index, columns=['value'])

Say I have for example the above dataframe, which is multi-indexed with 3 levels.
Now, my goal is to select a subset from this dataframe, where the first two levels of index comes from a subset of the cartesian product (A,B) * (a1, a2, b1, b2), say S = [(A, a1), (B, b2)]. I want to keep third level of the multi-index.
I expect the result to be like
Original DataFrame:
                        value
level_1 level_2 level_3       
A       a1      x     0.123456
                y     0.234567
        a2      x     0.345678
                y     0.456789
B       b1      x     0.567890
                y     0.678901
        b2      x     0.789012
                y     0.890123

Subset DataFrame:
                        value
level_1 level_2 level_3       
A       a1      x     0.123456
                y     0.234567
B       b2      x     0.789012
                y     0.890123

","Use DataFrame.droplevel for remove 3 level, so possible filter by subset by Index.isin in boolean indexing:
S = [('A', 'a1'), ('B', 'b2')]

out = df[df.droplevel(2).index.isin(S)]
print (out)
                            value
level_1 level_2 level_3          
A       a1      x        0.545790
                y       -1.298511
B       b2      x        0.018436
                y       -1.076408

",pandas
not getting decimals when extracting values,"So I am practicing data wrangling and I have encountered an issue.
food['GPA'].unique()

And the output is
array(['2.4', '3.654', '3.3', '3.2', '3.5', '2.25', '3.8', '3.904', '3.4',
       '3.6', '3.1', nan, '4', '2.2', '3.87', '3.7', '3.9', '2.8', '3',
       '3.65', '3.89', '2.9', '3.605', '3.83', '3.292', '3.35',
       'Personal ', '2.6', '3.67', '3.73', '3.79 btch', '2.71', '3.68',
       '3.75', '3.92', 'Unknown', '3.77', '3.63', '3.882'], dtype=object)

My idea is to convert them to strings first and then extract the floats and integers from them. But when I run the code
food['GPA'] = food['GPA'].astype(str).str.extract('(\d*\.\d+|\d+)', expand=False)
food['GPA'] = pd.to_numeric(food['GPA'], errors='coerce')

all the values in the GPA column are being converted to 3.0 and 4.0 instead of retaining their decimal values.
food['GPA'].unique()

[3. 2. 4.]

Can anyone help me figure out why the decimals are being lost, and how to preserve them?
","You need to add an r to make a raw string so the backslashes will be interpreted correctly
food['GPA'] = food['GPA'].astype(str).str.extract(r'(\d*\.\d+|\d+)', expand=False)
food['GPA'] = pd.to_numeric(food['GPA'], errors='coerce')
print(food['GPA'].unique())

",pandas
