instruction,input,output,Categories
possibility to load documents with azure document intelligence from azure container storage,"I have a storage account with Azure Container Storage configured consisting of multiple pdf/word/excel files. I would like to use Azure Document Intelligence to semantically chunk these files.
Is there a possibility to load the files directly from Container Storage to Azure Document Intelligence using langchain? According to the langchain docs it seems like either file has to be locally available or public url has to be handed over.
Attempt:
# Prerequisite: An Azure AI Document Intelligence resource in one of the 3 preview regions: East US, West US2, West Europe

import os
from langchain_community.document_loaders import AzureAIDocumentIntelligenceLoader

file_path = ""storage-path-to-file""
endpoint = os.getenv(""DOCUMENTINTELLIGENCE_ENDPOINT"")
key = os.getenv(""DOCUMENTINTELLIGENCE_API_KEY"")

loader = AzureAIDocumentIntelligenceLoader(
    api_endpoint=endpoint, api_key=key, file_path=file_path, api_model=""prebuilt-layout""
)

documents = loader.load()

# Returns:
# Message: Invalid request.
# Inner error: {
#    ""code"": ""InvalidManagedIdentity"",
#    ""message"": ""The managed identity configuration is invalid: Managed identity is not enabled  # for the current resource.""
# }

","
Is there a possibility to load the files directly from Container Storage to Azure Document Intelligence using langchain? According to the langchain  docs it seems like either file has to be locally available or public url has to be handed over.

You can use the below code which loads the files directly from Azure Blob storage using Azure Blob URL + SAS token.
Code:
import os
from langchain_community.document_loaders import AzureAIDocumentIntelligenceLoader

#bloburl + ?Sastoken
URL = ""https://venkat78932.blob.core.windows.net/test/24005356.pdf?sp=r&st=2024-07-09T12:41:41Z&se=2024-07-09T20:41:41Z&spr=https&sv=2022-11-02&sr=b&sig=eupT8WGH5ojQpXYd%2xxxxxD""
endpoint = os.getenv(""DOCUMENTINTELLIGENCE_ENDPOINT"")
key = os.getenv(""DOCUMENTINTELLIGENCE_API_KEY"")


loader = AzureAIDocumentIntelligenceLoader(
    api_endpoint=endpoint, api_key=key, url_path=URL, api_model=""prebuilt-layout""
)

documents = loader.load()

print(len(documents))
print(documents[0])

Output:
1
page_content='Word Documents Template\n===\n\n\n## Main heading:\n\nUse the Heading 1 style for primary headings so that screen readers can identify them as such.\n\nIf not already, manually change your heading 1 style to be:\n\n\\- sans serif (e.g. Arial, Verdana, Trebuchet or Calibri),\n\n\\- 16 pt, and\n\n\\- Bold\n\nThen set this formatting as your default for this style.\n\n\n## Sub Headings:\n\nUse Heading 2 style for sub headings.\n\nIf not already, manually change your heading 2 style to be:\n\n\\- sans serif (e.g. Arial, Verdana, Trebuchet or Calibri...................tands out, and does not distort the shape of text as italics and underlining do. Finally, block capitals can be difficult to follow as block capitals remove the natural shape of words, turning them into blocks. Clear layout allows one to focus on the content of visual materials rather than the format.\n\n\n## Furthermore\n\nIf you use headings it makes the creation and upkeep of tables of contents easier (For automatic creation and updating go to: Insert - Reference - Index and Tables - Table of contents).\n'


You can get the Azure Blob URL + SAS token from portal.
Portal -> Storage account -> Container -> your file -> Generate sas token -> click Generate sas token and url
Portal:

Reference:
Azure AI Document Intelligence | ðŸ¦œï¸ðŸ”— LangChain
",langchain
get last executed command in bash,"I need to know what was the last command executed while setting my bash prompt in the function corresponding to PROMPT_COMMAND. I have code as follows
function bash_prompt_command () { 
...
    local last_cmd=""$(history | tail -n 2 | head -n 1  | tr -s ' ' | cut -d ' ' -f3-)""
    [[ ${last_cmd} =~ .*git\s+checkout.* ]] && ( ... )
...
}

Is there is faster(bash built-in way) to know the what was the  command which invoked PROMPT_COMMAND.
I tried using BASH_COMMAND, but that too does not return the command which actually invoked PROMPT_COMMAND.
","General case: Collecting all commands
You can use a DEBUG trap to store each command before it's run.
store_command() {
  declare -g last_command current_command
  last_command=$current_command
  current_command=$BASH_COMMAND
  return 0
}
trap store_command DEBUG

...and thereafter you can check ""$last_command""

Special case: Only trying to shadow one (sub)command
If you only want to change how one command operates, you can just shadow that one command. For git checkout:
git() {
  # if $1 is not checkout, just run real git and pretend we weren't here
  [[ $1 = checkout ]] || { command git ""$@""; return; }
  # if $1 _is_ checkout, run real git and do our own thing
  local rc=0
  command git ""$@"" || rc=$?
  ran_checkout=1 # ...put the extra code you want to run here...
  return ""$rc""
}

...potentially used from something like:
bash_prompt_command() {
  if (( ran_checkout )); then
    ran_checkout=0
    : ""do special thing here""
  else
    : ""do other thing here""
  fi
}

",prompt
how do you get the context window of open ai models,"Is there a specific API call to get the context window of each model open ai offers?
I have printed out openai.Models.list() and there isn't any context window member in the json spit out.
","That information would probably best fit in the OpenAI API endpoint:
GET https://api.openai.com/v1/models
But the API reference shows it doesn't currently return the model's context window :(
Seems like the best available ""source of truth"" for this information is the Models page in the documentation.
",chatgpt
how to not display duplicate modules on ghci prompt,"Currently this is how my ghci prompt looks like:

and I want to make it so that my prompt doesn't display duplicate modules as shown below:

but I can't really figure out how. My configuration(ghci.conf) file's contents is as shown below:
:set +m

import qualified IPPrint
import qualified Language.Haskell.HsColour as HsColour
import qualified Language.Haskell.HsColour.Colourise as HsColour
import qualified Language.Haskell.HsColour.Output as HsColour

let myColourPrefs = HsColour.defaultColourPrefs { HsColour.conid = [HsColour.Foreground HsColour.Yellow, HsColour.Bold], HsColour.conop = [HsColour.Foreground HsColour.Yellow], HsColour.string = [HsColour.Foreground HsColour.Green], HsColour.char = [HsColour.Foreground HsColour.Cyan], HsColour.number = [HsColour.Foreground HsColour.Red, HsColour.Bold], HsColour.layout = [HsColour.Foreground HsColour.White], HsColour.keyglyph = [HsColour.Foreground HsColour.White] }

let myPrint = putStrLn . HsColour.hscolour (HsColour.TTYg HsColour.XTerm256Compatible) myColourPrefs False False """" False . IPPrint.pshow

:set -interactive-print=myPrint

:{
prompter :: [String] -> Int -> IO String
prompter modules line = return $
    concat [ ""\ESC[33m\STX[Module(s): ""
           -- this is the only line that changed
           , Data.List.intercalate "", "" $ zipWith (\n m -> concat [show n, ""."", m]) [1..] modules
           , ""]\ESC[0m\STX\n \ESC[38;5;86m\STX\x03BB > \ESC[0m\STX""
           ]   
:}
:set prompt-function prompter
clear = putStr ""\ESC[2J\ESC[H""


Thank you in advance.
","By using nub which removes duplicate elements from a list (see: https://hoogle.haskell.org/?hoogle=nub), I was able to remove duplicate elements from the module list as shown below:

In order to do that the code also has to be modified to:
:{
prompter :: [String] -> Int -> IO String
prompter modules line = return $
    concat [ ""\ESC[33m\STX[Module(s): ""
           -- this is the only line that changed
           , Data.List.intercalate "", "" $ zipWith (\n m -> concat [show n, ""."", m]) [1..] (nub modules)
           , ""]\ESC[0m\STX\n \ESC[38;5;86m\STX\x03BB > \ESC[0m\STX""
           ]   
:}

and since nub is part of the Data.List module, I had to also include:
import Data.List

",prompt
runtimeerror numpy is not available transformers,"I basically just want to use the transformers pipeline() to classify data, but independent of which model I try to use, it returns the same error, stating Numpy is not available
Code I'm running:
pipe = pipeline(""text-classification"", model=""AdamLucek/roberta-llama3.1405B-twitter-sentiment"")   
sentiment_pipeline('Today is a great day!')
# other model i've tried: 
sentiment_pipeline = pipeline(model=""cardiffnlp/twitter-roberta-base-sentiment-latest"", tokenizer=""cardiffnlp/twitter-roberta-base-sentiment-latest"")
sentiment_pipeline('Today is a great day!')

Error I receive:
RuntimeError                              Traceback (most recent call last)
Cell In[49], line 1
----> 1 sentiment_pipeline('Today is a great day!')

File ~\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\LocalCache\local-packages\Python312\site-packages\transformers\pipelines\text_classification.py:156, in TextClassificationPipeline.__call__(self, inputs, **kwargs)
    122 """"""
    123 Classify the text(s) given as inputs.
    124 
   (...)
    153     If `top_k` is used, one such dictionary is returned per label.
    154 """"""
    155 inputs = (inputs,)
--> 156 result = super().__call__(*inputs, **kwargs)
    157 # TODO try and retrieve it in a nicer way from _sanitize_parameters.
    158 _legacy = ""top_k"" not in kwargs

File ~\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\LocalCache\local-packages\Python312\site-packages\transformers\pipelines\base.py:1257, in Pipeline.__call__(self, inputs, num_workers, batch_size, *args, **kwargs)
   1249     return next(
   1250         iter(
   1251             self.get_iterator(
   (...)
   1254         )
   1255     )
   1256 else:
-> 1257     return self.run_single(inputs, preprocess_params, forward_params, postprocess_params)

File ~\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\LocalCache\local-packages\Python312\site-packages\transformers\pipelines\base.py:1265, in Pipeline.run_single(self, inputs, preprocess_params, forward_params, postprocess_params)
   1263 model_inputs = self.preprocess(inputs, **preprocess_params)
   1264 model_outputs = self.forward(model_inputs, **forward_params)
-> 1265 outputs = self.postprocess(model_outputs, **postprocess_params)
   1266 return outputs

File ~\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\LocalCache\local-packages\Python312\site-packages\transformers\pipelines\text_classification.py:208, in TextClassificationPipeline.postprocess(self, model_outputs, function_to_apply, top_k, _legacy)
    204 outputs = model_outputs[""logits""][0]
    206 if self.framework == ""pt"":
    207     # To enable using fp16 and bf16
--> 208     outputs = outputs.float().numpy()
    209 else:
    210     outputs = outputs.numpy()

RuntimeError: Numpy is not available

I already tried simply un- and reinstalling transformers and numpy and for both the most recent versions are installed (and should be compatible).
Anyone has an idea on how to solve this?
","Try:
pip install ""numpy<2""

then restart the kernel.
",huggingface-transformers
size mismatch for embed_outweight copying a param with shape torchsize0 from checkpoint  huggingface pytorch,"I want to finetune an LLM. I am able to successfully finetune LLM. But when reload the model after save, gets error. Below is the code
import argparse
import numpy as np
import torch
from datasets import load_dataset
from transformers import AutoTokenizer, AutoModelForCausalLM

from trl import DPOTrainer, DPOConfig
def preprocess_data(item):
    return {
        'prompt': 'Instruct: ' + item['prompt'] + '\n',
        'chosen': 'Output: ' + item['chosen'],
        'rejected': 'Output: ' + item['rejected']
    }        

def main():
    parser = argparse.ArgumentParser()
    parser.add_argument(""--epochs"", type=int, default=1)
    parser.add_argument(""--beta"", type=float, default=0.1)
    parser.add_argument(""--batch_size"", type=int, default=4)
    parser.add_argument(""--lr"", type=float, default=1e-6)
    parser.add_argument(""--seed"", type=int, default=2003)
    parser.add_argument(""--model_name"", type=str, default=""EleutherAI/pythia-14m"")
    parser.add_argument(""--dataset_name"", type=str, default=""jondurbin/truthy-dpo-v0.1"")
    parser.add_argument(""--local_rank"", type=int, default=0)

    args = parser.parse_args()

    # Determine device based on local_rank
    device = torch.device(""cuda"", args.local_rank) if torch.cuda.is_available() else torch.device(""cpu"")


    tokenizer = AutoTokenizer.from_pretrained(args.model_name)
    tokenizer.pad_token = tokenizer.eos_token
    model = AutoModelForCausalLM.from_pretrained(args.model_name).to(device)
    ref_model = AutoModelForCausalLM.from_pretrained(args.model_name).to(device)

    dataset = load_dataset(args.dataset_name, split=""train"")
    dataset = dataset.map(preprocess_data)

    # Split the dataset into training and validation sets
    dataset = dataset.train_test_split(test_size=0.1, seed=args.seed)
    train_dataset = dataset['train']
    val_dataset = dataset['test']

    training_args = DPOConfig(
        learning_rate=args.lr,
        num_train_epochs=args.epochs,
        per_device_train_batch_size=args.batch_size,
        logging_steps=10,
        remove_unused_columns=False,
        max_length=1024,
        max_prompt_length=512,
        fp16=True        
    )

    

    # Verify and print embedding dimensions before finetuning
    print(""Base model embedding dimension:"", model.config.hidden_size)

    model.train()
    ref_model.eval()

    dpo_trainer = DPOTrainer(
        model,
        ref_model,
        beta=args.beta,
        train_dataset=train_dataset,
        eval_dataset=val_dataset,
        tokenizer=tokenizer,
        args=training_args,
    )

    dpo_trainer.train()
    # Evaluate
    evaluation_results = dpo_trainer.evaluate()
    print(""Evaluation Results:"", evaluation_results)

    save_model_name = 'finetuned_model'
    model.save_pretrained(save_model_name)

if __name__ == ""__main__"":
    main()

Error I was getting as below
    return model_class.from_pretrained(
    File ""/.local/lib/python3.10/site-packages/transformers/modeling_utils.py"", line 3838, in from_pretrained
        ) = cls._load_pretrained_model(
    File ""/.local/lib/python3.10/site-packages/transformers/modeling_utils.py"", line 4349, in _load_pretrained_model
        raise RuntimeError(f""Error(s) in loading state_dict for {model.__class__.__name__}:\n\t{error_msg}"")
        RuntimeError: Error(s) in loading state_dict for GPTNeoXForCausalLM:
            size mismatch for gpt_neox.embed_in.weight: copying a param with shape torch.Size([0]) from checkpoint, the shape in current model is torch.Size([50304, 128]).
            size mismatch for embed_out.weight: copying a param with shape torch.Size([0]) from checkpoint, the shape in current model is torch.Size([50304, 128]).
            You may consider adding `ignore_mismatched_sizes=True` in the model `from_pretrained` method.

After finetuning,  model works perfectly. But after reloading the saved trained model its not working. Any idea why gets this error when reloading the model ?
","Instead of
model.save_pretrained(save_model_name)

try this
dpo_trainer.save_model(save_model_name)

",huggingface-transformers
delete stubborn folder with points in name,"I'm trying to delete a folder with weird name: ""T.E.E.M."" 
Windows doesn't recognize this kind of name and even tell me the folder can't be found (it was created via NodeJs, but i don't know why windows allowed it).

I tried to delete it normally and it says that the folder can't be found because it doesn't exist.

I tried in prompt too, using the 2 commands del ""T.E.E.M."" inside the right folder and Rmdir /S ""T.E.E.M."" but they both don't work. Any suggestion?
","Windows cannot handle files/folders ending on a period-symbol ('.'). That's what I discovered when I tried to create the ""T.E.E.M.""-folder myself. I ended up with a ""T.E.E.M""-folder (without the last period).
So this might do the trick for you:
del ""\\?\<full path to file>""

but since this is a directory, maybe
rmdir /s ""\\?\<full path to file>""

works better.
By entering \\?\ string parsing is disabled.
Original answer found here.
",prompt
what does quotiquot in the section quot_iqquot and quot_mquot mean in this name quotmetallama38binstructiq3_mggufquot,"Appreciate if someone could let me know what does ""I"" in the section ""_IQ"" and ""_M"" mean in this name ""Meta-Llama-3-8B-Instruct-IQ3_M.gguf""???
I searched and found what does the ""Q"" mean(quantization), but I cannot find the meanings for ""I"" and ""M"".
","IQ quantization uses an Importance Matrix (Imatrix) to determine the importance of different model activations during the quantization process.
This is an alternate quantization method to K quantization. The IQ quantization is generally a more advanced and higher-quality quantization technique than the legacy K-quant methods. Still, the optimal choice depends on the target hardware and performance requirements.
The ""M"", ""S"", ""XS"" and ""XXS"" suffixes in IQ quantization names refer to the model size, with ""M"" being the largest and ""XXS"" being the smallest. For example, the bitness is not exactly 3, as de M uses ~3.6 bits per parameter and XXS uses ~3.2 bits.
",llama
langchain python with structured output ollama functions,"I am following this guide to set up a self-RAG.
I am not allowed to use OpenAI models at the moment, so I've been using ChatOllama models instead. I want to pipe outputs using the ""with_structured_output()"" function, with OllamaFunctions instead of ChatOllama. It is demonstrated here.
Essentially here is the code:
from langchain_experimental.llms.ollama_functions import OllamaFunctions


from langchain_core.prompts import PromptTemplate
from langchain_core.pydantic_v1 import BaseModel, Field


# Schema for structured response
class Person(BaseModel):
    name: str = Field(description=""The person's name"", required=True)
    height: float = Field(description=""The person's height"", required=True)
    hair_color: str = Field(description=""The person's hair color"")


# Prompt template
prompt = PromptTemplate.from_template(
    """"""Alex is 5 feet tall. 
Claudia is 1 feet taller than Alex and jumps higher than him. 
Claudia is a brunette and Alex is blonde.

Human: {question}
AI: """"""
)

# Chain
llm = OllamaFunctions(model=""phi3"", format=""json"", temperature=0)
structured_llm = llm.with_structured_output(Person)
chain = prompt | structured_llm

I get two errors that bring me to a dead end. The first one is:
ValidationError: 1 validation error for OllamaFunctions
__root__
  langchain_community.chat_models.ollama.ChatOllama() got multiple values for keyword argument 'format' (type=type_error)

so I changed
llm = OllamaFunctions(model=""phi3"", format=""json"", temperature=0)
to
llm = OllamaFunctions(model=""phi3"", temperature=0)
and that brings me to the next line at least. Then, the with_structured_output(Person) line fails with error:
File ~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/langchain_core/language_models/base.py:208, in BaseLanguageModel.with_structured_output(self, schema, **kwargs)
    204 def with_structured_output(
    205     self, schema: Union[Dict, Type[BaseModel]], **kwargs: Any
    206 ) -> Runnable[LanguageModelInput, Union[Dict, BaseModel]]:
    207     """"""Implement this if there is a way of steering the model to generate responses that match a given schema.""""""  # noqa: E501
--> 208     raise NotImplementedError()

NotImplementedError:

And I don't know where to go from here. Anything would help. Thanks!
","Hobakjuk found the issue: pip, github, webdoc versions of ollama_functions are out of sync. which requires a temp workaround until the pypi version is updated.
The Workaround involves:

ctrl+c copy code contents from github ollama_functions.py

make a local ollama_functions.py file, ctrl+v paste code into it

in your python code then import the 'patched' local library by replacing
from langchain_experimental.llms.ollama_functions import OllamaFunctions
with
from ollama_functions import OllamaFunctions


keep track of your code
",langchain
could not find orgspringframeworkai,"I'm trying to integrate Llama3 in my spring application by using the following documentation:
https://docs.spring.io/spring-ai/reference/1.0-SNAPSHOT/api/chat/ollama-chat.html
When building the application with the added dependency:
 implementation 'org.springframework.ai:spring-ai-ollama-spring-boot-starter'

I got the following error:
Execution failed for task ':compileJava'.
> Could not resolve all files for configuration ':compileClasspath'.
   > Could not find org.springframework.ai:spring-ai-ollama-spring-boot-starter:.

This github issue https://github.com/spring-projects/spring-ai/issues/194 also doesn't help. After changing the path based on a comment there to:
    implementation 'org.springframework.experimental.ai:spring-ai-ollama-spring-boot-starter'

I got the following error:
Execution failed for task ':compileJava'.
> Could not resolve all files for configuration ':compileClasspath'.
   > Could not find org.springframework.experimental.ai:spring-ai-ollama-spring-boot-starter:

","If you go to the maven repository of the mentioned dependency, you will see that the dependency is relocated. To solve this problem, simply change the dependency location to the correct one:
implementation 'io.springboot.ai:spring-ai-ollama-spring-boot-starter:1.0.3'

It is also worth considering to include Spring AI Bill of Materials (BOM) as noted by @M.Deinum's comment to avoids the need for you to specify and maintain the dependency versions yourself:
    implementation platform(""io.springboot.ai:spring-ai-bom:1.0.3"")
    // Replace the following with the starter dependencies of specific modules you wish to use
    implementation 'io.springboot.ai:spring-ai-ollama-spring-boot-starter'

",llama
autogen response in a variable,"import autogen
from nicegui import ui, context
from uuid import uuid4

# AutoGen Configuration
config_list = [
    {
        'model': 'gpt-4',
        'api_key': '' 
    }
]
llm_config = {
    'seed': 42,
    'config_list': config_list,
    'temperature': 0.2
}

# Initialize AutoGen Agents
assistant = autogen.AssistantAgent(name='Albert', llm_config=llm_config)
user_proxy = autogen.UserProxyAgent(name='user_proxy', human_input_mode=""NEVER"", max_consecutive_auto_reply=1, is_termination_msg=lambda x: x.get(""content"", """").rstrip().endswith(""TERMINATE""), code_execution_config={""work_dir"": ""web""}, llm_config=llm_config)

@ui.page('/')
def main():
    messages = []
    user_id = str(uuid4())  # Unique ID for each user session

    @ui.refreshable
    def chat_messages():
        for name, text in messages:
            ui.chat_message(text=text, name=name, sent=name == 'You')
        if context.get_client().has_socket_connection:
            ui.run_javascript('setTimeout(() => window.scrollTo(0, document.body.scrollHeight), 0)')


    async def send():
        user_message = task_input.value
        messages.append(('You', user_message))  # Append user's message to the messages list
        chat_messages.refresh()  # Refresh chat messages to display the latest message
        task_input.value = ''  # Clear the input field after sending the message

        try:
            response = await user_proxy.initiate_chat(assistant, message=user_message)
            if response and 'content' in response[0]:
                assistant_response = response[0]['content']
                messages.append(('Albert', assistant_response))  # Append assistant's response to messages
            else:
                messages.append(('Albert', ""Assistant did not provide a response.""))
        except Exception as e:
            messages.append(('Albert', f""Error: {e}""))
        finally:
            chat_messages.refresh()


    with ui.scroll_area().classes('w-full h-60 p-3 bg-white overflow-auto'):
        chat_messages()

    with ui.footer().style('position: fixed; left: 0; bottom: 0; width: 100%; background: white; padding: 10px; box-shadow: 0 -2px 5px rgba(0,0,0,0.1);'):
        task_input = ui.input().style('width: calc(100% - 100px);')
        ui.button('Send', on_click=send).style('width: 90px;')

ui.run(title='Chat with Albert')

trying to use this GUI over Autogen. However, I cannot figure out where the response is coming from? The response variable doesn't seem to have it. When there is an exception, it is printed in the UI, when it works well, Autogen prints the answer in the terminal but not the UI.
","You need to pass your callback function via register_reply function of the Agent class to get or print the response on your web UI. Below is the concept of the code:
def print_messages(recipient, messages, sender, config):
    # each time when the agent receive the message
    # do your own logic here
    messages.append((messages[-1]['name'], messages[-1]['content']))
    return False, None

assistant.register_reply(
    [autogen.Agent, None],
    reply_func=print_messages, 
    config={""callback"": None},
)

You may want to check out the offical document
",llm
ifelse conditional behaviour in javascript,"I'm a begginer in JavaScript and while I was working with an if/else conditional something happened and I can't still understand it. My code is simple: A prompt asks if user wants to do an ADDITION or a SUBSTRACTION, then the following 2 prompts ask for a value and based on the user's answers, the code executes the addition or substraction respectively, by using the values provided.
The problem comes when user clicks on 'cancel' or 'ok' button on the prompt, my code is intended to do certain actions if any of these buttons is clicked, and it does, but it also does another action that I don't want it to execute.
If user clicks either 'cancel' or 'ok', the code should alert the following message and finish the execution:
alert('No option entered, aborting operation!')
The code alerts that message successfully, but it also alerts  another message that is intended to be displayed only if user enters a different value than SUMA or RESTA. The message is this:
alert('No valid option entered, aborting operation!');
Then I modified the code and it is working as expected, but according to me, the first version of the code should work without any issue. The goal is to display only the first message if either 'cancel' or 'ok' buttons are clicked and finish the execution, not both messages and finish the execution.
I hope to be clear enough, if I'm not please let me know and I can explain with more detail if necessary. Can anybody explain to me what's the problem here?
Here are the snippets:
FIRST VERSION (not working as expected):
let opText = document.getElementById('opText');
let opOperation = prompt('Enter SUMA or RESTA');
let opSuma1; 
let opSuma2; 
let opStatus = true;
let opResult;

if(opOperation === null || opOperation === """"){
    alert('No option entered, aborting operation!'); **//THE CODE DISPLAYS THIS MESSAGE IF EITHER 'CANCEL' OR 'OK' BUTTONS ARE CLICKED**
    opStatus = false;
}

else {
    opOperation = opOperation.trim().toLowerCase();
}

function addition(){
    opSuma1 = prompt('Enter first value:');;
    opSuma2 = prompt('Enter second value:');
    opSuma1 = Number(opSuma1);
    opSuma2 = Number(opSuma2);
    opResult = opSuma1 + opSuma2;
    opText.textContent = `Your result was a ADDITION and it is ${opResult}`;
}

function substraction(){
    opSuma1 = prompt('Enter first value:');;
    opSuma2 = prompt('Enter second value:');
    opSuma1 = Number(opSuma1);
    opSuma2 = Number(opSuma2);
    opResult = opSuma1 - opSuma2;
    opText.textContent = `Your result was a SUBSTRACTION and it is ${opResult}`;
}


if(opStatus && opOperation === 'suma'){
    addition();
}
else if(opStatus && opOperation === 'resta'){
    substraction();
}
**else {
    
    alert('No valid option entered, aborting operation!'); **// BUT IT ALSO DISPLAYS THIS MESSAGE**
}**


SECOND VERSION (working as expected)
let opText = document.getElementById('opText');
let opOperation = prompt('Enter SUMA or RESTA');
let opSuma1; 
let opSuma2; 
let opStatus = true;
let opResult;

if(opOperation === null || opOperation === """"){
    alert('No option entered, aborting operation!');
    opStatus = false;
}

else {
    opOperation = opOperation.trim().toLowerCase();
}

function addition(){
    opSuma1 = prompt('Enter first value:');;
    opSuma2 = prompt('Enter second value:');
    opSuma1 = Number(opSuma1);
    opSuma2 = Number(opSuma2);
    opResult = opSuma1 + opSuma2;
    opText.textContent = `Your result was a ADDITION and it is ${opResult}`;
}

function substraction(){
    opSuma1 = prompt('Enter first value:');;
    opSuma2 = prompt('Enter second value:');
    opSuma1 = Number(opSuma1);
    opSuma2 = Number(opSuma2);
    opResult = opSuma1 - opSuma2;
    opText.textContent = `Your result was a SUBSTRACTION and it is ${opResult}`;
}

 if (opStatus) {
     if (opOperation === 'suma') {
         addition();
     } else if (opOperation === 'resta') {
         substraction();
     } else {
         alert('No valid option entered, aborting operation!');
     }
 }

","You broke the conditions into two unrelated of branches.
The first one:
    if(opOperation === null || opOperation === """"){
        alert('No option entered, aborting operation!'); **//THE CODE DISPLAYS THIS MESSAGE IF EITHER 'CANCEL' OR 'OK' BUTTONS ARE CLICKED**
        opStatus = false;
    }

    else {
        opOperation = opOperation.trim().toLowerCase();
    }

When the first one ends, it doesnâ€™t stop the program, so after the else, it continues onto the next one.
The seconds one:
    if(opStatus && opOperation === 'suma'){
        addition();
    }
    else if(opStatus && opOperation === 'resta'){
        substraction();
    }
    **else {
    
        alert('No valid option entered, aborting operation!'); **// BUT IT ALSO DISPLAYS THIS MESSAGE**
    }**

If the first if was true, then the remaining two must be false since opStatus has already been established to be nothing. Therefore, it goes to the else statement (which is evaluated if nothing else in the if-branch was true) and executes it.
If you nested them in by putting the second one inside the else of the first, it would solve the problem.
    if(opOperation === null || opOperation === """"){
        alert('No option entered, aborting operation!'); **//THE CODE DISPLAYS THIS MESSAGE IF EITHER 'CANCEL' OR 'OK' BUTTONS ARE CLICKED**
        opStatus = false;
    }

    else {
        opOperation = opOperation.trim().toLowerCase();

        if(opStatus && opOperation === 'suma'){
            addition();
        }
        else if(opStatus && opOperation === 'resta'){
            substraction();
        }
        **else {
            alert('No valid option entered, aborting operation!'); **// BUT IT ALSO DISPLAYS THIS MESSAGE**
        }**
    }

You use this same principle in your second solution:
    if (opStatus) {
        if (opOperation === 'suma') {
            addition();
        } else if (opOperation === 'resta') {
            substraction();
        } else {
            alert('No valid option entered, aborting operation!');
        }
    }

In that example, you evaluate if opStatus is anything at all. I suggest combining the first and second if-branches from that example and removing opStatus as it doesnâ€™t do anything. Instead use if (opOperation). This works because there really is no such thing as true or false. Anything that loosely equals zero will be considered false, and false is zero. """" is zero, and null is equal to nothing except null, but it is treated as false in comparisons, despite having no way to convert.
    if (opOperation) {
        opOperation = opOperation.trim().toLowerCase();
        if (opOperation === 'suma') {
            addition();
        } else if (opOperation === 'resta') {
            substraction();
        } else {
            alert('No valid option entered, aborting operation!');
        }
    }
    else {
        alert('No option entered, aborting operation!');
    }



let opText = document.getElementById('opText');
let opOperation = prompt('Enter SUMA or RESTA');
let opSuma1; 
let opSuma2; 
let opResult;

function addition(){
    opSuma1 = prompt('Enter first value:');;
    opSuma2 = prompt('Enter second value:');
    opSuma1 = Number(opSuma1);
    opSuma2 = Number(opSuma2);
    opResult = opSuma1 + opSuma2;
    opText.textContent = `Your result was a ADDITION and it is ${opResult}`;
}

function substraction(){
    opSuma1 = prompt('Enter first value:');;
    opSuma2 = prompt('Enter second value:');
    opSuma1 = Number(opSuma1);
    opSuma2 = Number(opSuma2);
    opResult = opSuma1 - opSuma2;
    opText.textContent = `Your result was a SUBSTRACTION and it is ${opResult}`;
}

if (opOperation) {
    opOperation = opOperation.trim().toLowerCase();
    if (opOperation === 'suma') {
        addition();
    } else if (opOperation === 'resta') {
        substraction();
    } else {
        alert('No valid option entered, aborting operation!');
    }
}
else {
    alert('No option entered, aborting operation!');
}



Let me know if I made an error somewhere (I know my code works).
",prompt
how to use while loop on a prompt in javascript,"[


  let welcome;
  welcome = ""Welcome to my first COMP1231 Program."";
  alert(welcome)

  let name;
  name =(prompt(""Please enter your name:?"", ""Tafadzwa Marisa"" ));


  let program;
  program =(prompt(""Please enter your Program:?"", ""COMP1231"" ));



]1
So l am trying to use a prompt which asks a user for their name and when the user enters a name it proceeds to another prompt but when the user doesn't enter anything it loops back to the first prompt which is ask for the users name. If the user enters a valid input it then goes to the program prompt where if the user enters any input it proceeds but if the user doesn't it loops to the same prompt. I want to use the while loop for this
","You can create a function which will basically execute a while loop until user enters some input. This function returns the input value if it's not empty.


function loopUntilNonEmptyInput(prompt_ques, default_inp) {
  let retval;
  while (true) {
    retval = (prompt(prompt_ques, default_inp));
    
    if (retval !== '')
      return retval;
      
    alert(""Empty input, please re-try"");
  }
}

let welcome;
welcome = ""Welcome to my first COMP1231 Program."";
alert(welcome);

let name = loopUntilNonEmptyInput(""Please enter your name:?"", ""Tafadzwa Marisa"");
let program = loopUntilNonEmptyInput(""Please enter your Program:?"", ""COMP1231"");
console.log(name, program);



",prompt
prevent ncurses c library from creating subwindow,"I want to prevent the C++ ncurses library from creating its own sub-window when activated. Whenever you use initscr() with ncurses, it creates its own sub-window (text area in terminal). Not using initscr() and proceeding to use ncurses will lead to a segmentation fault. Does anyone know how to use ncurses without it creating its own sub-window?
In my case I am using ncurses to do a ""selection prompt"" where options are displayed, and the user can go though them with arrow keys until the find one they want (enter key twice to select).
#include <iostream>
#include <vector>
#include <string>
#include <ncurses.h>

std::string prompt(const std::vector<std::string> &options, const std::string &message)
{
    // * NCURSES init
    initscr();
    cbreak();
    keypad(stdscr, true);
    noecho();

    int choice;
    int highlight = 0;
    int num_options = options.size();

    while (true)
    {
        // * Clear line
        int y, x;
        getyx(stdscr, y, x);
        move(y, 0);
        clrtoeol();

        // * Display options
        printw(""%s: "", (message).c_str());
        for (int i = 0; i < num_options; ++i)
        {
            if (i == highlight)
                attron(A_REVERSE);

            printw(""%s "", options[i].c_str());
            attroff(A_REVERSE);
        }

        // * Get user input
        choice = getch();

        // * Decoding selection
        switch (choice)
        {
        case KEY_RIGHT:
            highlight = (highlight - 1 + num_options) % num_options;
            break;
        case KEY_LEFT:
            highlight = (highlight + 1) % num_options;
            break;
        case '\n':
            refresh();
            getch();
            endwin();
            printf(""\n"");
            return options[highlight];
        default:
            break;
        }
    }

    // * NCURSES cleanup
    refresh();
    getch();
    endwin();
}

int main()
{
    std::cout << ""Hello World!\n"";
    prompt({""agree"", ""disagree""}, ""choose one"");
    std::cout << ""Hello World #2\n"";

    return 0;
}

If you run this code, you will notice that in the ncurses sub window, only the stuff in the prompt function will appear. After selection, you will see the first message (""Hello World""), then a blank line where the prompt should be, then the second message (""Hello World #2"").
","Ncurses does not work without using it's own sub-window.
",prompt
client error 39404 not found39 for url 39httplocalhost11434apichat39 while using reactagent of llama_indexcoreagent,"I am following this tutorial, https://youtu.be/JLmI0GJuGlY?si=eeffNvHjaRHVV6r7&t=1915, and trying to build a simple LLM agent.
I am on WSL2, Windows 11, and I am coding in VSC. I use Ollama to download and store my LLMs. My python is 3.9.
My script my_main3.py is very simple:
from llama_index.llms.ollama import Ollama
from llama_parse import LlamaParse
from llama_index.core import VectorStoreIndex, SimpleDirectoryReader, PromptTemplate
from llama_index.core.embeddings import resolve_embed_model
from llama_index.core.tools import QueryEngineTool, ToolMetadata
from llama_index.core.agent import ReActAgent
from prompts import context

from dotenv import load_dotenv
load_dotenv()

llm = Ollama(model=""mistral"", request_timeout=30.0)

parser = LlamaParse(result_type=""markdown"") 

file_extractor = {"".pdf"": parser}
documents = SimpleDirectoryReader(""./data"", file_extractor=file_extractor).load_data()

embed_model = resolve_embed_model(""local:BAAI/bge-m3"")

vector_index = VectorStoreIndex.from_documents(documents, embed_model=embed_model)
query_engine = vector_index.as_query_engine(llm=llm)

tools = [
    QueryEngineTool(
        query_engine=query_engine,
        metadata=ToolMetadata(
            name=""api_documentation"",
            description=""this gives documentation about code for an API. Use this for reading docs for the API"",
        ),
    )
]

code_llm = Ollama(model=""codellama"")
agent = ReActAgent.from_tools(tools, llm=code_llm, verbose=True, context=context) # context is from prompts.py

while (prompt := input(""Enter a prompt (q to quit): "")) != ""q"":
    result = agent.query(prompt)
    print(result)

Then I run Python main.py in my Terminal. The script runs well until the while loop.
It prompts me to input, then in input:
Enter a prompt (q to quit): send a post request to make a new item using the api in Python.

It then throws me this error.
Traceback (most recent call last):
  File ""/home/ubuntu2022/MyUbunDev/210_AI_agent_basic/my_main3.py"", line 38, in <module>
    result = agent.query(prompt)
  File ""/home/ubuntu2022/miniconda/envs/llm/lib/python3.9/site-packages/llama_index/core/instrumentation/dispatcher.py"", line 102, in wrapper
    self.span_drop(*args, id=id, err=e, **kwargs)
  File ""/home/ubuntu2022/miniconda/envs/llm/lib/python3.9/site-packages/llama_index/core/instrumentation/dispatcher.py"", line 77, in span_drop
    h.span_drop(*args, id=id, err=err, **kwargs)
  File ""/home/ubuntu2022/miniconda/envs/llm/lib/python3.9/site-packages/llama_index/core/instrumentation/span_handlers/base.py"", line 48, in span_drop
    span = self.prepare_to_drop_span(*args, id=id, err=err, **kwargs)
  File ""/home/ubuntu2022/miniconda/envs/llm/lib/python3.9/site-packages/llama_index/core/instrumentation/span_handlers/null.py"", line 35, in prepare_to_drop_span
    raise err
  File ""/home/ubuntu2022/miniconda/envs/llm/lib/python3.9/site-packages/llama_index/core/instrumentation/dispatcher.py"", line 100, in wrapper
    result = func(*args, **kwargs)
  File ""/home/ubuntu2022/miniconda/envs/llm/lib/python3.9/site-packages/llama_index/core/base/base_query_engine.py"", line 51, in query
    query_result = self._query(str_or_query_bundle)
  File ""/home/ubuntu2022/miniconda/envs/llm/lib/python3.9/site-packages/llama_index/core/callbacks/utils.py"", line 41, in wrapper
    return func(self, *args, **kwargs)
  File ""/home/ubuntu2022/miniconda/envs/llm/lib/python3.9/site-packages/llama_index/core/agent/types.py"", line 40, in _query
    agent_response = self.chat(
  File ""/home/ubuntu2022/miniconda/envs/llm/lib/python3.9/site-packages/llama_index/core/instrumentation/dispatcher.py"", line 102, in wrapper
    self.span_drop(*args, id=id, err=e, **kwargs)
  File ""/home/ubuntu2022/miniconda/envs/llm/lib/python3.9/site-packages/llama_index/core/instrumentation/dispatcher.py"", line 77, in span_drop
    h.span_drop(*args, id=id, err=err, **kwargs)
  File ""/home/ubuntu2022/miniconda/envs/llm/lib/python3.9/site-packages/llama_index/core/instrumentation/span_handlers/base.py"", line 48, in span_drop
    span = self.prepare_to_drop_span(*args, id=id, err=err, **kwargs)
  File ""/home/ubuntu2022/miniconda/envs/llm/lib/python3.9/site-packages/llama_index/core/instrumentation/span_handlers/null.py"", line 35, in prepare_to_drop_span
    raise err
  File ""/home/ubuntu2022/miniconda/envs/llm/lib/python3.9/site-packages/llama_index/core/instrumentation/dispatcher.py"", line 100, in wrapper
    result = func(*args, **kwargs)
  File ""/home/ubuntu2022/miniconda/envs/llm/lib/python3.9/site-packages/llama_index/core/callbacks/utils.py"", line 41, in wrapper
    return func(self, *args, **kwargs)
  File ""/home/ubuntu2022/miniconda/envs/llm/lib/python3.9/site-packages/llama_index/core/agent/runner/base.py"", line 604, in chat
    chat_response = self._chat(
  File ""/home/ubuntu2022/miniconda/envs/llm/lib/python3.9/site-packages/llama_index/core/instrumentation/dispatcher.py"", line 102, in wrapper
    self.span_drop(*args, id=id, err=e, **kwargs)
  File ""/home/ubuntu2022/miniconda/envs/llm/lib/python3.9/site-packages/llama_index/core/instrumentation/dispatcher.py"", line 77, in span_drop
    h.span_drop(*args, id=id, err=err, **kwargs)
  File ""/home/ubuntu2022/miniconda/envs/llm/lib/python3.9/site-packages/llama_index/core/instrumentation/span_handlers/base.py"", line 48, in span_drop
    span = self.prepare_to_drop_span(*args, id=id, err=err, **kwargs)
  File ""/home/ubuntu2022/miniconda/envs/llm/lib/python3.9/site-packages/llama_index/core/instrumentation/span_handlers/null.py"", line 35, in prepare_to_drop_span
    raise err
  File ""/home/ubuntu2022/miniconda/envs/llm/lib/python3.9/site-packages/llama_index/core/instrumentation/dispatcher.py"", line 100, in wrapper
    result = func(*args, **kwargs)
  File ""/home/ubuntu2022/miniconda/envs/llm/lib/python3.9/site-packages/llama_index/core/agent/runner/base.py"", line 539, in _chat
    cur_step_output = self._run_step(
  File ""/home/ubuntu2022/miniconda/envs/llm/lib/python3.9/site-packages/llama_index/core/instrumentation/dispatcher.py"", line 102, in wrapper
    self.span_drop(*args, id=id, err=e, **kwargs)
  File ""/home/ubuntu2022/miniconda/envs/llm/lib/python3.9/site-packages/llama_index/core/instrumentation/dispatcher.py"", line 77, in span_drop
    h.span_drop(*args, id=id, err=err, **kwargs)
  File ""/home/ubuntu2022/miniconda/envs/llm/lib/python3.9/site-packages/llama_index/core/instrumentation/span_handlers/base.py"", line 48, in span_drop
    span = self.prepare_to_drop_span(*args, id=id, err=err, **kwargs)
  File ""/home/ubuntu2022/miniconda/envs/llm/lib/python3.9/site-packages/llama_index/core/instrumentation/span_handlers/null.py"", line 35, in prepare_to_drop_span
    raise err
  File ""/home/ubuntu2022/miniconda/envs/llm/lib/python3.9/site-packages/llama_index/core/instrumentation/dispatcher.py"", line 100, in wrapper
    result = func(*args, **kwargs)
  File ""/home/ubuntu2022/miniconda/envs/llm/lib/python3.9/site-packages/llama_index/core/agent/runner/base.py"", line 382, in _run_step
    cur_step_output = self.agent_worker.run_step(step, task, **kwargs)
  File ""/home/ubuntu2022/miniconda/envs/llm/lib/python3.9/site-packages/llama_index/core/callbacks/utils.py"", line 41, in wrapper
    return func(self, *args, **kwargs)
  File ""/home/ubuntu2022/miniconda/envs/llm/lib/python3.9/site-packages/llama_index/core/agent/react/step.py"", line 653, in run_step
    return self._run_step(step, task)
  File ""/home/ubuntu2022/miniconda/envs/llm/lib/python3.9/site-packages/llama_index/core/agent/react/step.py"", line 463, in _run_step
    chat_response = self._llm.chat(input_chat)
  File ""/home/ubuntu2022/miniconda/envs/llm/lib/python3.9/site-packages/llama_index/core/llms/callbacks.py"", line 130, in wrapped_llm_chat
    f_return_val = f(_self, messages, **kwargs)
  File ""/home/ubuntu2022/miniconda/envs/llm/lib/python3.9/site-packages/llama_index/llms/ollama/base.py"", line 105, in chat
    response.raise_for_status()
  File ""/home/ubuntu2022/miniconda/envs/llm/lib/python3.9/site-packages/httpx/_models.py"", line 761, in raise_for_status
    raise HTTPStatusError(message, request=request, response=self)
httpx.HTTPStatusError: Client error '404 Not Found' for url 'http://localhost:11434/api/chat'
For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/404

I checked my Edge browser, http://localhost:11434/ is running Ollama. Is this causing the clash? And Noticed I have never set up that http://localhost:11434/api/chat endpoint in my script.
","Someone posted further down in the comments on the video. I had this same issue.
@HodBuri
1 month ago
Error 404 not found - local host - api - chat [FIX]
If anyone else gets an error like that when trying to run the llamacode agent, just run the llamacode llm in terminal to download it, as it did not download it automatically for me at least as he said around 29:11
So similar to what he showed at the start with Mistral:
ollama run mistral.
You can run this in a new terminal to download codellama:
ollama run codellama
After running the line above in a new terminal I kept it up and reran the main.py in the terminal I was previously working in and it worked
",llm
bert sentence embeddings from transformers,"I'm trying to get sentence vectors from hidden states in a BERT model.  Looking at the huggingface BertModel instructions here, which say:
from transformers import BertTokenizer, BertModel
tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')
model = BertModel.from_pretrained(""bert-base-multilingual-cased"")
text = ""Replace me by any text you'd like.""
encoded_input = tokenizer(text, return_tensors='pt') 
output = model(**encoded_input)

So first note, as it is on the website, this does /not/ run. You get:
>>> Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
TypeError: 'BertTokenizer' object is not callable

But it looks like a minor change fixes it, in that you don't call the tokenizer directly, but ask it to encode the input:
encoded_input = tokenizer.encode(text, return_tensors=""pt"")
output = model(encoded_input)

OK, that aside, the tensors I get, however, have a different shape than I expected:
>>> output[0].shape
torch.Size([1,11,768])

This is a lot of layers.  Which is the correct layer to use for sentence embeddings?  [0]?  [-1]?  Averaging several?  I have the goal of being able to do cosine similarity with these, so I need a proper 1xN vector rather than an NxK tensor.
I see that the popular bert-as-a-service project appears to use [0]
Is this correct? Is there documentation for what each of the layers are?
","I don't think there is single authoritative documentation saying what to use and when. You need to experiment and measure what is best for your task. Recent observations about BERT are nicely summarized in this paper: https://arxiv.org/pdf/2002.12327.pdf.
I think the rule of thumb is:

Use the last layer if you are going to fine-tune the model for your specific task. And finetune whenever you can, several hundred or even dozens of training examples are enough.

Use some of the middle layers (7-th or 8-th) if you cannot finetune the model. The intuition behind that is that the layers first develop a more and more abstract and general representation of the input. At some point, the representation starts to be more target to the pre-training task.


Bert-as-services uses the last layer by default (but it is configurable). Here, it would be [:, -1]. However, it always returns a list of vectors for all input tokens. The vector corresponding to the first special (so-called [CLS]) token is considered to be the sentence embedding. This where the [0] comes from in the snipper you refer to.
",huggingface-transformers
finetuning model39s classifier layer with new label,"I would like to fine-tune already fine-tuned BertForSequenceClassification model with new dataset containing just 1 additional label which hasn't been seen by model before.
By that, I would like to add 1 new label to the set of labels that model is currently able of classifying properly.
Moreover, I don't want classifier weights to be randomly initialized, I'd like to keep them intact and just update them accordingly to the dataset examples while increasing the size of classifier layer by 1.
The dataset used for further fine-tuning could look like this:
sentece,label
intent example 1,new_label
intent example 2,new_label
...
intent example 10,new_label

My model's current classifier layer looks like this:
Linear(in_features=768, out_features=135, bias=True)

How could I achieve it?
Is it even a good approach?
","You can just extend the weights and bias of your model with new values. Please have a look at the commented example below:
#This is the section that loads your model
#I will just use an pretrained model for this example
import torch
from torch import nn
from transformers import AutoModelForSequenceClassification, AutoTokenizer
tokenizer = AutoTokenizer.from_pretrained(""jpcorb20/toxic-detector-distilroberta"")
model = AutoModelForSequenceClassification.from_pretrained(""jpcorb20/toxic-detector-distilroberta"")
#we check the output of one sample to compare it later with the extended layer
#to verify that we kept the previous learnt ""knowledge""
f = tokenizer.encode_plus(""This is an example"", return_tensors='pt')
print(model(**f).logits)

#Now we need to find out the name of the linear layer you want to extend
#The layers on top of distilroberta are wrapped inside a classifier section
#This name can differ for you because it can be chosen randomly
#use model.parameters instead find the classification layer
print(model.classifier)

#The output shows us that the classification layer is called `out_proj`
#We can now extend the weights by creating a new tensor that consists of the
#old weights and a randomly initialized tensor for the new label 
model.classifier.out_proj.weight = nn.Parameter(torch.cat((model.classifier.out_proj.weight, torch.randn(1,768)),0))

#We do the same for the bias:
model.classifier.out_proj.bias = nn.Parameter(torch.cat((model.classifier.out_proj.bias, torch.randn(1)),0))

#and be happy when we compare the output with our expectation 
print(model(**f).logits)

Output:
tensor([[-7.3604, -9.4899, -8.4170, -9.7688, -8.4067, -9.3895]],
       grad_fn=<AddmmBackward>)
RobertaClassificationHead(
  (dense): Linear(in_features=768, out_features=768, bias=True)
  (dropout): Dropout(p=0.1, inplace=False)
  (out_proj): Linear(in_features=768, out_features=6, bias=True)
)
tensor([[-7.3604, -9.4899, -8.4170, -9.7688, -8.4067, -9.3895,  2.2124]],
       grad_fn=<AddmmBackward>)

Please note, that you should fine-tune your model. The new weights are randomly initialized and will therefore negatively impact the performance.
",huggingface-transformers
completionscreate got an unexpected keyword argument 39request_timeout39,"I am using Autogen from microsoft with the below code:
import autogen
from autogen import AssistantAgent, UserProxyAgent

config_list = [
    {
        'model': 'gpt-4',
        'api_key': 'API_KEY'
    }
]

llm_config={
    ""request_timeout"": 600,
    ""seed"": 42,
    ""config_list"": config_list,
    ""temperature"": 0
}

assistant = autogen.AssistantAgent(
    name=""assistant"",
    llm_config=llm_config,
    system_message=""Chief technical officer of a tech company""
)

user_proxy = autogen.UserProxyAgent(
    name=""user_proxy"",
    human_input_mode=""ALWAYS"",
    max_consecutive_auto_reply=10,
    is_termination_msg=lambda x: x.get(""content"", """").rstrip().endswith(""TERMINATE""),
    code_execution_config={""work_dir"": ""web""},
    llm_config=llm_config,
    system_message=""""""Reply TERMINATE if the task has been solved at full satisfaction.
Otherwise, reply CONTINUE, or the reason why the task is not solved yet.""""""
)

task = """"""
Write python code to output numbers 1 to 100
""""""

user_proxy.initiate_chat(
    assistant,
    message=task
)

when I try to run the python, it gives me this error:
Completions.create() got an unexpected keyword argument 'request_timeout'
[autogen.oai.client: 09-05 14:32:12] {164} WARNING - The API key specified is not a valid OpenAI format; it won't work with the OpenAI-hosted model.
[autogen.oai.client: 09-05 14:32:12] {164} WARNING - The API key specified is not a valid OpenAI format; it won't work with the OpenAI-hosted model.
user_proxy (to assistant):


Write python code to output numbers 1 to 100


--------------------------------------------------------------------------------
Traceback (most recent call last):
  File ""c:\Users\HP\Desktop\prj\autogen-ve\Scripts\runningBots.py"", line 42, in <module>

How to resolve this?
","Please change your code to the following in order to make it work:
# config_list = [
#     {
#         'model': 'gpt-4o',
#         'api_key': 'API_KEY_HERE'
#     }
# ]
llm_config={""config_list"": config_list}

After this it should throw the error: you do not have access to the model, after which you should make a minimum payment of 5$ to open ai to access it
",chatgpt
transformers code works on its own but breaks when using gradio device mismatch,"I am attempting to make a gradio demo for nanoLLaVA by @stablequan. I am porting over just the structure of Apache 2.0 licensed code in the Moondream repo.
The nanoLLaVA repo has example code in the repo, which I used to make this script. This works and provides a reasonable output.
enter image description here
When I use the same code but in gradio here,  I get this error regarding a mismatch in devices.
Traceback (most recent call last):
  File ""C:\Users\Moo\AppData\Local\Programs\Python\Python311\Lib\site-packages\gradio\queueing.py"", line 495, in call_prediction
    output = await route_utils.call_process_api(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""C:\Users\Moo\AppData\Local\Programs\Python\Python311\Lib\site-packages\gradio\route_utils.py"", line 232, in call_process_api
    output = await app.get_blocks().process_api(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""C:\Users\Moo\AppData\Local\Programs\Python\Python311\Lib\site-packages\gradio\blocks.py"", line 1561, in process_api
    result = await self.call_function(
             ^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""C:\Users\Moo\AppData\Local\Programs\Python\Python311\Lib\site-packages\gradio\blocks.py"", line 1179, in call_function
    prediction = await anyio.to_thread.run_sync(
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""C:\Users\Moo\AppData\Local\Programs\Python\Python311\Lib\site-packages\anyio\to_thread.py"", line 33, in run_sync
    return await get_asynclib().run_sync_in_worker_thread(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""C:\Users\Moo\AppData\Local\Programs\Python\Python311\Lib\site-packages\anyio\_backends\_asyncio.py"", line 877, in run_sync_in_worker_thread
    return await future
           ^^^^^^^^^^^^
  File ""C:\Users\Moo\AppData\Local\Programs\Python\Python311\Lib\site-packages\anyio\_backends\_asyncio.py"", line 807, in run
    result = context.run(func, *args)
             ^^^^^^^^^^^^^^^^^^^^^^^^
  File ""C:\Users\Moo\AppData\Local\Programs\Python\Python311\Lib\site-packages\gradio\utils.py"", line 678, in wrapper
    response = f(*args, **kwargs)
               ^^^^^^^^^^^^^^^^^^
  File ""C:\Users\Moo\Downloads\llm\nanollava\nanollava_gradio_demo.py"", line 46, in answer_question
    output_ids = model.generate(
                 ^^^^^^^^^^^^^^^
  File ""C:\Users\Moo\AppData\Local\Programs\Python\Python311\Lib\site-packages\torch\utils\_contextlib.py"", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File ""C:\Users\Moo\AppData\Local\Programs\Python\Python311\Lib\site-packages\transformers\generation\utils.py"", line 1575, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File ""C:\Users\Moo\AppData\Local\Programs\Python\Python311\Lib\site-packages\transformers\generation\utils.py"", line 2697, in _sample
    outputs = self(
              ^^^^^
  File ""C:\Users\Moo\AppData\Local\Programs\Python\Python311\Lib\site-packages\torch\nn\modules\module.py"", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""C:\Users\Moo\AppData\Local\Programs\Python\Python311\Lib\site-packages\torch\nn\modules\module.py"", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""C:\Users\Moo\.cache\huggingface\modules\transformers_modules\qnguyen3\nanoLLaVA\4a1bd2e2854c6df9c4af831a408b14f7b035f4c0\modeling_llava_qwen2.py"", line 2267, in forward
    ) = self.prepare_inputs_labels_for_multimodal(
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""C:\Users\Moo\.cache\huggingface\modules\transformers_modules\qnguyen3\nanoLLaVA\4a1bd2e2854c6df9c4af831a408b14f7b035f4c0\modeling_llava_qwen2.py"", line 687, in prepare_inputs_labels_for_multimodal
    image_features = self.encode_images(images).to(self.device)
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""C:\Users\Moo\.cache\huggingface\modules\transformers_modules\qnguyen3\nanoLLaVA\4a1bd2e2854c6df9c4af831a408b14f7b035f4c0\modeling_llava_qwen2.py"", line 661, in encode_images
    image_features = self.get_model().mm_projector(image_features)
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""C:\Users\Moo\AppData\Local\Programs\Python\Python311\Lib\site-packages\torch\nn\modules\module.py"", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""C:\Users\Moo\AppData\Local\Programs\Python\Python311\Lib\site-packages\torch\nn\modules\module.py"", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""C:\Users\Moo\AppData\Local\Programs\Python\Python311\Lib\site-packages\torch\nn\modules\container.py"", line 217, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File ""C:\Users\Moo\AppData\Local\Programs\Python\Python311\Lib\site-packages\torch\nn\modules\module.py"", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""C:\Users\Moo\AppData\Local\Programs\Python\Python311\Lib\site-packages\torch\nn\modules\module.py"", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""C:\Users\Moo\AppData\Local\Programs\Python\Python311\Lib\site-packages\torch\nn\modules\linear.py"", line 116, in forward
    return F.linear(input, self.weight, self.bias)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument mat2 in method wrapper_CUDA_mm)

","Now if you've seen this before, and read through the error message, you might be able to tell me immediately, that ""oh obviously its running on a different thread and set_default_device() doesn't carry over to that thread"". This issue here is related to this. I'm not sure which version the fix applies to. But either way, since default is ""cpu"", if everything is on cpu that should be fine right?
So I modified the AutoModelForCausalLM call to use device_map='cpu', and printed the .device of both inputs and the model before executing model.generate:
<|im_start|>system
Answer the questions.<|im_end|><|im_start|>user
<image>
What do you see?<|im_end|><|im_start|>assistant

cpu
cpu
cpu

So what you need to do is figure out a way to execute set_default_device on that thread, or as seen here, you can try set_default_tensor_type, WHICH WORKS!!
# set device
torch.set_default_device('cuda')  # or 'cpu'
torch.set_default_tensor_type('torch.cuda.FloatTensor')

",multimodal
calculating total tokens for api request to chatgpt including functions,"Hello Stack Overflow community,
I've been working on integrating ChatGPT's API into my project, and I'm having some trouble calculating the total number of tokens for my API requests. Specifically, I'm passing both messages and functions in my API calls.
I've managed to figure out how to calculate the token count for the messages, but I'm unsure about how to account for the tokens used by the functions.
Could someone please guide me on how to properly calculate the total token count, including both messages and functions, for a request to ChatGPT's API?
Any help or insights would be greatly appreciated!
Thank you in advance.
I have been working on brute forcing a solution by formatting the data in the call in different ways. I have been using the tokenizer, and Tiktokenizer to test my formats.
","I am going to walk you through calculating the tokens for gpt-3.5 and gpt-4. You can apply a similar method to other models you just need to find the right settings.
We are going to calculate the tokens used by the messages and the functions separately then adding them together at the end to get the total.
Messages
Start by getting the tokenizer using tiktoken. We will use this to tokenize all the custom text in the messages and functions. Also add constants for the extra tokens the API will add to the request.
enc = tiktoken.encoding_for_model(model)

Make a variable to hold the total tokens for the messages and set it to 0.
msg_token_count = 0

Loop through the messages, and for each message add 3 to msg_token_count. Then loop through each element in the message and encode the value, adding the length of the encoded object to msg_token_count. If the dictionary has the ""name"" key set add an additional token to msg_token_count.
for message in messages:
    msg_token_count += 3  # Add tokens for each message
    for key, value in message.items():
        msg_token_count += len(enc.encode(value))  # Add tokens in set message
        if key == ""name"":
            msgTokenCount += 1 # Add token if name is set

Finally we need to add 3 to msg_token_count, for the ending tokens.
msgTokenCount += 3 # Add tokens to account for ending

Functions
Now we are going to calculate the number of tokens the functions will take.
Start by making a variable to hold the total tokens used by functions and set it to 0.
func_token_count = 0

Next we are going to loop through the functions and add tokens to func_token_count. Loop through the functions and add 7 to func_token_count for each function. Then add the length of the encoded name and description.
For each function, if it has properties, add 3 to func_token_count. Then for each key in the properties add another 3 and the length of the encoded property, making sure to subtract 3 if it has an ""enum"" key, and adding 3 for each item in the enum section.
Finally, add 12 to func_token_count to account for the tokens at the end of all the functions.
for function in functions:
    func_token_count += 7  # Add tokens for start of each function
    f_name = function[""name""]
    f_desc = function[""description""]
    if f_desc.endswith("".""):
        f_desc = f_desc[:-1]
    line = f_name + "":"" + f_desc
    func_token_count += len(enc.encode(line))  # Add tokens for set name and description
    if len(function[""parameters""][""properties""]) > 0:
        func_token_count += 3  # Add tokens for start of each property
        for key in list(function[""parameters""][""properties""].keys()):
            func_token_count += 3  # Add tokens for each set property
            p_name = key
            p_type = function[""parameters""][""properties""][key][""type""]
            p_desc = function[""parameters""][""properties""][key][""description""]
            if ""enum"" in function[""parameters""][""properties""][key].keys():
                func_token_count += 3  # Add tokens if property has enum list
                for item in function[""parameters""][""properties""][key][""enum""]:
                    func_token_count += 3
                    func_token_count += len(enc.encode(item))
            if p_desc.endswith("".""):
                p_desc = p_desc[:-1]
            line = f""{p_name}:{p_type}:{p_desc}""
            func_token_count += len(enc.encode(line))
func_token_count += 12

Here is the full code. Please note that instead of hard coding the additional token counts I used a constant to hold the value.
def get_token_count(model, messages, functions):
    # Initialize message settings to 0
    msg_init = 0
    msg_name = 0
    msg_end = 0
    
    # Initialize function settings to 0
    func_init = 0
    prop_init = 0
    prop_key = 0
    enum_init = 0
    enum_item = 0
    func_end = 0
    
    if model in [
        ""gpt-3.5-turbo-0613"",
        ""gpt-4-0613""
    ]:
        # Set message settings for above models
        msg_init = 3
        msg_name = 1
        msg_end = 3
        
        # Set function settings for the above models
        func_init = 7
        prop_init = 3
        prop_key = 3
        enum_init = -3
        enum_item = 3
        func_end = 12
    
    enc = tiktoken.encoding_for_model(model)
    
    msg_token_count = 0
    for message in messages:
        msg_token_count += msg_init  # Add tokens for each message
        for key, value in message.items():
            msg_token_count += len(enc.encode(value))  # Add tokens in set message
            if key == ""name"":
                msg_token_count += msg_name  # Add tokens if name is set
    msg_token_count += msg_end  # Add tokens to account for ending
    
    func_token_count = 0
    if len(functions) > 0:
        for function in functions:
            func_token_count += func_init  # Add tokens for start of each function
            f_name = function[""name""]
            f_desc = function[""description""]
            if f_desc.endswith("".""):
                f_desc = f_desc[:-1]
            line = f_name + "":"" + f_desc
            func_token_count += len(enc.encode(line))  # Add tokens for set name and description
            if len(function[""parameters""][""properties""]) > 0:
                func_token_count += prop_init  # Add tokens for start of each property
                for key in list(function[""parameters""][""properties""].keys()):
                    func_token_count += prop_key  # Add tokens for each set property
                    p_name = key
                    p_type = function[""parameters""][""properties""][key][""type""]
                    p_desc = function[""parameters""][""properties""][key][""description""]
                    if ""enum"" in function[""parameters""][""properties""][key].keys():
                        func_token_count += enum_init  # Add tokens if property has enum list
                        for item in function[""parameters""][""properties""][key][""enum""]:
                            func_token_count += enum_item
                            func_token_count += len(enc.encode(item))
                    if p_desc.endswith("".""):
                        p_desc = p_desc[:-1]
                    line = f""{p_name}:{p_type}:{p_desc}""
                    func_token_count += len(enc.encode(line))
        func_token_count += func_end
    
    return msg_token_count + func_token_count

Please let me know if something is not clear, or if you have a suggestion to make my post better.
",chatgpt
copying a folder and renaming it using command prompt,"I am trying to copy a folder and paste it in the same directory it was copied from.
For example
C:\Test is the main directory which consists of a folder ACDM, I would like to copy ACDM in the same directory and rename the new folder to ACDM1 which will have all the same files as ACDM has
I would like to do it using command prompt
I tried the following
C:>Xcopy C:\Test C:\Test\ACDM1 /E /U
Cannot perform a cyclic copy
0 File(s) copied
which fails, not sure hoe to add REN command with XCOPY command.
Need help ASAP as i would want to create a batch file which will create a copy of an existing folder and rename it according to a name retrieved from a text file..
","xcopy ""C:\Test\ACDM\*.*"" ""C:\Test\ACDM1\"" /s/h/e/k/f/c

",prompt
how to visualize crossattention matrices in marianmtmodel during output generation,"I am working on a machine translation task using the MarianMTModel from the Hugging Face transformers library. Specifically, I want to visualize the cross-attention matrices during the model's translation process. However, I encountered some difficulties in achieving this.
What Iâ€™ve Tried:

Initial Attempt: I noticed that the cross-attention matrices are not directly returned when the model generates a translation. The only example I found involved feeding both the source text and the translation to the model. However, my goal is to access the cross-attention matrices while the model generates the output, not for a translation given by me.

Using Forward Hooks: To achieve this, I implemented forward hooks on both the key and query projections of the attention mechanism, while disabling the key-value caching (use_cache=False) to capture the full matrices at the last step. Hereâ€™s my implementation:


# VISUALIZING CROSS ATTENTION FOR TRANSLATION TASK (NOT WORKING YET)
from transformers import MarianMTModel, MarianTokenizer
import torch
import matplotlib.pyplot as plt
from torch.nn import functional as F

model_name = ""Helsinki-NLP/opus-mt-en-de""
tokenizer = MarianTokenizer.from_pretrained(model_name)
model = MarianMTModel.from_pretrained(model_name)
model.eval()

keys = {}
queries = {}

def get_key(layer):
    def hook(module, input, output):
        key, = input
        keys[layer] = key
    return hook

def get_query(layer):
    def hook(module, input, output):
        query, = input
        queries[layer] = query
    return hook

def _shape(self, tensor: torch.Tensor, seq_len: int, bsz: int):
        return tensor.view(bsz, seq_len, self.num_heads, self.head_dim).transpose(1, 2).contiguous()

hooks = []
for i, layer in enumerate(model.model.decoder.layers):
    hooks.append(layer.encoder_attn.k_proj.register_forward_hook(get_key(i)))
    hooks.append(layer.encoder_attn.q_proj.register_forward_hook(get_query(i)))

input_text = ""Please translate this to German.""
inputs = tokenizer(input_text, return_tensors=""pt"")

translated_tokens = model.generate(**inputs, use_cache=False)

translated_text = tokenizer.decode(translated_tokens[0], skip_special_tokens=True)

input_tokens = tokenizer.convert_ids_to_tokens(inputs[""input_ids""][0])
output_tokens = tokenizer.convert_ids_to_tokens(translated_tokens[0])

attentions = []
for layer in range(len(keys)):
    K, Q = keys[layer], queries[layer]
    M = Q @ K.transpose(-2, -1)
    attentions.append(F.softmax(M, dim=-1))

attentions = torch.stack(attentions, dim=0)

print(""layers, heads, output tokens, input tokens"")
print(attentions.shape)
plt.figure(figsize=(10, 8))
plt.imshow(attentions[0, 0], cmap='viridis')
plt.colorbar()

plt.xticks(range(len(input_tokens)), input_tokens, rotation=90)
plt.yticks(range(len(output_tokens)), output_tokens)

plt.xlabel(""Input Tokens"")
plt.ylabel(""Output Tokens"")
plt.title(""Cross-Attention Matrix"")
plt.show()

This approach seemed to work in capturing the cross-attention matrices. However, I observed that the matrices only have 4 attention heads instead of the expected 8. This makes me question the correctness of my implementation.
My Question
Given the issues Iâ€™ve encountered, is there a more reliable method to extract and visualize the cross-attention matrices during the translation process? Additionally, if my current approach is fundamentally okay, how can I resolve the issue of capturing only 4 attention heads instead of 8?
I suspect that the issue might be related to that I'm currently not reshaping the key (K) and query (Q) tensors to the head dimension before multiplication, but I wanted to ask for advice in case thereâ€™s an easier or more effective way to do this.
","Huggingface has built in methods to return attention weights
translated_tokens = model.generate(**inputs, 
                                   output_attentions=True,
                                   return_dict_in_generate=True
                                  )

print(translated_tokens.keys())
> odict_keys(['sequences', 'encoder_attentions', 'decoder_attentions', 'cross_attentions', 'past_key_values'])

With return_dict_in_generate=True, model.generate returns a dict-like object. With output_attentions=True, the output dict will contain all attention weights.
For this model, it will include encoder attentions, decoder attentions and cross attentions.
",huggingface-transformers
how to download a model from huggingface,"For example, I want to download bert-base-uncased on https://huggingface.co/models, but can't find a 'Download' link. Or is it not downloadable?
","The models are automatically cached locally when you first use it.
So, to download a model, all you have to do is run the code that is provided in the model card (I chose the corresponding model card for bert-base-uncased).
At the top right of the page you can find a button called ""Use in Transformers"", which even gives you the sample code, showing you how to use it in Python. Again, for bert-base-uncased, this gives you the following code snippet:
from transformers import AutoTokenizer, AutoModelForMaskedLM
  
tokenizer = AutoTokenizer.from_pretrained(""bert-base-uncased"")
model = AutoModelForMaskedLM.from_pretrained(""bert-base-uncased"")

When you run this code for the first time, you will see a download bar appear on screen. See this post (disclaimer: I gave one of the answers) if you want to find the actual folder where Huggingface stores their models.
",huggingface-transformers
java39s keytool doesn39t prompt for key password,"Java's keytool has a parameter called -keypass which allows you to set a (separate) password to protect your private key, in addition to the password used for the entire key store.
According to the documentation:

The value of -keypass is a password used to protect the private key of the generated key pair. If a password is not provided, then the user is prompted for it. If you press the Return key at the prompt, then the key password is set to the same password as the keystore password. The -keypass value must have at least six characters.

However, when I leave out the password in the call to this command I don't seem to get prompted at all, at least not when this is used in combination with -genkeypair to generate an RSA key pair. Instead I just get the general help page. If I use """" to force an ""empty"" password then it (correctly) tells me that the password should at least be 6 characters.
Is there a way to force the keytool to prompt for a key specific password instead of having to offer it on the command line according to the documentation of -genkeypair?

I've tested this against Java 11 LTS:
keytool -genkeypair -alias test1 -keyalg RSA -keysize 4096 -sigalg SHA256withRSA -keystore test.pfx -storepass test1234
        -keypass

or
keytool -genkeypair -alias test1 -keyalg RSA -keysize 4096 -sigalg SHA256withRSA -keystore test.pfx -storepass test1234
        -keypass """"

both don't seem to work; as you can see I've already moved the -keypass parameter to the end so it cannot gobble up a parameter as password.
","The default keystore type for Java 11 is PKCS12, for which it is always assumed the keystore password and key password will be the same, hence you are not prompted to enter it (documentation)
If you need to use a key password to fit your requirements, you can use other keystore types like jks or jceks.
Note: If you are using jks or jceks, java will show you a warning message:

The JKS keystore uses a proprietary format. It is recommended to migrate to PKCS12 which is an industry standard format


If you type:
keytool -genkeypair -keyalg RSA -keysize 2048 -keystore double.pfx -storepass Storepass -keypass Keypass

You'll get the following warning:
Warning:  Different store and key passwords not supported for PKCS12 KeyStores.


Note that the PKCS#12 key stores themselves do support multiple passwords: they have separate derivations for multiple sections (keys, certificates) and KeyBag's and such. It's just that the Java support is missing. I found this out after parsing several key stores and looking at the format.
",prompt
nameerror name 39langchainllm39 is not defined,"I am following some tutorials online relating to RAG systems. As part of it I am trying to use LangChainLLM from LlamaIndex. I am working on Google Colab. As per the LlamaIndex docs I have imported LangChainLLM as follows:
from llama_index.llms.langchain import LangChainLLM
This does not throw an error when run
However when I try to use LangChainLLM as follows:
langchain_llm = LangchainLLM(llm_chain=llm_chain)
It throws the error NameError: name 'LangchainLLM' is not defined
I have checked the documentation and searched online for a solution but I cant see why this error is arising. I have rerun the notebook but there is no impact.  If anyone can help it would be greatly appreciated
","You have a typo error LangChainLLM not  LangchainLLM(llm_chain=llm_chain)
",llm
c aspnet mvc web app  update view after completion of asynch calls w openai,"I have an ASP.NET MVC app in C# that I'm trying to asynchronously call a bunch of OpenAI requests in parallel and then wait on all results to return.
The following is a snippet from this code:
        foreach (ChatMessage msg in messages)
        {
            List<ChatMessage> tmpMessage = new List<ChatMessage>();
            tmpMessage.Add(msg);

            Task<ClientResult<ChatCompletion>> chatResult = client.CompleteChatAsync(tmpMessage, chatOptions);

            await chatResult;

            if (chatResult.IsCompleted && chatResult.Result.Value != null)
            {
                output.Add(chatResult.Result.Value.Content[0].Text);
            }
        }

Each string in the output list is a json structured output from OpenAI. I then go through each json output and manipulate it as needed.
My questions are: Is this truly asynchronous? I call the CompleteChatAsync but then I use await chatResult and I am uncertain if this is the way to do that asynchronously?
Second - the view portion of the webpage does not update once all results return and are processed. It just sits there. How do I refresh the view in the ASP.NET MVC web app in .NET?
Thanks!
","To run several tasks ""in parallel"" (more correctly: concurrently), you can use Task.WhenAll(listOfTasks). The steps will be as follow:

Create the list of tasks (without awaiting them)
Fire them and wait for all of them to finish.
Handle the results.

example:
// Step 1
var allChatResults = new List<Task<ClientResult<ChatCompletion>>>();
foreach (ChatMessage msg in messages)
{
    var chatResult = client.CompleteChatAsync(tmpMessage, chatOptions);
    allChatResults.Add(chatResult);
}

// Step 2
await Task.WhenAll(allChatResults);

// Step 3
foreach (Task chatResult in allChatResults)
{
    if (chatResult.IsCompleted && chatResult.Result.Value != null)
    {
        output.Add(chatResult.Result.Value.Content[0].Text);
    }
}

",chatgpt
is there a way to force the dom to update in between repeated calls to prompt in javascript,"I am attempting to get a program to go into a cycle between prompts and updates on a page. However, if I isolate this problem down, in this case to two append calls and two prompts, the behavior I observe is that the page is updated visually only after the second call to prompt. The reason why I want to do this is esoteric, but related to making a program that sort of ""feels"" like a terminal where a user is typing in input once in a while. Here's an example, adapted from w3c schools, that demonstrates the problem:
<!DOCTYPE html>
<html>
<body>

<h1>The Window Object</h1>
<h2>The prompt() Method</h2>

<p>Click the button to demonstrate the prompt box.</p>

<button onclick=""myFunction()"">Try it</button>

<p id=""demo""></p>

<script>
function myFunction() {
   
  let person = prompt(""Please enter your name"", ""Prompt1"");
  if (person != null) {
    document.getElementById(""demo"").insertAdjacentHTML('beforeend', ""Test1\n"");
 }
  
  person = prompt(""Please enter your name"", ""Prompt2"")
  if (person != null) {
    document.getElementById(""demo"").insertAdjacentHTML('beforeend', ""Test2\n"");
  }
}

</script>

</body>
</html>

What I would ""like"" to happen is that in the first insertAdjacentHTML call that it updates the DOM, and visually on the page, before the second prompt, so you can see what you typed in. In practice, I observe that ""Test1"" and ""Test2"" visually appear at the same time after the second prompt. Console logging, as opposed to the insert adjacent call, works how I want, but isn't relevant to the application I'm writing.
I have experimented with various techniques to force the issue (e.g., trying to force trigger a layout with a scroll call, playing with requestAnimationFrame), but have not yet found a technique that seems to do what I want. My hypothesis, and I am not sure it is correct, is that the second prompt is called before the next frame, which means it blocks any new rendering, but that's just a hypothesis. Even if correct, suggestions on how to get the behavior I want would be appreciated.
Generally, I was hoping to observe Test1 placed on the page, then the second prompt call is fired, then after Test2, assuming the user chose the defaults.
","Using a short timeout seems to work...


function myFunction() {
  const d = document.getElementById(""demo"");
   
  let person = prompt(""Please enter your name"", ""Prompt1"");
  if (person != null) {
    d.insertAdjacentHTML('beforeend', ""Test1\n"");
  }
 
  setTimeout(() => {
    person = prompt(""Please enter your name"", ""Prompt2"")
    if (person != null) {
      d.insertAdjacentHTML('beforeend', ""Test2\n"");
    }
  }, 50);
}
<h4>Example with setTimeout</h4>
<button onclick=""myFunction()"">Try it</button>
<p id=""demo""></p>



BUT it will get ugly fast if you need a lot of prompts.  If so, maybe something that wraps the prompt/write text/delay part in a promise and chain them together with then:


function myFunction() {
  const d = document.getElementById(""demo"");
   
  delayPrompt(""Please enter your name"", ""Prompt1"", d)
    .then(() => delayPrompt(""Please enter your name"", ""Prompt2"", d))
    .then(() => delayPrompt(""something else"", ""Prompt3"", d));
}

async function delayPrompt(text, def, container) {
  return new Promise((resolve) => {     
    let response = prompt(text, def);
    if (response != null) {
      container.insertAdjacentHTML('beforeend', response);
    }
    setTimeout(resolve, 50);
  });
}
<h4>Example with Promise</h4>
<button onclick=""myFunction()"">Try it</button>
<p id=""demo""></p>



",prompt
return arguments from function calling with openai api when streaming,"I've made a simple OpenAI API example with function calling. I'm only using function calling to format the response, I'm not calling multiple functions or any external APIs.
When I don't stream the response I can return the function arguments, which is the data that I need.
In my NextJS route handler:
export async function POST(request: Request) {
  try {
    const openai = new OpenAI({
      apiKey: process.env[""OPENAI_API_KEY""],
    });
    const response = await openai.chat.completions.create({
      model: ""gpt-4"",
      // stream: true,
      messages: [
        {
          role: ""user"",
          content: ""Give me 5 questions and answers for a pub quiz"",
        },
      ],
      tools: [
        {
          type: ""function"",
          function: {
            name: ""get_questions_and_answers"",
            description: ""Get questions and answers"",
            parameters: simpleJsonSchema,
          },
        },
      ],
      tool_choice: {
        type: ""function"",
        function: { name: ""get_questions_and_answers"" },
      },
    });
    return Response.json(
       JSON.parse(
         response.choices[0].message.tool_calls?.[0].function.arguments || """",
       ),
    );
  } catch (serverError) {
    console.error({ serverError });
    throw new Error();
  }
}

simpleJsonSchema.json:
{
  ""type"": ""object"",
  ""properties"": {
    ""getQuestions"": {
      ""type"": ""array"",
      ""items"": {
        ""type"": ""object"",
        ""properties"": {
          ""Question"": {""type"": ""string""},
          ""Answer"": {""type"": ""string""}
        },
        ""required"": [""Question"", ""Answer""]
      }
    }
  },
  ""required"": [""getQuestions""]
}

Response from API:
{""getQuestions"":[{""Question"":""What is the capital of Australia?"",""Answer"":""Canberra""},{""Question"":""Who wrote 'To Kill a Mockingbird'?"",""Answer"":""Harper Lee""},{""Question"":""What is the highest peak in the world?"",""Answer"":""Mount Everest""},{""Question"":""Who is known as the 'Father of Computers'?"",""Answer"":""Charles Babbage""},{""Question"":""What is the largest ocean in the world?"",""Answer"":""Pacific Ocean""}]}

This is fine when developing locally, however when deployed to Vercel the request sometimes times out. I've tried to add streaming as this is the recommended solution:
const response = await openai.chat.completions.create({
  model: ""gpt-4"",
  stream: true,
  messages: [
    {
      role: ""user"",
      content: ""Give me 5 questions and answers for a pub quiz"",
    },
  ],
  tools: [
    {
      type: ""function"",
      function: {
        name: ""get_questions_and_answers"",
        description: ""Get questions and answers"",
        parameters: simpleJsonSchema,
      },
    },
  ],
  tool_choice: {
    type: ""function"",
    function: { name: ""get_questions_and_answers"" },
  },
});

const stream = OpenAIStream(response);
return new StreamingTextResponse(stream);

However now the response has a lot of unnecessary data. And when I try to JSON.parse on the client I get errors.
Response from API:
{""tool_calls"":[ {""id"": ""call_IhxvzkZ5EsmZpHc6tOznTmzb"", ""type"": ""function"", ""function"": {""name"": ""get_questions_and_answers"", ""arguments"": ""{\n  \""getQuestions\"": [\n    {\n      \""Question\"": \""Question 1\"",\n      \""Answer\"": \""Answer 1\""\n    },\n    {\n      \""Question\"": \""Question 2\"",\n      \""Answer\"": \""Answer 2\""\n    },\n    {\n      \""Question\"": \""Question 3\"",\n      \""Answer\"": \""Answer 3\""\n    },\n    {\n      \""Question\"": \""Question 4\"",\n      \""Answer\"": \""Answer 4\""\n    },\n    {\n      \""Question\"": \""Question 5\"",\n      \""Answer\"": \""Answer 5\""\n    }\n  ]\n}""}}

As far as I can see the docs only cover using useChat but I have some particular requirements so I need to handle the fetching and form state myself: https://sdk.vercel.ai/docs/api-reference/use-chat
Why am I getting invalid JSON?
Here is a repository which reproduces the issue: https://github.com/jameschetwood/openai-function-calling
","this is the response you are getting:
{""tool_calls"":[ {""id"": ""call_HRxqlP3yzeHsoN43tMyZjMlr"", ""type"": ""function"", ""function"": {""name"": ""get_questions_and_answers"", ""arguments"": ""{\n  \""getQuestions\"": [\n    {\n      \""Question\"": \""What is the capital city of France?\"",\n      \""Answer\"": \""Paris\""\n    },\n    {\n      \""Question\"": \""Who painted the Mona Lisa?\"",\n      \""Answer\"": \""Leonardo da Vinci\""\n    },\n    {\n      \""Question\"": \""What is the largest planet in our solar system?\"",\n      \""Answer\"": \""Jupiter\""\n    },\n    {\n      \""Question\"": \""What is the national flower of England?\"",\n      \""Answer\"": \""Rose\""\n    },\n    {\n      \""Question\"": \""Which country is famous for its tulips?\"",\n      \""Answer\"": \""Netherlands\""\n    }\n  ]\n}""}}

I used https://jsoneditoronline.org/ to auto correct the json and it just adds ""]}"". for some reason openai is not sending correct json response. you have to add it
accumulatedText += ""]}"";

then response works:

this is too specific error. if openai updates its response api, it might send the json data correctly. so a better approach would be parsing in try/catch
try {
      const parsed = JSON.parse(accumulatedText);
      console.log({ parsed });
    } catch (error) {
      // you should error for each specific case
      accumulatedText += ""]}"";
      console.log(""correct accumulatedText in catch block"", accumulatedText);
    }

",chatgpt
how to debug the llama2 inference command with vscode,"I am trying to run the LLAMA2 inference script (shown below) with vscode debugging mode:
    torchrun --nproc_per_node 1 example_text_completion.py \
    --ckpt_dir models/7B-Chat \
    --tokenizer_path tokenizer.model \
    --max_seq_len 128 --max_batch_size 4

Before this, I can successfully run it with my command line interface, which shows my python environment is correct.
I have tried these two debug configs below:


 {
     ""name"": ""Python: run_llama2_inference"",
     ""type"": ""python"",
     ""request"": ""launch"",
     ""module"": ""torchrun"",
     ""args"": [
         ""--nproc_per_node=1"",
         ""example_chat_completion.py"",
         ""--ckpt_dir=models/7B-Chat/"",
         ""--tokenizer_path=tokenizer.model"",
         ""--max_seq_len=512"",
         ""--max_batch_size=4"",
     ],
     ""console"": ""integratedTerminal"",
     ""justMyCode"": true,
     ""env"": {
         ""PYTHONPATH"": ""${workspaceFolder}""
     }
 },

Corresponding Error Messsage: ""No module named torchrun""


 {
     ""name"": ""Python: run_llama2_inference"",
     ""type"": ""python"",
     ""request"": ""launch"",
     ""module"": ""torch.distributed.launch"",
     ""args"": [
         ""--use-env"",
         ""example_chat_completion.py"",
         ""--nproc_per_node=1"",
         ""--ckpt_dir=models/7B-Chat/"",
         ""--tokenizer_path=tokenizer.model"",
         ""--max_seq_len=512"",
         ""--max_batch_size=4"",
     ],
     ""console"": ""integratedTerminal"",
     ""justMyCode"": true,
     ""env"":
         ""PYTHONPATH"": ""${workspaceFolder}""
     }
 },

Corresponding Error Messsage: ""Could not consume arg: --nproc_per_node=1""


Both configs do not work as expected. I would like to seek the advice from online experts. Appreciate your ideas or advice in advance!
","You can use the ""program"" field to specify the Python script you want to run (example_text_completion.py), and pass the rest of the arguments using the ""args"" field.
Here's an example of how you can modify your launch configuration:
{
    ""name"": ""Python: run_llama2_inference"",
    ""type"": ""python"",
    ""request"": ""launch"",
    ""program"": ""${workspaceFolder}/example_chat_completion.py"",
    ""args"": [
        ""--ckpt_dir=models/7B-Chat/"",
        ""--tokenizer_path=tokenizer.model"",
        ""--max_seq_len=512"",
        ""--max_batch_size=4"",
    ],
    ""console"": ""integratedTerminal"",
    ""justMyCode"": true,
    ""env"": {
        ""PYTHONPATH"": ""${workspaceFolder}""
    }
}

",llama
cannot read properties of null reading 39split39 prompt,"Good afternoon,
Please, could you help me with an issue below.
I am getting an error ""Cannot read properties of null (reading 'split')"" when trying to click ""cancel"" on prompt.
What all I want, to loop canceled when I cancel it in prompt.
ps.: this is a simple array with a name/surname which in future will shows via console.log
function UserList () {
    let users = [];
    while(true) {
        users.push (prompt('Please, enter your name surname?').split(' '));
        if (prompt=== null) {
            alert('cancel');
        }
    }
}
let userList = new UserList();

","You need to test whether the result of prompt() is null before you try to split it.
You need to break out of the loop when the user cancels, otherwise the function will never return.
Also, since you're using this as a object constructor, users should be a property this.users.
function UserList () {
    this.users = [];
    while(true) {
        let response = prompt('Please, enter your name surname?');
        if (response == null) {
            alert('cancel');
            break;
        }
        this.users.push (response.split(' '));
    }
}

",prompt
setting padding token as eos token when using datacollatorforlanguagemodeling from huggingface,"In https://huggingface.co/learn/nlp-course/chapter7/6#preparing-the-dataset, there is
from transformers import DataCollatorForLanguageModeling

tokenizer.pad_token = tokenizer.eos_token
data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)

What the tutorial is doing is using a pretrained GPT2 model and its tokenizer and trying to create a dataset for causal language modeling pretraining task.
My question with the above line is that padding token is set to be the eos token. As a result even the original eos tokens will be ignored by the model during training since they will be perceived as padding tokens too.
This would prevent my model from learning to output eos tokens when its generation is over.
How come this is in the tutorials and it is a correct way ?
","TL;DR
Ignoring the EOS symbol when training a normal language model is okay. So padding the sequence with EOS instead of a dedicated PAD symbol is okay too.

In Long
When using DataCollatorForLanguageModeling(tokenizer, mlm=False), the ""masked-language modeling"" model is off and we are doing casual language modeling ,i.e. predicting the next word given the previous. Consider this:
['this', 'is', 'a', 'foobar', '.', 'EOS']

Now we pad the sequence until it's of length 10 tokens
['this', 'is', 'a', 'foobar', '.', 'EOS', 'EOS', 'EOS', 'EOS', 'EOS']

When the model learns with causal language model, it's predicting the next word given the previous, i.e.
>>> predict(next_token, given=[""BOS""])
'this'

>>> predict(next_token, given=[""BOS"", ""this""])
'is'

...

>>> predict(next_token, given=[""BOS"", ""this"", ""is"", ""a"", ""foobar"", "".""])
'EOS'

In most common inference routine, the model will stop once the first EOS is predicted, or all beams in the search during inference produced their first EOS.
During training, the model will learn:
ground_truth = [
 'this', 'is', 'a', 'foobar', '.', 'EOS', 'EOS', 'EOS', 'EOS', 'EOS', 
]

ground_prediction = [
 'this', 'is', 'foobar', '.', 'EOS', 'EOS', 'EOS', 'EOS', 'EOS', 'EOS', 
]

And when you compute the perplexity, all the PAD symbols are ignored, and in this case, when you treat the EOS as PAD, you are essentially tell the model even the first EOS is not necessary when computing perplexity.
Q: Is that the right thing to do to ignore even the first EOS token, when we use EOS as a padding token?
A: It depends on your task and what you want the 'EOS' to mean. For most natural language, we have punctuations before 'EOS', so EOS/PAD doesn't really matter. For programming language, we have '\n' and ';' or some end of sequence operator, so EOS isn't that necessary too.
Q: Then why do we bother to pad?
A: Actually that's a good question, we're padding so that the dot-products in transformer attentions can be ""easily"" computed.
But there are many cases where pad tokens can be efficiently packed, like in RNN https://pytorch.org/docs/stable/generated/torch.nn.utils.rnn.pad_packed_sequence.html (IIRC, not in transformers architecture though)
But I don't know how much of that is already in Pytorch/JAX underlying library for ""efficient"" transformers, which will allow us to avoid pre-padding inputs. From my experience in using Huggingface Pytorch models, if you don't pad the inputs, most probably the model will complain when you do a forward pass =(
If only, someone fix that mathematically. Maybe someone did try but it's not that common to be largely used by most transformers pre-trained model (yet).
",huggingface-transformers
i load a float32 hugging face model cast it to float16 and save it how can i load it as float16,"I load a huggingface-transformers float32 model, cast  it to float16, and save it. How can I load it as float16?
Example:
# pip install transformers
from transformers import AutoModelForTokenClassification, AutoTokenizer

# Load model
model_path = 'huawei-noah/TinyBERT_General_4L_312D'
model = AutoModelForTokenClassification.from_pretrained(model_path)
tokenizer = AutoTokenizer.from_pretrained(model_path)

# Convert the model to FP16
model.half()

# Check model dtype
def print_model_layer_dtype(model):
    print('\nModel dtypes:')
    for name, param in model.named_parameters():
        print(f""Parameter: {name}, Data type: {param.dtype}"")

print_model_layer_dtype(model)
save_directory = 'temp_model_SE'
model.save_pretrained(save_directory)

model2 = AutoModelForTokenClassification.from_pretrained(save_directory, local_files_only=True)
print('\n\n##################')
print(model2)
print_model_layer_dtype(model2)

In this example, model2 loads as a float32 model (as shown by print_model_layer_dtype(model2)), even though model2 was saved as float16 (as shown in config.json). What is the proper way to load it as float16?
Tested with transformers==4.36.2 and Python 3.11.7 on Windows 10.
","Use torch_dtype='auto' in from_pretrained(). Example:
model2 = AutoModelForTokenClassification.from_pretrained(save_directory, 
                                                         local_files_only=True,
                                                         torch_dtype='auto')

Full example:
# pip install transformers
from transformers import AutoModelForTokenClassification, AutoTokenizer
import torch

# Load model
model_path = 'huawei-noah/TinyBERT_General_4L_312D'
model = AutoModelForTokenClassification.from_pretrained(model_path)
tokenizer = AutoTokenizer.from_pretrained(model_path)

# Convert the model to FP16
model.half()

# Check model dtype
def print_model_layer_dtype(model):
    print('\nModel dtypes:')
    for name, param in model.named_parameters():
        print(f""Parameter: {name}, Data type: {param.dtype}"")

print_model_layer_dtype(model)
save_directory = 'temp_model_SE'
model.save_pretrained(save_directory)

model2 = AutoModelForTokenClassification.from_pretrained(save_directory, local_files_only=True, torch_dtype='auto')
print('\n\n##################')
print(model2)
print_model_layer_dtype(model2)

It'll load model2 as torch.float16.
",huggingface-transformers
runtimeerror no validator found for ltclass 39langchain_communitytoolsjsontooljsonspec39gt see arbitrary_types_allowed in config,"I am getting RuntimeError: no validator found for <class 'langchain_community.tools.json.tool.JsonSpec'>, see arbitrary_types_allowed in Config even after to installing
import yaml
from langchain_community.agent_toolkits import JsonToolkit, create_json_agent
from langchain_community.tools.json.tool import JsonSpec
from langchain_openai import OpenAI

","The error you're encountering due to incorrect Pydantic configuration.
LangChain uses Pydantic models for validation, and this error can occur when Pydantic does know to handle certain types, such as the JsonSpec.
Pydantic's Config class has an option called arbitrary_types_allowed that needs to be set to True in such cases.
SOLUTIONS

always work in a virtual environment. (recommended)

check install pydantic and other modules related to langchain like langchain-community or langchain-openai. (recommended)

use the following code to set arbitrary_types_allowed to True
  from pydantic import BaseModel

  class Maintain(BaseModel):
        class Config:
              arbitrary_types_allowed = True



",langchain
how does one set the pad token correctly not to eos during finetuning to avoid model not predicting eos,"**tldr; what I really want to know is what is the official way to set pad token for fine tuning it wasn't set during original training, so that it doesn't not learn to predict EOS. **
colab: https://colab.research.google.com/drive/1poFdFYmkR_rDM5U5Z2WWjTepMQ8hvzNc?usp=sharing

The HF falcon tutorial has the following line:
tokenizer.pad_token = tokenizer.eos_token

it looks strange to me. It make sense pad and eos are the same but then why even make a difference between them in the first place in general?
Note its wrong to do pad = eos. This means during fine-tuning the model will never be trained to output eos (most likely) since eos is treated as pad token and no back propagated:
I just observed that when I set tokenizer.pad_token = tokenizer.eos_token during training, the model won't stop generating during inference, since it was trained to not output the eos token (per discussions above).

I saw this (here https://github.com/huggingface/transformers/issues/22794):
tokenizer.add_special_tokens({'pad_token': '[PAD]'})

But this assumes the model has a pad_token. I think an additional check has to be done that it does have an embedding for pad_token so that there are no run time errors (~type errors in the matrix extraction from the embedding ""table""/matrix).
But if one does that some care might be needed to initialize the new token so that it dominates the generation: https://nlp.stanford.edu/~johnhew/vocab-expansion.html

code:
def get_model_tokenizer_qlora_falcon7b(model_name: str = ""ybelkada/falcon-7b-sharded-bf16"",
                                       config: wand.Config,  # todo
                                       lora_alpha=16,  # todo
                                       lora_dropout=0.1,  # todo
                                       lora_r=64,  # todo
                                       bnb_4bit_compute_dtype=torch.float16,  # changed it from Guanaco hf
                                       ) -> tuple:
    """"""
    Load the Falcon 7B model, quantize it in 4bit and attach LoRA adapters on it.

    bf16 = 1S, 7Exp, 8Mantissa

    Do:
        pip install bitsandbytes
    ref:
        - https://colab.research.google.com/drive/1DOi8MFv4SWN9NImVornZ7t6BgmLoPQO-#scrollTo=AjB0WAqFSzlD
    """"""
    from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, AutoTokenizer

    # model_id = ""tiiuae/falcon-7b""
    # model_name: str = ""ybelkada/falcon-7b-sharded-bf16""

    # - get bnb config for bit-4 base model (bnb lib for using 4bit qlora quantization techniques by tim dettmers)
    bnb_config = BitsAndBytesConfig(
        load_in_4bit=True,  # load (usually huge) base model in 4 bits
        bnb_4bit_quant_type=""nf4"",  # normal float 4 for the (usually huge) base model. introduces error but fixed by ft
        # ref: https://gist.github.com/pacman100/1731b41f7a90a87b457e8c5415ff1c14
        bnb_4bit_compute_dtype=bnb_4bit_compute_dtype,
    )

    # - get falcon 4bit model
    model = AutoModelForCausalLM.from_pretrained(
        model_name,
        quantization_config=bnb_config,
        trust_remote_code=True  # allows to execute custom code you download from the uploaded model code you are using
    )
    model.config.use_cache = False  # todo: why? https://stackoverflow.com/questions/76633335/why-does-hugging-face-falcon-model-use-mode-config-use-cache-false-why-wouldn

    # get falcon tockenizer
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)  # execs code downloaded from hf hub
    tokenizer.pad_token = tokenizer.eos_token


Modifying model gives issues
Darn this still not works:
 UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation)

code:
""""""
sfttrainer (likely using peft) best practices:
https://huggingface.co/docs/trl/main/en/sft_trainer#best-practices

Best practices

Pay attention to the following best practices when training a model with that trainer:

- SFTTrainer always pads by default the sequences to the max_seq_length argument of the SFTTrainer. If none is passed, the trainer will retrieve that value from the tokenizer. Some tokenizers do not provide default value, so there is a check to retrieve the minimum between 2048 and that value. Make sure to check it before training.
- For training adapters in 8bit, you might need to tweak the arguments of the prepare_model_for_int8_training method from PEFT, hence we advise users to use prepare_in_int8_kwargs field, or create the PeftModel outside the SFTTrainer and pass it.
- For a more memory-efficient training using adapters, you can load the base model in 8bit, for that simply add load_in_8bit argument when creating the SFTTrainer, or create a base model in 8bit outside the trainer and pass it.
- If you create a model outside the trainer, make sure to not pass to the trainer any additional keyword arguments that are relative to from_pretrained() method.

todo: why trust_remote_code? I want more details.
""""""
import sys

import torch
from peft import LoraConfig

from transformers.modeling_utils import PreTrainedModel

from pdb import set_trace as st


def test_bfloat16_int4(compute_dtype: torch.dtype,
                       use_4bit,
                       ):
    """"""
python -c ""import torch; print(torch.cuda.get_device_capability());""
    todo: check other code test_bfloat16() do we need use_4bit?
    """"""
    if compute_dtype == torch.float16 and use_4bit:
        major, _ = torch.cuda.get_device_capability()
        if major >= 8:
            print(""="" * 80)
            print(""Your GPU supports bfloat16, you can accelerate training with the argument --bfloat16"")
            print(""="" * 80)


def get_model_tokenizer_qlora_falcon7b(
        # -- mode args
        # model_id = ""tiiuae/falcon-7b""
        pretrained_model_name_or_path: str = ""ybelkada/falcon-7b-sharded-bf16"",
        use_cache: bool = True,
        # -- lora args
        lora_alpha=16,  # todo
        lora_dropout=0.1,  # todo, evidence drop out really help? google, crfm, gpt4
        lora_r=64,  # todo
        bnb_4bit_compute_dtype=torch.float16,  # changed it from Guanaco hf

        # -- training args
        output_dir=""./results"",
        per_device_train_batch_size=4,
        gradient_accumulation_steps=4,
        # paging so that the sudden mem gpu spikes don't cause the run to shut down
        # (I think usually caused by too long seqs)
        # todo: why 32 bit opt?
        # todo: paged nadamw opt?
        optim=""paged_adamw_32bit"",
        save_steps=10,
        logging_steps=10,
        learning_rate=2e-4,
        max_grad_norm=0.3,
        max_steps=500,
        warmup_ratio=0.03,
        lr_scheduler_type=""constant"",
        # -- quant. args (not recommended to be changed unless you know what your doing?)
        load_in_4bit=True,  # load (usually huge) base model in 4 bits
        bnb_4bit_quant_type=""nf4"",  # normal float 4 for the (large) base models qlora
) -> tuple:
    """"""
    Load the Falcon 7B model, quantize it in 4bit and attach LoRA adapters on it.

    bf16 = 1S, 7Exp, 8Mantissa
    hypothesis: 7b trained due to 6.7 emergence rumour, I still don't think emergence is real.
    Notes:
        - ft a model is very specific to the model, tokenizer and training scheme. Thus we return
            - model, tokenizer, ft config (peft config), training args

    ref:
        - https://colab.research.google.com/drive/1DOi8MFv4SWN9NImVornZ7t6BgmLoPQO-#scrollTo=AjB0WAqFSzlD
    """"""
    from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, AutoTokenizer

    # - Get bnb config for bit-4 base model (bnb lib for using 4bit qlora quantization techniques by tim dettmers)
    bnb_config = BitsAndBytesConfig(
        load_in_4bit=load_in_4bit,  # load (usually huge) base model in 4 bits
        bnb_4bit_quant_type=bnb_4bit_quant_type,  # normal float 4 for the (usually huge) base model
        bnb_4bit_compute_dtype=bnb_4bit_compute_dtype,  # if you can, during computation use bf16
    )

    # - Get falcon 4bit model
    # todo, where is this being saved & how to download quicker
    model = AutoModelForCausalLM.from_pretrained(
        pretrained_model_name_or_path=pretrained_model_name_or_path,
        quantization_config=bnb_config,
        trust_remote_code=True  # allows to execute custom code you download from the uploaded model code you are using
    )
    print(f'{type(model)=}')
    print(f'{model=}')
    # this is here to save gpu vram. Likely only needed when using 40b or when oom issues happen ref: https://stackoverflow.com/questions/76633335/why-does-hugging-face-falcon-model-use-mode-config-use-cache-false-why-wouldn
    model.config.use_cache = use_cache
    print(f'{type(model)=}')

    # - Get falcon tokenizer
    tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name_or_path,
                                              trust_remote_code=True)  # execs code downloaded from hf hub
    # tokenizer.pad_token = tokenizer.eos_token  # ref: https://stackoverflow.com/questions/76633368/why-does-the-falcon-qlora-tutorial-code-use-eos-token-as-pad-token
    # tokenizer.add_special_tokens({'pad_token': '[PAD]'})  # I think this is fine if during the training pad is ignored
    tokenizer.add_special_tokens({'pad_token': '<|pad|>'})  # I think this is fine if during the training pad is ignored

    # - Modify model
    # add pad token embed
    model.resize_token_embeddings(len(tokenizer))  # todo: I think this is fine if during the training pad is ignored
    model.transformer.word_embeddings.padding_idx = len(tokenizer) - 1
    model.config.max_new_tokens = len(tokenizer)
    # model.config.min_length = 1
    print(f'{model=}')
    print(f'{type(tokenizer)=}')
    print(f'{tokenizer.pad_token=}')
    # data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False) todo

    # - Get falcon lora config
    peft_config = LoraConfig(
        lora_alpha=lora_alpha,
        lora_dropout=lora_dropout,
        r=lora_r,
        bias=""none"",
        task_type=""CAUSAL_LM"",
        # model card for falcon tiiuae/falcon-7b: https://huggingface.co/tiiuae/falcon-7b/blob/main/modelling_RW.py
        # does seem to include all trainable params as done by qlora on their own paper
        target_modules=[
            # word_embeddings,
            ""query_key_value"",
            ""dense"",
            ""dense_h_to_4h"",
            ""dense_4h_to_h"",
            # ""lm_head""
        ]
    )
    print(f'{type(peft_config)=}')

    # todo: print the num params of the lora = D1*r + D2*r and num of bytes by prec. (bytes) * num params
    return model, tokenizer, peft_config


# -- tests

def example_test_model_already_has_pad_token():
    """"""
    if it already has pad token, it likely has a small prob, so we are done.

    compare it's norm with other tokens to verify this is true.

python ~/ultimate-utils/ultimate-utils-proj-src/uutils/hf_uu/model_tokenizer/falcon_uu_mdl_tok.py
    """"""
    # - the get datasets todo: preprocessing, padding, streaming
    from uutils.hf_uu.data_hf.common import get_guanaco_datsets_add_splits_train_test_only
    trainset, _, testset = get_guanaco_datsets_add_splits_train_test_only()

    # qlora flacon7b
    from uutils.hf_uu.model_tokenizer.falcon_uu_mdl_tok import get_model_tokenizer_qlora_falcon7b
    model, tokenizer, peft_config = get_model_tokenizer_qlora_falcon7b()
    model: PreTrainedModel = model
    print(f'{model=}')
    sent = 'Dogs are great because they are '
    print()

    # print to see if pad tokens are present and if it ignores the tokens at the end
    encoded_input = tokenizer(sent, padding='max_length', max_length=10, return_tensors='pt')
    print(f'{encoded_input=}')

    # Print all special tokens
    print('\n---- start Print all special tokens')
    for token_name, token in tokenizer.special_tokens_map.items():
        print(f""{token_name}: {token}"")
    print('\n---- end Print all special tokens')

    # Get the ID for the '[PAD]' token
    try:
        pad_token_id = tokenizer.convert_tokens_to_ids('[PAD]')
    except KeyError:
        raise ValueError(""Token [PAD] is not present in the tokenizer vocabulary."")

    # Index into the model's embedding table
    try:
        print(f'{model.get_input_embeddings().weight.size()=}')
        pad_embedding = model.get_input_embeddings().weight[pad_token_id]
    except IndexError:
        raise ValueError(f""Token ID {pad_token_id} is not present in the model's embedding matrix."")

    print(f'{pad_embedding=}')
    print('Success!\n')

    # check it generates something sensible
    # tokenizer.decode(model.generate(**tokenizer(sent, return_tensors='pt'), do_sample=True)[0])
    input_ids, attention_mask = encoded_input['input_ids'], encoded_input['attention_mask']
    predicted_tokens_ids_options = model.generate(input_ids=input_ids, attention_mask=attention_mask, do_sample=True)
    predicted_tokens_ids = predicted_tokens_ids_options[0]
    predicted_sent = tokenizer.decode(predicted_tokens_ids)
    print(f'original sentence: {sent=}')
    print(f'predicted sentence: {predicted_sent=}')
    print('Success2!')


if __name__ == '__main__':
    import time

    start_time = time.time()
    example_test_model_already_has_pad_token()
    print(f""The main function executed in {time.time() - start_time} seconds.\a"")

it doesn't like the modifications to the model:
    model.transformer.word_embeddings.padding_idx = len(tokenizer) - 1
    model.config.max_new_tokens = len(tokenizer)

How to fix?
Errors:
/lfs/hyperturing1/0/brando9/miniconda/envs/data_quality/lib/python3.10/site-packages/transformers/generation/utils.py:1259: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation)
  warnings.warn(
Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.
/lfs/hyperturing1/0/brando9/miniconda/envs/data_quality/lib/python3.10/site-packages/transformers/generation/utils.py:1452: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cpu, whereas the model is on cuda. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cuda') before running `.generate()`.
  warnings.warn(
Traceback (most recent call last):
  File ""/lfs/hyperturing1/0/brando9/ultimate-utils/ultimate-utils-proj-src/uutils/hf_uu/model_tokenizer/falcon_uu_mdl_tok.py"", line 211, in <module>
    example_test_model_already_has_pad_token()
  File ""/lfs/hyperturing1/0/brando9/ultimate-utils/ultimate-utils-proj-src/uutils/hf_uu/model_tokenizer/falcon_uu_mdl_tok.py"", line 199, in example_test_model_already_has_pad_token
    predicted_tokens_ids_options = model.generate(input_ids=input_ids, attention_mask=attention_mask, do_sample=True)
  File ""/lfs/hyperturing1/0/brando9/miniconda/envs/data_quality/lib/python3.10/site-packages/torch/utils/_contextlib.py"", line 115, in decorate_context
    return func(*args, **kwargs)
  File ""/lfs/hyperturing1/0/brando9/miniconda/envs/data_quality/lib/python3.10/site-packages/transformers/generation/utils.py"", line 1572, in generate
    return self.sample(
  File ""/lfs/hyperturing1/0/brando9/miniconda/envs/data_quality/lib/python3.10/site-packages/transformers/generation/utils.py"", line 2633, in sample
    next_token_scores = logits_warper(input_ids, next_token_scores)
  File ""/lfs/hyperturing1/0/brando9/miniconda/envs/data_quality/lib/python3.10/site-packages/transformers/generation/logits_process.py"", line 92, in __call__
    scores = processor(input_ids, scores)
  File ""/lfs/hyperturing1/0/brando9/miniconda/envs/data_quality/lib/python3.10/site-packages/transformers/generation/logits_process.py"", line 302, in __call__
    indices_to_remove = scores < torch.topk(scores, top_k)[0][..., -1, None]
RuntimeError: ""topk_cpu"" not implemented for 'Half'


Bounty Section: Small GPT2 code example
Yes I agree that pad is assigned to eos. Eos is still eos. But during fine-tuning now the weights wrt to eos are unchanged. This might be an issue since the probability of eos has not shifted to the fine-tuning regime. One possibility is that eos is outputed with less chance. Yes we can still halt production when we see eos but we've not shifted the probability to output eos according to our fine-tuning distribution -- but all other tokens have changed distribution. I think this could be an issue because it's not like the old probability of eos is conserved since all tokens probs have changed except eos + even if the old eos prob was conserved, it's wrt wrong distribution (not the fine tuning one).
e.g.,
    if tokenizer.pad_token_id is None:
        tokenizer.pad_token = tokenizer.eos_token
...
    raw_text_batch='a'
    tokenize_batch={'input_ids': tensor([[   64, 50256, 50256, 50256, 50256]]), 'attention_mask': tensor([[1, 0, 0, 0, 0]])}

but it would have been better to have
    tokenize_batch={'input_ids': tensor([[   64, 50256, 50256, 50256, 50256]]), 'attention_mask': tensor([[1, 1, 0, 0, 0]])}

code
def test_eos_pad():
    from datasets import load_dataset
    import torch
    from transformers import GPT2Tokenizer, GPT2LMHeadModel

    raw_text_batch = 'a'

    tokenizer = GPT2Tokenizer.from_pretrained(""gpt2"")
    # print(f'{tokenizer.eos_token=}')
    # print(f'{tokenizer.eos_token_id=}')
    # print(f'{tokenizer.pad_token=}')
    # print(f'{tokenizer.pad_token_id=}')

    # print(f'{raw_text_batch=}')
    # tokenize_batch = tokenizer(raw_text_batch, padding=""max_length"", max_length=5, truncation=True, return_tensors=""pt"")
    # print(f'{tokenize_batch=}')

    if tokenizer.pad_token_id is None:
        tokenizer.pad_token = tokenizer.eos_token
    probe_network = GPT2LMHeadModel.from_pretrained(""gpt2"")
    device = torch.device(f""cuda:{0}"" if torch.cuda.is_available() else ""cpu"")
    probe_network = probe_network.to(device)

    print(f'{tokenizer.eos_token=}')
    print(f'{tokenizer.eos_token_id=}')
    print(f'{tokenizer.pad_token=}')
    print(f'{tokenizer.pad_token_id=}')

    print(f'{raw_text_batch=}')
    tokenize_batch = tokenizer(raw_text_batch, padding=""max_length"", max_length=5, truncation=True, return_tensors=""pt"")
    print(f'{tokenize_batch=}')
    print('Done')


cross:

hf discuss forum: https://discuss.huggingface.co/t/why-does-the-falcon-qlora-tutorial-code-use-eos-token-as-pad-token/45954
pytorch forum discuss: https://discuss.pytorch.org/t/how-does-one-set-the-pad-token-correctly-not-to-eos-during-fine-tuning-to-avoid-model-not-predicting-eos/213619
https://discord.com/channels/879548962464493619/1126681170957045770/1126681170957045770
context peft pacman100 code: https://gist.github.com/pacman100/1731b41f7a90a87b457e8c5415ff1c14
twitter tweet of this: https://twitter.com/BrandoHablando/status/1693676898013061337?s=20

","Update August/8/2024
One more improvement. Some models like DeepSeekCoder base 7B do have a pad token already. So no need to set the pad token to eos. But the code that pads up to 1st occurence of eos + pads the rest has to pad the rest assuming they are pad tokens. So that's the diff:
def get_lm_examples_1st_eos_mask_remaining_eos(
        examples,
        tokenizer: AutoTokenizer, 
        
        # desired_dataset_column: str = 'text',
        # method_to_remove_columns: str = 'keys',

        remove_to_long_seqs: bool = False,
        # format: str = 'torch',
        debug: bool = False,
        ) -> dict[str, torch.Tensor]:
    """""" 
    Train only on first occurence of eos. The remaining eos are masked out. If 
    - train up to 1st ocurrence of eos token, mask out the rest of the eos tokens.
    - drop or not seqs that are too long, i.e., have no eos token.
    
    Assumes: pad == eos

    ref: https://stackoverflow.com/questions/76633368/how-does-one-set-the-pad-token-correctly-not-to-eos-during-fine-tuning-to-avoi
    """"""
    # - Get lm example
    seq_length: int = examples['input_ids'].size(0)
    print(f'{examples[""input_ids""].size()=}, {seq_length=}') if debug else None
    examples[""labels""] = examples[""input_ids""].clone()  # labels is hardcoded in HF so put it!
    eos_token_id = tokenizer.eos_token_id
    # assert eos_token_id == tokenizer.pad_token_id, 'Error: pad should be eos token'
    print(f'{tokenizer.pad_token_id=}, {tokenizer.eos_token_id=}') if debug else None
    seqs_to_drop: list[int] = [] # store idx to drop (to long), we don't want to modify the two lists at the same time as we are looping through them
    for idx, input_ids in enumerate(examples[""input_ids""]):
        # Find all occurrences of eos_token
        eos_positions = (input_ids == eos_token_id).nonzero(as_tuple=True)[0]
        if eos_positions.nelement() > 0:  # Check if eos_token is present --> if yes then make sure to trian on it then mask the remaining eos (assumes pad == eos)
            first_eos_position = eos_positions[0]
            examples[""attention_mask""][idx, first_eos_position] = 1  # Set the mask value to 1
            # Assert that the label for the first occurrence of eos_token is eos_token_id
            assert examples[""labels""][idx, first_eos_position] == eos_token_id, ""The label for the first eos_token is incorrect!""
            # # For all subsequent occurrences of eos_token, set their labels to -100
            # for subsequent_eos_position in eos_positions[1:]:
            #     examples[""labels""][idx, subsequent_eos_position] = -100
            #     assert examples[""labels""][idx, subsequent_eos_position] == -100, ""The label for the subsequent_eos_position incorrect! Should be -100.""
            # after first eos token mask everything (eos AND pad, hopefully that's all there but we can sanity check later)
            for desired_mask_idx in range(first_eos_position, seq_length):
                examples[""labels""][idx, desired_mask_idx] = -100
                assert examples[""labels""][idx, desired_mask_idx] == -100, ""The label for the desired_mask_idx incorrect! Should be -100.""
        elif remove_to_long_seqs:
            assert eos_positions.nelement() == 0, 'Error: there should be no eos if this if stmt is exexuted.'
            # record to drop this seq, has no eos so too long + flag says to drop it
            seqs_to_drop.append(idx)
        else:
            pass # nop: no eos in seq so too long, but keep it for training anyway
    # assert len(examples[""labels""]) == 0, 'Error: no labels were set'
    # -- Drop seqs with no eos
    if seqs_to_drop:
        examples[""input_ids""] = torch.stack([input_ids for idx, input_ids in enumerate(examples[""input_ids""]) if idx not in seqs_to_drop])
        examples[""attention_mask""] = torch.stack([mask for idx, mask in enumerate(examples[""attention_mask""]) if idx not in seqs_to_drop])
        examples[""labels""] = torch.stack([labels for idx, labels in enumerate(examples[""labels""]) if idx not in seqs_to_drop])
    return examples


Update, I made this better. Now if seq does not have eos at all you can remove that seq or chose to train on it
def raw_ds_2_lm_ds_mask_eos_pad_toks(
        raw_dataset, 
        tokenizer, 
        max_length: int,

        raw_str_2_desired_str: Optional[callable] = None, # either return {'text': examples['text']} or preprocess str to get what you need e.g. {'text': f""[ex['nl'] ex['fl'] {tok.eos_token}]"" for ex in examples}
        desired_dataset_column: str = 'text', # good val to use if hf str ds already pre-processed for you: 'text',

        method_to_remove_columns: str = 'keys',

        padding: str = 'max_length',
        truncation: bool = True, 
        return_tensors: str = 'pt',

        batched: bool = True, # Setting `batched=True` in the `dataset.map` function of Hugging Face's datasets library processes the data in batches rather than one item at a time, significantly speeding up the tokenization and preprocessing steps.
        streaming: bool = False,

        format: str = 'torch',
        # get_lm_examples_function = get_lm_examples_1st_eos_mask_remaining_eos,
        ):
    """""" """"""
    # - Get desired str dataset
    if raw_str_2_desired_str is None:
        get_desired_examples_str_function = lambda examples: {'text': examples[desired_dataset_column]} if raw_str_2_desired_str is not None else raw_str_2_desired_str 
    else:
        get_desired_examples_str_function = raw_str_2_desired_str
    desired_examples_str_dataset = raw_dataset.map(get_desired_examples_str_function, batched=batched) # note: we can't remove all str columns here or we will remove the ones we want to tokenize by accident

    # - Get tokenized data set
    desired_examples_str_dataset = desired_examples_str_dataset.with_format(format)  # annoying that return tensors in the tokenizer on it's own doesn't put it into a pt tensor, so for now we keep both.
    remove_str_columns = get_column_names(desired_examples_str_dataset, streaming, method_to_remove_columns)  # remove all keys that are not tensors to avoid bugs in collate function in task2vec's pytorch data loader
    tokenize_function = lambda examples: tokenizer(examples[desired_dataset_column], padding=padding, max_length=max_length, truncation=truncation, return_tensors=return_tensors)
    tokenized_datasets = desired_examples_str_dataset.map(tokenize_function, batched=batched, remove_columns=remove_str_columns)

    # - Get lm data set
    # get_lm_examples_function = lambda examples : group_texts(examples, block_size)
    get_lm_examples_function = lambda examples : get_lm_examples_1st_eos_mask_remaining_eos(examples, tokenizer)
    lm_dataset = tokenized_datasets.map(get_lm_examples_function, batched=batched)
    return lm_dataset

def get_lm_examples_1st_eos_mask_remaining_eos(
        examples,
        tokenizer: AutoTokenizer, 
        
        # desired_dataset_column: str = 'text',
        # method_to_remove_columns: str = 'keys',

        remove_to_long_seqs: bool = False,
        # format: str = 'torch',
        ) -> dict[str, torch.Tensor]:
    """""" 
    Train only on first occurence of eos. The remaining eos are masked out. If 
    - train up to 1st ocurrence of eos token, mask out the rest of the eos tokens.
    - drop or not seqs that are too long, i.e., have no eos token.
    
    Assumes: pad == eos

    ref: https://stackoverflow.com/questions/76633368/how-does-one-set-the-pad-token-correctly-not-to-eos-during-fine-tuning-to-avoi
    """"""
    # - Get lm example
    examples[""labels""] = examples[""input_ids""].clone()  # labels is hardcoded in HF so put it!
    eos_token_id = tokenizer.eos_token_id
    assert eos_token_id == tokenizer.pad_token_id, 'Error: pad should be eos token'
    seqs_to_drop: list[int] = [] # store idx to drop (to long), we don't want to modify the two lists at the same time as we are looping through them
    for idx, input_ids in enumerate(examples[""input_ids""]):
        # Find all occurrences of eos_token
        eos_positions = (input_ids == eos_token_id).nonzero(as_tuple=True)[0]
        if eos_positions.nelement() > 0:  # Check if eos_token is present --> if yes then make sure to trian on it then mask the remaining eos (assumes pad == eos)
            first_eos_position = eos_positions[0]
            examples[""attention_mask""][idx, first_eos_position] = 1  # Set the mask value to 1
            # Assert that the label for the first occurrence of eos_token is eos_token_id
            assert examples[""labels""][idx, first_eos_position] == eos_token_id, ""The label for the first eos_token is incorrect!""
            # For all subsequent occurrences of eos_token, set their labels to -100
            for subsequent_eos_position in eos_positions[1:]:
                examples[""labels""][idx, subsequent_eos_position] = -100
                assert examples[""labels""][idx, subsequent_eos_position] == -100, ""The label for the subsequent_eos_position incorrect! Should be -100.""
        elif remove_to_long_seqs:
            assert eos_positions.nelement() == 0, 'Error: there should be no eos if this if stmt is exexuted.'
            # record to drop this seq, has no eos so too long + flag says to drop it
            seqs_to_drop.append(idx)
        else:
            pass # nop: no eos in seq so too long, but keep it for training anyway
    # assert len(examples[""labels""]) == 0, 'Error: no labels were set'
    # -- Drop seqs with no eos
    if seqs_to_drop:
        examples[""input_ids""] = torch.stack([input_ids for idx, input_ids in enumerate(examples[""input_ids""]) if idx not in seqs_to_drop])
        examples[""attention_mask""] = torch.stack([mask for idx, mask in enumerate(examples[""attention_mask""]) if idx not in seqs_to_drop])
        examples[""labels""] = torch.stack([labels for idx, labels in enumerate(examples[""labels""]) if idx not in seqs_to_drop])
    return examples

train script:
""""""
Refs:
    - https://claude.ai/chat/ad5c9e18-beb4-48fb-9f43-a2ba463ce158
    - https://chatgpt.com/c/349f2c8a-949e-444d-ae3c-8ca60ba77831
""""""
import glob
import os
import numpy as np
import torch
from transformers import GPT2Tokenizer, GPT2LMHeadModel, TrainingArguments, Trainer
from transformers import AutoModelForCausalLM, AutoTokenizer
from datasets import load_dataset, load_metric
from typing import Dict, Tuple, Optional
from pathlib import Path
import evaluate

from utils import eval_hf
from utils import raw_ds_2_lm_ds_mask_eos_pad_toks

def compute_metrics(eval_pred: Tuple[np.ndarray, np.ndarray],
                    path: str = 'accuracy',
                    ) -> Dict[str, float]:
    """"""
    Compute the accuracy of the model.

    Args:
    eval_pred: A tuple containing the model predictions and labels.

    Returns:
    A dictionary with the accuracy score.
    
    TODO: document properly what accuracy is. Is it tfa, ara, exact string match, avg acc (wrt length etc.) ref: https://huggingface.co/spaces/evaluate-metric/accuracy
    """"""
    metric = evaluate.load(path=path)   # load metric from file or hf
    predictions, references = eval_pred
    predictions = np.argmax(predictions, axis=1)
    return metric.compute(predictions=predictions, references=references)

def preprocess_function_proofnet_simple(examples: Dict[str, list], tokenizer: GPT2Tokenizer, max_length: int = 512) -> Dict[str, torch.Tensor]:
    """"""
    Preprocess the input data for the proofnet dataset.

    Args:
    examples: The examples to preprocess.
    tokenizer: The tokenizer for encoding the texts.

    Returns:
    The processed model inputs.
    """"""
    # - Get raw string ins,outs (so deal with HF data set columns at str level)
    inputs: list[str] = [f""{examples['nl_statement'][i]}{tokenizer.eos_token}{examples['formal_statement'][i]}"" for i in range(len(examples['nl_statement']))]
    # - Get tokenized ins,outs (so remove irrelevant ""string"" columns to get only ""tensor"" relevant columns)
    model_inputs = tokenizer(inputs, max_length=max_length, padding=""max_length"", truncation=True, return_tensors=""pt"")
    # - Get lm ins,outs for training e.g., deal with padd, masks etc.
    labels = model_inputs.input_ids.clone()
    labels[labels == tokenizer.pad_token_id] = -100
    model_inputs[""labels""] = labels
    return model_inputs

def setup_and_train_proofnet(
        # pretrained_model_name_or_path: str = ""gpt2"", 
        # pretrained_model_name_or_path: str = ""openai-community/gpt2-xl"", 
        pretrained_model_name_or_path: str = ""meta-llama/Meta-Llama-3.1-8B"", 
        path: str = ""hoskinson-center/proofnet"",
        output_dir_train: str = '~/tmp/proofnet/train',
        output_dir_val: Optional[str] = None,  # we are training on the val set so no val set
        output_dir_test: str = '~/tmp/proofnet/test',
        path_to_save_model: Optional[str] = None,  # suggested path: '~/tmp/proofnet/model' then expanduser in py code
        num_train_epochs: int = 3,
        per_device_train_batch_size: Optional[int] = 2,
        per_device_eval_batch_size: Optional[int] = 2,
        learning_rate: float = 5e-5,
        weight_decay: float = 0.01,
        max_grad_norm: float = 1.0, 
        lr_scheduler_type = 'cosine', # https://discord.com/channels/879548962464493619/1227708244697284724/1227708244697284724
        warmup_ratio=0.01,  # copying alpaca for now, number of steps for a linear warmup,  https://discord.com/channels/879548962464493619/1227708244697284724/1227708244697284724
        optim='paged_adamw_32bit',
        gradient_accumulation_steps = 2, # Allows to process effective_batch_size = gradient_accumulation_steps * batch_size, num its to accumulate before opt update step
        gradient_checkpointing: Optional[bool] = True,
        report_to: str = 'none',  # recommended values 'wandb' or `none`
        ) -> None:
    """"""
    Set up the environment, preprocess the dataset, and train the model.

    export CUDA_VISIBLE_DEVICES=7

    Args:
    tokenizer_name: The name of the tokenizer.
    model_name: The name of the model.
    dataset_path: The path to the dataset.
    """"""
    # Clear CUDA cache to free up memory
    torch.cuda.empty_cache()

    # Load tokenizer and model
    if pretrained_model_name_or_path == ""gpt2"":
        tokenizer = GPT2Tokenizer.from_pretrained(pretrained_model_name_or_path, max_length=1024)
        if tokenizer.pad_token_id is None:
            tokenizer.pad_token = tokenizer.eos_token
            print(f'{tokenizer.pad_token=}')
        print(f'{tokenizer.eos_token=}\n{tokenizer.eos_token_id=}')
        model = GPT2LMHeadModel.from_pretrained(pretrained_model_name_or_path)
        device = torch.device(f""cuda:{0}"" if torch.cuda.is_available() else ""cpu"")
        model = model.to(device)
        max_length: int = tokenizer.model_max_length
        print(f'{max_length=}')
    elif pretrained_model_name_or_path == ""openai-community/gpt2-xl"":
        tokenizer = GPT2Tokenizer.from_pretrained(pretrained_model_name_or_path, max_length=1024)
        if tokenizer.pad_token_id is None:
            tokenizer.pad_token = tokenizer.eos_token
            print(f'{tokenizer.pad_token=}')
        print(f'{tokenizer.eos_token=}\n{tokenizer.eos_token_id=}')
        model = GPT2LMHeadModel.from_pretrained(pretrained_model_name_or_path)
        device = torch.device(f""cuda:{0}"" if torch.cuda.is_available() else ""cpu"")
        model = model.to(device)
        max_length: int = tokenizer.model_max_length
        print(f'{max_length=}') 
    elif pretrained_model_name_or_path == ""meta-llama/Meta-Llama-3.1-8B"":
        torch_dtype = torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float32 
        model = AutoModelForCausalLM.from_pretrained(pretrained_model_name_or_path, torch_dtype=torch_dtype)
        # tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name_or_path, padding_side=""right"", use_auth_token=True)
        tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name_or_path, padding_side=""right"")
        print(f'{tokenizer.pad_token=} {tokenizer.eos_token_id=}')
        tokenizer.pad_token = tokenizer.eos_token if tokenizer.pad_token_id is None else tokenizer.pad_token
        print(f'{tokenizer.pad_token=} {tokenizer.eos_token_id=}')
        # get context length for setting max length for training
        if hasattr(model.config, ""context_length""):
            # SEEMS IT IS NOT IN THE model.config
            print(""Context length:"", model.config.context_length)
            max_length: int = model.config.context_length
        else:
            print(f""Context length not found in model.config, so using your default or hardcoded value. Model is {pretrained_model_name_or_path=}."")
            # max_length: int = 4096
            max_length: int = 8192
            # max_length: int = 128  # for debugging
            # max_length: int = 128_000  # ref: https://huggingface.co/meta-llama/Meta-Llama-3.1-8B
            print(f'->{max_length=}')
    else:
        raise ValueError(f""Model {pretrained_model_name_or_path} not supported."")
    print(""Number of parameters:"", sum(p.numel() for p in model.parameters()))

    # - Load the dataset
    print(f'-Load the dataset')
    ## Proofnet
    # dataset_val = load_dataset(path, split='validation')
    # dataset_test = load_dataset(path, split='test')
    # # Preprocess the dataset
    # if path == ""hoskinson-center/proofnet"":
    #     preprocess_function = preprocess_function_proofnet_simple
    #     # note: text field is usually more common!
    #     val_dataset = dataset_val.map(lambda examples: preprocess_function(examples, tokenizer), batched=True, remove_columns=[""nl_statement"", ""formal_statement""])
    #     test_dataset = dataset_test.map(lambda examples: preprocess_function(examples, tokenizer), batched=True, remove_columns=[""nl_statement"", ""formal_statement""])
    ## C4
    # train_dataset = load_dataset(path='allenai/c4', name='en', split='train', streaming=True)
    # eval_dataset = load_dataset(path='allenai/c4', name='en', split='validation', streaming=True)
    # train_dataset = raw_ds_2_lm_ds_mask_eos_pad_toks(train_dataset, tokenizer, max_length)
    # eval_dataset = raw_ds_2_lm_ds_mask_eos_pad_toks(eval_dataset, tokenizer, max_length)

    # json files for putnam are not consistent and it seems they have to be: https://chatgpt.com/c/9cecca7d-d50d-42e2-b2d3-c1057bc21ef2 solve later
    # ~/putnam-math/data/Putnam_MATH_variations_static3/original/test
    # json_files = glob.glob(os.path.expanduser('~/putnam-math/data/Putnam_MATH_original_static3/test/**/*.json'), recursive=True)
    # train_dataset = load_dataset('json', data_files=json_files)
    # json_files = glob.glob(os.path.expanduser('~/putnam-math/data/Putnam_MATH_variations_static3/variations/test/**/*.json'), recursive=True)
    # eval_dataset = load_dataset('json', data_files=json_files)
    # train_dataset = raw_ds_2_lm_ds_mask_eos_pad_toks(train_dataset, tokenizer, max_length)
    # eval_dataset = raw_ds_2_lm_ds_mask_eos_pad_toks(eval_dataset, tokenizer, max_length)

    # Proofnet with 1st eos token train remaining eos not train
    from train.utils import raw_str_2_desired_af_str
    _raw_str_2_desired_af_str = lambda examples: raw_str_2_desired_af_str(examples, tokenizer)  # tokenizer needed to get eos tok to form right str to train on.
    train_dataset = load_dataset(path, split='validation')
    eval_dataset = load_dataset(path, split='test')
    train_dataset = raw_ds_2_lm_ds_mask_eos_pad_toks(train_dataset, tokenizer, max_length, raw_str_2_desired_str=_raw_str_2_desired_af_str)
    eval_dataset = train_dataset
    print(f'->{len(train_dataset)=} {len(eval_dataset)=}')
    # max_steps: int = (len(train_dataset) * num_train_epochs) // per_device_train_batch_size  # TODO: really?

    # Training arguments
    output_dir_train: Path = Path(output_dir_train).expanduser()
    output_dir_train.mkdir(parents=True, exist_ok=True)
    training_args = TrainingArguments(
        output_dir=output_dir_train,
        max_steps=2,  # TODO get rid of this in favour of 1 or 2 or 3 epochs
        # num_train_epochs=num_train_epochs, 
        gradient_accumulation_steps=gradient_accumulation_steps,  # based on alpaca https://github.com/tatsu-lab/stanford_alpaca, allows to process effective_batch_size = gradient_accumulation_steps * batch_size, num its to accumulate before opt update step
        gradient_checkpointing = gradient_checkpointing,  # TODO depending on hardware set to true?
        per_device_train_batch_size=per_device_train_batch_size,
        per_device_eval_batch_size=per_device_eval_batch_size,
        learning_rate=learning_rate,
        weight_decay=weight_decay, 
        max_grad_norm=max_grad_norm, # TODO once real training change?
        lr_scheduler_type=lr_scheduler_type,  # TODO once real training change? using what I've seen most in vision 
        warmup_ratio=warmup_ratio,
        optim=optim,
        # logging_strategy='epoch', # TODO
        save_steps=100, # Save checkpoint every 500 steps
        save_total_limit=3, # save last 3
        logging_steps=10,  # Frequency of logging steps
        logging_first_step=True,
        logging_dir=output_dir_train,
        evaluation_strategy='no',  # ""no""`: No evaluation is done during training. no can be good to avoid memory issues.
        # evaluation_strategy=""steps"",  # TODO Evaluate model at specified steps
        # eval_steps=110,  # TODO Evaluate every 100 steps
        # remove_unused_columns=False,  # TODO https://stackoverflow.com/questions/76879872/how-to-use-huggingface-hf-trainer-train-with-custom-collate-function/76929999#76929999 , https://claude.ai/chat/475a4638-cee3-4ce0-af64-c8b8d1dc0d90
        report_to=report_to,  # options I recommend: 'none', 'wandb'
        fp16=False,  # never ever set to True
        bf16=torch.cuda.is_bf16_supported(),
        # full_determinism=True,  # TODO periphery, Ensure reproducibility
        # torchdynamo=""nvfuser"",  # TODO periphery, Use NVFuser backend for optimized torch operations
        # dataloader_prefetch_factor=2,  # TODO periphery, Number of batches to prefetch
        # dataloader_pin_memory=True,  # TODO periphery, Pin memory in data loaders for faster transfer to GPU
        # dataloader_num_workers=16,  # TODO Number of subprocesses for data loading
    )

    # Initialize the Trainer 
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset,
        eval_dataset=eval_dataset,  # set to None if eval is giving you memory issues
        tokenizer=tokenizer,
        compute_metrics=compute_metrics
    )
    # Train the model
    trainer.train()

    # Evaluate the model
    if output_dir_test is not None:
        output_dir_test: Path = Path(output_dir_test).expanduser()
        output_dir_test.mkdir(parents=True, exist_ok=True)
        eval_args = TrainingArguments(output_dir=output_dir_test, per_device_eval_batch_size=per_device_eval_batch_size, fp16=False, bf16=torch.cuda.is_bf16_supported(), report_to=report_to)
        trainer = Trainer(model=model, args=eval_args, train_dataset=None, eval_dataset=eval_dataset)
        # results: dict[str, float] = trainer.evaluate(test_dataset)
        results: dict[str, float] = eval_hf(trainer, name='', path=path, split='test')
        print(f'{path=} split=test {results=}')

    # Save the trained model
    if path_to_save_model is not None:
        model.save_pretrained(path_to_save_model)

def main() -> None:
    """"""
    Main function to execute the model training and evaluation.
    """"""
    setup_and_train_proofnet()

if __name__ == ""__main__"":
    import time
    start_time = time.time()
    main()
    print(f""Time taken: {time.time() - start_time:.2f} seconds, or {(time.time() - start_time) / 60:.2f} minutes, or {(time.time() - start_time) / 3600:.2f} hours.\a"")

code repo: https://github.com/brando90/snap-cluster-setup/blob/9778140d8eb378f7c7873ec3fa906d0b01064031/py_src/train/simple_train2.py#L1

OK I think this is the code that train on first occurrence of eos and makes sure the rest are NOT trained on (feedback welcomed):
def collate_fn_train_only_first_eos_token_mask_everything_after_it(data: list[dict[str, str]], 
                                                                    tokenizer: PreTrainedTokenizer, 
                                                                    max_length: int=1024,  # GPT2 default, likely worth you change it! This default might cause bugs.
                                                                    ) -> dict[str, torch.Tensor]:
    """""" Train only on first occurence of eos. The remaining eos are masked out.

    Sometimes the model might not have a padding token. Sometimes people set the padding token to be the eos token.
    But sometimes this seems to lead to the model to predict eos token to much. 
    So instead of actually using the pad token that was set to the eos token, we instead mask out all excesive eos tokens that act as pads 
    and leave the first eos token at the end to be predicted -- since that is the only one that semantically means end of sequence 
    and therby by not training on random eos at the end by masking it not unncesserily shift/amplify the distribution of eos. 
    
    ref: https://discuss.huggingface.co/t/why-does-the-falcon-qlora-tutorial-code-use-eos-token-as-pad-token/45954/13?u=brando 
    ref: https://chat.openai.com/share/02d16770-a1f3-4bf4-8fc2-464286daa8a1
    ref: https://claude.ai/chat/80565d1f-ece3-4fad-87df-364ce57aec15 on when to call .clone()
    """"""
    # we are training full context length for llama so remove code bellow, if it tries to pad hopefully it throws an error
    # -- Ensure tokenizer has a padding token
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    # -- Extract sequences
    # sequences: list[str] = [example.get(""text"", """") or """" for example in data]
    sequences: list[str] = []
    for idx, example in enumerate(data):
        # Retrieve the value for ""text"" from the dictionary or default to an empty string if not present or falsy. ref: https://chat.openai.com/share/bead51fe-2acf-4f05-b8f7-b849134bbfd4
        text: str = example.get(""text"", """") or """"
        sequences.append(text)
    # -- Tokenize the sequences
    tokenized_data = tokenizer(sequences, padding=""max_length"", max_length=max_length, truncation=True, return_tensors=""pt"")
    tokenized_data[""labels""] = tokenized_data[""input_ids""].clone()  # labels is hardcoded in HF so put it!
    # -- Set the mask value for the first eos_token in each sequence to 1 and remaining to -100
    eos_token_id = tokenizer.eos_token_id
    for idx, input_ids in enumerate(tokenized_data[""input_ids""]):
        # Find all occurrences of eos_token
        eos_positions = (input_ids == eos_token_id).nonzero(as_tuple=True)[0]
        if eos_positions.nelement() > 0:  # Check if eos_token is present
            first_eos_position = eos_positions[0]
            tokenized_data[""attention_mask""][idx, first_eos_position] = 1  # Set the mask value to 1
            
            # Assert that the label for the first occurrence of eos_token is eos_token_id
            assert tokenized_data[""labels""][idx, first_eos_position] == eos_token_id, ""The label for the first eos_token is incorrect!""
            
            # For all subsequent occurrences of eos_token, set their labels to -100
            for subsequent_eos_position in eos_positions[1:]:
                tokenized_data[""labels""][idx, subsequent_eos_position] = -100
                assert tokenized_data[""labels""][idx, subsequent_eos_position] == -100, ""The label for the subsequent_eos_position incorrect! Should be -100.""
    return tokenized_data

reference: Why does the falcon QLoRA tutorial code use eos_token as pad_token?
",huggingface-transformers
connectionerror when initializing bm25assembler in dbgpt,"I'm trying to set up a simple example in DB-GPT which uses Elasticsearch as the vector store backend. This is part of the knowledge base initialization process where BM25Assembler is used for document retrieval and ranking.
I have run DB-GPT with Ollama and Elasticsearch is deployed using Docker for both. Everything is fine as below


I'm encountering a ConnectionError while setting up a knowledge base in DB-GPT using BM25Assembler. The error occurs during the initialization of the assembler. After run python examples/rag/bm25_retriever_example.py
Traceback (most recent call last):
  File ""/media/manhdt4/sda1/miniconda3/envs/dbgpt/lib/python3.10/site-packages/urllib3/connectionpool.py"", line 789, in urlopen
    response = self._make_request(
  File ""/media/manhdt4/sda1/miniconda3/envs/dbgpt/lib/python3.10/site-packages/urllib3/connectionpool.py"", line 536, in _make_request
    response = conn.getresponse()
  File ""/media/manhdt4/sda1/miniconda3/envs/dbgpt/lib/python3.10/site-packages/urllib3/connection.py"", line 507, in getresponse
    httplib_response = super().getresponse()
  File ""/media/manhdt4/sda1/miniconda3/envs/dbgpt/lib/python3.10/http/client.py"", line 1375, in getresponse
    response.begin()
  File ""/media/manhdt4/sda1/miniconda3/envs/dbgpt/lib/python3.10/http/client.py"", line 318, in begin
    version, status, reason = self._read_status()
  File ""/media/manhdt4/sda1/miniconda3/envs/dbgpt/lib/python3.10/http/client.py"", line 287, in _read_status
    raise RemoteDisconnected(""Remote end closed connection without""
http.client.RemoteDisconnected: Remote end closed connection without response

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/media/manhdt4/sda1/miniconda3/envs/dbgpt/lib/python3.10/site-packages/elastic_transport/_node/_http_urllib3.py"", line 167, in perform_request
    response = self.pool.urlopen(
  File ""/media/manhdt4/sda1/miniconda3/envs/dbgpt/lib/python3.10/site-packages/urllib3/connectionpool.py"", line 843, in urlopen
    retries = retries.increment(
  File ""/media/manhdt4/sda1/miniconda3/envs/dbgpt/lib/python3.10/site-packages/urllib3/util/retry.py"", line 449, in increment
    raise reraise(type(error), error, _stacktrace)
  File ""/media/manhdt4/sda1/miniconda3/envs/dbgpt/lib/python3.10/site-packages/urllib3/util/util.py"", line 38, in reraise
    raise value.with_traceback(tb)
  File ""/media/manhdt4/sda1/miniconda3/envs/dbgpt/lib/python3.10/site-packages/urllib3/connectionpool.py"", line 789, in urlopen
    response = self._make_request(
  File ""/media/manhdt4/sda1/miniconda3/envs/dbgpt/lib/python3.10/site-packages/urllib3/connectionpool.py"", line 536, in _make_request
    response = conn.getresponse()
  File ""/media/manhdt4/sda1/miniconda3/envs/dbgpt/lib/python3.10/site-packages/urllib3/connection.py"", line 507, in getresponse
    httplib_response = super().getresponse()
  File ""/media/manhdt4/sda1/miniconda3/envs/dbgpt/lib/python3.10/http/client.py"", line 1375, in getresponse
    response.begin()
  File ""/media/manhdt4/sda1/miniconda3/envs/dbgpt/lib/python3.10/http/client.py"", line 318, in begin
    version, status, reason = self._read_status()
  File ""/media/manhdt4/sda1/miniconda3/envs/dbgpt/lib/python3.10/http/client.py"", line 287, in _read_status
    raise RemoteDisconnected(""Remote end closed connection without""
urllib3.exceptions.ProtocolError: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File ""/media/manhdt4/sda1/db-gpt/DB-GPT/examples/rag/bm25_retriever_example.py"", line 50, in <module>
    asyncio.run(main())
  File ""/media/manhdt4/sda1/miniconda3/envs/dbgpt/lib/python3.10/asyncio/runners.py"", line 44, in run
    return loop.run_until_complete(main)
  File ""/media/manhdt4/sda1/miniconda3/envs/dbgpt/lib/python3.10/asyncio/base_events.py"", line 649, in run_until_complete
    return future.result()
  File ""/media/manhdt4/sda1/db-gpt/DB-GPT/examples/rag/bm25_retriever_example.py"", line 37, in main
    assembler = BM25Assembler.load_from_knowledge(
  File ""/media/manhdt4/sda1/db-gpt/DB-GPT/dbgpt/rag/assembler/bm25.py"", line 144, in load_from_knowledge
    return cls(
  File ""/media/manhdt4/sda1/db-gpt/DB-GPT/dbgpt/rag/assembler/bm25.py"", line 110, in __init__
    if not self._es_client.indices.exists(index=self._index_name):
  File ""/media/manhdt4/sda1/miniconda3/envs/dbgpt/lib/python3.10/site-packages/elasticsearch/_sync/client/utils.py"", line 446, in wrapped
    return api(*args, **kwargs)
  File ""/media/manhdt4/sda1/miniconda3/envs/dbgpt/lib/python3.10/site-packages/elasticsearch/_sync/client/indices.py"", line 1227, in exists
    return self.perform_request(  # type: ignore[return-value]
  File ""/media/manhdt4/sda1/miniconda3/envs/dbgpt/lib/python3.10/site-packages/elasticsearch/_sync/client/_base.py"", line 423, in perform_request
    return self._client.perform_request(
  File ""/media/manhdt4/sda1/miniconda3/envs/dbgpt/lib/python3.10/site-packages/elasticsearch/_sync/client/_base.py"", line 271, in perform_request
    response = self._perform_request(
  File ""/media/manhdt4/sda1/miniconda3/envs/dbgpt/lib/python3.10/site-packages/elasticsearch/_sync/client/_base.py"", line 316, in _perform_request
    meta, resp_body = self.transport.perform_request(
  File ""/media/manhdt4/sda1/miniconda3/envs/dbgpt/lib/python3.10/site-packages/elastic_transport/_transport.py"", line 342, in perform_request
    resp = node.perform_request(
  File ""/media/manhdt4/sda1/miniconda3/envs/dbgpt/lib/python3.10/site-packages/elastic_transport/_node/_http_urllib3.py"", line 202, in perform_request
    raise err from e
elastic_transport.ConnectionError: Connection error caused by: ConnectionError(Connection error caused by: ProtocolError(('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))))

Here is code in simple example related to error:
    # create bm25 assembler
    assembler = BM25Assembler.load_from_knowledge(
        knowledge=knowledge,
        es_config=es_config,
        chunk_parameters=chunk_parameters,
    )

Config:
def _create_es_config():
    """"""Create vector connector.""""""
    return ElasticsearchVectorConfig(
        name=""bm25_es_dbgpt"",
        uri=""localhost"",
        port=""9200"",
        user=""elastic"",
        password=""changeme"",
    )

What I've Tried

Verified Elasticsearch container is running properly
Checked Elasticsearch Docker logs
Verified Ollama is running correctly
I have checked connection to ELK. It' ok. No error

from elasticsearch import Elasticsearch

es = Elasticsearch(['http://localhost:9200'], basic_auth=('elastic', 'changeme'))

What's causing this connection error in the DB-GPT context? I only want run a simple example.
I'm sorry, I can't assign this issue to db-gpt tag because it doesn't exist.
","Because I build ELK with Docker and have enabled SSL/TLS
   environment:
     - node.name=es01
     - cluster.name=${CLUSTER_NAME}
     - discovery.type=single-node
     - ELASTIC_PASSWORD=${ELASTIC_PASSWORD}
     - bootstrap.memory_lock=true
     - xpack.security.enabled=true
     - xpack.security.http.ssl.enabled=true
     - xpack.security.http.ssl.key=certs/es01/es01.key
     - xpack.security.http.ssl.certificate=certs/es01/es01.crt
     - xpack.security.http.ssl.certificate_authorities=certs/ca/ca.crt
     - xpack.security.transport.ssl.enabled=true
     - xpack.security.transport.ssl.key=certs/es01/es01.key
     - xpack.security.transport.ssl.certificate=certs/es01/es01.crt
     - xpack.security.transport.ssl.certificate_authorities=certs/ca/ca.crt
     - xpack.security.transport.ssl.verification_mode=certificate
     - xpack.license.self_generated.type=${LICENSE}

While BM25Assembler of DB_GPT does not support connecting to ELK with ssl=true. Here are some of my changes to successfully connect to ELK

bm25_retriever_example.py ElasticsearchVectorConfig

def _create_es_config():
    """"""Create vector connector.""""""
    return ElasticsearchVectorConfig(
        name=""bm25_es_dbgpt"",
        url=""127.0.0.1"",
        port=""9200"",
        user=""elastic"",
        password=""changeme"",
        ca_certs=""/path/to/cert/ca.crt"",
    )


__init__ method of class BM25Assembler(BaseAssembler). The code was extracted from library, the parts with comments are my modifications

...
...
        self._es_config = es_config
        self._es_url = es_config.uri
        self._es_port = es_config.port
        self._es_username = es_config.user
        self._es_password = es_config.password
        self._index_name = es_config.name
        self._k1 = k1
        self._b = b
        self._ca_certs = es_config.ca_certs # my changes
        if self._es_username and self._es_password and self._ca_certs: # my changes
            self._es_client = Elasticsearch( # my changes
                hosts=f""https://{self._es_url}:{self._es_port}"", # my changes
                basic_auth=(self._es_username, self._es_password), # my changes
                verify_certs=True, # my changes
                ca_certs=self._ca_certs # my changes
                
            )          
        elif self._es_username and self._es_password and not self._ca_certs: # my changes
            self._es_client = Elasticsearch(
                hosts=[f""http://{self._es_url}:{self._es_port}""],
                basic_auth=(self._es_username, self._es_password),
            )          
        else:
            self._es_client = Elasticsearch(
                hosts=[f""http://{self._es_url}:{self._es_port}""],
            )
...
...

",llm
unable to send a local image that isn39t under wwwroot to openai using its new net library,"I have an ASP.NET Core 6 Web API with the new official library from OpenAI (https://github.com/openai/openai-dotnet).
What I'm trying to do, is to use a local image file to OpenAI. The file isn't under wwwroot, but under backend/assets/1.jpg.

I've written a basic service to setup all the information needed in order to send a request to OpenAI. But the problem is that I'm not able to send the image.
I keep getting errors like ""url is too long"" or ""invalid image"",
Here is my code - OpenAiService:
using OpenAI.Chat;

namespace backend.Services
{
    public class OpenAiService
    {
        private readonly ChatClient _chatClient;
        private readonly ChatCompletionOptions _options;

        public OpenAiService(IConfiguration configuration)
        {
            var apiKey = configuration.GetValue<string>(""OpenAI:Key"");
            _chatClient = new ChatClient(""gpt-4o"", apiKey);

            _options = new ChatCompletionOptions()
            {
                MaxTokens = 300,
            };
        }

        public async Task<string> ExtractListOfItems()
        {
            var imagePath = Path.Combine(Directory.GetCurrentDirectory(), ""Assets"", ""1.jpg"");
            var localUrl = $""https://localhost:7068/assets/{Path.GetFileName(imagePath)}"";
            
            var messages = new List<ChatMessage>
            {
                new UserChatMessage(new List<ChatMessageContentPart>
                {
                    ChatMessageContentPart.CreateTextMessageContentPart(""Extract the items from the following image and return a list of items including prices and amount.""),
                    ChatMessageContentPart.CreateImageMessageContentPart(new Uri(localUrl))
                })
            };

            var completion = await _chatClient.CompleteChatAsync(messages, _options);
            return completion.Value.ToString();
        }
    }
}

Demo controller for testing:
using Microsoft.AspNetCore.Mvc;
using System.Threading.Tasks;
using backend.Services;
using OpenAI;
using OpenAI.Chat;

namespace backend.Controllers;

[ApiController]
[Route(""[controller]"")]
public class OpenAiDemoController : ControllerBase
{
    private readonly OpenAiService _openAiService;

    public OpenAiDemoController(OpenAiService openAiService)
    {
        _openAiService = openAiService;
    }
    
    [HttpPost]
    [Route(""extract-items"")]
    public async Task<IActionResult> CompleteSentence()
    {
        var completion = await _openAiService.ExtractListOfItems();
        return Ok(completion);
    }
}

program.cs file:
using backend.Configurations;
using backend.Services;
using Microsoft.Extensions.FileProviders;

var builder = WebApplication.CreateBuilder(args);

// Add services to the container.

builder.Services.Configure<OpenAiConfig>(builder.Configuration.GetSection(""OpenAI""));

//add services
builder.Services.AddSingleton<OpenAiService>();

builder.Services.AddControllers();
builder.Services.AddEndpointsApiExplorer();
builder.Services.AddSwaggerGen();

// builder.Services.AddScoped<IOpenAiService, OpenAiService>();

builder.Services.AddCors(opt =>
{
    opt.AddPolicy(""AllowAll"", builder =>
    {
        builder.AllowAnyOrigin()
            .AllowAnyMethod()
            .AllowAnyHeader();
    });
});

var app = builder.Build();

app.UseStaticFiles(new StaticFileOptions
{
    FileProvider = new PhysicalFileProvider(
        Path.Combine(builder.Environment.ContentRootPath, ""Assets"")),
    RequestPath = ""/assets""
});

app.UseStaticFiles(); // This serves files from wwwroot

app.UseStaticFiles(new StaticFileOptions
{
    FileProvider = new PhysicalFileProvider(
        Path.Combine(builder.Environment.ContentRootPath, ""Assets"")),
    RequestPath = ""/assets""
});

// Configure the HTTP request pipeline.
if (app.Environment.IsDevelopment())
{
    app.UseSwagger();
    app.UseSwaggerUI();
}

app.UseHttpsRedirection();
app.UseCors(""AllowAll"");
app.UseAuthorization();

app.MapControllers();

app.Run();

Any idea what I'm doing wrong?
","I was able to solve it on my own.
There's an example in openai official repo that helped me to solve it.
https://github.com/openai/openai-dotnet/blob/main/examples/Chat/Example05_ChatWithVisionAsync.cs

Tho, im not sure if my implementation is mostly correct. I will leave this open for any other suggestions.
service:
using OpenAI.Chat;


namespace backend.Services
{
    public class OpenAiService
    {
        private readonly ChatClient _chatClient;
        private readonly ChatCompletionOptions _options;

        public OpenAiService(IConfiguration configuration)
        {
            var apiKey = configuration.GetValue<string>(""OpenAI:Key"");
            _chatClient = new ChatClient(""gpt-4o"", apiKey);
            _options = new ChatCompletionOptions()
            {
                MaxTokens = 300,
            };
        }

        public async Task<string> ExtractListOfItems()
        {
            var imageFilePath = Path.Combine(""Assets"", ""1.jpg"");
            await using Stream imageStream = File.OpenRead(imageFilePath);
            var imageBytes = BinaryData.FromStream(imageStream);

            var messages = new List<ChatMessage>
            {
                new UserChatMessage(new List<ChatMessageContentPart>
                {
                    ChatMessageContentPart.CreateTextMessageContentPart(""describe the image. ""),
                    ChatMessageContentPart.CreateImageMessageContentPart(imageBytes, ""image/png"")
                })
            };

            var completion = await _chatClient.CompleteChatAsync(messages, _options);
            return completion.Value.ToString();
        }
        
    }
}

usage in controller:
using Microsoft.AspNetCore.Mvc;
using System.Threading.Tasks;
using backend.Services;
using OpenAI;
using OpenAI.Chat;

namespace backend.Controllers;

[ApiController]
[Route(""[controller]"")]
public class OpenAiDemoController : ControllerBase
{
    private readonly OpenAiService _openAiService;

    public OpenAiDemoController(OpenAiService openAiService)
    {
        _openAiService = openAiService;
    }
    
    [HttpPost]
    [Route(""extract-items"")]
    public async Task<IActionResult> CompleteSentence()
    {
        var completion = await _openAiService.ExtractListOfItems();
        return Ok(completion);
    }
    
}

no need to apply for static file middleware.
",chatgpt
debugging dynamic few shot langchain code,"# Import necessary modules and classes from langchain_community and langchain_core
from langchain_community.vectorstores import Chroma
from langchain_core.example_selectors import SemanticSimilarityExampleSelector
from langchain_openai import OpenAIEmbeddings   ## paid
from langchain.embeddings import HuggingFaceEmbeddings   #free


## setup vector db
# Create an instance of Chroma vector store
vectorstore = Chroma()

# Delete any existing collection in the vector store
# vectorstore.delete_collection()

# Create a SemanticSimilarityExampleSelector instance using examples, OpenAI embeddings, and the vector store
example_selector = SemanticSimilarityExampleSelector.from_examples(
    examples,  # List of example queries and inputs
    HuggingFaceEmbeddings(), #HuggingFaceEmbeddings() OpenAIEmbeddings() # OpenAI Embeddings for generating vector representations
    vectorstore,  # Chroma vector store for storing and querying vector representations
    k=2,  # Number of similar examples to retrieve
    input_keys=[""input""],  # Define the input keys to consider for semantic similarity
)

This is my code, its showing an error ""name 'examples' is not defined"". How to solve it ?
I tried to debug, but i failed. Could anyone hepl me out.
","Does examples get defined? Here's an example of SemanticSimilarityExampleSelector.from_examples
example_prompt = PromptTemplate(
    input_variables=[""input"", ""output""],
    template=""Input: {input}\nOutput: {output}"",
)

# Examples of a pretend task of creating antonyms.
examples = [
    {""input"": ""happy"", ""output"": ""sad""},
    {""input"": ""tall"", ""output"": ""short""},
    {""input"": ""energetic"", ""output"": ""lethargic""},
    {""input"": ""sunny"", ""output"": ""gloomy""},
    {""input"": ""windy"", ""output"": ""calm""},
]

example_selector = SemanticSimilarityExampleSelector.from_examples(
    # The list of examples available to select from.
    examples,
    # The embedding class used to produce embeddings which are used to measure semantic similarity.
    OpenAIEmbeddings(),
    # The VectorStore class that is used to store the embeddings and do a similarity search over.
    Chroma,
    # The number of examples to produce.
    k=1,
)
similar_prompt = FewShotPromptTemplate(
    # We provide an ExampleSelector instead of examples.
    example_selector=example_selector,
    example_prompt=example_prompt,
    prefix=""Give the antonym of every input"",
    suffix=""Input: {adjective}\nOutput:"",
    input_variables=[""adjective""],
)

Take a look at the reference > https://python.langchain.com/v0.2/docs/how_to/example_selectors_mmr/
",llm
is there any way to create prompt with two input fields,"Is there any way to create prompt in JavaScript with two input fields ?  
I tried that code, but it didn't help me :
var a = prompt(""A : "", """");
var b = prompt(""B : "", """");
alert(a + ""\n"" + b);

","This is not possible with an OS or native browser window popping up. You will have to create a custom overlay dialog.
I would advise using a library like jQuery UI to do this. You can then customize whatever is in the popup.
You can view a demo of the dialog here
",prompt
trying and failing to create a purely javascript calculator,"First post ever, after lurking for some weeks. I'm currently attending a full-stack bootcamp and recently we got into Javascript (so I'm extremely green, please be patient...I'm trying to reskill myself in another industry). One of the HLTs that is giving me a particular headache is as follows:

You are tasked with creating re-usable methods that can be used throughout the business. The business needs you to create methods for mathematics operations. Follow the instructions below:

Ask the user for a number with a prompt() and store the value in a variable called firstValue
Ask the user for a second number with a prompt()and store that value in a variable called secondValue
Ask the user for a third input with prompt()storing the value in a variable called operation. >Expected operations are:
a.+This is the addition symbol, located next to the backspace key(hold shift)
b.â€“This is the subtraction symbol, located next to number 0key (hold shift)
c./This is the division symbol, a forward slash, located next to the full stop key
d.*This is the multiplication symbol, a star, accessed by holding shift and pressing number 8
e.^This is the to-the-power-of symbol, known as a caretin programming, accessed by holding shift and pressing the number 6
Write a method for the 5 operations listed above (one for each operation) that takes in both valuesand returns the result of the operation.a.For examplefunction multiplication(firstValue, secondValue) { return firstValue * secondValue;}
Create a case-switch for evaluating the operation the user supplied, and depending on the value, execute the relevant function.
Print out to consolefirstValue, operator, secondValue, an equal sign, and the answer:
a.2 x 8 = 16
STRETCH CHALLENGE: Wrap the code in a continuous loop that only ends when the user responds to a prompt that asks them â€œwould you like to do another calculation?â€with their answer being â€œnoâ€.
STRETCH CHALLENGE: Change the above code to also include methods for processing sin, cos, and tan. You can use the methodsMath.sin(x), Math.cos(x), Math.tan(x)but be aware thatthe user only needs to supply a single value and the operation they wish to dowhen needing sin, cos, and tan!


I'm stuck even before attempting the stretch challenges (which I have no clue on how to do, but that's a problem for later) and looking online I couldn't find anything helpful (since most calculators employ HTML and CSS as well). Here below my two attempts at making the code work (I made multiple variations of both, trying to find a version that worked, but without any luck). I used some Shakespearean English, just to spice it up and to make it less boring. Also, it's called ""Calculathor"".
First attempt:


//Contemporary English to Shakespearean English translator found at https://lingojam.com/EnglishtoShakespearean
var firstValue = parseFloat(prompt(""Writeth h're thy first numb'r, m'rtal""));//I used parseFloat as I believe it would filter out some typing mistakes (by giving NaN if what's typed is not a number)
var secondValue = parseFloat(prompt(""And h're, prithee writeth thy second numb'r""));
var operator = prompt(""Writeth one of these ancient runes: + - / * ^""); //I changed the subtraction symbol from the assignment to the one I have on my Italian keyboard, which is the same to an hyphen
function operation(firstValue, secondValue){
    switch (operator) {
        case ('+'):
            return firstValue + secondValue;
            break;
        case ('-'):
            return firstValue - secondValue;
            break;
        case ('/'):
            return firstValue / secondValue;
            break;
        case ('*'):
            return firstValue * secondValue;
            break;
        case ('^'):
            return firstValue ^ secondValue;
            break;    
        default:
            alert(""Thee wroteth something inc'rrect, thee clotpole!"");
            break;
    }
}
console.log(`Thee hath asked Thor to solveth ${firstValue} ${operator} ${secondValue} and the solution appears to beest equat'd to ${operation}`);



Second attempt:


//Contemporary English to Shakespearean English translator found at https://lingojam.com/EnglishtoShakespearean
var firstValue = parseFloat(prompt(""Writeth h're thy first numb'r, m'rtal""));//I used parseFloat as I believe it would filter out some typing mistakes (by giving NaN if what's typed is not a number)
var secondValue = parseFloat(prompt(""And h're, prithee writeth thy second numb'r""));
var operator = prompt(""Writeth one of these ancient runes: + - / * ^""); //I changed the subtraction symbol from the assignment to the one I have on my Italian keyboard, which is the same to an hyphen
let result = (`${firstValue} ${operation} ${secondValue}`);
function operation(firstValue, secondValue, operator){
    switch (operator) {
        case ('+'):
            return result (firstValue + secondValue);
        case ('-'):
            return result (firstValue - secondValue);
        case ('/'):
            return result (firstValue / secondValue);
        case ('*'):
            return result (firstValue * secondValue);
        case ('^'):
            return result (firstValue ^ secondValue);
        default:
            alert(""Thee wroteth something inc'rrect, thee clotpole!"");
            break;
    }
}
console.log(`Thee hath asked Thor to solveth ${firstValue} ${operator} ${secondValue} and the solution appears to beest equat'd to ${result}`);



I know this must be something very stupid for most of you, but for me it's still pretty hard to try and understand what I'm doing wrong, without any guidance. Please help me, if you can! I've wasted already more than 2 days trying to understand what I'm getting wrong. :(
","The OP's code only mentioned the operation function, failing to invoke it. This modification (and not-at-all-time-wasting explanation) invokes operation inside the interpolated string...
operation(firstValue, operator, secondValue)

The complete code:
var firstValue = prompt(""Writeth h're thy first numb'r, m'rtal"");
firstValue = parseFloat(firstValue)
var secondValue = prompt(""And h're, prithee writeth thy second numb'r"");
secondValue = parseFloat(secondValue)

var operator = prompt(""Writeth one of these ancient runes: + - / * ^"");

function operation(firstValue, operator, secondValue){
let res;
switch (operator) {
    case ('+'):
        res=  firstValue + secondValue;
        break;
    case ('-'):
        res= firstValue - secondValue;
        break;
    case ('/'):
        res= firstValue / secondValue;
        break;
    case ('*'):
        res= firstValue * secondValue;
        break;
    case ('^'):
        res= firstValue ^ secondValue;
        break;    
    default:
        alert(""Thee wroteth something inc'rrect, thee clotpole!"");
        break;
}

return res;
}

console.log(`Thee hath asked Thor to solveth ${firstValue} ${operator} ${secondValue} and the solution appears to beest equat'd to ${operation(firstValue, operator, secondValue)}`);`

",prompt
using langchain for text to sql using custom llm api,"I am trying to use my llama2 model (exposed as an API using ollama). I want to chat with the llama agent and query my Postgres db (i.e. generate text to sql). I was able to find langchain code that uses open AI to do this. However, I am unable to find anything out there which fits my situation.
Any pointers will be of great help.
Code with openai
# Create connection to postgres
import psycopg2  # Import the library

database = 'postgres'
username = 'postgres'
password = 'password'
server = 'localhost'
port = '5432'

# Establish the connection
conn = psycopg2.connect(
    dbname=database,
    user=username,
    password=password,
    host=server,
    port=port
)

db = SQLDatabase.from_uri(
    ""postgresql://postgres:password@localhost:5432/postgres"")
toolkit = SQLDatabaseToolkit(db=db, llm=OpenAI(temperature=0))

agent_executor = create_sql_agent(
    llm=OpenAI(temperature=0),
    toolkit=toolkit,
    verbose=True,
    agent_type=AgentType.ZERO_SHOT_REACT_DESCRIPTION,
)

agent_executor.run(""Describe the transaction table"")

I want to make the above code work for my llama2 model exposed via an API at localhost:11434/api/generate
","Load your llm like mentioned here https://python.langchain.com/docs/integrations/llms/ollama
and then use that inplace of openai. You'll most probably have to change the prompts to fit llama2 desired format
",llama
how to parse answer  content  from openai response to markdown format,"I have a Next.js action that uses OpenAI to generate an answer :
export async function continueConversation(
  messages: CoreMessage[],
  data: PersonResponse,
  prompt: string
) {
  const { full_name, job_title, ...rest } = data
  const result = await streamText({
    system: `You are a ${job_title} and your name is ${full_name}. Answer the questions based on ${JSON.stringify(
      rest
    )}`,
    model: openai(""gpt-4o-2024-05-13""),
    messages,
    prompt,
  })

  const stream = createStreamableValue(result.textStream)
  return stream.value
}


In the UI, I have this part of code:
{messages.map((m, i) => (
              <div key={i} className=""whitespace-pre-wrap flex flex-col px-8"">
                <div
                  className={cn(
                    ""flex items-start"",
                    m.role === ""user"" && ""justify-end""
                  )}
                >
                  {m.role === ""assistant"" && (
                    <Avatar className=""w-8 h-8 mr-3 mt-1"">
                      <AvatarImage
                        src={`https://${github_url}.png`}
                        alt={getInitials(full_name as string)}
                      />
                      <AvatarFallback className=""text-sm font-semibold tracking-[2px] bg-gradient-to-r from-[#ffaa40] via-[#9c40ff] to-[#ffaa40]"">
                        {getInitials(full_name as string)}
                      </AvatarFallback>
                    </Avatar>
                  )}
                  <span
                    className={cn(
                      ""my-2"",
                      m.role === ""user"" &&
                        ""bg-neutral-800 px-4 py-3 rounded-full""
                    )}
                  >
                    {m.content as string}
                  </span>
                </div>
              </div>
            ))}


In the output, I get the result as Markdown:

","You can use react-markdown package to display the markdown and the package size is only 45.4kb
npm install react-markdown
import Markdown from ""react-markdown""

export default function Home(){
  // api calls here

  return (
   {messages.map((m, i) => (
      <div key={i} className=""whitespace-pre-wrap flex flex-col px-8"">
           <div>
             {m.role === ""assistant"" && (
              <Avatar >
                <AvatarImage
                  src={`https://${github_url}.png`}
                  alt={getInitials(full_name as string)}
                />
               <AvatarFallback className=""text-sm"">
                {getInitials(full_name as string)}
               </AvatarFallback>
             </Avatar>
             )}
             <span
              className={cn(
               ""my-2"",
              m.role === ""user"" &&
              ""bg-neutral-800 px-4 py-3 rounded-full""
            )}
            >
              <Markdown>{m.content}</Markdown>
          </span>
         </div>
      </div>
   ))}
  )
}

",chatgpt
doubts in the use of int variables,"I'm trying to do something really stupid, but maybe it will be my careless mistake ...
In short, how is it possible that in a simple function like this:
function limitFunc(name){
  var domain=prompt(`Inserisci il dominio della funzione`);
  var sx=domain-0.2;
  var dx=domain+0.2;
  console.log(`sx`,sx, `dx:`,dx);
  //console.table(graph.limitCalculation(name, domain));
}

If I then write 1 with the prompt, the dx variable then calculated gives me a number multiplied by 10.
For example if I write 1:
chrome console
Thanks for the help anyway
","prompt returns a string.
All you need to do is convert your string into a number, basically:


function limitFunc(name) {
    let domain = prompt(`Inserisci il dominio della funzione`);
    domain = +domain; // converted string into number
    const sx = domain - 0.2;
    const dx = domain + 0.2;
    console.log(`sx:`, sx, `dx:`,dx);
}

limitFunc();



Obviously, you'd want to check if the user prompts a valid answer (i.e. a number).
",prompt
unknown document type error while using llamaindex with azure openai,"I'm trying to reproduce the code from documentation: https://docs.llamaindex.ai/en/stable/examples/customization/llms/AzureOpenAI.html and receive the following error after index = VectorStoreIndex.from_documents(documents):
raise ValueError(f""Unknown document type: {type(document)}"")
ValueError: Unknown document type: <class 'llama_index.legacy.schema.Document'>

Due to the fact that all these generative ai libraries are being constantly updated, I have to switch the import of SimpleDirectoryReader and make it like from llama_index.legacy.readers.file.base import SimpleDirectoryReader
All the rest is actually the same with tutorial (using llama_index==0.10.18 and python of version 3.9.16). I have spent already several hours on that and actually don't have ideas how should I proceed. So if somebody can assist with that - it would be super helpful :)
Many thanks in advance.
","The error occurs because of the type of document you are passing for VectorStoreIndex.from_documents().
When you import SimpleDirectoryReader from legacy modules, the type of document is llama_index.legacy.schema.Document.

You are passing that to VectorStoreIndex, which is imported from core modules: from llama_index.core import VectorStoreIndex.
The document you referred to is correct for core modules, and you can import SimpleDirectoryReader as from llama_index.core import VectorStoreIndex, SimpleDirectoryReader, and everything will work fine.
If you wish to use legacy modules, then use the code below.
from llama_index.legacy.llms.azure_openai import AzureOpenAI
from llama_index.legacy.embeddings.azure_openai import AzureOpenAIEmbedding
from llama_index.legacy import SimpleDirectoryReader, VectorStoreIndex, ServiceContext
import logging
import sys

logging.basicConfig(
    stream=sys.stdout, level=logging.INFO
)  # logging.DEBUG for more verbose output
logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))

api_key = ""3c9xxxyyyyzzzzzssssssdb9""
azure_endpoint = ""https://<resource_name>.openai.azure.com/""
api_version = ""2023-07-01-preview""

llm = AzureOpenAI(
    model=""gpt-4"",
    deployment_name=""gpt4"",
    api_key=api_key,
    azure_endpoint=azure_endpoint,
    api_version=api_version,
)

# You need to deploy your own embedding model as well as your own chat completion model
embed_model = AzureOpenAIEmbedding(
    model=""text-embedding-ada-002"",
    deployment_name=""embeding1"",
    api_key=api_key,
    azure_endpoint=azure_endpoint,
    api_version=api_version,
)

documents = SimpleDirectoryReader(input_files=[""./data/s1.txt""]).load_data()
type(documents[0])

service_context = ServiceContext.from_defaults(
    llm=llm, embed_model=embed_model
)

index = VectorStoreIndex.from_documents(documents, service_context=service_context)

Output:
query = ""What is the model name and who updated it last?""
query_engine = index.as_query_engine()
answer = query_engine.query(query)
print(""query was:"", query)
print(""answer was:"", answer)


Here, when using legacy modules, all tools and models should be imported from the same legacy modules, and an additional service context is used for the vector store index.
",llama
vectorstoreindex api key while using azureopenai service,"I'm trying to use VectorStoreIndex from llama_index solving the RAG problem for chatbot just the following way:
import openai
from llama_index import VectorStoreIndex

index = VectorStoreIndex.from_documents(docs)
index.storage_context.persist()

When I read documentation, it is recommended to use openai.api_key = os.getenv('OPENAI_API_KEY') to be able to connect model with OpenAI. But what should I do if I use AzureOpenAI (meaning, that I have api_key, azure_endpoint and api_version and don't have openai api_key)? If I use openai.api_key = my_azure_api_key this also does not work because apparently the model refers to https://platform.openai.com/account/api-keys and not to AzureAI Services..
Sorry if the question looks dubious, was unable to find the information on web.
Thanks!
","Have solved it. Actually all you need is to add context parameter into VectorStoreIndex.from_documents(). In this parameter one can specify AzureOpenAI or AzureOpenAIEmbedding and obviously use api_key, azure_endpoint and api_version.
index = VectorStoreIndex.from_documents(documents, service_context=service_context)

",llama
splitting html file and saving chunks using langchain,"I'm very new to LangChain, and I'm working with around 100-150 HTML files on my local disk that I need to upload to a server for NLP model training. However, I have to divide my information into chunks because each file is only permitted to have a maximum of 20K characters. I'm trying to use the LangChain library to do so, but I'm not being successful in splitting my files into my desired chunks.
For reference, I'm using this URL:  http://www.hadoopadmin.co.in/faq/ Saved locally as HTML only.
It's a Hadoop FAQ page that I've downloaded as an HTML file onto my PC. There are many questions and answers there. I've noticed that sometimes, for some files, it gets split by a mere title, and another split is the paragraph following that title. But my desired output would be to have the title and the specific paragraph or following text from the body of the page, and as metadata, the title of the page.
I'm using this code:
from langchain_community.document_loaders import UnstructuredHTMLLoader
from langchain_text_splitters import HTMLHeaderTextSplitter
# Same Example with the URL http://www.hadoopadmin.co.in/faq/ Saved Locally as HTML Only
dir_html_file='FAQ â€“ BigData.html'

data_html = UnstructuredHTMLLoader(dir_html_file).load()

headers_to_split_on = [
    (""h1"", ""Header 1"")]
html_splitter = HTMLHeaderTextSplitter(headers_to_split_on=headers_to_split_on)
html_header_splits = html_splitter.split_text(str(data_html))

But is returning a bunch of weird characters and not splitting the document at all.
This is an output:
[Document(page_content='[Document(page_content=\'BigData\\n\\n""You can have data without information, but you cannot have information without Big data.""\\n\\nsaurabhmcakiet@gmail.com\\n\\n+91-8147644946\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nToggle navigation\\n\\nHome\\n\\nBigData\\n\\n\\tOverview of BigData\\n\\tSources of BigData\\n\\tPros & Cons of BigData\\n\\tSolutions of BigData\\n\\nHadoop Admin\\n\\n\\tHadoop\\n\\t\\n\\t\\tOverview of HDFS\\n\\t\\tOverview of MapReduce\\n\\t\\tApache YARN\\n\\t\\tHadoop Architecture\\n\\t\\n\\n\\tPlanning of Hadoop Cluster\\n\\tAdministration and Maintenance\\n\\tHadoop Ecosystem\\n\\tSetup HDP cluster from scratch\\n\\tInstallation and Configuration\\n\\tAdvanced Cluster Configuration\\n\\tOverview of Ranger\\n\\tKerberos\\n\\t\\n\\t\\tInstalling kerberos/Configuring the KDC and Enabling Kerberos Security\\n\\t\\tConfigure SPNEGO Authentication for Hadoop\\n\\t\\tDisabled kerberos via ambari\\n\\t\\tCommon issues after Disabling kerberos via Ambari\\n\\t\\tEnable https for ambari Server\\n\\t\\tEnable SSL or HTTPS for Oozie Web UI\\n\\nHadoop Dev\\n\\n\\tSolr\\n\\t\\n\\t\\tSolr Installation\\n\\t\\tCommits and Optimizing in Solr and its use for NRT\\n\\t\\tSolr FAQ\\n\\t\\n\\n\\tApache Kafka\\n\\t\\n\\t\\tKafka QuickStart\\n\\t\\n\\n\\tGet last access time of hdfs files\\n\\tProcess hdfs data with Java\\n\\tProcess hdfs data with Pig\\n\\tProcess hdfs data with Hive\\n\\tProcess hdfs data with Sqoop/Flume\\n\\nBigData Architect\\n\\n\\tSolution Vs Enterprise Vs Technical Architectâ€™s Role and Responsibilities\\n\\tSolution architect certification\\n\\nAbout me\\n\\nFAQ\\n\\nAsk Questions\\n\\nFAQ\\n\\nHome\\n\\nFAQ\\n\\nFrequently\\xa0Asked Questions about Big Data\\n\\nMany questions about big data have yet to be answered in a vendor-neutral way. With so many definitions, opinions run the gamut. Here I will attempt to cut to the heart of the matter by addressing some key questions I often get from readers, clients and industry analysts.\\n\\n1) What is Big Data?\\n\\n1) What is Big Data?\\n\\nBig dataâ€ is an all-inclusive term used to describe vast amounts of information. In contrast to traditional structured data which is typically stored in a relational database, big data varies in terms of volume, velocity, and variety.\\n\\nBig data\\xa0is characteristically generated in large volumes â€“ on the order of terabytes or exabytes of data (starts with 1 and has 18 zeros after it, or 1 million terabytes) per individual data set.\\n\\nBig data\\xa0is also generated with high velocity â€“ it is collected at frequent intervals â€“ which makes it difficult to analyze (though analyzing it rapidly makes it more valuable).\\n\\nOr in simple words we can say â€œBig Data includes data sets whose size is beyond the ability of traditional software tools to capture, manage, and process the data in a reasonable time.â€\\n\\n2) How much data does it take to be called Big Data?\\n\\nThis question cannot be easily answered absolutely. Based on the infrastructure on the market the lower threshold is at about 1 to 3 terabytes.\\n\\nBut using Big Data technologies can be sensible for smaller databases as well, for example if complex mathematiccal or statistical analyses are run against a database. Netezza offers about 200 built in functions and computer languages like Revolution R or Phyton which can be used in such cases.\\n\\

My Expected output will look something like this:
One chunk:

Frequently Asked Questions about Big Data

Many questions about big data have yet to be answered in a vendor-neutral way. With so many definitions, opinions run the gamut. Here I will attempt to cut to the heart of the matter by addressing some key questions I often get from readers, clients and industry analysts.

1) What is Big Data?
â€œBig dataâ€ is an all-inclusive term used to describe vast amounts of information. In contrast to traditional structured data which is typically stored in a relational database, big data varies in terms of volume, velocity, and variety. Big data is characteristically generated in large volumes â€“ on the order of terabytes or exabytes of data (starts with 1 and has 18 zeros after it, or 1 million terabytes) per individual data set. Big data is also generated with high velocity â€“ it is collected at frequent intervals â€“ which makes it difficult to analyze (though analyzing it rapidly makes it more valuable).
Or in simple words we can say â€œBig Data includes data sets whose size is beyond the ability of traditional software tools to capture, manage, and process the data in a reasonable time.â€
2) How much data does it take to be called Big Data?
This question cannot be easily answered absolutely. Based on the infrastructure on the market the lower threshold is at about 1 to 3 terabytes.
But using Big Data technologies can be sensible for smaller databases as well, for example if complex mathematical or statistical analyses are run against a database. Netezza offers about 200 built in functions and computer languages like Revolution R or Phyton which can be used in such cases.

Metadata: FAQ


Another Chunck
7) Where is the big data trend going?
Eventually the big data hype will wear off, but studies show that big data adoption will continue to grow. With a projected $16.9B market by 2015 (Wikibon goes even further to say $50B by 2017), it is clear that big data is here to stay. However, the big data talent pool is lagging behind and will need to catch up to the pace of the market. McKinsey & Company estimated in May 2011 that by 2018, the US alone could face a shortage of 140,000 to 190,000 people with deep analytical skills as well as 1.5 million managers and analysts with the know-how to use the analysis of big data to make effective decisions.
The emergence of big data analytics has permanently altered many businessesâ€™ way of looking at data. Big data can take companies down a long road of staff, technology, and data storage augmentation, but the payoff â€“ rapid insight into never-before-examined data â€“ can be huge. As more use cases come to light over the coming years and technologies mature, big data will undoubtedly reach critical mass and will no longer be labeled a trend. Soon it will simply be another mechanism in the BI ecosystem.
8) Who are some of the BIG DATA users?
From cloud companies like Amazon to healthcare companies to financial firms, it seems as if everyone is developing a strategy to use big data. For example, every mobile phone user has a monthly bill which catalogs every call and every text; processing the sheer volume of that data can be challenging. Software logs, remote sensing technologies, information-sensing mobile devices all pose a challenge in terms of the volumes of data created. The size of Big Data can be relative to the size of the enterprise. For some, it may be hundreds of gigabytes, for others, tens or hundreds of terabytes to cause consideration.
9) Data visualization is becoming more popular than ever.
In my opinion, it is absolutely essential for organizations to embrace interactive data visualization tools. Blame or thank big data for that and these tools are amazing. They are helping employees make sense of the never-ending stream of data hitting them faster than ever. Our brains respond much better to visuals than rows on a spreadsheet.
Companies like Amazon, Apple, Facebook, Google, Twitter, Netflix and many others understand the cardinal need to visualize data. And this goes way beyond Excel charts, graphs or even pivot tables. Companies like Tableau Software have allowed non-technical users to create very interactive and imaginative ways to visually represent information.

Metadata: FAQ  

My thought process is being able to gather all the information and split it into chunks, but I don't want titles without their following paragraphs separated, and I also want as much info as possible (max 20K characters) before creating another chunk.
I would also like to save these chunks and their meta data. Is there a function in LangChain to do this?
I am open to hearing not to do this in LangChain for efficiency reasons.
","check this super amazing HTML chunking package :package:
pip install html_chunking

Our HTML chunking algorithm operates through a well-structured process that involves several key stages, each tailored to efficiently chunk and merge HTML content while adhering to a token limit. This approach is highly suitable for scenarios where token limitations are critical, and the need for accurate HTML parsing is paramount, especially in tasks like web automation or navigation where HTML content serves as input.

For those of you who are interested in this, here's a demo


from html_chunking import get_html_chunks
merged_chunks = get_html_chunks(your_html_string_here, max_tokens=1000, is_clean_html=True, attr_cutoff_len=25)
merged_chunks


The output should consists of several HTML chunks, where each chunk contains valid HTML code with preserved structure and attributes (from root node all the way down to current node), and any excessively long attributes are truncated to the specified length.

Check out the html_chunking PYPI page and our Github page for more example DEMO!!

For those who are investigating the BEST way of chunking HTML for web automation or any other web agent tasks, you should definitely try html_chunking!!

LangChain (HTMLHeaderTextSplitter & HTMLSectionSplitter) and LlamaIndex (HTMLNodeParser) split text at the element level and add metadata for each header relevant to the chunk. However, they extract only the text content and exclude the HTML structure, attributes, and other non-text elements, limiting their use for tasks requiring the full HTML context.

Check our Github repo below and star :star2:
https://github.com/KLGR123/html_chunking


",langchain
error using llmfactory with quottheblokeopenhermes25mistral7bggufquot huggingface,"I tried replicating a simple Python code to create a small LLM model.
I have macOS M1 machine.
I created a separate environment where I installed Pytorch and llama-cpp-python. The code:
from llmflex import LlmFactory

# Load the model from Huggingface
try:
    # Instantiate the model with the correct identifier
    model = LlmFactory(""TheBloke/OpenHermes-2.5-Mistral-7B-GGUF"")

    # Configure parameters directly if the object itself is callable
    #llm = model(temperature=0.7, max_new_tokens=512)

    # Disable Metal and run on CPU
    llm = model(temperature=0.7, max_new_tokens=512, use_metal=False)

    # Generate a response
    response = llm.generate(""Hello, how are you?"")
    print(response)

except AttributeError as e:
    print(f""Attribute error: {e}"")
except AssertionError as e:
    print(f""Assertion error: {e}"")
except Exception as e:
    print(f""An error occurred: {e}"")

As you can see, I tried with and without Metal, but I received the same error (the last portion of the output):
llm_load_vocab: special tokens definition check successful ( 261/32002 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = SPM
llm_load_print_meta: n_vocab          = 32002
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: n_ctx_train      = 32768
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 8
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_gqa            = 4
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: n_ff             = 14336
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 32768
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q2_K
llm_load_print_meta: model params     = 7.24 B
llm_load_print_meta: model size       = 2.87 GiB (3.41 BPW) 
llm_load_print_meta: general.name     = teknium_openhermes-2.5-mistral-7b
llm_load_print_meta: BOS token        = 1 '<s>'
llm_load_print_meta: EOS token        = 32000 '<|im_end|>'
llm_load_print_meta: UNK token        = 0 '<unk>'
llm_load_print_meta: PAD token        = 0 '<unk>'
llm_load_print_meta: LF token         = 13 '<0x0A>'
llm_load_tensors: ggml ctx size =    0.11 MiB
llm_load_tensors: mem required  = 2939.69 MiB

llama_new_context_with_model: n_ctx      = 4096
llama_new_context_with_model: freq_base  = 10000.0
llama_new_context_with_model: freq_scale = 1
llama_new_context_with_model: KV self size  =  512.00 MiB, K (f16):  256.00 MiB, V     (f16):  256.00 MiB
llama_build_graph: non-view tensors processed: 676/676
ggml_metal_init: allocating
ggml_metal_init: found discrete device: Apple M1
ggml_metal_init: picking device: Apple M1
ggml_metal_init: default.metallib not found, loading from source
ggml_metal_init: GGML_METAL_PATH_RESOURCES = nil
ggml_metal_init: error: could not use bundle path to find ggml-metal.metal, falling     back to trying cwd
ggml_metal_init: loading 'ggml-metal.metal'
ggml_metal_init: error: Error Domain=NSCocoaErrorDomain Code=260 ""The file â€œggml-    metal.metalâ€ couldnâ€™t be opened because there is no such file."" UserInfo=.   {NSFilePath=ggml-metal.metal, NSUnderlyingError=0x600002eeb2a0 {Error     Domain=NSPOSIXErrorDomain Code=2 ""No such file or directory""}}
llama_new_context_with_model: ggml_metal_init() failed
AVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 |     NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 =     0 | SSSE3 = 0 | VSX = 0 | 

Assertion error: 

Obviously, something is wrong, but I cannot pinpoint the error because I am new to this.
I do not want to use CUDA; I want to use the CPU.
Please, help
Here is some additional information: https://huggingface.co/TheBloke/OpenHermes-2.5-Mistral-7B-GGUF
","I guess it is an issue when using it with MacOS M1. Metal has some problems that have not been fully resolved. I am closing this, but if there is an answer, please DM me
",llm
how to know which words are encoded with unknown tokens in huggingface berttokenizer,"I use the following code to count how many % of words are encoded to unknown tokens.
paragraph_chinese = '...' # It is a long paragraph from a text file.
from transformers import AutoTokenizer, BertTokenizer
tokenizer_bart = BertTokenizer.from_pretrained(""fnlp/bart-base-chinese"")
encoded_chinese_bart = tokenizer_bart.encode(paragraph_chinese)
unk_token_id_bart = tokenizer_bart.convert_tokens_to_ids([""[UNK]""])
len_paragraph_chinese   = len(paragraph_chinese)

unk_token_cnt_chinese_bart   = encoded_chinese_bart.count(unk_token_id_bart[0])
print(""BART Unknown Token count in Chinese Paragraph:"", unk_token_cnt_chinese_bart, ""("" + str(unk_token_cnt_chinese_bart * 100 / len_paragraph_chinese) + ""%)"")
print(type(tokenizer_bart))

which prints:
BART Unknown Token count in Chinese Paragraph: 1 (0.015938795027095953%)
<class 'transformers.models.bert.tokenization_bert.BertTokenizer'>

My question is: I noticed there is one unknown token. How can I know which word causes this unknown token?
p.s. I tried print(encoded_chinese_bart), but it is a list of Token IDs.
Using transformers 4.28.1
","When you use the BertTokenizerFast instead of the ""slow"" version, you will get a BatchEncoding object that gives you access to several convenient methods that allow you to map a token back to the original string.
The following code uses the token_to_chars method:
from transformers import BertTokenizerFast

# just an example
paragraph_chinese = 'é©¬äº‘ KoÄka ç¥–ç±æµ™æ±ŸçœåµŠåŽ¿ KoÄka çŽ°åµŠå·žå¸‚' 

tokenizer_bart = BertTokenizerFast.from_pretrained(""fnlp/bart-base-chinese"")
encoded_chinese_bart = tokenizer_bart(paragraph_chinese)
unk_token_id_bart = tokenizer_bart.unk_token_id
len_paragraph_chinese   = len(paragraph_chinese)

unk_token_cnt_chinese_bart   = encoded_chinese_bart.input_ids.count(unk_token_id_bart)
print(f'BART Unknown Token count in Chinese Paragraph: {unk_token_cnt_chinese_bart} ({unk_token_cnt_chinese_bart * 100 / len_paragraph_chinese}%)')

#find all indices
unk_indices = [i for i, x in enumerate(encoded_chinese_bart.input_ids) if x == unk_token_id_bart]
for unk_i in unk_indices:
  start, stop = encoded_chinese_bart.token_to_chars(unk_i)
  print(f""At {start}:{stop}: {paragraph_chinese[start:stop]}"")

Original:
BART Unknown Token count in Chinese Paragraph: 2 (7.407407407407407%)
At 3:8: KoÄka
At 17:22: KoÄka

",huggingface-transformers
how to create langfuse trace via net code,"I am developing an AI solution which combine of .Net and Python app. The reason for Python app is mostly to communicate with OpenAI and Langfuse. I want to cut off the Python app to make solution simpler and easier to deploy, but couldn't find a way to write Langfuse trace via its API. Does anyone know what is the endpoint and payload to do it?
","You need to use the ingestion endpoint: https://api.reference.langfuse.com/#post-/api/public/ingestion
You can also download the OpenAPI spec for types at the very top of the reference.
",chatgpt
while loop with prompt is never showing any consolelog,"I was just trying to check if my console gets connected to my code, and previosly it did for the same program, but now itâ€™s not even loading the basic HTML page which contains just the heading, and itâ€™s not showing anything in the console. Why is it behaving like this?



var todos = [ ""whats up dude!!"" ];
var input = prompt(""what would you like to do?"");

while (input !== ""quit"") {
  if (input === ""list"") {
    todos.forEach(function(todo, i) {
      console.log(i + "": "" + todo);
    });
  }
  else if (input === ""new"") {
    var newTodo = prompt(""what do you want?"");
    
    todos.push(newTodo);
  }
  else if (input === ""delete"") {
    var index = prompt(""Enter index of todo to delete"");
    
    todos.splice(index, 1);
    console.log(""Todo Removed"");
  }

  input = prompt(""what would you like to do?"");
}

console.log(""You have Quit!!"");
<!DOCTYPE html>
<html>

<head>
  <title>one more try</title>
  <script type=""text/javascript"" src=""tryy.js""></script>
</head>

<body>
  <h1>its the last resort</h1>
  <h4>hope i win this!!</h4>
</body>

</html>



","Instead of a tight loop that never yields control of the script, or page, to anything else, you can put a small delay before each prompt with setTimeout:


var todos=[""whats up dude!!""];_x000D_
_x000D_
function interactWithToDos()_x000D_
{_x000D_
  var input=prompt(""what would you like to do?"");_x000D_
  _x000D_
  if(input===""list"")_x000D_
  {_x000D_
    todos.forEach(function(todo, i)_x000D_
    {_x000D_
      console.log(i +"": ""+ todo);_x000D_
    });_x000D_
  }_x000D_
  else if(input===""new"")_x000D_
  {_x000D_
    var newTodo=prompt(""what do you want?"");_x000D_
    todos.push(newTodo);_x000D_
  }_x000D_
  else if(input === ""delete"")_x000D_
  {_x000D_
    var index = prompt(""Enter index of todo to delete"");_x000D_
    todos.splice(index, 1);_x000D_
    console.log(""Todo Removed"");_x000D_
  }_x000D_
  _x000D_
  if(input !== ""quit"")_x000D_
  {_x000D_
    setTimeout(interactWithToDos, 0);_x000D_
  }_x000D_
  else_x000D_
  {_x000D_
    console.log(""You have Quit!!"");_x000D_
  }_x000D_
}_x000D_
_x000D_
setTimeout(interactWithToDos, 0);



",prompt
alternative to device_map  quotautoquot in huggingface pretrained,"I have a model that I was reading from huggingface using the following code:
from transformers import AutoTokenizer, AutoModelForCausalLM

tokenizer = AutoTokenizer.from_pretrained(model_path)
model = AutoModelForCausalLM.from_pretrained(model_path, device_map=""auto"", trust_remote_code=True)

Now I read the model and I did some modifications to the internal layers and added more layers. When I started the training/fine-tuning I get that not everything is on the same model.
Now after more investigations, I found that my custom layers aren't distributed on multi GPUs as the original model. So I need something like device_map=""auto"" but after reading the model.
So simply something like
tokenizer = AutoTokenizer.from_pretrained(model_path)
model = AutoModelForCausalLM.from_pretrained(model_path, device_map=""auto"", trust_remote_code=True)

model.device_map = ""auto""

","I found out that there are actually several methods in accelerate for this. The first one is used to analyze your model and calculate the total amount of available memory that will be occupied by the model:
https://huggingface.co/docs/accelerate/en/package_reference/big_modeling#accelerate.infer_auto_device_map
The second one is used to match your model with the devices:
https://huggingface.co/docs/accelerate/en/package_reference/big_modeling#accelerate.dispatch_model
So basically, in your case, you can use the following code:
from accelerate import dispatch_model, infer_auto_device_map

model = AutoModelForCausalLM.from_pretrained(model_path, device_map=""auto"", trust_remote_code=True)

***
...
new_model = CustomModel(model)
...
***

device_map_dict = infer_auto_device_map(new_model)
dispatch_model(new_model, device_map_dict)

P.S. This code still needs to be tested on fine-tuning.
",huggingface-transformers
stucking at downloading shards for loading llm model from huggingface,"I am just using huggingface example to use their LLM model, but it stuck at the:
downloading shards:   0%|          | 0/5 [00:00<?, ?it/s]

(I am using Jupiter notebook, python 3.11, and all requirements were installed)
from transformers import AutoTokenizer, AutoModelForCausalLM
import transformers
import torch

model = ""tiiuae/falcon-40b-instruct""

tokenizer = AutoTokenizer.from_pretrained(model)
pipeline = transformers.pipeline(
    ""text-generation"",
    model=model,
    tokenizer=tokenizer,
    torch_dtype=torch.bfloat16,
    trust_remote_code=True,
    device_map=""auto"",
)
sequences = pipeline(
   ""Girafatron is obsessed with giraffes, the most glorious animal on the face of this Earth. Giraftron believes all other animals are irrelevant when compared to the glorious majesty of the giraffe.\nDaniel: Hello, Girafatron!\nGirafatron:"",
    max_length=200,
    do_sample=True,
    top_k=10,
    num_return_sequences=1,
    eos_token_id=tokenizer.eos_token_id,
)
for seq in sequences:
    print(f""Result: {seq['generated_text']}"")


how can I fix it?
","I think it's not stuck.
These are just very large models that take a while to download. tqdm only estimates after the first iteration, so it just looks like nothing is happening. I'm currently downloading the smallest version of LLama2 (7B parameters) and it's downloading two shards. The first took over 17 minutes to complete and I have reasonably fast internet connection.
",huggingface-transformers
ohmyposh themes not working correctly with powerline font and conemu,"I use PowerShell as my shell on Windows 7. I find that ConEmu is a really good terminal. Since I am not on Windows 10, I cannot use Windows Terminal but ConEmu is not bad at all. I found out about posh-git and oh-my-posh and how they can customize your PowerShell experience. I installed oh-my-posh and the Paradox theme looked nice. But I saw some empty boxes in random places in my Prompt. 
And this is an issue that I face on all the themes. The colors and the design are beautiful and I want to use it but those weird boxes are keeping me from doing that. I would also like to tell that I am using Cascadia Code as my font and also this is the Powerline version of Cascadia Code. So, I think it should work as excepted. Next, trying to fix this, I went to nerdfont.com and I installed Cascadia Code from there as opposed to Microsoft's official GitHub repository. Then I set ConEmu's font to Cascadia which I installed from nerdfonts and this happened:

It's better in a way that I can see the Python symbol and some more symbols but still there is one box that cannot be rendered. But it does get worse if I change repository:

There is a weird question mark after ""master"". I think I have met all the prerequisites to use oh-my-posh like install posh-git and PSReadLine and having powerline Cascadia Code font and also using ConEmu as they officially suggest. I would absolutely appreciate it a lot if anyone can help me out of this mess and suggest what to do to fix my prompt. 
P.S I am using PowerShell 7 Core.
","When you see boxes, that means that the font doesn't have that specified character. e.g. there are a lot of specialized fonts that don't have every character location defined.
Right on the oh-my-posh GitHub page, Quote:

In case you notice weird glyphs after installing a font of choice,
make sure the glyphs are available (maybe they have a different
location in the font, if so, adjust the correct $ThemeSettings icon).
If it turns out the character you want is not supported, select a
different font.

Also on the oh-my-posh GitHub page, the font used is:

The fonts I use are Powerline fonts, there is a great repository
containing them. I use Meslo LG M Regular for Powerline Nerd Font

If using Meslo LG M Regular doesn't solve your problem, then you have to manually remap the icons to the correct unicode locations in your chosen font.
For Version 2 of Oh My Posh, you have to edit the $ThemeSettings variable. Follow the instructions on the GitHub on configuring Theme Settings. e.g.:
$ThemeSettings.GitSymbols.BranchSymbol = [char]::ConvertFromUtf32(0xE0A0) 

For Version 3+ of Oh My Posh, you have to edit the JSON configuration file to make the changes, e.g.:
...
{
    ""type"": ""git"",
    ""style"": ""powerline"",
    ""powerline_symbol"": ""\uE0B0"",
....

",prompt
