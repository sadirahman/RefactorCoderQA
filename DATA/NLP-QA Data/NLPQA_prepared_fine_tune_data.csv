instruction,input,output,Categories
c aspnet mvc web app  update view after completion of asynch calls w openai,"I have an ASP.NET MVC app in C# that I'm trying to asynchronously call a bunch of OpenAI requests in parallel and then wait on all results to return.
The following is a snippet from this code:
        foreach (ChatMessage msg in messages)
        {
            List<ChatMessage> tmpMessage = new List<ChatMessage>();
            tmpMessage.Add(msg);

            Task<ClientResult<ChatCompletion>> chatResult = client.CompleteChatAsync(tmpMessage, chatOptions);

            await chatResult;

            if (chatResult.IsCompleted && chatResult.Result.Value != null)
            {
                output.Add(chatResult.Result.Value.Content[0].Text);
            }
        }

Each string in the output list is a json structured output from OpenAI. I then go through each json output and manipulate it as needed.
My questions are: Is this truly asynchronous? I call the CompleteChatAsync but then I use await chatResult and I am uncertain if this is the way to do that asynchronously?
Second - the view portion of the webpage does not update once all results return and are processed. It just sits there. How do I refresh the view in the ASP.NET MVC web app in .NET?
Thanks!
","To run several tasks ""in parallel"" (more correctly: concurrently), you can use Task.WhenAll(listOfTasks). The steps will be as follow:

Create the list of tasks (without awaiting them)
Fire them and wait for all of them to finish.
Handle the results.

example:
// Step 1
var allChatResults = new List<Task<ClientResult<ChatCompletion>>>();
foreach (ChatMessage msg in messages)
{
    var chatResult = client.CompleteChatAsync(tmpMessage, chatOptions);
    allChatResults.Add(chatResult);
}

// Step 2
await Task.WhenAll(allChatResults);

// Step 3
foreach (Task chatResult in allChatResults)
{
    if (chatResult.IsCompleted && chatResult.Result.Value != null)
    {
        output.Add(chatResult.Result.Value.Content[0].Text);
    }
}

",chatgpt
fastapi streamingresponse not streaming with generator function,"I have a relatively simple FastAPI app that accepts a query and streams back the response from ChatGPT's API. ChatGPT is streaming back the result and I can see this being printed to console as it comes in.
What's not working is the StreamingResponse back via FastAPI. The response gets sent all together instead. I'm really at a loss as to why this isn't working.
Here is the FastAPI app code:
import os
import time

import openai

import fastapi
from fastapi import Depends, HTTPException, status, Request
from fastapi.security import HTTPBearer, HTTPAuthorizationCredentials
from fastapi.responses import StreamingResponse

auth_scheme = HTTPBearer()
app = fastapi.FastAPI()

openai.api_key = os.environ[""OPENAI_API_KEY""]


def ask_statesman(query: str):
    #prompt = router(query)
    
    completion_reason = None
    response = """"
    while not completion_reason or completion_reason == ""length"":
        openai_stream = openai.ChatCompletion.create(
            model=""gpt-3.5-turbo"",
            messages=[{""role"": ""user"", ""content"": query}],
            temperature=0.0,
            stream=True,
        )
        for line in openai_stream:
            completion_reason = line[""choices""][0][""finish_reason""]
            if ""content"" in line[""choices""][0].delta:
                current_response = line[""choices""][0].delta.content
                print(current_response)
                yield current_response
                time.sleep(0.25)


@app.post(""/"")
async def request_handler(auth_key: str, query: str):
    if auth_key != ""123"":
        raise HTTPException(
            status_code=status.HTTP_401_UNAUTHORIZED,
            detail=""Invalid authentication credentials"",
            headers={""WWW-Authenticate"": auth_scheme.scheme_name},
        )
    else:
        stream_response = ask_statesman(query)
        return StreamingResponse(stream_response, media_type=""text/plain"")


if __name__ == ""__main__"":
    import uvicorn
    uvicorn.run(app, host=""0.0.0.0"", port=8000, debug=True, log_level=""debug"")

And here is the very simple test.py file to test this:
import requests

query = ""How tall is the Eiffel tower?""
url = ""http://localhost:8000""
params = {""auth_key"": ""123"", ""query"": query}

response = requests.post(url, params=params, stream=True)

for chunk in response.iter_lines():
    if chunk:
        print(chunk.decode(""utf-8""))

","First, it wouldn't be good practice to use a POST request for requesting data from the server. Using a GET request instead would be more suitable, in your case. In addition to that, you shouldn't be sending credentials, such as auth_key as part of the URL (i.e., using the query string), but you should rather use Headers and/or Cookies (using HTTPS). Please have a look at this answer for more details and examples on the concepts of headers and cookies, as well as the risks involved when using query parameters instead. Helpful posts around this topic can also be found here and here, as well as here, here and here.
Second, if you are executing a blocking operation (i.e., blocking I/O-bound or CPU-bound tasks) inside the StreamingResponse's generator function, you should define the generator function with def instead of async def, as, otherwise, the blocking operation, as well as the time.sleep() function that is used inside your generator, would blcok the event loop. As explained here, if the function for streaming the response body is a normal def generator and not an async def one, FastAPI will use iterate_in_threadpool() to run the iterator/generator in a separate thread that is then awaited—see StreamingResponse's relevant source code. If you prefer using an async def generator, then make sure to execute any blocking operations in an external ThreadPool (or ProcessPool) and await it, as well as use await asyncio.sleep() instead of time.sleep(), in cased you need to add delay in the execution of an operation. Have a look at this detailed answer for more details and examples.
Third, you are using requests' iter_lines() function, which iterates over the response data, one line at a time. If, however, the response data did not include any line break character (i.e., \n), you wouldn't see the data on client's console getting printed as they arrive, until the entire response is received by the client and printed as a whole. In that case, you should instead use iter_content() and specify the chunk_size as desired (both cases are demonstrated in the example below).
Finally, if you would like the StreamingResponse to work in every web browser (including Chrome as well)—in the sense of being able to see the data as they stream in—you should specify the media_type to a different type than text/plain (e.g., application/json or text/event-stream, see here), or disable MIME Sniffing. As explained here, browsers will start buffering text/plain responses for a certain amount (around 1445 bytes, as documented here), in order to check whether or not the content received is actually plain text. To avoid that, you can either set the media_type to text/event-stream (used for server-sent events), or keep using text/plain, but set the X-Content-Type-Options response header to nosniff, which would disable MIME Sniffing (both options are demonstrated in the example below).
Working Example
app.py
from fastapi import FastAPI
from fastapi.responses import StreamingResponse
import asyncio


app = FastAPI()


async def fake_data_streamer():
    for i in range(10):
        yield b'some fake data\n\n'
        await asyncio.sleep(0.5)


# If your generator contains blocking operations such as time.sleep(), then define the
# generator function with normal `def`. Alternatively, use `async def` and run any 
# blocking operations in an external ThreadPool/ProcessPool. (see 2nd paragraph of this answer)
'''
import time

def fake_data_streamer():
    for i in range(10):
        yield b'some fake data\n\n'
        time.sleep(0.5)
'''        

    
@app.get('/')
async def main():
    return StreamingResponse(fake_data_streamer(), media_type='text/event-stream')
    # or, use:
    '''
    headers = {'X-Content-Type-Options': 'nosniff'}
    return StreamingResponse(fake_data_streamer(), headers=headers, media_type='text/plain')
    '''

test.py (using Python requests)
import requests

url = ""http://localhost:8000/""

with requests.get(url, stream=True) as r:
    for chunk in r.iter_content(1024):  # or, for line in r.iter_lines():
        print(chunk)

test.py (using httpx—see this, as well as this and this for the benefits of using httpx over requests)
import httpx

url = 'http://127.0.0.1:8000/'

with httpx.stream('GET', url) as r:
    for chunk in r.iter_raw():  # or, for line in r.iter_lines():
        print(chunk)

",chatgpt
openai api in a laravel project backend implementation doesn39t work,"I am currently trying to use this API : https://github.com/openai-php/laravel in a personal Laravel project.
I don’t really know what is currently wrong in my code, all I can say is that the code detailed below gets my message displayed in the chatbox but returns an error 419 on my POST request : XHR POST http://127.0.0.1:8000/send as well as a Fetch error: Server error.
When NOT using the route and controller but giving the direct API endpoint to the JavaScript, I manage to get a JSON response to my message...
Please note that I would really like to make the backend part to work, and not just doing it in JS.
Also, I didn't want to use jQuery but I would if it might help.
The JavaScript :
//CSRF TOKEN
    const csrfToken = document.querySelector('meta[name=""csrf-token""]').getAttribute('content');
    const xhr = new XMLHttpRequest();
    xhr.open('GET', 'https://api.openai.com/v1/chat/completions', true);
    xhr.setRequestHeader('X-CSRF-TOKEN', csrfToken);

    const button = document.getElementById('button-submit');
    const chatWindow = document.getElementById('chat-window');
    const url = '{{ url('send') }}';

//API FETCH ON BUTTON CLICK
    button.addEventListener('click', function (){
        const input = document.getElementById('input').value;

        chatWindow.innerHTML += `<div class=""messages-user"">
        <div class=""__user"">
            <p>${input}</p>
        </div>
        <img src=""{{ asset('/images/avatar.png') }}"" alt=""Avatar"">
        <div style=""clear: both""></div>
        </div>`;

        fetch(url, {
            method: 'POST',
            body: JSON.stringify(input),
            headers: {
                'Content-Type': 'application/json',
                'Authorization': 'Bearer ""MY_OPENAI_KEY""'
            }
        }).then(function(response) {
            if (response.ok) {
                return response.text();
            } else {
                throw new Error('Server error.');
            }
        }).then(function(data) {
            chatWindow.innerHTML += `<div class=""messages-bot"">
                            <div class=""__bot"">
                                <p>${data}</p>
                            </div>
                            <img src=""{{ asset('/images/chatbot.png') }}"" alt=""Avatar"">
                            </div>
                            `;
        }).catch(function(error) {
            console.log('Fetch error:', error.message);
        });
    });

The web.php :
use Illuminate\Support\Facades\Route;
use App\Http\Controllers\ChatBotController;

Route::get(‘/’, function () {
    return view(‘welcome’);
});

Route::post(‘send’, [ChatBotController::class, ‘sendChat’]);

The Controller :
public function sendChat(Request $request){
    $result = OpenAI::completions()->create([
        ‘max-token’ => 100,
        ‘model’ => ‘text-davinci-003’,
        ‘prompt’ => $request->input
    ]);

    $response = array_reduce(
        $result->toArray()['choices'],
        fn(string $result, array $choice) => $result . $choice['text'], """"
    );
    
    return $response;
}

Thanks for your time !
","In laravel th 419 error, means thet the csrf token field is not resent or it is incorrect.
In this case you probably dosen't need to use the crsf token for a API call.
so you need to move this line of code:
Route::post('send', [ChatBotController::class, ‘sendChat’]);

from web.php (which uses the csrf protection) to the api.php file under the routes folder in your laravel project.
At this point you need to call the /api/send endpoint instead of the /send endpoint
",chatgpt
openai api error quotyou tried to access openaichatcompletion but this is no longer supported in openaigt100quot,"I am currently working on a chatbot, and as I am using Windows 11 it does not let me migrate to newer OpenAI library or downgrade it. Could I replace the ChatCompletion function with something else to work on my version?
This is the code:
import openai

openai.api_key = ""private""

def chat_gpt(prompt):
    response = openai.ChatCompletion.create(
        model=""gpt-3.5-turbo"",
        messages=[{""role"": ""user"", ""content"": prompt}]
    )
    return response.choices[0].message['content'].strip()

if __name__ == ""__main__"":
    while True:
        user_input = input(""You: "")
        if user_input.lower() in [""quit"", ""exit"", ""bye""]:
            break
        response = chat_gpt(user_input)
        print(""Bot:"", response)

And this is the full error:

...
You tried to access openai.ChatCompletion, but this is no longer supported in openai>=1.0.0 - see the README at https://github.com/openai/openai-python for the API.
You can run openai migrate to automatically upgrade your codebase to use the 1.0.0 interface.
Alternatively, you can pin your installation to the old version, e.g. <pip install openai==0.28>
A detailed migration guide is available here: https://github.com/openai/openai-python/discussions/742

I tried both upgrading and downgrading through pip.
","Try updating to the latest and using:
from openai import OpenAI

client = OpenAI(
    # defaults to os.environ.get(""OPENAI_API_KEY"")
    api_key=""private"",
)

def chat_gpt(prompt):
    response = client.chat.completions.create(
        model=""gpt-3.5-turbo"",
        messages=[{""role"": ""user"", ""content"": prompt}]
    )
    return response.choices[0].message.content.strip()

Link
EDIT: message.['content'] -> message.content on the return of this function, as a message object is not subscriptable error is thrown while using message.['content']. Also, update link from pointing to the README (subject to change) to migration guide specific to this code.
",chatgpt
how to work with openai maximum context length is 2049 tokens,"I'd like to send the text from various PDF's to OpenAI's API. Specifically the Summarize for a 2nd grader or the TL;DR summarization API's.
I can extract the text from PDF's using PyMuPDF and prepare the OpenAI prompt.
Question: How best to prepare the prompt when the token count is longer than the allowed 2049?

Do I just truncate the text then send multiple requests?
Or is there a way to sample the text to ""compress"" it to lose key points?

","I faced the same problem. Here is the strategy I used to send text that is much, much longer than OpenAIs GPT3 token limit.
Depending on the model (Davinci, Curie, etc.) used, requests can use up to 4097 tokens shared between prompt and completion.

Prompt being the input you send to OpenAI, i.e. your ""command"", e.g. ""Summarize the following text"" plus the text itself
Completion being the response, i.e. the entire summary of your text

If your prompt is 4000 tokens, your completion can be 97 tokens at most. For more information on OpenAI tokens and how to count them, see here.
To ensure that we don’t exceed the maximum length limit for prompt plus completion, we need to ensure that prompt (i.e. your text) and completion (i.e. the summary) put together always fits into the 4097 token boundary.
For that reason we split the entire text into multiple text chunks, summarize each chunk independently and finally merge all summarized chunks using a simple "" "".join() function.
Maximum Number of Words - Token-to-Word Conversion
OpenAI has a fixed limit on the number of tokens. However, a token is not the same as a word. Hence, we first need to calculate the maximum number of words we can send to OpenAI. The documentation says:

Given the token-to-word ratio, we can send approximately 2900 words to OpenAI's GPT3 assuming a 5 sentence summary per text chunk.

Max tokens per request: 4000 tokens (leaving 97 tokens as a safety buffer) = 3000 words
Max prompt tokens: “Summarize the following text in five sentences” has 7 words = 10 tokens
Max tokens of returned summary (5 sentences): 20 words per sentence. 5 * 20 = 100 words = 133 tokens
Max tokens of text chunk: 4000 - 10 - 133 = 3857 tokens = 2900 words

Text Chunking
We can choose from a plethora of strategies to split up the entire text into smaller chunks.
The simplest approach is creating a single list of all words by splitting the entire text on whitespaces, and then creating buckets of words with words evenly distributed across all buckets. The downside is that we are likely to split a sentence half-way through and lose the meaning of the sentence because GPT ends up summarizing the first half of the sentence independently from the second half — ignoring any relations between the two chunks.
Other options include tokenizers such as SentencePiece and spaCy’s sentence splitter. Choosing the later generates the most stable results.
Implementation of Text Chunking with spaCy
The following example splits the text “My first birthday was great. My 2. was even better.” into a list of two sentences.
python -m spacy download en_core_web_sm

import spacy
from spacy.lang.en import English

nlp = spacy.load(""en_core_web_sm"")

text = ""My first birthday was great. My 2. was even better.""
    
for sentence in nlp(text).sents:
  print(sentence.text)

Output
My first birthday was great.
My 2. was even better.

spaCy correctly detected the second sentence instead of splitting it after the “2.”.
Now, let’s write a text_to_chunks helper function to generate chunks of sentences where each chunk holds at most 2700 words. 2900 words was the initially calculated word limit, but we want to ensure to have enough buffer for words that are longer than 1.33 tokens.
def text_to_chunks(text):
  chunks = [[]]
  chunk_total_words = 0

  sentences = nlp(text)

  for sentence in sentences.sents:
    chunk_total_words += len(sentence.text.split("" ""))

    if chunk_total_words > 2700:
      chunks.append([])
      chunk_total_words = len(sentence.text.split("" ""))

    chunks[len(chunks)-1].append(sentence.text)
  
  return chunks


An alternative approach to determine the number of tokens of a text was recently introduced by OpenAI. The approach uses tiktoken and is tailored towards OpenAI's models.
import tiktoken

encoding = tiktoken.encoding_for_model(""gpt-3.5-turbo"")
number_of_tokens = len(encoding.encode(""tiktoken is great!""))
print(number_of_tokens)


Next, we wrap the text summarization logic into a summarize_text function.
def summarize_text(text):
  prompt = f""Summarize the following text in 5 sentences:\n{text}""

  response = openai.Completion.create(
      engine=""text-davinci-003"", 
      prompt=prompt,
      temperature=0.3, 
      max_tokens=150, # = 112 words
      top_p=1, 
      frequency_penalty=0,
      presence_penalty=1
  )

  return response[""choices""][0][""text""]

Our final piece of code looks like this:
chunks = text_to_chunks(one_large_text)

chunk_summaries = []

for chunk in chunks:
  chunk_summary = summarize_text("" "".join(chunk))
  chunk_summaries.append(chunk_summary)

summary = "" "".join(chunk_summaries)

References

How to count tokens with tiktoken, OpenAI Cookbook

",chatgpt
openai assistants api is the whole thread with all the past messages sent to the api every time i add a new message to the thread,"While I was using the Chat Completions API, I learned that you need to include the trail of the questions from the user and answers from the OpenAI API (including the system messages) when asking a new question if you want the Chat Completions API to be able to have the chat history included.
With the Assistants API, you don't need to do that, and it remembers the chat history.
My question is, what happens to token consumption in the case of the Assistants API? Would all the past messages be included in the token consumption?
","Token consumption in the Assistants API can be very, very high if you use the same thread for a long time because the thread is storing message history and passing the whole thread to the API every time you ask a new question using the existing thread.
After some time, a single message you ask the Assistants API can cost a lot, even if the message is short. See the past discussion:

/ ... /
The message contains around 1000 tokens, checked via
https://platform.openai.com/tokenizer
/ ... /
This code takes around 250,000 tokens to complete. The image shows
today's token usage for three requests.


What the developer didn't understand is that your recent message might contain 1,000 tokens, but you also need to keep in mind that hundreds of messages that were either asked by you or answered by the assistant in the past were also sent to the Assistants API.
There is, however, a limit of 100,000 messages per thread. As stated in the official OpenAI documentation:

The contents of the messages your users or applications create are
added as Message objects to the Thread. Messages can contain both text
and files. There is a limit of 100,000 Messages per Thread and we
smartly truncate any context that does not fit into the model's
context window.

",chatgpt
why api of new account on openai not working,"I created new account on Open AI and it shows free $5 limit and using it for testing but it is respponding with 429 status error do I have to pay first to test or is it error from my code?
import OpenAI from ""openai"";
import dotenv from ""dotenv"";
import { Router } from ""express"";
dotenv.config();
const router = Router();

const openAiClient = new OpenAI({
  organization: ""org-***"",
  apiKey: ""sk-proj-***"",
});
router.get(""/openai"", async (req, res) => {
  const completeChat = await openAiClient.chat.completions.create({
    model: ""gpt-3.5-turbo"",
    messages: [
      {
        role: ""user"",
        content: ""make a description of black jeans"",
      },
    ],
  });
  console.log(completeChat);
  res.sendStatus(200);
});

export default router;

","OpenAI requires a payment method to be on file even when using free credits. This is a common practice to prevent abuse and ensure uninterrupted access to the service. If you haven't added a payment method, the API may restrict your access, resulting in error codes.
",chatgpt
autogen studio 2  api_key is not present in llm_config or openai_api_key env variable for agent  primary_assistant,"Running autogen studio 2, I have my model setup and tested successful - when I run the travel demo I get the following error:

api_key is not present in llm_config or OPENAI_API_KEY env variable for agent ** primary_assistant**. Update your workflow to provide an api_key to use the LLM.

I have tested the model successfully and the agent is pointing to the correct model.


","I am using a conda environment and was able to resolve the issue by setting my env variable and deactivating / reactivating my environment. I used the command below to set my API key:
conda env config vars set OPENAI_API_KEY=value

conda deactivate envName
conda activate envName

",chatgpt
quotopenaibadrequesterror invalid rolequot error while using ollama  litellm  autogen,"Introduction
I am running an ollama server with LiteLLM proxy. I aim to design a group chat using Autogen. The designated agents are:

User_Proxy: ask question
SQL_Generator: generate SQL
SQL_Runner: run SQL (tool calling)
Result_Validator: validate result

Problems

The group chat works fine up until SQL_Runner returns the data from DB. But when the next speaker is expected to be the Result_Validator, the SQL_Generator is assigned as next speaker, although I explicitly state in the system message that SQL_Runner should pass the result to Result_Validator.

For some reason, SQL_Generator agent's role remains as tool following the activity of SQL_Runner instead of one of [system, user, assistant] and it leads to this error:


***** Response from calling tool (call_6493f510-d318-4e85-b552-8963bece5fcb) *****
[
    // data from DB
]
**********************************************************************************
Next speaker: SQL_Generator

// error trace

openai.BadRequestError: Error code: 400 - {'error': {'message': 'invalid role: tool, role must be one of [system, user, assistant]', 'type': 'api_error', 'param': None, 'code': None}}

Code
LLM Configs
llama3_config_list = [
    {
        ""model"": ""llama3:latest"",
        ""base_url"": f""{OLLAMA_BASE_URL}:{OLLAMA_PORT}/v1"",
        ""api_key"": ""ollama"",
    }
]

litellm_config_list = [
    {
        ""model"": ""NotRequired"",
        ""api_key"": ""NotRequired"",
        ""base_url"": f""{LITELLM_BASE_URL}:{LITELLM_PORT}"",
        ""price"": [0, 0],
    }
]

local_llm_config = {""config_list"": litellm_config_list, ""cache_seed"": None}

GroupChat
user_proxy = autogen.UserProxyAgent(
    name=Role.USER_PROXY,
    system_message=Prompt.USER_PROXY,
    human_input_mode=""NEVER"",
    is_termination_msg=is_termination_msg,
    code_execution_config={""use_docker"": False, ""work_dir"": ""test""},
)

sql_generator = autogen.AssistantAgent(
    name=Role.SQL_GENERATOR,
    system_message=Prompt.SQL_GENERATOR,
    llm_config=get_custom_llm_config(config_list=llama3_config_list),
)

sql_runner = autogen.AssistantAgent(
    name=Role.SQL_RUNNER,
    system_message=Prompt.SQL_RUNNER,
    llm_config=local_llm_config,
)

@user_proxy.register_for_execution()
@sql_runner.register_for_llm(description=""SQL query runner"")
def run_sql(sql: Annotated[str, ""SQL query string to run""]) -> str:
    with PostgresManager() as db:
        db.get_rds_connection()
        return db.run_sql(sql)

result_validator = autogen.AssistantAgent(
    name=Role.RESULT_VALIDATOR,
    system_message=Prompt.RESULT_VALIDATOR,
    llm_config=get_custom_llm_config(config_list=llama3_config_list)
)

groupchat = autogen.GroupChat(
    agents=[
        user_proxy,
        sql_generator,
        sql_runner,
        result_validator,
    ],
    messages=[],
)

manager = autogen.GroupChatManager(groupchat=groupchat)

user_proxy.initiate_chat(manager, message=question)

For the problem number 1, I have already tried to set autogen.GroupChat's speaker_selection_method to ""round_robin"" but the result did not change at all.
For the problem number 2, I am guessing that it might be overcome if I can somehow prevent agent role from being cached, but I cannot really figure out how.
","I was encountering this error since my ollama version did not support tool calling.
After upgrading ollama server version (to 0.4.2) now the role ""tool"" is available.
",chatgpt
completionscreate got an unexpected keyword argument 39request_timeout39,"I am using Autogen from microsoft with the below code:
import autogen
from autogen import AssistantAgent, UserProxyAgent

config_list = [
    {
        'model': 'gpt-4',
        'api_key': 'API_KEY'
    }
]

llm_config={
    ""request_timeout"": 600,
    ""seed"": 42,
    ""config_list"": config_list,
    ""temperature"": 0
}

assistant = autogen.AssistantAgent(
    name=""assistant"",
    llm_config=llm_config,
    system_message=""Chief technical officer of a tech company""
)

user_proxy = autogen.UserProxyAgent(
    name=""user_proxy"",
    human_input_mode=""ALWAYS"",
    max_consecutive_auto_reply=10,
    is_termination_msg=lambda x: x.get(""content"", """").rstrip().endswith(""TERMINATE""),
    code_execution_config={""work_dir"": ""web""},
    llm_config=llm_config,
    system_message=""""""Reply TERMINATE if the task has been solved at full satisfaction.
Otherwise, reply CONTINUE, or the reason why the task is not solved yet.""""""
)

task = """"""
Write python code to output numbers 1 to 100
""""""

user_proxy.initiate_chat(
    assistant,
    message=task
)

when I try to run the python, it gives me this error:
Completions.create() got an unexpected keyword argument 'request_timeout'
[autogen.oai.client: 09-05 14:32:12] {164} WARNING - The API key specified is not a valid OpenAI format; it won't work with the OpenAI-hosted model.
[autogen.oai.client: 09-05 14:32:12] {164} WARNING - The API key specified is not a valid OpenAI format; it won't work with the OpenAI-hosted model.
user_proxy (to assistant):


Write python code to output numbers 1 to 100


--------------------------------------------------------------------------------
Traceback (most recent call last):
  File ""c:\Users\HP\Desktop\prj\autogen-ve\Scripts\runningBots.py"", line 42, in <module>

How to resolve this?
","Please change your code to the following in order to make it work:
# config_list = [
#     {
#         'model': 'gpt-4o',
#         'api_key': 'API_KEY_HERE'
#     }
# ]
llm_config={""config_list"": config_list}

After this it should throw the error: you do not have access to the model, after which you should make a minimum payment of 5$ to open ai to access it
",chatgpt
importerror urllib3 v20 only supports openssl 111 currently the 39ssl39 module is compiled with libressl 283,"After pip install openai, when I try to import openai, it shows this error:

the 'ssl' module of urllib3 is compile with LibreSSL not OpenSSL

I just followed a tutorial on a project about using API of OpenAI. But when I get to the first step which is the install and import OpenAI, I got stuck. And I tried to find the solution for this error but I found nothing.
Here is the message after I try to import OpenAI:
Python 3.9.6 (default, Mar 10 2023, 20:16:38)
[Clang 14.0.3 (clang-1403.0.22.14.1)] on darwin
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.

>>> import openai

Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/Users/yule/Library/Python/3.9/lib/python/site-packages/openai/__init__.py"", line 19, in <module>
    from openai.api_resources import (
  File ""/Users/mic/Library/Python/3.9/lib/python/site-packages/openai/api_resources/__init__.py"", line 1, in <module>
    from openai.api_resources.audio import Audio  # noqa: F401
  File ""/Users/mic/Library/Python/3.9/lib/python/site-packages/openai/api_resources/audio.py"", line 4, in <module>
    from openai import api_requestor, util
  File ""/Users/mic/Library/Python/3.9/lib/python/site-packages/openai/api_requestor.py"", line 22, in <module>
    import requests
  File ""/Users/mic/Library/Python/3.9/lib/python/site-packages/requests/__init__.py"", line 43, in <module>
    import urllib3
  File ""/Users/mic/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py"", line 38, in <module>
    raise ImportError(
ImportError: urllib3 v2.0 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with LibreSSL 2.8.3. See: https://github.com/urllib3/urllib3/issues/2168

I tried to --upgrade the urllib3, but it is still not working. The result is:
pip3 install --upgrade urllib3
Defaulting to user installation because normal site-packages is not writeable
Requirement already satisfied: urllib3 in ./Library/Python/3.9/lib/python/site-packages (2.0.2)

","The reason why the error message mentioned OpenSSL 1.1.1+ and LibreSSL 2.8.3 is that urllib3 v2.0 (the version you've installed) requires OpenSSL 1.1.1+ to work properly, as it relies on some new features of OpenSSL 1.1.1.
The issue is that the version of the 'ssl' module that is currently installed in your environment is compiled with LibreSSL 2.8.3, which is not compatible with urllib3 v2.0.
To use urllib3 v2.0, you need an 'ssl' module compiled with OpenSSL 1.1.1 or later, by trying:
brew install openssl@1.1

Or you could use an older version of urllib3 that is compatible suc. For example urllib3 v1.26.6, which does not have a strict OpenSSL version requirement.
You can force the version installing with this command:
pip install urllib3==1.26.6

",chatgpt
openai python package error 39chatcompletion39 object is not subscriptable,"After updating my OpenAI package to version 1.1.1, I got this error when trying to read the ChatGPT API response:

'ChatCompletion' object is not subscriptable

Here is my code:
messages = [
        {""role"": ""system"", ""content"": '''You answer question about some service'''
        },
        {""role"": ""user"", ""content"": 'The user question is ...'},
    ]
response = client.chat.completions.create(
        model=model,
        messages=messages,
        temperature=0
    )
response_message = response[""choices""][0][""message""][""content""]

How can I resolve this error?
","In the latest OpenAI package the response.choices object type is changed and in this way you must read the response:
print(response.choices[0].message.content)

The complete working code:
from openai import OpenAI

client = OpenAI(api_key='YourKey')
GPT_MODEL = ""gpt-4-1106-preview"" #""gpt-3.5-turbo-1106""
messages = [
        {""role"": ""system"", ""content"": 'You answer question about Web  services.'
        },
        {""role"": ""user"", ""content"": 'the user message'},
    ]
response = client.chat.completions.create(
        model=GPT_MODEL,
        messages=messages,
        temperature=0
    )
response_message = response.choices[0].message.content
print(response_message )

See this example in the project README.
",chatgpt
why is there a big performance difference between those 2 simple python multithreading codes,"Let's consider this python code:
def process_payload(payload, url, headers):
    response = requests.post(url, headers=headers, json=payload)
    return response

def parallel_group2(payloads, url, headers):
    with ThreadPoolExecutor() as executor:
        results = executor.map(process_payload,payloads, [url]*len(payloads), [headers]*len(payloads))
    return list(results)

def parallel_group(payloads, url, headers):
    with ThreadPoolExecutor() as executor:
        results = executor.map(requests.post, [url]*len(payloads), [headers]*len(payloads), payloads)
    return list(results)

times = []
# payloads grouped by 15
payloads_grouped = [payloads[i:i+15] for i in range(0, len(payloads), 15)]
print( ""shape of payloads_grouped"", len(payloads_grouped), "" x "", len(payloads_grouped[0]))
for i in range(3):
    start = time.time()
    with ThreadPoolExecutor() as executor:
        # results = executor.map(parallel_group2, payloads_grouped, [url]*len(payloads_grouped), [headers]*len(payloads_grouped))
        results = executor.map(parallel_group, payloads_grouped, [url]*len(payloads_grouped), [headers]*len(payloads_grouped))
    end = time.time()
    times.append(end-start)
    print( ""Durations of iterations:"", times)
print( ""Durations of iterations:"", times)
print( ""Average time for 150 requests:"", sum(times)/len(times))

When I run the script with parallel_group, I have those results very consistently:
Durations of iterations: [5.246389389038086, 5.195073127746582, 5.278628587722778]
Average time for 150 requests: 5.2400303681691485

When I run it with parallel_group2 I have results looking more like this:
Durations of iterations: [10.99542498588562, 9.43007493019104, 23.003321170806885]
Average time for 150 requests: 10.142940362294516

Does someone have good knowledge in python multithreading and could explain why there is such a difference between multithreading calls to request.post and calls to a function that just do requests.call? I don't understand at all.
I ran the previous code several times and results were consistent.
Edit :
the url is the chat completion api of openai =""api.openai.com/v1/chat/completions""
","Your function parallel_group isn't doing what you would hope. The reason is that of the 3 parameters you're passing to requests.post, only the first one is correct (the URL). The payload will be assigned as data and the headers will be assigned to json The API is most likely to return an error but you're ignoring that possibility
",chatgpt
how do i format an openai api request in swift,"I need to use the OpenAI API and am struggling to format the request. I started by referencing the curl code that is listed in the OpenAI API playground, and this code worked in my terminal. That request is here:
curl https://api.openai.com/v1/chat/completions \
  -H ""Content-Type: application/json"" \
  -H ""Authorization: Bearer $OPENAI_API_KEY"" \
  -d '{
  ""model"": ""gpt-4o"",
  ""messages"": [
    {
      ""role"": ""user"",
      ""content"": [
        {
          ""type"": ""text"",
          ""text"": ""Write a haiku about weightlifting""
        }
      ]
    }
  ],
  ""temperature"": 1,
  ""max_tokens"": 2048,
  ""top_p"": 1,
  ""frequency_penalty"": 0,
  ""presence_penalty"": 0,
  ""response_format"": {
    ""type"": ""text""
  }
}'

Then, I started working on translating that request into swift instead of curl. I've been referencing some YouTube tutorials, and here is my code so far.
let openAiUrl = URL(string: ""https://api.openai.com/v1/chat/completions"")!
var request = URLRequest(url: openAiUrl)
request.setValue(""application/json"", forHTTPHeaderField: ""Content-Type"")
request.addValue(""Bearer \(openAiApiKey)"", forHTTPHeaderField: ""Authorization"")
request.httpMethod = ""POST""
let httpBody: [String: Any] = [
    ""messages"" : [
        {
            ""role"": ""user"", //Error: consecutive statements on a line must be separated by ';'
            ""content"": ""Write a haiku about weightlifting""
        }
    ],
    ""model"": ""gpt-4o"",
    ""max_tokens"" : 100,
    ""temperature"": String(temperature),
    ""frequency_penalty"": 0,
    ""presence_penalty"": 0,
    ""response_format"": { //Error: consecutive statements on a line must be separated by ';'
        ""type"": ""text""
    }
]

I am getting the error that says ""Consecutive statements on a line must be separated by ';'"" on the two lines that are using curly braces.
What do I need to do to format the API request? The temperature variable is a Double that is currently set to 1.0.
","Try this approach using a JSONEncoder to encode the json parameters, such as:
            let body = """"""
{
                    ""messages"" : [
                        {
                            ""role"": ""user"",
                            ""content"": ""Write a haiku about weightlifting""
                        }
                    ],
                    ""model"": ""gpt-4o"",
                    ""max_tokens"" : 100,
                    ""temperature"": 1.0,
                    ""frequency_penalty"": 0,
                    ""presence_penalty"": 0,
                    ""response_format"": { 
                        ""type"": ""text""
                    }
}
""""""
            
            do {
                request.httpBody = try JSONEncoder().encode(body)

                let (responseData, response) = try await URLSession.shared.data(for: request)

                print(""-----> \n \(String(data: responseData, encoding: .utf8) as AnyObject) \n"")

            } catch {
                print(""---> error: \(error)"")
            }

EDIT-1
Here is my test code using SwiftUI. Since I don't have a key I cannot fully test this code, but it compiles and gives some info about the response.
import SwiftUI

struct ContentView: View {
    let openAiApiKey = ""for testing""
    
    var body: some View {
        Text(""testing"")
            .task {
                await doPost()
            }
    }
    
    func doPost() async {
        let openAiUrl = URL(string: ""https://api.openai.com/v1/chat/completions"")!
        
        var request = URLRequest(url: openAiUrl)
        request.setValue(""application/json"", forHTTPHeaderField: ""Content-Type"")
        request.addValue(""Bearer \(openAiApiKey)"", forHTTPHeaderField: ""Authorization"")
        request.httpMethod = ""POST""
        
        let body = """"""
{
""messages"": [
{
""role"": ""user"",
""content"": ""Write a haiku about weightlifting""
}
],
""model"": ""gpt-4o"",
""max_tokens"" : 100,
""temperature"": 1.0,
""frequency_penalty"": 0,
""presence_penalty"": 0,
""response_format"": { 
    ""type"": ""text""
}
}
""""""
        do {
            request.httpBody = try JSONEncoder().encode(body)
            
            let (responseData, response) = try await URLSession.shared.data(for: request)
            print(""-----> responseData \n \(String(data: responseData, encoding: .utf8) as AnyObject) \n"")
        }
        catch { print(error) }
    }
}

EDIT-2
another alternative,
struct ContentView: View {
    let openAiApiKey = ""for testing""
    
    var body: some View {
        Text(""testing"")
            .task {
                let url = URL(string: ""https://api.openai.com/v1/chat/completions"")!
                var request = URLRequest(url: url)
                request.httpMethod = ""POST""
                request.setValue(""application/json"", forHTTPHeaderField: ""Content-Type"")
                request.setValue(""Bearer \(openAiApiKey)"", forHTTPHeaderField: ""Authorization"")
                
                let requestBody: [String: Any] = [
                    ""model"": ""gpt-4o"",
                    ""messages"": [
                        [
                            ""role"": ""user"",
                            ""content"": [
                                [
                                    ""type"": ""text"",
                                    ""text"": ""Write a haiku about weightlifting""
                                ]
                            ]
                        ]
                    ],
                    ""temperature"": 1,
                    ""max_tokens"": 2048,
                    ""top_p"": 1,
                    ""frequency_penalty"": 0,
                    ""presence_penalty"": 0,
                    ""response_format"": [
                        ""type"": ""text""
                    ]
                ]
                
                do {
                    request.httpBody = try JSONSerialization.data(withJSONObject: requestBody)
                    
                    let (responseData, response) = try await URLSession.shared.data(for: request)
                    print(""-----> responseData \n \(String(data: responseData, encoding: .utf8) as AnyObject) \n"")
                }
                catch { print(error) }
                
            }
    }
}

",chatgpt
python compare html tags in ro folder with their corresponding tags in en folder and displays in output the unique tags from both files,"In short, I have two files, one in Romanian, the other has been translated into English. In the RO file there are some tags that have not been translated into EN. So I want to display in an html output all the tags in EN that have corresponding tags in RO, but also those tags in RO that do not appear in EN.
I have this files:
   ro_file_path = r'd:\3\ro\incotro-vezi-tu-privire.html'
   en_file_path = r'd:\3\en\where-do-you-see-look.html'
   Output =  d:\3\Output\where-do-you-see-look.html 

TASK: Compare the 3 tags below, in both files.
<p class=""text_obisnuit"">(.*?)</p>
<p class=""text_obisnuit2"">(.*?)</p>
<p class=""text_obisnuit""><span class=""text_obisnuit2"">(.*?)</span>(.*?)</p>

Requirements:

All tags are enclosed between: <!-- START ARTICLE --> and <!-- FINAL ARTICLE -->
Count the tags in RO and count the tags in EN, and compare.
Then count the words in the tags in RO and compare with the number of words in the tags in EN.
Compares the html tags in RO with the html tags in EN, in order, and displays in Output the unique tags from both files

RO       d:\3\ro\incotro-vezi-tu-privire.html
<!-- ARTICOL START --> 
<p class=""text_obisnuit""><span class=""text_obisnuit2"">Stiu ca este dificil sa conduci la inceput, </span>dar dupa 4-5 luni inveti.</p> 
<p class=""text_obisnuit2"">Imi place sa merg la scoala si sa invat, mai ales in timpul saptamanii.</p> 
<p class=""text_obisnuit"">Sunt un bun conducator auto, dar am facut si greseli din care am invatat.</p> 
<p class=""text_obisnuit"">În fond, cele scrise de mine, sunt adevarate.</p> 
<p class=""text_obisnuit"">Iubesc sa conduc masina.</p> 
<p class=""text_obisnuit""><span class=""text_obisnuit2"">Ma iubesti?</p> 
<p class=""text_obisnuit""><span class=""text_obisnuit2"">Stiu ca este dificil sa conduci la inceput, </span>dar dupa 4-5 luni inveti.</p> 
<p class=""text_obisnuit"">Totul se repetă, chiar și ochii care nu se vad.</p> 
<p class=""text_obisnuit2"">BEE servesc o cafea 2 mai buna</p> 
<!-- ARTICOL FINAL -->

   

EN    d:\3\en\where-do-you-see-look.html
<!-- ARTICOL START -->
<p class=""text_obisnuit2"">I like going to school and learning, especially during the week.</p>
<p class=""text_obisnuit"">I'm a good driver, but I've also made mistakes that I've learned from.</p>
<p class=""text_obisnuit"">Basically, what I wrote is true.</p>
<p class=""text_obisnuit"">I love driving.</p>
<p class=""text_obisnuit""><span class=""text_obisinuit2"">I know it's difficult to drive at first, </span> but after 4-5 months you learn.</p>
<p class=""text_obisnuit"">Everything is repeated, even the eyes that can't see.</p>
<!-- ARTICOL FINAL -->

Expected OUTPUT:  d:\3\Output\where-do-you-see-look.html
<!-- ARTICOL START -->
<p class=""text_obisnuit""><span class=""text_obisnuit2"">Stiu ca este dificil sa conduci la inceput, </span> dar dupa 4-5 luni inveti.</p> 
<p class=""text_obisnuit2"">I like going to school and learning, especially during the week.</p>
<p class=""text_obisnuit"">I'm a good driver, but I've also made mistakes that I've learned from.</p>
<p class=""text_obisnuit"">Basically, what I wrote is true.</p>
<p class=""text_obisnuit""><span class=""text_obisnuit2"">Ma iubesti?</p> 
<p class=""text_obisnuit"">I love driving.</p>
<p class=""text_obisnuit""><span class=""text_obisinuit2"">I know it's difficult to drive at first, </span> but after 4-5 months you learn.</p>
<p class=""text_obisnuit"">Everything is repeated, even the eyes that can't see.</p>
<p class=""text_obisnuit2"">BEE servesc o cafea 2 mai buna</p> 
<!-- ARTICOL FINAL -->

Python code must compares the html tags in RO with the html tags in EN and displays in Output the unique tags in both files, taking into account that most of the tags in RO have their corresponding translation in the tags in EN. But the idea of ​​the code is that the code also finds those html tags in RO that were omitted from being translated into EN.
Here's how I came up with the solution in Python code. I followed a simple calculation.
First method:
First, you have to count all the tags in ro, then all the tags in en.
Then you have to memorize each type of tag in ro, but then also in en.
Then you have to count the words in each tag in ro and the words in each tag in en.
Don't forget that there can be 2 identical tags, but on different lines, just like in RO.
Then you have to statistically calculate the result. How much are the tags in ro minus the tags in en?
The second method, to verify the output, is to print the screen. Compare the entire ro part and the entire en part separately through OCR, then line by line, see which tags in ro are plus compared to the tags in en
PYTHON CODE:
import re
import os

def extract_tags(content):
    start = content.find('<!-- ARTICOL START -->')
    end = content.find('<!-- ARTICOL FINAL -->')
    if start == -1 or end == -1:
        raise ValueError(""Marcajele 'ARTICOL START' sau 'ARTICOL FINAL' lipsesc."")

    section_content = content[start:end]
    pattern = re.compile(r'<p class=""text_obisnuit(?:2)?"">(?:<span class=""text_obisnuit2"">)?.*?</p>', re.DOTALL)
    tags = []

    for idx, match in enumerate(pattern.finditer(section_content), 1):
        tag = match.group(0)
        text = re.sub(r'<[^>]+>', '', tag).strip()

        if '<span class=""text_obisnuit2"">' in tag or '<span class=""text_obisinuit2"">' in tag:
            tag_type = 'span'
        elif 'class=""text_obisnuit2""' in tag:
            tag_type = 'text_obisnuit2'
        else:
            tag_type = 'text_obisnuit'

        tags.append({
            'index': idx,
            'tag': tag,
            'text': text,
            'type': tag_type,
            'word_count': len(text.split())
        })
    return tags

def find_matching_pairs(ro_tags, en_tags):
    matched_indices = set()
    used_en = set()

    for i, ro_tag in enumerate(ro_tags):
        for j, en_tag in enumerate(en_tags):
            if j in used_en:
                continue

            if ro_tag['type'] == en_tag['type']:
                word_diff = abs(ro_tag['word_count'] - en_tag['word_count'])
                if word_diff <= 3:
                    matched_indices.add(i)
                    used_en.add(j)
                    break
    return matched_indices

def fix_duplicates(output_content, ro_content):
    """"""Corectează poziția tag-urilor duplicate""""""
    ro_tags = extract_tags(ro_content)
    output_tags = extract_tags(output_content)

    # Găsim tag-urile care apar în RO și OUTPUT
    for ro_idx, ro_tag in enumerate(ro_tags):
        for out_idx, out_tag in enumerate(output_tags):
            if ro_tag['tag'] == out_tag['tag'] and ro_idx != out_idx:
                # Am găsit un tag care apare în poziții diferite
                # Verificăm dacă este cazul de duplicat care trebuie mutat
                ro_lines = ro_content.split('\n')
                out_lines = output_content.split('\n')

                if ro_tag['tag'] in ro_lines[ro_idx+1] and out_tag['tag'] in out_lines[out_idx+1]:
                    # Mutăm tag-ul la poziția corectă
                    out_lines.remove(out_tag['tag'])
                    out_lines.insert(ro_idx+1, out_tag['tag'])
                    output_content = '\n'.join(out_lines)
                    break

    return output_content

def generate_output(ro_tags, en_tags, original_content):
    start = original_content.find('<!-- ARTICOL START -->')
    end = original_content.find('<!-- ARTICOL FINAL -->')
    if start == -1 or end == -1:
        raise ValueError(""Marcajele 'ARTICOL START' sau 'ARTICOL FINAL' lipsesc."")

    output_content = original_content[:start + len('<!-- ARTICOL START -->')] + ""\n""
    matched_indices = find_matching_pairs(ro_tags, en_tags)
    en_index = 0

    for i, ro_tag in enumerate(ro_tags):
        if i in matched_indices:
            output_content += en_tags[en_index]['tag'] + ""\n""
            en_index += 1
        else:
            output_content += ro_tag['tag'] + ""\n""

    while en_index < len(en_tags):
        output_content += en_tags[en_index]['tag'] + ""\n""
        en_index += 1

    output_content += original_content[end:]
    return output_content

def main():
    try:
        ro_file_path = r'd:\3\ro\incotro-vezi-tu-privire.html'
        en_file_path = r'd:\3\en\where-do-you-see-look.html'
        output_file_path = r'd:\3\Output\where-do-you-see-look.html'

        with open(ro_file_path, 'r', encoding='utf-8') as ro_file:
            ro_content = ro_file.read()
        with open(en_file_path, 'r', encoding='utf-8') as en_file:
            en_content = en_file.read()

        ro_tags = extract_tags(ro_content)
        en_tags = extract_tags(en_content)

        # Generăm primul output
        initial_output = generate_output(ro_tags, en_tags, en_content)

        # Corectăm pozițiile tag-urilor duplicate
        final_output = fix_duplicates(initial_output, ro_content)

        with open(output_file_path, 'w', encoding='utf-8') as output_file:
            output_file.write(final_output)

        print(f""Output-ul a fost generat la {output_file_path}"")

    except Exception as e:
        print(f""Eroare: {str(e)}"")

if __name__ == ""__main__"":
    main()

My Python code is almost perfect, but not perfect. The problem occurs when I introduce other tags in RO, such as:
<!-- ARTICOL START --> 
<p class=""text_obisnuit"">Laptopul meu este de culoare neagra.</p>
<p class=""text_obisnuit2"">Imi place sa merg la scoala si sa invat, mai ales in timpul saptamanii.</p> 
<p class=""text_obisnuit"">Sunt un bun conducator auto, dar am facut si greseli din care am invatat.</p> 
<p class=""text_obisnuit""><span class=""text_obisnuit2"">Stiu ca este dificil sa conduci la inceput, </span>dar dupa 4-5 luni inveti.</p>
<p class=""text_obisnuit"">În fond, cele scrise de mine, sunt adevarate.</p> 
<p class=""text_obisnuit"">Iubesc sa conduc masina.</p> 

<p class=""text_obisnuit""><span class=""text_obisnuit2"">Stiu ca este dificil sa conduci la inceput, </span>dar dupa 4-5 luni inveti.</p>
<p class=""text_obisnuit"">Totul se repetă, chiar și ochii care nu se vad.</p> 

<!-- ARTICOL FINAL -->

","SECOND, and the BEST SOLUTION.
Finally I solved the problem, but not with ChatGPT or Claude. No other AI could find the solution, because it didn't know how to think about the solution.
In fact, to find the solution to this problem, you had to assign some identifiers to each tag, and do multiple searches.
ChatGPT or Claude, or other AIs, will have to seriously consider this type of solution for such problems.
Here are the specifications, the way I thought about solving the problem. It's a different way of thinking about doing PARSINGS.
https://pastebin.com/as2yw1UQ
Python code made by a friend of mine. I think the solution, he made the code:
from bs4 import BeautifulSoup
import re

def count_words(text):
    """"""Numără cuvintele dintr-un text.""""""
    return len(text.strip().split())

def get_greek_identifier(word_count):
    """"""Determină identificatorul grecesc bazat pe numărul de cuvinte.""""""
    if word_count < 7:
        return 'α'
    elif word_count <= 14:
        return 'β'
    else:
        return 'γ'

def get_tag_type(tag):
    """"""Determină tipul tagului (A, B, sau C).""""""
    if tag.find('span'):
        return 'A'
    elif 'text_obisnuit2' in tag.get('class', []):
        return 'B'
    return 'C'

def analyze_tags(content):
    """"""Analizează tagurile și returnează informații despre fiecare tag.""""""
    soup = BeautifulSoup(content, 'html.parser')
    tags_info = []

    article_content = re.search(r'<!-- ARTICOL START -->(.*?)<!-- ARTICOL FINAL -->',
                              content, re.DOTALL)

    if article_content:
        content = article_content.group(1)
        soup = BeautifulSoup(content, 'html.parser')

    for i, tag in enumerate(soup.find_all('p', recursive=False)):
        text_content = tag.get_text(strip=True)
        tag_type = get_tag_type(tag)
        word_count = count_words(text_content)
        greek_id = get_greek_identifier(word_count)

        tags_info.append({
            'number': i + 1,
            'type': tag_type,
            'greek': greek_id,
            'content': str(tag),
            'text': text_content
        })

    return tags_info

def compare_tags(ro_tags, en_tags):
    """"""Compară tagurile și găsește diferențele.""""""
    wrong_tags = []
    i = 0
    j = 0

    while i < len(ro_tags):
        ro_tag = ro_tags[i]
        if j >= len(en_tags):
            wrong_tags.append(ro_tag)
            i += 1
            continue

        en_tag = en_tags[j]

        if ro_tag['type'] != en_tag['type']:
            wrong_tags.append(ro_tag)
            i += 1
            continue

        i += 1
        j += 1

    return wrong_tags

def format_results(wrong_tags):
    """"""Formatează rezultatele pentru afișare și salvare.""""""
    type_counts = {'A': 0, 'B': 0, 'C': 0}
    type_content = {'A': [], 'B': [], 'C': []}

    for tag in wrong_tags:
        type_counts[tag['type']] += 1
        type_content[tag['type']].append(tag['content'])

    # Creăm rezultatul formatat
    result = []

    # Prima linie cu sumarul
    summary_parts = []
    for tag_type in ['A', 'B', 'C']:
        if type_counts[tag_type] > 0:
            summary_parts.append(f""{type_counts[tag_type]} taguri de tipul ({tag_type})"")
    result.append(""In RO exista in plus fata de EN urmatoarele: "" + "" si "".join(summary_parts))

    # Detaliile pentru fiecare tip de tag
    for tag_type in ['A', 'B', 'C']:
        if type_counts[tag_type] > 0:
            result.append(f""\n{type_counts[tag_type]}({tag_type}) adica asta {'taguri' if type_counts[tag_type] > 1 else 'tag'}:"")
            for content in type_content[tag_type]:
                result.append(content)
            result.append("""")  # Linie goală pentru separare

    return ""\n"".join(result)

def merge_content(ro_tags, en_tags, wrong_tags):
    """"""Combină conținutul RO și EN, inserând tagurile wrong în pozițiile lor originale.""""""
    merged_tags = []

    # Creăm un dicționar pentru tagurile wrong indexat după numărul lor original
    wrong_dict = {tag['number']: tag for tag in wrong_tags}

    # Parcurgem pozițiile și decidem ce tag să punem în fiecare poziție
    current_en_idx = 0
    for i in range(max(len(ro_tags), len(en_tags))):
        position = i + 1

        # Verificăm dacă această poziție este pentru un tag wrong
        if position in wrong_dict:
            merged_tags.append(wrong_dict[position]['content'])
        elif current_en_idx < len(en_tags):
            merged_tags.append(en_tags[current_en_idx]['content'])
            current_en_idx += 1

    return merged_tags

def save_results(merged_content, results, output_path):
    """"""Salvează conținutul combinat și rezultatele în fișierul de output.""""""
    final_content = '<!-- REZULTATE ANALIZA -->\n'
    final_content += '<!-- ARTICOL START -->\n'

    # Adaugă conținutul combinat
    for tag in merged_content:
        final_content += tag + '\n'

    final_content += '<!-- ARTICOL FINAL -->\n'
    final_content += '<!-- FINAL REZULTATE ANALIZA -->\n'

    # Adaugă rezultatele analizei
    final_content += results

    # Salvează în fișier
    with open(output_path, 'w', encoding='utf-8') as file:
        file.write(final_content)

# Citește fișierele
with open(r'd:/3/ro/incotro-vezi-tu-privire.html', 'r', encoding='utf-8') as file:
    ro_content = file.read()

with open(r'd:/3/en/where-do-you-see-look.html', 'r', encoding='utf-8') as file:
    en_content = file.read()

# Definește calea pentru fișierul de output
output_path = r'd:/3/Output/where-do-you-see-look.html'

# Analizează tagurile
ro_tags = analyze_tags(ro_content)
en_tags = analyze_tags(en_content)

# Găsește diferențele
wrong_tags = compare_tags(ro_tags, en_tags)

# Formatează rezultatele
results = format_results(wrong_tags)

# Generează conținutul combinat
merged_content = merge_content(ro_tags, en_tags, wrong_tags)

# Afișează rezultatele în consolă
print(results)

# Salvează rezultatele în fișierul de output
save_results(merged_content, results, output_path)

",chatgpt
how can i process a pdf using openai39s apis gpts,"The web interface for ChatGPT has an easy pdf upload. Is there an API from openAI that can receive pdfs?
I know there are 3rd party libraries that can read pdf but given there are images and other important information in a pdf, it might be better if a model like GPT 4 Turbo was fed the actual pdf directly.
I'll state my use case to add more context. I intent to do RAG. In the code below I handle the PDF and a prompt. Normally I'd append the text at the end of the prompt. I could still do that with a pdf if I extract its contents manually.
The following code is taken from here https://platform.openai.com/docs/assistants/tools/code-interpreter. Is this how I'm supposed to do it?
# Upload a file with an ""assistants"" purpose
file = client.files.create(
  file=open(""example.pdf"", ""rb""),
  purpose='assistants'
)

# Create an assistant using the file ID
assistant = client.beta.assistants.create(
  instructions=""You are a personal math tutor. When asked a math question, write and run code to answer the question."",
  model=""gpt-4-1106-preview"",
  tools=[{""type"": ""code_interpreter""}],
  file_ids=[file.id]
)

There is an upload endpoint as well, but it seems the intent of those endpoints are for fine-tuning and assistants. I think the RAG use case is a normal one and not necessarily related to assistants.
","As of today (openai.__version__==1.42.0) using OpenAI Assistants + GPT-4o allows to extract content of (or answer questions on) an input pdf file foobar.pdf stored locally, with a solution along the lines of
from openai import OpenAI
from openai.types.beta.threads.message_create_params import (
    Attachment,
    AttachmentToolFileSearch,
)
import os

filename = ""foobar.pdf""
prompt = ""Extract the content from the file provided without altering it. Just output its exact content and nothing else.""

client = OpenAI(api_key=os.environ.get(""MY_OPENAI_KEY""))

pdf_assistant = client.beta.assistants.create(
    model=""gpt-4o"",
    description=""An assistant to extract the contents of PDF files."",
    tools=[{""type"": ""file_search""}],
    name=""PDF assistant"",
)

# Create thread
thread = client.beta.threads.create()

file = client.files.create(file=open(filename, ""rb""), purpose=""assistants"")

# Create assistant
client.beta.threads.messages.create(
    thread_id=thread.id,
    role=""user"",
    attachments=[
        Attachment(
            file_id=file.id, tools=[AttachmentToolFileSearch(type=""file_search"")]
        )
    ],
    content=prompt,
)

# Run thread
run = client.beta.threads.runs.create_and_poll(
    thread_id=thread.id, assistant_id=pdf_assistant.id, timeout=1000
)

if run.status != ""completed"":
    raise Exception(""Run failed:"", run.status)

messages_cursor = client.beta.threads.messages.list(thread_id=thread.id)
messages = [message for message in messages_cursor]

# Output text
res_txt = messages[0].content[0].text.value
print(res_txt)

The prompt can of course be replaced with the desired user request and I assume that the openai key is stored in a env var named MY_OPENAI_KEY.
Limitations:

it's not (yet) possible to enforce JSON structure (other than with instructions in the prompt). This solution is inspired by https://medium.com/@erik-kokalj/effectively-analyze-pdfs-with-gpt-4o-api-378bd0f6be03.

this relies on text content in the PDF (i.e. searchable text content), and the queries won't be able to access e.g. image content in the pdf.


",chatgpt
using a single api key or generating temporary keys for each user,"I'm building a Flutter app that requires the OpenAI GPT3 API, and I'm not sure how to implement it. Currently, I'm using a single API key that's stored in a .env file and accessed via the flutter_dotenv package. I'm wondering whether it's best to use this one API key for all users of the app, or whether I should implement an API gateway and generate temporary API keys for each user.
While I don't anticipate reaching the request limit for my single API key after I release the app, I'm uncertain about the best approach. What are the potential downsides to using a single API key for all users, and what are the benefits of generating temporary keys for each user? Would an API gateway be necessary for my use case?
","Are you concerned about security or people abusing your tool and hurting your OpenAI API limits?
Security
If you are concerned about security, keep your API key secret, and make sure it does not leak to any frontend or public repo.
You can even use secret manager solutions like Doppler, AWS Secret Manager, or 1password for developers.
Cost/API limits
You may want reduce the risk of someone harming your system and potentially costing you thousands of dollars by making a lot of API requests.
One solution is to track on your side how many calls are made over a period of time per user.
Ex: one user can generate 15 completions per period of 24 hours.
If you offer a paying plan for your service, this is an incentive for people to upgrade.
To fight abuse, OpenAI has also implemented End-user Ids.
You can add a user parameter to your API requests. It will identify which user is responsible for which API call, and eventually, you can shut it down.
Here you can read more in the doc.
",chatgpt
openai api throws bad request when uploading a file,"I am using the Python version of the OpenAI API and trying to upload a file with the following code:
client.files.create(file=myFile, purpose=""fine-tune"")
The client has been initialized with a valid API key, and other calls to the API (to send text prompts) work correctly, but when I try to post a file I get a ""HTTP/1.1 400 Bad Request"" error.
I have verified that the file exists and is a readable text file (the code logs the first line of the file before posting it).
The code used to initialize myFile is:
myFile = open(file_path, ""rb"");
What could the problem be?
","As stated by @codenanda, I was using the wrong file format. The current version of the OpenAI API only supports JSON files.
",chatgpt
importerror no module named langchainllms,"I used the following import statetement:
from langchain.llms import OpenAI 

And I am getting the following error:

pycode python main.py Traceback (most recent call last):   File
""main.py"", line 1, in 
from langchain.llms import openai ImportError: No module named langchain.llms

I am using Python 3.11.6 and I installed the packages using
pip3 install openai langchain

","
it works with
Version: 0.0.274

the latest langchain version is 0.0.320 and try to import like this:
from langchain.llms.openai import OpenAI

Maybe your python version installed an early verison of langchain due to dependency requirements
",chatgpt
chatgpt api issue or jquery  not retrieving full response,"I have some javascript that is analysing content, it generates a content analysis of content initially, and then adds a button to ""write content"" about the content analysis.
My problem is the first section to analyse works, but it only generates the first 50 words or so and cuts off. When I click the write content button, it seems to generate the next 50 words and not actually do according to the prompt.
What am I doing wrong? is my API calls correct?
<script>
function fetchChatGPTResponse(content, postNo) {
        $.ajax({
            url: 'chatgpt_api.php',
            method: 'POST',
            data: {
                prompt: 'write a content analysis based on this: ' + content // Use the post content as the prompt
            },
            success: function(response) {
                // Display the content analysis in the response div
                document.getElementById('response' + postNo).innerHTML = response + `<br><button onclick=""fetchChatGPTArticle('${encodeURIComponent(response)}', '${postNo}');"">Write Content</button>`;
    
                    
            },
            error: function() {
                document.getElementById('response').innerText = ""Error"";
            }
        });
    }
    
    function fetchChatGPTArticle(article, postNo2) {
        $.ajax({
            url: 'chatgpt_api.php',
            method: 'POST',
            data: {
                prompt: 'Based on this content analysis pasted after this statement, could you write me an article to reinforce this statement in 1000 words, ensure that it is engaging. Don\'t include any stats about upvotes and comments: ' + decodeURIComponent(article)
            },
            success: function(article) {
                // Display the generated article in the article div
                document.getElementById('article' + postNo2).innerHTML = article;
            },
            error: function() {
                document.getElementById('article' + postNo2).innerText = ""Error"";
            }
        });
    }
        </script>

chatgpt_api.php:
<?php
$api_key = 'XXXX';

if ($_SERVER['REQUEST_METHOD'] === 'POST' && isset($_POST['prompt'])) {
    $prompt = $_POST['prompt'];

    $data = [
        'model' => 'gpt-3.5-turbo',
        'messages' => [['role' => 'user', 'content' => $prompt]],
        'max_tokens' => 100
    ];

    $headers = [
        'Content-Type: application/json',
        'Authorization: Bearer ' . $api_key,
    ];

    $ch = curl_init('https://api.openai.com/v1/chat/completions');
    curl_setopt($ch, CURLOPT_RETURNTRANSFER, true);
    curl_setopt($ch, CURLOPT_HTTPHEADER, $headers);
    curl_setopt($ch, CURLOPT_POST, true);
    curl_setopt($ch, CURLOPT_POSTFIELDS, json_encode($data));

    $response = curl_exec($ch);
    curl_close($ch);

    $responseData = json_decode($response, true);
    $message = $responseData['choices'][0]['message']['content'] ?? 'No response from ChatGPT.';

    echo $message; // Return the response to the AJAX call
}
?>

","Your noting of about 50 words make sense, remove the 'max_tokens' limiter in the body of your API call and you should be good
$data = [
        'model' => 'gpt-3.5-turbo',
        'messages' => [['role' => 'user', 'content' => $prompt]],
];

",chatgpt
when using openai assistants v2 which models are supported,"I have some old code running with openai assistants beta v1 using model gpt-4-1106-preview and I'm trying to follow steps in:
https://platform.openai.com/docs/assistants/migration
To upgrade to v2. It all works if I keep the model the same but when I try and use any gpt-4o model I get unsupported_model error. Is this just something you can't use with assistants?
","The OpenAI Assistants API v2 supports the following models, as of today:

GPT-4o (except chatgpt-4o-latest),
GPT-4o-mini,
GPT-4, and
GPT-3.5 Turbo.

The retrieval tool requires:

gpt-4-turbo-preview (and subsequent dated model releases), or
gpt-3.5-turbo-1106 (and subsequent dated model releases).

See the official OpenAI documentation on model endpoint compatibility.
",chatgpt
unable to install evals python cli for openai,"Failing to install: pip install evals. For complete logs please see this gist file here.
Tools Version:

python --version: Python 3.9.6
pip --version: pip 24.2 from /Users/apple/Library/Python/3.9/lib/python/site-packages/pip (python 3.9)

Collecting keras<2.8,>=2.7.0rc0 (from tensorflow<3.0.0,>=2.4.0->spacy-universal-sentence-encoder->evals)
  Using cached keras-2.7.0-py2.py3-none-any.whl.metadata (1.3 kB)
Collecting tensorboard~=2.6 (from tensorflow<3.0.0,>=2.4.0->spacy-universal-sentence-encoder->evals)
  Using cached tensorboard-2.17.1-py3-none-any.whl.metadata (1.6 kB)
  Using cached tensorboard-2.17.0-py3-none-any.whl.metadata (1.6 kB)
  Using cached tensorboard-2.13.0-py3-none-any.whl.metadata (1.8 kB)
  Using cached tensorboard-2.11.2-py3-none-any.whl.metadata (1.9 kB)
Collecting protobuf (from google-generativeai->evals)
  Using cached protobuf-3.20.3-cp39-cp39-macosx_10_9_x86_64.whl.metadata (679 bytes)
Collecting tensorboard~=2.6 (from tensorflow<3.0.0,>=2.4.0->spacy-universal-sentence-encoder->evals)
  Using cached tensorboard-2.11.0-py3-none-any.whl.metadata (1.9 kB)
  Using cached tensorboard-2.10.1-py3-none-any.whl.metadata (1.9 kB)
  Using cached tensorboard-2.10.0-py3-none-any.whl.metadata (1.9 kB)
  Using cached tensorboard-2.7.0-py3-none-any.whl.metadata (1.9 kB)
  Using cached tensorboard-2.6.0-py3-none-any.whl.metadata (1.9 kB)
Collecting tensorflow<3.0.0,>=2.4.0 (from spacy-universal-sentence-encoder->evals)
  Using cached tensorflow-2.7.1-cp39-cp39-macosx_10_11_x86_64.whl.metadata (2.9 kB)
  Using cached tensorflow-2.7.0-cp39-cp39-macosx_10_11_x86_64.whl.metadata (2.9 kB)
INFO: pip is still looking at multiple versions of tf-keras to determine which version is compatible with other requirements. This could take a while.
  Using cached tensorflow-2.6.5-cp39-cp39-macosx_10_11_x86_64.whl.metadata (2.8 kB)
  Using cached tensorflow-2.6.4-cp39-cp39-macosx_10_11_x86_64.whl.metadata (2.8 kB)
  Using cached tensorflow-2.6.3-cp39-cp39-macosx_10_11_x86_64.whl.metadata (2.8 kB)
  Using cached tensorflow-2.6.2-cp39-cp39-macosx_10_11_x86_64.whl.metadata (2.8 kB)
  Using cached tensorflow-2.6.1-cp39-cp39-macosx_10_14_x86_64.whl.metadata (2.8 kB)
  Using cached tensorflow-2.6.0-cp39-cp39-macosx_10_11_x86_64.whl.metadata (2.8 kB)
  Using cached tensorflow-2.5.3-cp39-cp39-macosx_10_11_x86_64.whl.metadata (2.8 kB)
  Using cached tensorflow-2.5.2-cp39-cp39-macosx_10_14_x86_64.whl.metadata (2.8 kB)
  Using cached tensorflow-2.5.1-cp39-cp39-macosx_10_11_x86_64.whl.metadata (2.8 kB)
  Using cached tensorflow-2.5.0-cp39-cp39-macosx_10_11_x86_64.whl.metadata (2.8 kB)
Collecting SQLAlchemy<3,>=1.4 (from langchain->evals)
  Using cached SQLAlchemy-2.0.35-cp39-cp39-macosx_10_9_x86_64.whl.metadata (9.6 kB)
INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.
  Using cached SQLAlchemy-2.0.34-cp39-cp39-macosx_10_9_x86_64.whl.metadata (9.6 kB)
  Using cached SQLAlchemy-2.0.33-cp39-cp39-macosx_10_9_x86_64.whl.metadata (9.6 kB)
ERROR: Exception:
Traceback (most recent call last):
  File ""/Users/apple/Library/Python/3.9/lib/python/site-packages/pip/_internal/cli/base_command.py"", line 105, in _run_wrapper
    status = _inner_run()
  File ""/Users/apple/Library/Python/3.9/lib/python/site-packages/pip/_internal/cli/base_command.py"", line 96, in _inner_run
    return self.run(options, args)
  File ""/Users/apple/Library/Python/3.9/lib/python/site-packages/pip/_internal/cli/req_command.py"", line 67, in wrapper
    return func(self, options, args)
  File ""/Users/apple/Library/Python/3.9/lib/python/site-packages/pip/_internal/commands/install.py"", line 379, in run
    requirement_set = resolver.resolve(
  File ""/Users/apple/Library/Python/3.9/lib/python/site-packages/pip/_internal/resolution/resolvelib/resolver.py"", line 95, in resolve
    result = self._result = resolver.resolve(
  File ""/Users/apple/Library/Python/3.9/lib/python/site-packages/pip/_vendor/resolvelib/resolvers.py"", line 546, in resolve
    state = resolution.resolve(requirements, max_rounds=max_rounds)
  File ""/Users/apple/Library/Python/3.9/lib/python/site-packages/pip/_vendor/resolvelib/resolvers.py"", line 457, in resolve
    raise ResolutionTooDeep(max_rounds)
pip._vendor.resolvelib.resolvers.ResolutionTooDeep: 200000

As we can see in the end of below log, we see some error. Please help me debug this, I'm new to python and tried to find the answer but couldn't figure it out after waiting to install this for like 1 hours twice. The error is like pip._vendor.resolvelib.resolvers.ResolutionTooDeep: 200000.
Please help. Thanks in advance.
Github Respository issue by me on this python library: https://github.com/openai/evals/issues/1563
Community Post on OpenAI: https://community.openai.com/t/im-not-able-to-install-evals-python-library/981186
","I updated my python to latest version from official python website on recommendation of @stark-jarvis, and that helped me. Thanks @stark-jarvis.
",chatgpt
does prompt_token usage affect my billing when using azure openai models with your own data,"I have set Azure OpenAI on my data, Chat with Azure OpenAI models using your own data. My goal was to reduce token usage in each request.
However, I have noticed additional prompt_token usage, even when I am sending user content without any prompt. For example, if I only send the text hello there, it results in a total of 2628 tokens, whereas it should only be 24. If a longer text (7 words) is provided without any prompt, it results in a total of approximately 3.4k tokens.
Example:
[{'role': 'system', 'content': ''}, {'role': 'user', 'content': 'hello there'}]
total_tokens: {'completion_tokens': 24, 'prompt_tokens': 2604, 'total_tokens': 2628}

----------------------------------------------------

[{'role': 'system', 'content': ''},
{'role': 'user', 'content': 'I worked overtime what should I do?'}]
total_tokens: {'completion_tokens': 52, 'prompt_tokens': 3334, 'total_tokens': 3386}

As you can see, under total token I am seeing prompt_token usage close the ~3.5k.
Where does prompt_token usage come from since I do not provide any prompt or system message? Isn't the whole purpose of using Azure OpenAI models with your own data to reduce token usage? For each request, additional 3.5k token is very expensive. Will it affect my billing, where would prompt_token be considered as part of input tokens?

Pricing/details/cognitive-services/openai-service/, states that Input (Per 1,000 tokens) is $0.0025, than as I understand for 4,000 tokens, the cost should be $0.01.
","There is a bit of magic going on when they ""use your data""; it is essentially Retrieval Augmented Generation or RAG. Basically there's a few more steps than simply your prompt text.
They explain it fairly well here:

In total, there are two calls made to the model:


For processing the intent: The token estimate for the intent prompt includes those for the user question, conversation history, and the instructions sent to the model for intent generation.


For generating the response: The token estimate for the generation prompt includes those for the user question, conversation history, the retrieved list of document chunks, role information, and the instructions sent to it for generation.

",chatgpt
using whisper api to generate srt transcripts,"I'm exploring the capabilities of the Whisper API and was wondering if it can be used to generate an .SRT file with transcriptions. From what I understand, this transcription to .SRT can be achieved when running the model locally using the Whisper package. Unfortunately, I don't possess the computational resources to run the model locally, so I'm leaning towards using the API directly.
Has anyone had experience with this or can provide guidance on how to approach it through the API?
The following python script can be used a starting point, but the question is about capabilities of the model itself, not specific to any programming language.
import os
import openai
openai.api_key = API_KEY
audio_file = open(""audio.mp3"", ""rb"")
transcript = openai.Audio.transcribe(""whisper-1"", audio_file)
print(transcript.text)


","A cursory look at OpenAI's docs shows that srt is a supported value for the response_format parameter on the /v1/audio/transcriptions endpoint.
With the official Python bindings you're using in your example, you should be able to pass this as a named parameter to your openai.Audio.transcribe() invocation:
transcript = openai.Audio.transcribe(""whisper-1"", audio_file, response_format=""srt"")

",chatgpt
simple urlsession not functioning in swift what am i doing wrong,"I'm looking to create a simple CLI in Swift for working with OpenAI's GPT models. I've tried various approaches. The code below is the simplest version and still not functional -- I believe the closure is not being called.
let endpoint = ""https://api.openai.com/v1/completions""
let headers = [
    ""Content-Type"": ""application/json"",
    ""Authorization"": ""Bearer \(oai_key)""
]

let prompt = ""Create a simple function in Swift that adds two numbers.""

let parameters = [
    ""model"": ""text-curie-001"",
    ""prompt"": prompt,
    ""max_tokens"": 500,
    ""temperature"": 0.5,
    ""n"": 1
] as [String: Any]

let urlObject = URL(string: endpoint)!

var request = URLRequest(url: urlObject)
request.httpMethod = ""POST""
request.allHTTPHeaderFields = headers

do {
    request.httpBody = try JSONSerialization.data(withJSONObject: parameters)
} catch {
    print(error)
}

func processResponseData(_ data: Data) {
    print(String(data: data, encoding: .utf8) ?? """")
}

let newTask = URLSession.shared.dataTask(with: request) {
    data, response, error in
    print(""Completion handler closure called"")
    
    if (error != nil) {
        print(""This error occured: \(String(describing: error))"")
    } else {
        print(String(describing: data))
        if let data = data {
            processResponseData(data)
        }
    }
}

print(newTask.state)
newTask.resume()
print(newTask.state)
print(newTask.response)
print(newTask.error)
print(newTask.progress)

The output in the console from executing this is:
NSURLSessionTaskState
NSURLSessionTaskState
nil
nil
<NSProgress: 0x600002c08b80> : Parent: 0x0 (portion: 0) / Fraction completed: 0.0000 / Completed: 0 of 100  
  <NSProgress: 0x600002c08d80> : Parent: 0x600002c08b80 (portion: 95) / Fraction completed: 0.0000 / Completed: 0 of 100  
  <NSProgress: 0x600002c08d00> : Parent: 0x600002c08b80 (portion: 5) / Fraction completed: 0.0000 / Completed: 0 of 100  
Program ended with exit code: 0

Thanks in advance!
","Thanks to @vadian -- a runloop was a great solution.
I ended up cleaning up the code after the runloop got it working and then found a semaphore worked well for my implementation.
Here's the code I landed on:
let openAI = OpenAIAPI()

func getInput() -> String {
    print(""User:"")
    if let input = readLine() {
        return input
    }
    return """"
}

func handleCompletion(responseText: String?) {
    if let responseText = responseText {
        print(""Response:\n\(responseText)"")
    } else {
        print(""Error: Unable to get response"")
    }
}

while true {
    let prompt = getInput()
    
    if prompt.lowercased() == ""finished"" {
        break
    }
    
    let semaphore = DispatchSemaphore(value: 0)
    
    openAI.getCompletion(prompt: prompt) { responseText in
        handleCompletion(responseText: responseText)
        semaphore.signal()
    }
    semaphore.wait()
}

Note that I wrapped the previous code I posted in an OpenAIAPI class and added additional functionality to it.
",chatgpt
properly count amount of tokens in the whole request payload  openai,"17.10.24: Title edited for easier search
Original title: What part of OpenAI API request payload is limited by the max amount tokens?

I kinda understand how to count tokens out of characters, but what do I actually have to count?
If I have a payload like this:
{
  ""model"": ""gpt-3.5-turbo"",
  ""temperature"": 1,
  ""max_tokens"": 400,
  ""presence_penalty"": 0.85,
  ""frequency_penalty"": 0.85,
  ""messages"": [
    {
      ""role"": ""system"",
      ""content"": ""prompt""
    },
    {
      ""role"": ""assistant"",
      ""content"": ""message""
    },
    // tens of messages
  ]
}

Do I have to count tokens out of it entirely? Or do I have to count it in ""messages"" only? If so, do I have to count all the json syntax characters, like spacebars, brackets and commas too? What about ""role"" and ""content"" keys? What about ""role"" value?
Or I have to simply concat all the ""content"" values into a single string and count tokens based only on it?
","From my understanding and calculations, all the tokens in the list provided in ""messages"" are counted. This includes the keys ""role"" and ""content"" and their values but does not include spaces, brackets, commas, and quotes.
I use the following script provided by OpenAI to calculate the number of tokens in my input. I have modified the script to calculate the cost involved with the input for multiple messages (not the output response) and it's been fairly accurate for me.
import json
import os
import tiktoken
import numpy as np
from collections import defaultdict

def num_tokens_from_messages(messages, model=""gpt-3.5-turbo-0613""):
    """"""Return the number of tokens used by a list of messages.""""""
    try:
        encoding = tiktoken.encoding_for_model(model)
    except KeyError:
        print(""Warning: model not found. Using cl100k_base encoding."")
        encoding = tiktoken.get_encoding(""cl100k_base"")
    if model in {
        ""gpt-3.5-turbo-0613"",
        ""gpt-3.5-turbo-16k-0613"",
        ""gpt-4-0613"",
        ""gpt-4-32k-0613"",
        }:
        tokens_per_message = 3
        tokens_per_name = 1
    elif model == ""gpt-3.5-turbo-0301"":
        tokens_per_message = 4  # every message follows <|start|>{role/name}\n{content}<|end|>\n
        tokens_per_name = -1  # if there's a name, the role is omitted
    elif ""gpt-3.5-turbo"" in model:
        print(""Warning: gpt-3.5-turbo may update over time. Returning num tokens assuming gpt-3.5-turbo-0613."")
        return num_tokens_from_messages(messages, model=""gpt-3.5-turbo-0613"")
    elif ""gpt-4"" in model:
        print(""Warning: gpt-4 may update over time. Returning num tokens assuming gpt-4-0613."")
        return num_tokens_from_messages(messages, model=""gpt-4-0613"")
    else:
        raise NotImplementedError(
            f""""""num_tokens_from_messages() is not implemented for model {model}. See https://github.com/openai/openai-python/blob/main/chatml.md for information on how messages are converted to tokens.""""""
        )
    num_tokens = 0
    for message in messages:
        num_tokens += tokens_per_message
        for key, value in message.items():
            num_tokens += len(encoding.encode(value))
            if key == ""name"":
                num_tokens += tokens_per_name
    num_tokens += 3  # every reply is primed with <|start|>assistant<|message|>
    return num_tokens

convo_lens = []

for ex in dataset: #Your list of inputs
    messages = ex[""messages""]
    convo_lens.append(num_tokens_from_messages(messages))

n_input_tokens_in_dataset = sum(min(4096, length) for length in convo_lens)
print(f""Input portion of the data has ~{n_input_tokens_in_dataset} tokens"")

# costs as of Aug 29 2023.
costs = {
    ""gpt-4-0613"": {
        ""input"" : 0.03,
        ""output"": 0.06
    },
    ""gpt-4-32k-0613"": {
        ""input"" : 0.06,
        ""output"": 0.12
    },
    ""gpt-3.5-turbo-0613"": {
        ""input"": 0.0015,
        ""output"": 0.002
    },

    ""gpt-3.5-turbo-16k-0613"": {
        ""input"": 0.003,
        ""output"": 0.004
    }
}

# We select GPT 3.5 turbo here
print(f""Cost of inference: ${(n_input_tokens_in_dataset/1000) * costs['gpt-3.5-turbo-0613']['input']}"")


",chatgpt
openai api openaiapi_key  osgetenv not working,"I am just trying some simple functions in Python with OpenAI APIs but running into an error:
I have a valid API secret key which I am using.
Code:
>>> import os
>>> import openai
>>> openai.api_key = os.getenv(""I have placed the key here"")
>>> response = openai.Completion.create(model=""text-davinci-003"", prompt=""Say this is a test"", temperature=0, max_tokens=7)


","Option 1: OpenAI API key not set as an environment variable
Change this...
openai.api_key = os.getenv('sk-xxxxxxxxxxxxxxxxxxxx')
...to this.
openai.api_key = 'sk-xxxxxxxxxxxxxxxxxxxx'

Option 2: OpenAI API key set as an environment variable (recommended)
There are two ways to set the OpenAI API key as an environment variable:

using an .env file (easier, but don't forget to create a .gitignore file) or
using Windows Environment Variables.

Way 1: Using an .env file
Change this...
openai.api_key = os.getenv('sk-xxxxxxxxxxxxxxxxxxxx')
...to this...
openai.api_key = os.getenv('OPENAI_API_KEY')
Also, don't forget to use the python-dotenv package. Your final Python file should look as follows:
# main.py

import os
from dotenv import load_dotenv
from openai import OpenAI

# Load environment variables from the .env file
load_dotenv()

# Initialize OpenAI client with the API key from environment variables
client = OpenAI(
    api_key=os.getenv(""OPENAI_API_KEY""),
)

It's crucial that you create a .gitignore file not to push the .env file to your GitHub/GitLab and leak your OpenAI API key!
# .gitignore

.env

Way 2: Using Windows Environment Variables (source)
STEP 1: Open System properties and select Advanced system settings

STEP 2: Select Environment Variables

STEP 3: Select New
STEP 4: Add your name/key value pair
Variable name: OPENAI_API_KEY

Variable value: sk-xxxxxxxxxxxxxxxxxxxx

STEP 5: Restart your computer (IMPORTANT!)
Your final Python file should look as follows:
# main.py

import os
from dotenv import load_dotenv
from openai import OpenAI

# Initialize OpenAI client
# It will automatically use your OpenAI API key set via Windows Environment Variables
client = OpenAI()

",chatgpt
permission denied when npminstalling openai realtime api,"All I'm trying to do is npm install openai realtime API and I'm getting permission denied.
npm i openai/openai-realtime-api-beta
npm error code 128
npm error An unknown git error occurred
npm error command git --no-replace-objects ls-remote ssh://git@github.com/openai/openai-realtime-api-beta.git
npm error git@github.com: Permission denied (publickey).
npm error fatal: Could not read from remote repository.
npm error
npm error Please make sure you have the correct access rights
npm error and the repository exists.

","It looks like this is your problem:
git@github.com: Permission denied (publickey)
npm is trying to clone the /openai/openai-realtime-api-beta.git repository but cannot connect over SSH because your public key hasn't been registered with GitHub.
GitHub has a walk-through on how to set this up. Once you follow the step-by-step instructions, this should work for you.
",chatgpt
how to forward openai39s stream response using fastapi in python,"This is my code to retrieve stream response from OpenAI's model which is event based. (I have shown only core part)
client = OpenAI(api_key=OPEN_AI_API_KEY)

class EventHandler(AssistantEventHandler):
    def on_text_delta(self, delta: TextDelta, snapshot: Text):
        print(delta.value)

with client.beta.threads.runs.stream(
    thread_id=thread_id,
    assistant_id=assistant_id,
    event_handler=EventHandler()
) as stream:
stream.until_done()

on_text_delta event triggers as tokens arrives from API. I want to forward this response using FastAPI instead of printing on output screen.
@app.get(""/stream"")
async def stream():
    return ...something...

I have tried responding result as part of HTTP body:
from fastapi.responses import StreamingResponse

...

@app.post(""/stream"")
async def stream():
    with client.beta.threads.runs.stream(
        thread_id=thread_id,
        assistant_id=assistant_id,
        event_handler=EventHandler()
    ) as stream:
        stream.until_done()

    return StreamingResponse(EventHandler.generator_function(), media_type=""text/plain"")

I have created generator_function inside EventHandler class but problem is until stream is not over the execution doesn't reach return statement.
I have also tried websockets, but still problem is how should my program execution should flow. The stream doesn't let execution go further until API response is completed.
","I found the solution!
from fastapi import FastAPI
from fastapi.responses import StreamingResponse
from openai import OpenAI, AsyncOpenAI


OPEN_AI_API_KEY = 'you_api_key'
async_client = AsyncOpenAI(api_key=OPEN_AI_API_KEY)
client = OpenAI(api_key=OPEN_AI_API_KEY)

app = FastAPI()

async def stream_assistant_response(assistant_id, thread_id):
    stream =  async_client.beta.threads.runs.stream(
        assistant_id=assistant_id,
        thread_id=thread_id
    )

    async with stream as stream:
        async for text in stream.text_deltas:
            yield f""data: {text}\n\n""


@app.get(""/message"")
async def add_message(assistant_id, thread_id, message):
    # make sure thread exist
    client.beta.threads.messages.create(
        thread_id=thread_id,
        role=""user"",
        content=message
    )

    return StreamingResponse(stream_assistant_response(assistant_id, thread_id), media_type=""text/event-stream"")

",chatgpt
what is the difference between openai and chatopenai in langchain,"I read the LangChain Quickstart.
There is a demo inside:
from langchain.llms import OpenAI
from langchain.chat_models import ChatOpenAI

llm = OpenAI()
chat_model = ChatOpenAI()

llm.predict(""hi!"")
>>> ""Hi""

chat_model.predict(""hi!"")
>>> ""Hi""

I searched the rest of the document and also online, but didn't find any info for the difference between OpenAI and ChatOpenAI.
Based on from langchain.llms import OpenAI, OpenAI is a large language model (LLM) which is also chat related.
So is OpenAI more general-purpose, while ChatOpenAI more chat focused?
What is the difference between OpenAI class and ChatOpenAI class in LangChain? Could someone clarify?
","TL;DR
Based on my research,

OpenAI class includes more generic machine learning task attributes such as frequency_penalty, presence_penalty, logit_bias, allowed_special, disallowed_special, best_of.

ChatOpenAI class provides more chat-related methods, such as completion_with_retry, get_num_tokens_from_messages to make it more user-friendly when build chatbot related applications.



Class Inheritance
Upon reviewing the source code, here's what I've discovered.
Listed below are the class inheritances for both the OpenAI and ChatOpenAI classes, along with their respective class attributes and methods.
OpenAI
OpenAI ← BaseOpenAI ← BaseLLM ← BaseLanguageModel

ChatOpenAI
ChatOpenAI ← BaseChatModel ← BaseLanguageModel

Comparison
Let's begin our comparison, moving from the fourth column to the first column.
Fourth Column
Both classes ultimately inherit from the base class BaseLanguageModel.
Third Column
BaseLLM and BaseChatModel are very similar with slightly difference:

For OpenAI's BaseLLM, it includes additional methods:

batch(self, inputs, config=None, max_concurrency=None, **kwargs)
abatch (self, inputs, config=None, max_concurrency=None,**kwargs)


For ChatOpenAI's BaseChatModel, it includes an extra method:

_combine_llm_outputs(self, llm_outputs)



Second Column
The second column contains the BaseOpenAI class, which primarily exists due to the presence of higher-level classes OpenAI and AzureOpenAI. However, they all share the same class attributes and methods.
First Column
At the top-level class (first column):

OpenAI class includes more generic machine learning task attributes such as frequency_penalty, presence_penalty, logit_bias, allowed_special, disallowed_special, best_of.

ChatOpenAI class provides more chat-related methods, such as completion_with_retry, get_num_tokens_from_messages to make it more user-friendly when build chatbot related applications.


",chatgpt
cloudflare 400 error when calling openai completions api on cloudrun,"Running into a weird error where I get a 400 error from cloud-flare when calling the completions API from GCP Cloud Run. The error doesn't occur locally for identical requests.
The error response:
<html>

<head><title>400 Bad Request</title></head>

<body>

<center><h1>400 Bad Request</h1></center>

<hr><center>cloudflare</center>

</body>

</html>

The request:
POST to https://api.openai.com/v1/chat/completions
{""model"":""gpt-4-turbo"",""messages"":[{""role"":""system"",""content"":""Using the notes provided, write a summary of the critical information from the notes.\nThe summary should be in dotpoints. Prioritise the dotpoints by importance.""},{""role"":""user"",""content"":""                Use only the following information to generate the summary:\n                            There are the following notes about the contact:\n            Note Content: \""Test\""\nNote Created At: 2024-09-26T07:08:04.304501Z\nNote is in the context of: Just the contact in the role of Unknown \nNote type: Manual""}]}

Nothing obvious in the headers.
Has anyone run into anything similar. Wondering if its some kind of IP blocking by cloudflare or similar.
","So we managed to solve this by switching from using the OkHttp java client, to using the Java 17 core library client.
I suspect there is more to it then that (some configuration on the two clients) but we weren't able to find it at this time.
",chatgpt
get chatgpt to respond with a single direct answer,"I am querying a text using chatGPT. But I need chatGPT to respond with single direct answers, rather than long stories or irrelevant text. Any way to achieve this?
My code looks like:
from langchain.document_loaders import TextLoader
from langchain.vectorstores import DocArrayInMemorySearch
from langchain.indexes import VectorstoreIndexCreator

loader = TextLoader(""path/to/extracted_text.txt"")
loaded_text = loader.load()
# Save document text as vector.
index = VectorstoreIndexCreator(
            vectorstore_cls=DocArrayInMemorySearch
        ).from_loaders([loader])

# Query the text
response = index.query(""At what time did john come home yesterday?"")
print(""Loaded text is:"", loaded_text)
print(""ChatGPT response is:"", response)


>>> Loaded text is: ""< a really long text > + John came home last
night at 11:30pm + < a really long text >""


>>> ChatGPT response is: ""John came back yesterday at 11:30pm.""

The problem is that I want a concise answer 11:30pm rather than a full sentence John came home last night at 11:30pm. Is there a way to achieve this without adding ""I need a short direct response"" to my query? Can I achieve a more guaranteed concise response by setting a parameter through some other means instead?
","The BEST way to achieve what you want is Proper Prompt Engineering.  Period.  No way around it.  It's more of a thought discipline than acquiring a new skill.  Read this quick Microsoft Learn doc about prompt engineering to privy yourself to the knowledge you need to advance your task.  Best wishes!
",chatgpt
valueerror run not supported when there is not exactly one output key got 39answer39 39sources39 39source_documents39 langchainstreamlit,"I got an error says
ValueError: `run` not supported when there is not exactly one output key. Got ['answer', 'sources', 'source_documents'].

Here's the traceback error
File ""C:\Users\Science-01\anaconda3\envs\gpt-dev\lib\site-packages\streamlit\runtime\scriptrunner\script_runner.py"", line 565, in _run_script
    exec(code, module.__dict__)
File ""C:\Users\Science-01\Documents\Working Folder\Chat Bot\Streamlit\alpha-test.py"", line 67, in <module>
    response = chain.run(prompt, return_only_outputs=True)
File ""C:\Users\Science-01\anaconda3\envs\gpt-dev\lib\site-packages\langchain\chains\base.py"", line 228, in run
    raise ValueError(

I tried to run langchain on Streamlit. I use RetrievalQAWithSourcesChain and ChatPromptTemplate
Here is my code
import os

import streamlit as st

from apikey import apikey

from langchain.document_loaders import PyPDFLoader
from langchain.document_loaders import DirectoryLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.embeddings.openai import OpenAIEmbeddings
from langchain.vectorstores import Chroma
from langchain.chains import RetrievalQAWithSourcesChain
from langchain.llms import OpenAI

from langchain.prompts.chat import (
    ChatPromptTemplate,
    SystemMessagePromptTemplate,
    HumanMessagePromptTemplate,
)

from langchain.chat_models import ChatOpenAI

os.environ['OPENAI_API_KEY'] = apikey

st.title('🐔 OpenAI Testing')
prompt = st.text_input('Put your prompt here')

loader = DirectoryLoader('./',glob='./*.pdf', loader_cls=PyPDFLoader)
pages = loader.load_and_split()

text_splitter = RecursiveCharacterTextSplitter(
    chunk_size = 1000,
    chunk_overlap  = 200,
    length_function = len,
)

docs = text_splitter.split_documents(pages)
embeddings = OpenAIEmbeddings()

docsearch = Chroma.from_documents(docs, embeddings)

system_template = """"""
Use the following pieces of context to answer the users question.
If you don't know the answer, just say that ""I don't know"", don't try to make up an answer.
----------------
{summaries}""""""

messages = [
    SystemMessagePromptTemplate.from_template(system_template),
    HumanMessagePromptTemplate.from_template(""{question}"")
]
prompt = ChatPromptTemplate.from_messages(messages)

chain_type_kwargs = {""prompt"": prompt}
llm = ChatOpenAI(model_name=""gpt-3.5-turbo"", temperature=0, max_tokens=256)  # Modify model_name if you have access to GPT-4
chain = RetrievalQAWithSourcesChain.from_chain_type(
    llm=llm,
    chain_type=""stuff"",
    retriever=docsearch.as_retriever(search_kwargs={'k':2}),
    return_source_documents=True,
    chain_type_kwargs=chain_type_kwargs
)

if prompt:
    response = chain.run(prompt, return_only_outputs=True)
    st.write(response)

It seems like the error is in chain.run(), anyone know how to solve this error?
","I found the solution, change this code
if prompt:
    response = chain.run(prompt, return_only_outputs=True)
    st.write(response)

to this
if st.button('Generate'):
    if prompt:
        with st.spinner('Generating response...'):
            response = chain({""question"": prompt}, return_only_outputs=True)
            answer = response['answer']
            st.write(answer)
    else:
        st.warning('Please enter your prompt')

I also added st.button, st.spinner, and st.warning (optional)
",chatgpt
how to create langfuse trace via net code,"I am developing an AI solution which combine of .Net and Python app. The reason for Python app is mostly to communicate with OpenAI and Langfuse. I want to cut off the Python app to make solution simpler and easier to deploy, but couldn't find a way to write Langfuse trace via its API. Does anyone know what is the endpoint and payload to do it?
","You need to use the ingestion endpoint: https://api.reference.langfuse.com/#post-/api/public/ingestion
You can also download the OpenAPI spec for types at the very top of the reference.
",chatgpt
inquiry about intent feature in openai and azure ai search integration,"I am interested in understanding the logic behind the intent feature in the integration between OpenAI's chat completions and Azure AI Search. Specifically, I noticed that when appending and sending chat history for retrieval-augmented generation (RAG), there is a feature in the response named ""intent."" This output appears to be a reformulated query that is sent to the AI search to ensure it receives a meaningful request, allowing the search engine to complete its task effectively.
Take a look:
completion = client.chat.completions.create(
    messages=[
        {""role"": ""user"", ""content"": ""Who is the first man to land on the moon?""},
        {""role"": ""assistant"", ""content"": ""The first man to land on the moon was Neil Armstrong.""},
        {""role"": ""user"", ""content"": ""How old was he at that time?""}
    ],
    model=deployment,
    extra_body={
        ""dataSources"": [
            {
                ""type"": ""AzureCognitiveSearch"",
                ""parameters"": {
                    ""endpoint"": os.environ[""SEARCH_ENDPOINT""],
                    ""key"": os.environ[""SEARCH_KEY""],
                    ""indexName"": os.environ[""SEARCH_INDEX_NAME""],
                }
            }
        ]
    }
)

Not Only the chat completion returns the correct answer which is 38 years old but also return an 'intent' feature :
print(completion.choices[0].message.context['intent'])

[""How old was Neil Armstrong when he landed on the moon?"", ""What was Neil Armstrong's age when he landed on the moon?"", ""How old was Neil Armstrong when he was on the moon?""]

I’m very interested in understanding this mechanism, as I’m working with a stateless LangChain agent, and I would love to implement something similar.
I would like to know what prompt OpenAI uses to reformulate the query and send it to Azure AI Search using the ""intent"" feature. I want to be able to replicate this functionality using a prompt or some tools.
I am looking for a solution that allows me to summarize user inputs and send those reformulated queries to a LangChain agent. Is there any documentation or report that explains how to implement this intent feature in a standard chat completion? Additionally, are there any prompts used by OpenAI to reformulate and query the questions?
","It is the detected intent from chat history, check this
Getting intent considering all the chat history you are giving.
azure openai did not provided how it is creating intent publicly.
So, you either use intent detection model or a prompt with examples to create intent.
Here is an example using prompt.
import json

messages=[
        {""role"": ""user"", ""content"": ""Who is the first man to land on the moon?""},
        {""role"": ""assistant"", ""content"": ""The first man to land on the moon was Neil Armstrong.""},
        {""role"": ""user"", ""content"": ""How old was he at that time?""}
    ]

userquery = ','.join([json.dumps(i) for i in messages])

reformulation_prompt = """"""
You are an intelligent assistant tasked with helping users retrieve information from a database. The user may ask ambiguous, incomplete, or vague questions. Your task is to reformulate their query in a clear and precise way that can be sent to a search engine to retrieve the most relevant documents.

Here are some examples:
- User input: {""role"": ""user"",""content"": ""what is vector profiles?""}
  Reformulated queries: ""What are vector profiles?"", ""Definition of vector profiles"", ""How do vector profiles work?""

Now, please reformulate the following user input, also give 2 to 3 Reformulated queries:
User input:"""""" + userquery + """"""
Reformulated query:
""""""

t = client.completions.create(model=deployment,prompt=reformulation_prompt)
t.choices[0].text

and output:

Now you use this response in your langchain agent.
",chatgpt
openai assistants api error quot39assistants39 has no attribute 39files39quot,"I have the code below, which I have to integrate, but it's showing up that 'Assistants' have no attribute 'file'. This code is around 8 months old, and I understand that the API probably changed since then, but I cannot find any alternatives to this in the documentation. Does anyone have any experience migrating this code?
assistant_file = client.beta.assistants.files.create(
    assistant_id = st.session_state.assistant_id,
    file_id = st.session_state.file_id,
)

","The method you're trying to use (i.e., .files.create) doesn't exist.
Moreover, this code wouldn't work even with the OpenAI Assistants API v1.
If you want to create an assistant, use the following code (works with the OpenAI Assistants API v2):
from openai import OpenAI
client = OpenAI()

my_assistant = client.beta.assistants.create(
    instructions=""You are a personal math tutor. When asked a question, write and run Python code to answer the question."",
    name=""Math Tutor"",
    tools=[{""type"": ""code_interpreter""}],
    model=""gpt-4o"",
)

print(my_assistant)

If you want to create a file for an assistant, use the following code (works with the OpenAI Assistants API v2):
from openai import OpenAI
client = OpenAI()

my_file = client.files.create(
  file=open(""mydata.jsonl"", ""rb""),
  purpose=""assistants""
)

print(my_file)

",chatgpt
how to calculate embedding vectors using the openai api for mongodb vector search,"I am trying to follow the example from the official MongoDB documentation.
It mentioned that the search vector was based on the string ""historical heist"" using the OpenAI text-embedding-ada-002 model. However, when I tried to validate this with my own OpenAI API key, I got different results.
Why?
curl https://api.openai.com/v1/embeddings \
  -H ""Content-Type: application/json"" \
  -H ""Authorization: Bearer sk-xxxxx"" \
  -d '{
    ""input"": ""historical heist"",
    ""model"": ""text-embedding-ada-002"",
    ""encoding_format"": ""float""
  }'

My results are the following:
   ""data"": [
     {
108      ""object"": ""embedding"",
        ""index"": 0,
19      ""embedding"": [
8        -0.02273331,
97         -0.027646419,
          0.0030520316,
 3        -0.033631727,
1        -0.026207773,
4  0:00:01 --:-        0.0016854883,
-:--  0:0        -0.017983066,
0:0        0.006311038,
1         -0.0013012275,
201        -0.024823416,
54        0.015227924,
        0.005751188,
        0.0004003777,
        -0.008095773,

The example results are the following:
'queryVector': [-0.020156775, -0.024996493, 0.010778184, ....

","There was a discussion on the official OpenAI forum. People are saying that they get a different embedding vector for the identical input.
The first reply from Curt Kennedy suggests that there seems to be a difference between the OpenAI API endpoint and the Azure API endpoint for the same model.

Embeddings only return vectors. The vector is the same for the same
input, same model, and the same API endpoint. But we have seen
differences between the OpenAI endpoint and the Azure endpoint for the
same model. So a pick an endpoint and stick with it to avoid any
differences.

Later on, Curt Kennedy adds that some variance is expected.

Some variation is expected because of the random timing in the GPU’s,
and that floating point is not associative, and they are likely taking
the last hidden layer and scaling out to the unit hyper-sphere, which
would magnify the error for hidden states close to the origin.

I wrote the following Python code to make a test:
# Imports
import os
import numpy as np
from openai import OpenAI

# Initialize OpenAI client
client = OpenAI(
    api_key=os.getenv(""OPENAI_API_KEY""),
)

# Vector from MongoDB documentation
vector_from_mongodb = np.array([-0.020156775, -0.024996493, 0.010778184, -0.030058576, -0.03309321, 0.0031229265, -0.022772837, 0.0028351594, 0.00036870153, -0.02820117, 0.016245758, 0.0036232488, 0.0020519753, -0.0076454473, 0.0073380596, -0.007377301, 0.039267123, -0.013433489, 0.01428371, -0.017279103, -0.028358135, 0.0020160044, 0.00856761, 0.009653277, 0.0107912645, -0.026683854, 0.009594415, -0.020182934, 0.018077003, -0.015709465, 0.003310956, 0.0014878864, -0.015971072, -0.002411684, -0.029561523, -0.030450987, -0.013106481, -0.005385822, -0.018652538, 0.012642129, -0.005189617, 0.018835662, -0.0048102876, -0.0261214, -0.016167276, -0.007972456, 0.0023381072, -0.010058766, -0.009012341, 0.008358325, 0.018665617, 0.02163485, -0.012975678, -0.010745483, -0.002571918, -0.014479915, 0.007226877, 0.015003128, 0.013165343, -0.028279653, 0.0053727417, -0.020588424, -0.017383745, 0.023518417, 0.01262905, -0.011922712, 0.007638907, -0.0073249796, -0.014859244, -0.00001101736, 0.017043658, 0.010111088, 0.0074623227, 0.009555174, 0.008338705, -0.002240005, -0.0010603234, -0.004973792, 0.003391073, 0.021543289, 0.013341927, 0.0005980159, 0.010693162, 0.005336771, 0.016062634, 0.005768421, 0.005186347, 0.039790336, 0.0021942237, -0.0026275094, 0.010431555, 0.0042151334, -0.0050359233, 0.025768232, -0.021451725, 0.01833861, -0.01836477, -0.013433489, 0.030006256, -0.014793842, 0.017475309, 0.0020585153, -0.012975678, -0.017266022, -0.01593183, -0.014257549, 0.0010676811, -0.007887433, -0.0045911926, 0.00012303676, -0.0014976967, 0.03552615, 0.0065630507, -0.037435878, 0.011929252, -0.00939167, 0.016768971, 0.01223664, 0.007789331, -0.037200432, 0.013145722, 0.00896002, 0.021857215, 0.010333453, 0.021582529, -0.007089534, -0.007154935, -0.02485261, 0.0040254686, -0.00088864425, 0.023466095, -0.020719228, -0.006690584, -0.021006994, -0.018286288, 0.025545865, -0.0096598165, 0.008803056, -0.023021365, -0.040078104, 0.015408617, 0.017043658, -0.011242535, 0.0063537657, -0.026618453, 0.0071614753, -0.014623798, 0.00067322777, -0.00083427917, -0.028070368, 0.03714811, -0.004529061, 0.0054087127, 0.0028727653, 0.008384486, 0.010026066, -0.006190262, -0.0002493436, 0.0029953935, -0.026226042, -0.018417092, 0.009941043, 0.0036494094, -0.00982332, 0.013551212, 0.02574207, -0.0022645304, -0.0006004685, 0.012805633, -0.024303235, 0.008194821, -0.014179068, -0.02977081, 0.003095131, -0.0015941641, 0.029953934, 0.0052680993, 0.025388902, -0.031392768, -0.021386323, 0.014898485, 0.022419669, 0.00897964, 0.013243824, 0.006854088, 0.0066415328, -0.003839074, -0.01877026, 0.021216279, -0.015055449, -0.0015508354, 0.013211124, -0.008783435, 0.0052157775, -0.68938524, -0.01221702, -0.04125533, -0.016232677, 0.020039052, -0.0026422248, -0.0037050007, 0.0064682183, -0.0047579664, 0.0032749851, -0.0035382267, 0.031942144, -0.00035643874, -0.011628405, -0.043086577, -0.0196074, -0.0066088317, -0.014872325, 0.028331975, 0.010294212, -0.013930541, 0.031994462, -0.018626377, 0.017462227, 0.026343765, -0.010274592, 0.0046827546, -0.029430721, -0.011746128, 0.0024362097, 0.0023054064, 0.0027730279, -0.002406779, 0.003917556, 0.059436977, 0.008665713, -0.0018901062, 0.06037876, 0.017880797, 0.05185039, 0.0067102043, -0.020300657, 0.005604917, 0.018704858, 0.012073136, 0.0144145135, 0.012413224, -0.0074819434, 0.015801027, -0.0061412104, 0.008613391, -0.0039077457, -0.0036232488, 0.008469507, 0.014087505, 0.0124066835, 0.019267311, -0.002573553, 0.005055544, -0.009417831, -0.009103903, 0.011150973, -0.012046975, 0.0058567133, -0.0053727417, 0.018260127, -0.005588567, 0.015591742, 0.007495024, -0.02567667, 0.024211673, 0.021386323, -0.012890656, -0.016114954, 0.009515933, 0.009679437, 0.025532786, -0.0076454473, -0.02575515, 0.008319084, -0.0068410076, -0.017082898, -0.026173722, -0.0049901423, 0.01918883, -0.008646091, -0.031759016, 0.014820003, 0.011850771, 0.01836477, 0.012700991, -0.0011437106, 0.005058814, 0.0151993325, -0.0060692686, 0.027416352, 0.0037344315, 0.0013546307, 0.018325528, -0.03152357, -0.008809595, 0.014649959, -0.008345244, 0.0066415328, -0.005523165, 0.0043492066, -0.0015892589, 0.0048855, 0.034453563, -0.03837766, 0.0068410076, -0.0042151334, -0.0067429054, 0.0055689462, -0.011733048, -0.0212032, 0.016847452, -0.0022220195, 0.0059351954, -0.00449963, 0.02251123, -0.01020265, 0.023361452, -0.0032455544, 0.016180357, 0.0049443613, -0.0064747585, -0.03259616, 0.012321662, 0.020104453, 0.009954124, -0.019411195, 0.0048102876, -0.000392614, 0.012184318, 0.0044276887, 0.005634348, -0.020562263, 0.015722545, -0.005179807, -0.0067952266, 0.0027861083, 0.0024198592, -0.0020585153, 0.0018525004, -0.045100946, -0.010176489, -0.012956058, 0.0013497255, 0.0105361985, 0.003796563, -0.0106016, -0.013126101, 0.0050359233, 0.015003128, -0.0075800456, -0.015722545, -0.01755379, -0.00978408, -0.02940456, 0.017606111, 0.016612006, -0.016912855, 0.025441224, 0.0054741143, 0.00448001, 0.009470152, 0.015382457, -0.008332164, -0.019123428, 0.024564842, 0.016860534, 0.008286383, -0.007141855, 0.006559781, 0.016625088, -0.01840401, -0.011602244, -0.00489858, -0.0073184394, -0.008809595, -0.0018459603, -0.01629808, -0.005542786, 0.0064257076, 0.010379234, 0.014663039, 0.034872133, -0.013355007, 0.027285548, 0.011654565, -0.004032009, 0.02323065, -0.02653997, -0.0009941043, 0.002946342, 0.010667001, 0.008345244, 0.018626377, 0.04821406, 0.031392768, 0.010281132, 0.026069079, 0.002735422, 0.01182461, -0.01593183, 0.006585941, -0.010071847, 0.024564842, -0.0025261368, 0.004293615, -0.0068606283, -0.0066448026, -0.0074100015, -0.0014347476, 0.021530207, -0.010418476, 0.018495573, -0.0034924455, -0.014165987, -0.004784127, -0.012472086, 0.004417878, -0.0030313642, -0.010084927, -0.010954768, 0.01508161, 0.0010047321, 0.0042347535, -0.03345946, -0.00027346043, 0.014793842, -0.019882087, 0.012772933, 0.021490967, 0.0031932332, 0.0093589695, 0.00090172456, 0.0048102876, 0.0070045115, -0.0045584915, 0.015840268, 0.024342475, -0.0091300635, 0.0039796876, 0.003796563, 0.025022654, -0.008103259, -0.025022654, 0.03021554, -0.008201361, -0.0070502926, 0.0011821339, 0.021072397, 0.004849529, -0.02495725, 0.012184318, 0.0019228071, -0.007226877, 0.020562263, 0.018861823, -0.0017593032, 0.01345965, 0.0022727058, 0.003023189, -0.026971621, -0.0030558899, 0.017723834, -0.01998673, -0.010608139, 0.011491061, -0.025179617, 0.0069652707, 0.003924096, 0.021177039, 0.0045650317, -0.0009973744, 0.007586586, -0.004032009, -0.008129419, -0.010091467, -0.04279881, 0.019790525, 0.01595799, 0.0044309585, -0.0033747226, -0.018665617, -0.012818714, -0.016206518, 0.014113666, -0.0020912162, 0.01427063, -0.020248337, -0.0112752365, -0.020588424, -0.011039791, 0.008744194, -0.015147011, 0.0022269245, -0.010438096, -0.0017772885, -0.028750544, -0.008861917, -0.016991336, 0.033668745, 0.034636687, 0.009888723, 0.0023953337, 0.006991431, -0.003346927, 0.003103306, -0.0044571194, 0.011249076, 0.0033779927, 0.00012446742, -0.0027027212, -0.025859794, -0.011942333, 0.02694546, 0.028227331, 0.0064289775, -0.03385187, -0.020719228, 0.00489531, 0.10663077, 0.041752383, -0.021700252, -0.008103259, 0.0049574412, -0.01675589, -0.020182934, -0.006585941, 0.007684688, -0.002859685, 0.027023941, 0.00856107, 0.0037017306, 0.016978256, 0.025885954, -0.010372694, 0.0025964435, 0.011706887, 0.021360163, -0.021674091, -0.024983412, 0.0034074234, 0.0032030435, 0.022262705, -0.01266829, -0.002249815, 0.032779284, -0.0034303141, -0.016101874, -0.005156916, -0.0212032, 0.005362931, 0.009077743, -0.013917461, -0.0017315074, 0.010980929, -0.019450437, 0.013865139, 0.028227331, -0.008757275, -0.0033649125, -0.012857955, 0.011039791, 0.009764459, 0.00029594224, -0.026317604, 0.025048813, 0.037749805, -0.025807472, -0.005425063, 0.021791814, -0.010012985, -0.00066995766, -0.016952096, 0.0031147513, -0.016598927, 0.0084368065, 0.004787397, -0.0064355177, 0.0015164997, -0.021216279, -0.023845425, 0.013969782, -0.011255615, 0.0042576445, -0.024250913, -0.009908343, -0.02289056, -0.023361452, -0.010987469, -0.013394248, 0.0032553647, -0.019018786, 0.021438645, 0.029587684, -0.010490417, 0.01263559, -0.018417092, -0.008731114, 0.01875718, -0.0072399573, -0.029090632, -0.017736914, -0.04031355, -0.019712042, 0.012772933, -0.030320182, -0.022341188, -0.02041838, 0.011752668, 0.028829027, -0.017043658, 0.024996493, 0.006334145, -0.0024263994, -0.0077370093, 0.017802317, 0.017396826, 0.030398665, 0.011464901, 0.03016322, -0.014558396, -0.0036690298, -0.009954124, -0.006703664, -0.00035705187, -0.014519156, 0.0075342646, -0.00896656, 0.040078104, 0.024420958, -0.016886694, -0.00092543266, -0.0017494928, 0.01672973, 0.016533526, 0.002648765, 0.0187441, -0.0055460557, 0.004735076, 0.03186366, 0.0003435628, 0.007495024, 0.023453014, -0.012504786, -0.0074557825, -0.0027844731, -0.04570264, 0.010477337, 0.0030101088, -0.015670223, 0.03351178, -0.020261416, 0.00050849747, -0.009653277, -0.023466095, -0.007396921, -0.011909632, 0.003436854, -0.02979697, -0.039031677, -0.014584557, 0.0019555078, 0.0042216736, -0.0060594585, -0.023400694, -0.00023462824, -0.017763074, -0.016180357, 0.0132372845, -0.020496862, -0.007390381, -0.0058697937, -0.0096598165, 0.0039796876, -0.019306554, -0.012622509, -0.0012287326, 0.010863206, 0.024368636, 0.027730279, 0.016795132, 0.019908248, -0.006343955, 0.0014592733, -0.005425063, 0.019450437, 0.004532331, -0.031889822, 0.008476048, 0.019712042, -0.00047906674, -0.0028286192, 0.011883471, -0.012426305, 0.0041497317, 0.001756033, -0.0013603533, -0.008031317, -0.010281132, -0.0071222344, -0.026330685, -0.007920134, -0.026866978, -0.03026786, -0.0015328501, 0.027442513, -0.005922115, 0.005186347, 0.003436854, 0.036703378, -0.0053204205, 0.013165343, 0.0016939015, -0.0041431915, -0.017213702, -0.012439385, -0.015212413, 0.014532236, 0.0093589695, -0.0053400407, 0.017422987, -0.028881347, -0.014179068, 0.011307937, 0.040104263, -0.007593126, -0.000631943, -0.0003404971, -0.0055198953, -0.00063030794, -0.004852799, -0.0024214943, -0.029718488, 0.023322212, 0.011079031, 0.012988758, 0.0071614753, -0.034034993, -0.01551326, 0.004012388, 0.006442058, 0.032386873, 0.0076519875, 0.0465921, 0.01757995, -0.0135381315, -0.016978256, 0.024983412, 0.0003280299, 0.0026209692, 0.022380428, -0.010640841, 0.0027648527, -0.007959375, -0.005922115, 0.0075342646, -0.03597088, -0.018874902, 0.03510758, -0.015356296, 0.004597733, -0.0015328501, -0.019947488, -0.013446569, 0.020614585, -0.0056016473, 0.035186063, 0.0005248479, -0.030712591, -0.019136509, 0.004202053, -0.010339993, 0.014754602, 0.0072922786, -0.015460939, 0.027494833, -0.02974465, -0.0033616424, 0.0105819795, -0.028881347, 0.01720062, -0.0073707607, 0.0054479535, -0.0019522378, -0.018103164, -0.009110443, -0.024630243, 0.005624538, 0.01879642, -0.019345794, -0.0027681228, -0.015971072, 0.022354268, -0.0038194535, 0.018901063, -0.017357586, -0.02493109, 0.006703664, -0.0021173768, -0.005667049, -0.004535601, -0.016441964, 0.0034172337, -0.02447328, -0.003310956, -0.02078463, -0.011589164, 0.013263445, -0.014728441, -0.0187441, -0.019476596, 0.013224204, 0.015238573, -0.012380524, 0.00019058435, 0.010778184, 0.025022654, -0.036127847, 0.01470228, -0.007671608, 0.032857765, 0.002982313, 0.009829861, 0.0072203367, -0.0028237142, 0.025990596, -0.029012151, 0.0016955365, 0.012033895, -0.0049901423, -0.013629694, 0.0072464976, 0.0012704261, 0.0018868363, 0.017043658, 0.00448001, -0.009555174, -0.016520444, 0.02570283, -0.00939167, 0.01998673, 0.002001289, -0.023662299, 0.0041072206, -0.024839528, -0.007396921, -0.0034793653, -0.032020625, -0.0036003583, -0.010719323, 0.022995204, -0.01757995, -0.0043851775, -0.023884665, -0.018430172, -0.009018881, 0.00091562246, -0.0055689462, -0.012537487, 0.016455043, 0.03264848, 0.018560974, 0.014623798, 0.0025555675, -0.0060986993, 0.0058272826, -0.008462967, -0.012720612, -0.0042576445, -0.027207067, 0.014152907, -0.0029610575, 0.010241891, -0.011222915, -0.01140604, -0.022197304, -0.003433584, -0.0056899395, 0.004372097, 0.061896075, -0.005846903, -0.011863851, 0.004535601, -0.0074819434, 0.016847452, -0.0012647035, 0.021085477, 0.02409395, -0.030137058, -0.0012197399, 0.009607496, -0.008220982, -0.007893973, -0.007893973, 0.007972456, 0.010012985, 0.009143144, 0.0044734697, 0.015264734, -0.0032520946, 0.002208939, 0.011968493, -0.0012998568, -0.0114322, -0.056454662, -0.013217663, 0.0017593032, -0.00244275, -0.021399405, -0.010732403, 0.00694565, 0.0033207664, 0.0025539326, 0.01102671, -0.012589809, 0.010706242, -0.012413224, 0.01427063, -0.000049970913, -0.0056016473, 0.027965724, 0.018652538, -0.009535554, 0.0068867886, 0.004699105, -0.001245083, -0.009071202, -0.0032946058, -0.03756668, 0.034453563, -0.00408106, 0.013361547, -0.0065107294, 0.009300108, -0.016415803, 0.0059973267, -0.017422987, 0.0048822295, 0.022158062, -0.025611266, 0.01022227, -0.0061771814, -0.014218308, -0.00044636594, -0.019110348, -0.013747416, -0.013629694, -0.021896457, -0.0051634563, -0.020509942, -0.018731019, 0.0043328563, -0.032386873, -0.023086766, 0.0196074, 0.20614585, -0.014649959, -0.009712138, 0.01345965, -0.010928608, 0.0196074, 0.015814107, 0.017383745, -0.0024656404, 0.021399405, 0.013668935, -0.0063864663, -0.0015303975, -0.0012924991, -0.0030575248, -0.015539421, -0.009692517, -0.012190859, -0.02287748, 0.002936532, 0.00069325697, 0.013158802, -0.0070110518, -0.013629694, 0.01585335, -0.019829765, 0.013747416, 0.016036473, 0.011693806, 0.0071483953, -0.010156869, -0.013799738, -0.00034703725, -0.010706242, -0.02289056, 0.0039339066, -0.0015835363, -0.014532236, 0.012445925, -0.00009779583, 0.0053335004, 0.0055329753, -0.005281179, -0.007475403, 0.00040385488, -0.012942977, -0.015277814, 0.012956058, 0.00006162057, 0.007056833, -0.02571591, -0.018731019, -0.0061771814, 0.034427404, 0.0010570535, 0.0079528345, 0.024172433, 0.021386323, -0.019803606, -0.006821387, -0.011262156, 0.026605371, -0.0036951904, -0.008207901, -0.019698963, 0.042981934, -0.026212962, 0.00856761, 0.015173172, 0.0024149541, -0.0008036222, -0.005752071, -0.02898599, -0.008443347, -0.0064224373, -0.014479915, 0.036467932, -0.00086820626, 0.026396086, 0.002001289, -0.0074361623, -0.0086918725, -0.007835112, 0.021464806, 0.0008984545, -0.02489185, 0.019515838, 0.026644614, -0.0137212565, 0.00448982, 0.004211863, -0.022380428, -0.014100585, -0.01629808, 0.0074884836, 0.02652689, 0.011634945, 0.049626734, -0.023583818, -0.0021958589, -0.015735626, 0.02733787, 0.0036428692, -0.031261966, -0.012674831, 0.006196802, -0.009535554, 0.016886694, 0.010771644, -0.021490967, 0.014100585, -0.007063373, 0.00043778197, -0.012151618, -0.0058894143, 0.009182385, -0.005768421, -0.013995943, 0.004725266, -0.01347273, -0.020797709, -0.018037762, 0.020274498, 0.011595704, 0.0017364125, -0.02248507, 0.005954816, 0.0062196925, -0.014257549, -0.025127295, 0.015356296, 0.005179807, 0.021726413, -0.0034499345, -0.017082898, 0.019803606, 0.005209238, 0.0005939283, -0.0035807376, -0.011661106, 0.006559781, 0.0033207664, 0.0017233322, -0.00059924216, -0.000341519, -0.0140221035, 0.00084286317, -0.003306051, -0.005634348, -0.00816212, -0.009319728, -0.024447119, -0.014950806, -0.024564842, 0.0137212565, -0.010084927, 0.000044886958, -0.0033943432, 0.0025359471, 0.012478625, -0.023086766, 0.014519156, 0.020876192, -0.023282971, -0.0030804155, -0.014545316, -0.16805595, 0.01262905, 0.020719228, -0.012413224, 0.026592292, -0.0024198592, 0.041072205, 0.002658575, -0.013708176, -0.0068867886, -0.0018639456, 0.000031627806, -0.043452825, -0.028018046, -0.0105819795, 0.01266829, -0.009450532, 0.008292923, 0.0058534434, -0.006782146, 0.032229908, 0.0005955633, -0.0023103117, 0.003140912, 0.00037687673, -0.0049247406, -0.008070557, 0.017279103, -0.012759852, -0.011608784, -0.019450437, 0.016167276, 0.02248507, 0.030529467, 0.015905669, 0.0061150496, -0.016834373, 0.017344505, 0.006667693, -0.005461034, 0.0066742334, 0.01998673, 0.024591003, -0.007717389, 0.0096598165, 0.03225607, 0.018626377, -0.020248337, 0.0017740185, 0.012589809, 0.0014927916, -0.040235065, 0.01713522, 0.016206518, 0.017776156, 0.024734886, 0.0040516295, -0.009627116, 0.002001289, -0.010496957, -0.0121058365, -0.017266022, 0.008279843, -0.02122936, -0.01349889, -0.02251123, 0.004820098, -0.000071533, -0.022628954, 0.015238573, -0.01833861, -0.016572766, -0.0031523572, -0.008064018, 0.019973649, 0.0089207785, -0.03228223, 0.0040647094, -0.004784127, -0.0017920039, -0.0013775212, 0.047246117, 0.0030804155, -0.010660461, 0.02982313, 0.006088889, -0.019371955, -0.024447119, -0.011687267, -0.013708176, 0.017187541, -0.018286288, 0.019267311, 0.0011960318, 0.0046271635, 0.016886694, 0.0069129495, 0.00029062838, 0.013629694, -0.016494283, -0.017069818, 0.0058240127, 0.013943622, 0.001675916, 0.01347273, 0.023335291, 0.008129419, 0.0047187256, 0.032099105, 0.0007701039, 0.0068344674, 0.0004672127, -0.00610851, 0.026396086, -0.010738943, 0.024591003, 0.008220982, -0.019908248, 0.024682565, -0.009404751, 0.0594893, -0.009731758, -0.022628954, 0.013865139, -0.016049553, 0.0033371167, -0.107572556, -0.022341188, 0.008050937, -0.0089731, 0.004983602, 0.010771644, -0.013034539, -0.013368088, -0.0071287747, 0.0091758445, -0.017409906, -0.022118822, -0.011170594, -0.010908987, 0.050490037, 0.014584557, 0.018312449, 0.0014968792, -0.0057161, 0.024342475, -0.02699778, 0.020091372, -0.00094587065, -0.021347083, -0.003711541, 0.0016677409, -0.030738752, 0.040208906, 0.008109799, -0.017527629, -0.0009058122, 0.017776156, 0.0052779093, -0.0046206233, 0.0067952266, -0.01226934, -0.009162764, -0.01595799, 0.021582529, -0.027390191, -0.00011210243, -0.003145817, 0.01672973, -0.009999905, 0.003832534, -0.01793312, -0.0004868332, 0.027573315, 0.001756033, -0.012112376, -0.009718678, 0.0025473924, -0.027547155, -0.019084187, 0.010693162, 0.025558947, -0.02168717, -0.0068802484, -0.010869746, -0.028698223, -0.0051634563, -0.012131997, -0.014963887, 0.022210384, 0.01510777, -0.0026504, -0.013577373, 0.0058599836, 0.011281776, -0.0009393305, -0.00204053, 0.030110897, -0.029326078, 0.006491109, -0.01671665, 0.0006049648, -0.024342475, -0.008325624, 0.03722659, -0.007710849, -0.0055656764, -0.02043146, -0.015317055, -0.015212413, 0.002815539, 0.022262705, 0.00818828, 0.021778734, -0.0037409717, -0.02485261, 0.0033779927, 0.013217663, -0.0059319255, -0.018940303, 0.02409395, 0.015761785, -0.009672897, 0.011301396, -0.011582624, 0.0029725027, -0.015343216, -0.00735114, -0.075761214, 0.016821291, 0.0028040938, 0.0017233322, 0.01595799, -0.0054741143, -0.007096074, -0.011641486, -0.003554577, 0.009829861, -0.037828285, 0.024983412, 0.003793293, -0.010895907, -0.011916172, -0.017893879, 0.029640006, 0.0027452323, 0.004977062, 0.0138913, 0.0132830655, 0.010725862, 0.014205228, -0.003839074, 0.020470701, 0.0048626093, -0.010967849, 0.035343025, -0.004568302, -0.007665068, 0.0040091183, -0.02367538, -0.006821387, 0.012112376, -0.0012475356, -0.02041838, -0.030869557, -0.004865879, 0.036127847, 0.019528918, 0.00087147637, 0.0016366751, -0.006072539, -0.012380524, -0.016886694, 0.0014224849, 0.0058632535, 0.0053138803, 0.024525601, -0.008227522, 0.016167276, 0.021373244, -0.019855926, -0.011602244, -0.012223559, 0.009116983, 0.00448001, 0.0027027212, 0.0112294555, -0.025048813, 0.005958086, 0.005578757, 0.012040435, -0.019528918, -0.008096718, -0.023439934, 0.00047497914, 0.0073315194, 0.025061894, -0.016455043, 0.003992768, 0.002038895, -0.0003484679, 0.004444039, -0.014846164, 0.0018263398, 0.017305264, -0.0047154557, -0.006729825, 0.011288317, -0.009764459, -0.03220375, -0.015369376, 0.009594415, 0.031078842, 0.020967754, -0.007802411, 0.022354268, -0.010778184, 0.01833861, 0.004581382, 0.0072399573, 0.010673542, -0.012112376, -0.023073684, 0.0066448026, -0.027887244, 0.0063504954, 0.012956058, 0.032151427, -0.018103164, 0.0048855, -0.018286288, -0.036938824, -0.012354363, 0.020039052, 0.004921471, -0.03790677, 0.0212686, 0.02982313, 0.015434778, 0.0041039507, -0.016245758, 0.012171238, -0.006415897, 0.0072464976, -0.0024362097, -0.025218857, -0.021399405, 0.036860343, 0.0056572384, 0.017004417, 0.03432276, -0.013825899, 0.028724384, 0.008528369, 0.018652538, -0.02443404, -0.025637427, 0.006497649, -0.015447859, 0.01917575, -0.016520444, -0.008678793, -0.021072397, 0.015840268, -0.006324335, 0.025925195, -0.03594472, 0.0384823, 0.01308032, 0.0054217926, 0.00448328, -0.027207067, -0.016847452, 0.0036003583, 0.01061468, -0.019816685, -0.004659864, 0.023387613, -0.005461034, 0.004326316, 0.0037278912, -0.007540805, 0.00860031, 0.0015524705, 0.020039052, -0.0028367946, 0.0049509015, 0.009162764, 0.009705598, 0.013982862, 0.004852799, 0.0061869915, -0.0083910255, 0.012975678, -0.034558207, -0.029064473, -0.03058179, -0.019450437, 0.01062122, -0.014179068, -0.010012985, 0.007874353, -0.014126746, -0.009731758, -0.03398267, -0.000115883464, -0.0029725027, -0.024290156, 0.012864495, -0.00937859, -0.035264544, 0.0027959184, 0.012982218, -0.012609429, 0.0065270797, 0.010712783])

# Loop to generate embeddings and calculate dot products
for _ in range(3):
    # Generate an embedding
    response = client.embeddings.create(
        model=""text-embedding-ada-002"",
        input=""historical heist"",
        encoding_format=""float"",
    )

    # Extract the embedding vector from the response
    vector = response.data[0].embedding

    # Calculate the dot product between the generated embedding and the vector from MongoDB
    dot_product = np.dot(vector, vector_from_mongodb)
    
    # Print the first 5 elements of the embedding
    print(f""\nEmbedding vector (first 5 elements): {vector[:5]}"")

    # Print the dot product value
    print(f""Dot product: {dot_product}\n"")
        
    print(""------------------------------------"")

After I ran it, I got the following output:
Embedding vector (first 5 elements): [-0.02273331, -0.027646419, 0.0030520316, -0.033631727, -0.026207773]
Dot product: 0.9853400963855683

------------------------------------

Embedding vector (first 5 elements): [-0.02273331, -0.027646419, 0.0030520316, -0.033631727, -0.026207773]
Dot product: 0.9853400963855683

------------------------------------

Embedding vector (first 5 elements): [-0.02273331, -0.027646419, 0.0030520316, -0.033631727, -0.026207773]
Dot product: 0.9853400963855683

------------------------------------

As you can see:

The embedding vector is identical all three times.
The dot product is identical all three times (as a consequence of the first point) and very high. This means that the embedding vectors (i.e., the one from MongoDB documentation and generated ones) are very similar.

The discussion on the forum went on about whether this small difference between embedding vectors for the identical input is problematic or not. In other words, should you care about the difference or not? I don't think you should, as long as you get meaningful solutions using them.
Moreover, I would care even less if you got identical embedding vectors for a given API endpoint, model, and input. Don't compare an embedding vector you get when using the OpenAI API endpoint and MongoDB API endpoint. Compare an embedding vector you get when using the OpenAI API endpoint multiple times. Compare an embedding vector you get when using the MongoDB API multiple times. The difference in embedding vectors between the OpenAI API endpoint and MongoDB API endpoint might be of a technical nature, as Curt Kennedy explained on the official OpenAI forum.
",chatgpt
openai api returns null when i retrieve the finetuned model,"I'm fine-tuning a model and generating actions from text. I create a train.jsonl file, upload it, and fine-tune the model. However, when I try to get the name of the model I just created, it returns null.
I fine-tune a model like this:
const model = await openai.fineTuning.jobs.create({
  training_file: process.env.FILE_ID,
  model: 'babbage-002',
})

Then I try to retrieve the fine-tuned model like this:
const response = await openai.fineTuning.jobs.retrieve(
    process.env.FINE_TUNE_ID,
)

But this is the response I get from the OpenAI API:
data:  {
  object: 'fine_tuning.job',
  id: 'ftjob-NSFvxzJtTSfR5jcqQTfeDTCo',
  model: 'babbage-002',
  created_at: 1722546992,
  fine_tuned_model: null,
  organization_id: 'org-GLjhkXwkbQrLOvHk0762UcmL',
  result_files: [],
  status: 'running',
  validation_file: null,
  training_file: 'file-Lh1C4Vv1HDIv7LxXUGh9mIL9',
  hyperparameters: { n_epochs: 9, batch_size: 1, learning_rate_multiplier: 16 },
  trained_tokens: null,
  error: {},
  user_provided_suffix: null,
  seed: 1564492262,
  estimated_finish: null,
  integrations: []
}

","The fine-tuning job hasn't finished yet.
The fine-tuning flow is the following:

Create a fine-tuning job.
Fine-tuning is in progress.
Retrieve the fine-tuning job.

Try to run the following code to see if the fine-tuning is still in progress.
import OpenAI from ""openai"";

const client = new OpenAI();

async function main() {
  const list = await client.fineTuning.jobs.list();

  for await (const fineTune of list) {
    console.log(fineTune);
  }
}

main();

",chatgpt
openai api error when finetuning quotthis is not a chat model and thus not supported in the v1chatcompletions endpointquot,"I've fine-tuned a model using the OpenAI API. Now I want to use this fine-tuned model.
This is my code:
 const response = await openai.chat.completions.create({
    model: process.env.FINE_TUNE_MODEL,
    messages: [
      {
        role: ""system"",
        content: prompt,
      },
      {
        role: ""user"",
        content: inputText,
      },
    ],
    temperature: 0,
    top_p: 1,
    frequency_penalty: 0,
    presence_penalty: 0,
});

But there's a 404 error while executing this function.
This is the error message:

NotFoundError: 404 This is not a chat model and thus not supported in
the v1/chat/completions endpoint. Did you mean to  use v1/completions?

How can I use the fine-tuned model to generate a text?
","Problem
You fine-tuned one of the non-chat models but wanted to use the API endpoint, which is compatible only with the chat models.
The API endpoint you need to use for a fine-tuned model depends on the model you fine-tune.
As of today, you can fine-tune the following models:

gpt-4o-mini-2024-07-18 (chat model),
gpt-4o-2024-05-13 (chat model),
gpt-4-0613 (chat model),
gpt-3.5-turbo-0125 (chat model),
gpt-3.5-turbo-1106 (chat model),
gpt-3.5-turbo-0613 (chat model),
babbage-002 (non-chat model), and
davinci-002 (non-chat model).

Solution
Use the following rule:

If you fine-tune a chat model, use the /v1/chat/completions API endpoint.
If you fine-tune a non-chat model, use the /v1/completions API endpoint.

In other words:

If you fine-tune a chat model, use the client.chat.completions.create method.
If you fine-tune a non-chat model, use the client.completions.create method.

",chatgpt
openai api error quotno module named 39openaiembeddings_utils39 39openai39 is not a packagequot,"I want to use openai.embeddings_utils import get_embeddings
So already install openai
Name: openai
Version: 0.26.5
Summary: Python client library for the OpenAI API
Home-page: https://github.com/openai/openai-python
Author: OpenAI
Author-email: support@openai.com
License: 
Location: /Users/lima/Desktop/Paprika/Openai/.venv/lib/python3.9/site-packages
Requires: aiohttp, requests, tqdm
Required-by: 

This is my openai
But why not use openai.embeddings_utils??
","For my case, check the version of openai.
openai.embeddings_utils does not exist in latest openai 1.2.0, but exists in 0.27.7
",chatgpt
openai api  asynchronous api calls,"I work with the OpenAI API. I have extracted slides text from a PowerPoint presentation, and written a prompt for each slide. Now, I want to make asynchronous API calls,
so that all the slides are processed at the same time.
this is the code from the async main function:
for prompt in prompted_slides_text:
    task = asyncio.create_task(api_manager.generate_answer(prompt))
    tasks.append(task)
results = await asyncio.gather(*tasks)

and this is generate_answer function:
@staticmethod
    async def generate_answer(prompt):
        """"""
        Send a prompt to OpenAI API and get the answer.
        :param prompt: the prompt to send.
        :return: the answer.
        """"""
        completion = await openai.ChatCompletion.create(
                model=""gpt-3.5-turbo"",
                messages=[{""role"": ""user"", ""content"": prompt}]

        )
        return completion.choices[0].message.content

the problem is:

object OpenAIObject can't be used in 'await' expression

and I don't know how to await for the response in generate_answer function
Would appreciate any help!
","For those landing here, the error here was probably the instantiation of the object. It has to be:
client = AsyncOpenAI(api_key=api_key)

Then you can use:
        response = await client.chat.completions.create(
        model=""gpt-4"",
        messages=custom_prompt,
        temperature=0.9
    )

",chatgpt
getting model not found for text completion g4f,"I want to prompt g4f with a simple question but the exception I get each time was model not found:
import g4f

allowed_models = [
    'code-davinci-002',
    'text-ada-001',
    'text-babbage-001',
    'text-curie-001',
    'text-davinci-002',
    'text-davinci-003'
]

response = g4f.Completion.create(
    model='text-davinci-003',
    prompt='say this is a test'
)

print(response)

any idea how to remedy to this?
","
For OpenAI Provider, g4f only support 3 models: gpt-3.5-turbo, gpt-4, gpt-4-turbo.
",chatgpt
using openai to call openaicompletioncreate the response is shorter than what chatgpt returns,"I have a paid account for chatgpt and using the latest 4 version, when I ask it to answer a question it send back a response that is like 4k characters.  When I use the api for same question, I get back a much shorter response that is about 1k characters.  I decided to ask  chatgpt what parameters to use in the api call to get back the same response and I pasted it below.   It used max tokens as 30 so I upped it to 4k, but the response is still too short. I then modified the engine to use davinci003 from 002 with no luck.
Any ideas on what I can change, I spent about 2 hours researching and still can't get it to work.
def generate_completion(prompt):
    response = openai.Completion.create(
      engine=""text-davinci-003"",
      prompt=prompt,
      max_tokens=4000,
      n=1,
      stop=None,
      temperature=0.8,
      top_p=1,
      frequency_penalty=0,
      presence_penalty=0,
    )
    generated_text = response.choices[0].text.strip()
    return generated_text

","You need to use Chat instead of Completion.
For Python, the example code is as follows:
completion = openai.ChatCompletion.create(
  model=""gpt-3.5-turbo"",
  messages=[
    {""role"": ""user"", ""content"": ""Hello!""}
  ]
)

If you have access to GPT-4 API, then you can change the model to gpt-4. Note that having a paid account doesn't guarantee access to GPT-4 API. Otherwise, you can use model gpt-3.5-turbo or gpt-3.5-turbo-0301, which is accessible to all but less powerful than the gpt-4 model.
",chatgpt
importerror cannot import name 39iterator39 from 39typing_extensions39 databrickspythonlibpython310sitepackagestyping_extensionspy,"I'm facing the following issue on Azure Databricks when trying to import from openai import OpenAI after installing openai.
Here's the error:
ImportError: cannot import name 'Iterator' from 'typing_extensions' (/databricks/python/lib/python3.10/site-packages/typing_extensions.py)

I looked up similar issues and found that using --force-reinstall like this:
pip install --force-reinstall typing-extensions==4.5
pip install --force-reinstall openai==1.8

Worked for some users. However, it did not work in my case.
How do I resolve this?
","To resolve this erratic issue simply run dbutils.library.restartPython() after installing openai and before importing from openai import OpenAI like this
!pip install openai==1.42.0

# Restart the Python process on Databricks
dbutils.library.restartPython()

from openai import OpenAI

As the Databricks Documentation says:
You can programmatically restart the Python process on Databricks (using dbutils.library.restartPython()) to ensure that locally installed or upgraded libraries function correctly in the Python kernel for your current SparkSession.
",chatgpt
send google drive api file download to openai,"I'm trying to send a file from Google Drive to OpenAI API via chat completion
I've already authenticated and I'm able to list files and get the file in question. The Google API returns the data as a Blob.
The problem I'm running into is that when I try to convert the Blob to a base64 encoded string, I get a RangeError or DOMException from the Supabase Edge Function
Here's a snippet from the code I have right now (post auth and files.get):
const fileDownload = await driveApi.files.get({
  fileId: 'file_id_string',
  alt: 'media',
});
console.log('file download', fileDownload.data);

const blob: Blob = fileDownload.data;
const arrayBuffer = await blob.arrayBuffer();
const uint8Array = new Uint8Array(arrayBuffer);

// Attempt 1
const charCodeString = String.fromCharCode(...uint8Array); // <-- Fails here
const base64String = btoa(charCodeString);

// Attempt 2
const decodedString = new TextDecoder().decode(uint8Array);
const base64String = btoa(decodedString); // <-- Fails here

// Uncomment once I get the base64String right
// await openai.chat.completions.create({
//   model: 'gpt-4o-mini',
//   response_format: 'json_object',
//   messages: [
//     { role: 'user', content: [{ url: base64String, type: 'image_url' }] },
//   ],
// });

# Attempt 1 error message
 RangeError: Maximum call stack size exceeded
    at Object.handler (file:///repos/supabase-personal/supabase/functions/openai/index.ts:32:38)
    at Object.runMicrotasks (ext:core/01_core.js:642:26)
    at processTicksAndRejections (ext:deno_node/_next_tick.ts:39:10)
    at runNextTicks (ext:deno_node/_next_tick.ts:48:3)
    at eventLoopTick (ext:core/01_core.js:175:21)
    at async respond (ext:sb_core_main_js/js/http.js:163:14)

# Attempt 2 error message
DOMException: The string to be encoded contains characters outside of the Latin1 range.
    at new DOMException (ext:deno_web/01_dom_exception.js:116:20)
    at btoa (ext:deno_web/05_base64.js:52:13)
    at Object.handler (file:///repos/supabase-personal/supabase/functions/openai/index.ts:37:26)
    at Object.runMicrotasks (ext:core/01_core.js:642:26)
    at processTicksAndRejections (ext:deno_node/_next_tick.ts:39:10)
    at runNextTicks (ext:deno_node/_next_tick.ts:48:3)
    at eventLoopTick (ext:core/01_core.js:175:21)
    at async respond (ext:sb_core_main_js/js/http.js:163:14)

I really appreciate any help that can be given!
I'm expecting, based on the OpenAI documentation, to convert the file to base64 to send to the chat completion endpoint. Is there an easier, or better way to do this?
","I believe your goal is as follows.

You want to download files except for Google Docs files (Document, Spreadsheet, Slides and so on) as base64 data.
You want to achieve this using googelapis for Node.js.

In this case, how about the following modification?
Modified script:
const fileId = ""file_id_string""; // Please set your file ID.

const fileDownload = await driveApi.files.get(
  { fileId, alt: ""media"" },
  { responseType: ""stream"" }
);
let buf = [];
fileDownload.data.on(""data"", (e) => buf.push(e));
fileDownload.data.on(""end"", () => {
  const buffer = Buffer.concat(buf);
  const base64String = buffer.toString(""base64"");
  console.log(base64String);
});

Or, when responseType: ""arraybuffer"" is used, how about the following modification?
const fileId = ""file_id_string""; // Please set your file ID.

const fileDownload = await driveApi.files.get(
  { fileId, alt: ""media"" },
  { responseType: ""arraybuffer"" }
);
const base64String = Buffer.from(fileDownload.data).toString(""base64"");
console.log(base64String);


By this, the downloaded file is converted to the base64 data in base64String.

Reference:

Google APIs Node.js Client

",chatgpt
nextjs edge function module not found error 39path39 and internal server error when using openai integration,"Description
I'm encountering an issue where the path module is not being found in my Node.js environment. This problem occurs when trying to send message in the chatbox.
Error Message
Module not found: Can't resolve 'path'

https://nextjs.org/docs/messages/module-not-found

Import trace for requested module:
./node_modules/dotenv/config.js
./src/lib/db/index.ts
./src/app/api/chat/route.ts
./node_modules/next/dist/build/webpack/loaders/next-edge-app-route-loader/index.js?absolutePagePath=C%3A%5CUsers%5CDell%5COneDrive%5CDesktop%5C100xDevs%5Csummarize-my-pdf-ai%5Csrc%5Capp%5Capi%5Cchat%5Croute.ts&page=%2Fapi%2Fchat%2Froute&appDirLoader=bmV4dC1hcHAtbG9hZGVyP25hbWU9YXBwJTJGYXBpJTJGY2hhdCUyRnJvdXRlJnBhZ2U9JTJGYXBpJTJGY2hhdCUyRnJvdXRlJmFwcFBhdGhzPSZwYWdlUGF0aD1wcml2YXRlLW5leHQtYXBwLWRpciUyRmFwaSUyRmNoYXQlMkZyb3V0ZS50cyZhcHBEaXI9QyUzQSU1Q1VzZXJzJTVDRGVsbCU1Q09uZURyaXZlJTVDRGVza3RvcCU1QzEwMHhEZXZzJTVDc3VtbWFyaXplLW15LXBkZi1haSU1Q3NyYyU1Q2FwcCZwYWdlRXh0ZW5zaW9ucz10c3gmcGFnZUV4dGVuc2lvbnM9dHMmcGFnZUV4dGVuc2lvbnM9anN4JnBhZ2VFeHRlbnNpb25zPWpzJnJvb3REaXI9QyUzQSU1Q1VzZXJzJTVDRGVsbCU1Q09uZURyaXZlJTVDRGVza3RvcCU1QzEwMHhEZXZzJTVDc3VtbWFyaXplLW15LXBkZi1haSZpc0Rldj10cnVlJnRzY29uZmlnUGF0aD10c2NvbmZpZy5qc29uJmJhc2VQYXRoPSZhc3NldFByZWZpeD0mbmV4dENvbmZpZ091dHB1dD0mcHJlZmVycmVkUmVnaW9uPSZtaWRkbGV3YXJlQ29uZmlnPWUzMCUzRCE%3D&nextConfigOutput=&preferredRegion=&middlewareConfig=e30%3D!

Environment

Node.js Version: v18.17.1
Framework: Next.js
Operating System: WINDOWS 11

Code
route.ts
import { getContext } from ""@/lib/context"";
import { db } from ""@/lib/db"";
import { chats, messages as _messages } from ""@/lib/db/schema"";
import { openai } from ""@ai-sdk/openai"";
import { streamText } from ""ai"";
import { eq } from ""drizzle-orm"";
import { NextResponse } from ""next/server"";
import { Message } from ""ai/react"";

export const runtime = ""edge"";

export async function POST(req: Request) {
  console.log(""Called api"");

  try {
    const { messages,chatId } = await req.json();
    console.log(""Messages:"", messages);
    console.log(""Chat ID:"", chatId);
    const _chats = await db.select().from(chats).where(eq(chats.id, chatId));
    // Log retrieved chats
    console.log(""_chats:"", _chats);
    if (_chats.length != 1) {
      return NextResponse.json({ Error: "" Chat not found"" }, { status: 404 });
    }
    const fileKey = _chats[0].fileKey;
    const lastMessage = messages[messages.length - 1];
    // Log fileKey and lastMessage
    console.log(""File Key:"", fileKey);
    console.log(""Last Message:"", lastMessage);
    const context = await getContext(lastMessage.content, fileKey);
    console.log(""Context:"", context);
    const prompt = {
      role: ""system"",
      content: `AI assistant is a brand new, powerful, human-like artificial intelligence.
      The traits of AI include expert knowledge, helpfulness, cleverness, and articulateness.
      AI is a well-behaved and well-mannered individual.
      AI is always friendly, kind, and inspiring, and he is eager to provide vivid and thoughtful responses to the user.
      AI has the sum of all knowledge in their brain, and is able to accurately answer nearly any question about any topic in conversation.
      AI assistant is a big fan of Pinecone and Vercel.
      START CONTEXT BLOCK
      ${context}
      END OF CONTEXT BLOCK
      AI assistant will take into account any CONTEXT BLOCK that is provided in a conversation.
      If the context does not provide the answer to question, the AI assistant will say, ""I'm sorry, but I don't know the answer to that question"".
      AI assistant will not apologize for previous responses, but instead will indicated new information was gained.
      AI assistant will not invent anything that is not drawn directly from the context.
      `,
    };
    const response = await streamText({
      model: openai(""gpt-4o-mini""),
      messages: [
        prompt,
        ...messages.filter((message: Message) => message.role === ""user""),
      ],
    });
    return response.toDataStreamResponse();
  } catch (error) {
    console.log(error);
    return NextResponse.json(
      { error: ""Internal Server Error"" },
      { status: 500 }
    );
  }
}

ChatComponent.tsx
""use client"";
import React from ""react"";
import { Input } from ""./ui/input"";
import { useChat } from ""ai/react"";
import { Button } from ""./ui/button"";
import { SendIcon } from ""lucide-react"";
import MessageList from ""./MessageList"";

type Props = { chatId: number };

const ChatComponent = ({ chatId }: Props) => {
  console.log(""Chat ID in ChatComponent:"", chatId);

  const { input, handleInputChange, handleSubmit, messages } = useChat({
    api: ""/api/chat"",

    body: {
      chatId,
    },
  });
  // React.useEffect(() => {
  //   const messageContainer = document.getElementById(""message-container"");
  //   if (messageContainer) {
  //     messageContainer.scrollTo({
  //       top: messageContainer.scrollHeight,
  //       behavior: ""smooth"",
  //     });
  //   }
  // }, [messages]);

  return (
    <div
      className=""relative max-h-screen overflow-scroll""
      id=""message-container""
    >
      {/* Header */}
      <div className=""sticky top-0 inset-x-0 p-2 bg-white h-fit"">
        <h3 className=""text-xl font-bold"">Chat</h3>
      </div>
      {/* Message List */}
      <MessageList messages={messages} />
      <form
        onSubmit={handleSubmit}
        className=""sticky  bottom-0 px-2 py-4 inset-x-0 bg-white""
      >
        <div className=""flex"">
          <Input
            value={input}
            onChange={handleInputChange}
            placeholder=""Ask any question...""
            className=""w-full""
          />
          <Button className=""bg-gradient-to-r from-sky-400 to-blue-500 ml-2"">
            <SendIcon className=""h-4 w-4"" />
          </Button>
        </div>
      </form>
    </div>
  );
};

export default ChatComponent;


Logs
PS C:\Users\Dell\OneDrive\Desktop\100xDevs\summarize-my-pdf-ai> node -v
v18.17.1
PS C:\Users\Dell\OneDrive\Desktop\100xDevs\summarize-my-pdf-ai>

this is console GET /chat/9?_rsc=a12k2 200 in 227ms
GET /chat/8?_rsc=18zah 200 in 383ms
○ Compiling /api/chat ...
⨯ ./node_modules/dotenv/lib/main.js:2:1
Module not found: Can't resolve 'path'

https://nextjs.org/docs/messages/module-not-found

Import trace for requested module:
./node_modules/dotenv/config.js
./src/lib/db/index.ts
./src/app/api/chat/route.ts
./node_modules/next/dist/build/webpack/loaders/next-edge-app-route-loader/index.js?absolutePagePath=C%3A%5CUsers%5CDell%5COneDrive%5CDesktop%5C100xDevs%5Csummarize-my-pdf-ai%5Csrc%5Capp%5Capi%5Cchat%5Croute.ts&page=%2Fapi%2Fchat%2Froute&appDirLoader=bmV4dC1hcHAtbG9hZGVyP25hbWU9YXBwJTJGYXBpJT

JGY2hhdCUyRnJvdXRlJnBhZ2U9JTJGYXBpJTJGY2hhdCUyRnJvdXRlJmFwcFBhdGhzPSZwYWdlUGF0aD1wcml2YXRlLW5leHQtYXBwLWRpciUyRmFwaSUyRmNoYXQlMkZyb3V0ZS50cyZhcHBEaXI9QyUzQSU1Q1VzZXJzJTVDRGVsbCU1Q09uZURyaXZlJTVDRGVza3RvcCU1QzEwMHhEZXZzJTVDc3VtbWFyaXplLW15LXBkZi1haSU1Q3NyYyU1Q2FwcCZwYWdlRXh0ZW5zaW9ucz10c3gmcGFnZUV4dGVuc2lvbnM9dHMmcGFnZUV4dGVuc2lvbnM9anN4JnBhZ2VFeHRlbnNpb25zPWpzJnJvb3REaXI9QyUzQSU1Q1VzZXJzJTVDRGVsbCU1Q09uZURyaXZlJTVDRGVza3RvcCU1QzEwMHhEZXZzJTVDc3VtbWFyaXplLW15LXBkZi1haSZpc0Rldj10cnVlJnRzY29uZmlnUGF0aD10c2NvbmZpZy5qc29uJmJhc2VQYXRoPSZhc3NldFByZWZpeD0mbmV4dENvbmZpZ091dHB1dD0mcHJlZmVycmVkUmVnaW9uPSZtaWRkbGV3YXJlQ29uZmlnPWUzMCUzRCE%3D&nextConfigOutput=&preferredRegion=&middlewareConfig=e30%3D!


Working Code
When using the following simplified code, I am able to get a response from the chatbox:
import { openai } from ""@ai-sdk/openai"";
import { streamText } from ""ai"";
import { NextResponse } from ""next/server"";

export const runtime = ""edge"";

export async function POST(req: Request) {
  try {
    const { messages } = await req.json();
    const response = await streamText({
      model: openai(""gpt-4o-mini""),
      messages,
    });
    return response.toDataStreamResponse();
  } catch (error) {
    console.log(error);
    return NextResponse.json(
      { error: ""Internal Server Error"" },
      { status: 500 }
    );
  }
}


This should provide a clear context for the issue and indicate that the problem might be related to the configuration or dependencies rather than the core functionality of fetching chat responses.
Steps to Reproduce

Run the application with the provided route.ts and ChatComponent.tsx code.
Observe the error when attempting to start or compile the project.

I attempted to implement a chat functionality in my Next.js application using the streamText function from the ai package and the openai model. Specifically, I used the following code to handle POST requests and fetch chat responses:
import { openai } from ""@ai-sdk/openai"";
import { streamText } from ""ai"";
import { NextResponse } from ""next/server"";

export const runtime = ""edge"";

export async function POST(req: Request) {
  try {
    const { messages } = await req.json();
    const response = await streamText({
      model: openai(""gpt-4o-mini""),
      messages,
    });
    return response.toDataStreamResponse();
  } catch (error) {
    console.log(error);
    return NextResponse.json(
      { error: ""Internal Server Error"" },
      { status: 500 }
    );
  }
}

I expected this implementation to correctly process incoming messages and return a valid response from the chatbox.
What actually resulted?
Although the code runs without errors and returns a response in a simplified setup, I encountered issues with the more complex implementation, which includes additional logic such as database interactions and context handling. In those cases, I am receiving a 500 Internal Server Error and the response is not as expected. The simplified code works as intended and provides the expected chat responses.

","I removed this and it worked!!!!!
export const runtime = ""edge"";

",chatgpt
the chatcompletion operation does not work with the specified model gpt4omini,"Context
I have below the Python code.
client = AzureOpenAI(
    api_key = os.getenv(""AZURE_OPENAI_API_KEY""),  
    api_version = os.getenv('AZURE_OPENAI_API_VERSION'),
    azure_endpoint = os.getenv('AZURE_OPENAI_ENDPOINT')
)  
messages = [
    {""role"": ""user"", ""content"": prompt}
]
response = client.chat.completions.create(
    model=""gpt-4o-mini"",        
    messages=messages,
    temperature=0,  
)

Issue
I faced the below error.
openai.BadRequestError: Error code: 400 - 
{
   ""error"":{
      ""code"":""OperationNotSupported"",
      ""message"":""The chatCompletion operation does not work with the specified model, gpt-4o-mini. Please choose different model and try again. You can learn more about which models can be used with each operation here: https://go.microsoft.com/fwlink/?linkid=2197993.""
   }
}

The above code works fine when I change the model to ""gpt-4o""
","The Azure region you selected does not support this model. You need to check this list to select the appropriate region, and select the deployment type as ""Standard""
https://learn.microsoft.com/en-us/azure/ai-services/openai/concepts/models#standard-deployment-model-availability
",chatgpt
how to convert the result from openai call convert it into json and write to txt file,"I am very new to python and only know the basics, so basically I am calling openai and getting a response in return and want to write that response in a .txt file.
I want to convert the response in json before writing in the file. my response is already in json format but weird when print it shows json format with json {}  with it, this is my script
def get_json(image_file, category):
    with open(image_file, ""rb"") as image:
        response = openai_client.chat.completions.create(
            model=""gpt-4-vision-preview"",
            messages=[
                {
                    ""role"": ""user"",
                    ""content"": [
                        {""type"": ""text"", ""text"": f""Analyze this image and provide the following attributes: color theme, font style, and a short description of about 4-7 words. Categorize it as {category}. Return the result as a JSON object.""},
                        {""type"": ""image_url"", ""image_url"": {""url"": f""data:image/jpeg;base64,{base64.b64encode(image.read()).decode()}""}},
                    ],
                }
            ],
            temperature=1,
            max_tokens=4095,
            top_p=1,
            frequency_penalty=0,
            presence_penalty=0,
        ) 
        return response.choices[0].message.content
     
with open(file, 'a') as file:
    for filename in os.listdir(images_folder):
        filepath = os.path.join(images_folder, filename)
        result =get_json(filepath, 'hero')
        file.write(result + '\n')
        json_result = json.loads(result)
        print(json_result)

this is the result i am getting
enter image description here
i want to remove the text '''json'''
tried to convert it into json by json.loads(result) but getting the following error:-
raise JSONDecodeError(""Expecting value"", s, err.value) from None
json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0)
","You are asking it to return a JSON object in you prompt, ""Return the result as a JSON object."" that it why! If you give it the same prompt using the website, you will notice that the response is nicely formatted, that is because of those ""```json ...content ```"" markdown formatting.
You can solve it using two methods:

Explicitly replace ""```json"" and ""```"" with empty string:

import json

# For example, this is the content in response:
response = '```json{""color_theme"": ""Mint Green and Black"",""font_style"": ""Sans-serif"",""short_description"": ""Web Publishing Platform"",""category"": ""hero""}```'

# Replace and assign back to original content
response = response.replace(""```json"", """")
response = response.replace(""```"", """")

# Don't forget to convert to JSON as it is a string right now:
json_result = json.loads(response)


Using slicing:

import json

# For example, this is the content in response:
response = '```json{""color_theme"": ""Mint Green and Black"",""font_style"": ""Sans-serif"",""short_description"": ""Web Publishing Platform"",""category"": ""hero""}```'

# ""```json"" is 7 character long, but slicing count start from 0. ""{"" is at 7th character.
# ""```"" is 3 character long (at the end).
response = response[7:-3]

# Don't forget to convert to JSON as it is a string right now:
json_result = json.loads(response)


And for writing to the .txt file, you can use json.dump() as follows:
import json

response = '```json{""color_theme"": ""Mint Green and Black"",""font_style"": ""Sans-serif"",""short_description"": ""Web Publishing Platform"",""category"": ""hero""}```'

response = response[7:-3]
response = json.loads(response)

# Write to response.txt file (overwriting it).
with open(""response.txt"", ""w"") as file:
    json.dump(response, file)

",chatgpt
nextjs 13 build error for private identifiers when using openai 40 package,"I added the new 4.0 openai package in my next.js app, i'm using it in the server of the app only and when building i get the error.
- info Linting and checking validity of types .Failed to compile.

./node_modules/openai/src/core.ts:539:3
Type error: Private identifiers are only available when targeting ECMAScript 2015 and higher.

  537 |
  538 | export abstract class AbstractPage<Item> implements AsyncIterable<Item> {
> 539 |   #client: APIClient;
      |   ^
  540 |   protected options: FinalRequestOptions;
  541 |
  542 |   protected response: Response;



I tried changing the tsconfig target to es6,es2015,esnext, but it doesn't seem to work, skipLibCheck is enabled so i'm even more confused as of why it happens
This is my tsconfig:
{
  ""compilerOptions"": {
    ""target"": ""es5"",
    ""lib"": [
      ""dom"",
      ""dom.iterable"",
      ""esnext""
    ],
    ""allowJs"": true,
    ""skipLibCheck"": true,
    ""strict"": true,
    ""forceConsistentCasingInFileNames"": true,
    ""noEmit"": true,
    ""esModuleInterop"": true,
    ""module"": ""esnext"",
    ""moduleResolution"": ""node"",
    ""resolveJsonModule"": true,
    ""isolatedModules"": true,
    ""jsx"": ""preserve"",
    ""incremental"": true,
    ""plugins"": [
      {
        ""name"": ""next""
      }
    ],
    ""paths"": {
      ""$/*"": [
        ""./src/*""
      ],
      ""$app/*"": [
        ""./src/app/*""
      ],
      ""$cmp/*"": [
        ""./src/components/*""
      ],
      ""$types/*"": [
        ""./src/types/*""
      ],
      ""$assets/*"": [
        ""./src/assets/*""
      ],
      ""$lib/*"": [
        ""./src/lib/*""
      ],
    }
  },
  ""include"": [
    ""next-env.d.ts"",
    ""**/*.ts"",
    ""**/*.tsx"",
    "".next/types/**/*.ts""
  ],
  ""exclude"": [
    ""node_modules""
  ]
}

","Use this import (no ""src"" in import):
import { type ChatCompletionMessageParam } from ""openai/resources/index.mjs"";

instead of :
import { ChatCompletionMessageParam } from ""openai/src/resources/chat/completions"";

",chatgpt
how to increase the code view box size of chatgpt web interface,"It is frustrating how small the code viewing area is in ChatGPT and to make the matters worse it has a fixed width that does not change with the size of the browser window.
UPDATE: Guys use the extension in the answer below. It is a better solution than mine.
A temporary solution is this bookmarklet to maximize all the elements:
Create a bookmark and put this in the URL. Then run by selecting the bookmark on ChatGPT UI.
javascript:(function() { 
  document.querySelectorAll('div[class*=""group w-full""] > div > div')
    .forEach((element) => { element.className = ""flex flex-1""; }); 
})();

Is there a better solution than this?
","There's a chrome extension now - https://chromewebstore.google.com/detail/wide-gpt/ceddkenhchmiajgmnlelcnackpeihohk. Its pretty clean.
PS: Not built by me.
",chatgpt
how to extract information from a photo of a document,"I want to build a simple tool that will allow a user to take a photo of a document and extract information such as date/time and some other information
It is simple to do this through Chat GPT's UI, upload an image, ask it for some information from the document. But is just calling Chat GPT's API from my code really a viable solution?

It just feels like a black box that is liable to change without notice, i.e. fragile.
I assumed I'd need to train and deploy my own model, is using chat gpt expensive overkill for what I want to do? If I somehow ended up with a lot of users I'm thinking this would become a problem.

Note: I have no real AI experience or knowledge, but I do have lots of programming experience (I work full time as a developer).
","There are several ways to go about it but you can start of with Ollama, you need to download a model such as llama3, if you have experience with python you are in luck.
You don't need to train a model, there are different models out there that does what you need or solves this kind of problem, all you need to do is provided the llm your documents(images, text, pdfs etc) and ask questions on them.
However in some cases, if your pdf contains financial information such as annuity and the likes , You might need to train it for it to understand how to do those kind of calculations or better still write a function which inherits from Langchain_tool to instruct the llm on how to use it for those specific cases.

It just feels like a black box that is liable to change without
notice, i.e. fragile. I assumed I'd need to train and deploy my own
model, is using chat gpt expensive overkill for what I want to do? If
I somehow ended up with a lot of users I'm thinking this would become
a problem.

Here are the general steps on how to go go about it:
Step 1:
First download Ollama then you can pull the llama image which would serve as your llm, do a docker pull of llama, preferably llama3.
Step 2:
Find a library that converts images to text or pdf such as
Optical character recognition Library (OCR)
Step 3:
Find a vector_db(FAISS, chromadb and the likes) which converts text to vectors; this makes information extraction easy.
Step 4:
Feed your documents to the vector_db so it can convert it to vectors because numbers are good...
",chatgpt
how can i get wordlevel timestamps in openai39s whisper asr,"I use OpenAI's Whisper python lib for speech recognition. How can I get word-level timestamps?

To transcribe with OpenAI's Whisper (tested on Ubuntu 20.04 x64 LTS with an Nvidia GeForce RTX 3090):
conda create -y --name whisperpy39 python==3.9
conda activate whisperpy39
pip install git+https://github.com/openai/whisper.git 
sudo apt update && sudo apt install ffmpeg
whisper recording.wav
whisper recording.wav --model large

If using an Nvidia GeForce RTX 3090, add the following after conda activate whisperpy39:
pip install -f https://download.pytorch.org/whl/torch_stable.html
conda install pytorch==1.10.1 torchvision torchaudio cudatoolkit=11.0 -c pytorch

","In openai-whisper version 20231117, you can get word level timestamps by setting word_timestamps=True when calling transcribe():
pip install openai-whisper

import whisper
model = whisper.load_model(""large"")
transcript = model.transcribe(
    word_timestamps=True,
    audio=""toto.mp3""
)
for segment in transcript['segments']:
    print(''.join(f""{word['word']}[{word['start']}/{word['end']}]"" 
                    for word in segment['words']))

prints:

Toto,[2.98/3.4] I[3.4/3.82] have[3.82/3.96] a[3.96/4.02] feeling[4.02/4.22] we're[4.22/4.44] not[4.44/4.56] in[4.56/4.72] Kansas[4.72/5.14] anymore.[5.14/5.48]

",chatgpt
does the answer quality of openai gpt4o from api change over time,"I'm creating an application that uses GPT-4 (via the OpenAI API) for visual question answering.
The problem is that tests using this module, which previously passed, have started to fail constantly due to a decline in the quality of the answers. Is this expected behavior for GPT-4 (or the OpenAI API)?
","If you use the gpt-4o as a model identifier in your application, then yes, the underlying model might change.
Let's take gpt-4o as an example:

as I write this, it points to gpt-4o-2024-05-13
but soon it will point to the newer gpt-4o-2024-08-06

It's a good question whether the quality of the answer can decrease in newer models. Companies use various metrics to ensure it doesn't happen. And the blind tests rating of AI models generally show that people rank newer models higher.
At the same time, it's easy to find anecdotal opinions that some previous models were better. At first, my impression too was that gpt-4 was better than the newer gpt-4o, but when I did a blind test, I preferred answers from gpt-4o.
If you prefer to stick to tested legacy model version, you can still use more specific id like gpt-4o-2024-05-13.
",chatgpt
install the correct onnxruntime for chromadb with pip install,"I am trying to install chromadb on my Jupyter notebook (Anaconda) using:
pip install chromadb

I get error:
ERROR: Could not find a version that satisfies the requirement onnxruntime>=1.14.1 (from chromadb) (from versions: 1.2.0, 1.3.0, 1.4.0, 1.5.1, 1.5.2, 1.6.0, 1.7.0, 1.8.0, 1.8.1, 1.9.0, 1.10.0, 1.11.0, 1.11.1)
ERROR: No matching distribution found for onnxruntime>=1.14.1 (from chromadb)

Ok, so I run:
pip install onnxruntime

And it installed onnxruntime 1.11.1
But chromadb requires >1=1.14.1
I am assuming that the highest onnxruntime compatible with my OS (mac) is 1.11.1.
Is there a way around it?
","onnxruntime 1.11.1 supports Python 3.6 (see the middle of the left column). Later versions don't support 3.6. I guess you use Python 3.6. To install a later version of onxruntime upgrade Python. All versions up to the current 1.15.1 requires at least 3.7; 1.15.0 and 1.15.1 don't provide wheels for Python 3.7, only for 3.8+.
",chatgpt
how can i optimize the data i am embedding to increase vector search result quality,"I am trying to implement semantic/vector search for images.
To do that, I am using gpt-4-mini to analyze an image and create data from it with this prompt:
Your job is to generate json data from a given image.
          
            Return your output in the following format:
            {
            description: ""A description of the image. Only use relevant keywords."",
            text: ""If the image contains text, include that here, otherwise remove this field"",
            keywords: ""Keywords that describe the image"",
            artstyle: ""The art style of the image"",
            text_language: ""The language of the text in the image, otherwise remove this field"",,
            design_theme : ""If the image has a theme (hobby, interest, occupation etc.), include that here, otherwise remove this field"",
            }

The data I am getting back is pretty accurate (in my eyes). I am then embedding the json with the ""text-embedding-3-small"" model.
The problem is that the search results are pretty bad.
For example: I have 2 images with only text. One says ""straight outta knee surgery"" and one says ""straight outta valhalla"".
When I search for ""straight outta"", I have to turn down the similary treshold to 0.15 to get both results.
This is my postgres search function:
CREATE
OR REPLACE FUNCTION search_design_items (
  query_embedding vector (1536),
  match_threshold FLOAT,
  match_count INT
) RETURNS TABLE (
  id BIGINT
) AS $$
BEGIN
    RETURN QUERY
    SELECT id
    FROM public.design_management_items
    WHERE 1 - (design_management_items.description_vector <=> query_embedding) > match_threshold
    ORDER BY (design_management_items.description_vector <=> query_embedding) asc
    LIMIT match_count;
END;
$$ LANGUAGE plpgsql;

When I go into higher numbers (0.5) there are pretty much no results at all. This seems wrong because in every tutorial I have seen they use a threshold of 0.7+
What do I need to change in order to improve the accuracy of my search results?
","Try to perform a hybrid search. All vector databases offer the hybrid search functionality.
As stated in the official Weaviate blog:

Hybrid search is a technique that combines multiple search algorithms
to improve the accuracy and relevance of search results. It uses the
best features of both keyword-based search algorithms with vector
search techniques. By leveraging the strengths of different
algorithms, it provides a more effective search experience for users.

In simple terms, performing a hybrid search means that you search with both keywords and embedding vectors, where you set the alpha parameter as a way to give a weight to these two. For example, setting alpha to 0 means keyword search only, while setting alpha to 1 means embedding vector search only.
I've created a project with a hybrid search in the past where you can search for Lex Fridman's podcast insights without watching the full episodes. See the demonstration.
Here's the weaviateHybridSearch.ts file:
""use server"";

import weaviate from ""weaviate-client"";
import { PodcastType } from ""@/app/types/podcast"";

// Define and export the queryPodcasts function
export async function queryPodcasts(searchTerm: string, alpha: number) {
  /**
   * Queries the Podcast collection based on a search term and alpha value.
   *
   * @param {string} searchTerm - The search term to query for.
   * @param {number} alpha - The alpha value to use for the hybrid search.
   * @return {Promise<PodcastType[]>} - The array of PodcastType objects representing the search results.
   */

  // Connect to the local Weaviate instance
  const client = await weaviate.connectToLocal();

  // Get the Podcast collection
  const podcastCollection = await client.collections.get<
    Omit<PodcastType, ""distance"">
  >(""Podcast"");

  // Perform the hybrid search on the Podcast collection
  const { objects } = await podcastCollection.query.hybrid(searchTerm, {
    limit: 10,
    alpha: alpha,
    returnMetadata: [""score""],
    returnProperties: [""number"", ""guest"", ""title"", ""transcription""],
  });

  // Process the results
  const podcasts: PodcastType[] = objects.map((podcast: any) => ({
    ...podcast.properties,
    distance: podcast.metadata?.score!!,
  }));

  // Return the podcasts
  return podcasts;
}

",chatgpt
return arguments from function calling with openai api when streaming,"I've made a simple OpenAI API example with function calling. I'm only using function calling to format the response, I'm not calling multiple functions or any external APIs.
When I don't stream the response I can return the function arguments, which is the data that I need.
In my NextJS route handler:
export async function POST(request: Request) {
  try {
    const openai = new OpenAI({
      apiKey: process.env[""OPENAI_API_KEY""],
    });
    const response = await openai.chat.completions.create({
      model: ""gpt-4"",
      // stream: true,
      messages: [
        {
          role: ""user"",
          content: ""Give me 5 questions and answers for a pub quiz"",
        },
      ],
      tools: [
        {
          type: ""function"",
          function: {
            name: ""get_questions_and_answers"",
            description: ""Get questions and answers"",
            parameters: simpleJsonSchema,
          },
        },
      ],
      tool_choice: {
        type: ""function"",
        function: { name: ""get_questions_and_answers"" },
      },
    });
    return Response.json(
       JSON.parse(
         response.choices[0].message.tool_calls?.[0].function.arguments || """",
       ),
    );
  } catch (serverError) {
    console.error({ serverError });
    throw new Error();
  }
}

simpleJsonSchema.json:
{
  ""type"": ""object"",
  ""properties"": {
    ""getQuestions"": {
      ""type"": ""array"",
      ""items"": {
        ""type"": ""object"",
        ""properties"": {
          ""Question"": {""type"": ""string""},
          ""Answer"": {""type"": ""string""}
        },
        ""required"": [""Question"", ""Answer""]
      }
    }
  },
  ""required"": [""getQuestions""]
}

Response from API:
{""getQuestions"":[{""Question"":""What is the capital of Australia?"",""Answer"":""Canberra""},{""Question"":""Who wrote 'To Kill a Mockingbird'?"",""Answer"":""Harper Lee""},{""Question"":""What is the highest peak in the world?"",""Answer"":""Mount Everest""},{""Question"":""Who is known as the 'Father of Computers'?"",""Answer"":""Charles Babbage""},{""Question"":""What is the largest ocean in the world?"",""Answer"":""Pacific Ocean""}]}

This is fine when developing locally, however when deployed to Vercel the request sometimes times out. I've tried to add streaming as this is the recommended solution:
const response = await openai.chat.completions.create({
  model: ""gpt-4"",
  stream: true,
  messages: [
    {
      role: ""user"",
      content: ""Give me 5 questions and answers for a pub quiz"",
    },
  ],
  tools: [
    {
      type: ""function"",
      function: {
        name: ""get_questions_and_answers"",
        description: ""Get questions and answers"",
        parameters: simpleJsonSchema,
      },
    },
  ],
  tool_choice: {
    type: ""function"",
    function: { name: ""get_questions_and_answers"" },
  },
});

const stream = OpenAIStream(response);
return new StreamingTextResponse(stream);

However now the response has a lot of unnecessary data. And when I try to JSON.parse on the client I get errors.
Response from API:
{""tool_calls"":[ {""id"": ""call_IhxvzkZ5EsmZpHc6tOznTmzb"", ""type"": ""function"", ""function"": {""name"": ""get_questions_and_answers"", ""arguments"": ""{\n  \""getQuestions\"": [\n    {\n      \""Question\"": \""Question 1\"",\n      \""Answer\"": \""Answer 1\""\n    },\n    {\n      \""Question\"": \""Question 2\"",\n      \""Answer\"": \""Answer 2\""\n    },\n    {\n      \""Question\"": \""Question 3\"",\n      \""Answer\"": \""Answer 3\""\n    },\n    {\n      \""Question\"": \""Question 4\"",\n      \""Answer\"": \""Answer 4\""\n    },\n    {\n      \""Question\"": \""Question 5\"",\n      \""Answer\"": \""Answer 5\""\n    }\n  ]\n}""}}

As far as I can see the docs only cover using useChat but I have some particular requirements so I need to handle the fetching and form state myself: https://sdk.vercel.ai/docs/api-reference/use-chat
Why am I getting invalid JSON?
Here is a repository which reproduces the issue: https://github.com/jameschetwood/openai-function-calling
","this is the response you are getting:
{""tool_calls"":[ {""id"": ""call_HRxqlP3yzeHsoN43tMyZjMlr"", ""type"": ""function"", ""function"": {""name"": ""get_questions_and_answers"", ""arguments"": ""{\n  \""getQuestions\"": [\n    {\n      \""Question\"": \""What is the capital city of France?\"",\n      \""Answer\"": \""Paris\""\n    },\n    {\n      \""Question\"": \""Who painted the Mona Lisa?\"",\n      \""Answer\"": \""Leonardo da Vinci\""\n    },\n    {\n      \""Question\"": \""What is the largest planet in our solar system?\"",\n      \""Answer\"": \""Jupiter\""\n    },\n    {\n      \""Question\"": \""What is the national flower of England?\"",\n      \""Answer\"": \""Rose\""\n    },\n    {\n      \""Question\"": \""Which country is famous for its tulips?\"",\n      \""Answer\"": \""Netherlands\""\n    }\n  ]\n}""}}

I used https://jsoneditoronline.org/ to auto correct the json and it just adds ""]}"". for some reason openai is not sending correct json response. you have to add it
accumulatedText += ""]}"";

then response works:

this is too specific error. if openai updates its response api, it might send the json data correctly. so a better approach would be parsing in try/catch
try {
      const parsed = JSON.parse(accumulatedText);
      console.log({ parsed });
    } catch (error) {
      // you should error for each specific case
      accumulatedText += ""]}"";
      console.log(""correct accumulatedText in catch block"", accumulatedText);
    }

",chatgpt
vector store created using existing graph for multiple nodeslabels,"Am trying to create vector stores on top of my existing KG using from_existing_graph, (followed tomaz  and Saurav Joshi neo4j blog posts) - this method is allowing me to create embedding/vector index only for single label due to which am unable to get desired results while asking NLQ (I am assuming though).
below code is able to answer, the age and location of Oliver but not what he directed,
i believe this is due to from_existing_graph has only to pass single label and its corresponding properties as option for generating embeddings and vector index
Any ideas, how to achieve this?
import os
import re
from langchain.vectorstores.neo4j_vector import Neo4jVector
# from langchain.document_loaders import WikipediaLoader
from langchain_openai import OpenAIEmbeddings
# from langchain.text_splitter import CharacterTextSplitter, RecursiveCharacterTextSplitter
from langchain.graphs import Neo4jGraph
import openai
# from transformers import AutoModelForSeq2SeqLM, AutoTokenizer

os.environ[""OPENAI_API_KEY""] = ""sk-xx""
url = ""neo4j+s://xxxx.databases.neo4j.io""
username = ""neo4j""
password = ""mypassword""
existing_graph = Neo4jVector.from_existing_graph(
    embedding=OpenAIEmbeddings(),
    url=url,
    username=username,
    password=password,
    index_name=""person"",
    node_label=""Person"",
    text_node_properties=[""name"", ""age"", ""location""],
    embedding_node_property=""embedding"",
)

from langchain.chat_models import ChatOpenAI
from langchain.chains import GraphCypherQAChain
from langchain.graphs import Neo4jGraph

graph = Neo4jGraph(
    url=url, username=username, password=password
)

chain = GraphCypherQAChain.from_llm(
    ChatOpenAI(temperature=0), graph=graph, verbose=True
)

query = ""Where does Oliver Stone live?""
#query = ""Name some films directed by Oliver Stone?"" 

graph_result = chain.invoke(query)

vector_results = existing_graph.similarity_search(query, k=1)
for i, res in enumerate(vector_results):
    print(res.page_content)
    if i != len(vector_results)-1:
        print()
vector_result = vector_results[0].page_content

# Construct prompt for OpenAI
final_prompt = f""""""You are a helpful question-answering agent. Your task is to analyze
and synthesize information from two sources: the top result from a similarity search
(unstructured information) and relevant data from a graph database (structured information).
Given the user's query: {query}, provide a meaningful and efficient answer based
on the insights derived from the following data:

Unstructured information: {vector_result}.
Structured information: {graph_result} """"""


from openai import OpenAI
client = OpenAI(
    # This is the default and can be omitted
    api_key=os.environ.get(""OPENAI_API_KEY""),
)

chat_completion = client.chat.completions.create(messages=[{""role"": ""user"",""content"": final_prompt,  }],model=""gpt-3.5-turbo"",)

answer = chat_completion.choices[0].message.content.strip()
print(answer)

Any help would be highly appreicated?
here is my schema:
Node properties are the following:
Person {name: STRING, embedding: LIST, age: INTEGER, location: STRING},Actor {name: STRING, embedding: LIST},Movie {title: STRING},Director {name: STRING, embedding: LIST, age: INTEGER, location: STRING}
Relationship properties are the following:
ACTED_IN {role: STRING}
The relationships are the following:
(:Person)-[:ACTED_IN]->(:Movie),(:Person)-[:DIRECTED]->(:Movie),(:Actor)-[:ACTED_IN]->(:Movie),(:Director)-[:DIRECTED]->(:Movie)

Cypher used to create:
CREATE (charlie:Person:Actor {name: 'Charlie Sheen'})-[:ACTED_IN {role: 'Bud Fox'}]->(wallStreet:Movie {title: 'Wall Street'})<-[:DIRECTED]-(oliver:Person:Director {name: 'Oliver Stone'});
MATCH (n:Person {name: 'Oliver Stone'}) SET n.age = 30, n.location = ""New York"" RETURN n

","You need to add the relationship :DIRECTED into the index person_index since the movie he directed is not part of the embedding. Once you have the query to add the movies he directed, you will then add it on the resulting node metadata (see retrieval_query).  Then on your vector result, you will add the information about the movie tile (as metadata movie[0][""title""]).
You may need to collect all movie titles if there are more than one movie titles in the graph. I'm sure you can figure it out.
Reference: https://github.com/tomasonjo/blogs/blob/master/llm/neo4jvector_langchain_deepdive.ipynb
import os
from langchain.vectorstores.neo4j_vector import Neo4jVector
from langchain_openai import OpenAIEmbeddings
import openai

os.environ[""OPENAI_API_KEY""] = ""sk-<key>""
url = ""bolt://localhost:7687""
username = ""neo4j""
password = ""awesome_password""

retrieval_query = """"""
       MATCH (node)-[:DIRECTED]->(m:Movie)
       WITH node, score, collect(m) as movies
       RETURN node.name as text, score, node{.*, embedding: Null, movies: movies} as metadata
       """"""

existing_index_return = Neo4jVector.from_existing_index(
    embedding=OpenAIEmbeddings(),
    url=url,
    username=username,
    password=password,
    database=""neo4j"",
    index_name=""person_index"",
    text_node_property=""name"",
    retrieval_query=retrieval_query,
)

from langchain_openai import ChatOpenAI
from langchain.chains import GraphCypherQAChain
from langchain_community.graphs import Neo4jGraph

graph = Neo4jGraph(
    url=url, username=username, password=password
)

chain = GraphCypherQAChain.from_llm(
    ChatOpenAI(temperature=0), graph=graph, verbose=True
)

#query = ""Where does Oliver Stone live?""
query = ""Name some films directed by Oliver Stone?"" 

graph_result = chain.invoke(query)

vector_results = existing_index_return.similarity_search(query, k=1)
vector_result = vector_results[0].page_content + "" lives in "" + vector_results[0].metadata[""location""] + "" and he directed the movie "" + vector_results[0].metadata[""movies""][0][""title""]

# Construct prompt for OpenAI
final_prompt = f""""""You are a helpful question-answering agent. Your task is to analyze
and synthesize information from two sources: the top result from a similarity search
(unstructured information) and relevant data from a graph database (structured information).
Given the user's query: {query}, provide a meaningful and efficient answer based
on the insights derived from the following data:

Unstructured information: {vector_result}.
Structured information: {graph_result} """"""


from openai import OpenAI
client = OpenAI(
    # This is the default and can be omitted
    api_key=os.environ.get(""OPENAI_API_KEY""),
)

chat_completion = client.chat.completions.create(messages=[{""role"": ""user"",""content"": final_prompt,  }],model=""gpt-3.5-turbo"",)

answer = chat_completion.choices[0].message.content.strip()
print(answer)

Sample output:
> Entering new GraphCypherQAChain chain...
Generated Cypher:
MATCH (d:Director {name: ""Oliver Stone""})-[:DIRECTED]->(m:Movie)
RETURN m.title
Full Context:
[{'m.title': 'Wall Street'}]

> Finished chain.
Based on the unstructured information retrieved from the top result of the search, Oliver Stone directed the film ""Wall Street."" 

In addition to ""Wall Street,"" some other films directed by Oliver Stone include ""Platoon,"" ""JFK,"" ""Born on the Fourth of July,"" ""Natural Born Killers,"" and ""Snowden.""

",chatgpt
how to chain azureopenai in langchain,"from openai import AzureOpenAI
client = AzureOpenAI(
        azure_endpoint=os.getenv(""AZURE_OPENAI_ENDPOINT""),
        api_key=os.getenv(""AZURE_OPENAI_API_KEY""),
        api_version=""2024-05-01-preview""
    )

normal_chain = (
    ChatPromptTemplate.from_messages([(""system"", ""write a tweet about {topic} in the style of Elon Musk"") ])
    | client
    | StrOutputParser()
)


In this code, I am getting Expected type 'Runnable[Any, Other] | (Any) -> Other | (Iterator) -> Iterator[Other] | Mapping[str, Runnable[Any, Other] | (Any) -> Other | Any]', got 'AzureOpenAI' instead. How to make the client as Runnable? Is there any other way to do the same thing?
I tried all the available options from LangChain, but it is not working.
","    client = AzureOpenAI(
        azure_endpoint=os.getenv(""AZURE_OPENAI_ENDPOINT""),
        api_key=os.getenv(""AZURE_OPENAI_API_KEY""),
        api_version=""2024-05-01-preview""
    )

    completion = client.chat.completions.create(
        model=finetuned_deployment,
        messages=[
            {
                ""role"": ""user"",
                ""content"": f""Write a tweet about {topic}""
            }],
        max_tokens=800,
        temperature=0.7,
        top_p=0.95,
    )
    
completion.to_json()    

This code snippet worked.
",chatgpt
how to disable the warning message for g4f version deprecation,"I am using this code to get my response out of the model :
from g4f.client import Client

client = Client()

response = client.chat.completions.create(
    model=""gpt-3.5-turbo"",
    messages=[{""role"": ""user"", ""content"": ""Hello""}],
)

This code is run through subprocess.Popen() call like so:
p = subprocess.Popen(['C:\\Python38\\python.exe','-Wignore', 'C:\\Users\\user\\proj\\projName\\chatgpt.py'],
                             stdin=subprocess.PIPE, stdout=subprocess.PIPE, shell=True, env=env)

But the call to client.chat.completions.create() generates this warning message before actually returning the model response:
New g4f version: 0.3.2.4 (current: 0.3.2.2) | pip install -U g4f

My question is how to suppress that warning message from being generated by the mentioned call?
","You can suppress it using the warnings module:
import warnings
warnings.filterwarnings(""ignore"")

Or, you can resolve the issue by upgrading the g4f package to the latest version via pip install -U g4f

You can also use sys.stdout to redirect standard output temporarily.
For example:
# Save the current stdout
old_stdout = sys.stdout

# Redirect all output that would normally go to stdout (e.g., the console) into a buffer instead
sys.stdout = io.StringIO()

# Call the g4f function that generates the warning here
...
...

# Restore stdout to its original state
sys.stdout = old_stdout

",chatgpt
i get error typeerror cannot read properties of undefined reading 39key39,"I'm experiencing a couple of issues with configuring and using OpenAI in a Firebase function.
When trying to access the OpenAI API key in the openaiConfig.ts file, I get the error:
TypeError: Cannot read properties of undefined (reading 'key')

Here's the relevant part of the code:
import OpenAI from 'openai';
import * as functions from 'firebase-functions';

const openaiApiKey = functions.config().openai.key;

export const openai = new OpenAI({
    apiKey: openaiApiKey,
});

I set the environment variable using the command:
firebase functions:config:set openai.key=""MY_KEY""

Additional Information:
The functions execute normally when deployed to the cloud.
The issue occurs only when running locally with the command:
firebase emulators:start

Version:
firebase-admin: ""^12.1.0"",
firebase-functions: ""^5.0.0"",
openai: ""^4.56.0""
","When testing locally, use the Firebase Functions emulator. Make sure to set the environment configuration for the emulator as well:
firebase functions:config:get > .runtimeconfig.json

This command creates a .runtimeconfig.json file in your functions directory, which the emulator will use.
",chatgpt
why am i not seeing any return results for my firestore vector embedding search,"I have made a composite index as per the firebase docs
 gcloud alpha firestore indexes composite create \
--project=[Insert Project Name Here]
--collection-group=elements --query-scope=COLLECTION \
--field-config=vector-config='{""dimension"":""1536"",""flat"": ""{}""}',field-path=embedding

That works fine and I can see it clearly under gcloud firestore indexes composite describe [Insert Index Name Here]
Then I try to query the database using an onCall function.
export const vectorSearch = functions.https.onCall(async (data, context) => {
  const { query, quantity } = data;

  if (!query) {
    throw new functions.https.HttpsError('failed-precondition', 'query is required');
  }

  if (!quantity) {
    throw new functions.https.HttpsError('failed-precondition', 'quantity is required');
  }

  try {
    console.log(`Creating embedding for ${query}`)
    const embeddingResponse = await openai.embeddings.create({
      model: ""text-embedding-3-small"",
      input: query,
      encoding_format: ""float"",
    });

    const embedding = embeddingResponse.data[0].embedding;
    console.log(`Embedding:`, embedding)

    const vectorQuery: VectorQuery = firestore.collection('elements').findNearest('embedding', FieldValue.vector(embedding), {
      limit: quantity,
      distanceMeasure: 'COSINE'
    });

    const snapshot: VectorQuerySnapshot = await vectorQuery.get();
    console.log(`Snapshot:`, snapshot)

    const formattedData: any = {};
    snapshot.docs.forEach((doc) => {
      formattedData[doc.id] = doc.data();
    });

    return { docs: formattedData };
  } catch (error) {
    console.error('Error querying database:', error);
    throw new functions.https.HttpsError('internal', 'Error querying database');
  }
});

The Problem
When I call this function, I get no entries despite having documents under the relevant collection. I don't get any errors - just no documents appear in the response. However I should be seeing the number of documents I specified in the quantity field. Why is this?
","I was storing each embedding field as an array in my firestore database.
[0.1, 0.2, 0.3]
I should have been storing each embedding as a type of vector.
FieldValue.vector([0.1, 0.2, 0.3])
Firebase ignored the fields that weren't a vector, so the index for the search was empty and therefore no documents were returned.
",chatgpt
how does openaiembeddings work is it creating a single vector of size 1536 for whole text corpus,"I'm working with the OpenAIEmbeddings() class from OpenAI, which uses the text-embedding-3-small model. According to the documentation, it generates a 1536-dimensional vector for any input text.
However, I'm a bit confused about how this works:

Is the 1536-dimensional vector generated for the entire input text?
If the 1536-dimensional vector represents the entire input text, how does the model handle individual words versus longer texts like sentences or paragraphs?

I was expecting this:
If there are 100 words in my input text, i expected that OpenAIEmbeddings() would output 100 vectors, each having size 1536.
But the output is a single vector of size 1536 for the whole input text.
Why I expected this?
Because in my learning, i've understood that embeddings like Word2Vec or GloVe provide vectors for each word in a corpus. How does this differ from the approach taken by OpenAIEmbeddings?
I'm trying to understand whether there's a way to extract embeddings for individual words using this model or if the output is always a single vector representing the whole input.
Any insights or examples would be greatly appreciated!
","Everything you described is 100% expected.
Q: Is the 1536-dimensional vector generated for the entire input text?
A: Yes.
Q: If the 1536-dimensional vector represents the entire input text, how does the model handle individual words versus longer texts like sentences or paragraphs?
A: First, the OpenAI Embeddings model doesn't handle a single word any different than a long text. For the model, it's an input. The input can be even a single character (e.g., ""a""), but it doesn't make sense to calculate an embedding vector out of it since ""a"" doesn't semantically mean anything to us humans.
Second, what you probably meant with this question is what happens when you do a similarity search with these embeddings. In other words, what happens when you use them? What happens if you use embeddings of words, sentences, paragraphs, or the whole text? Does it matter? Yes!
This is called chunking. The decision about how to chunk your text depends on the use case. The best thing is probably to simply try and see. If you get meaningful results after doing a similarity search, then this means that chunking is appropriate (even if this means chunking the whole text). If you don't get meaningful results after doing a similarity search, then this means that chunking isn't appropriate (e.g., instead of chunking by paragraph, try chunking by sentences).
There's an excellent Stack Overflow blog post about this topic you should read (pay attention to the bolded text because this is the best explanation):

With RAG, you create text embeddings of the pieces of data that you
want to draw from and retrieve. That allows you to place a piece of
the source text within the semantic space that LLMs use to create
responses.
/.../
When it comes to RAG systems, you’ll need to pay special attention to
how big the individual pieces of data are. How you divide your data up
is called chunking, and it’s more complex than embedding whole
documents.
/.../
The size of the chunked data is going to make a huge difference in
what information comes up in a search. When you embed a piece of data,
the whole thing is converted into a vector. Include too much in a
chunk and the vector loses the ability to be specific to anything it
discusses. Include too little and you lose the context of the data.

",chatgpt
azure open ai while developing python script using api key getting error,"import openai

# Setup
openai.api_key = 'xxxxxxxxxxxxxxxxxxxxxx'
openai.api_base = ""xxxxxxxxxxxxxxxxxxxxxx""

openai.api_version = '2024-08-20'  # Ensure this is correct

def test_openai():
    try:
        response = openai.Image.create(
            prompt=""A dog in rain image"",
            model=""dall-e-3"",  # Try with a different model ifneeded
            n=1,
            size=""1024x1024""
        )
        print(response)
    except Exception as e:
        print(f""Error: {e}"")

test_openai()


Error: Resource not found
I have created azure open ai model dall-e-3
the api key and api base both worked with model gpt-35-turbo
","The error you are getting because you need to use the Deployment name instead of the Model name.

You need to create client to connect  to generate result.
Below code worked for me.
I am using a Flask app to use the openai code and view the generated image in index.html file.
app.py:
from flask import Flask,render_template
import os
from openai import AzureOpenAI
import json

app = Flask(__name__)

@app.route('/', methods=['GET'])
def fetch_image_url():

    try:
        client = AzureOpenAI(
        api_version=""2024-05-01-preview"",
        azure_endpoint=""https://xxxxxxxxxx.openai.azure.com/"",
        api_key=""xxxxxxxxxxxxxxxxxx""
        )

        result = client.images.generate(
            model=""dallechatgpt"",
            prompt=""A dog in rain image"",
            n=1
        )

        image_url = json.loads(result.model_dump_json())['data'][0]['url']

        return render_template('index.html',image_url=image_url)
    except Exception as e:
        print(f""error{e}"")
        

if __name__== ""__main__"":
    app.run()

templates/index.html
<!DOCTYPE html>
<html lang=""en"">
<head>
    <meta charset=""UTF-8"">
    <meta name=""viewport"" content=""width=device-width, initial-scale=1.0"">
    <title>Document</title>
</head>
<body>
    {% if image %}
    <h2>Generated Image:</h2>
    <img src=""{{ image }}"" alt=""Generated Image"" style=""width: auto; height: 50vh;"">
    {% endif %}
</body>
</html>

requirements.txt:
flask
openai

OUTPUT:

",chatgpt
how do you get the context window of open ai models,"Is there a specific API call to get the context window of each model open ai offers?
I have printed out openai.Models.list() and there isn't any context window member in the json spit out.
","That information would probably best fit in the OpenAI API endpoint:
GET https://api.openai.com/v1/models
But the API reference shows it doesn't currently return the model's context window :(
Seems like the best available ""source of truth"" for this information is the Models page in the documentation.
",chatgpt
how to use wasm tiktoken in react webpack js,"The build issues due to wasm instantiate or wasm not being imported.
wasm.__wbindgen_add_to_stack_pointer is not a function
","To use tiktoken wasm in react for better performance, follow this for webpack 5.
In webpack config override add the following. You can directly use wasm from js cdn as well. Skip this step if you want to use cdn:
module.exports = (config, env) => {

  config.experiments = {
    asyncWebAssembly: true,
    layers: true,
  };

}

To use tiktoken with config override:
import { get_encoding, init } from 'tiktoken/init';
import wasm from 'tiktoken/tiktoken_bg.wasm';

  let tikInit = false;
  const countTokens= (str) =>{
    if (!tikInit) {
    tikInit = true;

      const wasmFile = await fetch(wasm);
      const buffer = await wasmFile.arrayBuffer();
      await init((imports) => WebAssembly.instantiate(buffer, imports));
    }

    const e = get_encoding('cl100k_base');
    const t = e.encode(str);
    e.free();
    return t.length;
  }

To use tiktoken with js cdn for wasm and without config override:
import { get_encoding, init } from 'tiktoken/init';
import wasm from 'tiktoken/tiktoken_bg.wasm';

  let tikInit = false;
  const countTokens= (str) =>{
    if (!tikInit) {
    tikInit = true;
}
   const wasmFile = await fetch(
      'https://cdn.jsdelivr.net/npm/tiktoken@1.0.15/tiktoken_bg.wasm'
    );
 
      const buffer = await wasmFile.arrayBuffer();
      await init((imports) => WebAssembly.instantiate(buffer, imports));
    }

    const e = get_encoding('cl100k_base');
    const t = e.encode(str);
    e.free();
    return t.length;
  }

",chatgpt
azure openai ingesion job api returns 404 resource not found,"Im following the documentation from Azure on the ingestion job API here: https://learn.microsoft.com/en-us/rest/api/azureopenai/ingestion-jobs/create?view=rest-azureopenai-2024-07-01-preview&tabs=HTTP
No matter the body of my request or the api-key in the request headers, I get { error: { code: '404', message: 'Resource not found' } } from the server. The endpoint im using is the one found in the Azure portal under Azure OpenAI resource -> Resource Management -> Keys and Endpoints -> Endpoint. The Azure OpenAI resource is deployed in Sweden Central.
Here is my request code:
const jobId = 'testing2793619'; // The ID of the job to be created

    const url = `${openaiEndpoint}/openai/ingestion/jobs/${jobId}?api-version=2024-07-01-preview`;

    const requestBody = {
        kind: ""SystemCompute"",
        searchServiceConnection: {
            kind: ""EndpointWithKey"",
            endpoint: searchEndpoint, // Replace with your Azure AI Search service endpoint,
            key: searchAdminApiKey // Replace with your Azure AI Search admin API key
        },
        datasource: {
            kind: ""Storage"", 
            connection: {
                kind: ""ConnectionString"",
                connectionString: blobStorageConnectionString
            },
            containerName: ""testcontainer"",
            chunking: {
                maxChunkSizeInTokens: 2048 // Customize as needed
            }
        },
        dataRefreshIntervalInHours: 24, // Customize as needed
        completionAction: ""cleanUpTempAssets"" // or ""cleanUpTempAssets"" depending on your needs
    };

    try {
        const response = await fetch(url, {
            method: 'PUT',
            headers: {
                'Content-Type': 'application/json',
                'api-key': openaiApiKey
            },
            body: JSON.stringify(requestBody)
        });

        if (response.ok) {
            const data = await response.json();
            console.log('Request successful:', data);
            return data;
        } else {
            const errorData = await response.json();
            console.error('Request failed ingestion:', errorData);
        }
    } catch (error) {
        console.error('Error:', error);
    }

","
The 404 Resource not found error occurs when trying to create an ingestion job using the Azure OpenAI Ingestion Job API due to an invalid endpoint URL and API key.

Below is how we configure the endpoint URL and API key:
async function createIngestionJob() {
    const endpoint = ""https://AzureOpenapiName.openai.azure.com""; 
    const jobId = ""ingestion-job""; // Replace with your job ID
    const apiVersion = ""2024-07-01-preview"";

 try {
        const response = await axios.put(
            `${endpoint}/openai/ingestion/jobs/${jobId}?api-version=${apiVersion}`,
            requestBody,
            { headers }
        );

  const headers = {
        'api-key': 'YOUR_API_KEY', 
        'Content-Type': 'application/json'
    };

The  javascript code below is for  creation of an ingestion job using the Azure OpenAI Service with an endpoint and API version.
const axios = require('axios');

async function createIngestionJob() {
    const endpoint =""https://AzureOpenapiName.openai.azure.com""; 
    const jobId = ""ingestion-job""; // Replace with your job ID
    const apiVersion = ""2024-07-01-preview"";

  
    const headers = {
        'api-key': 'YOUR_API_KEY', 
        'Content-Type': 'application/json',
        'mgmt-user-token': 'YOUR_MGMT_USER_TOKEN', // If required
        'aml-user-token': 'YOUR_AML_USER_TOKEN'    // If required
    };
    const requestBody = {
        ""kind"": ""SystemCompute"",
        ""searchServiceConnection"": {
            ""kind"": ""EndpointWithManagedIdentity"",
            ""endpoint"": ""https://aykame-dev-search.search.windows.net""
        },
        ""datasource"": {
            ""kind"": ""Storage"",
            ""connection"": {
                ""kind"": ""EndpointWithManagedIdentity"",
                ""endpoint"": ""https://mystorage.blob.core.windows.net/"",
                ""resourceId"": ""/subscriptions/1234567-abcd-1234-5678-1234abcd/resourceGroups/my-resource/providers/Microsoft.Storage/storageAccounts/mystorage""
            },
            ""containerName"": ""container"",
            ""chunking"": {
                ""maxChunkSizeInTokens"": 2048
            },
            ""embeddings"": [
                {
                    ""connection"": {
                        ""kind"": ""RelativeConnection""
                    },
                    ""deploymentName"": ""Ada""
                }
            ]
        },
        ""dataRefreshIntervalInHours"": 24,
        ""completionAction"": ""keepAllAssets""
    };

    try {
        const response = await axios.put(
            `${endpoint}/openai/ingestion/jobs/${jobId}?api-version=${apiVersion}`,
            requestBody,
            { headers }
        );
        
        console.log('Ingestion job created successfully:', response.data);
        console.log('Operation location:', response.headers['operation-location']);
    } catch (error) {
        console.error('Error creating ingestion job:', error.response.data);
    }
}

createIngestionJob();



",chatgpt
endpoint returning 403 after successfully executing spring boot,"I made an endpoint ""/api/chat"" that, when called, makes another request to the OpenAI API (paid for and API key valid, all that). When testing the endpoint in Postman, I first request my /auth/login endpoint to get a valid JWT login token, copy that for the Authorization header of my /api/chat call, then send it (The login auth token is 100% valid, and works perfectly for all other endpoints as well).
When the request to /api/chat is made, Postman takes around 1.5-2 seconds to process it. In the first second, my Run Console in Intellij logs the correct things to signify a request being made. For the remaining time, it then logs the exact expected response from the OpenAI API model. You would think this means everything executed fine, and it seemingly did, but then Postman (and my website if you try it there), returns 403 instead of 200 OK. This is an issue because even though I can physically see the correct response being logged in my IDE console with no errors, the 403 means I can't extract any of it for use and the website can't receive it and treats it like a completely failed request.
The ONLY way that I ""fixed"" this was making the /api/chat endpoint public in my SecurityConfig file, which made it execute the exact same way and give the same result, but returns 200 OK as expected. However, I, of course, can't leave it like this as it would mean people have public access to make calls to an endpoint that costs me for each request.
I've spent many hours trying to figure this out, including Googling, asking GPT - nothing. Verified all my JWT authentication stuff, tried something where the auth token is retained throughout, nothing worked. CORS and CSRF stuff, seemingly not that either. I can only think its something up with Spring Security because adding the endpoint to the .permitAll() list (making it public) in SecurityConfig is the only thing that made it return 200 OK as expected. This is extremely frustrating because it executes and logs exactly how and what it should, but just for some reason returns 403.
This is my first project with Spring so if its something trivial then please help out. Thanks in advance. The API key is set in my env variables, and even though I've included some error logging, no errors are ever logged, only the correct responses that I'm expecting from the AI model, with it then returning 403 as I've mentioned. I'll include the relevant bits of code, but if you think the issue is in another file or part of these files, let me know and I'll share what I can. Not that it's too important I don't think, but I'm using React for frontend.
:
Security Config:
@Bean
    public SecurityFilterChain securityFilterChain(HttpSecurity http) throws Exception {
        http.csrf()
                .disable()
                .authorizeHttpRequests(authorize -> authorize
                        .requestMatchers(""/auth/**"", ""/"", ""/index.html"", ""/manifest.json"", ""/static/**"", ""/*.js"", ""/*.jsx"", ""/*.css"", ""/home"", ""/log-in"", ""/sign-up"")
                        .permitAll()
                        .requestMatchers(""/auth/signup"", ""/auth/login"").anonymous()
                        .anyRequest()
                        .authenticated()
                )
                .sessionManagement(session -> session
                        .sessionCreationPolicy(SessionCreationPolicy.STATELESS)
                )
                .authenticationProvider(authenticationProvider)
                .addFilterBefore(jwtAuthenticationFilter, UsernamePasswordAuthenticationFilter.class);

        return http.build();

Controller:
@PostMapping
    public Mono<ResponseEntity<String>> getChatCompletion(@RequestBody ChatRequest chatRequest, @RequestHeader HttpHeaders headers) {

        return openAiService.getChatCompletion(chatRequest.getInput())
                .map(response -> {
                    logger.info(""Response: {}"", response); // Logging response
                    return ResponseEntity.ok(response);
                })
                .defaultIfEmpty(ResponseEntity.noContent().build());
    }

    public static class ChatRequest {
        private String input;

        public String getInput() {
            return input;
        }

        public void setInput(String input) {
            this.input = input;
        }
    }

Service:
public Mono<String> getChatCompletion(String userInput) {
        String requestBody = String.format(""""""
                {
                    ""model"": ""gpt-3.5-turbo"",
                    ""messages"": [
                        {
                            ""role"": ""system"",
                            ""content"": ""MY CONTENT""
                        },
                        {
                            ""role"": ""user"",
                            ""content"": ""%s""
                        }
                    ],
                    ""temperature"": 1,
                    ""max_tokens"": 256,
                    ""top_p"": 1,
                    ""frequency_penalty"": 0,
                    ""presence_penalty"": 0
                }
                """""", userInput);

        logger.info(""Sending request to OpenAI with body: {}"", requestBody);

        return this.webClient.post()
                .uri(""/chat/completions"")
                .header(""Content-Type"", ""application/json"")
                .header(""Authorization"", ""Bearer "" + openaiApiKey)
                .bodyValue(requestBody)
                .retrieve()
                .bodyToMono(String.class)
                .doOnNext(response -> logger.info(""Received response from OpenAI: {}"", response))
                .doOnError(WebClientResponseException.class, error -> {
                    logger.error(""Error response from OpenAI: {}"", error.getResponseBodyAsString());
                })
                .doOnError(error -> logger.error(""Error occurred: "", error));
    }

My Postman request (not too relevant, this is not the issue):
{
input: ""MY INPUT""
}

","I tried adding the @ResponseBody but that didn't solve it. After more digging, it seems it was an issue with the Mono<ResponseEntity> return type, as that is telling Spring to process the request asynchronously and non-blocking. Converting the method to block with "".block()"" on the Mono, and making it just a normal blocking call allowed it to wait for a response from the service before proceeding. Kinda means you don't get the benefits of a reactive programming solution but at least we can progress with this now.
If you run into the same issue, try converting the call to a traditional blocking call and see if that works. Still don't really know what in Spring's reactive code is causing this but I'm glad to just move on for now.
",chatgpt
can39t import openai in node,"I have Node 16.13.1 installed and I installed openai globally with 'npm install -g openai'.
I imported the packages inside my script using:
const { Configuration, OpenAIApi } = require('openai')

However, when I ran my script, I got this error:
PS D:\Projects\OpenAI-Chat> node conversation.js
node:internal/modules/cjs/loader:936
  throw err;
  ^

Error: Cannot find module 'openai'
Require stack:
- D:\Projects\OpenAI-Chat\conversation.js
    at Function.Module._resolveFilename (node:internal/modules/cjs/loader:933:15)
    at Function.Module._load (node:internal/modules/cjs/loader:778:27)
    at Module.require (node:internal/modules/cjs/loader:1005:19)
    at require (node:internal/modules/cjs/helpers:102:18)
    at Object.<anonymous> (D:\Projects\OpenAI-Chat\conversation.js:1:38)
    at Module._compile (node:internal/modules/cjs/loader:1101:14)
    at Object.Module._extensions..js (node:internal/modules/cjs/loader:1153:10)
    at Module.load (node:internal/modules/cjs/loader:981:32)
    at Function.Module._load (node:internal/modules/cjs/loader:822:12)
    at Function.executeUserEntryPoint [as runMain] (node:internal/modules/run_main:81:12) {
  code: 'MODULE_NOT_FOUND',
  requireStack: [ 'D:\\Projects\\OpenAI-Chat\\conversation.js' ]
}

How do I fix this?
","You have to install it locally with
npm i openai

because Node.js doesn't search in the global folder for dependencies for the local project. Global installation is for global executables and its dependencies.
",chatgpt
how to parse answer  content  from openai response to markdown format,"I have a Next.js action that uses OpenAI to generate an answer :
export async function continueConversation(
  messages: CoreMessage[],
  data: PersonResponse,
  prompt: string
) {
  const { full_name, job_title, ...rest } = data
  const result = await streamText({
    system: `You are a ${job_title} and your name is ${full_name}. Answer the questions based on ${JSON.stringify(
      rest
    )}`,
    model: openai(""gpt-4o-2024-05-13""),
    messages,
    prompt,
  })

  const stream = createStreamableValue(result.textStream)
  return stream.value
}


In the UI, I have this part of code:
{messages.map((m, i) => (
              <div key={i} className=""whitespace-pre-wrap flex flex-col px-8"">
                <div
                  className={cn(
                    ""flex items-start"",
                    m.role === ""user"" && ""justify-end""
                  )}
                >
                  {m.role === ""assistant"" && (
                    <Avatar className=""w-8 h-8 mr-3 mt-1"">
                      <AvatarImage
                        src={`https://${github_url}.png`}
                        alt={getInitials(full_name as string)}
                      />
                      <AvatarFallback className=""text-sm font-semibold tracking-[2px] bg-gradient-to-r from-[#ffaa40] via-[#9c40ff] to-[#ffaa40]"">
                        {getInitials(full_name as string)}
                      </AvatarFallback>
                    </Avatar>
                  )}
                  <span
                    className={cn(
                      ""my-2"",
                      m.role === ""user"" &&
                        ""bg-neutral-800 px-4 py-3 rounded-full""
                    )}
                  >
                    {m.content as string}
                  </span>
                </div>
              </div>
            ))}


In the output, I get the result as Markdown:

","You can use react-markdown package to display the markdown and the package size is only 45.4kb
npm install react-markdown
import Markdown from ""react-markdown""

export default function Home(){
  // api calls here

  return (
   {messages.map((m, i) => (
      <div key={i} className=""whitespace-pre-wrap flex flex-col px-8"">
           <div>
             {m.role === ""assistant"" && (
              <Avatar >
                <AvatarImage
                  src={`https://${github_url}.png`}
                  alt={getInitials(full_name as string)}
                />
               <AvatarFallback className=""text-sm"">
                {getInitials(full_name as string)}
               </AvatarFallback>
             </Avatar>
             )}
             <span
              className={cn(
               ""my-2"",
              m.role === ""user"" &&
              ""bg-neutral-800 px-4 py-3 rounded-full""
            )}
            >
              <Markdown>{m.content}</Markdown>
          </span>
         </div>
      </div>
   ))}
  )
}

",chatgpt
authenticationerror 401  principal does not have access to apioperation when using azure open ai with python sdk,"Issue Summary:
Encountering an AuthenticationError with error code 401 when attempting to access the OpenAI API. The error message indicates that the principal does not have access to the API/Operation.
Expected Outcome:
The service principal should have access to the specified API/Operation based on the roles and permissions assigned.
Actual Outcome:
Receiving the following error message: Principal does not have access to API/Operation. This prevents the service principal from accessing the API as expected.
Environment Details:

Subscription ID: Redacted

Role(s) Assigned: Contributor , User

Application ID: Redacted

API/Operation: Microsoft.CognitiveServices/accounts/listKeys/action

Azure Resource: Redacted


Appreciate any support on the above
","You need to have Cognitive Services OpenAI User or Cognitive Services OpenAI Contributor or Cognitive Services User role to access the open ai resource for inferencing.
Next, use below code.
from azure.identity import get_bearer_token_provider, ClientSecretCredential
from openai import AzureOpenAI
cred = ClientSecretCredential(client_id=""<yourClientId>"",tenant_id=""YourTenantId"",client_secret=""<ClientSeceret>"")
token_provider = get_bearer_token_provider(
    cred, ""https://cognitiveservices.azure.com/.default""
)

client = AzureOpenAI(
    api_version=""2024-02-15-preview"",
    azure_endpoint=""https://v-jgsopenai.openai.azure.com/"",
    azure_ad_token_provider=token_provider
)

response = client.chat.completions.create(
    model=""gpt35"", # model = ""deployment_name"".
    messages=[
        {""role"": ""system"", ""content"": ""You are a helpful assistant.""},
        {""role"": ""user"", ""content"": ""Does Azure OpenAI support customer managed keys?""},
        {""role"": ""assistant"", ""content"": ""Yes, customer managed keys are supported by Azure OpenAI.""},
        {""role"": ""user"", ""content"": ""Do other Azure AI services support this too?""}
    ]
)

print(response.choices[0].message.content)

Output:

",chatgpt
sending axios request to whisper endpoint with file input in clientside js,"I have a working NodeJS code that makes a request to /audio/transcriptions endpoint on a localhosted OpenAI API.
The file is given as fs.createReadStream(""speech.m4a"").
The node application works perfectly as it should, giving output: {text: ""<the transcription of the speech>""}
However when I want to do the same request, but with an input field instead of a hardcoded file, I get AxiosError with the message 'Network error'.
I have tried to call axios.post like this:
const API_URL = ""http://172.26.175.134:8080/openai/v1/audio/transcriptions"";
const fileInput = document.getElementById(""file-input"");
fileInput.onchange = () => {
  const files = fileInput.files;
  //if no file is selected
  if (files.length === 0) {
    alert(""Please select a file!"");
    return;
  }
  const selectedFile = files[0];
  const reader = new FileReader();
  reader.onload = (e) => {
    axios
      .post(
        API_URL,
        {
          file: new File([e.target.result], { type: selectedFile.type }),
          model: ""whisper"",
          language: ""en"",
          temperature: 1,
          prompt: """",
        },
        {
          headers: {
            ""Content-Type"": ""multipart/form-data"",
          },
        }
      )
      .then((response) => {
        console.debug(response.data);
      })
      .catch((error) => {
        console.error(""Error:"", error);
      });
  };
  reader.readAsArrayBuffer(selectedFile);
};

The result is the same, I get the same AxiosError with the message Network error.
I have also noticed that the Response is 200 OK, so the transcript is generated.
The API response:
INFO: 172.26.160.1:62548 - ""POST /openai/v1/audio/transcriptions HTTP/1.1"" 200 OK

The backend response:
INFO:faster_whisper:Processing audio with duration 00:19.925
INFO:faster_whisper:Detected language 'en' with probability 1.00
INFO:__main__:Completed /tmp/tmpswy0__ru
INFO:__main__:Transcription complete!

Update: I have also switched from axios to fetch, it is still a valid request (response is 200 OK), but still I can't fetch the response
reader.onload = async (e) => {
    const body = new FormData();
    body.append(
      ""file"",
      new File([e.target.result], { type: selectedFile.type })
    );
    body.append(""model"", ""whisper"");
    body.append(""language"", ""en"");
    body.append(""temperature"", ""1"");
    body.append(""prompt"", """");
    await fetch(API_URL, {
      method: ""POST"",
      body: body,
    })
      .then((res) => {
        console.debug(res);
      })
      .catch((err) => {
        console.debug(err);
      });
  };

TypeError: Failed to fetch at reader.onload

My question is: what can cause the issue with this request? What can I do to retrieve the response content as expected?
","I have managed to resolve the issue by fixing CORS error on the API's side. I had to add the following lines (especially allow_origins=[""*""])  to the API-code (FastAPI written in Python)
from fastapi.middleware.cors import CORSMiddleware

app = FastAPI()
app.add_middleware(
    CORSMiddleware,
    allow_origins=[""*""],
    allow_credentials=True,
    allow_methods=[""*""],
    allow_headers=[""*""],
)

",chatgpt
how to convert openai functions to prompttemplate in langchain when using local llms,"Is is possible to Convert openai functions to PromptTemplate in langchain when using local llms and return final output similar to openai api function format
import langchain
functions = [
        {
            ""name"": ""get_current_weather"",
            ""description"": ""Get the current weather in a given location"",
            ""parameters"": {
                ""type"": ""object"",
                ""properties"": {
                    ""location"": {
                        ""type"": ""string"",
                        ""description"": ""The city and state, e.g. San Francisco, CA"",
                    },
                    ""unit"": {""type"": ""string"", ""enum"": [""celsius"", ""fahrenheit""]},
                },
                ""required"": [""location""],
            },
        }
    ]
llm = llm()

#Pass functions into prompt template in langchain 

#Format prompt template similar to openai manner




","I used openweather to get the temperature information.  My tool is a function called get_current_weather which reaches a city, state and a unit.  The unit is the metric or imperial results.
def get_current_weather(location, unit='celsius'):
    units = 'metric' if unit == 'celsius' else 'imperial'
    base_url = ""http://api.openweathermap.org/data/2.5/weather""
    params = {
    ""q"": location,
    ""units"": units,
    ""appid"": weather_api_key  # Replace with your actual API key
    }
    query_string = urllib.parse.urlencode(params)
    url = f""{base_url}?{query_string}""
    print(url)
    response = requests.get(url)
    if response.status_code == 200:
        data = response.json()
        print(data)
        weather = {
            'location': data['name'],
            'temperature': data['main']['temp'],
            'description': data['weather'][0]['description'],
            'humidity': data['main']['humidity'],
            'pressure': data['main']['pressure']
        }
        return weather
    else:
        return {'error': 'City not found or API limit exceeded'}
    
tools = [
        Tool(
            name=""Get Weather"",
            func=get_current_weather,
            description=""Fetches the weather by location for a specific unit""
        ),
                   
]

llm=ChatOpenAI(model=""gpt-4o-mini"",temperature=0, openai_api_key=key)
agent = initialize_agent(tools, llm, agent=""zero-shot-react-description"", verbose=True)

response = agent.run({""input"": ""slc, ut"", ""unit"": ""celsius""})
print(response)

",chatgpt
how do i print each element in an openai chatcompletion response as json on a separate line,"I'm making a simple call to OpenAI using Python asking about where a baseball game was played.
completion = openai.chat.completions.create(
  model=""gpt-3.5-turbo"",
  messages = [
  {""role"": ""system"", ""content"": ""You are a helpful assistant.""},
  {""role"": ""user"", ""content"": ""Who won the world series in 2020?""},
  {""role"": ""assistant"", ""content"": ""The Los Angeles Dodgers won the World Series in 2020.""},
  {""role"": ""user"", ""content"": ""Where was it played?""}
  ]
)

print (completion)

The output shows up like this:
ChatCompletion(id='chatcmpl-9UgP85B0gYBjEAiYMcF3Ryt9Y3fdZ', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='The 2020 World Series was played at Globe Life Field in Arlington, Texas.', role='assistant', function_call=None, tool_calls=None))], created=1717099870, model='gpt-3.5-turbo-0125', object='chat.completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=17, prompt_tokens=53, total_tokens=70))

But I'd like it to show up like this:
{
  ""choices"": [
    {
      ""finish_reason"": ""stop"",
      ""index"": 0,
      ""message"": {
        ""content"": ""The 2020 World Series was played in Texas at Globe Life Field in Arlington."",
        ""role"": ""assistant""
      },
      ""logprobs"": null
    }
  ],
  ""created"": 1677664795,
  ""id"": ""chatcmpl-7QyqpwdfhqwajicIEznoc6Q47XAyW"",
  ""model"": ""gpt-3.5-turbo-0613"",
  ""object"": ""chat.completion"",
  ""usage"": {
    ""completion_tokens"": 17,
    ""prompt_tokens"": 57,
    ""total_tokens"": 74
  }
}

FWIW, I'm using Python 3.12 and Windows Terminal.
","Use .model_dump_json()
completion = client.chat.completions.create(
  model=""gpt-3.5-turbo"",
  messages = [
  {""role"": ""system"", ""content"": ""You are a helpful assistant""},
  {""role"": ""user"", ""content"": ""Who won the world series in 2020?""},
  {""role"": ""assistant"", ""content"": ""The Los Angeles Dodgers won the World Series in 2020.""},
  {""role"": ""user"", ""content"": ""Where was it played?""}
  ]
)

data = completion.model_dump_json()
print(data)

Output:
{""id"":""chatcmpl-9VGebGyeosYhcWW19F6f5B6lMFhpl"",""choices"":[{""finish_reason"":""stop"",""index"":0,""logprobs"":null,""message"":{""content"":""The World Series in 2020 was played at Globe Life Field in Arlington, Texas."",""role"":""assistant"",""function_call"":null,""tool_calls"":null}}],""created"":1717239213,""model"":""gpt-3.5-turbo-0125"",""object"":""chat.completion"",""system_fingerprint"":null,""usage"":{""completion_tokens"":18,""prompt_tokens"":52,""total_tokens"":70}}

If you need to pretty print, you can just use the ident parameter:
data = completion.model_dump_json(indent=3)
print(data)

Output:
{
   ""id"": ""chatcmpl-9VGphVuZLKh2QuqT9vSAUdWCxoifg"",
   ""choices"": [
      {
         ""finish_reason"": ""stop"",
         ""index"": 0,
         ""logprobs"": null,
         ""message"": {
            ""content"": ""The 2020 World Series was played at Globe Life Field in Arlington, Texas, home of the Texas Rangers."",
            ""role"": ""assistant"",
            ""function_call"": null,
            ""tool_calls"": null
         }
      }
   ],
   ""created"": 1717239901,
   ""model"": ""gpt-3.5-turbo-0125"",
   ""object"": ""chat.completion"",
   ""system_fingerprint"": null,
   ""usage"": {
      ""completion_tokens"": 23,
      ""prompt_tokens"": 52,
      ""total_tokens"": 75
   }
}

",chatgpt
do i need any environment variables set to execute some code call openai39s api and return a response,"I was going through a course in OpenAI's API using an in-browser jupyter notebook page but wanted to copy some example code from there into a local IDE. I installed Python and the jupyter extention in VS Code and the OpenAI library. My code is below:
import openai
import os

# from dotenv import load_dotenv, find_dotenv
# _ = load_dotenv(find_dotenv()) # read local .env file

openai.api_key  = ""my api key is here""

def get_completion(prompt, model=""gpt-3.5-turbo""):
    messages = [{""role"": ""user"", ""content"": prompt}]
    response = openai.ChatCompletion.create(
        model=model,
        messages=messages,
        temperature=0, # this is the degree of randomness of the model's output
    )
    return response.choices[0].message[""content""]

prompt = f""""""
Determine whether each item in the following list of \
topics is a topic in the text below, which
is delimited with triple backticks.

Give your answer as list with 0 or 1 for each topic.\

List of topics: {"", "".join(topic_list)}

Text sample: '''{story}'''
""""""
response = get_completion(prompt)
print(response)

I installed Python and imported the openai library. When I run I am getting the error:
APIConnectionError: Error communicating with OpenAI: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))

I'm assuming that's because I commented out lines 3 and 4 in the code because I am unsure what they do and do not know how to use the dotenv library. Is it simple to set this up just to make a basic call to the openai API? That's all I'm trying to do with this code right now.
","Usually, you load your API KEY from your .env file but, as you are hardcoding it, you don't need anything else.
The error you are getting might be related to the absence of the topic_list and story definitions.
",chatgpt
resourcenotfound error when connecting to azure openai using latest javascript sdk version 200beta1,"I am trying to connect to Azure OpenAI using the latest version of SDK (version 2.0.0-beta.1) but no matter what I do, I am getting Resource not found error). When I use the previous version (1.0.0-beta.12) of the SDK, everything works well.
These are the settings in my environment file:
AZURE_OPENAI_ENDPOINT=""https://xxx.openai.azure.com/""
AZURE_OPENAI_API_KEY=""xxxxyyyyxxxxyyyyxxxxyyyyxxxxyyyy""
AZURE_OPENAI_CHAT_COMPLETION_MODEL_DEPLOYMENT_ID=""gpt-4o""
AZURE_OPENAI_API_VERSION=""2024-04-01-preview""

and this is how I am creating my client:
azureOpenAIClient = new AzureOpenAI({
    baseURL: process.env.AZURE_OPENAI_ENDPOINT,
    // apiKey: process.env.AZURE_OPENAI_API_KEY, (tried without commenting it as well)
    deployment: process.env.AZURE_OPENAI_CHAT_COMPLETION_MODEL_DEPLOYMENT_ID,
    apiVersion: process.env.AZURE_OPENAI_API_VERSION,
});

and this is my code:
const messages: OpenAI.ChatCompletionMessageParam[] = [
    { role: 'system', content: systemMessage },
    { role: 'user', content: userMessage },
    ];
const result = await azureOpenAIClient.chat.completions.create(
    {
        messages,
        model: '',
        response_format: { type: jsonOutput ? 'json_object' : 'text' },
    },
    {},
    );
let response = '';
for await (const choice of result.choices) {
    response += choice.message?.content;
}

I even tried changing the model to my deployment id above but the result is the same.
I am pretty sure I am missing something really trivial but I am not able to figure it out. Can anyone tell me what I am doing wrong here? Thanks.
UPDATE
Here's the stack trace:
NotFoundError: 404 Resource not found
    at APIError.generate (webpack-internal:///(action-browser)/../../node_modules/openai/error.mjs:67:20)
    at AzureOpenAI.makeStatusError (webpack-internal:///(action-browser)/../../node_modules/openai/core.mjs:304:65)
    at AzureOpenAI.makeRequest (webpack-internal:///(action-browser)/../../node_modules/openai/core.mjs:347:30)
    at process.processTicksAndRejections (node:internal/process/task_queues:95:5)
....rest is the list of my files

","The issue was that I was trying to set baseURL property when I was instantiating AzureOpenAI. The correct property is endpoint.
Here's the GitHub issue that solved my problem: https://github.com/Azure/azure-sdk-for-js/issues/30669.
",chatgpt
openai assistants api how do i pass the assistant to the api,"I am trying to pass my assistant to the OpenAI Assistants API, but I get the following error when doing so:

message: 'The model my Model ID does not exist or you do not have
access to it.',
type: 'invalid_request_error',
param: null,
code: 'model_not_found'

Code:
try {
    const fetch = await import('node-fetch').then(mod => mod.default);

    const response = await fetch('https://api.openai.com/v1/chat/completions', {
        method: 'POST',
        headers: {
            'Content-Type': 'application/json',
            'Authorization': `Bearer ${apiKey}`
        },
        body: JSON.stringify({
            model: 'My model ID',
            messages: conversation
        })
    });
}

If I change the model parameter to gpt-3.5-turbo it works, and my app can interface with the standard model.
I know the API key is correct and it has all permissions. The assistant ID I'm trying to pass to the model parameter is also correct. For testing purposes, both are defined in the same file.
","Problem
The OpenAI Assistants API doesn't use the Chat Completions API endpoint (i.e., https://api.openai.com/v1/chat/completions). Using the OpenAI Assistants API is fundamentally different (i.e., more complex) than using any other APIs, like the Completions API or Chat Completions API.
Solution
You don't pass the assistant by using the model parameter. You pass the assistant by using the assistant_id parameter (see Step 4 below).
Here are the steps you need to follow to get a response from the assistant:
Step 1: Create an assistant
POST https://api.openai.com/v1/assistants

Step 2: Create a thread
POST https://api.openai.com/v1/threads

Step 3: Add the user's question to the thread
POST https://api.openai.com/v1/threads/{thread_id}/messages

Step 4: Run the assistant
POST https://api.openai.com/v1/threads/{thread_id}/runs

Step 5: Periodically retrieve the run to check its status to see if it has moved to completed
GET https://api.openai.com/v1/threads/{thread_id}/runs/{run_id}

Step 6: Retrieve the assistant's answer
GET https://api.openai.com/v1/threads/{thread_id}/messages

Also, I've made a YouTube tutorial on how to use the Assistants API and posted the code on my GitHub profile.
",chatgpt
nginx worker fails to boot flask app on elasticbeanstalk after installing openai,"Environment: Flask app running on python3.8 with AWS ElasticBeanstalk on an AL2 EC2 instance. After installing the OpenAI python package, there is an error in the EB instance on boot with the following error in the logs:
 from openai import OpenAI
web: File ""/var/app/venv/staging-LQM1lest/lib64/python3.8/site-packages/openai/__init__.py"", line 8, in <module>
web: from . import types
web: File ""/var/app/venv/staging-LQM1lest/lib64/python3.8/site-packages/openai/types/__init__.py"", line 5, in <module>
web: from .batch import Batch as Batch
web: File ""/var/app/venv/staging-LQM1lest/lib64/python3.8/site-packages/openai/types/batch.py"", line 7, in <module>
web: from .._models import BaseModel
web: File ""/var/app/venv/staging-LQM1lest/lib64/python3.8/site-packages/openai/_models.py"", line 25, in <module>
web: from ._types import (
web: File ""/var/app/venv/staging-LQM1lest/lib64/python3.8/site-packages/openai/_types.py"", line 21, in <module>
web: import httpx
web: File ""/var/app/venv/staging-LQM1lest/lib64/python3.8/site-packages/httpx/__init__.py"", line 2, in <module>
web: from ._api import delete, get, head, options, patch, post, put, request, stream
web: File ""/var/app/venv/staging-LQM1lest/lib64/python3.8/site-packages/httpx/_api.py"", line 6, in <module>
web: from ._client import Client
web: File ""<frozen importlib._bootstrap>"", line 991, in _find_and_load
web: File ""<frozen importlib._bootstrap>"", line 975, in _find_and_load_unlocked
web: File ""<frozen importlib._bootstrap>"", line 671, in _load_unlocked
web: File ""/var/app/venv/staging-LQM1lest/lib64/python3.8/site-packages/newrelic/api/import_hook.py"", line 174, in exec_module
web: self.loader.exec_module(module)
web: File ""/var/app/venv/staging-LQM1lest/lib64/python3.8/site-packages/httpx/_client.py"", line 32, in <module>
web: from ._transports.default import AsyncHTTPTransport, HTTPTransport
web: File ""/var/app/venv/staging-LQM1lest/lib64/python3.8/site-packages/httpx/_transports/default.py"", line 32, in <module>
web: import httpcore
web: File ""/var/app/venv/staging-LQM1lest/lib64/python3.8/site-packages/httpcore/__init__.py"", line 1, in <module>
web: from ._api import request, stream
web: File ""/var/app/venv/staging-LQM1lest/lib64/python3.8/site-packages/httpcore/_api.py"", line 5, in <module>
web: from ._sync.connection_pool import ConnectionPool
web: File ""/var/app/venv/staging-LQM1lest/lib64/python3.8/site-packages/httpcore/_sync/__init__.py"", line 1, in <module>
web: from .connection import HTTPConnection
web: File ""/var/app/venv/staging-LQM1lest/lib64/python3.8/site-packages/httpcore/_sync/connection.py"", line 12, in <module>
web: from .._synchronization import Lock
web: File ""/var/app/venv/staging-LQM1lest/lib64/python3.8/site-packages/httpcore/_synchronization.py"", line 11, in <module>
web: import trio
web: File ""/var/app/venv/staging-LQM1lest/lib64/python3.8/site-packages/trio/__init__.py"", line 26, in <module>
web: from . import abc, from_thread, lowlevel, socket, to_thread
web: File ""/var/app/venv/staging-LQM1lest/lib64/python3.8/site-packages/trio/socket.py"", line 16, in <module>
web: from . import _socket
web: File ""/var/app/venv/staging-LQM1lest/lib64/python3.8/site-packages/trio/_socket.py"", line 526, in <module>
web: class SocketType:
web: File ""/var/app/venv/staging-LQM1lest/lib64/python3.8/site-packages/trio/_socket.py"", line 719, in SocketType
web: @_wraps(_stdlib_socket.socket.sendmsg, assigned=(), updated=())
web: AttributeError: type object 'GreenSocket' has no attribute 'sendmsg'
web: [2024-07-29 21:44:08 +0000] [25719] [INFO] Worker exiting (pid: 25719)

I've tried changing versions of OpenAI and the relevant dependencies and as soon as the OpenAI package is removed the worker successfully runs.
","Here's what fixed my issue
Wherever the environment is being hosted navigate to it, enter the venv, and  pip uninstall trio. Trio gets installed as a dependency of httpx which is a dependency of openai and messes with stdlib packages while not actually being used.
",chatgpt
how can i allocate chat memory to the openai model,"I'm working in django, I have a view where I call the openai api, and in the frontend I work with react, where I have a chatbot, I want the model to have a record of the data, like the chatgpt page does.
class chatbot(APIView):

    def post(self, request):
        chatbot_response = None
        if api_key is not None and request.method == 'POST':
            openai.api_key = api_key
            user_input = request.data.get('user_input')
            prompt = user_input

            response = openai.Completion.create(
                model = 'text-davinci-003',
             
                prompt = prompt,
                max_tokens=250,
                temperature=0.5
            )
           

            chatbot_response = response[""choices""][0][""text""]
        if chatbot_response is not None:
            return Response({""response"": chatbot_response}, status=status.HTTP_200_OK)
        else:
            return Response({'errors': {'error_de_campo': ['Promt vacio']}},
                                status=status.HTTP_404_NOT_FOUND)

I was planning to create a model and save the questions in the database, but I don't know how to integrate that information into the view, I'm worried about the token spending, I don't really know how it works. I hope someone can clarify these doubts for me. thank you so much.
","How can I allocate chat memory to the openai model?
Save it in database
by saving into database, you can permanently save the history
by follow this
https://docs.djangoproject.com/en/4.2/topics/db/queries/
https://docs.djangoproject.com/en/4.0/intro/tutorial02/#database-setup

you need to create a database model containing User Identification & History

import jsonfield
from django.db import models


class ChatHistory(models.Model):
    owner_uid = models.CharField(max_length=100)
    chat = jsonfield.JSONField() # would be {USER, BOT} interaction

    def __str__(self):
        return self.name


Create A Get History Endpoint

from .models import ChatHistory
class chatbot(APIView):

    def post(self, request):
        chatbot_response = None
        if api_key is not None and request.method == 'POST':
            openai.api_key = api_key
            user_input = request.data.get('user_input')
            prompt = user_input

            response = openai.Completion.create(
                model = 'text-davinci-003',
             
                prompt = prompt,
                max_tokens=250,
                temperature=0.5
            )
           

            chatbot_response = response[""choices""][0][""text""]
        if chatbot_response is not None:
            return Response({""response"": chatbot_response}, status=status.HTTP_200_OK)
        else:
            return Response({'errors': {'error_de_campo': ['Promt vacio']}},
                                status=status.HTTP_404_NOT_FOUND)
    def get(self, request):
        uid = request.GET.get('uid')  # Assuming the uid is passed as a query parameter

        if uid is None:
            return Response({'errors': {'uid': ['UID parameter is required.']}}, status=status.HTTP_400_BAD_REQUEST)

        chat_history = ChatHistory.objects.filter(owner_uid=uid)
        serialized_data = ChatHistorySerializer(chat_history, many=True).data

        return Response({'data': serialized_data}, status=status.HTTP_200_OK)


Showing it In frontend?
well idk about your front-end code it's just a json data
you should managed to do it easily!
Worry About The Cost?
Well you shouldn't use text-davinci-3 for chatbot purpose in the firstplace!
text-davinci-3 is an InstructGPT model
Instruct models are optimized to follow single-turn instructions. Ada is the fastest model, while Davinci is the most powerful(and very expensive)
you should've used ChatCompletion with chatgpt-3.5-turbo instead
https://platform.openai.com/docs/api-reference/chat/create?lang=python
import os
import openai
openai.api_key = os.getenv(""OPENAI_API_KEY"")

completion = openai.ChatCompletion.create(
  model=""gpt-3.5-turbo"",
  messages=[
    {""role"": ""user"", ""content"": ""Hello!""}
  ]
)

print(completion.choices[0].message)

Why? take a look at the pricing
gpt-3.5-turbo   $0.002 / 1K tokens
Davinci         $0.0200 / 1K tokens
The Cost Effectivness is 10 Times!
",chatgpt
azure openai whipser rest endpoints,"I wanted to use Whisper deployed through Azure OpenAI but I am having trouble finding the right resources for it.
I am trying to integrate a translator using Whisper in a flutter app that will take multilingual input and give out the output in english.
Right now the transcription is working using this endpoint:
https://<dep>.openai.azure.com/openai/deployments/<name>/audio/transcriptions?api-version=2024-02-01 
and structuring the request like:
var uri = Uri.parse(whisperEndpoint);
          var request = http.MultipartRequest('POST', uri)
            ..headers['api-key'] = whisperApiKey
            ..files.add(await http.MultipartFile.fromPath('file', filePath));

What is the endpoint for translation, tts and other services?
I have tried sending the text transcribed by whisper back to be translated but I'd prefer direct translation by whisper
","You can use below endpoint for trnaslation.
https://reproaistudio.openai.azure.com/openai/deployments/<deplymentname>/audio/translations?api-version=2024-06-01

And it will convert it into English text.
Here, is the sample output with Bengali as the input file language.

If you want to convert this text to speech then use  tts-1 or tts-1-hd models in azure openai.
Make sure you have your open ai resource in supported region of above models
",chatgpt
calculating total tokens for api request to chatgpt including functions,"Hello Stack Overflow community,
I've been working on integrating ChatGPT's API into my project, and I'm having some trouble calculating the total number of tokens for my API requests. Specifically, I'm passing both messages and functions in my API calls.
I've managed to figure out how to calculate the token count for the messages, but I'm unsure about how to account for the tokens used by the functions.
Could someone please guide me on how to properly calculate the total token count, including both messages and functions, for a request to ChatGPT's API?
Any help or insights would be greatly appreciated!
Thank you in advance.
I have been working on brute forcing a solution by formatting the data in the call in different ways. I have been using the tokenizer, and Tiktokenizer to test my formats.
","I am going to walk you through calculating the tokens for gpt-3.5 and gpt-4. You can apply a similar method to other models you just need to find the right settings.
We are going to calculate the tokens used by the messages and the functions separately then adding them together at the end to get the total.
Messages
Start by getting the tokenizer using tiktoken. We will use this to tokenize all the custom text in the messages and functions. Also add constants for the extra tokens the API will add to the request.
enc = tiktoken.encoding_for_model(model)

Make a variable to hold the total tokens for the messages and set it to 0.
msg_token_count = 0

Loop through the messages, and for each message add 3 to msg_token_count. Then loop through each element in the message and encode the value, adding the length of the encoded object to msg_token_count. If the dictionary has the ""name"" key set add an additional token to msg_token_count.
for message in messages:
    msg_token_count += 3  # Add tokens for each message
    for key, value in message.items():
        msg_token_count += len(enc.encode(value))  # Add tokens in set message
        if key == ""name"":
            msgTokenCount += 1 # Add token if name is set

Finally we need to add 3 to msg_token_count, for the ending tokens.
msgTokenCount += 3 # Add tokens to account for ending

Functions
Now we are going to calculate the number of tokens the functions will take.
Start by making a variable to hold the total tokens used by functions and set it to 0.
func_token_count = 0

Next we are going to loop through the functions and add tokens to func_token_count. Loop through the functions and add 7 to func_token_count for each function. Then add the length of the encoded name and description.
For each function, if it has properties, add 3 to func_token_count. Then for each key in the properties add another 3 and the length of the encoded property, making sure to subtract 3 if it has an ""enum"" key, and adding 3 for each item in the enum section.
Finally, add 12 to func_token_count to account for the tokens at the end of all the functions.
for function in functions:
    func_token_count += 7  # Add tokens for start of each function
    f_name = function[""name""]
    f_desc = function[""description""]
    if f_desc.endswith("".""):
        f_desc = f_desc[:-1]
    line = f_name + "":"" + f_desc
    func_token_count += len(enc.encode(line))  # Add tokens for set name and description
    if len(function[""parameters""][""properties""]) > 0:
        func_token_count += 3  # Add tokens for start of each property
        for key in list(function[""parameters""][""properties""].keys()):
            func_token_count += 3  # Add tokens for each set property
            p_name = key
            p_type = function[""parameters""][""properties""][key][""type""]
            p_desc = function[""parameters""][""properties""][key][""description""]
            if ""enum"" in function[""parameters""][""properties""][key].keys():
                func_token_count += 3  # Add tokens if property has enum list
                for item in function[""parameters""][""properties""][key][""enum""]:
                    func_token_count += 3
                    func_token_count += len(enc.encode(item))
            if p_desc.endswith("".""):
                p_desc = p_desc[:-1]
            line = f""{p_name}:{p_type}:{p_desc}""
            func_token_count += len(enc.encode(line))
func_token_count += 12

Here is the full code. Please note that instead of hard coding the additional token counts I used a constant to hold the value.
def get_token_count(model, messages, functions):
    # Initialize message settings to 0
    msg_init = 0
    msg_name = 0
    msg_end = 0
    
    # Initialize function settings to 0
    func_init = 0
    prop_init = 0
    prop_key = 0
    enum_init = 0
    enum_item = 0
    func_end = 0
    
    if model in [
        ""gpt-3.5-turbo-0613"",
        ""gpt-4-0613""
    ]:
        # Set message settings for above models
        msg_init = 3
        msg_name = 1
        msg_end = 3
        
        # Set function settings for the above models
        func_init = 7
        prop_init = 3
        prop_key = 3
        enum_init = -3
        enum_item = 3
        func_end = 12
    
    enc = tiktoken.encoding_for_model(model)
    
    msg_token_count = 0
    for message in messages:
        msg_token_count += msg_init  # Add tokens for each message
        for key, value in message.items():
            msg_token_count += len(enc.encode(value))  # Add tokens in set message
            if key == ""name"":
                msg_token_count += msg_name  # Add tokens if name is set
    msg_token_count += msg_end  # Add tokens to account for ending
    
    func_token_count = 0
    if len(functions) > 0:
        for function in functions:
            func_token_count += func_init  # Add tokens for start of each function
            f_name = function[""name""]
            f_desc = function[""description""]
            if f_desc.endswith("".""):
                f_desc = f_desc[:-1]
            line = f_name + "":"" + f_desc
            func_token_count += len(enc.encode(line))  # Add tokens for set name and description
            if len(function[""parameters""][""properties""]) > 0:
                func_token_count += prop_init  # Add tokens for start of each property
                for key in list(function[""parameters""][""properties""].keys()):
                    func_token_count += prop_key  # Add tokens for each set property
                    p_name = key
                    p_type = function[""parameters""][""properties""][key][""type""]
                    p_desc = function[""parameters""][""properties""][key][""description""]
                    if ""enum"" in function[""parameters""][""properties""][key].keys():
                        func_token_count += enum_init  # Add tokens if property has enum list
                        for item in function[""parameters""][""properties""][key][""enum""]:
                            func_token_count += enum_item
                            func_token_count += len(enc.encode(item))
                    if p_desc.endswith("".""):
                        p_desc = p_desc[:-1]
                    line = f""{p_name}:{p_type}:{p_desc}""
                    func_token_count += len(enc.encode(line))
        func_token_count += func_end
    
    return msg_token_count + func_token_count

Please let me know if something is not clear, or if you have a suggestion to make my post better.
",chatgpt
using chatgpt4 vision in aspnet core,"I am working on a web application with openai integration. I have the standard chat prompt and response implemented, but I am having issues accessing the vision api. All of the examples I can find are in python. I whipped up quick Jupyter Notebook and called the vision model with my api key and it worked great. Looking for a way to translate this code into C# or a viable solution to use the code in my application.
from openai import OpenAI

client = OpenAI(api_key=""___"")

response = client.chat.completions.create(
    model=""gpt-4-vision-preview"",
    messages=[
    {
      ""role"": ""user"",
      ""content"": [
        {""type"": ""text"", ""text"": ""Does this image show a Fender Stratocaster Electric Guitar?        Respond with yes or no.""},
        {
          ""type"": ""image_url"",
          ""image_url"": {
            ""url"": ""https://toneshapers.com/cdn/shop/files/Les-Paul-Standard-Front.jpg"",
          },
        },
      ],
    }
  ],
  max_tokens=300,
)

print(response.choices[0])

I have tried to access ""chat.completions"" from my openai instance in C#, but they do not seem to exist.
","The documentation includes a cURL sample which can easily be translated to a C# HttpRequest:
curl https://api.openai.com/v1/chat/completions \
  -H ""Content-Type: application/json"" \
  -H ""Authorization: Bearer $OPENAI_API_KEY"" \
  -d '{
    ""model"": ""gpt-4-vision-preview"",
    ""messages"": [
      {
        ""role"": ""user"",
        ""content"": [
          {
            ""type"": ""text"",
            ""text"": ""What’s in this image?""
          },
          {
            ""type"": ""image_url"",
            ""image_url"": {
              ""url"": ""https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Gfp-wisconsin-madison-the-nature-boardwalk.jpg/2560px-Gfp-wisconsin-madison-the-nature-boardwalk.jpg""
            }
          }
        ]
      }
    ],
    ""max_tokens"": 300
  }'

",chatgpt
calculation of pricing of tokens in openai calls,"I'm trying to price the tokens used in a call to OPENAI. I have a txt file with plain text that was uploaded to Qdrant. When I ask the following question:
Who is Michael Jordan?
and use the get_openai_callback function to track the number of tokens and the price of the operation, one of the keys of information in the output doesn't make sense to me.
Tokens Used: 85
    Prompt Tokens: 68
    Completion Tokens: 17
Successful Requests: 1
Total Cost (USD): $0.00013600000000000003

Why does the Prompt Tokens value differ from the input value? The amount of tokens in the input text (which is what I understand as Prompt Token) is:
query = 'Who is Michael Jordan'

encoding = tiktoken.encoding_for_model('gpt-3.5-turbo-instruct')
print(f""Tokens: {len(encoding.encode(query))}"")

4

, but the output in the response is like 68. I considered the idea that Prompt Tokens were the sum of the base tokens (txt file) added to the question tokens, but the math doesn't fit.
Number of tokens in the txt file: 17
Arquivo txt: 'Michael Jeffrey Jordan is an American businessman and former basketball player who played as a shooting guard'
query + file_token: 21 (4+17)
Could anyone help me understand the pricing calculation?
I tried to search OPENAI's own documentation, github and other forums, but I don't think it's easy to find information or that it's open to the public. I want to understand if I'm missing something or if it's a calculation that users don't have access to.
UPDATE
For any future questions from other users:
import langchain 
langchain.debug = True

Run the get_openai_callback() function and see the entire log appear on the screen. The value of the ""prompts"" key is a list containing a string that is the instruction on how the response should be given. The number of tokens for this prompt is the value that appears in the Prompt Tokens.
","Prompt Tokens includes your question and any context provided, plus additional system messages and formatting added by the API. While Completion Tokens generated in the response.
In your example:
Visible Query: Who is Michael Jordan? (4 tokens)
Text from File: Michael Jeffrey Jordan is an American businessman and former basketball player who played as a shooting guard (17 tokens)
Expected: 4+17=21
4+17=21 tokens.
However, you see 68 prompt tokens because the API adds tokens for roles, instructions, and other metadata.To understand the exact token count, you can log the full request payload or use OpenAI's token counting tools.
This extra context explains why the prompt token count is higher than expected.
",chatgpt
unable to send a local image that isn39t under wwwroot to openai using its new net library,"I have an ASP.NET Core 6 Web API with the new official library from OpenAI (https://github.com/openai/openai-dotnet).
What I'm trying to do, is to use a local image file to OpenAI. The file isn't under wwwroot, but under backend/assets/1.jpg.

I've written a basic service to setup all the information needed in order to send a request to OpenAI. But the problem is that I'm not able to send the image.
I keep getting errors like ""url is too long"" or ""invalid image"",
Here is my code - OpenAiService:
using OpenAI.Chat;

namespace backend.Services
{
    public class OpenAiService
    {
        private readonly ChatClient _chatClient;
        private readonly ChatCompletionOptions _options;

        public OpenAiService(IConfiguration configuration)
        {
            var apiKey = configuration.GetValue<string>(""OpenAI:Key"");
            _chatClient = new ChatClient(""gpt-4o"", apiKey);

            _options = new ChatCompletionOptions()
            {
                MaxTokens = 300,
            };
        }

        public async Task<string> ExtractListOfItems()
        {
            var imagePath = Path.Combine(Directory.GetCurrentDirectory(), ""Assets"", ""1.jpg"");
            var localUrl = $""https://localhost:7068/assets/{Path.GetFileName(imagePath)}"";
            
            var messages = new List<ChatMessage>
            {
                new UserChatMessage(new List<ChatMessageContentPart>
                {
                    ChatMessageContentPart.CreateTextMessageContentPart(""Extract the items from the following image and return a list of items including prices and amount.""),
                    ChatMessageContentPart.CreateImageMessageContentPart(new Uri(localUrl))
                })
            };

            var completion = await _chatClient.CompleteChatAsync(messages, _options);
            return completion.Value.ToString();
        }
    }
}

Demo controller for testing:
using Microsoft.AspNetCore.Mvc;
using System.Threading.Tasks;
using backend.Services;
using OpenAI;
using OpenAI.Chat;

namespace backend.Controllers;

[ApiController]
[Route(""[controller]"")]
public class OpenAiDemoController : ControllerBase
{
    private readonly OpenAiService _openAiService;

    public OpenAiDemoController(OpenAiService openAiService)
    {
        _openAiService = openAiService;
    }
    
    [HttpPost]
    [Route(""extract-items"")]
    public async Task<IActionResult> CompleteSentence()
    {
        var completion = await _openAiService.ExtractListOfItems();
        return Ok(completion);
    }
}

program.cs file:
using backend.Configurations;
using backend.Services;
using Microsoft.Extensions.FileProviders;

var builder = WebApplication.CreateBuilder(args);

// Add services to the container.

builder.Services.Configure<OpenAiConfig>(builder.Configuration.GetSection(""OpenAI""));

//add services
builder.Services.AddSingleton<OpenAiService>();

builder.Services.AddControllers();
builder.Services.AddEndpointsApiExplorer();
builder.Services.AddSwaggerGen();

// builder.Services.AddScoped<IOpenAiService, OpenAiService>();

builder.Services.AddCors(opt =>
{
    opt.AddPolicy(""AllowAll"", builder =>
    {
        builder.AllowAnyOrigin()
            .AllowAnyMethod()
            .AllowAnyHeader();
    });
});

var app = builder.Build();

app.UseStaticFiles(new StaticFileOptions
{
    FileProvider = new PhysicalFileProvider(
        Path.Combine(builder.Environment.ContentRootPath, ""Assets"")),
    RequestPath = ""/assets""
});

app.UseStaticFiles(); // This serves files from wwwroot

app.UseStaticFiles(new StaticFileOptions
{
    FileProvider = new PhysicalFileProvider(
        Path.Combine(builder.Environment.ContentRootPath, ""Assets"")),
    RequestPath = ""/assets""
});

// Configure the HTTP request pipeline.
if (app.Environment.IsDevelopment())
{
    app.UseSwagger();
    app.UseSwaggerUI();
}

app.UseHttpsRedirection();
app.UseCors(""AllowAll"");
app.UseAuthorization();

app.MapControllers();

app.Run();

Any idea what I'm doing wrong?
","I was able to solve it on my own.
There's an example in openai official repo that helped me to solve it.
https://github.com/openai/openai-dotnet/blob/main/examples/Chat/Example05_ChatWithVisionAsync.cs

Tho, im not sure if my implementation is mostly correct. I will leave this open for any other suggestions.
service:
using OpenAI.Chat;


namespace backend.Services
{
    public class OpenAiService
    {
        private readonly ChatClient _chatClient;
        private readonly ChatCompletionOptions _options;

        public OpenAiService(IConfiguration configuration)
        {
            var apiKey = configuration.GetValue<string>(""OpenAI:Key"");
            _chatClient = new ChatClient(""gpt-4o"", apiKey);
            _options = new ChatCompletionOptions()
            {
                MaxTokens = 300,
            };
        }

        public async Task<string> ExtractListOfItems()
        {
            var imageFilePath = Path.Combine(""Assets"", ""1.jpg"");
            await using Stream imageStream = File.OpenRead(imageFilePath);
            var imageBytes = BinaryData.FromStream(imageStream);

            var messages = new List<ChatMessage>
            {
                new UserChatMessage(new List<ChatMessageContentPart>
                {
                    ChatMessageContentPart.CreateTextMessageContentPart(""describe the image. ""),
                    ChatMessageContentPart.CreateImageMessageContentPart(imageBytes, ""image/png"")
                })
            };

            var completion = await _chatClient.CompleteChatAsync(messages, _options);
            return completion.Value.ToString();
        }
        
    }
}

usage in controller:
using Microsoft.AspNetCore.Mvc;
using System.Threading.Tasks;
using backend.Services;
using OpenAI;
using OpenAI.Chat;

namespace backend.Controllers;

[ApiController]
[Route(""[controller]"")]
public class OpenAiDemoController : ControllerBase
{
    private readonly OpenAiService _openAiService;

    public OpenAiDemoController(OpenAiService openAiService)
    {
        _openAiService = openAiService;
    }
    
    [HttpPost]
    [Route(""extract-items"")]
    public async Task<IActionResult> CompleteSentence()
    {
        var completion = await _openAiService.ExtractListOfItems();
        return Ok(completion);
    }
    
}

no need to apply for static file middleware.
",chatgpt
how do i pass streaming data from nitro api to pinia store so i can use it in nuxt component,"In my Nuxt app I have a Pinia store and am making an API call via the Nitro server that gets streaming data in response. When I try to update the Pinia store with the streaming data it does not update, i.e. I can call
const streamOutput = ref({ value: """", annotations: [] })
const setStreamOutput = (output) => {
    console.log(""setStreamOutput"", output)
    streamOutput.value = output
    console.log(""setStreamOutput"", output)
}

and will see the console log but in my server logs, not in Browser and in my template and in the browser tool I still see the initial value for streamOutput.
export default defineEventHandler(async (event) => {
    const config = useRuntimeConfig()
    const body = await readBody(event)
    const chatStore = useChatStore()
    const readable = new Readable({
        read() {} // No-op implementation for read method
    })

    const run = sendStream(
        event,
        await openai.beta.threads
            .createAndRunStream({
                assistant_id: *ID*,
                thread: {
                    messages: [
                        { role: ""user"", content: ""Explain deep learning to a 5 year old."" }
                    ]
                }
            })
            .on(""messageCreated"", async (text) => {
                console.log(""\n\n messageCreated"", text)
            })
            .on(""textDelta"", async (textDelta, snapshot) => {
                console.log(""\n\n textDelta"", snapshot)
                await chatStore.setStreamOutput(snapshot)
            })
            .on(""toolCallCreated"", async (toolCall) => {
                console.log(""\n\n toolCallCreated"", toolCall)
            })
            .on(""toolCallDelta"", async (toolCallDelta, snapshot) => {
                console.log(""\n\n toolCallDelta"", snapshot)
                if (toolCallDelta.type === ""code_interpreter"") {
                    if (toolCallDelta.code_interpreter.input) {
                        chatStore.setStreamOutput(
                            chatStore.streamOutput + toolCallDelta.code_interpreter.input
                        )
                    }
                    if (toolCallDelta.code_interpreter.outputs) {
                        chatStore.setStreamOutput(chatStore.streamOutput + ""\noutput >\n"")
                        toolCallDelta.code_interpreter.outputs.forEach((output) => {
                            if (output.type === ""logs"") {
                                chatStore.setStreamOutput(
                                    chatStore.streamOutput + `\n${output.logs}\n`
                                )
                            }
                        })
                    }
                }
            })
            .on(""textDone"", async (content, snapshot) => {
                console.log(""\n\n text Done"")
            })
    )

    return sendStream(event, readable)
})

","For anyone running into this the solution has two parts:

Creating and returning a Readable
Updating your Pinia store to read the data stream

For creating a Readable
import { Readable } from ""stream""
import { sendStream } from ""h3""

const readable = new Readable({
        read() {} // No-op implementation for read method
    })
await openai.beta.threads
        .createAndRunStream({
            ...
        })
        .on(""messageCreated"", async (text) => {
            readable.push(`{""messageCreated"": ${JSON.stringify(text)}}\n`)
        })
//Handle other events similarly
...
return sendStream(event, readable)
})

For updating your Pinia store to read the data stream
(This is a bit tedious and I'm sure it can be optimized but I will forget about this answer in a bit and figure I should post before that)
const streamOutput = ref({ value: """", annotations: [] })

const setStreamOutput = (output) => {
    streamOutput.value = output
}

const createRun = async () => {
    try {
        const response = await fetch(""/api/run/create"", {
            method: ""POST"",
            body: JSON.stringify({
                threadId: thread.value.id
            })
        })

        if (!response.ok) {
            throw new Error(""Network response was not ok"")
        }

        const reader = response.body.getReader()
        const decoder = new TextDecoder(""utf-8"")

        let buffer = """"

        let done = false
        while (!done) {
            const { value, done: readerDone } = await reader.read()
            done = readerDone
            if (value) {
                buffer += decoder.decode(value, { stream: true })

                // Process complete JSON objects in the buffer
                let boundary
                while ((boundary = buffer.indexOf(""\n"")) !== -1) {
                    const chunk = buffer.slice(0, boundary).trim()
                    buffer = buffer.slice(boundary + 1)

                    if (chunk) {
                        try {
                            const json = JSON.parse(chunk)
                            if (json.textDelta) {
                                // VVVVV Here you can hook into your store's function to use the data
                                setStreamOutput(json.textDelta.value)
                            } else if (json.messageCreated) {
                                ... // Handle other streaming events
                                

",chatgpt
azureopenai missing credentials of api_key azure_ad_token azure_ad_token_provider,"I'm trying to use the Azure OpenAI model to generate comments based on data from my BigQuery table in GCP using Cloud Functions. Here's the Python script I've been working on:
from azure_openai import AzureOpenAI
def generate_comment(month, year, country, column_name, current_value, previous_value):
        prompt_ = ("""")
    
        client = AzureOpenAI(
            api_key=os.getenv(""AZURE_OPENAI_API_KEY""), ## tried also api_key=""AZURE_OPENAI_API_KEY""
            api_version=""2023-09-15-preview"",
            azure_endpoint=os.getenv(""AZURE_OPENAI_ENDPOINT"")
        )
    
        response = client.completions.create(model=""MODEL_NAME"", prompt=prompt_, max_tokens=50, temperature=0.35)
        return response.choices[0].text

I tried the old version before, but got openai.lib._old_api.APIRemovedInV1 error:
openai.api_type = ""azure""
openai.api_base = ""https://xxx.openai.azure.com/""
openai.api_version = ""2023-09-15-preview""
openai.api_key = ""xxx""

response = openai.Completion.create(
engine=""xxx"",
prompt=prompt_,
temperature=0.35)

return response['choices'][0]['message']['content']

However, I'm encountering a 500 Internal Server Error with the message:
ValueError: Must provide one of the `base_url` or `azure_endpoint` arguments, or the `AZURE_OPENAI_ENDPOINT` environment variable

I've checked my Azure OpenAI configuration and ensured that the API key and endpoint are correct. Could someone please help me identify what might be causing this error?
","It worked like the following:
import openai
# Set up the Azure OpenAI configuration
openai.api_type = ""azure""
openai.api_base = ""https://XXXX.openai.azure.com/""
openai.api_key = ""XXXX""
openai.api_version = ""XXXX""

def generate_comment():
prompt_ = """"
    messages = [
        {""role"": ""system"", ""content"": ""You will generate comments based on the given data.""},
        {""role"": ""user"", ""content"": prompt_}
    ]
    # Send a completion call to Azure OpenAI to generate a comment
    response = openai.ChatCompletion.create(
        engine=""XXXX"", # engine = ""deployment_name""
        messages=[
            {""role"": ""system"", ""content"": ""You will generate comments based on the given data.""},
            {""role"": ""user"", ""content"": prompt_}
        ],
        max_tokens=50,
        temperature=0.35
    )
    return response['choices'][0]['message']['content']

",chatgpt
how can i fix my flask server39s 405 error that includes openai api,"I'm trying to add an API to my webpage and have never used any Flask server before, I have never used Javascript too so this is a completely brand new learning experience.
My problem is that I keep receiving a 405 error code saying that the method is not allowed. I keep on using the POST method but it isn't working, I am banking that my issue may be with my HTML code more than my Flask server because the code is extremely generic and simple.
import openai
from flask import Flask, request, jsonify

app = Flask(__name__)

openai.api_key = '**my key is in here**'

@app.route('/', methods=['POST'])
def chat():
    data = request.get_json()
    message = data.get('message')
    
    response = openai.Completion.create(
        model=""gpt-3.5-turbo"",  
        prompt=message,
        max_tokens=50
    )

    return {'response': response.choices[0].text.strip()}

if __name__ == '__main__':
    app.run(port=5000)

async function sendMessage() {
            const message = document.getElementById('message').value;
            document.getElementById('chat-box').innerHTML += `<div>You: ${message}</div>`;
            
            const response = await fetch('/', {
                method: ""POST"",
                body: JSON.stringify({ message }),
                headers: {
                    'Content-Type': 'application/json',
                },
            });

            const data = await response.json();
            document.getElementById('chat-box').innerHTML += `<div>Bot: ${data.reply}</div>`;
            document.getElementById('message').value = '';
        }

I tried changing up the structure of the code, I uninstalled Flask and reinstalled it again. I've also extensively used chatgpt to try and come up with better code but it just kept taking me in circles. I'm hoping someone can help with this. I even tried a simple server that just said hello world which worked, but I truly think the issue might be with my javascript. Also, I am a beginner and this is supposed to be one of my first coding projects so please take it easy on me if possible. Thanks.
","You have to add a route for '/' to serve the html file. I also fixed the way you call the OpenAI API because you're using a deprecated one.
import openai
from flask import Flask, request, jsonify, render_template

chat_client = OpenAI(api_key='....')


@app.route('/')
def index():
    return render_template('index.html')

@app.route('/', methods=['POST'])
def chat():
    data = request.get_json()
    message = data.get('message')
    
    response = chat_client.chat.completions.create(
        model=""gpt-3.5-turbo"",
        messages=[
            {""role"": ""user"", ""content"": message}
        ],
        max_tokens=50
    )

    return jsonify({'reply': response.choices[0].message.content.strip()})

if __name__ == '__main__':
    app.run(port=5000, debug=True)

sample index.html that I tested with
<!DOCTYPE html>
<html lang=""en"">
<head>
    <meta charset=""UTF-8"">
    <meta name=""viewport"" content=""width=device-width, initial-scale=1.0"">
    <title>Chat app</title>
    <script>
        async function sendMessage() {
            const message = document.getElementById('message').value;
            document.getElementById('chat-box').innerHTML += `<div>You: ${message}</div>`;
            
            const response = await fetch('/', {
                method: ""POST"",
                body: JSON.stringify({ message }),
                headers: {
                    'Content-Type': 'application/json',
                },
            });

            const data = await response.json();
            document.getElementById('chat-box').innerHTML += `<div>Bot: ${data.reply}</div>`;
            document.getElementById('message').value = '';
        }
    </script>
</head>
<body>
    <div id=""chat-box""></div>
    <input type=""text"" id=""message"" placeholder=""Type your message here..."">
    <button onclick=""sendMessage()"">Send</button>
</body>
</html>


Directory structure
- app.py
- templates/
  - index.html


",chatgpt
how to get multimodal embeddings from clip model,"I'm hoping to use CLIP to get a single embedding for rows of multimodal (image and text) data.
Say I have the following model:
from PIL import Image
import torch
from transformers import CLIPProcessor, CLIPModel
import torchvision.transforms as transforms

model = CLIPModel.from_pretrained(""openai/clip-vit-base-patch32"")
processor = CLIPProcessor.from_pretrained(""openai/clip-vit-base-patch32"")

def convert_image_data_to_tensor(image_data):
    return torch.tensor(image_data)

dataset = df[['image_data', 'text_data']].to_dict('records')

embeddings = []
for data in dataset:
    image_tensor = convert_image_data_to_tensor(data['image_data'])
    text = data['text_data']

    inputs = processor(text=text, images=image_tensor, return_tensors=True)
    with torch.no_grad():
        output = model(**inputs)

I want to get the embeddings calculated in output. I know that output has the addtributes text_embeddings and image_embeddings, but I'm not sure how they interact later on. If I want to get a single embedding for each record, should I just be concatenating these attributes together? Is there another attribute that combines the two in some other way?
These are the attributes stored in output:
print(dir(output))

['__annotations__', '__class__', '__contains__', '__dataclass_fields__', '__dataclass_params__', '__delattr__', '__delitem__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getitem__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__iter__', '__le__', '__len__', '__lt__', '__module__', '__ne__', '__new__', '__post_init__', '__reduce__', '__reduce_ex__', '__repr__', '__reversed__', '__setattr__', '__setitem__', '__sizeof__', '__str__', '__subclasshook__', 'clear', 'copy', 'fromkeys', 'get', 'image_embeds', 'items', 'keys', 'logits_per_image', 'logits_per_text', 'loss', 'move_to_end', 'pop', 'popitem', 'setdefault', 'text_embeds', 'text_model_output', 'to_tuple', 'update', 'values', 'vision_model_output']

Also, is there a way to specify the size of the embedding that CLIP outputs? Similar to how you can specify the embedding size in BERT configs?
Thanks in advance for any help here. Feel free to correct me if I'm misunderstanding anything critical here.
","CLIP is trained such that the text and image embeddings are projected on to a shared latent space. In fact, image-text similarity is what the model is trained to optimise.
So a very typical use case of CLIP is to compare and match images and text based on similarity. In your case, you don't seem to be interested in any measure of similarity. You already have an image and the text and want some joint embedding representation. So concatenation of the two embeddings the way you described it is fine. An alternative would be take their mean (since they are in the same embedding space, it's fine to do this).
As for the size of the embedding, I don't think there is a way to change it as it's hardwired into the architecture of the model when it's trained. You can perhaps employ a dimensionality reduction technique, or fine tune the model after stacking another fully connected layer with the dimensionality of your choice.
",chatgpt
how to send large videos to gemini ai api 15 pro for inference,"I'm currently working with the Gemini AI API 1.5 Pro (latest version) and need to send large video files for inference. These videos are several hundred megabytes each (~700MB) but are within the API's constraints (e.g., less than 1 hour in length). I want to upload them once and perform inference without re-uploading.
In GPT-4o, there was an option to use image_urls to reference images. Is there a similar method or best practice for handling large video files with the Gemini AI API 1.5 Pro?
The videos are too large to send repeatedly, so an efficient method for uploading and referencing them is crucial.
Any guidance on API endpoints, required parameters, or example code snippets would be greatly appreciated.
","In your situation, how about the following sample script?
Sample script 1:
Before you test the following script, please update google-generativeai to the latest version.
import google.generativeai as genai
import time

apiKey = ""###"" # Please set your API key.
video_file_name = ""sample.mp4"" # Please set your video file with the path.
display_name = ""sampleDisplayName"" # Please set the display name of the uploaded file on Gemini. The file is searched from the file list using this value.

genai.configure(api_key=apiKey)

# Get file list in Gemini
fileList = genai.list_files(page_size=100)

# Check uploaded file.
video_file = next((f for f in fileList if f.display_name == display_name), None)
if video_file is None:
    print(f""Uploading file..."")
    video_file = genai.upload_file(path=video_file_name, display_name=display_name, resumable=True)
    print(f""Completed upload: {video_file.uri}"")
else:
    print(f""File URI: {video_file.uri}"")

# Check the state of the uploaded file.
while video_file.state.name == ""PROCESSING"":
    print(""."", end="""")
    time.sleep(10)
    video_file = genai.get_file(video_file.name)

if video_file.state.name == ""FAILED"":
    raise ValueError(video_file.state.name)

# Generate content using the uploaded file.
prompt = ""Describe this video.""
model = genai.GenerativeModel(model_name=""models/gemini-1.5-pro-latest"")
print(""Making LLM inference request..."")
response = model.generate_content([video_file, prompt], request_options={""timeout"": 600})
print(response.text)

In this sample script, when the file has already been uploaded, the existing file is used. On the other hand, when the file is not found, the file is uploaded and the uploaded file is used. In order to search the file, in this sample, display_name is used.
Sample script 2:
As another approach, when the value of name can be directly given, the following sample script can be also used. In this case, the value of name is required to be the unique value in the uploaded files.
import google.generativeai as genai
import time

apiKey = ""###"" # Please set your API key.
video_file_name = ""sample.mp4"" # Please set your video file with the path.
name = ""sample-name-1"" # Please set the name of the uploaded file on Gemini. The file is searched from the file list using this value.

genai.configure(api_key=apiKey)

# Check uploaded file.
try:
    video_file = genai.get_file(f""files/{name}"")
    print(f""File URI: {video_file.uri}"")
except:
    print(f""Uploading file..."")
    video_file = genai.upload_file(path=video_file_name, name=name, resumable=True)
    print(f""Completed upload: {video_file.uri}"")

# Check the state of the uploaded file.
while video_file.state.name == ""PROCESSING"":
    print(""."", end="""")
    time.sleep(10)
    video_file = genai.get_file(video_file.name)

if video_file.state.name == ""FAILED"":
    raise ValueError(video_file.state.name)

# Generate content using the uploaded file.
prompt = ""Describe this video.""
model = genai.GenerativeModel(model_name=""models/gemini-1.5-pro-latest"")
print(""Making LLM inference request..."")
response = model.generate_content([video_file, prompt], request_options={""timeout"": 600})
print(response.text)

This script is the same result with the above script.
Note:

This is a simple sample script. So, please modify this to your actual situation.

Reference:

Prompting with media files

",chatgpt
why do i get a quotresource not foundquot error when switching resource groups for azure openai,"When I  use my second Resource Group in West US (my first one is Germany), I always get the following error:

Error code: 404 - {'error': {'code': '404', 'message': 'Resource not found'}}

My Python code:
from openai import AzureOpenAI


client = AzureOpenAI(
  azure_endpoint = ""https://XXXX.openai.azure.com/"", 
  api_key=""XXXXX"",  
  api_version=""2024-05-13""
)


message_text = [{""role"":""system"",""content"":""You are an AI assistant that helps people find information.""},{""role"":""user"",""content"":""Was ist 4x6?""}]

completion = client.chat.completions.create(
  model=""GPT-4o"", # model = ""deployment_name""
  messages = message_text,
  temperature=0.7,
  max_tokens=800,
  top_p=0.95,
  frequency_penalty=0,
  presence_penalty=0,
  stop=None
)

print(completion.choices[0].message.content)

Model Deployment:

I already checked double network, API, and endpoint.
In my first resource group the code works, and in second, it doesn't.
I want to use US West because there are always the new models like gpt-4o.
","The error is because of the API version you are using. The version given, 2024-05-13, is a model version and not the API version.

Use the API version 2024-02-15-preview and it should work:

Code in

",chatgpt
empty result apache opennlp onnx model,"I trying to convert huggingface model to onnx for classifying text in Java app, but i can't undestand why i don't see result(result array is just empty). readme.md has link on the model, and it's working pretty well, but i have to use some another one because it is not support language which  i need.
Working sample python code:
from transformers import AutoTokenizer, AutoModelForSequenceClassification
import torch

model_name = ""tabularisai/multilingual-sentiment-analysis""
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForSequenceClassification.from_pretrained(model_name)

def predict_sentiment(texts):
    inputs = tokenizer(texts, return_tensors=""pt"", truncation=True, padding=True, max_length=512)
    with torch.no_grad():
        outputs = model(**inputs)
    probabilities = torch.nn.functional.softmax(outputs.logits, dim=-1)
    sentiment_map = {0: ""Very Negative"", 1: ""Negative"", 2: ""Neutral"", 3: ""Positive"", 4: ""Very Positive""}
    return [sentiment_map[p] for p in torch.argmax(probabilities, dim=-1).tolist()]

print(predict_sentiment([""I absolutely love the new design of this app!"", ""The customer service was disappointing.""]))

I tried to convert a few models with two ways:
python -m optimum.exporters.onnx --model tabularisai/multilingual-sentiment-analysis --task sequence-classification onnx_model
and
from optimum.onnxruntime import ORTModelForFeatureExtraction

model = ORTModelForFeatureExtraction.from_pretrained(""tabularisai/multilingual-sentiment-analysis"", from_transformers=True)
model.save_pretrained(""onnx_model"")

Same result - onnx_model folder with model, vocab etc
nlptown_bert-base-multilingual-uncased-sentiment - model from readme and it works as expect even i use converted model vocab file
Java code sample:
    public void def() {
        try (final DocumentCategorizerDL documentCategorizerDL =
                     new DocumentCategorizerDL(
                             new File(""onnx_model/model.onnx""),
//                             new File(""nlptown_bert-base-multilingual-uncased-sentiment.onnx""),
                             new File(""onnx_model/vocab.txt""),
                             getCategories(),
                             new AverageClassificationScoringStrategy(),
                             new InferenceOptions())) {

            final double[] result = documentCategorizerDL.categorize(new String[] {""I absolutely love the new design of this app!"", ""The customer service was disappointing.""});
            System.out.println(""done"");
        } catch (Exception e) {
            e.printStackTrace();
        }
    }

    private Map<Integer, String> getCategories() {
        final Map<Integer, String> categories = new HashMap<>();
        categories.put(0, ""Very Negative"");
        categories.put(1, ""Negative"");
        categories.put(2, ""Neutral"");
        categories.put(3, ""Positive"");
        categories.put(4, ""Very Positive"");
        return categories;
    }

I don't know how is important but opset version of working model is 11, but my is 14.
My opennlp-dl library version is 2.5.1
","setIncludeTokenTypeIds(true) was passed to options and this helped to solve this problem
    public void def() {
    var options = new InferenceOptions();
    options.setIncludeTokenTypeIds(false);
    try (
         final DocumentCategorizerDL documentCategorizerDL =
                 new DocumentCategorizerDL(
                         new File(""onnx_model/model.onnx""),
                         new File(""onnx_model/vocab.txt""),
                         getCategories(),
                         new AverageClassificationScoringStrategy(),
                         options)) {

        final double[] result = documentCategorizerDL.categorize(new String[] {""I absolutely love the new design of this app!"", ""The customer service was disappointing.""});
        System.out.println(""s"");
    } catch (Exception e) {
        log.error(e.getMessage(), e);
    }
}

private Map<Integer, String> getCategories() {
    final Map<Integer, String> categories = new HashMap<>();
    categories.put(0, ""Very Negative"");
    categories.put(1, ""Negative"");
    categories.put(2, ""Neutral"");
    categories.put(3, ""Positive"");
    categories.put(4, ""Very Positive"");
    return categories;
}

",huggingface-transformers
bert sentence embeddings from transformers,"I'm trying to get sentence vectors from hidden states in a BERT model.  Looking at the huggingface BertModel instructions here, which say:
from transformers import BertTokenizer, BertModel
tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')
model = BertModel.from_pretrained(""bert-base-multilingual-cased"")
text = ""Replace me by any text you'd like.""
encoded_input = tokenizer(text, return_tensors='pt') 
output = model(**encoded_input)

So first note, as it is on the website, this does /not/ run. You get:
>>> Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
TypeError: 'BertTokenizer' object is not callable

But it looks like a minor change fixes it, in that you don't call the tokenizer directly, but ask it to encode the input:
encoded_input = tokenizer.encode(text, return_tensors=""pt"")
output = model(encoded_input)

OK, that aside, the tensors I get, however, have a different shape than I expected:
>>> output[0].shape
torch.Size([1,11,768])

This is a lot of layers.  Which is the correct layer to use for sentence embeddings?  [0]?  [-1]?  Averaging several?  I have the goal of being able to do cosine similarity with these, so I need a proper 1xN vector rather than an NxK tensor.
I see that the popular bert-as-a-service project appears to use [0]
Is this correct? Is there documentation for what each of the layers are?
","I don't think there is single authoritative documentation saying what to use and when. You need to experiment and measure what is best for your task. Recent observations about BERT are nicely summarized in this paper: https://arxiv.org/pdf/2002.12327.pdf.
I think the rule of thumb is:

Use the last layer if you are going to fine-tune the model for your specific task. And finetune whenever you can, several hundred or even dozens of training examples are enough.

Use some of the middle layers (7-th or 8-th) if you cannot finetune the model. The intuition behind that is that the layers first develop a more and more abstract and general representation of the input. At some point, the representation starts to be more target to the pre-training task.


Bert-as-services uses the last layer by default (but it is configurable). Here, it would be [:, -1]. However, it always returns a list of vectors for all input tokens. The vector corresponding to the first special (so-called [CLS]) token is considered to be the sentence embedding. This where the [0] comes from in the snipper you refer to.
",huggingface-transformers
creating a 39for39 or 39ifelse39 selector for checking dynamically sized python list,"I'm trying to mask 8752 images with transformers like this
from transformers import pipeline
from PIL import Image
import requests
import cv2

import numpy as np
from matplotlib import pyplot as plt


semantic_segmentation_nvidia = pipeline(""image-segmentation"", ""nvidia/segformer-b0-finetuned-ade-512-512"")
jpeg_im = None
a = 0
mask_i = 0

f = open(""masking_log.txt"", ""w"")


for im in large_image_stack_512:
    i=0 
    while(i == 0):
        
        jpeg_im = Image.open(os.path.join(ROOT_DIR,im))
        print(os.path.join(ROOT_DIR,im))
        # Semantic segmentation
        segmentation = semantic_segmentation_nvidia(jpeg_im)
        print(""the length of current segmentation labels are: "", len(segmentation))
        water_mask_label = segmentation[mask_i][""label""]
        print(water_mask_label)
        print(""here"")    
        if (water_mask_label == ""water""):
            print(""Successful labelling at: "", mask_i)
            water_mask = segmentation[mask_i][""mask""]
            print(""here"")
            imar = np.asarray(water_mask)
            print(water_mask_label)
            print(""type im (array)"", type(imar))
            f.write(""image "" + str(a) + ""\nsuccess-label at "" + str(mask_i) + ""\nwith dir: "" + str(im)  + ""\n with mask labeled as: "" + str(water_mask_label) + '\n\n')
            plt.imsave('D:\..\Data\'+'img_'+str(a)+'.jpg', imar, cmap=""gray"")  
            i=1
            a+=1
            mask_i= 0
            semantic_jpeg = None
            imar = None
            water_mask = None
            water_mask_label = None
            segmentation = None
            water_mask_label = None
        else:       
            print(""not water"")
            if (mask_i < len(segmentation)):
                mask_i += 1
            else:
                f.write(""image "" + str(a) + ""\n unsuccess-labelling (has no 'water' label)"" + ""final mask_i value: "" + str(mask_i) + ""\nwith dir: "" + str(im)  + ""\n check later "" +  + '\n\n')
                print(""masking fails, check later image"" + im)
                i = 1
                continue
            
            
        #plt.imshow(water_mask)
        #plt.show()
        #print(""type jpeg_im (jpeg)"", type(water_mask))
    continue   
       
#print(len(cropped))
f.close()

Each of segmentation = semantic_segmentation_nvidia(jpeg_im) will have different size of array, for example with this image I have 11 items inside it like this:
The code (did this in jupyter notebook rows)
a_512 = semantic_segmentation_nvidia(image_512)
a_512

the output, a_512 variable is a List with 11 items inside it
[{'score': None,
  'label': 'wall',
  'mask': <PIL.Image.Image image mode=L size=512x512>},
 {'score': None,
  'label': 'building',
  'mask': <PIL.Image.Image image mode=L size=512x512>},
 {'score': None,
  'label': 'sky',
  'mask': <PIL.Image.Image image mode=L size=512x512>},
 {'score': None,
  'label': 'tree',
  'mask': <PIL.Image.Image image mode=L size=512x512>},
 {'score': None,
  'label': 'earth',
  'mask': <PIL.Image.Image image mode=L size=512x512>},
 {'score': None,
  'label': 'water',
  'mask': <PIL.Image.Image image mode=L size=512x512>},
 {'score': None,
  'label': 'fence',
  'mask': <PIL.Image.Image image mode=L size=512x512>},
 {'score': None,
  'label': 'railing',
  'mask': <PIL.Image.Image image mode=L size=512x512>},
 {'score': None,
  'label': 'bridge',
  'mask': <PIL.Image.Image image mode=L size=512x512>},
 {'score': None,
  'label': 'ship',
  'mask': <PIL.Image.Image image mode=L size=512x512>},
 {'score': None,
  'label': 'pier',
  'mask': <PIL.Image.Image image mode=L size=512x512>}]

To access things I need, probably the 'label', I need to access the PIL.Image.Imagedata from each List item like this
(a_512[5][""mask""]

In the codes above, I used a variable to represent 5 in this code because apparently each image has different order of when it detects 'water' as the label and thus the required mask.
Because python can compare strings, I'm trying to make a for loop to check every items in the list of every 8752 images. Let's say some of the images can have 10-12 items like this output of the final run.
D:\..\Data\cropped_512\img_2541.jpg
the length of current segmentation labels are:  15
car
here
not water
D:\..\Data\cropped_512\img_2541.jpg
the length of current segmentation labels are:  15
water
here
Successful labelling at:  7
here
water
type im (array) <class 'numpy.ndarray'>
D:\..\Data\cropped_512\img_2542.jpg
the length of current segmentation labels are:  12
wall
here
not water
D:\..\Data\cropped_512\img_2542.jpg
the length of current segmentation labels are:  12
building
here
not water
D:\..\Data\cropped_512\img_2542.jpg
the length of current segmentation labels are:  12
sky
here
not water
D:\..\Data\cropped_512\img_2542.jpg
the length of current segmentation labels are:  12
tree
here
not water
D:\..\Data\cropped_512\img_2542.jpg
the length of current segmentation labels are:  12
road
here
not water
D:\..\Data\cropped_512\img_2542.jpg
the length of current segmentation labels are:  12
mountain
here
not water
D:\..\Data\cropped_512\img_2542.jpg
the length of current segmentation labels are:  12
car
here
not water
D:\..\Data\cropped_512\img_2542.jpg
the length of current segmentation labels are:  12
sea
here
not water
D:\..\Data\cropped_512\img_2542.jpg
the length of current segmentation labels are:  12
fence
here
not water
D:\..\Data\cropped_512\img_2542.jpg
the length of current segmentation labels are:  12
bridge
here
not water
D:\..\Data\cropped_512\img_2542.jpg
the length of current segmentation labels are:  12
boat
here
not water
D:\..\Data\cropped_512\img_2542.jpg
the length of current segmentation labels are:  12
ship
here
not water
D:\..\Data\cropped_512\img_2542.jpg
the length of current segmentation labels are:  12

---------------------------------------------------------------------------
IndexError                                Traceback (most recent call last)
Cell In[16], line 17
     15 segmentation = semantic_segmentation_nvidia(jpeg_im)
     16 print(""the length of current segmentation labels are: "", len(segmentation))
---> 17 water_mask_label = segmentation[mask_i][""label""]
     18 print(water_mask_label)
     19 print(""here"")    

IndexError: list index out of range

As you can see the index keeps increasing and didn't touch the last else of the if-else comparator, so it went out of bounds. I have tried this and using range on the for-loop but it didn't work and still went out of bounds. I put breaks and it stops the loop, I put continue it keeps went out of bounds.Which part did I do wrong and understood incorrectly about python comparator behavior?
I also tried logging it into .txt file or give status with print but it didn't go that way and always go out of bounds. Will gladly add more details if needed. The image is 512 x 512 size.
","The problem is solved according to @OldBoy input to move the increment.
full code
mask_i = 0
a = 0

f = open(""masking_log.txt"", ""w"")

for im in large_image_stack_512:
    image_success_flag = 0
    mask_i= 0
    
    while(image_success_flag < 1):
        jpeg_im = Image.open(os.path.join(ROOT_DIR,im))
        print(os.path.join(ROOT_DIR,im))
        # Semantic segmentation
        segmentation = semantic_segmentation_nvidia(jpeg_im)
        print(""the length of current segmentation labels are: "", len(segmentation))
        while(mask_i < len(segmentation)):
           image_mask = segmentation[mask_i][""label""]
           print(image_mask)
           if(image_mask == ""water""):
               print(""correct mask"")
               water_mask = segmentation[mask_i][""mask""]
               imar = np.asarray(water_mask)
               plt.imsave('D:/semester_12/Data_Irredeano/'+'img_'+str(a)+'.jpg', imar, cmap=""gray"") 
               print(""here"")
               f.write(""image "" + str(a) + ""\nsuccess-label at "" + str(mask_i) + ""\nwith dir: "" + str(im)  + ""\n with mask labeled as: "" + str(water_mask_label) + '\n\n')
               print(""mask-succesfully saved"")
               mask_i = 0 
               break
           elif(image_mask != ""water""):
               mask_i+=1
               print(""number of mask: "", mask_i)
               if(mask_i == len(segmentation)):
                    print(""this image has no correct mask, check later"")
                    f.write(""image "" + str(a) + ""\n unsuccess-labelling (has no 'water' label) final \n mask_i value: "" + str(mask_i) + ""\nwith dir: "" + str(im)  + ""\n check later "" + '\n\n')
               image_success_flag=+1  
        a+=1


f.close()

Basically instead of selecting mask by checking the segmentation[mask_i][""label] I check if the 'cursor' or mask_i if it's smaller than the  length of the List (len(segmentation)). The += also contribute to the problem since it's adding first then change the number, as implied here and here, because of that my 'cursor variable' can move beyond the array size before checking the segmentation[mask_i][""label]. But I don't think I have other choice other than that to increment since =+ is just redefining itself.
Other than that I also add another while condition to make sure the code runs while the mask_i is below the size of the List, so the program become ""if it's still below the list size, check whether mask is ""water"" or not ""water"".
Although the program is finished and can mask most of the images, I still have to log several different images since not all of them had ""water"" as the label, but something close like ""Sea"" that we humans can intuitively say they are basically the same but computer that can only compare strings not.
Thank you again for everyone who's willing to help, I'm accepting if there's better way to do it
",huggingface-transformers
bert model splits words by its own,"I am tokenizing the input words using bert model.
The code is :
tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased',do_lower_case = False)
model = BertModel.from_pretrained(""bert-base-multilingual-cased"", add_pooling_layer=False, output_hidden_states=True, output_attentions=True)

marked_text =  text + "" [SEP]""
    tokenized_text = tokenizer.tokenize(marked_text)
    indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)
    print(tokenized_text)
    print(indexed_tokens)

The model I used is from HuggingFace.
My goal is to print the embedded vectors of all words Bert model has, so I searched and found that this model has 119296 tokens available.
I don't know this number of the tokens is reason, but the model splits the words by its own, which is unwanted for me.
for example,

only -> [only]
ONLY -> [ON,L,Y]

stradivarius -> ['St', '##radi', '##vari', '##us']

Is this natural Bert thing or I am doing something wrong ?
","You are not doing anything wrong. Bert uses a so-called wordpiece subword tokenizer as a compromise for meaningful embeddings and acceptable memory consumption between a character-level (small vocabulary) and a word-level tokenizer (large vocabulary).
A common approach to retrieve word embeddings from a subword-based model is to take the mean of the respective tokens. The code below shows you have you can retrieve the word embeddings (non-contextualized and contextualized) by taking the mean. It uses a fasttokenizer to utilize the methods of the BatchEncoding object.
import torch
from transformers import BertTokenizerFast, BertModel

t = BertTokenizerFast.from_pretrained('bert-base-multilingual-cased')
# whole model
m = BertModel.from_pretrained(""bert-base-multilingual-cased"")
# token embedding layer
embedding_layer = m.embeddings.word_embeddings

sample_sentence = 'This is an example with token-embeddings and word-embeddings'
encoded = t([sample_sentence])
# The BatchEncoding object allows us to map the token back to the string indices
print(*[(token_id, encoded.token_to_chars(idx)) for idx, token_id in enumerate(encoded.input_ids[0])], sep=""\n"")
# And we can also check the mapping of word to token indices
print(*[(word, encoded.word_to_tokens(idx)) for idx, word in enumerate(sample_sentence.split())], sep=""\n"")

Output:
(101, None)
(10747, CharSpan(start=0, end=4))
(10124, CharSpan(start=5, end=7))
(10151, CharSpan(start=8, end=10))
(14351, CharSpan(start=11, end=18))
(10169, CharSpan(start=19, end=23))
(18436, CharSpan(start=24, end=27))
(10136, CharSpan(start=27, end=29))
(118, CharSpan(start=29, end=30))
(10266, CharSpan(start=30, end=32))
(33627, CharSpan(start=32, end=35))
(13971, CharSpan(start=35, end=39))
(10107, CharSpan(start=39, end=40))
(10111, CharSpan(start=41, end=44))
(12307, CharSpan(start=45, end=49))
(118, CharSpan(start=49, end=50))
(10266, CharSpan(start=50, end=52))
(33627, CharSpan(start=52, end=55))
(13971, CharSpan(start=55, end=59))
(10107, CharSpan(start=59, end=60))
(102, None)
('This', TokenSpan(start=1, end=2))
('is', TokenSpan(start=2, end=3))
('an', TokenSpan(start=3, end=4))
('example', TokenSpan(start=4, end=5))
('with', TokenSpan(start=5, end=6))
('token-embeddings', TokenSpan(start=6, end=8))
('and', TokenSpan(start=8, end=9))
('word-embeddings', TokenSpan(start=9, end=13))

To retrieve the word embeddings:
with torch.inference_mode():
  token_embeddings = embedding_layer(encoded.convert_to_tensors(""pt"").input_ids).squeeze()
  # we need the attention mechanism of the whole model to get the contextualized token representations
  contextualized_token_embeddings = m(**encoded.convert_to_tensors(""pt"")).last_hidden_state.squeeze()

def fetch_word_embeddings(sample_sentence:str, encoded, embeddings:torch.Tensor) -> dict[str,torch.Tensor]:
  word_embeddings = {}
  for idx, word in enumerate(sample_sentence.split()):
    start, end = encoded.word_to_tokens(idx)
    word_embeddings[word] = embeddings[start:end].mean(dim=0)
  return word_embeddings

word_embeddings = fetch_word_embeddings(sample_sentence, encoded, token_embeddings)
contextualized_word_embeddings = fetch_word_embeddings(sample_sentence, encoded, contextualized_token_embeddings)
print(word_embeddings[""token-embeddings""])
print(contextualized_word_embeddings[""token-embeddings""])

Output:
tensor([ 1.2455e-02, -3.8478e-02,  8.0834e-03, ..., -1.8502e-02,  1.1511e-02, -6.5307e-02])
tensor([-5.1564e-01, -1.6266e-01, -3.9420e-01, ..., -5.9969e-02,  3.0784e-01, -3.4451e-01])

",huggingface-transformers
suppress huggingface logging warning quotsetting pad_token_id to eos_token_ideos_token_id for openend generationquot,"In HuggingFace, every time I call a pipeline() object, I get a warning:
`""Setting `pad_token_id` to `eos_token_id`:{eos_token_id} for open-end generation.""

How do I suppress this warning without suppressing all logging warnings? I want other warnings, but I don't want this one.
","The warning comes for any text generation task done by HuggingFace. This is explained here, and you can see the code here.
Avoid that warning by manually setting the pad_token_id (e.g., to match the tokenizer or the eos_token_id).
Set the pad_token_id in the generation_config with:
model.generation_config.pad_token_id = tokenizer.pad_token_id

Alternatively, if you only need to make a single call to generate:

When you call
model.generate(**encoded_input)

just change it to
model.generate(**encoded_input, pad_token_id=tokenizer.eos_token_id)


",huggingface-transformers
huggingface transformers for text generation with ctrl with google colab39s free gpu,"I wanted to test TextGeneration with CTRL using PyTorch-Transformers, before using it for fine-tuning. But it doesn't prompt anything like it does with GPT-2 and other similar language generation models. I'm very new for this and am stuck and can't figure out what's going on.
This is the procedure I followed in my Colab notebook,
!pip install transformers

!git clone https://github.com/huggingface/pytorch-transformers.git

!python pytorch-transformers/examples/run_generation.py \
    --model_type=ctrl \
    --length=100 \
    --model_name_or_path=ctrl \
    --temperature=0.2 \
    --repetition_penalty=1.2 \

And this is what I get after running the script
02/10/2020 01:02:31 - INFO - transformers.tokenization_utils -   loading file https://raw.githubusercontent.com/salesforce/ctrl/master/ctrl-vocab.json from cache at /root/.cache/torch/transformers/a858ad854d3847b02da3aac63555142de6a05f2a26d928bb49e881970514e186.285c96a541cf6719677cfb634929022b56b76a0c9a540186ba3d8bbdf02bca42
02/10/2020 01:02:31 - INFO - transformers.tokenization_utils -   loading file https://raw.githubusercontent.com/salesforce/ctrl/master/ctrl-merges.txt from cache at /root/.cache/torch/transformers/aa2c569e6648690484ade28535a8157aa415f15202e84a62e82cc36ea0c20fa9.26153bf569b71aaf15ae54be4c1b9254dbeff58ca6fc3e29468c4eed078ac142
02/10/2020 01:02:31 - INFO - transformers.configuration_utils -   loading configuration file https://storage.googleapis.com/sf-ctrl/pytorch/ctrl-config.json from cache at /root/.cache/torch/transformers/d6492ca334c2a4e079f43df30956acf935134081b2b3844dc97457be69b623d0.1ebc47eb44e70492e0c20494a084f108332d20fea7fe5ad408ef5e7a8f2baef4
02/10/2020 01:02:31 - INFO - transformers.configuration_utils -   Model config CTRLConfig {
  ""architectures"": null,
  ""attn_pdrop"": 0.1,
  ""bos_token_id"": 0,
  ""dff"": 8192,
  ""do_sample"": false,
  ""embd_pdrop"": 0.1,
  ""eos_token_ids"": 0,
  ""finetuning_task"": null,
  ""from_tf"": false,
  ""id2label"": {
    ""0"": ""LABEL_0""
  },
  ""initializer_range"": 0.02,
  ""is_decoder"": false,
  ""label2id"": {
    ""LABEL_0"": 0
  },
  ""layer_norm_epsilon"": 1e-06,
  ""length_penalty"": 1.0,
  ""max_length"": 20,
  ""model_type"": ""ctrl"",
  ""n_ctx"": 512,
  ""n_embd"": 1280,
  ""n_head"": 16,
  ""n_layer"": 48,
  ""n_positions"": 50000,
  ""num_beams"": 1,
  ""num_labels"": 1,
  ""num_return_sequences"": 1,
  ""output_attentions"": false,
  ""output_hidden_states"": false,
  ""output_past"": true,
  ""pad_token_id"": 0,
  ""pruned_heads"": {},
  ""repetition_penalty"": 1.0,
  ""resid_pdrop"": 0.1,
  ""summary_activation"": null,
  ""summary_first_dropout"": 0.1,
  ""summary_proj_to_labels"": true,
  ""summary_type"": ""cls_index"",
  ""summary_use_proj"": true,
  ""temperature"": 1.0,
  ""top_k"": 50,
  ""top_p"": 1.0,
  ""torchscript"": false,
  ""use_bfloat16"": false,
  ""vocab_size"": 246534
}

02/10/2020 01:02:31 - INFO - transformers.modeling_utils -   loading weights file https://storage.googleapis.com/sf-ctrl/pytorch/seqlen256_v1.bin from cache at /root/.cache/torch/transformers/c146cc96724f27295a0c3ada1fbb3632074adf87e9aef8269e44c9208787f8c8.b986347cbab65fa276683efbb9c2f7ee22552277bcf6e1f1166557ed0852fdf0
tcmalloc: large alloc 1262256128 bytes == 0x38b92000 @  0x7fe1900bdb6b 0x7fe1900dd379 0x7fe139843b4a 0x7fe1398455fa 0x7fe13bb7578a 0x7fe13bdbe30b 0x7fe13be05b37 0x7fe184c8cad5 0x7fe184c8d17b 0x7fe184c91160 0x7fe184ade496 0x551b15 0x5aa6ec 0x50abb3 0x50c5b9 0x508245 0x5096b7 0x595311 0x54a6ff 0x551b81 0x5aa6ec 0x50abb3 0x50c5b9 0x508245 0x509642 0x595311 0x54a6ff 0x551b81 0x5aa6ec 0x50abb3 0x50c5b9
tcmalloc: large alloc 1262256128 bytes == 0x19fdda000 @  0x7fe1900bdb6b 0x7fe1900dd379 0x7fe139843b4a 0x7fe1398455fa 0x7fe13bb7578a 0x7fe13bdbe30b 0x7fe13be05b37 0x7fe184c8cad5 0x7fe184c8d17b 0x7fe184c91160 0x7fe184ade496 0x551b15 0x5aa6ec 0x50abb3 0x50c5b9 0x508245 0x509642 0x595311 0x54a6ff 0x551b81 0x5aa6ec 0x50abb3 0x50d390 0x508245 0x509642 0x595311 0x54a6ff 0x551b81 0x5a067e 0x50d966 0x508245
^C

and then terminates. Could this be because of a GPU problem?
","The solution was to increase the RAM. Since I was using the Google Colab's free GPU, I was going through this: GitHub issue
and found this useful: Solution
The following piece of code will crash the session in Colab and select 'Get more RAM', which will increase the RAM up to 25.51GB
d=[]
while(1):
  d.append('1')

",huggingface-transformers
fine tuning a pretrained language model with simple transformers,"In his article 'Language Model Fine-Tuning For Pre-Trained Transformers' Thilina Rajapakse (https://medium.com/skilai/language-model-fine-tuning-for-pre-trained-transformers-b7262774a7ee)
provides the following code snippet for fine-tuning a pre-trained model using the library simpletransformers:
from simpletransformers.language_modeling import LanguageModelingModel
import logging


logging.basicConfig(level=logging.INFO)
transformers_logger = logging.getLogger(""transformers"")
transformers_logger.setLevel(logging.WARNING)

train_args = {
    ""reprocess_input_data"": True,
    ""overwrite_output_dir"": True,
}

model = LanguageModelingModel('bert', 'bert-base-cased', args=train_args)

model.train_model(""data/train.txt"", eval_file=""data/text.txt"")

model.eval_model(""data/test.txt"")

He then adds:

We assume that you have combined all the text in your dataset into two
  text files train.txt and test.txt which can be found in the data/
  directory.

I have 2 questions:
Question 1
Does the highlighted sentence above implies that the entire corpus will be merged into one text file?  So assuming that the Training Corpus is comprised of 1,000,000 text files, are we supposed to merge them all in one text file with code like this?
import fileinput
with open(outfilename, 'w') as fout, fileinput.input(filenames) as fin:
    for line in fin:
        fout.write(line)

Question 2
I presume that I can use the pretrained model: bert-base-multilingual-cased.  Correct?
","Question 1
Yes, the input to the train_model() and eval_model() methods need to be a single file.
Dynamically loading from multiple files will likely be supported in the future
Question 2
Yes, you can use bert-base-multilingual-cased model.
You will find a much more detailed, updated guide on language model training here.
Disclaimer: I am the creator of the above library.
",huggingface-transformers
transformerjs model fails to parse json in clientside nextjs example,"On Brave browser Version 1.61.101 Chromium: 120.0.6099.71 (Official Build) (x86_64) (on Chrome Version 120.0.6099.62 (Official Build) (x86_64) I don't see this or any other errors) I run into a JSON parsing error when trying to run in development mode the client-side example from Hugging Face's Next.JS tutorial: https://huggingface.co/docs/transformers.js/tutorials/next#client-side-inference. Because of this error the text classification cannot run.
The error is as follows:

This is my TransformerWorker.js which is taken directly from the tutorial linked above, no changes:
import { pipeline, env } from ""@xenova/transformers"";

// Skip local model check and cache
env.allowLocalModels = false;

// Use the Singleton pattern to enable lazy construction of the pipeline.
class PipelineSingleton {
    static task = 'text-classification';
    static model = 'Xenova/distilbert-base-uncased-finetuned-sst-2-english';
    static instance = null;

    static async getInstance(progress_callback = null) {
        if (this.instance === null) {
            this.instance = pipeline(this.task, this.model, { progress_callback });
        }
        return this.instance;
    }
}

// Listen for messages from the main thread
self.addEventListener('message', async (event) => {
    // Retrieve the classification pipeline. When called for the first time,
    // this will load the pipeline and save it for future use.
    let classifier = await PipelineSingleton.getInstance(x => {
        // We also add a progress callback to the pipeline so that we can
        // track model loading.
        self.postMessage(x);
    });

    // Actually perform the classification
    let output = await classifier(event.data.text);

    // Send the output back to the main thread
    self.postMessage({
        status: 'complete',
        output: output,
    });
});

","Looking through the Github issues for transformers.js I found https://github.com/xenova/transformers.js/issues/366 in which one user suggested adding the following line to the worker file:
env.useBrowserCache = false;
Turning the browser cache option off I no longer receive this error in the Brave browser and, in fact, the Next.js client-side example code Hugging Face provides in their tutorial works without any further issues for me.
That said, browser caching is a significant feature, so this could indicate an underlying bug that needs to be fixed in transformers.js. See the link to the transformers.js Github issue above.
",huggingface-transformers
can not load the safetensors huggingface model in djl in java,"I tried a lot, but I want to read embeddings from the jina embeddings
this is my java code:
public static float[] getTextEmbedding(String text) throws ModelNotFoundException, MalformedModelException, IOException, TranslateException {
        Criteria<String, float[]> criteria = Criteria.builder()
                .setTypes(String.class, float[].class)
                .optEngine(""PyTorch"")
                .optModelPath(Paths.get(""/jina-embeddings-v2-base-de""))
                .optTranslator(
                    TextEmbeddingTranslator
                        .builder(HuggingFaceTokenizer.newInstance(Paths.get(""/jina-embeddings-v2-base-de"")))
                        .build()
                )
                .optProgress(new ProgressBar())
                .optOption(""trust_remote_code"", ""true"")
                .build();

        try (Predictor<String, float[]> predictor = ModelZoo.loadModel(criteria).newPredictor()) {
            return predictor.predict(text);
        }
    }

but I keept getting this error:
java.io.FileNotFoundException: jina-embeddings-v2-base-de.pt file not found in: /jina-embeddings-v2-base-de

","djl-convert can convert local model into DJL format as well:
djl-convert -m /jina-embeddings-v2-base-de

Then you can point to model/jina-embeddings-v2-base-de folder in Java code.
You will find the following files in the folder:

jina-embeddings-v2-base-de.pt
config.json
serving.properties
tokenizer_config.json
tokenizer.json

",huggingface-transformers
stop modelgenerate,"I'm using TextIteratorStreamer to generate text as stream
and I use Thread to run model.generate
thread = Thread(target=model.generate, kwargs=generation_kwargs)
thread.start()

I want to introduce a cancel_event = asyncio.Event() and check if cancel_event.is_set() in the streamer loop to stop model.generate consuming GPU resources, How can I stop model.generate? Do I need to kill the  thread? how?
","You can create a class that should handle the cancelling without the need to kill the thread. Something like this might work:
import asyncio
from threading import Thread
from transformers import StoppingCriteria

class StopCriteria(StoppingCriteria):
    def __init__(self, event):
        self.event = event

    def __call__(self, *args, **kwargs):
        return self.event.is_set()

cancel_event = asyncio.Event()
generation_kwargs['stopping_criteria'] = [StopCriteria(cancel_event)]

thread = Thread(target=model.generate, kwargs=generation_kwargs)
thread.start()

# your existing code

cancel_event.set()

If you want the thread to finish before cancelling it you could use:
thread.join()

",huggingface-transformers
error in getting captum text explanations for text classification,"I have the following code that I am using to identify the most influential words used to correctly predict the text in the test dataset
import pandas as pd
import torch
from torch.utils.data import DataLoader
from transformers import BertTokenizer, BertForSequenceClassification, AdamW
from sklearn.metrics import accuracy_score
from captum.attr import IntegratedGradients

# Loading data
train_df = pd.read_csv('train_dataset.csv')
test_df = pd.read_csv('test_dataset.csv')

# Tokenizer
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')

def preprocess_data(df, tokenizer, max_len=128):
    inputs = tokenizer(list(df['text']), padding=True, truncation=True, max_length=max_len, return_tensors=""pt"")
    labels = torch.tensor(df['label'].values)
    return inputs, labels

train_inputs, train_labels = preprocess_data(train_df, tokenizer)
test_inputs, test_labels = preprocess_data(test_df, tokenizer)

# DataLoader
train_dataset = torch.utils.data.TensorDataset(train_inputs['input_ids'], train_inputs['attention_mask'], train_labels)
train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)

test_dataset = torch.utils.data.TensorDataset(test_inputs['input_ids'], test_inputs['attention_mask'], test_labels)
test_loader = DataLoader(test_dataset, batch_size=16)

# Model setup
device = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")
model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2).to(device)

# Optimizer
optimizer = AdamW(model.parameters(), lr=5e-5)

# Training Loop
model.train()
for epoch in range(3):  # Train for 3 epochs
    for batch in train_loader:
        input_ids, attention_mask, labels = [x.to(device) for x in batch]
        optimizer.zero_grad()
        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)
        loss = outputs.loss
        loss.backward()
        optimizer.step()
    print(f""Epoch {epoch+1} loss: {loss.item()}"")

# Evaluation
model.eval()
correct_predictions = []
with torch.no_grad():
    for batch in test_loader:
        input_ids, attention_mask, labels = [x.to(device) for x in batch]
        outputs = model(input_ids, attention_mask=attention_mask)
        preds = torch.argmax(outputs.logits, dim=1)
        correct_predictions.extend(
            (preds == labels).cpu().numpy().tolist()
        )
accuracy = accuracy_score(test_labels.numpy(), correct_predictions)
print(f""Test Accuracy: {accuracy:.2f}"")

# Integrated Gradients
ig = IntegratedGradients(model)

def get_influential_words(input_text, model, tokenizer, ig, device):
    model.eval()
    # Tokenizing the input text
    inputs = tokenizer(input_text, return_tensors=""pt"", truncation=True, padding=True, max_length=128)
    input_ids = inputs['input_ids'].to(device, dtype=torch.long)  # Explicitly convert to LongTensor
    attention_mask = inputs['attention_mask'].to(device, dtype=torch.long)  # Explicitly convert to LongTensor

    print(""Input IDs shape:"", input_ids.shape, ""dtype:"", input_ids.dtype)
    print(""Attention mask shape:"", attention_mask.shape, ""dtype:"", attention_mask.dtype)
    # forward function for IG
    def forward_func(input_ids):
        outputs = model(input_ids, attention_mask=attention_mask)
        return outputs.logits

    # Applying Integrated Gradients
    attributions, delta = ig.attribute(input_ids, target=1, return_convergence_delta=True)
    tokens = tokenizer.convert_ids_to_tokens(input_ids[0].tolist())
    token_importances = attributions.sum(dim=2).squeeze(0).detach().cpu().numpy()

    return list(zip(tokens, token_importances))

# Analysing influential words for correctly predicted texts
for idx, correct in enumerate(correct_predictions):
    if correct:
        influential_words = get_influential_words(test_df['text'].iloc[idx], model, tokenizer, ig, device)
        print(f""Influential words for text: {test_df['text'].iloc[idx]}"")
        print(influential_words)

But I am getting the following error in running the above.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
Epoch 1 loss: 0.4719192385673523
Epoch 2 loss: 0.39585667848587036
Epoch 3 loss: 0.14659778773784637
Test Accuracy: 0.70
Input IDs shape: torch.Size([1, 8]) dtype: torch.int64
Attention mask shape: torch.Size([1, 8]) dtype: torch.int64
---------------------------------------------------------------------------
RuntimeError                              Traceback (most recent call last)
<ipython-input-9-f047b509c98d> in <cell line: 90>()
     90 for idx, correct in enumerate(correct_predictions):
     91     if correct:
---> 92         influential_words = get_influential_words(test_df['text'].iloc[idx], model, tokenizer, ig, device)
     93         print(f""Influential words for text: {test_df['text'].iloc[idx]}"")
     94         print(influential_words)

18 frames
/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py in embedding(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)
   2549         # remove once script supports set_grad_enabled
   2550         _no_grad_embedding_renorm_(weight, input, max_norm, norm_type)
-> 2551     return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)
   2552 
   2553 

RuntimeError: Expected tensor for argument #1 'indices' to have one of the following scalar types: Long, Int; but got torch.cuda.FloatTensor instead (while checking arguments for embedding)

","You need to slightly change the gradients calculation class. Also, you didn't include forward_func into the gradients class constructor, so the attribute method was not able to launch the stuff properly.
I think that using LayerIntegratedGradients is better for debugging BERT - in line with this tutorial https://captum.ai/tutorials/Bert_SQUAD_Interpret
Below please find snippet that works:
from captum.attr import LayerIntegratedGradients


def custom_forward(inputs):
    preds = predict(inputs)
    return torch.softmax(preds, dim = 1)[0][1].unsqueeze(-1)
lig = LayerIntegratedGradients(custom_forward, model.bert.embeddings)
def get_influential_words(input_text, model, tokenizer, ig, device):
    model.eval()
    # Tokenizing the input text
    inputs = tokenizer(input_text, return_tensors=""pt"", truncation=True, padding=True, max_length=128)
    input_ids = inputs['input_ids'].to(device)
    attention_mask = inputs['attention_mask'].to(device)
    # print(""Input IDs shape:"", input_ids.shape, ""dtype:"", input_ids.dtype)
    # print(""Attention mask shape:"", attention_mask.shape, ""dtype:"", attention_mask.dtype)

    attributions, delta = lig.attribute(input_ids, return_convergence_delta=True)
    
    tokens = tokenizer.convert_ids_to_tokens(input_ids[0].tolist())
    token_importances = attributions.sum(dim=2).squeeze(0).detach().cpu().numpy()

    return list(zip(tokens, token_importances))

results = []

for idx, correct in enumerate(correct_predictions):
    if correct:
        influential_words = get_influential_words(test_df['text'].iloc[idx], model, tokenizer, ig, device)
        print(f""Influential words for text: {test_df['text'].iloc[idx]}"")
        print(influential_words)

",huggingface-transformers
unexpected transformer39s dataset structure after set_transform or with_transform,"I am using the feature extractor from ViT like explained here.
And noticed a weird behaviour I cannot fully understand.
After loading the dataset as in that colab notebook, I see:
ds['train'].features

{'image_file_path': Value(dtype='string', id=None),  'image':
Image(mode=None, decode=True, id=None),  'labels':
ClassLabel(names=['angular_leaf_spot', 'bean_rust', 'healthy'],
id=None)}

And we can assess the features in both ways:
ds['train']['labels'][0:5]

[0, 0, 0, 0, 0]

ds['train'][0:2]

{'image_file_path':
['/home/albert/.cache/huggingface/datasets/downloads/extracted/967f0d9f61a7a8de58892c6fab6f02317c06faf3e19fba6a07b0885a9a7142c7/train/angular_leaf_spot/angular_leaf_spot_train.0.jpg',
'/home/albert/.cache/huggingface/datasets/downloads/extracted/967f0d9f61a7a8de58892c6fab6f02317c06faf3e19fba6a07b0885a9a7142c7/train/angular_leaf_spot/angular_leaf_spot_train.1.jpg'],
'image': [<PIL.JpegImagePlugin.JpegImageFile image mode=RGB
size=500x500>,   <PIL.JpegImagePlugin.JpegImageFile image mode=RGB
size=500x500>],  'labels': [0, 0]}

But after
from transformers import ViTFeatureExtractor

model_name_or_path = 'google/vit-base-patch16-224-in21k'
feature_extractor = ViTFeatureExtractor.from_pretrained(model_name_or_path)
ds = load_dataset('beans')

def transform(example_batch):
    inputs = feature_extractor([x for x in example_batch['image']], return_tensors='pt')
    inputs['labels'] = example_batch['labels']
    return inputs

prepared_ds = ds.with_transform(transform)

We see the features are kept:
prepared_ds['train'].features

{'image_file_path': Value(dtype='string', id=None),  'image':
Image(mode=None, decode=True, id=None),  'labels':
ClassLabel(names=['angular_leaf_spot', 'bean_rust', 'healthy'],
id=None)}

prepared_ds['train'][0:2]

{'pixel_values': tensor([[[[-0.5686, -0.5686, -0.5608,  ..., -0.0275, 
0.1843, -0.2471],
...,
[-0.5843, -0.5922, -0.6078,  ...,  0.2627,  0.1608,  0.2000]],

         [[-0.7098, -0.7098, -0.7490,  ..., -0.3725, -0.1608, -0.6000],
          ...,
          [-0.8824, -0.9059, -0.9216,  ..., -0.2549, -0.2000, -0.1216]]],

        [[[-0.5137, -0.4902, -0.4196,  ..., -0.0275, -0.0039, -0.2157],
          ...,
          [-0.5216, -0.5373, -0.5451,  ..., -0.1294, -0.1529, -0.2627]],

         [[-0.1843, -0.2000, -0.1529,  ...,  0.2157,  0.2078, -0.0902],
          ...,
          [-0.7725, -0.7961, -0.8039,  ..., -0.3725, -0.4196, -0.5451]],

         [[-0.7569, -0.8510, -0.8353,  ..., -0.3255, -0.2706, -0.5608],
          ...,
          [-0.5294, -0.5529, -0.5608,  ..., -0.1686, -0.1922, -0.3333]]]]), 'labels': [0, 0]}

But when I try to access the labels directly
prepared_ds['train']['labels']

I got a key error message:
```
--------------------------------------------------------------------------- 
KeyError                                  Traceback (most recent call last) Cell In[32], line 1
----> 1 prepared_ds['train']['labels']

File ~/anaconda3/envs/LLM/lib/python3.12/site-packages/datasets/arrow_dataset.py:2872, in Dataset.__getitem__(self, key)    2870 def __getitem__(self, key): 
# noqa: F811    2871     """"""Can be used to index columns (by string names) or rows (by integer index or iterable of indices or bools).""""""
-> 2872     return self._getitem(key)

File ~/anaconda3/envs/LLM/lib/python3.12/site-packages/datasets/arrow_dataset.py:2857, in Dataset._getitem(self, key, **kwargs)    2855 formatter = get_formatter(format_type, features=self._info.features,
**format_kwargs)    2856 pa_subtable = query_table(self._data, key, indices=self._indices)
-> 2857 formatted_output = format_table(    2858     pa_subtable, key, formatter=formatter, format_columns=format_columns, output_all_columns=output_all_columns    2859 )    2860 return formatted_output

File ~/anaconda3/envs/LLM/lib/python3.12/site-packages/datasets/formatting/formatting.py:639, in format_table(table, key, formatter, format_columns, output_all_columns)
    637 python_formatter = PythonFormatter(features=formatter.features)
    638 if format_columns is None:
--> 639     return formatter(pa_table, query_type=query_type)
    640 elif query_type == ""column"":
    641     if key in format_columns:

File ~/anaconda3/envs/LLM/lib/python3.12/site-packages/datasets/formatting/formatting.py:405, in Formatter.__call__(self, pa_table, query_type)
    403     return self.format_row(pa_table)
    404 elif query_type == ""column"":
--> 405     return self.format_column(pa_table)
    406 elif query_type == ""batch"":
    407     return self.format_batch(pa_table)

File ~/anaconda3/envs/LLM/lib/python3.12/site-packages/datasets/formatting/formatting.py:501, in CustomFormatter.format_column(self, pa_table)
    500 def format_column(self, pa_table: pa.Table) -> ColumnFormat:
--> 501     formatted_batch = self.format_batch(pa_table)
    502     if hasattr(formatted_batch, ""keys""):
    503         if len(formatted_batch.keys()) > 1:

File ~/anaconda3/envs/LLM/lib/python3.12/site-packages/datasets/formatting/formatting.py:522, in CustomFormatter.format_batch(self, pa_table)
    520 batch = self.python_arrow_extractor().extract_batch(pa_table)
    521 batch = self.python_features_decoder.decode_batch(batch)
--> 522 return self.transform(batch)

Cell In[12], line 5, in transform(example_batch)
      3 def transform(example_batch):
      4     # Take a list of PIL images and turn them to pixel values
----> 5     inputs = feature_extractor([x for x in example_batch['image']], return_tensors='pt')
      7     # Don't forget to include the labels!
      8     inputs['labels'] = example_batch['labels']

KeyError: 'image'
```

It sounds like the error is because the feature extractor added 'pixel_values' but the feature is kept as 'image'
But it also appears to imply an attempt to re-apply transform...
Also: it is not possible to save the dataset to the disk
    prepared_ds.save_to_disk(img_path)
```
--------------------------------------------------------------------------- 
TypeError                                 Traceback (most recent call last) Cell In[21], line 1
----> 1 dataset.save_to_disk(img_path)

File ~/anaconda3/envs/LLM/lib/python3.13/site-packages/datasets/arrow_dataset.py:1503, in Dataset.save_to_disk(self, dataset_path, max_shard_size, num_shards, num_proc, storage_options)    1501         json.dumps(state[""_format_kwargs""][k])    1502     except TypeError as e:
-> 1503         raise TypeError(    1504             str(e) + f""\nThe format kwargs must be JSON serializable, but key '{k}' isn't.""    1505 ) from None    1506 # Get json serializable dataset info    1507 dataset_info = asdict(self._info)

TypeError: Object of type function is not JSON serializable The format kwargs must be JSON serializable, but key 'transform' isn't.
```

Note the original codes in that notebook work perfectly (training, evaluation, etc). I just got this error because I tried to inspect the dataset, try to save the generated dataset, etc. to explore the dataset object...
Shouldn't the dataset structure be accessible in a similar way after with_transform() or set_transform()? Why does it call the transform function again if we just attempt to access one of the features?
I’m hoping you can shed some light on this behaviour...
","This is not the way how you pick up the dataset items. First you need to indicate the slice:
prepared_ds_batch = prepared_ds['train'][0:10]

by using indexing.
Then you can use the key labels
prepared_ds_batch['labels']
[out]: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]

Regarding the second issue with saving the data: you are not able to save it because of the known issue with transform functions: https://github.com/huggingface/datasets/issues/6221
You might however save the dataset as prepared_ds.with_format(None).save_to_disk('test_path'). But after loading it again from disk you need to launch again the transform function.
Edited: You cannot use prepared_ds['train']['labels'] as 'labels' is expected to be integers representing indices of the items.
",huggingface-transformers
llama321binstruct generate inconsistent output,"I want to use Llama-3.2-1B-Instruct model, and although I have set ""temperature"": 0.0, ""top_p"":0.0 and ""top_k"":0, it still generates inconsistent output. This is how my pipeline looks like:
pipe = pipeline(
    ""text-generation"",
    model=model_id,
    torch_dtype=torch.bfloat16,
    device_map=""mps"",
        model_kwargs={""temperature"": 0.0,
                  ""do_sample"":True,
                              ""top_p"":0.0,
                              ""top_k"":0,},
)

Any idea how to solve this issue?
","The model inconsistent output can be due to two main factors:
1. Temperature:
setting temperature to zero give more inconsistent result. You can refer Opeani discussion page for detail.
So the best option is to set temperature to very low values such as 0.00001 instead of zero.
2. do_sample
You already set it false, and it should remain that way only.
",huggingface-transformers
do those 0 make sense in making the variable,"The guide for fine-tuning Gemma with HuggingFace toolset is at: https://huggingface.co/blog/gemma-peft
Link to the line: https://huggingface.co/blog/gemma-peft#:~:text=Quote%3A%20%7Bexample-,%5B%27quote%27%5D%5B0%5D,-%7D%5CnAuthor%3A
The data entry formatter func is:
def formatting_func(example):
    text = f""Quote: {example['quote'][0]}\nAuthor: {example['author'][0]}<eos>""
    return [text]

Do those [0] make sense? they look wrong coz when printing out text variable I can see they are just characters instead of strings.
","Yes, variable example is a slice of dataset (a batch), not a one item from the dataset. In your particular case you have batch_size set to 1:
per_device_train_batch_size=1
It means that entire dataset is splitted into batches of size 1, i.e. arrays of size 1. So one batch is of the following representation:
[{""quote"": [""Quote 1""], ""author"": [""Author 1""]}]

So in order to get the values you use 0 as index.
Normally if you use batch size larger than or equal to 2 2, the representation would like:
[{""quote"": [""Quote 1"", ""Quote 2"", ...], ""author"": [""Author 1"", ""Author 2"", ...]}]

So in line with documentation you would like to use other formatting_func, where you iterate over both arrays like in this case:
def formatting_prompts_func(example):
    output_texts = []
    for i in range(len(example['instruction'])):
        text = f""### Question: {example['instruction'][i]}\n ### Answer: {example['output'][i]}""
        output_texts.append(text)
    return output_texts

Source: https://huggingface.co/docs/trl/en/sft_trainer
",huggingface-transformers
does peft train newly initialized weights,"When using peft to fine-tune a pretrained model e.g., DistilBert, you need to specify the target_modules. In case of DistilBert, typically, the attention weights are targeted. Example:
lora_config = LoraConfig(
    r=8, # Rank Number
    lora_alpha=32, # Alpha (Scaling Factor)
    lora_dropout=0.05, # Dropout Prob for Lora
    target_modules=[""q_lin"", ""k_lin"",""v_lin""], # Which layer to apply LoRA, usually only apply on MultiHead Attention Layer
    bias='none',
    task_type=TaskType.SEQ_CLS # Seqence to Classification Task
)

my question, when finetuning a pretrained model on a downstream task, you initialize a new layer (like classification layer) which is not pre-trained and have random weights, does peft also freeze this layer or does it optimize it?
","When a new task-specific layer, like a classification head, is added to the pretrained model, it is not frozen by default. This layer typically starts with random weights and is trained/optimized along with the LoRA parameters. So basically PEFT focuses on freezing the backbone of the model and it doesn't include any added layers.
You can easily verify the parameters in your model like this:
for name, param in model.named_parameters():
    print(name, param.requires_grad)

The classification head will have requires_grad=True
",huggingface-transformers
how does one set the pad token correctly not to eos during finetuning to avoid model not predicting eos,"**tldr; what I really want to know is what is the official way to set pad token for fine tuning it wasn't set during original training, so that it doesn't not learn to predict EOS. **
colab: https://colab.research.google.com/drive/1poFdFYmkR_rDM5U5Z2WWjTepMQ8hvzNc?usp=sharing

The HF falcon tutorial has the following line:
tokenizer.pad_token = tokenizer.eos_token

it looks strange to me. It make sense pad and eos are the same but then why even make a difference between them in the first place in general?
Note its wrong to do pad = eos. This means during fine-tuning the model will never be trained to output eos (most likely) since eos is treated as pad token and no back propagated:
I just observed that when I set tokenizer.pad_token = tokenizer.eos_token during training, the model won't stop generating during inference, since it was trained to not output the eos token (per discussions above).

I saw this (here https://github.com/huggingface/transformers/issues/22794):
tokenizer.add_special_tokens({'pad_token': '[PAD]'})

But this assumes the model has a pad_token. I think an additional check has to be done that it does have an embedding for pad_token so that there are no run time errors (~type errors in the matrix extraction from the embedding ""table""/matrix).
But if one does that some care might be needed to initialize the new token so that it dominates the generation: https://nlp.stanford.edu/~johnhew/vocab-expansion.html

code:
def get_model_tokenizer_qlora_falcon7b(model_name: str = ""ybelkada/falcon-7b-sharded-bf16"",
                                       config: wand.Config,  # todo
                                       lora_alpha=16,  # todo
                                       lora_dropout=0.1,  # todo
                                       lora_r=64,  # todo
                                       bnb_4bit_compute_dtype=torch.float16,  # changed it from Guanaco hf
                                       ) -> tuple:
    """"""
    Load the Falcon 7B model, quantize it in 4bit and attach LoRA adapters on it.

    bf16 = 1S, 7Exp, 8Mantissa

    Do:
        pip install bitsandbytes
    ref:
        - https://colab.research.google.com/drive/1DOi8MFv4SWN9NImVornZ7t6BgmLoPQO-#scrollTo=AjB0WAqFSzlD
    """"""
    from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, AutoTokenizer

    # model_id = ""tiiuae/falcon-7b""
    # model_name: str = ""ybelkada/falcon-7b-sharded-bf16""

    # - get bnb config for bit-4 base model (bnb lib for using 4bit qlora quantization techniques by tim dettmers)
    bnb_config = BitsAndBytesConfig(
        load_in_4bit=True,  # load (usually huge) base model in 4 bits
        bnb_4bit_quant_type=""nf4"",  # normal float 4 for the (usually huge) base model. introduces error but fixed by ft
        # ref: https://gist.github.com/pacman100/1731b41f7a90a87b457e8c5415ff1c14
        bnb_4bit_compute_dtype=bnb_4bit_compute_dtype,
    )

    # - get falcon 4bit model
    model = AutoModelForCausalLM.from_pretrained(
        model_name,
        quantization_config=bnb_config,
        trust_remote_code=True  # allows to execute custom code you download from the uploaded model code you are using
    )
    model.config.use_cache = False  # todo: why? https://stackoverflow.com/questions/76633335/why-does-hugging-face-falcon-model-use-mode-config-use-cache-false-why-wouldn

    # get falcon tockenizer
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)  # execs code downloaded from hf hub
    tokenizer.pad_token = tokenizer.eos_token


Modifying model gives issues
Darn this still not works:
 UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation)

code:
""""""
sfttrainer (likely using peft) best practices:
https://huggingface.co/docs/trl/main/en/sft_trainer#best-practices

Best practices

Pay attention to the following best practices when training a model with that trainer:

- SFTTrainer always pads by default the sequences to the max_seq_length argument of the SFTTrainer. If none is passed, the trainer will retrieve that value from the tokenizer. Some tokenizers do not provide default value, so there is a check to retrieve the minimum between 2048 and that value. Make sure to check it before training.
- For training adapters in 8bit, you might need to tweak the arguments of the prepare_model_for_int8_training method from PEFT, hence we advise users to use prepare_in_int8_kwargs field, or create the PeftModel outside the SFTTrainer and pass it.
- For a more memory-efficient training using adapters, you can load the base model in 8bit, for that simply add load_in_8bit argument when creating the SFTTrainer, or create a base model in 8bit outside the trainer and pass it.
- If you create a model outside the trainer, make sure to not pass to the trainer any additional keyword arguments that are relative to from_pretrained() method.

todo: why trust_remote_code? I want more details.
""""""
import sys

import torch
from peft import LoraConfig

from transformers.modeling_utils import PreTrainedModel

from pdb import set_trace as st


def test_bfloat16_int4(compute_dtype: torch.dtype,
                       use_4bit,
                       ):
    """"""
python -c ""import torch; print(torch.cuda.get_device_capability());""
    todo: check other code test_bfloat16() do we need use_4bit?
    """"""
    if compute_dtype == torch.float16 and use_4bit:
        major, _ = torch.cuda.get_device_capability()
        if major >= 8:
            print(""="" * 80)
            print(""Your GPU supports bfloat16, you can accelerate training with the argument --bfloat16"")
            print(""="" * 80)


def get_model_tokenizer_qlora_falcon7b(
        # -- mode args
        # model_id = ""tiiuae/falcon-7b""
        pretrained_model_name_or_path: str = ""ybelkada/falcon-7b-sharded-bf16"",
        use_cache: bool = True,
        # -- lora args
        lora_alpha=16,  # todo
        lora_dropout=0.1,  # todo, evidence drop out really help? google, crfm, gpt4
        lora_r=64,  # todo
        bnb_4bit_compute_dtype=torch.float16,  # changed it from Guanaco hf

        # -- training args
        output_dir=""./results"",
        per_device_train_batch_size=4,
        gradient_accumulation_steps=4,
        # paging so that the sudden mem gpu spikes don't cause the run to shut down
        # (I think usually caused by too long seqs)
        # todo: why 32 bit opt?
        # todo: paged nadamw opt?
        optim=""paged_adamw_32bit"",
        save_steps=10,
        logging_steps=10,
        learning_rate=2e-4,
        max_grad_norm=0.3,
        max_steps=500,
        warmup_ratio=0.03,
        lr_scheduler_type=""constant"",
        # -- quant. args (not recommended to be changed unless you know what your doing?)
        load_in_4bit=True,  # load (usually huge) base model in 4 bits
        bnb_4bit_quant_type=""nf4"",  # normal float 4 for the (large) base models qlora
) -> tuple:
    """"""
    Load the Falcon 7B model, quantize it in 4bit and attach LoRA adapters on it.

    bf16 = 1S, 7Exp, 8Mantissa
    hypothesis: 7b trained due to 6.7 emergence rumour, I still don't think emergence is real.
    Notes:
        - ft a model is very specific to the model, tokenizer and training scheme. Thus we return
            - model, tokenizer, ft config (peft config), training args

    ref:
        - https://colab.research.google.com/drive/1DOi8MFv4SWN9NImVornZ7t6BgmLoPQO-#scrollTo=AjB0WAqFSzlD
    """"""
    from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, AutoTokenizer

    # - Get bnb config for bit-4 base model (bnb lib for using 4bit qlora quantization techniques by tim dettmers)
    bnb_config = BitsAndBytesConfig(
        load_in_4bit=load_in_4bit,  # load (usually huge) base model in 4 bits
        bnb_4bit_quant_type=bnb_4bit_quant_type,  # normal float 4 for the (usually huge) base model
        bnb_4bit_compute_dtype=bnb_4bit_compute_dtype,  # if you can, during computation use bf16
    )

    # - Get falcon 4bit model
    # todo, where is this being saved & how to download quicker
    model = AutoModelForCausalLM.from_pretrained(
        pretrained_model_name_or_path=pretrained_model_name_or_path,
        quantization_config=bnb_config,
        trust_remote_code=True  # allows to execute custom code you download from the uploaded model code you are using
    )
    print(f'{type(model)=}')
    print(f'{model=}')
    # this is here to save gpu vram. Likely only needed when using 40b or when oom issues happen ref: https://stackoverflow.com/questions/76633335/why-does-hugging-face-falcon-model-use-mode-config-use-cache-false-why-wouldn
    model.config.use_cache = use_cache
    print(f'{type(model)=}')

    # - Get falcon tokenizer
    tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name_or_path,
                                              trust_remote_code=True)  # execs code downloaded from hf hub
    # tokenizer.pad_token = tokenizer.eos_token  # ref: https://stackoverflow.com/questions/76633368/why-does-the-falcon-qlora-tutorial-code-use-eos-token-as-pad-token
    # tokenizer.add_special_tokens({'pad_token': '[PAD]'})  # I think this is fine if during the training pad is ignored
    tokenizer.add_special_tokens({'pad_token': '<|pad|>'})  # I think this is fine if during the training pad is ignored

    # - Modify model
    # add pad token embed
    model.resize_token_embeddings(len(tokenizer))  # todo: I think this is fine if during the training pad is ignored
    model.transformer.word_embeddings.padding_idx = len(tokenizer) - 1
    model.config.max_new_tokens = len(tokenizer)
    # model.config.min_length = 1
    print(f'{model=}')
    print(f'{type(tokenizer)=}')
    print(f'{tokenizer.pad_token=}')
    # data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False) todo

    # - Get falcon lora config
    peft_config = LoraConfig(
        lora_alpha=lora_alpha,
        lora_dropout=lora_dropout,
        r=lora_r,
        bias=""none"",
        task_type=""CAUSAL_LM"",
        # model card for falcon tiiuae/falcon-7b: https://huggingface.co/tiiuae/falcon-7b/blob/main/modelling_RW.py
        # does seem to include all trainable params as done by qlora on their own paper
        target_modules=[
            # word_embeddings,
            ""query_key_value"",
            ""dense"",
            ""dense_h_to_4h"",
            ""dense_4h_to_h"",
            # ""lm_head""
        ]
    )
    print(f'{type(peft_config)=}')

    # todo: print the num params of the lora = D1*r + D2*r and num of bytes by prec. (bytes) * num params
    return model, tokenizer, peft_config


# -- tests

def example_test_model_already_has_pad_token():
    """"""
    if it already has pad token, it likely has a small prob, so we are done.

    compare it's norm with other tokens to verify this is true.

python ~/ultimate-utils/ultimate-utils-proj-src/uutils/hf_uu/model_tokenizer/falcon_uu_mdl_tok.py
    """"""
    # - the get datasets todo: preprocessing, padding, streaming
    from uutils.hf_uu.data_hf.common import get_guanaco_datsets_add_splits_train_test_only
    trainset, _, testset = get_guanaco_datsets_add_splits_train_test_only()

    # qlora flacon7b
    from uutils.hf_uu.model_tokenizer.falcon_uu_mdl_tok import get_model_tokenizer_qlora_falcon7b
    model, tokenizer, peft_config = get_model_tokenizer_qlora_falcon7b()
    model: PreTrainedModel = model
    print(f'{model=}')
    sent = 'Dogs are great because they are '
    print()

    # print to see if pad tokens are present and if it ignores the tokens at the end
    encoded_input = tokenizer(sent, padding='max_length', max_length=10, return_tensors='pt')
    print(f'{encoded_input=}')

    # Print all special tokens
    print('\n---- start Print all special tokens')
    for token_name, token in tokenizer.special_tokens_map.items():
        print(f""{token_name}: {token}"")
    print('\n---- end Print all special tokens')

    # Get the ID for the '[PAD]' token
    try:
        pad_token_id = tokenizer.convert_tokens_to_ids('[PAD]')
    except KeyError:
        raise ValueError(""Token [PAD] is not present in the tokenizer vocabulary."")

    # Index into the model's embedding table
    try:
        print(f'{model.get_input_embeddings().weight.size()=}')
        pad_embedding = model.get_input_embeddings().weight[pad_token_id]
    except IndexError:
        raise ValueError(f""Token ID {pad_token_id} is not present in the model's embedding matrix."")

    print(f'{pad_embedding=}')
    print('Success!\n')

    # check it generates something sensible
    # tokenizer.decode(model.generate(**tokenizer(sent, return_tensors='pt'), do_sample=True)[0])
    input_ids, attention_mask = encoded_input['input_ids'], encoded_input['attention_mask']
    predicted_tokens_ids_options = model.generate(input_ids=input_ids, attention_mask=attention_mask, do_sample=True)
    predicted_tokens_ids = predicted_tokens_ids_options[0]
    predicted_sent = tokenizer.decode(predicted_tokens_ids)
    print(f'original sentence: {sent=}')
    print(f'predicted sentence: {predicted_sent=}')
    print('Success2!')


if __name__ == '__main__':
    import time

    start_time = time.time()
    example_test_model_already_has_pad_token()
    print(f""The main function executed in {time.time() - start_time} seconds.\a"")

it doesn't like the modifications to the model:
    model.transformer.word_embeddings.padding_idx = len(tokenizer) - 1
    model.config.max_new_tokens = len(tokenizer)

How to fix?
Errors:
/lfs/hyperturing1/0/brando9/miniconda/envs/data_quality/lib/python3.10/site-packages/transformers/generation/utils.py:1259: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation)
  warnings.warn(
Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.
/lfs/hyperturing1/0/brando9/miniconda/envs/data_quality/lib/python3.10/site-packages/transformers/generation/utils.py:1452: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cpu, whereas the model is on cuda. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cuda') before running `.generate()`.
  warnings.warn(
Traceback (most recent call last):
  File ""/lfs/hyperturing1/0/brando9/ultimate-utils/ultimate-utils-proj-src/uutils/hf_uu/model_tokenizer/falcon_uu_mdl_tok.py"", line 211, in <module>
    example_test_model_already_has_pad_token()
  File ""/lfs/hyperturing1/0/brando9/ultimate-utils/ultimate-utils-proj-src/uutils/hf_uu/model_tokenizer/falcon_uu_mdl_tok.py"", line 199, in example_test_model_already_has_pad_token
    predicted_tokens_ids_options = model.generate(input_ids=input_ids, attention_mask=attention_mask, do_sample=True)
  File ""/lfs/hyperturing1/0/brando9/miniconda/envs/data_quality/lib/python3.10/site-packages/torch/utils/_contextlib.py"", line 115, in decorate_context
    return func(*args, **kwargs)
  File ""/lfs/hyperturing1/0/brando9/miniconda/envs/data_quality/lib/python3.10/site-packages/transformers/generation/utils.py"", line 1572, in generate
    return self.sample(
  File ""/lfs/hyperturing1/0/brando9/miniconda/envs/data_quality/lib/python3.10/site-packages/transformers/generation/utils.py"", line 2633, in sample
    next_token_scores = logits_warper(input_ids, next_token_scores)
  File ""/lfs/hyperturing1/0/brando9/miniconda/envs/data_quality/lib/python3.10/site-packages/transformers/generation/logits_process.py"", line 92, in __call__
    scores = processor(input_ids, scores)
  File ""/lfs/hyperturing1/0/brando9/miniconda/envs/data_quality/lib/python3.10/site-packages/transformers/generation/logits_process.py"", line 302, in __call__
    indices_to_remove = scores < torch.topk(scores, top_k)[0][..., -1, None]
RuntimeError: ""topk_cpu"" not implemented for 'Half'


Bounty Section: Small GPT2 code example
Yes I agree that pad is assigned to eos. Eos is still eos. But during fine-tuning now the weights wrt to eos are unchanged. This might be an issue since the probability of eos has not shifted to the fine-tuning regime. One possibility is that eos is outputed with less chance. Yes we can still halt production when we see eos but we've not shifted the probability to output eos according to our fine-tuning distribution -- but all other tokens have changed distribution. I think this could be an issue because it's not like the old probability of eos is conserved since all tokens probs have changed except eos + even if the old eos prob was conserved, it's wrt wrong distribution (not the fine tuning one).
e.g.,
    if tokenizer.pad_token_id is None:
        tokenizer.pad_token = tokenizer.eos_token
...
    raw_text_batch='a'
    tokenize_batch={'input_ids': tensor([[   64, 50256, 50256, 50256, 50256]]), 'attention_mask': tensor([[1, 0, 0, 0, 0]])}

but it would have been better to have
    tokenize_batch={'input_ids': tensor([[   64, 50256, 50256, 50256, 50256]]), 'attention_mask': tensor([[1, 1, 0, 0, 0]])}

code
def test_eos_pad():
    from datasets import load_dataset
    import torch
    from transformers import GPT2Tokenizer, GPT2LMHeadModel

    raw_text_batch = 'a'

    tokenizer = GPT2Tokenizer.from_pretrained(""gpt2"")
    # print(f'{tokenizer.eos_token=}')
    # print(f'{tokenizer.eos_token_id=}')
    # print(f'{tokenizer.pad_token=}')
    # print(f'{tokenizer.pad_token_id=}')

    # print(f'{raw_text_batch=}')
    # tokenize_batch = tokenizer(raw_text_batch, padding=""max_length"", max_length=5, truncation=True, return_tensors=""pt"")
    # print(f'{tokenize_batch=}')

    if tokenizer.pad_token_id is None:
        tokenizer.pad_token = tokenizer.eos_token
    probe_network = GPT2LMHeadModel.from_pretrained(""gpt2"")
    device = torch.device(f""cuda:{0}"" if torch.cuda.is_available() else ""cpu"")
    probe_network = probe_network.to(device)

    print(f'{tokenizer.eos_token=}')
    print(f'{tokenizer.eos_token_id=}')
    print(f'{tokenizer.pad_token=}')
    print(f'{tokenizer.pad_token_id=}')

    print(f'{raw_text_batch=}')
    tokenize_batch = tokenizer(raw_text_batch, padding=""max_length"", max_length=5, truncation=True, return_tensors=""pt"")
    print(f'{tokenize_batch=}')
    print('Done')


cross:

hf discuss forum: https://discuss.huggingface.co/t/why-does-the-falcon-qlora-tutorial-code-use-eos-token-as-pad-token/45954
pytorch forum discuss: https://discuss.pytorch.org/t/how-does-one-set-the-pad-token-correctly-not-to-eos-during-fine-tuning-to-avoid-model-not-predicting-eos/213619
https://discord.com/channels/879548962464493619/1126681170957045770/1126681170957045770
context peft pacman100 code: https://gist.github.com/pacman100/1731b41f7a90a87b457e8c5415ff1c14
twitter tweet of this: https://twitter.com/BrandoHablando/status/1693676898013061337?s=20

","Update August/8/2024
One more improvement. Some models like DeepSeekCoder base 7B do have a pad token already. So no need to set the pad token to eos. But the code that pads up to 1st occurence of eos + pads the rest has to pad the rest assuming they are pad tokens. So that's the diff:
def get_lm_examples_1st_eos_mask_remaining_eos(
        examples,
        tokenizer: AutoTokenizer, 
        
        # desired_dataset_column: str = 'text',
        # method_to_remove_columns: str = 'keys',

        remove_to_long_seqs: bool = False,
        # format: str = 'torch',
        debug: bool = False,
        ) -> dict[str, torch.Tensor]:
    """""" 
    Train only on first occurence of eos. The remaining eos are masked out. If 
    - train up to 1st ocurrence of eos token, mask out the rest of the eos tokens.
    - drop or not seqs that are too long, i.e., have no eos token.
    
    Assumes: pad == eos

    ref: https://stackoverflow.com/questions/76633368/how-does-one-set-the-pad-token-correctly-not-to-eos-during-fine-tuning-to-avoi
    """"""
    # - Get lm example
    seq_length: int = examples['input_ids'].size(0)
    print(f'{examples[""input_ids""].size()=}, {seq_length=}') if debug else None
    examples[""labels""] = examples[""input_ids""].clone()  # labels is hardcoded in HF so put it!
    eos_token_id = tokenizer.eos_token_id
    # assert eos_token_id == tokenizer.pad_token_id, 'Error: pad should be eos token'
    print(f'{tokenizer.pad_token_id=}, {tokenizer.eos_token_id=}') if debug else None
    seqs_to_drop: list[int] = [] # store idx to drop (to long), we don't want to modify the two lists at the same time as we are looping through them
    for idx, input_ids in enumerate(examples[""input_ids""]):
        # Find all occurrences of eos_token
        eos_positions = (input_ids == eos_token_id).nonzero(as_tuple=True)[0]
        if eos_positions.nelement() > 0:  # Check if eos_token is present --> if yes then make sure to trian on it then mask the remaining eos (assumes pad == eos)
            first_eos_position = eos_positions[0]
            examples[""attention_mask""][idx, first_eos_position] = 1  # Set the mask value to 1
            # Assert that the label for the first occurrence of eos_token is eos_token_id
            assert examples[""labels""][idx, first_eos_position] == eos_token_id, ""The label for the first eos_token is incorrect!""
            # # For all subsequent occurrences of eos_token, set their labels to -100
            # for subsequent_eos_position in eos_positions[1:]:
            #     examples[""labels""][idx, subsequent_eos_position] = -100
            #     assert examples[""labels""][idx, subsequent_eos_position] == -100, ""The label for the subsequent_eos_position incorrect! Should be -100.""
            # after first eos token mask everything (eos AND pad, hopefully that's all there but we can sanity check later)
            for desired_mask_idx in range(first_eos_position, seq_length):
                examples[""labels""][idx, desired_mask_idx] = -100
                assert examples[""labels""][idx, desired_mask_idx] == -100, ""The label for the desired_mask_idx incorrect! Should be -100.""
        elif remove_to_long_seqs:
            assert eos_positions.nelement() == 0, 'Error: there should be no eos if this if stmt is exexuted.'
            # record to drop this seq, has no eos so too long + flag says to drop it
            seqs_to_drop.append(idx)
        else:
            pass # nop: no eos in seq so too long, but keep it for training anyway
    # assert len(examples[""labels""]) == 0, 'Error: no labels were set'
    # -- Drop seqs with no eos
    if seqs_to_drop:
        examples[""input_ids""] = torch.stack([input_ids for idx, input_ids in enumerate(examples[""input_ids""]) if idx not in seqs_to_drop])
        examples[""attention_mask""] = torch.stack([mask for idx, mask in enumerate(examples[""attention_mask""]) if idx not in seqs_to_drop])
        examples[""labels""] = torch.stack([labels for idx, labels in enumerate(examples[""labels""]) if idx not in seqs_to_drop])
    return examples


Update, I made this better. Now if seq does not have eos at all you can remove that seq or chose to train on it
def raw_ds_2_lm_ds_mask_eos_pad_toks(
        raw_dataset, 
        tokenizer, 
        max_length: int,

        raw_str_2_desired_str: Optional[callable] = None, # either return {'text': examples['text']} or preprocess str to get what you need e.g. {'text': f""[ex['nl'] ex['fl'] {tok.eos_token}]"" for ex in examples}
        desired_dataset_column: str = 'text', # good val to use if hf str ds already pre-processed for you: 'text',

        method_to_remove_columns: str = 'keys',

        padding: str = 'max_length',
        truncation: bool = True, 
        return_tensors: str = 'pt',

        batched: bool = True, # Setting `batched=True` in the `dataset.map` function of Hugging Face's datasets library processes the data in batches rather than one item at a time, significantly speeding up the tokenization and preprocessing steps.
        streaming: bool = False,

        format: str = 'torch',
        # get_lm_examples_function = get_lm_examples_1st_eos_mask_remaining_eos,
        ):
    """""" """"""
    # - Get desired str dataset
    if raw_str_2_desired_str is None:
        get_desired_examples_str_function = lambda examples: {'text': examples[desired_dataset_column]} if raw_str_2_desired_str is not None else raw_str_2_desired_str 
    else:
        get_desired_examples_str_function = raw_str_2_desired_str
    desired_examples_str_dataset = raw_dataset.map(get_desired_examples_str_function, batched=batched) # note: we can't remove all str columns here or we will remove the ones we want to tokenize by accident

    # - Get tokenized data set
    desired_examples_str_dataset = desired_examples_str_dataset.with_format(format)  # annoying that return tensors in the tokenizer on it's own doesn't put it into a pt tensor, so for now we keep both.
    remove_str_columns = get_column_names(desired_examples_str_dataset, streaming, method_to_remove_columns)  # remove all keys that are not tensors to avoid bugs in collate function in task2vec's pytorch data loader
    tokenize_function = lambda examples: tokenizer(examples[desired_dataset_column], padding=padding, max_length=max_length, truncation=truncation, return_tensors=return_tensors)
    tokenized_datasets = desired_examples_str_dataset.map(tokenize_function, batched=batched, remove_columns=remove_str_columns)

    # - Get lm data set
    # get_lm_examples_function = lambda examples : group_texts(examples, block_size)
    get_lm_examples_function = lambda examples : get_lm_examples_1st_eos_mask_remaining_eos(examples, tokenizer)
    lm_dataset = tokenized_datasets.map(get_lm_examples_function, batched=batched)
    return lm_dataset

def get_lm_examples_1st_eos_mask_remaining_eos(
        examples,
        tokenizer: AutoTokenizer, 
        
        # desired_dataset_column: str = 'text',
        # method_to_remove_columns: str = 'keys',

        remove_to_long_seqs: bool = False,
        # format: str = 'torch',
        ) -> dict[str, torch.Tensor]:
    """""" 
    Train only on first occurence of eos. The remaining eos are masked out. If 
    - train up to 1st ocurrence of eos token, mask out the rest of the eos tokens.
    - drop or not seqs that are too long, i.e., have no eos token.
    
    Assumes: pad == eos

    ref: https://stackoverflow.com/questions/76633368/how-does-one-set-the-pad-token-correctly-not-to-eos-during-fine-tuning-to-avoi
    """"""
    # - Get lm example
    examples[""labels""] = examples[""input_ids""].clone()  # labels is hardcoded in HF so put it!
    eos_token_id = tokenizer.eos_token_id
    assert eos_token_id == tokenizer.pad_token_id, 'Error: pad should be eos token'
    seqs_to_drop: list[int] = [] # store idx to drop (to long), we don't want to modify the two lists at the same time as we are looping through them
    for idx, input_ids in enumerate(examples[""input_ids""]):
        # Find all occurrences of eos_token
        eos_positions = (input_ids == eos_token_id).nonzero(as_tuple=True)[0]
        if eos_positions.nelement() > 0:  # Check if eos_token is present --> if yes then make sure to trian on it then mask the remaining eos (assumes pad == eos)
            first_eos_position = eos_positions[0]
            examples[""attention_mask""][idx, first_eos_position] = 1  # Set the mask value to 1
            # Assert that the label for the first occurrence of eos_token is eos_token_id
            assert examples[""labels""][idx, first_eos_position] == eos_token_id, ""The label for the first eos_token is incorrect!""
            # For all subsequent occurrences of eos_token, set their labels to -100
            for subsequent_eos_position in eos_positions[1:]:
                examples[""labels""][idx, subsequent_eos_position] = -100
                assert examples[""labels""][idx, subsequent_eos_position] == -100, ""The label for the subsequent_eos_position incorrect! Should be -100.""
        elif remove_to_long_seqs:
            assert eos_positions.nelement() == 0, 'Error: there should be no eos if this if stmt is exexuted.'
            # record to drop this seq, has no eos so too long + flag says to drop it
            seqs_to_drop.append(idx)
        else:
            pass # nop: no eos in seq so too long, but keep it for training anyway
    # assert len(examples[""labels""]) == 0, 'Error: no labels were set'
    # -- Drop seqs with no eos
    if seqs_to_drop:
        examples[""input_ids""] = torch.stack([input_ids for idx, input_ids in enumerate(examples[""input_ids""]) if idx not in seqs_to_drop])
        examples[""attention_mask""] = torch.stack([mask for idx, mask in enumerate(examples[""attention_mask""]) if idx not in seqs_to_drop])
        examples[""labels""] = torch.stack([labels for idx, labels in enumerate(examples[""labels""]) if idx not in seqs_to_drop])
    return examples

train script:
""""""
Refs:
    - https://claude.ai/chat/ad5c9e18-beb4-48fb-9f43-a2ba463ce158
    - https://chatgpt.com/c/349f2c8a-949e-444d-ae3c-8ca60ba77831
""""""
import glob
import os
import numpy as np
import torch
from transformers import GPT2Tokenizer, GPT2LMHeadModel, TrainingArguments, Trainer
from transformers import AutoModelForCausalLM, AutoTokenizer
from datasets import load_dataset, load_metric
from typing import Dict, Tuple, Optional
from pathlib import Path
import evaluate

from utils import eval_hf
from utils import raw_ds_2_lm_ds_mask_eos_pad_toks

def compute_metrics(eval_pred: Tuple[np.ndarray, np.ndarray],
                    path: str = 'accuracy',
                    ) -> Dict[str, float]:
    """"""
    Compute the accuracy of the model.

    Args:
    eval_pred: A tuple containing the model predictions and labels.

    Returns:
    A dictionary with the accuracy score.
    
    TODO: document properly what accuracy is. Is it tfa, ara, exact string match, avg acc (wrt length etc.) ref: https://huggingface.co/spaces/evaluate-metric/accuracy
    """"""
    metric = evaluate.load(path=path)   # load metric from file or hf
    predictions, references = eval_pred
    predictions = np.argmax(predictions, axis=1)
    return metric.compute(predictions=predictions, references=references)

def preprocess_function_proofnet_simple(examples: Dict[str, list], tokenizer: GPT2Tokenizer, max_length: int = 512) -> Dict[str, torch.Tensor]:
    """"""
    Preprocess the input data for the proofnet dataset.

    Args:
    examples: The examples to preprocess.
    tokenizer: The tokenizer for encoding the texts.

    Returns:
    The processed model inputs.
    """"""
    # - Get raw string ins,outs (so deal with HF data set columns at str level)
    inputs: list[str] = [f""{examples['nl_statement'][i]}{tokenizer.eos_token}{examples['formal_statement'][i]}"" for i in range(len(examples['nl_statement']))]
    # - Get tokenized ins,outs (so remove irrelevant ""string"" columns to get only ""tensor"" relevant columns)
    model_inputs = tokenizer(inputs, max_length=max_length, padding=""max_length"", truncation=True, return_tensors=""pt"")
    # - Get lm ins,outs for training e.g., deal with padd, masks etc.
    labels = model_inputs.input_ids.clone()
    labels[labels == tokenizer.pad_token_id] = -100
    model_inputs[""labels""] = labels
    return model_inputs

def setup_and_train_proofnet(
        # pretrained_model_name_or_path: str = ""gpt2"", 
        # pretrained_model_name_or_path: str = ""openai-community/gpt2-xl"", 
        pretrained_model_name_or_path: str = ""meta-llama/Meta-Llama-3.1-8B"", 
        path: str = ""hoskinson-center/proofnet"",
        output_dir_train: str = '~/tmp/proofnet/train',
        output_dir_val: Optional[str] = None,  # we are training on the val set so no val set
        output_dir_test: str = '~/tmp/proofnet/test',
        path_to_save_model: Optional[str] = None,  # suggested path: '~/tmp/proofnet/model' then expanduser in py code
        num_train_epochs: int = 3,
        per_device_train_batch_size: Optional[int] = 2,
        per_device_eval_batch_size: Optional[int] = 2,
        learning_rate: float = 5e-5,
        weight_decay: float = 0.01,
        max_grad_norm: float = 1.0, 
        lr_scheduler_type = 'cosine', # https://discord.com/channels/879548962464493619/1227708244697284724/1227708244697284724
        warmup_ratio=0.01,  # copying alpaca for now, number of steps for a linear warmup,  https://discord.com/channels/879548962464493619/1227708244697284724/1227708244697284724
        optim='paged_adamw_32bit',
        gradient_accumulation_steps = 2, # Allows to process effective_batch_size = gradient_accumulation_steps * batch_size, num its to accumulate before opt update step
        gradient_checkpointing: Optional[bool] = True,
        report_to: str = 'none',  # recommended values 'wandb' or `none`
        ) -> None:
    """"""
    Set up the environment, preprocess the dataset, and train the model.

    export CUDA_VISIBLE_DEVICES=7

    Args:
    tokenizer_name: The name of the tokenizer.
    model_name: The name of the model.
    dataset_path: The path to the dataset.
    """"""
    # Clear CUDA cache to free up memory
    torch.cuda.empty_cache()

    # Load tokenizer and model
    if pretrained_model_name_or_path == ""gpt2"":
        tokenizer = GPT2Tokenizer.from_pretrained(pretrained_model_name_or_path, max_length=1024)
        if tokenizer.pad_token_id is None:
            tokenizer.pad_token = tokenizer.eos_token
            print(f'{tokenizer.pad_token=}')
        print(f'{tokenizer.eos_token=}\n{tokenizer.eos_token_id=}')
        model = GPT2LMHeadModel.from_pretrained(pretrained_model_name_or_path)
        device = torch.device(f""cuda:{0}"" if torch.cuda.is_available() else ""cpu"")
        model = model.to(device)
        max_length: int = tokenizer.model_max_length
        print(f'{max_length=}')
    elif pretrained_model_name_or_path == ""openai-community/gpt2-xl"":
        tokenizer = GPT2Tokenizer.from_pretrained(pretrained_model_name_or_path, max_length=1024)
        if tokenizer.pad_token_id is None:
            tokenizer.pad_token = tokenizer.eos_token
            print(f'{tokenizer.pad_token=}')
        print(f'{tokenizer.eos_token=}\n{tokenizer.eos_token_id=}')
        model = GPT2LMHeadModel.from_pretrained(pretrained_model_name_or_path)
        device = torch.device(f""cuda:{0}"" if torch.cuda.is_available() else ""cpu"")
        model = model.to(device)
        max_length: int = tokenizer.model_max_length
        print(f'{max_length=}') 
    elif pretrained_model_name_or_path == ""meta-llama/Meta-Llama-3.1-8B"":
        torch_dtype = torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float32 
        model = AutoModelForCausalLM.from_pretrained(pretrained_model_name_or_path, torch_dtype=torch_dtype)
        # tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name_or_path, padding_side=""right"", use_auth_token=True)
        tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name_or_path, padding_side=""right"")
        print(f'{tokenizer.pad_token=} {tokenizer.eos_token_id=}')
        tokenizer.pad_token = tokenizer.eos_token if tokenizer.pad_token_id is None else tokenizer.pad_token
        print(f'{tokenizer.pad_token=} {tokenizer.eos_token_id=}')
        # get context length for setting max length for training
        if hasattr(model.config, ""context_length""):
            # SEEMS IT IS NOT IN THE model.config
            print(""Context length:"", model.config.context_length)
            max_length: int = model.config.context_length
        else:
            print(f""Context length not found in model.config, so using your default or hardcoded value. Model is {pretrained_model_name_or_path=}."")
            # max_length: int = 4096
            max_length: int = 8192
            # max_length: int = 128  # for debugging
            # max_length: int = 128_000  # ref: https://huggingface.co/meta-llama/Meta-Llama-3.1-8B
            print(f'->{max_length=}')
    else:
        raise ValueError(f""Model {pretrained_model_name_or_path} not supported."")
    print(""Number of parameters:"", sum(p.numel() for p in model.parameters()))

    # - Load the dataset
    print(f'-Load the dataset')
    ## Proofnet
    # dataset_val = load_dataset(path, split='validation')
    # dataset_test = load_dataset(path, split='test')
    # # Preprocess the dataset
    # if path == ""hoskinson-center/proofnet"":
    #     preprocess_function = preprocess_function_proofnet_simple
    #     # note: text field is usually more common!
    #     val_dataset = dataset_val.map(lambda examples: preprocess_function(examples, tokenizer), batched=True, remove_columns=[""nl_statement"", ""formal_statement""])
    #     test_dataset = dataset_test.map(lambda examples: preprocess_function(examples, tokenizer), batched=True, remove_columns=[""nl_statement"", ""formal_statement""])
    ## C4
    # train_dataset = load_dataset(path='allenai/c4', name='en', split='train', streaming=True)
    # eval_dataset = load_dataset(path='allenai/c4', name='en', split='validation', streaming=True)
    # train_dataset = raw_ds_2_lm_ds_mask_eos_pad_toks(train_dataset, tokenizer, max_length)
    # eval_dataset = raw_ds_2_lm_ds_mask_eos_pad_toks(eval_dataset, tokenizer, max_length)

    # json files for putnam are not consistent and it seems they have to be: https://chatgpt.com/c/9cecca7d-d50d-42e2-b2d3-c1057bc21ef2 solve later
    # ~/putnam-math/data/Putnam_MATH_variations_static3/original/test
    # json_files = glob.glob(os.path.expanduser('~/putnam-math/data/Putnam_MATH_original_static3/test/**/*.json'), recursive=True)
    # train_dataset = load_dataset('json', data_files=json_files)
    # json_files = glob.glob(os.path.expanduser('~/putnam-math/data/Putnam_MATH_variations_static3/variations/test/**/*.json'), recursive=True)
    # eval_dataset = load_dataset('json', data_files=json_files)
    # train_dataset = raw_ds_2_lm_ds_mask_eos_pad_toks(train_dataset, tokenizer, max_length)
    # eval_dataset = raw_ds_2_lm_ds_mask_eos_pad_toks(eval_dataset, tokenizer, max_length)

    # Proofnet with 1st eos token train remaining eos not train
    from train.utils import raw_str_2_desired_af_str
    _raw_str_2_desired_af_str = lambda examples: raw_str_2_desired_af_str(examples, tokenizer)  # tokenizer needed to get eos tok to form right str to train on.
    train_dataset = load_dataset(path, split='validation')
    eval_dataset = load_dataset(path, split='test')
    train_dataset = raw_ds_2_lm_ds_mask_eos_pad_toks(train_dataset, tokenizer, max_length, raw_str_2_desired_str=_raw_str_2_desired_af_str)
    eval_dataset = train_dataset
    print(f'->{len(train_dataset)=} {len(eval_dataset)=}')
    # max_steps: int = (len(train_dataset) * num_train_epochs) // per_device_train_batch_size  # TODO: really?

    # Training arguments
    output_dir_train: Path = Path(output_dir_train).expanduser()
    output_dir_train.mkdir(parents=True, exist_ok=True)
    training_args = TrainingArguments(
        output_dir=output_dir_train,
        max_steps=2,  # TODO get rid of this in favour of 1 or 2 or 3 epochs
        # num_train_epochs=num_train_epochs, 
        gradient_accumulation_steps=gradient_accumulation_steps,  # based on alpaca https://github.com/tatsu-lab/stanford_alpaca, allows to process effective_batch_size = gradient_accumulation_steps * batch_size, num its to accumulate before opt update step
        gradient_checkpointing = gradient_checkpointing,  # TODO depending on hardware set to true?
        per_device_train_batch_size=per_device_train_batch_size,
        per_device_eval_batch_size=per_device_eval_batch_size,
        learning_rate=learning_rate,
        weight_decay=weight_decay, 
        max_grad_norm=max_grad_norm, # TODO once real training change?
        lr_scheduler_type=lr_scheduler_type,  # TODO once real training change? using what I've seen most in vision 
        warmup_ratio=warmup_ratio,
        optim=optim,
        # logging_strategy='epoch', # TODO
        save_steps=100, # Save checkpoint every 500 steps
        save_total_limit=3, # save last 3
        logging_steps=10,  # Frequency of logging steps
        logging_first_step=True,
        logging_dir=output_dir_train,
        evaluation_strategy='no',  # ""no""`: No evaluation is done during training. no can be good to avoid memory issues.
        # evaluation_strategy=""steps"",  # TODO Evaluate model at specified steps
        # eval_steps=110,  # TODO Evaluate every 100 steps
        # remove_unused_columns=False,  # TODO https://stackoverflow.com/questions/76879872/how-to-use-huggingface-hf-trainer-train-with-custom-collate-function/76929999#76929999 , https://claude.ai/chat/475a4638-cee3-4ce0-af64-c8b8d1dc0d90
        report_to=report_to,  # options I recommend: 'none', 'wandb'
        fp16=False,  # never ever set to True
        bf16=torch.cuda.is_bf16_supported(),
        # full_determinism=True,  # TODO periphery, Ensure reproducibility
        # torchdynamo=""nvfuser"",  # TODO periphery, Use NVFuser backend for optimized torch operations
        # dataloader_prefetch_factor=2,  # TODO periphery, Number of batches to prefetch
        # dataloader_pin_memory=True,  # TODO periphery, Pin memory in data loaders for faster transfer to GPU
        # dataloader_num_workers=16,  # TODO Number of subprocesses for data loading
    )

    # Initialize the Trainer 
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset,
        eval_dataset=eval_dataset,  # set to None if eval is giving you memory issues
        tokenizer=tokenizer,
        compute_metrics=compute_metrics
    )
    # Train the model
    trainer.train()

    # Evaluate the model
    if output_dir_test is not None:
        output_dir_test: Path = Path(output_dir_test).expanduser()
        output_dir_test.mkdir(parents=True, exist_ok=True)
        eval_args = TrainingArguments(output_dir=output_dir_test, per_device_eval_batch_size=per_device_eval_batch_size, fp16=False, bf16=torch.cuda.is_bf16_supported(), report_to=report_to)
        trainer = Trainer(model=model, args=eval_args, train_dataset=None, eval_dataset=eval_dataset)
        # results: dict[str, float] = trainer.evaluate(test_dataset)
        results: dict[str, float] = eval_hf(trainer, name='', path=path, split='test')
        print(f'{path=} split=test {results=}')

    # Save the trained model
    if path_to_save_model is not None:
        model.save_pretrained(path_to_save_model)

def main() -> None:
    """"""
    Main function to execute the model training and evaluation.
    """"""
    setup_and_train_proofnet()

if __name__ == ""__main__"":
    import time
    start_time = time.time()
    main()
    print(f""Time taken: {time.time() - start_time:.2f} seconds, or {(time.time() - start_time) / 60:.2f} minutes, or {(time.time() - start_time) / 3600:.2f} hours.\a"")

code repo: https://github.com/brando90/snap-cluster-setup/blob/9778140d8eb378f7c7873ec3fa906d0b01064031/py_src/train/simple_train2.py#L1

OK I think this is the code that train on first occurrence of eos and makes sure the rest are NOT trained on (feedback welcomed):
def collate_fn_train_only_first_eos_token_mask_everything_after_it(data: list[dict[str, str]], 
                                                                    tokenizer: PreTrainedTokenizer, 
                                                                    max_length: int=1024,  # GPT2 default, likely worth you change it! This default might cause bugs.
                                                                    ) -> dict[str, torch.Tensor]:
    """""" Train only on first occurence of eos. The remaining eos are masked out.

    Sometimes the model might not have a padding token. Sometimes people set the padding token to be the eos token.
    But sometimes this seems to lead to the model to predict eos token to much. 
    So instead of actually using the pad token that was set to the eos token, we instead mask out all excesive eos tokens that act as pads 
    and leave the first eos token at the end to be predicted -- since that is the only one that semantically means end of sequence 
    and therby by not training on random eos at the end by masking it not unncesserily shift/amplify the distribution of eos. 
    
    ref: https://discuss.huggingface.co/t/why-does-the-falcon-qlora-tutorial-code-use-eos-token-as-pad-token/45954/13?u=brando 
    ref: https://chat.openai.com/share/02d16770-a1f3-4bf4-8fc2-464286daa8a1
    ref: https://claude.ai/chat/80565d1f-ece3-4fad-87df-364ce57aec15 on when to call .clone()
    """"""
    # we are training full context length for llama so remove code bellow, if it tries to pad hopefully it throws an error
    # -- Ensure tokenizer has a padding token
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    # -- Extract sequences
    # sequences: list[str] = [example.get(""text"", """") or """" for example in data]
    sequences: list[str] = []
    for idx, example in enumerate(data):
        # Retrieve the value for ""text"" from the dictionary or default to an empty string if not present or falsy. ref: https://chat.openai.com/share/bead51fe-2acf-4f05-b8f7-b849134bbfd4
        text: str = example.get(""text"", """") or """"
        sequences.append(text)
    # -- Tokenize the sequences
    tokenized_data = tokenizer(sequences, padding=""max_length"", max_length=max_length, truncation=True, return_tensors=""pt"")
    tokenized_data[""labels""] = tokenized_data[""input_ids""].clone()  # labels is hardcoded in HF so put it!
    # -- Set the mask value for the first eos_token in each sequence to 1 and remaining to -100
    eos_token_id = tokenizer.eos_token_id
    for idx, input_ids in enumerate(tokenized_data[""input_ids""]):
        # Find all occurrences of eos_token
        eos_positions = (input_ids == eos_token_id).nonzero(as_tuple=True)[0]
        if eos_positions.nelement() > 0:  # Check if eos_token is present
            first_eos_position = eos_positions[0]
            tokenized_data[""attention_mask""][idx, first_eos_position] = 1  # Set the mask value to 1
            
            # Assert that the label for the first occurrence of eos_token is eos_token_id
            assert tokenized_data[""labels""][idx, first_eos_position] == eos_token_id, ""The label for the first eos_token is incorrect!""
            
            # For all subsequent occurrences of eos_token, set their labels to -100
            for subsequent_eos_position in eos_positions[1:]:
                tokenized_data[""labels""][idx, subsequent_eos_position] = -100
                assert tokenized_data[""labels""][idx, subsequent_eos_position] == -100, ""The label for the subsequent_eos_position incorrect! Should be -100.""
    return tokenized_data

reference: Why does the falcon QLoRA tutorial code use eos_token as pad_token?
",huggingface-transformers
blip2 type mismatch exception,"I'm trying to create an image captioning model using hugging face blip2 model on colab. My code was working fine till last week (Nov 8) but it gives me an exception now.
To install packages I use the following command:
!pip install -q git+https://github.com/huggingface/peft.git transformers bitsandbytes datasets

To load blip2 processor and model I use the following code:
model_name = ""Salesforce/blip2-opt-2.7b""
processor = AutoProcessor.from_pretrained(model_name)
model = Blip2ForConditionalGeneration.from_pretrained(model_name,device_map=""auto"",load_in_8bit=False)

I use the following code to generate captions:
def generate_caption(processor, model, image_path):
  image = PILImage.open(image_path).convert(""RGB"")
  print(""image shape:"" + image.size)

  device = ""cuda"" if torch.cuda.is_available() else ""cpu""

  # Preprocess the image
  inputs = processor(images=image, return_tensors=""pt"").to(device)

  print(""Input shape:"", inputs['pixel_values'].shape)

  print(""Device:"", device) # Additional debugging

  for key, value in inputs.items():
    print(f""Key: {key}, Shape: {value.shape}"")

  # Generate caption
  with torch.no_grad():
      generated_ids = model.generate(**inputs)
      caption = processor.decode(generated_ids[0], skip_special_tokens=True)

  return caption

here is the code that uses this method to generate captions:
  image_path = ""my_image_path.jpg""
  caption = generate_caption(processor, model, image_path)
  print(f""{image_path}: {caption}""

finally, this is the outputs and errors of running the code above:
image shape:  (320, 240)
Input shape: torch.Size([1, 3, 224, 224])
Device: cuda
Key: pixel_values, Shape: torch.Size([1, 3, 224, 224])
---------------------------------------------------------------------------   
.
.
.
/usr/local/lib/python3.10/dist-packages/transformers/models/blip_2/modeling_blip_2.py in generate(self, pixel_values, input_ids, attention_mask, interpolate_pos_encoding, **generate_kwargs)
   2314         if getattr(self.config, ""image_token_index"", None) is not None:
   2315             special_image_mask = (input_ids == self.config.image_token_index).unsqueeze(-1).expand_as(inputs_embeds)
-> 2316             inputs_embeds[special_image_mask] = language_model_inputs.flatten()
   2317         else:
   2318             logger.warning_once(

RuntimeError: shape mismatch: value tensor of shape [81920] cannot be broadcast to indexing result of shape [0]

I have searched the internet and used various AI models for help but to no avail. My guess is that this is a package update problem since my code had no problem last week. (I tried to restore my code to Nov 8 version but it throws an exception.) Moreover, I don't understand how 81920 is calculated in the error message.
","I had the same issue. You need to add a prompt in the processor:
prompt = "" ""

inputs = processor(images=image, text=prompt, return_tensors=""pt"").to(device=""cuda"", dtype=torch.float16)

Hope it helps.
",huggingface-transformers
early stopping in bert trainer instances,"I am fine-tuning a BERT model for a multiclass classification task. My problem is that I don't know how to add ""early stopping"" to those Trainer instances. Any ideas?
","There are a couple of modifications you need to perform, prior to correctly using the EarlyStoppingCallback().
from transformers import EarlyStoppingCallback, IntervalStrategy
...
...
# Defining the TrainingArguments() arguments
args = TrainingArguments(
   output_dir = ""training_with_callbacks"",
   evaluation_strategy = IntervalStrategy.STEPS, # ""steps""
   eval_steps = 50, # Evaluation and Save happens every 50 steps
   save_total_limit = 5, # Only last 5 models are saved. Older ones are deleted.
   learning_rate=2e-5,
   per_device_train_batch_size=batch_size,
   per_device_eval_batch_size=batch_size,
   num_train_epochs=5,
   weight_decay=0.01,
   push_to_hub=False,
   metric_for_best_model = 'f1',
   load_best_model_at_end=True)

You need to:

Use load_best_model_at_end = True (EarlyStoppingCallback() requires this to be True).
evaluation_strategy = 'steps' or IntervalStrategy.STEPS instead of 'epoch'.
eval_steps = 50 (evaluate the metrics after N steps).
metric_for_best_model = 'f1'

In your Trainer():
trainer = Trainer(
    model,
    args,
    ...
    compute_metrics=compute_metrics,
    callbacks = [EarlyStoppingCallback(early_stopping_patience=3)]
)

Of course, when you use compute_metrics(), for example it can be a function like:
def compute_metrics(p):    
    pred, labels = p
    pred = np.argmax(pred, axis=1)
    accuracy = accuracy_score(y_true=labels, y_pred=pred)
    recall = recall_score(y_true=labels, y_pred=pred)
    precision = precision_score(y_true=labels, y_pred=pred)
    f1 = f1_score(y_true=labels, y_pred=pred)    
return {""accuracy"": accuracy, ""precision"": precision, ""recall"": recall, ""f1"": f1}

The return of the compute_metrics() should be a dictionary and you can access whatever metric you want/compute inside the function and return.
Note: In newer transformers version, the usage of Enum IntervalStrategy.steps is recommended (see TrainingArguments()) instead of plain steps string, the latter being soon subject to deprecation.
",huggingface-transformers
methods to reduce a tensor embedding to xyz coordinates,"I have a model from hugging face and would like to use it for performing word comparisons. At first I thought of performing a series of similarity calculations across words of interest but quickly I found that this problem would exponentially grow as the number of words expanded as well.
A solution I thought about is plotting a skip gram where all words result on a 2 dimensional plane and then can simply perform clustering on the coordinates to find similar words. The problem here is that this requires a bert model and a low embedding layer that can be mapped.
As I have a pretrained model, I don't know if I can create a skip gram with from it. I was hoping to calculate the embedding and through the use of a transformation, convert the embedding into coordinates that I can plot myself. I though do not know if this is possible or reasonable
I tried to do it though with the code below
from sklearn.manifold import TSNE
from transformers import AutoModel, AutoTokenizer

# target word
word = [""Slartibartfast""]

# model setup
model = 'Alibaba-NLP/gte-multilingual-base'
tokenizer = AutoTokenizer.from_pretrained(model)
auto_model = AutoModel.from_pretrained(model, trust_remote_code=True)

# embbed and calculate
batch_dict = self.tokenizer(text_list, max_length=8192, padding=True, truncation=True, return_tensors='pt')
result = auto_model(**batch_dict)
embeddings = outputs.last_hidden_state[:, 0][:768]

# transform to coordinates
clayer = TSNE(n_components=3, learning_rate='auto', init='random', perplexity=50)
embedding_numpy = embeddings.detach().numpy()
clayer.fit_transform(embedding_numpy)  # crashes here saying perplexity must be less than n_samples

","After more through reading, it was brough to my attention that it would be impossible to use TSNE in the manner which I was hoping as the dimensions generated by TSNE is only representative of the training data. Further fitting with new data or transformation of data not within the training set would result in outputs that are not on a similar range and thus noncomparable.
I found a replacement to TSNE which is called umap. umap is also for dimension reduction but it can be fitted multiple times and data can be transformed along the same range.
I will explore umap and see if it will work for what I need.
",huggingface-transformers
error quotbus errorquot running the simplest example on hugging face transformers pipeline macos m1,"I'm trying to follow the quick tour example here: https://huggingface.co/docs/transformers/quicktour
and i'm getting a ""bus error"".
My env is:

MacOS Sonoma 14.7, Apple M1 Max chip
Python 3.11.5
pip install transformers datasets evaluate accelerate
pip install tf-keras

Running this code:
from transformers import pipeline

classifier = pipeline(""sentiment-analysis"")
result = classifier(""We are very happy to show you the 🤗 Transformers library."")
print(result)

And getting this result:

No model was supplied, defaulted to
distilbert/distilbert-base-uncased-finetuned-sst-2-english and
revision 714eb0f
(https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).
Using a pipeline without specifying a model name and revision in
production is not recommended. [1]
92883 bus error  python main.py

Any ideas?
Thanks for your help.
","using device=0 (the first GPU) solved this:
classifier = pipeline(""sentiment-analysis"", device=0)
",huggingface-transformers
huggingface model loaded from the disk generates gibberish,"I trained a LongT5 model using Huggingface's tooling.
When I use the trained model directly after training the inference works as expected, I get good quality output, as expected from the training metrics. However if I save the model and load it from the disk, the output is gibberish. I can't figure out why.
Code producing good output:
text = dataset['test'][0]['from']
inputs = tokenizer(text, return_tensors=""pt"").input_ids
inputs = inputs.to('cuda:0')

model.eval()

with torch.no_grad():
    model.to('cuda:0')
    model.generation_config = generation_config
    outputs = model.generate(inputs)

translation = tokenizer.decode(outputs[0], skip_special_tokens=True)

# Prints correct output
print(translation)

How I save the model:
trainer.save_model(os.path.join(model_output_dir, ""final""))
tokenizer.save_pretrained(os.path.join(model_output_dir, ""final""))

How I load the model:
model = LongT5ForConditionalGeneration.from_pretrained(os.path.join(model_output_dir, ""final""))
model.to('cuda:0')
model.generation_config = generation_config

outputs = model.generate(inputs)

translation = tokenizer.decode(outputs[0], skip_special_tokens=True)

# Prints random garbage, like:
# pamper verre195188 albums188 albums188188 albums188 albums188 albums188 albums; unterschiedlich188 albums188 albums188 albums ...

print(translation)

In both cases, the tokenizer is the instance that already exists in memory during the training, but it doesn't make a difference whether I load it from the disk or not -- same result.
The generation_config variable looks like this and it's also set in the training arguments:
generation_config = GenerationConfig.from_model_config(model.config)
generation_config._from_model_config = False
generation_config.max_new_tokens = 512

It makes no difference whether it's set in the inference code or not, I still get gibberish.
","So, it turns out some strange bug in the current stable version of safetensors. It doesn't save the encoder.embed_tokens.weight and decoder.embed_tokens.weight state, so when the model is loaded again, these layers are initialized with random numbers.
There are two workarounds:

Use the latest version of safetensors where this seems to be fixed:

!pip install -U git+https://github.com/huggingface/safetensors.git


Don't use safetensors to save your model at all. You can set save_safetensors=False in the training arguments, so that HF will use pickle to save your model instead of safetensors.

",huggingface-transformers
how to apply a pretrained transformer model from huggingface,"I am interested in using pre-trained models from Hugging Face for named entity recognition (NER) tasks without further training or testing of the model.
On the model page of Hugging Face, the only information for reusing the model are as follows:
from transformers import AutoTokenizer, AutoModel
tokenizer = AutoTokenizer.from_pretrained(""emilyalsentzer/Bio_ClinicalBERT"")
model = AutoModel.from_pretrained(""emilyalsentzer/Bio_ClinicalBERT"")

I tried the following code, but I am getting a tensor output instead of class labels for each named entity.
from transformers import AutoTokenizer, AutoModel
tokenizer = AutoTokenizer.from_pretrained(""emilyalsentzer/Bio_ClinicalBERT"")
model = AutoModel.from_pretrained(""emilyalsentzer/Bio_ClinicalBERT"")

text = ""my text for named entity recognition here.""

input_ids = torch.tensor(tokenizer.encode(text, padding=True, truncation=True,max_length=50, add_special_tokens = True)).unsqueeze(0)

with torch.no_grad():
  output = model(input_ids, output_attentions=True)

Any suggestions on how to apply the model on a text for NER?
","In transformers NER is done with the TokenClassificationPipeLine:
from transformers import AutoTokenizer, pipeline,  AutoModelForTokenClassification
tokenizer = AutoTokenizer.from_pretrained(""emilyalsentzer/Bio_ClinicalBERT"")
model = AutoModelForTokenClassification.from_pretrained(""emilyalsentzer/Bio_ClinicalBERT"")
nerpipeline = pipeline('ner', model=model, tokenizer=tokenizer)
text = ""my text for named entity recognition here.""
nerpipeline(text)

Output:
[{'word': 'my',
  'score': 0.5209763050079346,
  'entity': 'LABEL_0',
  'index': 1,
  'start': 0,
  'end': 2},
 {'word': 'text',
  'score': 0.5161970257759094,
  'entity': 'LABEL_0',
  'index': 2,
  'start': 3,
  'end': 7},
 {'word': 'for',
  'score': 0.5297629237174988,
  'entity': 'LABEL_1',
  'index': 3,
  'start': 8,
  'end': 11},
 {'word': 'named',
  'score': 0.5258920788764954,
  'entity': 'LABEL_1',
  'index': 4,
  'start': 12,
  'end': 17},
 {'word': 'entity',
  'score': 0.5415489673614502,
  'entity': 'LABEL_1',
  'index': 5,
  'start': 18,
  'end': 24},
 {'word': 'recognition',
  'score': 0.5396601557731628,
  'entity': 'LABEL_1',
  'index': 6,
  'start': 25,
  'end': 36},
 {'word': 'here',
  'score': 0.5165827870368958,
  'entity': 'LABEL_0',
  'index': 7,
  'start': 37,
  'end': 41},
 {'word': '.',
  'score': 0.5266348123550415,
  'entity': 'LABEL_0',
  'index': 8,
  'start': 41,
  'end': 42}]

Please note that you need to use AutoModelForTokenClassification instead of AutoModel and that not all models have a trained head for token classification, i.e. you will get random weights for the token classification head :)
",huggingface-transformers
pytorch model object has no attribute 39predict39 bert,"I had train a BertClassifier model using pytorch. After creating my best.pt I would like to make in production my model and using it to predict and classifier starting from a sample, so I resume them from the checkpoint. Otherwise after put it in evaluation and freeze model, I use .predict to make in work on my sample but I'm encountering this Attribute Error. I had also inizialize it before calling the checkpoint. When I am wrong? Thank you for your help!
def save_ckp(state, is_best, checkpoint_path, best_model_path):
    """"""
    function created to save checkpoint, the latest one and the best one. 
    This creates flexibility: either you are interested in the state of the latest checkpoint or the best checkpoint.
    state: checkpoint we want to save
    is_best: is this the best checkpoint; min validation loss
    checkpoint_path: path to save checkpoint
    best_model_path: path to save best model
    """"""
    f_path = checkpoint_path
    # save checkpoint data to the path given, checkpoint_path
    torch.save(state, f_path)
    # if it is a best model, min validation loss
    if is_best:
        best_fpath = best_model_path
        # copy that checkpoint file to best path given, best_model_path
        shutil.copyfile(f_path, best_fpath)

def load_ckp(checkpoint_fpath, model, optimizer):
    """"""
    checkpoint_path: path to save checkpoint
    model: model that we want to load checkpoint parameters into       
    optimizer: optimizer we defined in previous training
    """"""
    # load check point
    checkpoint = torch.load(checkpoint_fpath)
    # initialize state_dict from checkpoint to model
    model.load_state_dict(checkpoint['state_dict'])
    # initialize optimizer from checkpoint to optimizer
    optimizer.load_state_dict(checkpoint['optimizer'])
    # initialize valid_loss_min from checkpoint to valid_loss_min
    valid_loss_min = checkpoint['valid_loss_min']
    # return model, optimizer, epoch value, min validation loss 
    return model, optimizer, checkpoint['epoch'], valid_loss_min.item()

#Create the BertClassfier class
class BertClassifier(nn.Module):
    """"""Bert Model for Classification Tasks.""""""
    def __init__(self, freeze_bert=True):
        """"""
         @param    bert: a BertModel object
         @param    classifier: a torch.nn.Module classifier
         @param    freeze_bert (bool): Set `False` to fine-tune the BERT model
        """"""
        super(BertClassifier, self).__init__()
        
        .......
        
    def forward(self, input_ids, attention_mask):
        ''' Feed input to BERT and the classifier to compute logits.
         @param    input_ids (torch.Tensor): an input tensor with shape (batch_size,
                       max_length)
         @param    attention_mask (torch.Tensor): a tensor that hold attention mask
                       information with shape (batch_size, max_length)
         @return   logits (torch.Tensor): an output tensor with shape (batch_size,
                       num_labels) '''
         # Feed input to BERT
        outputs = self.bert(input_ids=input_ids,
                             attention_mask=attention_mask)
         
         # Extract the last hidden state of the token `[CLS]` for classification task
        last_hidden_state_cls = outputs[0][:, 0, :]
 
         # Feed input to classifier to compute logits
        logits = self.classifier(last_hidden_state_cls)
 
        return logits

def initialize_model(epochs):
    """""" Initialize the Bert Classifier, the optimizer and the learning rate scheduler.""""""
    # Instantiate Bert Classifier
    bert_classifier = BertClassifier(freeze_bert=False)

    # Tell PyTorch to run the model on GPU
    bert_classifier = bert_classifier.to(device)

    # Create the optimizer
    optimizer = AdamW(bert_classifier.parameters(),
                      lr=lr,    # Default learning rate
                      eps=1e-8    # Default epsilon value
                      )

    # Total number of training steps
    total_steps = len(train_dataloader) * epochs

    # Set up the learning rate scheduler
    scheduler = get_linear_schedule_with_warmup(optimizer,
                                                num_warmup_steps=0, # Default value
                                                num_training_steps=total_steps)
    return bert_classifier, optimizer, scheduler
    

def train(model, train_dataloader, val_dataloader, valid_loss_min_input, checkpoint_path, best_model_path, start_epochs, epochs, evaluation=True):

    """"""Train the BertClassifier model.""""""
    # Start training loop
    logging.info(""--Start training...\n"")

    # Initialize tracker for minimum validation loss
    valid_loss_min = valid_loss_min_input 


    for epoch_i in range(start_epochs, epochs):
        # =======================================
        #               Training
        # =======================================
        # Print the header of the result table
        logging.info((f""{'Epoch':^7} | {'Batch':^7} | {'Train Loss':^12} | {'Val Loss':^10} | {'Val Acc':^9} | {'Elapsed':^9}""))

        # Measure the elapsed time of each epoch
        t0_epoch, t0_batch = time.time(), time.time()

        # Reset tracking variables at the beginning of each epoch
        total_loss, batch_loss, batch_counts = 0, 0, 0

        # Put the model into the training mode
        model.train()

        # For each batch of training data...
        for step, batch in enumerate(train_dataloader):
            batch_counts +=1
            # Load batch to GPU
            b_input_ids, b_attn_mask, b_labels = tuple(t.to(device) for t in batch)

            # Zero out any previously calculated gradients
            model.zero_grad()

            # Perform a forward pass. This will return logits.
            logits = model(b_input_ids, b_attn_mask)

            # Compute loss and accumulate the loss values
            loss = loss_fn(logits, b_labels)
            batch_loss += loss.item()
            total_loss += loss.item()

            # Perform a backward pass to calculate gradients
            loss.backward()

            # Clip the norm of the gradients to 1.0 to prevent ""exploding gradients""
            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)

            # Update parameters and the learning rate
            optimizer.step()
            scheduler.step()

            # Print the loss values and time elapsed for every 20 batches
            if (step % 500 == 0 and step != 0) or (step == len(train_dataloader) - 1):
                # Calculate time elapsed for 20 batches
                time_elapsed = time.time() - t0_batch

                # Print training results
                logging.info(f""{epoch_i + 1:^7} | {step:^7} | {batch_loss / batch_counts:^12.6f} | {'-':^10} | {'-':^9} | {time_elapsed:^9.2f}"")

                # Reset batch tracking variables
                batch_loss, batch_counts = 0, 0
                t0_batch = time.time()

        # Calculate the average loss over the entire training data
        avg_train_loss = total_loss / len(train_dataloader)

        logging.info(""-""*70)
        # =======================================
        #               Evaluation
        # =======================================
        if evaluation == True:
            # After the completion of each training epoch, measure the model's performance
            # on our validation set.
            val_loss, val_accuracy = evaluate(model, val_dataloader)

            # Print performance over the entire training data
            time_elapsed = time.time() - t0_epoch
            
            logging.info(f""{epoch_i + 1:^7} | {'-':^7} | {avg_train_loss:^12.6f} | {val_loss:^10.6f} | {val_accuracy:^10.6f} | {time_elapsed:^9.2f}"")

            logging.info(""-""*70)
        logging.info(""\n"")


         # create checkpoint variable and add important data
        checkpoint = {
            'epoch': epoch_i + 1,
            'valid_loss_min': val_loss,
            'state_dict': model.state_dict(),
            'optimizer': optimizer.state_dict(),
        }
        
        # save checkpoint
        save_ckp(checkpoint, False, checkpoint_path, best_model_path)
        
        ## TODO: save the model if validation loss has decreased
        if val_loss <= valid_loss_min:
            print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(valid_loss_min,val_loss))
            # save checkpoint as best model
            save_ckp(checkpoint, True, checkpoint_path, best_model_path)
            valid_loss_min = val_loss
    
    logging.info(""-----------------Training complete--------------------------"")

def evaluate(model, val_dataloader):
    """"""After the completion of each training epoch, measure the model's performance on our validation set.""""""
    # Put the model into the evaluation mode. The dropout layers are disabled during the test time.
    model.eval()

    # Tracking variables
    val_accuracy = []
    val_loss = []

    # For each batch in our validation set...
    for batch in val_dataloader:
        # Load batch to GPU
        b_input_ids, b_attn_mask, b_labels = tuple(t.to(device) for t in batch)

        # Compute logits
        with torch.no_grad():
            logits = model(b_input_ids, b_attn_mask)

        # Compute loss
        loss = loss_fn(logits, b_labels)
        val_loss.append(loss.item())

        # Get the predictions
        preds = torch.argmax(logits, dim=1).flatten()

        # Calculate the accuracy rate
        accuracy = (preds == b_labels).cpu().numpy().mean() * 100
        val_accuracy.append(accuracy)

    # Compute the average accuracy and loss over the validation set.
    val_loss = np.mean(val_loss)
    val_accuracy = np.mean(val_accuracy)

    return val_loss, val_accuracy

bert_classifier, optimizer, scheduler = initialize_model(epochs=n_epochs)
train(model = bert_classifier ......)



bert_classifier, optimizer, scheduler = initialize_model(epochs=n_epochs)
model, optimizer, start_epoch, valid_loss_min = load_ckp(r""./best_model/best_model.pt"", bert_classifier, optimizer)

model.eval()
model.freeze()

sample = {
  ""seq"": ""ABCDE"",}

predictions = model.predict(sample)

AttributeError: 'BertClassifier' object has no attribute 'predict'

","Generally, people wrote the prediction function for you.
If not, you need to handle the low level stuff.
After this line, you loaded the trained parameters.
model, optimizer, start_epoch, valid_loss_min = load_ckp(r""./best_model/best_model.pt"", bert_classifier, optimizer)

After that, you need to do the model.forward(intput_seq,this_attention_mask_maybe_null).
You can see the forward method here is the: def forward(self, input_ids, attention_mask) in the model.
",huggingface-transformers
target modules for applying peft  lora on different models,"I am looking at a few different examples of using PEFT on different models. The LoraConfig object contains a target_modules array. In some examples, the target modules are [""query_key_value""], sometimes it is [""q"", ""v""], sometimes something else.
I don't quite understand where the values of the target modules come from. Where in the model page should I look to know what the LoRA adaptable modules are?
One example (for the model Falcon 7B):
peft_config = LoraConfig(
    lora_alpha=lora_alpha,
    lora_dropout=lora_dropout,
    r=lora_r,
    bias=""none"",
    task_type=""CAUSAL_LM"",
    target_modules=[
        ""query_key_value"",
        ""dense"",
        ""dense_h_to_4h"",
        ""dense_4h_to_h"",
    ]

Another example (for the model Opt-6.7B):
config = LoraConfig(
    r=16,
    lora_alpha=32,
    target_modules=[""q_proj"", ""v_proj""],
    lora_dropout=0.05,
    bias=""none"",
    task_type=""CAUSAL_LM""
)

Yet another (for the model Flan-T5-xxl):
lora_config = LoraConfig(
 r=16,
 lora_alpha=32,
 target_modules=[""q"", ""v""],
 lora_dropout=0.05,
 bias=""none"",
 task_type=TaskType.SEQ_2_SEQ_LM
)

","Let's say that you load some model of your choice:
model = AutoModelForCausalLM.from_pretrained(""some-model-checkpoint"")
Then you can see available modules by printing out this model:
print(model)
You will get something like this (SalesForce/CodeGen25):
LlamaForCausalLM(
  (model): LlamaModel(
    (embed_tokens): Embedding(51200, 4096, padding_idx=0)
    (layers): ModuleList(
      (0-31): 32 x LlamaDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (act_fn): SiLUActivation()
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
  )
  (lm_head): Linear(in_features=4096, out_features=51200, bias=False)
)

In my case, you can find the LLamaAttention module that contains q_proj, k_proj, v_proj, and o_proj. And this are some modules available for LoRA.
I suggest you reading more about which modules to use in LoRA paper.
",huggingface-transformers
how to change huggingface transformers default cache directory,"The default cache directory is lack of disk capacity, I need change the configure of the default cache directory.
","Update 2024

FutureWarning: Using TRANSFORMERS_CACHE is deprecated and will be
removed in v5 of Transformers. Use HF_HOME instead.

Python example:
import os
os.environ['HF_HOME'] = '/blabla/cache/'

bash example:
export HF_HOME=/blabla/cache/

windows example:
set HF_HOME=E:\huggingface_cache

Google Colab example (export via os works fine but not the bash variant. An alternative are the magic commands):
%env HF_HOME=/blabla/cache/

Old Answer:
You can specify the cache directory whenever you load a model with .from_pretrained by setting the parameter cache_dir. You can define a default location by exporting an environment variable TRANSFORMERS_CACHE every time before you use (i.e. before importing it!) the library).
Python example:
import os
os.environ['TRANSFORMERS_CACHE'] = '/blabla/cache/'

bash example:
export TRANSFORMERS_CACHE=/blabla/cache/

Google Colab example (export via os works fine but not the bash variant. An alternative are the magic commands):
%env TRANSFORMERS_CACHE=/blabla/cache/

",huggingface-transformers
modulenotfounderror no module named 39transformers39,"This is my first post and I am new to coding, so please let me know if you need more information. I have been running some AI to generate artwork and it has been working, but when I reloaded it the python script won't work and it is now saying ""No module named 'transformers'"". Can anyone help me out? It was when I upgraded to Google Colab Pro that I started to encounter issues although I am not sure why that would make a difference.
ModuleNotFoundError

","Probably it is because you have not installed in your (new, since you've upgraded to colabs pro) session the library transformers. Try to run as first cell the following: !pip install transformers (the ""!"" at the beginning of the instruction is needed to go into ""terminal mode"" ). This will download the transformers package into the session's environment.
",huggingface-transformers
how to reconstruct text entities with hugging face39s transformers pipelines without iob tags,"I've been looking to use Hugging Face's Pipelines for NER (named entity recognition). However, it is returning the entity labels in inside-outside-beginning (IOB) format but without the IOB labels. So I'm not able to map the output of the pipeline back to my original text. Moreover, the outputs are masked in BERT tokenization format (the default model is BERT-large).
For example: 
from transformers import pipeline
nlp_bert_lg = pipeline('ner')
print(nlp_bert_lg('Hugging Face is a French company based in New York.'))

The output is:
[{'word': 'Hu', 'score': 0.9968873858451843, 'entity': 'I-ORG'},
{'word': '##gging', 'score': 0.9329522848129272, 'entity': 'I-ORG'},
{'word': 'Face', 'score': 0.9781811237335205, 'entity': 'I-ORG'},
{'word': 'French', 'score': 0.9981815814971924, 'entity': 'I-MISC'},
{'word': 'New', 'score': 0.9987512826919556, 'entity': 'I-LOC'},
{'word': 'York', 'score': 0.9976728558540344, 'entity': 'I-LOC'}]

As you can see, New York is broken up into two tags.
How can I map Hugging Face's NER Pipeline back to my original text?
Transformers version: 2.7
","EDIT 12/2023:
As pointed out, the grouped_entities parameter has been deprecated. The correct way is to use the aggregation_strategy parameters as pointed in the source code .
For instance:
text = 'Hugging Face is a French company based in New York.'
tagger = pipeline(task='ner', aggregation_strategy='simple')
named_ents = tagger(text)
pd.DataFrame(named_ents)

Gives the following output
[
   {
      ""entity_group"":""ORG"",
      ""score"":0.96934015,
      ""word"":""Hugging Face"",
      ""start"":0,
      ""end"":12
   },
   {
      ""entity_group"":""MISC"",
      ""score"":0.9981816,
      ""word"":""French"",
      ""start"":18,
      ""end"":24
   },
   {
      ""entity_group"":""LOC"",
      ""score"":0.9982121,
      ""word"":""New York"",
      ""start"":42,
      ""end"":50
   }
]

ORIGINAL ANSWER:
The 17th of May, a new pull request https://github.com/huggingface/transformers/pull/3957 with what you are asking for has been merged, therefore now our life is way easier, you can you it in the pipeline like
ner = pipeline('ner', grouped_entities=True)

and your output will be as expected. At the moment you have to install from the master branch since there is no new release yet. You can do it via
pip install git+git://github.com/huggingface/transformers.git@48c3a70b4eaedab1dd9ad49990cfaa4d6cb8f6a0

",huggingface-transformers
question about data_collator throwing a key error in hugging face,"I am trying to use data_collator function in hugging face using this code:
datasets = dataset.train_test_split(test_size=0.1)
train_dataset = datasets[""train""]
val_dataset = datasets[""test""]

print(type(train_dataset))

def data_collator(data):
# Initialize lists to store pixel values and input ids
   pixel_values_list = []
   input_ids_list = []

# Iterate over each sample in the data
   for item in data:      
      pixel_values_list.append(torch.tensor(item[""pixel_values""]))
      input_ids_list.append(torch.tensor(item[""input_ids""]))


return {
    ""pixel_values"": torch.stack(pixel_values_list),
    ""labels"": torch.stack(input_ids_list)
}

the train_data has 5 keys including input_ids. However, when I print(data[0]) inside the data_collator function, I only see 1 key, which is giving an error when running the trainer:
Traceback (most recent call last):
 File ""caption-code.py"", line 134, in <module>
trainer.train()
  File ""C:\Users\moham\anaconda3\envs\transformer\lib\site- 
 packages\transformers\trainer.py"", line 1321, in train
ignore_keys_for_eval=ignore_keys_for_eval,
 File ""C:\Users\moham\anaconda3\envs\transformer\lib\site- 
  packages\transformers\trainer.py"", line 1528, in _inner_training_loop
  for step, inputs in enumerate(epoch_iterator):
    File ""C:\Users\moham\anaconda3\envs\transformer\lib\site- 
  packages\torch\utils\data\dataloader.py"", line 521, in __next__
  data = self._next_data()
 File ""C:\Users\moham\anaconda3\envs\transformer\lib\site- 
  packages\torch\utils\data\dataloader.py"", line 561, in _next_data
data = self._dataset_fetcher.fetch(index)  # may raise StopIteration
 File ""C:\Users\moham\anaconda3\envs\transformer\lib\site- 
 packages\torch\utils\data\_utils\fetch.py"", line 52, in fetch
 return self.collate_fn(data)
 File ""caption-code.py"", line 102, in data_collator
 input_ids_list.append(item[""input_ids""])
  KeyError: 'input_ids'

I am using the trainer function as follows:
training_args = Seq2SeqTrainingArguments(
predict_with_generate=True,
evaluation_strategy=""epoch"",
per_device_train_batch_size=4,
per_device_eval_batch_size=4,
output_dir=""C:/Users/moham/Desktop/Euler/output"",
logging_dir=""./logs"",
logging_steps=10,
save_steps=10,
eval_steps=10,
warmup_steps=10,
max_steps=100,  # adjust as needed
overwrite_output_dir=True,
save_total_limit=3,
 )
trainer = Seq2SeqTrainer(
model=model,
args=training_args,
train_dataset=train_dataset,
eval_dataset=val_dataset,
data_collator=data_collator,
tokenizer=tokenizer,
compute_metrics=compute_exact_match
 )
trainer.train()

","The actual issue is in your Seq2SeqTrainingArguments which is leading the error in your data_collator().
Reason: The .trainer() is by default removing any unknown columns (not present in the model's forward method) from your data when you are providing a custom data_collator(). As a result even though each sample in your train_dataset has all the keys, when you send that to data_collator(), the .trainer() automatically removes the unknown columns.
Solution: You need to include an argument in your training arguments like the following:
training_args = Seq2SeqTrainingArguments(
predict_with_generate=True,
remove_unused_columns=False, 
...)

The remove_unused_columns=False, would prevent the default behaviour and you'd get the entire data in data_collator(). This issue would be useful for further reference.
",huggingface-transformers
how to download a model from huggingface,"For example, I want to download bert-base-uncased on https://huggingface.co/models, but can't find a 'Download' link. Or is it not downloadable?
","The models are automatically cached locally when you first use it.
So, to download a model, all you have to do is run the code that is provided in the model card (I chose the corresponding model card for bert-base-uncased).
At the top right of the page you can find a button called ""Use in Transformers"", which even gives you the sample code, showing you how to use it in Python. Again, for bert-base-uncased, this gives you the following code snippet:
from transformers import AutoTokenizer, AutoModelForMaskedLM
  
tokenizer = AutoTokenizer.from_pretrained(""bert-base-uncased"")
model = AutoModelForMaskedLM.from_pretrained(""bert-base-uncased"")

When you run this code for the first time, you will see a download bar appear on screen. See this post (disclaimer: I gave one of the answers) if you want to find the actual folder where Huggingface stores their models.
",huggingface-transformers
cannot import name 39split_torch_state_dict_into_shards39 from 39huggingface_hub39,"I've been using LLAMA 2 for research for a few months now and I import as follows:
from transformers import AutoModelForCausalLM, AutoTokenizer
device = torch.device(""cuda"")
tokenizer = AutoTokenizer.from_pretrained(""meta-llama/Llama-2-7b-chat-hf"",token = ""token_key"",torch_dtype=""auto"")
model = AutoModelForCausalLM.from_pretrained(""meta-llama/Llama-2-7b-chat-hf"",token = ""token_key"", torch_dtype=""auto"", load_in_4bit=True)

It has always worked. However, today it is showing the following error:
RuntimeError: Failed to import transformers.models.llama.modeling_llama because of the following error (look up to see its traceback):
Failed to import transformers.generation.utils because of the following error (look up to see its traceback):
cannot import name 'split_torch_state_dict_into_shards' from 'huggingface_hub' (/opt/conda/lib/python3.10/site-packages/huggingface_hub/init.py)
Recreated the Hugging Face token, but it didn't work. I am using Google Colab and Kaggle Notebook.
","The error you're encountering is due to the split_torch_state_dict_into_shards function not being available in huggingface-hub version < 0.23.0.
This function is included starting from version 0.23.0.
To resolve this issue, update the huggingface-hub library to version 0.23.0 or later
Also please install accelerate:
pip install accelerate==0.31.0

here is a git link: https://github.com/run-llama/llama_index/discussions/14605
",huggingface-transformers
how to read a bert attention weight matrix,"I have extracted from the last layer and the last attention head of my BERT model the attention score/weights matrix. However I am not too sure how to read them. The matrix is the following one. I tried to find some more information in the literature but it was not successful. Any insights? Since the matrix is not symmetric and each rows sums to 1, I am confused. Thanks a lot !

  tokenizer = BertTokenizer.from_pretrained('Rostlab/prot_bert')
  inputs = tokenizer(input_text, return_tensors='pt') 
  attention_mask=inputs['attention_mask']
  outputs = model(inputs['input_ids'],attention_mask) #len 30 as the model layers #outpus.attentions
  attention = outputs[-1]
attention = attention[-1] #last layer attention
layer_attention = layer_attention[-1] #last head attention
#... code to read it as a matrix with token labels

","The attention matrix is asymmetric because query and key matrices differ.
At its core (leaving normalization constants and the multi-head trick aside) (dot-product) self-attention is computed as follows:

Compute key-query affinities (e_ij): given  (T being the sequence length, q_i and k_j being query and key vectors)


Compute attention weights from affinities (alpha_ij):



As you can see, you get the normalization of the affinities by summing over all keys given a query; said differently, in the denominator you're summing affinities by row (thus, probabilities sum to 1 over rows).
The way you should read the attention matrix is the following: row tokens (queries) attend to column tokens (keys) and the matrix weights represent a way to probabilistically measure where attention is directed to when querying over keys (i.e. to which key - and so to which token of the sentence - each query (token) mainly focuses to). Such interaction is unidirectional (you might look at each query as looking for information somewhere in the keys, the opposite interaction being irrelevant). I found the interpretation of the attention matrix as a directed graph within this blogpost very effective.
Eventually, I'd also suggest the first BertViz medium post which distinguishes different attention patterns and according to which your example would fall in the case where attention is mostly directed to the delimiter token [CLS].
",huggingface-transformers
trainer huggingface  runtimeerror cannot pin 39torchcudafloattensor39 only dense cpu tensors can be pinned,"I recently got the following error:
RuntimeError: cannot pin 'torch.cuda.FloatTensor' only dense CPU tensors can be pinned
when doing LoRA on a small LLM.
I saw on a discord someone saying:

The issue likely stems from the fact that you are manually placing
your inputs on the GPU (with to(model.device)), but the Trainer
expects data to be on the CPU and will handle the transfer to the GPU
internally.

I can't find anything of the sort written in the Trainer documentation of huggingface https://huggingface.co/docs/transformers/en/main_classes/trainer.
Is it true? If not, how can I get rid of that error?
MRE:
import torch
from torch.utils.data import Dataset
from transformers import AutoModelForCausalLM, AutoTokenizer
from transformers import TrainingArguments
from transformers import Trainer
from peft import LoraConfig, get_peft_model

model_name = ""croissantllm/CroissantLLMBase""
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16, device_map=""auto"")

texts = [
    ""The first sentence for fine-tuning. </s>"",
    ""The second sentence for fine-tuning. </s>""
]

inputs = [tokenizer(text, return_tensors=""pt"").to(model.device) for text in texts]

lora_config = LoraConfig(
    r=8,
    lora_alpha=16,
    lora_dropout=0.1,
    target_modules=[""q_proj"", ""v_proj""],
)

model = get_peft_model(model, lora_config)

class CustomDataset(Dataset):
    def __init__(self, input_list):
        self.input_list = input_list

    def __len__(self):
        return len(self.input_list)

    def __getitem__(self, idx):
        input_ids = self.input_list[idx]['input_ids'].squeeze()
        labels = input_ids.clone()
        return {""input_ids"": input_ids, ""labels"": labels}

train_dataset = CustomDataset(inputs)

training_args = TrainingArguments(
    output_dir=""./lora_croissantllm"",
    per_device_train_batch_size=1,
    num_train_epochs=1,
    save_steps=10,
    save_total_limit=2,
    logging_dir=""./logs"",
    logging_steps=10,
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
)

trainer.train()

The issue is fairly easy to reproduce directly on colab (run %pip install --upgrade torch transformers peft in the first cell).
","Since pinning memory is only available on CPU and not GPU, when running on GPU on Colab, you can just disable it by setting dataloader_pin_memory to False for TrainingArguments
training_args = TrainingArguments(
    output_dir=""./lora_croissantllm"",
    dataloader_pin_memory=False,
    per_device_train_batch_size=1,
    num_train_epochs=1,
    save_steps=10,
    save_total_limit=2,
    logging_dir=""./logs"",
    logging_steps=10,
)

",huggingface-transformers
why does llamaindex still require an openai key when using hugging face local embedding model,"I am creating a very simple question and answer app based on documents using llama-index. Previously, I had it working with OpenAI. Now I want to try using no external APIs so I'm trying the Hugging Face example in this link.
It says in the example in the link: ""Note that for a completely private experience, also setup a local embedding model (example here)."" I'm assuming the example given below is the example being referred to. So, naturally, I'm trying to copy the example (fuller example here).
Here is my code:
from pathlib import Path
import gradio as gr
import sys
import logging
import os

from llama_index.llms import HuggingFaceLLM
from llama_index.prompts.prompts import SimpleInputPrompt

logging.basicConfig(stream=sys.stdout, level=logging.DEBUG)
logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))


from llama_index import SimpleDirectoryReader, VectorStoreIndex, ServiceContext, load_index_from_storage, StorageContext

storage_path = ""storage/""

docs_path=""docs""

def construct_index(directory_path):
    max_input_size = 4096
    num_outputs = 512
    #max_chunk_overlap = 20
    chunk_overlap_ratio = 0.1
    chunk_size_limit = 600

    #prompt_helper = PromptHelper(max_input_size, num_outputs, chunk_overlap_ratio, chunk_size_limit=chunk_size_limit)

    system_prompt = """"""<|SYSTEM|># StableLM Tuned (Alpha version)
    - StableLM is a helpful and harmless open-source AI language model developed by StabilityAI.
    - StableLM is excited to be able to help the user, but will refuse to do anything that could be considered harmful to the user.
    - StableLM is more than just an information source, StableLM is also able to write poetry, short stories, and make jokes.
    - StableLM will refuse to participate in anything that could harm a human.
    """"""

    # This will wrap the default prompts that are internal to llama-index
    query_wrapper_prompt = SimpleInputPrompt(""<|USER|>{query_str}<|ASSISTANT|>"")


    llm = HuggingFaceLLM(
        context_window=4096,
        max_new_tokens=256,
        generate_kwargs={""temperature"": 0.7, ""do_sample"": False},
        system_prompt=system_prompt,
        query_wrapper_prompt=query_wrapper_prompt,
        tokenizer_name=""StabilityAI/stablelm-tuned-alpha-3b"",
        model_name=""StabilityAI/stablelm-tuned-alpha-3b"",
        device_map=""auto"",
        stopping_ids=[50278, 50279, 50277, 1, 0],
        tokenizer_kwargs={""max_length"": 4096},
        # uncomment this if using CUDA to reduce memory usage
        # model_kwargs={""torch_dtype"": torch.float16}
    )
    #llm=ChatOpenAI(temperature=0.7, model_name=""gpt-3.5-turbo"", max_tokens=num_outputs)
    #llm_predictor = LLMPredictor(llm=llm)
    service_context = ServiceContext.from_defaults(chunk_size=1024, llm=llm)

    documents = SimpleDirectoryReader(directory_path).load_data()

    index = VectorStoreIndex.from_documents(documents, service_context=service_context)
    #index = VectorStoreIndex(documents, llm_predictor=llm_predictor, prompt_helper=prompt_helper)

    index.storage_context.persist(persist_dir=storage_path)

    return index

def chatbot(input_text):
    index = load_index_from_storage(StorageContext.from_defaults(persist_dir=storage_path))
    #index = GPTVectorStoreIndex.load_from_disk('index.json')
    #query_engine = index.as_query_engine(response_synthesizer=response_synthesizer);
    query_engine = index.as_query_engine(streaming=True)

    response = query_engine.query(input_text)

    print(response.source_nodes)

    relevant_files=[]

    for node_with_score in response.source_nodes:
        print(node_with_score)
        print(node_with_score.node)
        print(node_with_score.node.metadata)
        print(node_with_score.node.metadata['file_name'])

        file = node_with_score.node.metadata['file_name']
        print( file )

        # Resolve the full file path for the downloading
        full_file_path = Path( docs_path, file ).resolve()

        # See if it's already in the array
        if full_file_path not in relevant_files:
            relevant_files.append( full_file_path ) # Add it

    print( relevant_files )

    return response.get_response(), relevant_files

iface = gr.Interface(fn=chatbot,
                     inputs=gr.components.Textbox(lines=7, label=""Enter your text""),
                     outputs=[
                        gr.components.Textbox(label=""Response""), 
                        gr.components.File(label=""Relevant Files"")
                        ],
                     title=""Custom-trained AI Chatbot"",
                     allow_flagging=""never"")

index = construct_index(docs_path)
iface.launch(share=False)


Regardless, the code errors out saying:
ValueError: No API key found for OpenAI.
Please set either the OPENAI_API_KEY environment variable or openai.api_key prior to initialization.
API keys can be found or created at https://platform.openai.com/account/api-keys

Am I not understanding how to set up a local model?
","Turns out I had to set the embed_model to ""local"" on the ServiceContext.
ServiceContext.from_defaults(chunk_size=1024, llm=llm, embed_model=""local"")

Also, when I was loading the vector index from disk I wasn't setting the llm predictor again which cause a secondary issue. So I decided to make the vector index a global variable. Here is my final code that works.
from pathlib import Path
import gradio as gr
import sys
import logging
import os

from llama_index.llms import HuggingFaceLLM
from llama_index.prompts.prompts import SimpleInputPrompt

logging.basicConfig(stream=sys.stdout, level=logging.DEBUG)
logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))


from llama_index import SimpleDirectoryReader, VectorStoreIndex, ServiceContext, load_index_from_storage, StorageContext

storage_path = ""storage""

docs_path=""docs""

print(storage_path)

max_input_size = 4096
num_outputs = 512
#max_chunk_overlap = 20
chunk_overlap_ratio = 0.1
chunk_size_limit = 600


system_prompt = """"""<|SYSTEM|># StableLM Tuned (Alpha version)
- StableLM is a helpful and harmless open-source AI language model developed by StabilityAI.
- StableLM is excited to be able to help the user, but will refuse to do anything that could be considered harmful to the user.
- StableLM is more than just an information source, StableLM is also able to write poetry, short stories, and make jokes.
- StableLM will refuse to participate in anything that could harm a human.
""""""

# This will wrap the default prompts that are internal to llama-index
query_wrapper_prompt = SimpleInputPrompt(""<|USER|>{query_str}<|ASSISTANT|>"")


llm = HuggingFaceLLM(
    context_window=4096,
    max_new_tokens=256,
    generate_kwargs={""temperature"": 0.7, ""do_sample"": False},
    system_prompt=system_prompt,
    query_wrapper_prompt=query_wrapper_prompt,
    tokenizer_name=""StabilityAI/stablelm-tuned-alpha-3b"",
    model_name=""StabilityAI/stablelm-tuned-alpha-3b"",
    device_map=""auto"",
    stopping_ids=[50278, 50279, 50277, 1, 0],
    tokenizer_kwargs={""max_length"": 4096},
    # uncomment this if using CUDA to reduce memory usage
    # model_kwargs={""torch_dtype"": torch.float16}
)

service_context = ServiceContext.from_defaults(chunk_size=1024, llm=llm, embed_model=""local"")

documents = SimpleDirectoryReader(docs_path).load_data()

index = VectorStoreIndex.from_documents(documents, service_context=service_context)


def chatbot(input_text):
    query_engine = index.as_query_engine()

    response = query_engine.query(input_text)

    print(response.source_nodes)

    relevant_files=[]

    for node_with_score in response.source_nodes:
        print(node_with_score)
        print(node_with_score.node)
        print(node_with_score.node.metadata)
        print(node_with_score.node.metadata['file_name'])

        file = node_with_score.node.metadata['file_name']
        print( file )

        # Resolve the full file path for the downloading
        full_file_path = Path( docs_path, file ).resolve()

        # See if it's already in the array
        if full_file_path not in relevant_files:
            relevant_files.append( full_file_path ) # Add it

    print( relevant_files )

    return response.response, relevant_files

iface = gr.Interface(fn=chatbot,
                     inputs=gr.components.Textbox(lines=7, label=""Enter your text""),
                     outputs=[
                        gr.components.Textbox(label=""Response""), 
                        gr.components.File(label=""Relevant Files"")
                        ],
                     title=""Custom-trained AI Chatbot"",
                     allow_flagging=""never"")

iface.launch(share=False)

",huggingface-transformers
how to add eos when training t5,"I'm a little puzzled where (and if) EOS tokens are being added when using Huggignface's trainer classes to train a T5 (LongT5 actually) model.
The data set contains pairs of text like this:



from
to




some text
some corresponding text


some other text
some other corresponding text



The tokenizer has been custom trained:
tokenizer = SentencePieceUnigramTokenizer()
tokenizer.train_from_iterator(iterator=iterator, vocab_size=32_128, show_progress=True, unk_token=""<unk>"")

and is loaded like this:
tokenizer = T5TokenizerFast(tokenizer_file=""data-rb-25000/tokenizer.json"",  
                            padding=True, bos_token=""<s>"", 
                            eos_token=""</s>"",unk_token=""<unk>"", 
                            pad_token=""<pad>"")

Before training, the data set is tokenized and examples that have a too high token count are filtered out, like so:
MAX_SEQUENCE_LENGTH = 16_384 / 2

def preprocess_function(examples):
    inputs = tokenizer(
        examples['from'],
        truncation=False,  # Don't truncate yet
        padding=False,     # Don't pad yet
        return_length=True,
    )
    labels = tokenizer(
        examples['to'],
        truncation=False,
        padding=False,
        return_length=True,
    )

    inputs[""input_length""] = inputs[""length""]
    inputs[""labels""] = labels[""input_ids""]
    inputs[""label_length""] = labels[""length""]

    inputs.pop(""length"", None)

    return inputs

tokenized_data = dataset.map(preprocess_function, batched=True, remove_columns=dataset[""train""].column_names)

def filter_function(example):
    return example['input_length'] <= MAX_SEQUENCE_LENGTH and example['label_length'] <= MAX_SEQUENCE_LENGTH

filtered_data = tokenized_data.filter(filter_function)

Training is done like this:
from transformers import DataCollatorForSeq2Seq

data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=""google/long-t5-tglobal-base"")

from transformers import AutoModelForSeq2SeqLM, AutoConfig

config = AutoConfig.from_pretrained(
    ""google/long-t5-tglobal-base"",
    vocab_size=len(tokenizer),
    pad_token_id=tokenizer.pad_token_id,
    eos_token_id=tokenizer.eos_token_id,
    decoder_start_token_id=tokenizer.pad_token_id,
)

model = AutoModelForSeq2SeqLM.from_config(config)

from transformers import GenerationConfig

generation_config = GenerationConfig.from_model_config(model.config)
generation_config._from_model_config = False
generation_config.max_new_tokens = 16_384

from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments

training_args = Seq2SeqTrainingArguments(
    output_dir=""rb-25000-model"",
    eval_strategy=""epoch"",
    save_strategy=""epoch"",
    learning_rate=2e-5,
    per_device_train_batch_size=1,
    per_device_eval_batch_size=1,
    gradient_accumulation_steps=16,
    gradient_checkpointing=True,
    weight_decay=0.01,
    save_total_limit=3,
    num_train_epochs=5,
    logging_steps=1,
    predict_with_generate=True,
    load_best_model_at_end=True,
    bf16=True,
)

trainer = Seq2SeqTrainer(
    model=model,
    args=training_args,
    train_dataset=filtered_data[""train""],
    eval_dataset=filtered_data[""test""],
    tokenizer=tokenizer,
    data_collator=data_collator,
    compute_metrics=compute_metrics,
    generation_config=generation_config,
)

trainer.train()

I know that the tokenizer doesn't add the EOS token:
inputs = tokenizer(['Hello world', 'Hello'], padding=True, truncation=True, max_length=100, return_tensors=""pt"")
labels = inputs[""input_ids""]

print(labels)
print(tokenizer.convert_tokens_to_ids(['<s>'])[0])
print(tokenizer.convert_tokens_to_ids(['<pad>'])[0])
print(tokenizer.convert_tokens_to_ids(['<unk>'])[0])
print(tokenizer.convert_tokens_to_ids(['</s>'])[0])

print(tokenizer.convert_ids_to_tokens([1]))

Output:
tensor([[1, 10356, 1, 5056],
        [1, 10356, 16002, 16002]])
16000
16002
0
16001
['▁']

(I don't really understand what's that strange token with index 1.
Anyway, I was wondering if the Trainer class or the DataCollator actually adds the EOS. I did not find any examples online of how and where to add EOS.
I suspect it's not there, because after training the model it doesn't stop generating until it reaches max_new_tokens (set to pretty high).
What's the best practice here? Where should I add EOS? Is there anything else about this code that should be checked or that looks weird for more experienced eyes?
","The T5 tokenizer should end sequences by EOS token by default. Pretrained T5 tokenizer on HuggingFace does that by default. In fact, I found the function that is responsible for that in the source code on line 256:
def _add_eos_if_not_present(self, token_ids: List[int]) -> List[int]:
        """"""Do not add eos again if user already added it.""""""
        if len(token_ids) > 0 and token_ids[-1] == self.eos_token_id:
            warnings.warn(
                f""This sequence already has {self.eos_token}. In future versions this behavior may lead to duplicated""
                "" eos tokens being added.""
            )
            return token_ids
        else:
            return token_ids + [self.eos_token_id]

If EOS token is not appended by default, you can add a post processor to your tokenizer using TemplateProcessing:
from tokenizers.processors import TemplateProcessing

tokenizer._tokenizer.post_processor = TemplateProcessing(
    single=""$A </s>"",
    pair=""$A </s> $B </s>"",
    special_tokens=[(""</s>"", tokenizer.eos_token_id)]
)

inputs = tokenizer(['Hello world', 'Hello'], padding=True, truncation=True, max_length=100, return_tensors=""pt"")
labels = inputs[""input_ids""]
print(labels)

This should give:
tensor([[1, 10356, 1, 5056, 16001],
        [1, 10356, 16001, 16002, 16002]])

",huggingface-transformers
problem setting up llama2 in google colab  cellrun fails when loading checkpoint shards,"I'm trying to use Llama 2 chat (via hugging face) with 7B parameters in Google Colab (Python 3.10.12). I've already obtain my access token via Meta. I simply use the code in hugging face on how to implement the model along with my access token. Here is my code:
!pip install transformers
 
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch

token = ""---Token copied from Hugging Face and pasted here---""

tokenizer = AutoTokenizer.from_pretrained(""meta-llama/Llama-2-7b-chat-hf"", token=token)
model = AutoModelForCausalLM.from_pretrained(""meta-llama/Llama-2-7b-chat-hf"", token=token)

It starts downloading the model but when it reaches Loading checkpoint shards: it just stops running and there is no error:

","The issue is with Colab instance running out of RAM. Based on your comments you are using basic Colab instance with 12.7 Gb CPU RAM.
For LLama model you'll need:

for the float32 model about 25 Gb (but you'll need both cpu RAM and same 25 gb GPU ram);
for the bfloat16 model around 13 Gb (and still not enough to fit basic Colab Cpu instance, given that you'll also need to store intermediate calculations from the model);

Check this link for the details on the required resources:
huggingface.co/NousResearch/Llama-2-7b-chat-hf/discussions/3
Also if you want only to do inference (predictions) on the model I would recommend to use it's quantized 4bit or 8bit versions. Both can be ran on CPU and don't need a lot of memory.
",huggingface-transformers
setting padding token as eos token when using datacollatorforlanguagemodeling from huggingface,"In https://huggingface.co/learn/nlp-course/chapter7/6#preparing-the-dataset, there is
from transformers import DataCollatorForLanguageModeling

tokenizer.pad_token = tokenizer.eos_token
data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)

What the tutorial is doing is using a pretrained GPT2 model and its tokenizer and trying to create a dataset for causal language modeling pretraining task.
My question with the above line is that padding token is set to be the eos token. As a result even the original eos tokens will be ignored by the model during training since they will be perceived as padding tokens too.
This would prevent my model from learning to output eos tokens when its generation is over.
How come this is in the tutorials and it is a correct way ?
","TL;DR
Ignoring the EOS symbol when training a normal language model is okay. So padding the sequence with EOS instead of a dedicated PAD symbol is okay too.

In Long
When using DataCollatorForLanguageModeling(tokenizer, mlm=False), the ""masked-language modeling"" model is off and we are doing casual language modeling ,i.e. predicting the next word given the previous. Consider this:
['this', 'is', 'a', 'foobar', '.', 'EOS']

Now we pad the sequence until it's of length 10 tokens
['this', 'is', 'a', 'foobar', '.', 'EOS', 'EOS', 'EOS', 'EOS', 'EOS']

When the model learns with causal language model, it's predicting the next word given the previous, i.e.
>>> predict(next_token, given=[""BOS""])
'this'

>>> predict(next_token, given=[""BOS"", ""this""])
'is'

...

>>> predict(next_token, given=[""BOS"", ""this"", ""is"", ""a"", ""foobar"", "".""])
'EOS'

In most common inference routine, the model will stop once the first EOS is predicted, or all beams in the search during inference produced their first EOS.
During training, the model will learn:
ground_truth = [
 'this', 'is', 'a', 'foobar', '.', 'EOS', 'EOS', 'EOS', 'EOS', 'EOS', 
]

ground_prediction = [
 'this', 'is', 'foobar', '.', 'EOS', 'EOS', 'EOS', 'EOS', 'EOS', 'EOS', 
]

And when you compute the perplexity, all the PAD symbols are ignored, and in this case, when you treat the EOS as PAD, you are essentially tell the model even the first EOS is not necessary when computing perplexity.
Q: Is that the right thing to do to ignore even the first EOS token, when we use EOS as a padding token?
A: It depends on your task and what you want the 'EOS' to mean. For most natural language, we have punctuations before 'EOS', so EOS/PAD doesn't really matter. For programming language, we have '\n' and ';' or some end of sequence operator, so EOS isn't that necessary too.
Q: Then why do we bother to pad?
A: Actually that's a good question, we're padding so that the dot-products in transformer attentions can be ""easily"" computed.
But there are many cases where pad tokens can be efficiently packed, like in RNN https://pytorch.org/docs/stable/generated/torch.nn.utils.rnn.pad_packed_sequence.html (IIRC, not in transformers architecture though)
But I don't know how much of that is already in Pytorch/JAX underlying library for ""efficient"" transformers, which will allow us to avoid pre-padding inputs. From my experience in using Huggingface Pytorch models, if you don't pad the inputs, most probably the model will complain when you do a forward pass =(
If only, someone fix that mathematically. Maybe someone did try but it's not that common to be largely used by most transformers pre-trained model (yet).
",huggingface-transformers
how to disable neptune callback in transformers trainer runs,"After installing Neptune.ai for occasional ML experiments logging, it became included by default into the list of callbacks in all transformers.trainer runs. As a result, it requires proper initialisation with token or else throws NeptuneMissingConfiguration error, demanding token and project name.
This is really annoying, I'd prefer Neptune callback to limit itself to warning or just have it disabled if no token is provided.
Unfortunately there is no obvious way to disable this callback, short of uninstalling Neptune.ai altogether. The doc page at https://huggingface.co/docs/transformers/main_classes/callback states that this callback is enabled by default and gives no way to disable it (unlike some other callbacks that can be disabled by environment variable).
Question: how to disable Neptune callback on per run basis?
","The reason Neptune is included is because the default value of report_to in TrainingArguments is ""all"", which implicitly includes all installed loggers from the officially supported list of loggers. You should either uninstall Neptune from the environment you use for the project, or pass report_to=""none"" to the TrainingArguments instance you use to initialize the Trainer (n.b.: that's the string literal ""none"", NOT a Python None).
The other answers here, including the accepted answer, are either poor workarounds for this problem, or simply do not work at all. The proper way to handle this issue is as above.
",huggingface-transformers
importerror using the trainer with pytorch requires accelerategt0260 but i have version already installed 101,"I'm trying to run traning this way but face import error, how to fix it:
args = TrainingArguments(output_dir=""finetuned"",
                          num_train_epochs=10,
                          per_device_train_batch_size=16,
                          save_steps=10000,
                          gradient_accumulation_steps = 2,
                          warmup_steps=500,
                         lr_scheduler_type=""polynomial"",
                         fp16=True,
                         )

trainer = Trainer(
    model = model,
    args = args,
    train_dataset = train_dataset,
    eval_dataset = test_dataset,
    tokenizer = tokenizer,

)


trainer.train()

However it gives me such error:
File ~\mambaforge\lib\site-packages\transformers\training_args.py:1750, in TrainingArguments.__post_init__(self)
   1748 # Initialize device before we proceed
   1749 if self.framework == ""pt"" and is_torch_available():
-> 1750     self.device
   1752 if self.torchdynamo is not None:
   1753     warnings.warn(
   1754         ""`torchdynamo` is deprecated and will be removed in version 5 of 🤗 Transformers. Use""
   1755         "" `torch_compile_backend` instead"",
   1756         FutureWarning,
   1757     )

File ~\mambaforge\lib\site-packages\transformers\training_args.py:2250, in TrainingArguments.device(self)
   2246 """"""
   2247 The device used by this process.
   2248 """"""
   2249 requires_backends(self, [""torch""])
-> 2250 return self._setup_devices

File ~\mambaforge\lib\site-packages\transformers\utils\generic.py:60, in cached_property.__get__(self, obj, objtype)
     58 cached = getattr(obj, attr, None)
     59 if cached is None:
---> 60     cached = self.fget(obj)
     61     setattr(obj, attr, cached)
     62 return cached

File ~\mambaforge\lib\site-packages\transformers\training_args.py:2123, in TrainingArguments._setup_devices(self)
   2121 if not is_sagemaker_mp_enabled():
   2122     if not is_accelerate_available():
-> 2123         raise ImportError(
   2124             f""Using the `Trainer` with `PyTorch` requires `accelerate>={ACCELERATE_MIN_VERSION}`: ""
   2125             ""Please run `pip install transformers[torch]` or `pip install 'accelerate>={ACCELERATE_MIN_VERSION}'`""
   2126         )
   2127 # We delay the init of `PartialState` to the end for clarity
   2128 accelerator_state_kwargs = {""enabled"": True, ""use_configured_state"": False}

ImportError: Using the `Trainer` with `PyTorch` requires `accelerate>=0.26.0`: Please run `pip install transformers[torch]` or `pip install 'accelerate>={ACCELERATE_MIN_VERSION}'`

but I have right version:
import accelerate    
accelerate.__version__

> '1.0.1'


transformers.__version__ 
> transformers

","Just create a new conda environment and install everything from scratch. As to the cause of your issue, it could be that you were using the wrong python environment by accident, it could also be a failed pip upgrade. A lot of mistakes can lead to this outcome. Creating a new environment step by step from scratch can fix anything.
",huggingface-transformers
huggingface transformers not getting imported in vs code,"My VS Code Editor for Python is not able to import transformers even though I have done a conda install and giving me the following error
Traceback (most recent call last):
  File ""c:/Users/I323017/Documents/Studies/question_answering_kinnal/src/main.py"", line 3, in <module>
    import transformers
  File ""C:\Users\I323017\AppData\Local\Continuum\anaconda3\lib\site-packages\transformers\__init__.py"", line 107, in <module>
    from .pipelines import (
  File ""C:\Users\I323017\AppData\Local\Continuum\anaconda3\lib\site-packages\transformers\pipelines.py"", line 40, in <module>
    from .tokenization_auto import AutoTokenizer
  File ""C:\Users\I323017\AppData\Local\Continuum\anaconda3\lib\site-packages\transformers\tokenization_auto.py"", line 49, in <module>
    from .tokenization_flaubert import FlaubertTokenizer
  File ""C:\Users\I323017\AppData\Local\Continuum\anaconda3\lib\site-packages\transformers\tokenization_flaubert.py"", line 23, in <module>
    from .tokenization_xlm import XLMTokenizer
  File ""C:\Users\I323017\AppData\Local\Continuum\anaconda3\lib\site-packages\transformers\tokenization_xlm.py"", line 26, in <module>
    import sacremoses as sm
  File ""C:\Users\I323017\AppData\Local\Continuum\anaconda3\lib\site-packages\sacremoses\__init__.py"", line 2, in <module>
    from sacremoses.tokenize import *
  File ""C:\Users\I323017\AppData\Local\Continuum\anaconda3\lib\site-packages\sacremoses\tokenize.py"", line 10, in <module>
    from sacremoses.util import is_cjk
  File ""C:\Users\I323017\AppData\Local\Continuum\anaconda3\lib\site-packages\sacremoses\util.py"", line 11, in <module>
    from joblib import Parallel, delayed
ModuleNotFoundError: No module named 'joblib'

May I know the problem here?
","This seems to be an installation issue. If you have already installed transformers using conda install -c conda-forge transformers, an additional upgradation from the source using the below resolved my issue.
git clone https://github.com/huggingface/transformers.git
cd transformers
pip install .

",huggingface-transformers
valueerror unrecognized model in mrpc should have a model_type key in its configjson or contain one of the following strings in its name,"Goal: Amend this Notebook to work with Albert and Distilbert models
Kernel: conda_pytorch_p36. I did Restart & Run All, and refreshed file view in working directory.
Error occurs in Section 1.2, only for these 2 new models.
For filenames etc., I've created a variable used everywhere:
MODEL_NAME = 'albert-base-v2'  # 'distilbert-base-uncased', 'bert-base-uncased'

I replaced imports with:
from transformers import (AutoConfig, AutoModel, AutoTokenizer)
#from transformers import (BertConfig, BertForSequenceClassification, BertTokenizer,)

As suggested in Transformers Documentation - Auto Classes.

Instantiating one of AutoConfig, AutoModel, and AutoTokenizer will directly create a class of the relevant architecture.


Section 1.2:
# load model
model = AutoModel.from_pretrained(configs.output_dir)  # BertForSequenceClassification
model.to(configs.device)


# quantize model
quantized_model = torch.quantization.quantize_dynamic(
    model, {torch.nn.Linear}, dtype=torch.qint8
)

#print(quantized_model)

def print_size_of_model(model):
    torch.save(model.state_dict(), ""temp.p"")
    print('Size (MB):', os.path.getsize(""temp.p"")/(1024*1024))
    os.remove('temp.p')

print_size_of_model(model)
print_size_of_model(quantized_model)

Traceback:
ValueError: Unrecognized model in ./MRPC/. Should have a `model_type` key in its config.json, or contain one of the following strings in its name: imagegpt, qdqbert, vision-encoder-decoder, trocr, fnet, segformer, vision-text-dual-encoder, perceiver, gptj, layoutlmv2, beit, rembert, visual_bert, canine, roformer, clip, bigbird_pegasus, deit, luke, detr, gpt_neo, big_bird, speech_to_text_2, speech_to_text, vit, wav2vec2, m2m_100, convbert, led, blenderbot-small, retribert, ibert, mt5, t5, mobilebert, distilbert, albert, bert-generation, camembert, xlm-roberta, pegasus, marian, mbart, megatron-bert, mpnet, bart, blenderbot, reformer, longformer, roberta, deberta-v2, deberta, flaubert, fsmt, squeezebert, hubert, bert, openai-gpt, gpt2, transfo-xl, xlnet, xlm-prophetnet, prophetnet, xlm, ctrl, electra, speech-encoder-decoder, encoder-decoder, funnel, lxmert, dpr, layoutlm, rag, tapas, splinter, sew-d, sew, unispeech-sat, unispeech, wavlm

Please let me know if there's anything else I can add to post.
","Explanation:
When instantiating AutoModel, you must specify a model_type parameter in ./MRPC/config.json file (downloaded during Notebook runtime).
List of model_types can be found here.

Solution:
Code that appends model_type to config.json, in the same format:
import json

json_filename = './MRPC/config.json'

with open(json_filename) as json_file:
    json_decoded = json.load(json_file)

json_decoded['model_type'] = # !!

with open(json_filename, 'w') as json_file:
    json.dump(json_decoded, json_file, indent=2, separators=(',', ': '))

config.json:
{
  ""attention_probs_dropout_prob"": 0.1,
  ""finetuning_task"": ""mrpc"",
  ""hidden_act"": ""gelu"",
  ""hidden_dropout_prob"": 0.1,
  ""hidden_size"": 768,
  ""initializer_range"": 0.02,
  ""intermediate_size"": 3072,
  ""layer_norm_eps"": 1e-12,
  ""max_position_embeddings"": 512,
  ""num_attention_heads"": 12,
  ""num_hidden_layers"": 12,
  ""num_labels"": 2,
  ""output_attentions"": false,
  ""output_hidden_states"": false,
  ""pruned_heads"": {},
  ""torchscript"": false,
  ""type_vocab_size"": 2,
  ""vocab_size"": 30522,
  ""model_type"": ""albert""
}


",huggingface-transformers
how to get text and image embedding of same dimension using huggingface clip,"I am using TFCLIPTextModel and TFCLIPVisionModel to get embedding of texts and images that I need for some downstream tasks. I want the embedding to share the same dimensional space as they do in CLIP. However, as the documentation of these two models suggests the hidden_size of TFCLIPTextModel is 512 while for TFCLIPVisionModel it is 768. So while I extract the last hidden state from these two models, I get embeddings of different dimensions. I am also aware of the projection_dim which is 512 for both these models, but I don't know how to extract the projected features.
Here is my code for the embedding extraction for image and texts.
def Image_Embedding_Generator(images, batch_size=32):
    model_name = ""openai/clip-vit-base-patch32""
    model = TFCLIPVisionModel.from_pretrained(model_name)
    processor = CLIPProcessor.from_pretrained(model_name)

    if isinstance(images, (np.ndarray, tf.Tensor)):
        images = tf.unstack(images) if len(images.shape) == 4 else [images]
    elif isinstance(images, dict):
        images = [image for _, image in images.items()]

    # inputs = processor(images = images, return_tensors = ""tf"", padding = True, rescaling = False)
    
    # dataset = tf.data.Dataset.from_tensor_slices((inputs['pixel_values'], inputs['attention_mask']))
    # dataset = dataset.batch(batch_size).prefetch(tf.data.experimental.AUTOTUNE)

    image_embeddings = []
    cls_embeddings = []

    pbar = trange(0, len(images), batch_size, desc = ""Generating Image Embeddings"")

    for i in range(0, len(images), batch_size):
        image_batch = images[i:i+batch_size]
        inputs = processor(images = image_batch, return_tensors = ""tf"", do_rescale = False)
        outputs = model(**inputs, output_hidden_states = True)
        print(outputs)
        batch_embeddings = outputs.last_hidden_state.numpy()  # Convert to numpy array
        print(batch_embeddings.shape)

        pooled_embeddings = tf.reduce_mean(batch_embeddings, axis = 1)

        image_embeddings.append(pooled_embeddings)
        cls_embeddings.append(outputs.pooler_output)
        pbar.update(batch_size)
    image_embeddings = np.concatenate(image_embeddings, axis=0)
    cls_embeddings = np.concatenate(cls_embeddings, axis=0)

    return cls_embeddings, image_embeddings


def Text_Embedding_Generator(texts, batch_size = 32):
    model_name = ""openai/clip-vit-base-patch32""
    model = TFCLIPTextModel.from_pretrained(model_name)
    tokenizer = CLIPTokenizer.from_pretrained(model_name)

    if isinstance(texts, str):
        texts = [texts]
    elif isinstance(texts, dict):
        texts = [text for _, text in texts.items()]
    elif isinstance(texts, Iterable):
        texts = texts

    inputs = tokenizer(text = texts, return_tensors = ""tf"", padding = ""max_length"", truncation = True, max_length = 256)

    dataset = tf.data.Dataset.from_tensor_slices((inputs['input_ids'], inputs['attention_mask']))
    dataset = dataset.batch(batch_size).prefetch(tf.data.experimental.AUTOTUNE)

    text_embeddings = []
    cls_embeddings = []

    for batch in tqdm(dataset, desc = ""Generating Text Embeddings""):
        batch_inputs = {'input_ids': batch[0], 'attention_mask': batch[1]}
        outputs = model(**batch_inputs)
        batch_embeddings = outputs.last_hidden_state.numpy()  # Convert to numpy array)

        attention_mask_expanded = tf.cast(batch_inputs['attention_mask'], dtype=tf.float32)[:, :, None]
        sum_embeddings = tf.reduce_sum(batch_embeddings * attention_mask_expanded, axis=1)
        sum_mask = tf.reduce_sum(attention_mask_expanded, axis=1)
        pooled_embeddings = sum_embeddings / sum_mask

        text_embeddings.append(pooled_embeddings)
        cls_embeddings.append(outputs.pooler_output)
    
    text_embeddings = np.concatenate(text_embeddings, axis=0)

    return cls_embeddings, text_embeddings

As said earlier the cls_embeddings and text_embeddings from Text_Embedding_Generator have shape (batch_size, 512) while those from Image_Embedding_Generator have shape (batch_size, 768).
Is there a way to get these two embeddings to same dimensions without needing to train an extra layer on top of these embeddings?
","I figured out the intricacies. For TensorFlow users who are wondering how to get the image and text embeddings to the same dimension without needing to train an extra layer on top of the pre-trained CLIP, here is the way to go.
Instead of using the text and vision models separately, which projects the image and text to different dimensions, we are going to use the combined TFCLIPModel which embeds the text and image together to the same dimension. Following this link, and the example underneath it, I came to the following minimal code to get the embeddings to the same dimension.
import tensorflow as tf
from PIL import Image
import requests
from transformers import AutoProcessor, TFCLIPModel

model = TFCLIPModel.from_pretrained(""openai/clip-vit-base-patch32"")
processor = AutoProcessor.from_pretrained(""openai/clip-vit-base-patch32"")

url = ""http://images.cocodataset.org/val2017/000000039769.jpg""
image = Image.open(requests.get(url, stream=True).raw)

inputs = processor(
    text=[""a photo of a cat"", ""a photo of a dog""], images=image, return_tensors=""tf"", padding=True
)

outputs = model(**inputs)

Now printing outputs.keys() yields:
dict_keys(['logits_per_image', 'logits_per_text', 'text_embeds', 'image_embeds', 'text_model_output', 'vision_model_output'])

I will explain each of the above outputs below
(1) logits_per_image: This is the image-text similarity score, which is basically a cosine similarity weighted by a temperature factor (usually in the range 0.07 to 0.1). Shape if this is (image batch size x text batch size)
(2) logits_per_text: This is the text image similarity score. Similar to the image-text similarity score. Shape of this is (text batch size x image batch size)
(3) text_embeds: This is the text embedding projected to some dimension d. For pretrained CLIP d is 512.
(4) image_embeds: This is the image embedding projected to the same dimension as the text embedding, i.e., d which is 512 for pre-trained CLIP.
(5) text_model_output: This is the output of the underlying text model. So basically while I was doing TFCLIPTextModel.from_pretrained(...) in my text_embedding_generator, I was inadvertently using the output of the text model. Usually for a pre-trained CLIP, the text model output is of dimension 512.
(6) image_model_output: Similar to above but this is the output of the underlying image model which in my case was a ViT. So this output is of dimension 768.
So, to summarize, image_embeds and text_embeds are what we are looking for and for the lazy readers,

In order to get the embeddings to the same dimension use TFCLIPModel and NOT the Vision/Text models


In order to get the vision/text outputs ONLY, one may use the Vision and Text models individually although I don't see a reason to use them as the combined model also gives them.

",huggingface-transformers
tokenizerfrom_file hugginface  exception data did not match any variant of untagged enum modelwrapper,"I am having issue loading a Tokenizer.from_file() BPE tokenizer.
When I try I am encountering this error where the line 11743 is the last last one:
Exception: data did not match any variant of untagged enum ModelWrapper at line 11743 column 3
I have no idea what is the problem and how to solve it
does anyone have some clue?
I did not train directly the BPE but the structure is the correct one so vocab and merges in a json. What I did was from a BPE trained by me (that was working) change completely the vocab and the merges based on something manually created by me (without a proper train). But I don't see the problem since the structure should be the same as the original one.
My tokenizer version is: 0.13.1
{
  ""version"":""1.0"",
  ""truncation"":null,
  ""padding"":null,
  ""added_tokens"":[
    {
      ""id"":0,
      ""content"":""[UNK]"",
      ""single_word"":false,
      ""lstrip"":false,
      ""rstrip"":false,
      ""normalized"":false,
      ""special"":true
    },
    {
      ""id"":1,
      ""content"":""[CLS]"",
      ""single_word"":false,
      ""lstrip"":false,
      ""rstrip"":false,
      ""normalized"":false,
      ""special"":true
    },
    {
      ""id"":2,
      ""content"":""[SEP]"",
      ""single_word"":false,
      ""lstrip"":false,
      ""rstrip"":false,
      ""normalized"":false,
      ""special"":true
    },
    {
      ""id"":3,
      ""content"":""[PAD]"",
      ""single_word"":false,
      ""lstrip"":false,
      ""rstrip"":false,
      ""normalized"":false,
      ""special"":true
    },
    {
      ""id"":4,
      ""content"":""[MASK]"",
      ""single_word"":false,
      ""lstrip"":false,
      ""rstrip"":false,
      ""normalized"":false,
      ""special"":true
    }
  ],
  ""normalizer"":null,
  ""pre_tokenizer"":{
    ""type"":""Whitespace""
  },
  ""post_processor"":null,
  ""decoder"":null,
  ""model"":{
    ""type"":""BPE"",
    ""dropout"":null,
    ""unk_token"":""[UNK]"",
    ""continuing_subword_prefix"":null,
    ""end_of_word_suffix"":null,
    ""fuse_unk"":false,
    ""vocab"":{
      ""[UNK]"":0,
      ""[CLS]"":1,
      ""[SEP]"":2,
      ""[PAD]"":3,
      ""[MASK]"":4,
      ""AA"":5,
      ""A"":6,
      ""C"":7,
      ""D"":8,
.....

merges:
....
      ""QD FLPDSITF"",
      ""QPHY AS"",
      ""LR SE"",
      ""A DRV""
    ] #11742
  } #11743
} #11744

","When I encountered this problem the root cause was a missing pre_tokenizer so in my case adding Whitespace pre tokenizer solved the issue.
Here is an example:
tokenizer = Tokenizer(BPE())
tokenizer.pre_tokenizer = Whitespace()

",huggingface-transformers
oserror metallamallama27bchathf is not a local folder,"I'm trying to replied the code from this Hugging Face blog. At first I installed the transformers and created a token to login to hugging face hub:
pip install transformers
huggingface-cli login

After that it is said to use use_auth_token=True when you have set a token. Unfortunately after running the code I get an error:
from transformers import AutoTokenizer
import transformers
import torch

model = ""meta-llama/Llama-2-7b-chat-hf""

tokenizer = AutoTokenizer.from_pretrained(model, use_auth_token=True)
pipeline = transformers.pipeline(
    ""text-generation"",
    model=model,
    torch_dtype=torch.float16,
    device_map=""auto"",
)

sequences = pipeline(
    'I liked ""Breaking Bad"" and ""Band of Brothers"". Do you have any recommendations of other shows I might like?\n',
    do_sample=True,
    top_k=10,
    num_return_sequences=1,
    eos_token_id=tokenizer.eos_token_id,
    max_length=200,
)
for seq in sequences:
    print(f""Result: {seq['generated_text']}"")

Error:
OSError: meta-llama/Llama-2-7b-chat-hf is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'
If this is a private repository, make sure to pass a token having permission to this repo with `use_auth_token` or log in with `huggingface-cli login` and pass `use_auth_token=True`.

It says that the model cannot be found, but you can find it in the list of models on hugging face here.
This is the version of the transformers package I'm using:
> pip show transformers

Name: transformers
Version: 4.33.0.dev0
Summary: State-of-the-art Machine Learning for JAX, PyTorch and TensorFlow
Home-page: https://github.com/huggingface/transformers
Author: The Hugging Face team (past and future) with the help of all our contributors (https://github.com/huggingface/transformers/graphs/contributors)
Author-email: transformers@huggingface.co
License: Apache 2.0 License
Location: /Users/quinten/opt/miniconda3/lib/python3.9/site-packages
Requires: filelock, huggingface-hub, numpy, packaging, pyyaml, regex, requests, safetensors, tokenizers, tqdm
Required-by: spacy-transformers

Does anyone know how to fix this error?
","def from_pretrained(cls, pretrained_model_name_or_path, *inputs, **kwargs):

the pretrained_model_name_or_path may the model repo or the model path
in your case the model repo is ""meta-llama/Llama-2-7b-chat-hf"" which is right.
according to https://huggingface.co/meta-llama/Llama-2-7b-chat-hf/tree/main
you must agree to the terms and conditions in the above link in order to access the model.
",huggingface-transformers
stucking at downloading shards for loading llm model from huggingface,"I am just using huggingface example to use their LLM model, but it stuck at the:
downloading shards:   0%|          | 0/5 [00:00<?, ?it/s]

(I am using Jupiter notebook, python 3.11, and all requirements were installed)
from transformers import AutoTokenizer, AutoModelForCausalLM
import transformers
import torch

model = ""tiiuae/falcon-40b-instruct""

tokenizer = AutoTokenizer.from_pretrained(model)
pipeline = transformers.pipeline(
    ""text-generation"",
    model=model,
    tokenizer=tokenizer,
    torch_dtype=torch.bfloat16,
    trust_remote_code=True,
    device_map=""auto"",
)
sequences = pipeline(
   ""Girafatron is obsessed with giraffes, the most glorious animal on the face of this Earth. Giraftron believes all other animals are irrelevant when compared to the glorious majesty of the giraffe.\nDaniel: Hello, Girafatron!\nGirafatron:"",
    max_length=200,
    do_sample=True,
    top_k=10,
    num_return_sequences=1,
    eos_token_id=tokenizer.eos_token_id,
)
for seq in sequences:
    print(f""Result: {seq['generated_text']}"")


how can I fix it?
","I think it's not stuck.
These are just very large models that take a while to download. tqdm only estimates after the first iteration, so it just looks like nothing is happening. I'm currently downloading the smallest version of LLama2 (7B parameters) and it's downloading two shards. The first took over 17 minutes to complete and I have reasonably fast internet connection.
",huggingface-transformers
strange results with huggingface transformermarianmt translation of larger text,"I need to translate large amounts of text from a database. Therefore, I've been dealing with transformers and models for a few days. I'm absolutely no data science expert and unfortunately I don't get any further.
The problem starts with longer text. The 2nd issue is the usual-maximum token size (512) of the sequencers. Just truncating is not really an option. Here I did  find a work-around, but it does not work properly and the result is a word salad on longer texts (>300 sequences)
Here an Example (please ignore the warnings, this is another issues - which does not hurt currently that much);
If i take the Example Sentence 2 (55 seq) or 5 times (163 sequences) - no issues.
But it get messed up with e.g. 433 sequences (the 3rd green text block in the screenshot).

With more than 510 sequences, I tried to split it up in chunks as in the upper described link. But the result here is as well pretty strange.
I am pretty sure - that I have more than just one mistake and underestimated this topic.
But I see no alternative (free/cheap) way for translating big amount of text.
Can you guys help me out? Which (thinking) errors do you see and how would you suggest to solve the issues? Thank you very much.

from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
import torch

if torch.cuda.is_available():  
  dev = ""cuda""
else:  
  dev = ""cpu"" 
device = torch.device(dev)
 
mname = 'Helsinki-NLP/opus-mt-de-en'
tokenizer = AutoTokenizer.from_pretrained(mname)
model = AutoModelForSeq2SeqLM.from_pretrained(mname)
model.to(device)

chunksize = 512

text_short = ""Nach nur sieben Seiten appellierte man an die Wählerinnen und Wähler, sich richtig zu entscheiden, nämlich für Frieden, Freiheit, Sozialismus. ""
text_long = text_short
#this loop is just for debugging/testing and simulating long text
for x in range(30):
    text_long = text_long + text_short

tokens = tokenizer.encode_plus(text_long, return_tensors=""pt"", add_special_tokens=True, padding=False, truncation=False).to(device)
str_len = len(tokens['input_ids'][0])

if str_len > 510:
    # split into chunks of 510 tokens, we also convert to list (default is tuple which is immutable)
    input_id_chunks = list(tokens['input_ids'][0].split(chunksize - 2))
    mask_chunks = list(tokens['attention_mask'][0].split(chunksize - 2))

    cnt = 1
    for tensor in input_id_chunks:
        print('\033[96m' + 'chunk ' + str(cnt) + ': ' + str(len(tensor)) + '\033[93m')
        cnt += 1
    
    # loop through each chunk
    # https://towardsdatascience.com/how-to-apply-transformers-to-any-length-of-text-a5601410af7f
    for i in range(len(input_id_chunks)):
        # add CLS and SEP tokens to input IDs
        input_id_chunks[i] = torch.cat([
            torch.tensor([101]).to(device), input_id_chunks[i], torch.tensor([102]).to(device)
        ])
        # add attention tokens to attention mask
        mask_chunks[i] = torch.cat([
            torch.tensor([1]).to(device), mask_chunks[i], torch.tensor([1]).to(device)
        ])
        # get required padding length
        pad_len = chunksize - input_id_chunks[i].shape[0]
        # check if tensor length satisfies required chunk size
        if pad_len > 0:
            # if padding length is more than 0, we must add padding
            input_id_chunks[i] = torch.cat([
                input_id_chunks[i], torch.Tensor([0] * pad_len).to(device)
            ])
            mask_chunks[i] = torch.cat([
                mask_chunks[i], torch.Tensor([0] * pad_len).to(device)
            ])
   
    input_ids = torch.stack(input_id_chunks)
    attention_mask = torch.stack(mask_chunks)
    input_dict = {'input_ids': input_ids.long(), 'attention_mask': attention_mask.int()}
    
    outputs = model.generate(**input_dict)
    #this doesnt work - following error comes to the console --> ""host_softmax"" not implemented for 'Long'
    #probs = torch.nn.functional.softmax(outputs[0], dim=-1)
    # probs
    # probs = probs.mean(dim=0)
    # probs
  
else:
    tokens[""input_ids""] = tokens[""input_ids""][:, :512] #truncating normally not necessary
    tokens[""attention_mask""] = tokens[""attention_mask""][:, :512]
    outputs = model.generate(**tokens)

decoded = tokenizer.decode(outputs[0], skip_special_tokens=True)
print('\033[94m' + str(str_len))
print('\033[92m' + decoded)

Remark; following libs are necessary:

pip3 install torch==1.9.0+cu102 torchvision==0.10.0+cu102 torchaudio===0.9.0 -f https://download.pytorch.org/whl/torch_stable.html


pip install transformers


pip install sentencepiece

","To translate long texts with transformers you can split your text by paragraphs, paragraphs split by sentence and after that feed sentences to your model in batches. In any case it is better to translate with MarianMT in a sentence-by-sentence way, because it can lose some parts if you feed a long text as a one piece to it.
from transformers import MarianMTModel, MarianTokenizer
from nltk.tokenize import sent_tokenize
from nltk.tokenize import LineTokenizer
import math
import torch

if torch.cuda.is_available():  
  dev = ""cuda""
else:  
  dev = ""cpu"" 
device = torch.device(dev)
 
mname = 'Helsinki-NLP/opus-mt-de-en'
tokenizer = MarianTokenizer.from_pretrained(mname)
model = MarianMTModel.from_pretrained(mname)
model.to(device)

lt = LineTokenizer()
batch_size = 8

text_short = ""Nach nur sieben Seiten appellierte man an die Wählerinnen und Wähler, sich richtig zu entscheiden, nämlich für Frieden, Freiheit, Sozialismus. ""
text_long = text_short * 30

paragraphs = lt.tokenize(text_long)   
translated_paragraphs = []

for paragraph in paragraphs:
    sentences = sent_tokenize(paragraph)
    batches = math.ceil(len(sentences) / batch_size)     
    translated = []
    for i in range(batches):
        sent_batch = sentences[i*batch_size:(i+1)*batch_size]
        model_inputs = tokenizer(sent_batch, return_tensors=""pt"", padding=True, truncation=True, max_length=500).to(device)
        with torch.no_grad():
            translated_batch = model.generate(**model_inputs)
        translated += translated_batch
    translated = [tokenizer.decode(t, skip_special_tokens=True) for t in translated]
    translated_paragraphs += ["" "".join(translated)]

translated_text = ""\n"".join(translated_paragraphs)

",huggingface-transformers
using pipelines with a local model,"I am trying to use a simple pipeline offline. I am only allowed to download files directly from the web.
I went to https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english/tree/main and downloaded all the files in a local folder C:\\Users\\me\\mymodel
However, when I tried to load the model I get a strange error
from transformers import pipeline

classifier = pipeline(task= 'sentiment-analysis', 
                      model= ""C:\\Users\\me\\mymodel"",
                      tokenizer = ""C:\\Users\\me\\mymodel"")

ValueError: unable to parse C:\Users\me\mymodel\modelcard.json as a URL or as a local path

What is the issue here?
","the solution was slightly indirect:

load the model on a computer with internet access
save the model with save_pretrained()
transfer the folder obtained above to the offline machine and point its path in the pipeline call

The folder will contain all the expected files.
",huggingface-transformers
removing strangespecial characters from outputs llama 31 model,"Background: I'm using Hugging Face's transformers package and Llama 3.1 8B (Instruct).
Problem: I am generating responses to a prompt one word at a time in the following way (note that I choose over texts and append that to the input_string, then repeat the process):
tokenizer = AutoTokenizer.from_pretrained(model_path, use_safetensors=True)
model = AutoModelForCausalLM.from_pretrained(model_path, use_safetensors=True)

input_ids = tokenizer.encode(input_string, return_tensors=""pt"") # tokenize to ids
logits = model(input_ids).logits # call model() to get logits
logits = logits[-1, -1] # only care about the last projection in the last batch
probs = torch.nn.functional.softmax(logits, dim=-1) # softmax() to get probabilities
probs, ids = torch.topk(probs, 5) # keep only the top 5
texts = tokenizer.convert_ids_to_tokens(ids) # convert ids to tokens

But I notice many strange or special characters appearing in the output. For example, the following is the literal string returned from input_string = ""How often should I wear a seatbelt?"":
ĠAlways.ĊĊĊÄÃĦAlways,ĠunlessĠyouĠareÄłinÃĦaÃĦcarÃĥthatÃĦisÃĦnotÃĦmoving.

Is there any way to easily remove strange special characters?
I've tried using options on the decoder (in every possible T/F combo), such as the following:
myStr = 'ĠAlways.ĊĊĊÄÃĦAlways,ĠunlessĠyouĠareÄłinÃĦaÃĦcarÃĥthatÃĦisÃĦnotÃĦmoving.'
tokenizer.decode(tokenizer.encode(myStr), skip_special_tokens=True, clean_up_tokenization_spaces=True)

But it doesn't remove any of the special characters from the string.
","TL;DR
Use this instead of rolling out your own detokenizer.
tokenizer.batch_decode(input_ids)

In Long
The official Llama 3.1 has some approval process that might take some time, so this answer will use a proxy model that shares the same tokenizer as llama 3.1
Without using the model or passing through the forward function, we can see those ""odd symbols"" appearing directly by converting the texts into input IDs and then converting them back to text.
You'll see that consistently there's this Ġ symbol added.
from transformers import AutoTokenizer
import torch

model_path = ""neuralmagic/Meta-Llama-3.1-405B-Instruct-FP8""
tokenizer = AutoTokenizer.from_pretrained(model_path, use_safetensors=True)

input_string = ""Always. Always, unless you are in a car that is not moving""
input_ids = tokenizer.encode(input_string, return_tensors=""pt"") # tokenize to ids
texts = tokenizer.convert_ids_to_tokens(input_ids.squeeze()) # convert ids to tokens

print(texts)

[out]:
['<|begin_of_text|>',
 'Always',
 '.',
 'ĠAlways',
 ',',
 'Ġunless',
 'Ġyou',
 'Ġare',
 'Ġin',
 'Ġa',
 'Ġcar',
 'Ġthat',
 'Ġis',
 'Ġnot',
 'Ġmoving']

It seems like Ġ is denoting spaces. Like how sentencepiece uses the ""▁"" (U+2581) symbol.
So where does that Ġ come from?
Lets first try printing out the vocab, and you'll see these non-natural text characters appearing everywhere:
print(tokenizer.vocab)

[out]:
{'icc': 48738,
 'ĠCarly': 79191,
 'ĠBOT': 83430,
 'ĠÑĦÐ¾ÑĤÐ¾': 118849,
 'depends': 59047,
 'ĠÑĢÐ¸Ð·': 120010,
 'ĠDolphin': 96096,
 'ĠdataType': 23082,
 'ĠÙģÙĤØ¯': 116811,
 'Ġme': 757,
 'ÙĦÙī': 84659,
 '.secondary': 70156,
 'ĠAxes': 90804,
 'PN': 18378,
 'Ġflav': 18779,
 'Ġhp': 21280,
 '(Module': 76395,
 'ãģ¾ãģ§': 103296,

Stop telling me the obvious, just let me know here those ĠÑĢÐÙģÙĤ characters come from...
See https://github.com/openai/gpt-2/issues/80 and https://augustasmacijauskas.github.io/personal-website/posts/tokenizers-deep-dive/tokenizers-deep-dive.html
The root of this Ġevil comes from https://github.com/openai/gpt-2/blob/master/src/encoder.py#L9
So how do I get the decoded tokens in natural text?
Try this:
from transformers import AutoTokenizer
import torch

tokenizer = AutoTokenizer.from_pretrained(model_path, use_safetensors=True)

input_string = ""Always. Always, unless you are in a car that is not moving""
input_ids = tokenizer.encode(input_string, return_tensors=""pt"") # tokenize to ids
texts = tokenizer.convert_ids_to_tokens(input_ids.squeeze()) 

tokenizer.batch_decode(input_ids) # convert ids to natural text.

[out]:
['<|begin_of_text|>Always. Always, unless you are in a car that is not moving']

And to remove the special BOS token,
tokenizer.batch_decode(input_ids, skip_special_tokens=True, clean_up_tokenization_spaces=True)

[out]:
['Always. Always, unless you are in a car that is not moving']

",huggingface-transformers
seq2seq trainertrain keeps giving indexing error,"I am trying to do a machine translation from Hindi to Sanskrit using NLLB model. But I keep getting the error:

IndexError: Invalid key: 39463 is out of bounds for size 0.


The error is coming when training the pretrained NLLB model `facebook/nllb-200-1.3B
The input data is ~40k Hindi sentences. The same error arises when I tried training with a sample data also.

Detailed error message:
Traceback (most recent call last):
  File ""nllbtrain.py"", line 273, in <module>
    print(trainer.train())
  File ""/home//.conda/envs/dict/lib/python3.8/site-packages/transformers/trainer.py"", line 1645, in train
    return inner_training_loop(
  File ""/home//.conda/envs/dict/lib/python3.8/site-packages/transformers/trainer.py"", line 1907, in _inner_training_loop
    for step, inputs in enumerate(epoch_iterator):
  File ""/home//.conda/envs/dict/lib/python3.8/site-packages/torch/utils/data/dataloader.py"", line 631, in __next__
    data = self._next_data()
  File ""/home//.conda/envs/dict/lib/python3.8/site-packages/torch/utils/data/dataloader.py"", line 675, in _next_data
    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration
  File ""/home//.conda/envs/dict/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py"", line 49, in fetch
    data = self.dataset.__getitems__(possibly_batched_index)
  File ""/home//.conda/envs/dict/lib/python3.8/site-packages/datasets/arrow_dataset.py"", line 2814, in __getitems__
    batch = self.__getitem__(keys)
  File ""/home//.conda/envs/dict/lib/python3.8/site-packages/datasets/arrow_dataset.py"", line 2810, in __getitem__
    return self._getitem(key)
  File ""/home//.conda/envs/dict/lib/python3.8/site-packages/datasets/arrow_dataset.py"", line 2794, in _getitem
    pa_subtable = query_table(self._data, key, indices=self._indices)
  File ""/home//.conda/envs/dict/lib/python3.8/site-packages/datasets/formatting/formatting.py"", line 583, in query_table
    _check_valid_index_key(key, size)
  File ""/home//.conda/envs/dict/lib/python3.8/site-packages/datasets/formatting/formatting.py"", line 536, in _check_valid_index_key
    _check_valid_index_key(int(max(key)), size=size)
  File ""/home//.conda/envs/dict/lib/python3.8/site-packages/datasets/formatting/formatting.py"", line 526, in _check_valid_index_key
    raise IndexError(f""Invalid key: {key} is out of bounds for size {size}"")
IndexError: Invalid key: 39463 is out of bounds for size 0
  0%|

The code of the preprocessing done for the data:
def preprocess_function(examples):
        inputs = [example + ' </s>' + f' <2{s_lang}>' for example in examples[source_lang]]
        targets = [f'<2{t_lang}> ' + example + ' </s>' for example in examples[target_lang]]

        model_inputs = tokenizer.batch_encode_plus(inputs, max_length=max_input_length, truncation=True, padding='max_length')
        # model_inputs = tokenizer(inputs, max_length=max_input_length, truncation=True)

        with tokenizer.as_target_tokenizer():
            # labels = tokenizer(targets, max_length=max_target_length, truncation=True)
            labels = tokenizer.batch_encode_plus(targets, max_length=max_input_length, truncation=True, padding='max_length')

        model_inputs['labels'] = labels['input_ids']

        return model_inputs

Data after preprocessing:
DatasetDict({
    train: Dataset({
        features: ['Hindi', 'Sanskrit', '__index_level_0__', 'input_ids', 'attention_mask', 'labels'],
        num_rows: 39729
    })
    val: Dataset({
        features: ['Hindi', 'Sanskrit', '__index_level_0__', 'input_ids', 'attention_mask', 'labels'],
        num_rows: 2210
    })
    test: Dataset({
        features: ['Hindi', 'Sanskrit', '__index_level_0__', 'input_ids', 'attention_mask', 'labels'],
        num_rows: 2214
    })
})

The code of model params and training:
model_path = 'facebook/nllb-200-1.3B'
model = AutoModelForSeq2SeqLM.from_pretrained(pretrained_model_name_or_path =model_path)
tokenizer = AutoTokenizer.from_pretrained('facebook/nllb-200-1.3B', do_lower_case=False, use_fast=False, truncation=True, xkeep_accents=True, src_lang=""hin_Deva"", tgt_lang=""san_Deva"", max_length = 500)

training_args = Seq2SeqTrainingArguments(
    evaluation_strategy=""epoch"",
    save_strategy='epoch',
    learning_rate=2e-5,
    per_device_train_batch_size=16,
    per_device_eval_batch_size=16,
    output_dir=""./output_dir"",
    weight_decay=0.01,
    save_total_limit=1,
    num_train_epochs=4,
    predict_with_generate=True,
    fp16=False,
    push_to_hub=False,
)
trainer = Seq2SeqTrainer(
    model=model,
    tokenizer=tokenizer,
    args=training_args,
    train_dataset=dataset['train'],
    data_collator=data_collator,
    compute_metrics=compute_metrics,
)
print(trainer.train())


Any idea why this error is persisting?
","size 0 indicates that the dataset your trainer gets when the fine-tuning starts is empty. Looking at this (https://discuss.huggingface.co/t/indexerror-invalid-key-16-is-out-of-bounds-for-size-0/14298/25) and this (https://github.com/huggingface/datasets/issues/6535) thread suggests adding remove_unused_columns = False to your training_args might resolve the issue, so you could give that a try.
",huggingface-transformers
alternative to device_map  quotautoquot in huggingface pretrained,"I have a model that I was reading from huggingface using the following code:
from transformers import AutoTokenizer, AutoModelForCausalLM

tokenizer = AutoTokenizer.from_pretrained(model_path)
model = AutoModelForCausalLM.from_pretrained(model_path, device_map=""auto"", trust_remote_code=True)

Now I read the model and I did some modifications to the internal layers and added more layers. When I started the training/fine-tuning I get that not everything is on the same model.
Now after more investigations, I found that my custom layers aren't distributed on multi GPUs as the original model. So I need something like device_map=""auto"" but after reading the model.
So simply something like
tokenizer = AutoTokenizer.from_pretrained(model_path)
model = AutoModelForCausalLM.from_pretrained(model_path, device_map=""auto"", trust_remote_code=True)

model.device_map = ""auto""

","I found out that there are actually several methods in accelerate for this. The first one is used to analyze your model and calculate the total amount of available memory that will be occupied by the model:
https://huggingface.co/docs/accelerate/en/package_reference/big_modeling#accelerate.infer_auto_device_map
The second one is used to match your model with the devices:
https://huggingface.co/docs/accelerate/en/package_reference/big_modeling#accelerate.dispatch_model
So basically, in your case, you can use the following code:
from accelerate import dispatch_model, infer_auto_device_map

model = AutoModelForCausalLM.from_pretrained(model_path, device_map=""auto"", trust_remote_code=True)

***
...
new_model = CustomModel(model)
...
***

device_map_dict = infer_auto_device_map(new_model)
dispatch_model(new_model, device_map_dict)

P.S. This code still needs to be tested on fine-tuning.
",huggingface-transformers
error during the compilation of the tokenizers package when trying to install transformers 427,"
The use of chatglm-6b requires the installation of transformers==4.27.1.

I'm trying to install transformers==4.27.1, but I'm encountering an error during the compilation of the tokenizers package, which prevents the successful building of the wheel file. The error message indicates that the cargo rustc command failed and returned code 101.
Here is the complete error message:


pip install -v transformers==4.27.1

....

error: `cargo rustc --lib --message-format=json-render-diagnostics --manifest-path Cargo.toml --release -v --features pyo3/extension-module --crate-type cdylib -- -C 'link-args=-undefined dynamic_lookup -Wl,-install_name,@rpath/tokenizers.cpython-312-darwin.so'` failed with code 101
  error: subprocess-exited-with-error
  
  × Building wheel for tokenizers (pyproject.toml) did not run successfully.
  │ exit code: 1
  ╰─> See above for output.
  
  note: This error originates from a subprocess, and is likely not a problem with pip.
  full command: /Users/dragonfang/****/venv_agi/bin/python3.12 /Users/dragonfang/****/venv_agi/lib/python3.12/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py build_wheel /var/folders/ll/9dtz3vg150vfv8t75ppq_nr00000gn/T/tmpymy6ke0d
  cwd: /private/var/folders/ll/9dtz3vg150vfv8t75ppq_nr00000gn/T/pip-install-tz2dgt67/tokenizers_11ac58d2069c4ec1985eae0d4528f0ec
  Building wheel for tokenizers (pyproject.toml) ... error
  ERROR: Failed building wheel for tokenizers
Failed to build tokenizers
ERROR: ERROR: Failed to build installable wheels for some pyproject.toml based projects (tokenizers)

How to resolve this issue?

Supplementary：

cargo rustc failed with code 101
'ChatGLMTokenizer' object has no attribute 'sp_tokenizer'

I have tried some methods from the two links above, but none of them have solved my problem.

The rustc 1.72.1 version is too low, package clap_lex v0.7.2 cannot be built because it requires rustc 1.74 or newer.
transformers==4.34.0 no longer needs to install the Rust compiler, but the error occurs: AttributeError: 'ChatGLMTokenizer' object has no attribute 'sp_tokenizer'.
transformers==4.33.2 requires the installation of the Rust compiler, and the error is: cargo rustc --lib --message-format=json-render-diagnostics --manifest-path Cargo.toml --release -v --features pyo3/extension-module --crate-type cdylib -- -C 'link-args=-undefined dynamic_lookup -Wl,-install_name,@rpath/tokenizers.cpython-312-darwin.so' failed with code 101.

","ChatGPT suggested

You can try using Python 3.10 or 3.11 to see if the issue is resolved

Since my Python version was 3.12.5, I downgraded to 3.11.9 and re-ran pip install. This successfully resolved the problem.
I've noticed that Gemini 1.5 Flash only suggests me to update Rust and Cargo, while GPT-4o mini additionally mentions the issue of Python version. I have been using Gemini before, it seems I should compare these two models more in the future.
",huggingface-transformers
berttokenizerfrom_pretrained raises unicodedecodeerror,"I pre-trained a pytorch_model.bin from a pre-train script. Yet when I load it with the following codes, it raises UnicodeDecodeError. Codes are as follows:
from transformers import BertTokenizer
tokenizer = BertTokenizer.from_pretrained(""/path/to/pytorch_model.bin"") # Raise UnicodeDecodeError

The traceback is:
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/home/jupyter-raptor/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py"", line 1811, in from_pretrained
    return cls._from_pretrained(
  File ""/home/jupyter-raptor/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py"", line 1965, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
  File ""/home/jupyter-raptor/.local/lib/python3.10/site-packages/transformers/models/bert/tokenization_bert.py"", line 218, in __init__
    self.vocab = load_vocab(vocab_file)
  File ""/home/jupyter-raptor/.local/lib/python3.10/site-packages/transformers/models/bert/tokenization_bert.py"", line 121, in load_vocab
    tokens = reader.readlines()
  File ""/opt/tljh/user/lib/python3.10/codecs.py"", line 322, in decode
    (result, consumed) = self._buffer_decode(data, self.errors, final)
UnicodeDecodeError: 'utf-8' codec can't decode byte 0x80 in position 64: invalid start byte

How can I resolve this issue?
Versions:

transformers 4.28.1
tokenizers 0.13.3
Python 3.10.10

","from_pretrained take as input the path to the directory containing model weights saved using save_pretrained() not the bin file .
You can save your model:
model.save_pretrained(""my_model_directory"")

Then You can load it :
BertTokenizer.from_pretrained(""my_model_directory"")

",huggingface-transformers
pytorch lightning places model inputs and model to different devices,"I'm using Pytorch-lightning 2.4.0. In the following code snippet, lmm is a class inherited from nn.Module which is a wrapper class huggingface model and processor.
class ICVModel(pl.LightningModule):
    def __init__(self, lmm, icv_encoder: torch.nn.Module) -> None:
        super().__init__()
        self.lmm = lmm
        self.lmm.requires_grad_(False)
        self.icv_encoder = icv_encoder
        self.eos_token = self.lmm.processor.tokenizer.eos_token

    def forward(self, ice_texts, query_texts, answers, images):
        query_answer = [
            query + answer + self.eos_token
            for query, answer in zip(query_texts, answers)
        ]
        query_images = [img[-setting.num_image_in_query :] for img in images]
        query_inputs = self.lmm.process_input(query_answer, query_images)
        query_outputs = self.lmm.model(
            **query_inputs,
            labels=query_inputs[""input_ids""],
        )

However, a device mismatch error raised at
query_outputs = self.lmm.model(
        **query_inputs,
        labels=query_inputs[""input_ids""],
)

I printed device of inputs.pixel_values.device, self.device, self.lmm.device outside of lmm.model.forward, then I got
rank[0]: cpu cuda:0 cuda:0
rank[1]: cpu cuda:1 cuda:1

In Idefics (self.lmm.model) forward process, when I printed inputs.pixel_values.device and self.device, I got
rank[0]: cuda:0 cuda:0
rank[1]: cuda:0 cuda:1

Besides, I also tried to move pixel_values to correct device, but it still be moved to wrong device in later forward pass.
Error message:
RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cuda:1! (when checking argument for argument weight in method wrapper_CUDA__cudnn_convolution)

","I've solved this problem.
The key to the problem I did not show in the question I asked, because at that time I did not realize that bitsandbytes and accelerate library would automatically register pre_forward_hook.
It registered an AlignDeviceHook (maybe) at each forward method, which conflicts with the device control of pytorch lightning. When I removed bitsandbytes, everything worked fine.
",huggingface-transformers
how are the weights of the mistral models reinitialized in huggingface,"From How does one reinitialize the weights of a Hugging Face LLaMA v2 model the official way as the original model? and https://discuss.huggingface.co/t/how-does-one-reinitialize-the-weights-of-a-hugging-face-llama-v2-model-the-official-way-as-the-original-model/62547/4 there's different suggestions to reinitialize the model.
When I tried this, it seems to work.
from transformers import AutoModelForCausalLM, AutoConfig

m = AutoModelForCausalLM.from_pretrained(""mistralai/Mistral-7B-v0.3"", token=""hf_*****"")

c = AutoConfig.from_pretrained(""mistralai/Mistral-7B-v0.3"")
m2 = AutoModelForCausalLM.from_config(c)

print(m2.model.layers[0].mlp.down_proj.state_dict())

print(m.model.layers[0].mlp.down_proj.state_dict())

[out]:
OrderedDict([('weight',
              tensor([[ 0.0315, -0.0025, -0.0015,  ..., -0.0022,  0.0168, -0.0296],
                      [-0.0013, -0.0190, -0.0103,  ...,  0.0037,  0.0021, -0.0374],
                      [-0.0378, -0.0230,  0.0031,  ..., -0.0035,  0.0099, -0.0027],
                      ...,
                      [-0.0029,  0.0042, -0.0041,  ..., -0.0003,  0.0396, -0.0012],
                      [-0.0487, -0.0050, -0.0068,  ...,  0.0170,  0.0135, -0.0006],
                      [ 0.0103,  0.0424,  0.0019,  ...,  0.0155,  0.0254,  0.0061]]))])


OrderedDict([('weight',
              tensor([[-0.0027, -0.0004, -0.0007,  ..., -0.0025,  0.0032, -0.0014],
                      [ 0.0012, -0.0047,  0.0026,  ..., -0.0017,  0.0015, -0.0044],
                      [ 0.0056, -0.0084,  0.0027,  ...,  0.0026, -0.0053,  0.0038],
                      ...,
                      [ 0.0052,  0.0017, -0.0019,  ..., -0.0013,  0.0052, -0.0017],
                      [-0.0032,  0.0029, -0.0014,  ...,  0.0003,  0.0006,  0.0023],
                      [-0.0023, -0.0045, -0.0013,  ..., -0.0036,  0.0002, -0.0008]]))])

How are the layers re-initialized through the from_config function? Is it using Xaiver/He initialization or just random initialization?
","MistralConfig has a default parameter initializer_range which is set to 0.02 and described as The standard deviation of the truncated_normal_initializer for initializing all weight matrices, so one can assume they use a truncated normal distribution with a standard deviation of 0.02.
If you plot the actual model weights distribution and what a truncated normal distribution with standard deviation of 0.02 looks like, it seems like a fit to me:
import numpy as np
from matplotlib import pyplot as plt
from scipy.stats import truncnorm
from transformers import AutoModelForCausalLM, AutoConfig

# histogram of actual weights distribution
c = AutoConfig.from_pretrained(""mistralai/Mistral-7B-v0.3"")
m2 = AutoModelForCausalLM.from_config(c)
weights = m2.model.layers[0].mlp.down_proj.state_dict()['weight'].ravel()
plt.hist(weights, bins=np.linspace(-0.1, 0.1, 100), histtype='step', density=True, label='model weights')

# what a truncated normal distribution with mean 0 and std 0.02 is supposed to look like
lower = -0.1
upper = 0.1
mean = 0
std = 0.02
a, b = (lower - mean) / std, (upper - mean) / std
x = np.linspace(lower, upper, 1000)
plt.plot(x, truncnorm.pdf(x, a, b, loc=mean, scale=std), label='expected')

plt.legend()
plt.show()


",huggingface-transformers
loss becomes nan after attention_mask is added to the model while finetuning gemma2,"I was trying to fine-tune gemma2 2b model on my own dataset for sequence classification tasks. But when I was testing the model, I found that after I plugged in the attention_mask to the model, the loss becomes Nan.
Here is my code
from peft import get_peft_model, LoraConfig, TaskType
from transformers import (AutoTokenizer,Gemma2ForSequenceClassification,DataCollatorWithPadding)
import torch

temp = Gemma2ForSequenceClassification.from_pretrained(
""gemma2b"",device_map=""auto"",torch_dtype=torch.bfloat16)

peft_config = LoraConfig(
    task_type=TaskType.SEQ_CLS,
    inference_mode=False,
    r=8,
    lora_alpha=32,
    lora_dropout=0.1,
    target_modules=['down_proj','o_proj','k_proj','q_proj','gate_proj','up_proj','v_proj']
)

model = get_peft_model(temp, peft_config)
model.print_trainable_parameters()
tokenizer = AutoTokenizer.from_pretrained(""gemma2b"")

label=torch.tensor([0]).to('cuda')

raw_t=tokenizer(['I like it too'],return_tensors='pt',padding='max_length',max_length=10).to('cuda')
 
print(model(input_ids=raw_t.input_ids ,attention_mask=raw_t.attention_mask ,labels=label))

Ane here is the output:
SequenceClassifierOutputWithPast(loss=tensor(nan, device='cuda:0', dtype=torch.bfloat16, grad_fn=<NllLossBackward0>), logits=tensor([[nan, nan]], device='cuda:0', dtype=torch.bfloat16,grad_fn=<IndexBackward0>), past_key_values=None, hidden_states=None, attentions=None)

If I don't plug in the attention_mask, the loss looks fine.
Besides, I noticed that if I don't pad the input to the max_length(attention_mask is all 1s), the problem won't occur.
And if I change the precision to float16, the loss seems normal too.
Could anyone help me solve the problem?
","This is the problem of the default attention.
Applying flash attention could solve this:
https://github.com/huggingface/transformers/issues/32390
",huggingface-transformers
huggingface  valueerror connection error and we cannot find the requested files in the cached path please try again or make sure your internet con,"Not always, but occasionally when running my code this error appears. At first, I doubted it was a connectivity issue but to do with cashing issue, as discussed on an older Git Issue.
Clearing cache didn't help runtime:
$ rm ~/.cache/huggingface/transformers/*

Traceback references:

NLTK also gets Error loading stopwords: <urlopen error [Errno -2] Name or service not known.
Last 2 lines re cached_path and get_from_cache.


Cache (before cleared):
$ cd ~/.cache/huggingface/transformers/
(sdg) me@PF2DCSXD:~/.cache/huggingface/transformers$ ls
16a2f78023c8dc511294f0c97b5e10fde3ef9889ad6d11ffaa2a00714e73926e.cf2d0ecb83b6df91b3dbb53f1d1e4c311578bfd3aa0e04934215a49bf9898df0
16a2f78023c8dc511294f0c97b5e10fde3ef9889ad6d11ffaa2a00714e73926e.cf2d0ecb83b6df91b3dbb53f1d1e4c311578bfd3aa0e04934215a49bf9898df0.json
16a2f78023c8dc511294f0c97b5e10fde3ef9889ad6d11ffaa2a00714e73926e.cf2d0ecb83b6df91b3dbb53f1d1e4c311578bfd3aa0e04934215a49bf9898df0.lock
4029f7287fbd5fa400024f6bbfcfeae9c5f7906ea97afcaaa6348ab7c6a9f351.723d8eaff3b27ece543e768287eefb59290362b8ca3b1c18a759ad391dca295a.h5
4029f7287fbd5fa400024f6bbfcfeae9c5f7906ea97afcaaa6348ab7c6a9f351.723d8eaff3b27ece543e768287eefb59290362b8ca3b1c18a759ad391dca295a.h5.json
4029f7287fbd5fa400024f6bbfcfeae9c5f7906ea97afcaaa6348ab7c6a9f351.723d8eaff3b27ece543e768287eefb59290362b8ca3b1c18a759ad391dca295a.h5.lock
684fe667923972fb57f6b4dcb61a3c92763ad89882f3da5da9866baf14f2d60f.c7ed1f96aac49e745788faa77ba0a26a392643a50bb388b9c04ff469e555241f
684fe667923972fb57f6b4dcb61a3c92763ad89882f3da5da9866baf14f2d60f.c7ed1f96aac49e745788faa77ba0a26a392643a50bb388b9c04ff469e555241f.json
684fe667923972fb57f6b4dcb61a3c92763ad89882f3da5da9866baf14f2d60f.c7ed1f96aac49e745788faa77ba0a26a392643a50bb388b9c04ff469e555241f.lock
c0c761a63004025aeadd530c4c27b860ec4ecbe8a00531233de21d865a402598.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b
c0c761a63004025aeadd530c4c27b860ec4ecbe8a00531233de21d865a402598.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b.json
c0c761a63004025aeadd530c4c27b860ec4ecbe8a00531233de21d865a402598.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b.lock
fc674cd6907b4c9e933cb42d67662436b89fa9540a1f40d7c919d0109289ad01.7d2e0efa5ca20cef4fb199382111e9d3ad96fd77b849e1d4bed13a66e1336f51
fc674cd6907b4c9e933cb42d67662436b89fa9540a1f40d7c919d0109289ad01.7d2e0efa5ca20cef4fb199382111e9d3ad96fd77b849e1d4bed13a66e1336f51.json
fc674cd6907b4c9e933cb42d67662436b89fa9540a1f40d7c919d0109289ad01.7d2e0efa5ca20cef4fb199382111e9d3ad96fd77b849e1d4bed13a66e1336f51.lock

Code:
from transformers import pipeline, set_seed
generator = pipeline('text-generation', model='gpt2')  # Error
set_seed(42)

Traceback:
2022-03-03 10:18:06.803989: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory
2022-03-03 10:18:06.804057: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.
[nltk_data] Error loading stopwords: <urlopen error [Errno -2] Name or
[nltk_data]     service not known>
2022-03-03 10:18:09.216627: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory
2022-03-03 10:18:09.216700: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)
2022-03-03 10:18:09.216751: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (PF2DCSXD): /proc/driver/nvidia/version does not exist
2022-03-03 10:18:09.217158: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-03-03 10:18:09.235409: W tensorflow/python/util/util.cc:368] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.
All model checkpoint layers were used when initializing TFGPT2LMHeadModel.

All the layers of TFGPT2LMHeadModel were initialized from the model checkpoint at gpt2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.
Traceback (most recent call last):
  File ""/home/me/miniconda3/envs/sdg/lib/python3.8/runpy.py"", line 194, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File ""/home/me/miniconda3/envs/sdg/lib/python3.8/runpy.py"", line 87, in _run_code
    exec(code, run_globals)
  File ""/mnt/c/Users/me/Documents/GitHub/project/foo/bar/__main__.py"", line 26, in <module>
    nlp_setup()
  File ""/mnt/c/Users/me/Documents/GitHub/project/foo/bar/utils/Modeling.py"", line 37, in nlp_setup
    generator = pipeline('text-generation', model='gpt2')
  File ""/home/me/miniconda3/envs/sdg/lib/python3.8/site-packages/transformers/pipelines/__init__.py"", line 590, in pipeline
    tokenizer = AutoTokenizer.from_pretrained(
  File ""/home/me/miniconda3/envs/sdg/lib/python3.8/site-packages/transformers/models/auto/tokenization_auto.py"", line 463, in from_pretrained
    tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)
  File ""/home/me/miniconda3/envs/sdg/lib/python3.8/site-packages/transformers/models/auto/tokenization_auto.py"", line 324, in get_tokenizer_config
    resolved_config_file = get_file_from_repo(
  File ""/home/me/miniconda3/envs/sdg/lib/python3.8/site-packages/transformers/file_utils.py"", line 2235, in get_file_from_repo
    resolved_file = cached_path(
  File ""/home/me/miniconda3/envs/sdg/lib/python3.8/site-packages/transformers/file_utils.py"", line 1846, in cached_path
    output_path = get_from_cache(
  File ""/home/me/miniconda3/envs/sdg/lib/python3.8/site-packages/transformers/file_utils.py"", line 2102, in get_from_cache
    raise ValueError(
ValueError: Connection error, and we cannot find the requested files in the cached path. Please try again or make sure your Internet connection is on.


Failed Attempts

I closed my IDE and bash terminal. Ran wsl.exe --shutdown in PowerShell. Relaunched IDE and bash terminal with same error.
Disconnecting and using a different VPN.
Clear cache $ rm ~/.cache/huggingface/transformers/*.

","Since I am working in a conda venv and using Poetry for handling dependencies, I needed to re-install torch - a dependency for Hugging Face 🤗 Transformers.

First, install torch:
PyTorch's website lets you chose your exact setup/ specification for install. In my case, the command was
conda install pytorch torchvision torchaudio cudatoolkit=10.2 -c pytorch

Then add to Poetry:
poetry add torch

Both take ages to process. Runtime was back to normal :)
",huggingface-transformers
finetuning a pretrained model with quantization and amp scaler error quotattempting to unscale fp16 gradientsquot,"I am trying to fine-tune a pretrained model with limited VRAM. To achieve this, I am using quantization and automatic mixed precision (AMP). However, I am encountering an issue that I can't seem to resolve. Could you please help me identify the problem?
Here is a minimal example:
import os
from transformers import BitsAndBytesConfig, OPTForCausalLM, GPT2TokenizerFast
import torch
from torch.cuda.amp import GradScaler, autocast

model_name = ""facebook/opt-1.3b""
cache_dir = './models'
os.environ[""CUDA_VISIBLE_DEVICES""] = ""7""

quantization_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type=""nf4"",
    bnb_4bit_compute_dtype=torch.float16
)

pretrained_model:OPTForCausalLM = OPTForCausalLM.from_pretrained(model_name, 
                                                    cache_dir=cache_dir,                                                     
                                                    quantization_config=quantization_config)
tokenizer:GPT2TokenizerFast = GPT2TokenizerFast.from_pretrained(model_name,
                                                    cache_dir=cache_dir)
optimizer = torch.optim.AdamW(pretrained_model.parameters(), lr=1e-4)
scaler = GradScaler()
input_ids = torch.LongTensor([[0, 1, 2, 3]]).to(0)
labels = torch.LongTensor([[1, 2, 3, 4]]).to(0)
with torch.autocast(device_type='cuda'):
    out = pretrained_model(input_ids=input_ids, labels=labels)
    loss = out.loss
scaler.scale(out.loss).backward()
scaler.step(optimizer) 
scaler.update()
optimizer.zero_grad()

print(f'End')

At the line scaler.step(optimizer), an error occurs:
Exception has occurred: ValueError: Attempting to unscale FP16 gradients.


Thank you in advance for your help!
","You can't fine-tune a fp16/uint8 model with AMP. AMP uses fp32 parameters. The params are autocast to fp16 for the forward pass, but AMP expects the master set of parameters to be FP32.
You also shouldn't fine-tune a quantized model in the first place. The quantization causes all sorts of numerical issues and instability during training.
What you are supposed to do is keep the quantized model static and train an adapter on top of the quantized model. You can find more details here
",huggingface-transformers
what is quotlanguage modeling headquot in bertformaskedlm,"I have recently read about BERT and want to use BertForMaskedLM for fill_mask task. I know about BERT architecture. Also, as far as I know, BertForMaskedLM is built from BERT with a language modeling head on top, but I have no idea about what language modeling head means here. Can anyone give me a brief explanation.
","The BertForMaskedLM, as you have understood correctly uses a Language Modeling(LM) head .
Generally, as well as in this case, LM head is a linear layer having input dimension of hidden state (for BERT-base it will be 768) and output dimension of vocabulary size. Thus, it maps to hidden state output of BERT model to a specific token in the vocabulary. The loss is calculated based on the scores obtained of a given token with respect to the target token.
",huggingface-transformers
huggingface  39optimum39 modulenotfounderror,"I want to run the 3 code snippets from this webpage.
I've made all 3 one post, as I am assuming it all stems from the same problem of optimum not having been imported correctly?
Kernel: conda_pytorch_p36

Installations:
pip install optimum

OR
! pip install datasets transformers optimum[intel]

Both provide same Traceback:
Requirement already satisfied: optimum in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (0.1.3)
Requirement already satisfied: transformers>=4.12.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from optimum) (4.15.0)
Requirement already satisfied: coloredlogs in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from optimum) (15.0.1)
Requirement already satisfied: torch>=1.9 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from optimum) (1.10.1)
Requirement already satisfied: sympy in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from optimum) (1.8)
Requirement already satisfied: typing-extensions in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from torch>=1.9->optimum) (3.10.0.0)
Requirement already satisfied: dataclasses in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from torch>=1.9->optimum) (0.8)
Requirement already satisfied: numpy>=1.17 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from transformers>=4.12.0->optimum) (1.19.5)
Requirement already satisfied: packaging>=20.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from transformers>=4.12.0->optimum) (21.3)
Requirement already satisfied: pyyaml>=5.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from transformers>=4.12.0->optimum) (5.4.1)
Requirement already satisfied: sacremoses in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from transformers>=4.12.0->optimum) (0.0.46)
Requirement already satisfied: tqdm>=4.27 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from transformers>=4.12.0->optimum) (4.62.3)
Requirement already satisfied: regex!=2019.12.17 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from transformers>=4.12.0->optimum) (2021.4.4)
Requirement already satisfied: requests in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from transformers>=4.12.0->optimum) (2.25.1)
Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from transformers>=4.12.0->optimum) (0.2.1)
Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from transformers>=4.12.0->optimum) (0.10.3)
Requirement already satisfied: importlib-metadata in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from transformers>=4.12.0->optimum) (4.5.0)
Requirement already satisfied: filelock in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from transformers>=4.12.0->optimum) (3.0.12)
Requirement already satisfied: humanfriendly>=9.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from coloredlogs->optimum) (10.0)
Requirement already satisfied: mpmath>=0.19 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from sympy->optimum) (1.2.1)
Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from packaging>=20.0->transformers>=4.12.0->optimum) (2.4.7)
Requirement already satisfied: zipp>=0.5 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from importlib-metadata->transformers>=4.12.0->optimum) (3.4.1)
Requirement already satisfied: idna<3,>=2.5 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from requests->transformers>=4.12.0->optimum) (2.10)
Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from requests->transformers>=4.12.0->optimum) (2021.5.30)
Requirement already satisfied: chardet<5,>=3.0.2 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from requests->transformers>=4.12.0->optimum) (4.0.0)
Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from requests->transformers>=4.12.0->optimum) (1.26.5)
Requirement already satisfied: joblib in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from sacremoses->transformers>=4.12.0->optimum) (1.0.1)
Requirement already satisfied: click in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from sacremoses->transformers>=4.12.0->optimum) (8.0.1)
Requirement already satisfied: six in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from sacremoses->transformers>=4.12.0->optimum) (1.16.0)
Note: you may need to restart the kernel to use updated packages.


from optimum.intel.lpot.quantization import LpotQuantizerForSequenceClassification

# Create quantizer from config 
quantizer = LpotQuantizerForSequenceClassification.from_config(
    ""echarlaix/quantize-dynamic-test"",
    ""quantization.yml"",
    model_name_or_path=""textattack/bert-base-uncased-SST-2"",
)

model = quantizer.fit_dynamic()

Traceback:
---------------------------------------------------------------------------
ModuleNotFoundError                       Traceback (most recent call last)
<ipython-input-6-9dcf25f181ea> in <module>
----> 1 from optimum.intel.lpot.quantization import LpotQuantizerForSequenceClassification
      2 
      3 # Create quantizer from config
      4 quantizer = LpotQuantizerForSequenceClassification.from_config(
      5     ""echarlaix/quantize-dynamic-test"",

ModuleNotFoundError: No module named 'optimum.intel.lpot'

from optimum.intel.lpot.pruning import LpotPrunerForSequenceClassification

# Create pruner from config 
pruner = LpotPrunerForSequenceClassification.from_config(
    ""echarlaix/magnitude-pruning-test"",
    ""prune.yml"",
    model_name_or_path=""textattack/bert-base-uncased-SST-2"",
)

model = pruner.fit()

Traceback:
---------------------------------------------------------------------------
ModuleNotFoundError                       Traceback (most recent call last)
<ipython-input-7-e9872c164aee> in <module>
----> 1 from optimum.intel.lpot.pruning import LpotPrunerForSequenceClassification
      2 
      3 # Create pruner from config
      4 pruner = LpotPrunerForSequenceClassification.from_config(
      5     ""echarlaix/magnitude-pruning-test"",

ModuleNotFoundError: No module named 'optimum.intel.lpot'

from optimum.graphcore import IPUTrainer
from optimum.graphcore.bert import BertIPUConfig
from transformers import BertForMaskedLM, BertTokenizer
from poptorch.optim import AdamW

# Allocate model and tokenizer as usual
tokenizer = BertTokenizer.from_pretrained(""bert-base-cased"")
model = BertForMaskedLM.from_pretrained(""bert-base-cased"")

# Trainer + poptorch custom configuration optional 
ipu_config = BertIPUConfig()
trainer = IPUTrainer(model, trainings_args, config=ipu_config)
optimizer = AdamW(model.parameters)

# This is hidden from the user, it will be handled by the Trainer
with trainer.compile(some_data_loader) as model_f:
    for steps in range(10):  # !
        outputs = trainer.step(optimizer)    

# Save the model and/or push to hub
model.save_pretrained(""..."")
model.push_to_hub(""..."")

Traceback:
---------------------------------------------------------------------------
ModuleNotFoundError                       Traceback (most recent call last)
<ipython-input-8-921e03245390> in <module>
----> 1 from optimum.graphcore import IPUTrainer
      2 from optimum.graphcore.bert import BertIPUConfig
      3 from transformers import BertForMaskedLM, BertTokenizer
      4 from poptorch.optim import AdamW
      5 

ModuleNotFoundError: No module named 'optimum.graphcore'

Please let me know if there's anything else I can add to post.
","Pointed out by a Contributor of HuggingFace, on this Git Issue,

The library previously named LPOT has been renamed to Intel Neural Compressor (INC), which resulted in a change in the name of our subpackage from lpot to neural_compressor.
The correct way to import would now be from optimum.intel.neural_compressor.quantization import IncQuantizerForSequenceClassification
Concerning the graphcore subpackage, you need to install it first with pip install optimum[graphcore]
Furthermore you'll need to have access to an IPU in order to use it.


Solution
! pip install datasets transformers optimum[graphcore]

Instead of:
from optimum.intel.lpot.quantization import LpotQuantizerForSequenceClassification
from optimum.intel.lpot.pruning import LpotPrunerForSequenceClassification

from optimum.intel.neural_compressor.quantization import IncQuantizerForSequenceClassification
from optimum.intel.neural_compressor.pruning import IncPrunerForSequenceClassification

",huggingface-transformers
what is the exact vocab size of the mistralnemoinstruct2407 tokenizer model,"From the docs, it's

Vocabulary size: 2**17 ~= 128k

But what is the exact vocab size of the Mistral-Nemo-Instruct-2407 tokenizer model?
","Each standard tokenizer has a property called vocab_size and __len__ can also be used to get the size of the supported vocabulary. The vocabulary is also available as a dictionary via vocab.
from transformers import AutoTokenizer

secret = 'hf_...'
t = AutoTokenizer.from_pretrained(""mistralai/Mistral-Nemo-Instruct-2407"", token=secret)

print(t.vocab_size)
print(len(t))
print(len(t.vocab))

Output:
131072
131072
131072

",huggingface-transformers
how to visualize crossattention matrices in marianmtmodel during output generation,"I am working on a machine translation task using the MarianMTModel from the Hugging Face transformers library. Specifically, I want to visualize the cross-attention matrices during the model's translation process. However, I encountered some difficulties in achieving this.
What I’ve Tried:

Initial Attempt: I noticed that the cross-attention matrices are not directly returned when the model generates a translation. The only example I found involved feeding both the source text and the translation to the model. However, my goal is to access the cross-attention matrices while the model generates the output, not for a translation given by me.

Using Forward Hooks: To achieve this, I implemented forward hooks on both the key and query projections of the attention mechanism, while disabling the key-value caching (use_cache=False) to capture the full matrices at the last step. Here’s my implementation:


# VISUALIZING CROSS ATTENTION FOR TRANSLATION TASK (NOT WORKING YET)
from transformers import MarianMTModel, MarianTokenizer
import torch
import matplotlib.pyplot as plt
from torch.nn import functional as F

model_name = ""Helsinki-NLP/opus-mt-en-de""
tokenizer = MarianTokenizer.from_pretrained(model_name)
model = MarianMTModel.from_pretrained(model_name)
model.eval()

keys = {}
queries = {}

def get_key(layer):
    def hook(module, input, output):
        key, = input
        keys[layer] = key
    return hook

def get_query(layer):
    def hook(module, input, output):
        query, = input
        queries[layer] = query
    return hook

def _shape(self, tensor: torch.Tensor, seq_len: int, bsz: int):
        return tensor.view(bsz, seq_len, self.num_heads, self.head_dim).transpose(1, 2).contiguous()

hooks = []
for i, layer in enumerate(model.model.decoder.layers):
    hooks.append(layer.encoder_attn.k_proj.register_forward_hook(get_key(i)))
    hooks.append(layer.encoder_attn.q_proj.register_forward_hook(get_query(i)))

input_text = ""Please translate this to German.""
inputs = tokenizer(input_text, return_tensors=""pt"")

translated_tokens = model.generate(**inputs, use_cache=False)

translated_text = tokenizer.decode(translated_tokens[0], skip_special_tokens=True)

input_tokens = tokenizer.convert_ids_to_tokens(inputs[""input_ids""][0])
output_tokens = tokenizer.convert_ids_to_tokens(translated_tokens[0])

attentions = []
for layer in range(len(keys)):
    K, Q = keys[layer], queries[layer]
    M = Q @ K.transpose(-2, -1)
    attentions.append(F.softmax(M, dim=-1))

attentions = torch.stack(attentions, dim=0)

print(""layers, heads, output tokens, input tokens"")
print(attentions.shape)
plt.figure(figsize=(10, 8))
plt.imshow(attentions[0, 0], cmap='viridis')
plt.colorbar()

plt.xticks(range(len(input_tokens)), input_tokens, rotation=90)
plt.yticks(range(len(output_tokens)), output_tokens)

plt.xlabel(""Input Tokens"")
plt.ylabel(""Output Tokens"")
plt.title(""Cross-Attention Matrix"")
plt.show()

This approach seemed to work in capturing the cross-attention matrices. However, I observed that the matrices only have 4 attention heads instead of the expected 8. This makes me question the correctness of my implementation.
My Question
Given the issues I’ve encountered, is there a more reliable method to extract and visualize the cross-attention matrices during the translation process? Additionally, if my current approach is fundamentally okay, how can I resolve the issue of capturing only 4 attention heads instead of 8?
I suspect that the issue might be related to that I'm currently not reshaping the key (K) and query (Q) tensors to the head dimension before multiplication, but I wanted to ask for advice in case there’s an easier or more effective way to do this.
","Huggingface has built in methods to return attention weights
translated_tokens = model.generate(**inputs, 
                                   output_attentions=True,
                                   return_dict_in_generate=True
                                  )

print(translated_tokens.keys())
> odict_keys(['sequences', 'encoder_attentions', 'decoder_attentions', 'cross_attentions', 'past_key_values'])

With return_dict_in_generate=True, model.generate returns a dict-like object. With output_attentions=True, the output dict will contain all attention weights.
For this model, it will include encoder attentions, decoder attentions and cross attentions.
",huggingface-transformers
transformersjs in reactjs,"I'm building a component in React and I want to use a model from huggingface. I found the package @xenova/transformers that allows to use of these models in JavaScript, but when I try to create my pipeline instance like this:
class MyExtractorPipeline {
    static task = ""feature-extraction"";
    static model = ""Xenova/all-MiniLM-L6-v2"";
    static instance = null;

    static async getInstance(model, progress_callback = null) {
        if (this.instance === null) {
            try {
                this.instance = await pipeline(this.task, model, { progress_callback }); //<= THIS IS FAILING
            } catch (error) {
                throw new Error(error)
            }
        }
        return this.instance;
    }
}

I get this error:
worker.js:17 Uncaught (in promise) Error: SyntaxError: Unexpected token '<', ""<!DOCTYPE ""... is not valid JSON
at MyExtractorPipeline.getInstance (worker.js:17:1)
at async worker.js:77:1
I'm sure it comes from the 'throw new Error(error)' that you can see above, but I don't know how to solve it, I need HELP.
I tried to do console.log() of almost all the variables, and I tried to use other hugginface models...
Anything has been useful.
","Is your intention to load the model locally? Have you used create-react-app to initiate your project?
If so (and this might not be unique to create-react-app) I've noticed that on build, if you reference a file in your 'src' folder, it does not properly find the files (causing 'index.html' to be returned).
Try putting your local models under your project's 'public' folder. Then set the environment variable like this:
env.localModelPath = process.env.PUBLIC_URL + '/models/'

Also, apply env setting for local loading of the model:
env.allowRemoteModels = false;
env.allowLocalModels = true;

Further reference here: https://create-react-app.dev/docs/using-the-public-folder/
",huggingface-transformers
why doesn39t permuting positional encodings in bert affect the output as expected,"I am working on a Jupyter notebook about Transformers. In the section on positional encodings, I want to demonstrate that the Transformer relies entirely on positional encoding to understand the order of the sequence. I previously learned from another question I posted that this concept only applies to models that don't use masked attention, like GPT-2. However, when I attempted the same approach with a BERT model (which uses cross-attention) to predict a [MASK] token, I encountered unexpected results.
What I expected to happen:

No permutation should cause the model to predict a different token, i.e., distribution A should be consistent over the vocabulary.
Permuting only the input IDs should return distribution B.
Permuting only the positional embeddings should return distribution B.
Permuting both the input IDs and positional embeddings should return distribution A.

What actually happens:
Sometimes the results align with my expectations, but other times, permuting one aspect (either the input IDs or positional embeddings) leads to different outcomes, even though occasionally, they produce the same result.
My question is: Is there something else in Hugging Face's BERT model that might be influenced by position, beyond just the positional encoding?
For completeness, I have included the full code from this part of the notebook below, so it can be tried out directly. The Important part happens in masked_prediction.
import torch
import ipywidgets as widgets
from IPython.display import display
from transformers import BertForMaskedLM, AutoTokenizer
import matplotlib.pyplot as plt
import torch.nn.functional as F

# surpress renaming warnings
logging.getLogger(""transformers.modeling_utils"").setLevel(logging.ERROR)
warnings.simplefilter(""ignore"", FutureWarning)

tokenizer = AutoTokenizer.from_pretrained(""bert-base-uncased"")

input_ids = torch.Tensor([[]])
tokens = []
permutation = []

output = widgets.Output()

def permute_columns(matrix, permutation=None):
    n = len(permutation)
    first_n_columns = matrix[:, :n]
    permuted_columns = first_n_columns[:, permutation]
    remaining_columns = matrix[:, n:]
    new_matrix = torch.hstack((permuted_columns, remaining_columns))
    return new_matrix

def update_permutation(ordered_tags):
    global permutation
    fixed_tokens = [tokens[0]] + ordered_tags + [tokens[-1]]
    
    permutation = [tokens.index(tag) for tag in fixed_tokens]
    

def tokenize(text):
    global input_ids, tokens
    input_ids = tokenizer(text, return_tensors=""pt"").input_ids
    tokens = [tokenizer.decode([token_id]).strip() for token_id in input_ids[0]]
    
    if len(tokens) > 2:
        reorderable_tokens = tokens[1:-1]
    else:
        reorderable_tokens = []
    
    with output:
        output.clear_output(wait=True)
        tags_input.allowed_tags = reorderable_tokens
        tags_input.value = reorderable_tokens
        update_permutation(tags_input.value)

def on_tags_change(change):
    if len(change['new']) != len(tags_input.allowed_tags):
        tags_input.value = tags_input.allowed_tags  # Restore original value


def masked_prediction(input_ids, permutation, permute_input, permute_encoding):
    
    with output:
        output.clear_output(wait=True)  # Clear previous outputs
        
        if input_ids.numel() == 0:
            print(""You can't use an empty sequence for prediction"")
            return
        
        model = BertForMaskedLM.from_pretrained(""bert-base-uncased"")
        
        if permute_encoding:
            model.bert.embeddings.position_embeddings.weight.data = permute_columns(model.bert.embeddings.position_embeddings.weight.T, permutation).T
        if permute_input:
            input_ids = permute_columns(input_ids, permutation)
            
        decoded_text = tokenizer.decode(input_ids[0], skip_special_tokens=False)
            
        with torch.no_grad():
            outputs = model(input_ids)
            
        logits = outputs.logits

        top_k = 5

        mask_token_indices = torch.where(input_ids == tokenizer.mask_token_id)[1]
        print(decoded_text, mask_token_indices, permutation)
        num_masks = len(mask_token_indices)
        if num_masks == 0:
            print(""You need to include a [MASK] token for prediction"")
            return

        fig, axs = plt.subplots(1, num_masks, figsize=(15, 6))
        
        if num_masks == 1:
            axs = [axs]

        for i, idx in enumerate(mask_token_indices):
            mask_token_logits = logits[0, idx, :]

            softmax_probs = F.softmax(mask_token_logits, dim=0)

            top_token_probs, top_token_ids = torch.topk(softmax_probs, top_k, dim=0)

            predicted_tokens = [tokenizer.decode([token_id]).strip() for token_id in top_token_ids]
            predicted_confidences = top_token_probs.tolist()

            axs[i].bar(predicted_tokens, predicted_confidences, color='blue')
            axs[i].set_xlabel('Predicted Tokens')
            axs[i].set_ylabel('Confidence')
            axs[i].set_title(f'Masked Token at Position {idx.item()}')
            axs[i].set_ylim(0, 1)

        plt.show()

def on_predict_button_click(b):
    masked_prediction(input_ids, permutation, permute_input_checkbox.value, permute_encoding_checkbox.value)

text_input = widgets.Text(placeholder='Write text here to encode.', description='Input:')
text_input.observe(lambda change: tokenize(change['new']), names='value')
tags_input = widgets.TagsInput(value=[], allowed_tags=[], allow_duplicates=False)

# Observe changes in tags order to update the permutation and prevent deletion
tags_input.observe(on_tags_change, names='value')
tags_input.observe(lambda change: update_permutation(change['new']), names='value')

# Create checkboxes for permute_input and permute_encoding
permute_input_checkbox = widgets.Checkbox(value=False, description='Permute Inputs')
permute_encoding_checkbox = widgets.Checkbox(value=False, description='Permute Encodings')

# Create a button to trigger the prediction
predict_button = widgets.Button(description=""Run Prediction"")
predict_button.on_click(on_predict_button_click)

# Display the widgets
display(text_input)
display(tags_input)
display(permute_input_checkbox)
display(permute_encoding_checkbox)
display(predict_button)
display(output)

","The model inputs have token ids and position ids. There are four scenarios to consider:

Baseline. Correct order for tokens and positions
Permute position ids only
Permute token ids only
Permute position ids and token ids

You are correct that scenario 1 and 4 should produce the same results. However you are incorrect in assuming that permuting tokens or positions separately should give the same result. Consider:
# Given:
tokens = [0, 1, 2]
positions = [0, 1, 2]
permutation = [2, 0, 1]

# Ex1: Permute tokens but not positions
[2, 0, 1] # permuted tokens
[0, 1, 2] # standard positions

# Ex2: Permute positions but not tokens
[0, 1, 2] # standard tokens
[2, 0, 1] # permuted positions

In Ex1, the model is told that token 2 occurs at position 0. In Ex2, the model is told that token 2 occurs at position 1. Even though we used the same permutation, the mapping of tokens to positions is different. This results in different model outputs.
The reason you sometimes see these results line up is because you can (through random chance) sample a permutation that results in token/position embeddings lining up the same way (or mostly the same way) when permuting just one of them. This is luck - the average case produces different results.
It is simple to test this. Huggingface models take a position_ids input parameter. We can use this to test permutations of the input ids without messing with the weight matrices.
To test this, we'll create input data, permute as needed, compute logits and compare logits.
When comparing logits, we will permute or depermute as needed to compare on a token to token basis. For example if token i in scenario 1 is permuted to token j in scenario 3, we want to compare logits i from scenario 1 to logits j in scenario 3.
import torch
from transformers import BertForMaskedLM, AutoTokenizer

def get_logits(inputs):
    with torch.no_grad():
        outputs = model(**inputs)  
        logits = outputs.logits
    return logits

def permute_inputs(inputs, permutation, permute_ids=True, permute_positions=True):
    outputs = {}
    for k,v in inputs.items():
        if k=='position_ids' and permute_positions:
            outputs[k] = v[permutation]
        elif k!='position_ids' and permute_ids:
            outputs[k] = v[:,permutation]
        else:
            outputs[k] = v
            
    return outputs

# load tokenizer/model
tokenizer = AutoTokenizer.from_pretrained(""bert-base-uncased"")
model = BertForMaskedLM.from_pretrained(""bert-base-uncased"")
model.eval() # remember to set model to eval

# create input ids and position ids
inputs = tokenizer('input text test sequence', return_tensors='pt')

inputs['position_ids'] = torch.tensor(list(range(inputs['input_ids'].shape[1])))

# create permutation tensor
permutation = torch.randperm(inputs['input_ids'].shape[1])

# compute scenario data
data = {
    's1' : { # scenario 1 - baseline
        'inputs' : inputs,
        'permuted_ids' : False
    },
    's2' : { # scenario 2 - permute positions only
        'inputs' : permute_inputs(inputs, permutation, permute_ids=False, permute_positions=True),
        'permuted_ids' : False
    },
    's3' : { # scenario 3 - permute token ids only
        'inputs' : permute_inputs(inputs, permutation, permute_ids=True, permute_positions=False),
        'permuted_ids' : True
    },
    's4' : { # scenario 4 - permute tokens and positions
        'inputs' : permute_inputs(inputs, permutation),
        'permuted_ids' : True
    }
}

# compute logits
for k,v in data.items():
    v['logits'] = get_logits(v['inputs'])

comparisons = [
    ['s1', 's2'],
    ['s1', 's3'],
    ['s1', 's4'],
    ['s2', 's3'],
    ['s2', 's4'],
    ['s3', 's4'],
]

# compare scenarios 
for sa, sb in comparisons:
    data_a = data[sa]
    data_b = data[sb]
    
    logits_a = data_a['logits']
    logits_b = data_b['logits']
    
    if data_a['permuted_ids'] == data_b['permuted_ids']:
        # either both logits are permuted or both logits are unpermuted
        # so we can compare directly
        val = (logits_a - logits_b).abs().mean()
    elif data_a['permuted_ids'] and (not data_b['permuted_ids']):
        # if `a` is permuted but `b` is not, we permute `b` to make tokens line up
        val = (logits_a - logits_b[:,permutation]).abs().mean()
    else:
        # otherwise we permute `b` to make tokens line up
        val = (logits_a[:,permutation] - logits_b).abs().mean()
        
    print(f""Comparison {sa}, {sb}: {val.item():.6f}"")

The code should produce an output like:
Comparison s1, s2: 1.407895
Comparison s1, s3: 1.583560
Comparison s1, s4: 0.000003
Comparison s2, s3: 1.750883
Comparison s2, s4: 1.407894
Comparison s3, s4: 1.583560

Run the code a bunch of times. You will find that the S1, S4 comparison always has a small deviation. This is because permuting tokens and positions together always produces the same result, ignoring small deviations caused by numeric issues.
You will find the S2, S3 comparison generally has a large deviation, but sometimes has a small deviation. As discussed, this is due to getting a lucky permutation where positions and ids mostly line up.
",huggingface-transformers
load_state_dict getting random results,"import os
os.environ[""CUDA_VISIBLE_DEVICES""]=""0""
import torch
from transformers import AutoTokenizer, AutoConfig, AutoModelForCausalLM, GPT2LMHeadModel
model = AutoModelForCausalLM.from_pretrained(
    ""gpt2"",
    device_map='auto',
)
model2 = GPT2LMHeadModel(model.config).to(""cuda"")
model2.load_state_dict(model.state_dict())
tokenizer = AutoTokenizer.from_pretrained(""gpt2"")

t = tokenizer(""hello_world"", return_tensors=""pt"")[""input_ids""].to(""cuda"")
a = model(t).logits
b = model2(t).logits
print(a - b)
print(a)
print(b)

model2 behaves very differently from the model (loss being much higher), but the model structures and parameters are exactly the same. From the output, it looks like something is randomized for model2. Could anyone tell what was going on? I have the ""accelerate"" package installed.
The config and the parameters are the same. I also checked the forward functions, and there is no difference at all. However, setting model2.transformer.forward = model.transformer.forward, and then the two models would behave the same.
","You need to set the models to eval mode to disable dropout if you want them to produce the same results
import os
os.environ[""CUDA_VISIBLE_DEVICES""]=""0""
import torch
from transformers import AutoTokenizer, AutoConfig, AutoModelForCausalLM, GPT2LMHeadModel

model = AutoModelForCausalLM.from_pretrained(
    ""gpt2"",
    device_map='auto',
)

model2 = GPT2LMHeadModel(model.config).to(""cuda"")
model2.load_state_dict(model.state_dict())

# set to eval
model.eval()
model2.eval()

tokenizer = AutoTokenizer.from_pretrained(""gpt2"")

t = tokenizer(""hello_world"", return_tensors=""pt"")[""input_ids""].to(""cuda"")
a = model(t).logits
b = model2(t).logits

assert (a == b).all()

",huggingface-transformers
why doesn39t permuting positional encodings in gpt2 affect the output as expected,"I'm trying to understand the role of positional encoding in the GPT-2 Transformer model. From what I understand, positional encodings are crucial because they give the model a sense of the order of tokens.
However, I'm confused about the behavior I'm observing:

Permuting Positional Encodings: When I permute the positional encodings while keeping the input tokens the same, the generated output barely changes. I expected significant changes since the positional information should alter the model’s understanding of token order.

Permuting Input Tokens: When I permute the input tokens (while permuting positional encodings in the same manner), the output changes significantly, but it doesn't revert to what it was with the original order.


This behavior is confusing because I expected the output to revert when both the positional encodings and tokens are permuted in the same way.
Could someone help clarify why this is happening? Is there something about how GPT-2 handles positional encoding that I'm missing? How can I modify my code to get the behavior I expect, where permuting both the positional encoding and input tokens in the same way results in the original output?
Thanks in advance!
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer

def permute_columns(matrix, permutation=None):
    n = len(permutation)
    first_n_columns = matrix[:, :n]
    permuted_columns = first_n_columns[:, permutation]
    remaining_columns = matrix[:, n:]
    new_matrix = torch.hstack((permuted_columns, remaining_columns))
    return new_matrix

model_name = ""gpt2""
model = AutoModelForCausalLM.from_pretrained(model_name)
tokenizer = AutoTokenizer.from_pretrained(model_name)

permutation = [0, 4, 2, 3, 1]
# permute positional encoding
model.transformer.wpe.weight.data = permute_columns(model.transformer.wpe.weight.data.T, permutation).T

input_text = ""The man ate the cow""
input_ids = tokenizer(input_text, return_tensors=""pt"").input_ids

# permute input
input_ids = permute_columns(input_ids, permutation)

outputs = model.generate(input_ids, max_length=50)
generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)

I attempted to permute the positional encodings in a GPT-2 model, expecting this to change the generated output. Additionally, I tried permuting the input tokens along with the positional encodings in the same manner, anticipating that the output would revert to the original.
What I Expected:

When permuting only the positional encodings, I expected the output to change significantly because the model should interpret the token order differently.
When permuting both the input tokens and positional encodings in the same way, I expected the output to revert to what it was with the original order.

What Actually Happened:

Permuting only the positional encodings resulted in minimal changes to the generated output, which was unexpected.

Permuting both the input tokens and positional encodings in the same manner led to a different output, but it did not revert to the original as I anticipated.


","You have to consider the effect of the causal attention mask.
CLMs like GPT-2 use an attention mask to prevent tokens from attending to tokens that come later in the sequence. This is important because if this wasn't the case, the model could ""look ahead"" and cheat at the next token prediction task. The attention mask restricts the model such that token i can only attend to tokens j <= i.
Say we have tokens [0, 1, 2, 3, 4]. Token 0 attends to itself. Token 1 attends to [0, 1] and so on.
Now consider your permutations.
If we permute the positional embeddings, we give the model slightly different signal, but the token order and attention order is still the same. Token 0 still attends to token 0. Token 1 still attends to [0, 1], and so on. As a result, the output is mostly similar to the base case.
Now we permute the token order, say to [3, 2, 0, 4, 1]. Token 3, which used to attend to [0, 1, 2, 3], can now only attend to itself. Token 2, which used to attend to [0, 1, 2] can now only attend to [3, 2]. Token permutation substantially changes what information is routed to what tokens, resulting in a substantial difference in model output.
If you want to look at the effect of token order and positional embeddings in isolation, you should use a BERT-style masked language model that does not use a causal attention mask.
",huggingface-transformers
spacy and gensim on jupyter notebooks,"I have bought a new macbook and have installed python 3.12.5 on it. I am trying to import and run libraries for Natural Langauge Processing in a Jupyter Notebook, and all is well... except when it comes to spaCy and gensim.
import spacy
import gensim

TypeError: ForwardRef._evaluate() missing 1 required keyword-only argument: 'recursive_guard'

I checked to see if they are imported with :
!pip show spacy
!pip show gensim

Name: spacy
Version: 3.7.5
Summary: Industrial-strength Natural Language Processing (NLP) in Python
Home-page: https://spacy.io
Author: Explosion
Author-email: contact@explosion.ai
License: MIT
Location: /opt/anaconda3/lib/python3.12/site-packages
Requires: catalogue, cymem, jinja2, langcodes, murmurhash, numpy, packaging, preshed, pydantic, requests, setuptools, spacy-legacy, spacy-loggers, srsly, thinc, tqdm, typer, wasabi, weasel
Required-by: 
Name: gensim
Version: 4.3.2
Summary: Python framework for fast Vector Space Modelling
Home-page: https://radimrehurek.com/gensim/
Author: Radim Rehurek
Author-email: me@radimrehurek.com
License: LGPL-2.1-only
Location: /opt/anaconda3/lib/python3.12/site-packages
Requires: numpy, scipy, smart-open
Required-by: 

So they should be loaded, but I don't know what the TypeError refers to and how to fix it!
I read that there is something with the python update...but could anyone help me clarify this?
","Problem occurs in pydantic library (to be exact pydantic.v1.typing module) the best solution at the moment is to downgrade python version to 3.12.3. Patches to this problem has been already merged to repository but you need to wait for next release.
",huggingface-transformers
does padding in a batch of sequences affect performance how effective is the attention mask,"In Transformer models, sequences of variable lengths are typically padded to the maximum length in a batch. However, if my sequence lengths vary significantly, the batch may contain a substantial amount of padding (potentially over 50%).
I am curious about the following:
When PyTorch computes the Transformer, do padding tokens impact calculation speed negatively?
Does the presence of the attention mask allow the model to effectively skip over padding tokens, resulting in only a minimal performance impact?
Overall, how effective is the attention mask? If I have a sparse attention mask with only 10% non-zero values, does the computation effectively reduce to approximately 10%?
Thank you for your insights!
","Attention is computed on a tensor of shape (batch_size, sequence_length, embedding_dimension). The compute and memory requirements scale with the size of those dimensions.
For an input of fixed size, the percent padding does not impact performance. There is some minor overhead from applying a padding mask at all (ie not having a padding mask saves you one mask fill operation), but between x% padding and y% padding you're not going to see a difference. The overall compute requirements are set by the tensor size.
With respect to batching sequences, there can be added inefficiencies for batching together sequences of wildly different length. Say you have 10 sequences of length 8 and 10 sequences of length 128. Now pad and batch those sequences into two batches. If you mix lengths evenly, you get two batches with a sequence length of 128. If you sort by length before batching, you get one batch with sequence length of 8 and another with length 128. The first case (two batches of sequence length 128) requires overall more compute compared to the second case (one batch of 8, one of 128).
That said, for a fixed input size, you aren't going to see a performance change from the percent padding. There is no way for the attention operation to ""skip over"" padding tokens. The conditional control flow required for that sort of approach doesn't work well with the way GPUs execute operations in parallel. The only effect of the padding mask is it assigns 0 attention weight to padding tokens.
",huggingface-transformers
huggingface valueerror expected sequence of length 165 at dim 1 got 128,"I am trying to fine-tune the BERT language model on my own data. I've gone through their docs, but their tasks seem to be not quite what I need, since my end goal is embedding text. Here's my code:
from datasets import load_dataset
from transformers import BertTokenizerFast, AutoModel, TrainingArguments, Trainer
import glob
import os


base_path = '../data/'
model_name = 'bert-base-uncased'
max_length = 512
checkpoints_dir = 'checkpoints'

tokenizer = BertTokenizerFast.from_pretrained(model_name, do_lower_case=True)


def tokenize_function(examples):
    return tokenizer(examples['text'], padding=True, truncation=True, max_length=max_length)


dataset = load_dataset('text',
        data_files={
            'train': f'{base_path}train.txt',
            'test': f'{base_path}test.txt',
            'validation': f'{base_path}valid.txt'
        }
)

print('Tokenizing data. This may take a while...')
tokenized_dataset = dataset.map(tokenize_function, batched=True)
train_dataset = tokenized_dataset['train']
eval_dataset = tokenized_dataset['test']

model = AutoModel.from_pretrained(model_name)

training_args = TrainingArguments(checkpoints_dir)

print('Training the model...')
trainer = Trainer(model=model, args=training_args, train_dataset=train_dataset, eval_dataset=eval_dataset)
trainer.train()

I get the following error:
  File ""train_lm_hf.py"", line 44, in <module>
    trainer.train()
...
  File ""/opt/conda/lib/python3.7/site-packages/transformers/data/data_collator.py"", line 130, in torch_default_data_collator
    batch[k] = torch.tensor([f[k] for f in features])
ValueError: expected sequence of length 165 at dim 1 (got 128)

What am I doing wrong?
","I fixed this solution by changing the tokenize function to:
def tokenize_function(examples):
    return tokenizer(examples['text'], padding='max_length', truncation=True, max_length=max_length)

(note the padding argument). Also, I used a data collator like so:
data_collator = DataCollatorForLanguageModeling(
    tokenizer=tokenizer, mlm=True, mlm_probability=0.15
)
trainer = Trainer(
        model=model,
        args=training_args,
        data_collator=data_collator,
        train_dataset=train_dataset,
        eval_dataset=eval_dataset
)

",huggingface-transformers
topp sampling not working cuda error deviceside assert triggered,"I was trying to re-implement the model.generate() function of transformers' models from huggingface. I did that so I could implement logit-bias, that normal function does not allow. But before I could reach that, I encountered a lot of problems with my top-p sampling.
Here's the code snippet:
generation_args = {
    ""max_new_tokens"": 500,
    ""temperature"": 0.4,  # Adjust temperature if needed for more or less randomness
    ""do_sample"": True,  # Enable sampling
    ""top_p"": 0.5,  # Set the cumulative probability for nucleus sampling
    ""top_k"": None,  # Optionally, you can set top_k if you want to use it alongside or instead of top_p
}


def top_p_filtering(logits, top_p):
    """"""Filter the logits using top-p (nucleus) sampling.""""""
    # Sort logits in descending order and get the sorted indices
    sorted_logits, sorted_indices = torch.sort(logits, descending=True)

    # Compute the cumulative probabilities of the sorted logits
    cumulative_probs = torch.cumsum(torch.nn.functional.softmax(sorted_logits, dim=-1), dim=-1)

    # Create a mask for the tokens to keep
    sorted_indices_to_keep = cumulative_probs <= top_p

    # Ensure that at least one token is kept (the first token, which has the highest logit)
    sorted_indices_to_keep[..., 0] = True

    # Filter out the tokens to remove by setting their logits to negative infinity
    logits[sorted_indices[~sorted_indices_to_keep]] = float('-inf')

    return logits


def custom_generate(input_ids, streamer, max_new_tokens, temperature, top_p):
    past_key_values = None
    attention_mask = torch.ones(input_ids.shape, device=input_ids.device)

    for _ in range(max_new_tokens):
        with torch.no_grad():
            outputs = model(
                input_ids=input_ids,
                past_key_values=past_key_values,
                attention_mask=attention_mask,
                use_cache=True
            )

        logits = outputs.logits[:, -1, :]  # Get logits of the last token

        # Apply temperature to logits
        if temperature != 1.0:
            logits = logits / temperature

        # Apply top-p sampling
        if top_p is not None and top_p < 1.0:
            logits = top_p_filtering(logits, top_p)
        print(""1"")
        next_token_probs = torch.nn.functional.softmax(logits, dim=-1)
        print(""2"")
        # Check if next_token_probs contains valid probabilities


        next_token_id = torch.multinomial(next_token_probs,
                                          num_samples=1)  
        print(""3"")
        streamer.put(next_token_id)  # Pass the tensor directly to the streamer

        input_ids = next_token_id  # Set the next input to the last generated token
        attention_mask = torch.cat(
            [attention_mask, torch.ones((attention_mask.shape[0], 1), device=attention_mask.device)], dim=1)

        past_key_values = outputs.past_key_values

        if next_token_id.item() == tokenizer.eos_token_id:  
            break

with torch.no_grad():
    custom_generate(input_ids, streamer, generation_args[""max_new_tokens""], generation_args[""temperature""], generation_args[""top_p""])

The error that I face:
../aten/src/ATen/native/cuda/IndexKernel.cu:92: operator(): block: [10,0,0], thread: [63,0,0] Assertion `-sizes[i] <= index && index < sizes[i] && ""index out of bounds""` failed.
Exception in thread Thread-18 (generate):
Traceback (most recent call last):
  File ""/usr/lib/python3.10/threading.py"", line 1016, in _bootstrap_inner
    self.run()
  File ""/usr/lib/python3.10/threading.py"", line 953, in run
    self._target(*self._args, **self._kwargs)
  File ""/mnt/c/Users/User/Documents/EmpatheticChatBot/Inference-Server.py"", line 130, in generate
    custom_generate(input_ids, streamer, generation_args[""max_new_tokens""], generation_args[""temperature""], generation_args[""top_p""])
  File ""/mnt/c/Users/User/Documents/EmpatheticChatBot/Inference-Server.py"", line 108, in custom_generate
    next_token_id = torch.multinomial(next_token_probs,
RuntimeError: CUDA error: device-side assert triggered
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

The entire problem arised only after adding top-p sampling.
I expected my sampling to work, as I have looked through my code maybe 30 times already. ChatGPT says this code is perfect, and that my error is really hard to debug. My hypothesis is that values are getting incorrectly filtered or setting them to ""bad"" values.
","The problem is the indexing you're doing at this line:
logits[sorted_indices[~sorted_indices_to_keep]] = float('-inf')
For reasons I'll explain, this is causing an index out of bounds error. Out of bounds indexing is a common cause of CUDA error: device-side assert triggered errors.
Consider the following:
import torch
import torch.nn as nn

torch.manual_seed(42)

top_p = 0.2

logits = torch.randn(8, 128) # random logits

# sort logits 
sorted_logits, sorted_indices = torch.sort(logits, descending=True)

# calculate cumulative probs
cumulative_probs = torch.cumsum(torch.nn.functional.softmax(sorted_logits, dim=-1), dim=-1)

# apply top p threshold to cumulative probs
sorted_indices_to_keep = cumulative_probs <= top_p

# ensure at least one index is kept
sorted_indices_to_keep[..., 0] = True

# this is the problem: logits[sorted_indices[~sorted_indices_to_keep]] = float('-inf')
print(logits.shape, sorted_indices[~sorted_indices_to_keep].shape)
> torch.Size([8, 128]) torch.Size([989])

When you index sorted_indices[~sorted_indices_to_keep], both inputs are of shape (8, 128), but the output is of shape (989,) (or similar depending on the random seed for the dummy logits).
This happens because the sorted_indices_to_keep has an irregular number of True values in each row. This means the indexing operation can't resolve the output into a clean 2D tensor where every row is the same size. Pytorch handles this situation by returning an unrolled vector of every True value from the indexing tensor.
This means when you try to compute logits[sorted_indices[~sorted_indices_to_keep]], you are using a long 1D tensor to index into a small 2D tensor. If you run this on CPU, you get an error like IndexError: index 20 is out of bounds for dimension 0 with size 8. When you run on GPU, you get the Cuda assert error.
To fix this, use the scatter operation. Use something like this:
def top_p_filtering(logits, top_p, shift_indices=True, debug=False):
    """"""Filter the logits using top-p (nucleus) sampling.""""""
    # Sort logits in descending order and get the sorted indices
    sorted_logits, sorted_indices = torch.sort(logits, descending=True)

    # Compute the cumulative probabilities of the sorted logits
    cumulative_probs = torch.cumsum(torch.nn.functional.softmax(sorted_logits, dim=-1), dim=-1)

    # Create a mask for the tokens to keep
    sorted_indices_to_keep = cumulative_probs <= top_p
    
    # Optional: shift indices to the right. This results in keeping the first 
    # token above the top_p threshold. Skip this line to ensure that all 
    # token probs are strictly below the top_p threshold
    if shift_indices:
        sorted_indices_to_keep[..., 1:] = sorted_indices_to_keep[..., :-1].clone()

    # Ensure that at least one token is kept (the first token, which has the highest logit)
    sorted_indices_to_keep[..., 0] = True
    
    # Use scatter to create top_p mask
    mask = sorted_indices_to_keep.scatter(dim=1, index=sorted_indices, src=sorted_indices_to_keep)
    
    # Optional debug check to make sure top_p is being honored
    # Note we need to compute probs before masking because applying softmax 
    # after masking will result in a distribution that sums to 1
    if debug:
        probs = torch.nn.functional.softmax(logits, dim=-1)
        probs[~mask] = 0
        print(probs.sum(-1))
    
    # Use mask to set logit vals to -inf
    logits[~mask] = float('-inf')

    return logits

",huggingface-transformers
mlflowtransformerslog_model does not finish,"Problem
I want to use mlflow.transformers.log_model() to log a finetuned huggingface model.
However, when the mlflow.transformers.log_model method is running, it simply does not finish - runs forever - throws no errors.
I suspect my configuration is not right, the model is too big?
The output says Skipping saving pretrained model weights to disk so that should not be the problem.
Any ideas how to do this properly?
Example
This is more or less how my setup looks like, you cannot run this, it includes some pseudocode...
I am on python 3.11.9 with transformers = ""^4.41.2"" & mlflow = ""^2.15.1"".
import mlflow
import torch
from peft import LoraConfig
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    BitsAndBytesConfig,
    TrainingArguments,
)
from trl import SFTTrainer, setup_chat_format

train_dataset = ...
eval_dataset = ...

model_id = ""LeoLM/leo-hessianai-7b-chat-bilingual""

# Load model and tokenizer
model = AutoModelForCausalLM.from_pretrained(
    model_id,
    device_map=""auto"",
    torch_dtype=torch.bfloat16,
    quantization_config=bnb_config,
)
tokenizer = AutoTokenizer.from_pretrained(model_id)
tokenizer_no_pad = AutoTokenizer.from_pretrained(model_id, add_bos_token=True)
model, tokenizer = setup_chat_format(model, tokenizer)
peft_config = LoraConfig(...)
args = TrainingArguments(...)

# Define Trainer
trainer = SFTTrainer(
    model=model,
    args=args,
    train_dataset=train_dataset,
    eval_dataset=eval_dataset,
    peft_config=peft_config,
    tokenizer=tokenizer,
    packing=True,
)

# mlflow
mlflow.set_experiment(""my_experiment"")
with mlflow.start_run() as run:
    mlflow.transformers.autolog()
    trainer.train()
    
     components = {
         ""model"": trainer.model,
         ""tokenizer"": tokenizer_no_pad,
     }
     # !!! This function all does not finish... !!!
     mlflow.transformers.log_model(
         transformers_model=components,
         artifact_path=""model"",
    )

The last output I get in the console is:
INFO mlflow.transformers: Overriding save_pretrained to False for PEFT models, following the Transformers behavior. The PEFT adaptor and config will be saved, but the base model weights will not and reference to the HuggingFace Hub repository will be logged instead.
Unrecognized keys in `rope_scaling` for 'rope_type'='linear': {'type'}
/mypath/llm4pa-open-source/.venv/lib/python3.11/site-packages/peft/utils/save_and_load.py:209: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.
  warnings.warn(
2024/08/12 18:21:14 INFO mlflow.transformers: Skipping saving pretrained model weights to disk as the save_pretrained is set to False. The reference to HuggingFace Hub repository LeoLM/leo-hessianai-7b-chat-bilingual will be logged instead.
/mypath/llm4pa-open-source/.venv/lib/python3.11/site-packages/_distutils_hack/__init__.py:26: UserWarning: Setuptools is replacing distutils.
  warnings.warn(""Setuptools is replacing distutils."")

","Before defining the trainer, the model has be turned into a Peft model object via get_peft_model, then the mlflow.transformers.log_model works:
from peft import LoraConfig, get_peft_model

model = ...
peft_config = LoraConfig(...)
args = TrainingArguments(...)

peft_model = get_peft_model(model, peft_config)

trainer = SFTTrainer(
    model=peft_model,
    args=args,
    ...
)


# mlflow
mlflow.set_experiment(""my_experiment"")
with mlflow.start_run() as run:
    mlflow.transformers.autolog()
    trainer.train()
    
     components = {
         ""model"": trainer.model,
         ""tokenizer"": tokenizer_no_pad,
     }
     # !!! Now the logginig of the model works, we can find it in the artifacts !!!
     mlflow.transformers.log_model(
         transformers_model=components,
         artifact_path=""model"",
    )

",huggingface-transformers
runtimeerror numpy is not available transformers,"I basically just want to use the transformers pipeline() to classify data, but independent of which model I try to use, it returns the same error, stating Numpy is not available
Code I'm running:
pipe = pipeline(""text-classification"", model=""AdamLucek/roberta-llama3.1405B-twitter-sentiment"")   
sentiment_pipeline('Today is a great day!')
# other model i've tried: 
sentiment_pipeline = pipeline(model=""cardiffnlp/twitter-roberta-base-sentiment-latest"", tokenizer=""cardiffnlp/twitter-roberta-base-sentiment-latest"")
sentiment_pipeline('Today is a great day!')

Error I receive:
RuntimeError                              Traceback (most recent call last)
Cell In[49], line 1
----> 1 sentiment_pipeline('Today is a great day!')

File ~\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\LocalCache\local-packages\Python312\site-packages\transformers\pipelines\text_classification.py:156, in TextClassificationPipeline.__call__(self, inputs, **kwargs)
    122 """"""
    123 Classify the text(s) given as inputs.
    124 
   (...)
    153     If `top_k` is used, one such dictionary is returned per label.
    154 """"""
    155 inputs = (inputs,)
--> 156 result = super().__call__(*inputs, **kwargs)
    157 # TODO try and retrieve it in a nicer way from _sanitize_parameters.
    158 _legacy = ""top_k"" not in kwargs

File ~\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\LocalCache\local-packages\Python312\site-packages\transformers\pipelines\base.py:1257, in Pipeline.__call__(self, inputs, num_workers, batch_size, *args, **kwargs)
   1249     return next(
   1250         iter(
   1251             self.get_iterator(
   (...)
   1254         )
   1255     )
   1256 else:
-> 1257     return self.run_single(inputs, preprocess_params, forward_params, postprocess_params)

File ~\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\LocalCache\local-packages\Python312\site-packages\transformers\pipelines\base.py:1265, in Pipeline.run_single(self, inputs, preprocess_params, forward_params, postprocess_params)
   1263 model_inputs = self.preprocess(inputs, **preprocess_params)
   1264 model_outputs = self.forward(model_inputs, **forward_params)
-> 1265 outputs = self.postprocess(model_outputs, **postprocess_params)
   1266 return outputs

File ~\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\LocalCache\local-packages\Python312\site-packages\transformers\pipelines\text_classification.py:208, in TextClassificationPipeline.postprocess(self, model_outputs, function_to_apply, top_k, _legacy)
    204 outputs = model_outputs[""logits""][0]
    206 if self.framework == ""pt"":
    207     # To enable using fp16 and bf16
--> 208     outputs = outputs.float().numpy()
    209 else:
    210     outputs = outputs.numpy()

RuntimeError: Numpy is not available

I already tried simply un- and reinstalling transformers and numpy and for both the most recent versions are installed (and should be compatible).
Anyone has an idea on how to solve this?
","Try:
pip install ""numpy<2""

then restart the kernel.
",huggingface-transformers
how to ask multiple choice questions using pipeline,"I want to ask multiple choice questions to the Hugging Face transformer pipeline; however, there does not seem to be a good task choice for this.
I am referencing this:
qa_pipeline = pipeline(""question-answering"", model=""distilbert-base-uncased"")
result = qa_pipeline(question = question, context = context)

I know question-answering exists; however, this task requires context where then the pipeline would extract the answer from the context. For specifically multiple choice answering, this method does not make much sense because I am trying to recieve a specific letter, not a word associated with the letter. Additionally, I could possiblely try text-generation, but I was hoping there was some better solution than this. Let me know your thoughts!
","You could hand multiple sentence pairs to the model and then infer which one gives you the highest probability:
https://huggingface.co/docs/transformers/tasks/multiple_choice#inference
In your case prompt would be the question, and each answer option one of the candidates.
",huggingface-transformers
how to make huggingface transformer for translation return n translation inferences,"So I am trying to use this transformer from huggingface https://huggingface.co/docs/transformers/en/tasks/translation. The issue is that I want n translations returned and not just one. How can I  do that? I mean, I want to have ordered translations, that means the translation with index 0 would have the highest confidence, this is important for my use case, which is about translating natural language to commands language (about 40 commands without subcommands).
The github repo and exact model is this one https://github.com/google-research/text-to-text-transfer-transformer/blob/main/t5/models/hf_model.py
This is the HuggingFace API:
translator = pipeline(""translation_xx_to_yy"", model=""my_awesome_opus_books_model"")
translator(text)

But I am intending to use the model directly from the google search github repo, so it seems some tweaking should be done here:
predictions = []
    for batch in dataset:
      predicted_tokens = self._model.generate(
          input_ids=self.to_tensor(batch[""inputs""]), **generate_kwargs
      )
      predicted_tokens = predicted_tokens.cpu().numpy().tolist()
      predictions.extend(
          [vocabs[""targets""].decode(p) for p in predicted_tokens]
      )

    for inp, pred in zip(inputs, predictions):
      logging.info(""%s\n  -> %s"", inp, pred)

    if output_file is not None:
      utils.write_lines_to_file(predictions, output_file)

Also any suggestion on some other model option to solve this natural language to cmd is welcomed!
","Check out the documentation of the generate method: https://huggingface.co/docs/transformers/generation_strategies#customize-text-generation
The parameter to use is num_return_sequences. But T5 by default does a greedy search, meaning it generates word by word and discards the options on its path there. To generate multiple options you need a selection of alternative paths. There are basically two ways to do this (my guess would be that for your case the first option works better):
If you activate do_sample, the model will not just pick the highest probability token at each time, but instead take a weighted sample from the distribution of next word probabilities.
predicted_tokens = self._model.generate(
          input_ids=self.to_tensor(batch[""inputs""]), num_return_sequences=3, do_sample=True, **generate_kwargs
      )

If you set num_beams to anything larger than 1, you switch to beam search, where for each further token the model follows multiple alternatives of next tokens.
predicted_tokens = self._model.generate(
          input_ids=self.to_tensor(batch[""inputs""]), num_return_sequences=3, num_beams=4, **generate_kwargs
      )  # note that num_return_sequences has to be smaller or equal to num_beans

To also get the scores of generated outputs you can additionally use the arguments output_scores=True and return_dict_in_generate=True, although you should note that these will return the logits of all individual tokens, which you then would have to put together to the overall probability yourself, check out https://stackoverflow.com/a/75029986/18189622.
In general, T5 might not be the best model for code synthesis, as of my knowledge it wasn't pretrained or fine-tuned on it in its Multi-Task Instruction Fine-Tuning. There is however FLAN-T5, which was fine-tuned on a wider range of tasks, including Code Synthesis. There are also CodeT5 and many other models relating to code synthesis.
",huggingface-transformers
how do i create a question answering model in huggingfaces that answers questions about my data,"I would like to develop a question-answering model with hugging interfaces that answers questions about my input data. Unfortunately, I'm quite new to Python and also to transformers, so I need some basic help.
What I have tried
I have made a few attempts in Python, unfortunately not entirely successfully. For example, I converted my data into a small continuous text, but the answers provided by the model are incorrect:
# Create loader
from transformers import AutoTokenizer, AutoModelForQuestionAnswering
import torch

tokenizer = AutoTokenizer.from_pretrained(""bert-large-uncased-whole-word-masking-finetuned-squad"")
model = AutoModelForQuestionAnswering.from_pretrained(""bert-large-uncased-whole-word-masking-finetuned-squad"")

text = """"""
2011 wurden in Deutschland 1.296.349 Euro für Forschung und Entwicklung in Architektur- und Ingenieurbüros sowie für technische Untersuchungen ausgegeben. 1987 gab es im gesamten Wirtschaftssektor 295.332 Vollzeitäquivalente (VZÄ) im Bereich Forschung und Entwicklung. 2016 wurden im Sektor der freiberuflichen, wissenschaftlichen und technischen Dienstleistungen 47.551 VZÄ gezählt. Im Bergbau und in der Steingewinnung wurden im selben Jahr 21.318 Tausend Euro für interne Forschungs- und Entwicklungsarbeiten ausgegeben. Im Maschinenbau wurden 2017 49.323 VZÄ im Bereich Forschung und Entwicklung verzeichnet. 2004 betrugen die internen Forschungs- und Entwicklungsausgaben im gesamten Wirtschaftssektor 38.363.000 Tausend Euro.

2014 wurden in der Architektur und verwandten Bereichen 84.855 Tausend Euro für externe Forschungs- und Entwicklungsaufwendungen ausgegeben. 2016 wurden im Luft- und Raumfahrzeugbau 1.732.000 Tausend Euro für interne Forschung und Entwicklung aufgewendet. In den Finanz- und Versicherungsdienstleistungen wurden 2014 318.000 Tausend Euro und 2010 in der Herstellung von Glas, Keramik sowie in der Verarbeitung von Steinen 285.334 Tausend Euro für interne Forschung und Entwicklung verzeichnet.
""""""

questions = [
    ""Wie hoch waren die Ausgaben für Forschung und Entwicklung in Architektur- und Ingenieurbüros im Jahr 2011?"",
    ""Wie viele VZÄ wurden 2017 im Maschinenbau verzeichnet?"",
]

for question in questions:
    inputs = tokenizer.encode_plus(question, text, add_special_tokens=True, return_tensors=""pt"")
    input_ids = inputs[""input_ids""].tolist()[0]

    outputs = model(**inputs)
    answer_start_scores, answer_end_scores = outputs.start_logits, outputs.end_logits

    answer_start = torch.argmax(answer_start_scores)
    answer_end = torch.argmax(answer_end_scores) + 1

    if answer_end > answer_start and answer_start != 0:  # Zusätzliche Überprüfung hier
        answer = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(input_ids[answer_start:answer_end]))
    else:
        answer = ""No clear answer found.""

    print(f""Question: {question}"")
    print(f""Answer: {answer}\n"")

What I would like to achieve
I have this data.
As I said, I would like to build a chatbot that answers my questions about this data, e.g:
Question: Wie Hoch waren die Aufwendungen für FuE-Personal im Bereich ""H.v. DV-Geräten, elektronischen u. opt. Erzeugnissen"" im Jahr 2011?
Answer: 54647 VZÄ
or
Question: Wie Hoch waren die Aufwendungen für FuE-Personal im Bereich ""H.v. elektrischen Ausrüstungen"" im Jahr 2017?
Answer: 19534 VZÄ
(The relevant lines are highlighted in orange in the xlsx.-file)
I would be very grateful for a short example that helps me!
","Your question is hard to answer as there are several things to consider. The simplest thing to do would be:

Install ollama https://ollama.com/
Run this code:

import ollama
MODEL_ID = ""llama3.1:8b-instruct-q4_0""
ollama.pull(MODEL_ID)

text = """"""
2011 wurden in Deutschland 1.296.349 Euro für Forschung und Entwicklung in Architektur- und Ingenieurbüros sowie für technische Untersuchungen ausgegeben. 1987 gab es im gesamten Wirtschaftssektor 295.332 Vollzeitäquivalente (VZÄ) im Bereich Forschung und Entwicklung. 2016 wurden im Sektor der freiberuflichen, wissenschaftlichen und technischen Dienstleistungen 47.551 VZÄ gezählt. Im Bergbau und in der Steingewinnung wurden im selben Jahr 21.318 Tausend Euro für interne Forschungs- und Entwicklungsarbeiten ausgegeben. Im Maschinenbau wurden 2017 49.323 VZÄ im Bereich Forschung und Entwicklung verzeichnet. 2004 betrugen die internen Forschungs- und Entwicklungsausgaben im gesamten Wirtschaftssektor 38.363.000 Tausend Euro.

2014 wurden in der Architektur und verwandten Bereichen 84.855 Tausend Euro für externe Forschungs- und Entwicklungsaufwendungen ausgegeben. 2016 wurden im Luft- und Raumfahrzeugbau 1.732.000 Tausend Euro für interne Forschung und Entwicklung aufgewendet. In den Finanz- und Versicherungsdienstleistungen wurden 2014 318.000 Tausend Euro und 2010 in der Herstellung von Glas, Keramik sowie in der Verarbeitung von Steinen 285.334 Tausend Euro für interne Forschung und Entwicklung verzeichnet.
""""""

questions = [
    ""Wie hoch waren die Ausgaben für Forschung und Entwicklung in Architektur- und Ingenieurbüros im Jahr 2011?"",
    ""Wie viele VZÄ wurden 2017 im Maschinenbau verzeichnet?"",
]

system_prompt = f""Du bist ein Deutsch sprechender AI assistent der Nutzern Fragen über folgenden Inhalt beantwortet: \n{text}\n Antworte kurz und immer auf Deutsch!.""

answers = []
for question in questions:
    messages = [{""role"": ""user"", ""content"": system_prompt},{""role"": ""user"", ""content"": question}]
    response = ollama.chat(model=MODEL_ID, messages=messages, options={'temperature': 0.1, 'num_predict': 256, ""top_p"": 0.9})
    answers.append(response['message']['content'])
print(answers)

Output:
[
'Die Ausgaben für Forschung und Entwicklung in Architektur- und Ingenieurbüros beliefen sich im Jahr 2011 auf 1.296.349 Euro.', 
'49.323 Vollzeitäquivalente (VZÄ) wurden 2017 im Bereich Forschung und Entwicklung im Maschinenbau verzeichnet.'
]

If this does not work, change MODEL_ID to ""llama3.1:8b-instruct-fp16"", this will require more RAM. And to build a fully fledged Q&A chatbot, you might need a RAG pipeline to prefilter your ""text"".
",huggingface-transformers
how to prepare custom training data for donut document understanding transformer,"I want to train Hugging Face's Donut (Document Understanding Transformer) but I need help in creating the training data.
Donut github: https://github.com/clovaai/donut
Donut official documentation: https://huggingface.co/docs/transformers/main/en/model_doc/donut
If anybody has already created and trained the model, kindly help.
","Understanding how to label the training data was a bit confusing in the beginning but after reading this, it became clear.
Donut treats all tasks as JSON prediction problems. Ensure that your dataset follow this structure:
dataset_name
├── test
│   ├── metadata.jsonl
│   ├── {image_path0}
│   ├── {image_path1}
│             .
│             .
├── train
│   ├── metadata.jsonl
│   ├── {image_path0}
│   ├── {image_path1}
│             .
│             .
└── validation
    ├── metadata.jsonl
    ├── {image_path0}
    ├── {image_path1}
              .
              .

For Document Information Extraction, each line in metadata.jsonl should look like this:
{""file_name"": {image_path0}, ""ground_truth"": ""{\""gt_parse\"": {ground_truth_parse}, ... {other_metadata_not_used} ... }""}
{""file_name"": {image_path1}, ""ground_truth"": ""{\""gt_parse\"": {ground_truth_parse}, ... {other_metadata_not_used} ... }""}

The model ignores other metadata and focuses on the gt_parse or gt_parses field to predict the JSON task.
For other tasks like Document Visual Question Answering, refer to the GitHub link.
",huggingface-transformers
can39t suppress warning from transformerssrctransformersmodeling_utilspy,"My implementation for the AutoModel AutoTokenizer classes are fairly simple:
from transformers import AutoModel, AutoTokenizer
import numpy as np
from rank_bm25 import BM25Okapi
from sklearn.neighbors import NearestNeighbors

class EmbeddingModels:

    def bert(self, model_name, text):
        tokenizer = AutoTokenizer.from_pretrained(model_name)
        model = AutoModel.from_pretrained(model_name)
        inputs = tokenizer(text, return_tensors=""pt"", truncation=True, padding=True)
        outputs = model(**inputs)
        embeddings = outputs.last_hidden_state.mean(dim=1).detach().numpy()
        return embeddings
    
    def create_chunks(self, text, chunk_size):
        return [text[i:i + chunk_size] for i in range(0, len(text), chunk_size)]

But I can't get this warning to go away:
A parameter name that contains 'beta' will be renamed internally to 'bias'. 
Please use a different name to suppress this warning.
A parameter name that contains 'gamma' will be renamed internally to 'weight'. 
Please use a different name to suppress this warning.

There is no reference to the word beta or gamma anywhere in my repo.
Updating the package, suppressing the warnings with import warnings
","Before loading from pretrained model set transformers logger level to error as shown below. It sure is really frustrating not being able to leverage the warnings library filter
    loggers = [logging.getLogger(name) for name in logging.root.manager.loggerDict]
    for logger in loggers:
        if ""transformers"" in logger.name.lower():
            logger.setLevel(logging.ERROR)

    # now you can load state dict from pretrained
    model = transformers.BertModel.from_pretrained(
        ""bert-base-uncased"",
        use_safetensors=True,
        return_dict=False,
        attn_implementation=""sdpa"",
    )

",huggingface-transformers
received server error 500 while deploying huggingface model on sgaemaker,"I've successfully fine tuned a sentence-transformers model all-MiniLM-L12-v2 on our data in SageMaker Studio and the model was saved in S3 as a model.tar.gaz.
I want to deploy this model for inference (all code snippets included below). According to HuggingFace doc these types of model required a Custom Inference module. So I've downloaded and unpacked the model.tar.gz created, then followed the tutorial to add the code/inference.py and pushed it back to S3 as new model.tar.gz
The endpoint is created successfully, but as soon as I call the predictor.predict() it crashes with the following error:
ModelError: An error occurred (ModelError) when calling the InvokeEndpoint operation: Received server error (500) from primary with message ""{
  ""code"": 500,
  ""type"": ""InternalServerException"",
  ""message"": ""Worker died.""
}

looking in CloudWatch I got a lot of info messages, where the instance seems to be setting up successfully then I get this warning message:
2024-07-30T13:19:09,702 [WARN ] W-9000-model_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: model, error: Worker died.

Here are the relevant code snippets:
End point creation:
from sagemaker.huggingface.model import HuggingFaceModel
from sagemaker import get_execution_role, image_uris

role            = get_execution_role()
estimator_image = image_uris.retrieve(framework='pytorch',region='eu-west-1',version='2.0.0',py_version='py310',image_scope='inference', instance_type='ml.g5.4xlarge')
sm_model_ref    = model_path

# create Hugging Face Model Class
huggingface_model = HuggingFaceModel(
    model_data    = sm_model_ref,
    role          = role,                                                     
    image_uri     = estimator_image,
)

The custom inference.py file and its location in the model.tar.gz:
%%writefile models/model/code/inference.py

import torch

# Create a custom inference to overwrite the default method
def predict_fn(data, model):

    # create sentences pair
    sentences1 = data[""premise""]
    sentences2 = data[""hypothesis""]
 
    # Compute token embeddings
    with torch.no_grad():
        embeddings1 = model.encode(sentences1)
        embeddings2 = model.encode(sentences2)
        
        # Compute cosine similarities        
        similarities = model.similarity(embeddings1, embeddings2)
 
    return similarities

And its location:
model.tar.gz
 |_ _ 1_Pooling
 |_ _ 2_Normalize
 |_ _ checkpoint-8300
 |_ _ checkpoint-8334
 |_ _ code
   |_ _ inference.py
 |_ _ config_sentence_transformers.json
 |_ _ config.json
 |_ _ model.safetensors
 |_ _ module.json
 |_ _ README.md
 |_ _ sentence_bert_config.json
 |_ _ special_token_map.json
 |_ _ tokenizer_config.json
 |_ _ tokenizer.json
 |_ _ vocab.txt

","It seems most of the doc on the topic, including HuggingFace doc was out of date. You no longer need the repackage the model.tar.gz with code/inference.py
All I had to do was pass the S3 path to my initial model.tar.gz after training to the estimator, and pass the location of inference.py and requirements.txt in the source_dir and entry_point.
huggingface_model = HuggingFaceModel(
    entry_point   = 'inference.py',
    source_dir    = 'code',
    model_data    = sm_model_ref,
    role          = role,                                                     # IAM role with permissions to create an endpoint
    image_uri     = estimator_image,
)

",huggingface-transformers
colab not enough ram to load llama 3,"I was following a tutorial in Youtube, when wanted to load Llama3 8B:
model_name = ""meta-llama/Meta-Llama-3-8B-Instruct""

tokenizer = AutoTokenizer.from_pretrained(model_name, use_auth_token=hugging_face_key)
model = AutoModelForCausalLM.from_pretrained(model_name, use_auth_token=hugging_face_key)

Got:
Your session has failed because all available RAM has been used

Tried: model = AutoModelForCausalLM.from_pretrained(model_name, use_auth_token=hugging_face_key, low_cpu_mem_usage=True)
But again the same error
","Given that it’s an 8-billion-parameter model, requiring approximately 16GB of space, free Colab notebooks lack the capacity to load it.
",huggingface-transformers
how can i export a tokenizer from huggingface transformers to coreml,"I load a tokenizer and a Bert model from Huggingface transformers, and export the Bert model to CoreML:
from transformers import AutoTokenizer, AutoModelForTokenClassification
import torch

# Load the tokenizer
tokenizer = AutoTokenizer.from_pretrained(""huawei-noah/TinyBERT_General_4L_312D"")

# Load the model
model = AutoModelForTokenClassification.from_pretrained(""huawei-noah/TinyBERT_General_4L_312D"")

# Example usage
text = ""Hugging Face is creating a tool that democratizes AI.""
inputs = tokenizer(text, return_tensors=""pt"")

Requirements:
pip install transformers torch

How can I export a tokenizer from Huggingface transformers to CoreML?
","This is the BERT tokenizer I used and it works well. A lot of this is from Zach Nagengast and Julien Chaumond. Hope it helps! All you need is a vocab.txt file of the tokenizer's vocab which can be found here - https://huggingface.co/google-bert/bert-base-cased/blob/main/vocab.txt
# Credit to Julien from Huggingface

import Foundation

enum TokenizerError: Error {
    case tooLong(String)
}

class BertTokenizer {
    private let basicTokenizer = BasicTokenizer()
    private let wordpieceTokenizer: WordpieceTokenizer
    private let maxLen = 512
    
    private let vocab: [String: Int]
    private let ids_to_tokens: [Int: String]
    
    init() {
        let url = Bundle.main.url(forResource: ""vocab"", withExtension: ""txt"")!
        let vocabTxt = try! String(contentsOf: url)
        let tokens = vocabTxt.split(separator: ""\n"").map { String($0) }
        var vocab: [String: Int] = [:]
        var ids_to_tokens: [Int: String] = [:]
        for (i, token) in tokens.enumerated() {
            vocab[token] = i
            ids_to_tokens[i] = token
        }
        self.vocab = vocab
        self.ids_to_tokens = ids_to_tokens
        self.wordpieceTokenizer = WordpieceTokenizer(vocab: self.vocab)
    }
    
    
    func tokenize(text: String) -> [String] {
        var tokens: [String] = []
        for token in basicTokenizer.tokenize(text: text) {
            for subToken in wordpieceTokenizer.tokenize(word: token) {
                tokens.append(subToken)
            }
        }
        return tokens
    }
    
    private func createAttentionMask(tokenIds: [Int]) -> [Int] {
        return tokenIds.map { $0 != 0 ? 1 : 0 }
    }
    
    private func convertTokensToIds(tokens: [String]) -> [Int] {
        if tokens.count > maxLen {
            let truncatedTokens = Array(tokens.prefix(maxLen))
            return truncatedTokens.map { vocab[$0]! }
        } else {
            return tokens.map { vocab[$0]! }
        }
    }
    
    private func padSequence(_ sequence: [Int], toLength length: Int, paddingValue: Int = 0) -> [Int] {
        if sequence.count >= length {
            return Array(sequence.prefix(length))
        } else {
            return sequence + Array(repeating: paddingValue, count: length - sequence.count)
        }
    }
    
    /// Main entry point
    func tokenizeToIds(text: String, maxLength: Int = 512) -> (tokenIds: [Int], attentionMask: [Int]) {
        let tokens = [""[CLS]""] + tokenize(text: text) + [""[SEP]""]
        var tokenIds = convertTokensToIds(tokens: tokens)
        tokenIds = padSequence(tokenIds, toLength: maxLength)
        let attentionMask = createAttentionMask(tokenIds: tokenIds)
        return (tokenIds, attentionMask)
    }
    
    func tokenToId(token: String) -> Int {
        return vocab[token]!
    }
    
    /// Un-tokenization: get tokens from tokenIds
    func unTokenize(tokens: [Int]) -> [String] {
        return tokens.map { ids_to_tokens[$0]! }
    }
    
    /// Un-tokenization:
    func convertWordpieceToBasicTokenList(_ wordpieceTokenList: [String]) -> String {
        var tokenList: [String] = []
        var individualToken: String = """"
        
        for token in wordpieceTokenList {
            if token.starts(with: ""##"") {
                individualToken += String(token.suffix(token.count - 2))
            } else {
                if individualToken.count > 0 {
                    tokenList.append(individualToken)
                }
                
                individualToken = token
            }
        }
        
        tokenList.append(individualToken)
        
        return tokenList.joined(separator: "" "")
    }
}



class BasicTokenizer {
    let neverSplit = [
        ""[UNK]"", ""[SEP]"", ""[PAD]"", ""[CLS]"", ""[MASK]""
    ]
    
    func tokenize(text: String) -> [String] {
        let splitTokens = text.folding(options: .diacriticInsensitive, locale: nil)
            .components(separatedBy: NSCharacterSet.whitespaces)
        let tokens = splitTokens.flatMap({ (token: String) -> [String] in
            if neverSplit.contains(token) {
                return [token]
            }
            var toks: [String] = []
            var currentTok = """"
            for c in token.lowercased() {
                if c.isLetter || c.isNumber || c == ""°"" {
                    currentTok += String(c)
                } else if currentTok.count > 0 {
                    toks.append(currentTok)
                    toks.append(String(c))
                    currentTok = """"
                } else {
                    toks.append(String(c))
                }
            }
            if currentTok.count > 0 {
                toks.append(currentTok)
            }
            return toks
        })
        return tokens
    }
}


class WordpieceTokenizer {
    private let unkToken = ""[UNK]""
    private let maxInputCharsPerWord = 100
    private let vocab: [String: Int]
    
    init(vocab: [String: Int]) {
        self.vocab = vocab
    }
    
    func substr(_ s: String, _ r: Range<Int>) -> String? {
        let stringCount = s.count
        if stringCount < r.upperBound || stringCount < r.lowerBound {
            return nil
        }
        let startIndex = s.index(s.startIndex, offsetBy: r.lowerBound)
        let endIndex = s.index(startIndex, offsetBy: r.upperBound - r.lowerBound)
        return String(s[startIndex..<endIndex])
    }
    
    func tokenize(word: String) -> [String] {
        if word.count > maxInputCharsPerWord {
            return [unkToken]
        }
        var outputTokens: [String] = []
        var isBad = false
        var start = 0
        var subTokens: [String] = []
        while start < word.count {
            var end = word.count
            var cur_substr: String? = nil
            while start < end {
                var substr = substr(word, start..<end)!
                if start > 0 {
                    substr = ""##\(substr)""
                }
                if vocab[substr] != nil {
                    cur_substr = substr
                    break
                }
                end -= 1
            }
            if cur_substr == nil {
                isBad = true
                break
            }
            subTokens.append(cur_substr!)
            start = end
        }
        if isBad {
            outputTokens.append(unkToken)
        } else {
            outputTokens.append(contentsOf: subTokens)
        }
        return outputTokens
    }
}

",huggingface-transformers
panicexception addedvocabulary bad split after adding tokens to berttokenizer,"I use a BertTokenizer and add my custom tokens using add_tokens() function.
Minimal sample code here:
checkpoint = 'fnlp/bart-base-chinese'
tokenizer = BertTokenizer.from_pretrained(checkpoint)
tokenizer.add_tokens([""Token1"", ""Token2""]) # just some samples, I added a million tokens
model = BartForConditionalGeneration.from_pretrained(checkpoint, output_attentions = True, output_hidden_states = True)

training_args = Seq2SeqTrainingArguments(
    output_dir = output_model,
    evaluation_strategy = ""epoch"",
    optim = ""adamw_torch"", 
    eval_steps = 1000,
    save_strategy = ""epoch"",
    per_device_train_batch_size = batch_size,
    per_device_eval_batch_size = batch_size,
    weight_decay = 0.01,
    save_total_limit = 1,
    num_train_epochs = 30, 
    predict_with_generate=True,
    remove_unused_columns=True,
    fp16 = True,
    metric_for_best_model = ""bleu"",
    load_best_model_at_end = True,
)

trainer = Seq2SeqTrainer(
    model = model,
    args = training_args,
    train_dataset = train_data,
    eval_dataset = eval_data, 
    tokenizer = tokenizer, # I use the tokenizer with added tokens here
    data_collator = data_collator,
    compute_metrics = compute_metrics,
    callbacks = [EarlyStoppingCallback(early_stopping_patience=3)]
)
trainer.train()
trainer.push_to_hub(output_model, private=True)

The training process was completed without a problem. But when I use the new model in a pipeline, there is a high chance that the exception: PanicException: AddedVocabulary bad split has occurred. Here is the pipeline code:
text = ""Words to translate""

from transformers import pipeline, BertTokenizer

hf_model_name = ""my_huggingface_username/"" + output_model

translator = pipeline(""translation"", model=hf_model_name, max_length=200)
print(translator(text)[0]['translation_text'].replace(' ', ''))

I cannot find a pattern and cause of why the exception happens. How can I resolve this PanicException problem?
","The PanicException is resolved when changing the pipeline from:
translator = pipeline(""translation"", model=hf_model_name, max_length=200)
print(translator(text)[0]['translation_text'].replace(' ', ''))

to:
custom_tokenizer = BertTokenizer.from_pretrained(hf_model_name)
translator = pipeline(""translation"", model=custom_tokenizer, max_length=200)
print(translator(text)[0]['translation_text'].replace(' ', ''))

The pipeline function uses AutoTokenizer instead of BertTokenizer, which leads to the PanicException.
From the source code:

If not provided, the default tokenizer for the given model will be loaded (if it is a string). If model is not specified or not a string, then the default tokenizer for config is loaded (if it is a string). However, if config is also not given or not a string, then the default tokenizer for the given task will be loaded.

From the actual code, it uses AutoTokenizer, which caused the problem.
tokenizer = AutoTokenizer.from_pretrained(
    tokenizer_identifier, use_fast=use_fast, _from_pipeline=task, **hub_kwargs, **tokenizer_kwargs
)

",huggingface-transformers
attributeerror module 39torch39 has no attribute 39version39,"OS: Ubuntu 18.04 LTS
CUDA: 11.3
GPU: NVIDIA P5000 Quadro
IDE: Jupyter Notebook
Environment: VirtualEnv (venv)

Code:
# Importing the required libraries
import torch

from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig

# Defining the name of the Falcon model
model_name = ""ybelkada/falcon-7b-sharded-bf16""

# Configuring the BitsAndBytes quantization
bnb_config = BitsAndBytesConfig(
load_in_4bit=True,
bnb_4bit_quant_type=""nf4"",
bnb_4bit_compute_dtype=torch.float16,
)

# Loading the Falcon model with quantization configuration
model = AutoModelForCausalLM.from_pretrained(
model_name,
quantization_config=bnb_config,
trust_remote_code=True
)

# Disabling cache usage in the model configuration
model.config.use_cache = False

Error:
---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
File ~/FYP_Chatbot/test02/src/myenv-test002-02/lib/python3.9/site-packages/torch/cuda/__init__.py:242, in _lazy_init()
    241 try:
--> 242     queued_call()
    243 except Exception as e:

File ~/FYP_Chatbot/test02/src/myenv-test002-02/lib/python3.9/site-packages/torch/cuda/__init__.py:122, in _check_capability()
    116 old_gpu_warn = """"""
    117 Found GPU%d %s which is of cuda capability %d.%d.
    118 PyTorch no longer supports this GPU because it is too old.
    119 The minimum cuda capability supported by this library is %d.%d.
    120 """"""
--> 122 if torch.version.cuda is not None:  # on ROCm we don't want this check
    123     CUDA_VERSION = torch._C._cuda_getCompiledVersion()

AttributeError: module 'torch' has no attribute 'version'

The above exception was the direct cause of the following exception:

DeferredCudaCallError                     Traceback (most recent call last)
Cell In[10], line 17
     10 bnb_config = BitsAndBytesConfig(
     11 load_in_4bit=True,
     12 bnb_4bit_quant_type=""nf4"",
     13 bnb_4bit_compute_dtype=torch.float16,
     14 )
     16 # Loading the Falcon model with quantization configuration
---> 17 model = AutoModelForCausalLM.from_pretrained(
     18 model_name,
     19 quantization_config=bnb_config,
     20 trust_remote_code=True
     21 )
     23 # Disabling cache usage in the model configuration
     24 model.config.use_cache = False

File ~/FYP_Chatbot/test02/src/myenv-test002-02/lib/python3.9/site-packages/transformers/models/auto/auto_factory.py:563, in _BaseAutoModelClass.from_pretrained(cls, pretrained_model_name_or_path, *model_args, **kwargs)
    561 elif type(config) in cls._model_mapping.keys():
    562     model_class = _get_model_class(config, cls._model_mapping)
--> 563     return model_class.from_pretrained(
    564         pretrained_model_name_or_path, *model_args, config=config, **hub_kwargs, **kwargs
    565     )
    566 raise ValueError(
    567     f""Unrecognized configuration class {config.__class__} for this kind of AutoModel: {cls.__name__}.\n""
    568     f""Model type should be one of {', '.join(c.__name__ for c in cls._model_mapping.keys())}.""
    569 )

File ~/FYP_Chatbot/test02/src/myenv-test002-02/lib/python3.9/site-packages/transformers/modeling_utils.py:3053, in PreTrainedModel.from_pretrained(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)
   3049 hf_quantizer.validate_environment(
   3050     torch_dtype=torch_dtype, from_tf=from_tf, from_flax=from_flax, device_map=device_map
   3051 )
   3052 torch_dtype = hf_quantizer.update_torch_dtype(torch_dtype)
-> 3053 device_map = hf_quantizer.update_device_map(device_map)
   3055 # Force-set to `True` for more mem efficiency
   3056 if low_cpu_mem_usage is None:

File ~/FYP_Chatbot/test02/src/myenv-test002-02/lib/python3.9/site-packages/transformers/quantizers/quantizer_bnb_4bit.py:246, in Bnb4BitHfQuantizer.update_device_map(self, device_map)
    244 def update_device_map(self, device_map):
    245     if device_map is None:
--> 246         device_map = {"""": torch.cuda.current_device()}
    247         logger.info(
    248             ""The device_map was not initialized. ""
    249             ""Setting device_map to {'':torch.cuda.current_device()}. ""
    250             ""If you want to use the model for inference, please set device_map ='auto' ""
    251         )
    252     return device_map

File ~/FYP_Chatbot/test02/src/myenv-test002-02/lib/python3.9/site-packages/torch/cuda/__init__.py:552, in current_device()
    550 def current_device() -> int:
    551     r""""""Returns the index of a currently selected device.""""""
--> 552     _lazy_init()
    553     return torch._C._cuda_getDevice()

File ~/FYP_Chatbot/test02/src/myenv-test002-02/lib/python3.9/site-packages/torch/cuda/__init__.py:246, in _lazy_init()
    243         except Exception as e:
    244             msg = (f""CUDA call failed lazily at initialization with error: {str(e)}\n\n""
    245                    f""CUDA call was originally invoked at:\n\n{orig_traceback}"")
--> 246             raise DeferredCudaCallError(msg) from e
    247 finally:
    248     delattr(_tls, 'is_initializing')

DeferredCudaCallError: CUDA call failed lazily at initialization with error: module 'torch' has no attribute 'version'

Environment Packages:
accelerate==0.29.1 
bitsandbytes==0.43.0 
datasets==2.18.0 
einops==0.7.0 
fsspec==2023.10.0 
peft @ git+https://github.com/huggingface/peft.git@26726bf1ddee6ca75ed4e1bfd292094526707a78 torch==1.13.0 transformers==4.39.3 
trl==0.8.1 
wandb==0.16.6

I encountered the error after downgrading PyTorch 2.2.2 to PyTorch 1.13.0. I had to downgrade PyTorch 2.2.2 because of the fact that I have cuda toolkit of version 11.3 which was not compatible with the later versions of PyTorch. I downgraded PyTorch to version 1.13.0 specifically because I am using ""transformers"" library from huggingface which requires PyTorch version >= 1.13.0.
Nvidia Graphics Cards Details (nvidia-smi):
Sat Apr  6 22:40:45 2024       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 470.182.03   Driver Version: 470.182.03   CUDA Version: 11.4     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  Quadro P5000        Off  | 00000000:01:00.0  On |                  Off |
| 27%   44C    P8     6W / 180W |    295MiB / 16275MiB |      3%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|    0   N/A  N/A       938      G   /usr/lib/xorg/Xorg                103MiB |
|    0   N/A  N/A      1150      G   /usr/bin/gnome-shell               37MiB |
|    0   N/A  N/A      1986      G   /usr/lib/firefox/firefox          150MiB |
+-----------------------------------------------------------------------------+

nvcc -V:
nvcc: NVIDIA (R) Cuda compiler driver
Copyright (c) 2005-2021 NVIDIA Corporation
Built on Sun_Mar_21_19:15:46_PDT_2021
Cuda compilation tools, release 11.3, V11.3.58
Build cuda_11.3.r11.3/compiler.29745058_0

","According to the information given in your post, you have two requirements:

the transformers package you are trying to use requires a PyTorch version of >= 1.13.0

the maximum CUDA version supported by your GPU is 11.4, so the CUDA toolkit  installed via PyTorch must be <= 11.4


That CUDA version was decommissioned on the release of PyTorch 1.13, see release notes. So a viable solution for you would be to see whether you can update your CUDA driver to 11.6 or 11.7. With that version, you will be able to install PyTorch 1.13 with the appropriate CUDA toolkit.
",huggingface-transformers
sgaemaker training what39s the correct regex patrern to capture metrics,"This is the pattern I've seen suggested in a few different posts on SO:
metric_definitions = [
    {'Name': 'loss', 'Regex': ""'loss': ([0-9]+(.|e\-)[0-9]+),?""},
    {'Name': 'learning_rate', 'Regex': ""'learning_rate': ([0-9]+(.|e\-)[0-9]+),?""},
    {'Name': 'eval_loss', 'Regex': ""'eval_loss': ([0-9]+(.|e\-)[0-9]+),?""},
    {'Name': 'eval_accuracy', 'Regex': ""'eval_accuracy': ([0-9]+(.|e\-)[0-9]+),?""},
    {'Name': 'eval_f1', 'Regex': ""'eval_f1': ([0-9]+(.|e\-)[0-9]+),?""},
    {'Name': 'eval_precision', 'Regex': ""'eval_precision': ([0-9]+(.|e\-)[0-9]+),?""},
    {'Name': 'eval_recall', 'Regex': ""'eval_recall': ([0-9]+(.|e\-)[0-9]+),?""},
    {'Name': 'eval_runtime', 'Regex': ""'eval_runtime': ([0-9]+(.|e\-)[0-9]+),?""},
    {'Name': 'eval_samples_per_second', 'Regex': ""'eval_samples_per_second': ([0-9]+(.|e\-)[0-9]+),?""},
    {'Name': 'epoch', 'Regex': ""'epoch': ([0-9]+(.|e\-)[0-9]+),?""}
]

The issue is, it fails to capture the e-0x after the digit. I've tried a few variant like these one: ([0-9]+(.*e\-)[0-9]+)\w+ which I have tested on https://regexr.com/. While it works on the website it still fails to capture the exponent part in CloudWatch.
I noticed the issue because my loss was going up and down, and when I checked the log directly I could see the loss was only going down, except every time it went from 1.254e-05 to 9.365e-06 only the first portion was captured, so it looked like the loss was just going back up and the model was not learning.
","The expression you used has some issues. It only works for ""1234e-05"", and doesn't work for ""1.234e-05"". Also ""."" has to be escaped with back-slash (""\."") to strictly match a period character.
Instead, please try (\d+(\.\d+)?(e-\d+)?)
I only tested on Python's regular expression module, but it should capture all following patterns.

1234
1.234
123e-4
1.23e-4

",huggingface-transformers
how do i increase max_new_tokens,"I'm facing this error while running my code:

ValueError: Input length of input_ids is 1495, but max_length is set to 20. This can lead to unexpected behavior. You should consider increasing max_length or, better yet, setting max_new_tokens.

I wanted the code to generate the query instead it says about the max length issue as basically I am using 8 bit quantized llama using vector embedding to develop a RAG chat bot
import os
from langchain.document_loaders import PyPDFLoader
from langchain.text_splitter import CharacterTextSplitter
from transformers import BitsAndBytesConfig, AutoTokenizer
from langchain_huggingface import ChatHuggingFace, HuggingFacePipeline
from time import time
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.vectorstores import Chroma
from langchain.chains import RetrievalQA

# Load PDF and split into chunks
def split_doc(file, chunk_size, chunk_overlap):
    text_splitter = CharacterTextSplitter(
        separator=""\n\n"",
        chunk_size=chunk_size,
        chunk_overlap=chunk_overlap,
        length_function=len
    )
    return text_splitter.split_documents(file)

loader = PyPDFLoader(""/kaggle/input/report/report.pdf"")
pages = loader.load()
docs = split_doc(pages, 700, 450)

# Configure and load the model
model_name = ""NousResearch/Llama-2-7b-chat-hf""
bnb_config = BitsAndBytesConfig(load_in_8bit=True)
llm = HuggingFacePipeline.from_model_id(
    model_id=model_name,
    task=""text-generation"",
    model_kwargs={""trust_remote_code"": True, ""quantization_config"": bnb_config}
)
chat_model = ChatHuggingFace(llm=llm)

# Set up embeddings and vector store
embedding_model_name = ""sentence-transformers/all-mpnet-base-v2""
embedding_model_kwargs = {""device"": ""cuda""}
embeddings = HuggingFaceEmbeddings(model_name=embedding_model_name, model_kwargs=embedding_model_kwargs)
vectordb = Chroma.from_documents(documents=docs, embedding=embeddings, persist_directory=""chroma_db"")
retriever = vectordb.as_retriever()

# Set up the QA system
qa = RetrievalQA.from_chain_type(
    llm=llm,
    chain_type=""stuff"",
    retriever=retriever,
    verbose=True
)

# Define the testing function
def test_rag(qa, query):
    print(f""Query: {query}\n"")
    result = qa.run(query)
    print(""\nResult: "", result)

query = ""What were the main topics in this ""
test_rag(qa, query)# get stuck here

","thanks it helped i added the following details:
using the pipeline_kwargs in huggingface.py file i was able to find the variable i could use although using this method will render the quantization method a bit useless as you will consume more memory upon increasing the tokens
llm = HuggingFacePipeline.from_model_id(
    model_id=model_name,
    task=""text-generation"",
    model_kwargs={
        ""trust_remote_code"": True,
        ""quantization_config"": bnb_config,
        ""use_auth_token"": auth_token
    },
    pipeline_kwargs={""max_new_tokens"": 8096}# this part is how i reconfigured the tokens

)

",huggingface-transformers
gpt2 model from hugging face always generate same result,"Why were all the results I got from the GPT-2 model the same no matter what I fed into it?
The following are my operating details.
First I download the needed files from the official website. These files included config.json, merges.txt, pytorch_model.bin, tokenizer.json, tokenizer_config.json and vocab.json. Then I stored them in the root path of the project ./gpt2.
Second, I loaded the model and predicted the next word based on the input context. The code is displayed as follows.

model = GPT2Model.from_pretained('./gpt2') 
gpt_tokenizer=GPT2Tokenizer.from_pretrained('./gpt2')
start_context=""The white man worked as a "" 
ids_text=gpt_tokenizer(start_ontext,return_tensor='pt') 
output=model(**ids_text) 
output=output.last_hidden_state[:,-1,:] 
idx_next=torch.argmax(output,dim=-1,keepdim=True) 
ids=idx_next.squeeze(0) 
text=gpt_tokenizer.decode(ids.tolist()) 
print(text) 

Here, the text always indicates age, even though I changed the start_context to other, like ""I see a cat under"".
I hope someone can tell me the reason and help me work it out, thanks.
","The reason is that you got the ouput of shape [batch, hidden_size], which is 1,768 I guess. You cannot fit it into a argmax and do tokenization as 768 is the dimension of vector space instead of vocab.
Try using GPT2LMHeadModel:
from transformers import GPT2Tokenizer, GPT2LMHeadModel
import os
os.environ[""CUDA_VISIBLE_DEVICES""] = ""2""
import torch

# Model and tokenizer paths
model_path = ""/mnt/sda/agent_mxz/models/gpt2""
tokenizer = GPT2Tokenizer.from_pretrained(model_path)
model = GPT2LMHeadModel.from_pretrained(model_path)

# Input texts
texts = [""Replace me by any text you'd like."", ""Hello, this is"", ""Write a story for me.""]

# Ensure padding is done on the left
tokenizer.padding_side = ""left""

# Define PAD Token = EOS Token
tokenizer.pad_token = tokenizer.eos_token
model.config.pad_token_id = model.config.eos_token_id
model.config.do_sample = False

# Tokenize the inputs with padding
encoded_inputs = tokenizer(texts, return_tensors='pt', padding=True, truncation=True)
print(encoded_inputs)
# Get model output
outputs = model(**encoded_inputs)
# outputs1 = model.generate()
# Print the outputs
# print(outputs[0][1])
print(tokenizer.batch_decode(torch.argmax(outputs[0], dim=-1)))

It will give you ['. the with a means, want like.\n', ',HelloHelloHelloHelloHello Hello Hello hello hello', ',Write""""Write write write the I.']
",huggingface-transformers
indexerror list index out of range when trying to predict from the fine tuned model using hugginface,"i am trying to learn on how to fine tune a pretrained model and use it. this is my code
from transformers import AutoModelForSequenceClassification, AutoTokenizer, TrainingArguments, Trainer
from datasets import load_dataset
import numpy as np
import torch

# Define a simple accuracy metric
def compute_metrics(p):
    predictions, labels = p
    preds = np.argmax(predictions, axis=1)
    return {""accuracy"": (preds == labels).mean()}

# Load the dataset
dataset = load_dataset(""imdb"", split='train[:1%]')
small_train_dataset = dataset.train_test_split(test_size=0.1)['train']
small_eval_dataset = dataset.train_test_split(test_size=0.1)['test']

# Load the tokenizer and model
model_name = ""bert-base-uncased""
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)

# Tokenize the dataset
def tokenize_function(examples):
    return tokenizer(examples['text'], padding=""max_length"", truncation=True)

small_train_dataset = small_train_dataset.map(tokenize_function, batched=True)
small_eval_dataset = small_eval_dataset.map(tokenize_function, batched=True)
small_train_dataset = small_train_dataset.rename_column(""label"", ""labels"")
small_eval_dataset = small_eval_dataset.rename_column(""label"", ""labels"")
small_train_dataset.set_format(""torch"", columns=[""input_ids"", ""attention_mask"", ""labels""])
small_eval_dataset.set_format(""torch"", columns=[""input_ids"", ""attention_mask"", ""labels""])

# Define training arguments
training_args = TrainingArguments(
    output_dir=""test_trainer"",
    evaluation_strategy=""epoch"",
    per_device_train_batch_size=8,
    per_device_eval_batch_size=8,
    num_train_epochs=3,
    weight_decay=0.01
)

# Initialize the Trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=small_train_dataset,
    eval_dataset=small_eval_dataset,
    compute_metrics=compute_metrics
)

# Train the model
trainer.train()

# Evaluate the model
validation_results = trainer.evaluate()
print(validation_results)

now, i am trying to make a prediction on the fine tuned model, like this
inputs=tokenizer(dataset[0]['text'], padding=""max_length"", truncation=True,return_tensors=""pt"")
predictions = trainer.predict(test_dataset=inputs)

i am getting this error when i am trying to make a prediction,

IndexError Traceback (most recent call last) Cell In[8], line 7 3
inputs=tokenizer(dataset[0][‘text’], padding=“max_length”,
truncation=True,return_tensors=“pt”) 6 # Make predictions
----> 7 predictions = trainer.predict(test_dataset=inputs)
File C:\Python311\Lib\site-packages\transformers\trainer.py:3305, in
Trainer.predict(self, test_dataset, ignore_keys, metric_key_prefix)
3302 start_time = time.time() 3304 eval_loop = self.prediction_loop if
self.args.use_legacy_prediction_loop else self.evaluation_loop → 3305
output = eval_loop( 3306 test_dataloader, description=“Prediction”,
ignore_keys=ignore_keys, metric_key_prefix=metric_key_prefix 3307 )
3308 total_batch_size = self.args.eval_batch_size *
self.args.world_size 3309 if
f""{metric_key_prefix}_jit_compilation_time"" in output.metrics:
File C:\Python311\Lib\site-packages\transformers\trainer.py:3408, in
Trainer.evaluation_loop(self, dataloader, description,
prediction_loss_only, ignore_keys, metric_key_prefix) 3406
observed_num_examples = 0 3407 # Main evaluation loop → 3408 for step,
inputs in enumerate(dataloader): 3409 # Update the observed num
examples 3410 observed_batch_size = find_batch_size(inputs) 3411 if
observed_batch_size is not None:
File C:\Python311\Lib\site-packages\accelerate\data_loader.py:454, in
DataLoaderShard.iter(self) 452 # We iterate one batch ahead to check
when we are at the end 453 try: → 454 current_batch =
next(dataloader_iter) 455 except StopIteration: 456 yield
File
C:\Python311\Lib\site-packages\torch\utils\data\dataloader.py:631, in
_BaseDataLoaderIter.next(self) 628 if self._sampler_iter is None: 629 # TODO(Bug in dataloader iterator found by mypy · Issue #76750 · pytorch/pytorch · GitHub) 630 self._reset() # type: ignore[call-arg] →
631 data = self._next_data() 632 self._num_yielded += 1 633 if
self._dataset_kind == _DatasetKind.Iterable and 634
self._IterableDataset_len_called is not None and 635 self._num_yielded

self._IterableDataset_len_called:

File
C:\Python311\Lib\site-packages\torch\utils\data\dataloader.py:675, in
_SingleProcessDataLoaderIter._next_data(self) 673 def _next_data(self): 674 index = self._next_index() # may raise StopIteration → 675 data = self._dataset_fetcher.fetch(index) # may
raise StopIteration 676 if self._pin_memory: 677 data =
_utils.pin_memory.pin_memory(data, self._pin_memory_device)
File
C:\Python311\Lib\site-packages\torch\utils\data_utils\fetch.py:51, in
_MapDatasetFetcher.fetch(self, possibly_batched_index) 49 data = self.dataset.getitems(possibly_batched_index) 50 else: —> 51 data =
[self.dataset[idx] for idx in possibly_batched_index] 52 else: 53 data
= self.dataset[possibly_batched_index]
File
C:\Python311\Lib\site-packages\torch\utils\data_utils\fetch.py:51, in
(.0) 49 data = self.dataset.getitems(possibly_batched_index) 50 else:
—> 51 data = [self.dataset[idx] for idx in possibly_batched_index] 52
else: 53 data = self.dataset[possibly_batched_index]
File
C:\Python311\Lib\site-packages\transformers\tokenization_utils_base.py:255,
in BatchEncoding.getitem(self, item) 253 return self.data[item] 254
elif self._encodings is not None: → 255 return self._encodings[item]
256 elif isinstance(item, slice): 257 return {key:
self.data[key][item] for key in self.data.keys()}
IndexError: list index out of range

","The error you are encountering is because the trainer.predict method expects a dataset as input, but you are passing a single example that has been tokenized into tensors.
To perform predictions on a single input, you need to prepare it similarly to how the dataset was prepared before training, and then use the model directly for prediction.
Here's how you can modify your code to make predictions on a single input:

Prepare the input correctly
Use the model directly for prediction

Here's the revised code:

from transformers import AutoModelForSequenceClassification, AutoTokenizer, TrainingArguments, Trainer
from datasets import load_dataset
import numpy as np
import torch

# Define a simple accuracy metric
def compute_metrics(p):
    predictions, labels = p
    preds = np.argmax(predictions, axis=1)
    return {""accuracy"": (preds == labels).mean()}

# Load the dataset
dataset = load_dataset(""imdb"", split='train[:1%]')
small_train_dataset = dataset.train_test_split(test_size=0.1)['train']
small_eval_dataset = dataset.train_test_split(test_size=0.1)['test']

# Load the tokenizer and model
model_name = ""bert-base-uncased""
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)

# Tokenize the dataset
def tokenize_function(examples):
    return tokenizer(examples['text'], padding=""max_length"", truncation=True)

small_train_dataset = small_train_dataset.map(tokenize_function, batched=True)
small_eval_dataset = small_eval_dataset.map(tokenize_function, batched=True)
small_train_dataset = small_train_dataset.rename_column(""label"", ""labels"")
small_eval_dataset = small_eval_dataset.rename_column(""label"", ""labels"")
small_train_dataset.set_format(""torch"", columns=[""input_ids"", ""attention_mask"", ""labels""])
small_eval_dataset.set_format(""torch"", columns=[""input_ids"", ""attention_mask"", ""labels""])

# Define training arguments
training_args = TrainingArguments(
    output_dir=""test_trainer"",
    evaluation_strategy=""epoch"",
    per_device_train_batch_size=8,
    per_device_eval_batch_size=8,
    num_train_epochs=3,
    weight_decay=0.01
)

# Initialize the Trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=small_train_dataset,
    eval_dataset=small_eval_dataset,
    compute_metrics=compute_metrics
)

# Train the model
trainer.train()

# Evaluate the model
validation_results = trainer.evaluate()
print(validation_results)

# Make a prediction on a single input
inputs = tokenizer(dataset[0]['text'], padding=""max_length"", truncation=True, return_tensors=""pt"")
model.eval()  # Set the model to evaluation mode
with torch.no_grad():  # Disable gradient calculation
    outputs = model(**inputs)
    predictions = torch.argmax(outputs.logits, dim=-1)

print(f""Predicted label: {predictions.item()}"")

",huggingface-transformers
how to know which words are encoded with unknown tokens in huggingface berttokenizer,"I use the following code to count how many % of words are encoded to unknown tokens.
paragraph_chinese = '...' # It is a long paragraph from a text file.
from transformers import AutoTokenizer, BertTokenizer
tokenizer_bart = BertTokenizer.from_pretrained(""fnlp/bart-base-chinese"")
encoded_chinese_bart = tokenizer_bart.encode(paragraph_chinese)
unk_token_id_bart = tokenizer_bart.convert_tokens_to_ids([""[UNK]""])
len_paragraph_chinese   = len(paragraph_chinese)

unk_token_cnt_chinese_bart   = encoded_chinese_bart.count(unk_token_id_bart[0])
print(""BART Unknown Token count in Chinese Paragraph:"", unk_token_cnt_chinese_bart, ""("" + str(unk_token_cnt_chinese_bart * 100 / len_paragraph_chinese) + ""%)"")
print(type(tokenizer_bart))

which prints:
BART Unknown Token count in Chinese Paragraph: 1 (0.015938795027095953%)
<class 'transformers.models.bert.tokenization_bert.BertTokenizer'>

My question is: I noticed there is one unknown token. How can I know which word causes this unknown token?
p.s. I tried print(encoded_chinese_bart), but it is a list of Token IDs.
Using transformers 4.28.1
","When you use the BertTokenizerFast instead of the ""slow"" version, you will get a BatchEncoding object that gives you access to several convenient methods that allow you to map a token back to the original string.
The following code uses the token_to_chars method:
from transformers import BertTokenizerFast

# just an example
paragraph_chinese = '马云 Kočka 祖籍浙江省嵊县 Kočka 现嵊州市' 

tokenizer_bart = BertTokenizerFast.from_pretrained(""fnlp/bart-base-chinese"")
encoded_chinese_bart = tokenizer_bart(paragraph_chinese)
unk_token_id_bart = tokenizer_bart.unk_token_id
len_paragraph_chinese   = len(paragraph_chinese)

unk_token_cnt_chinese_bart   = encoded_chinese_bart.input_ids.count(unk_token_id_bart)
print(f'BART Unknown Token count in Chinese Paragraph: {unk_token_cnt_chinese_bart} ({unk_token_cnt_chinese_bart * 100 / len_paragraph_chinese}%)')

#find all indices
unk_indices = [i for i, x in enumerate(encoded_chinese_bart.input_ids) if x == unk_token_id_bart]
for unk_i in unk_indices:
  start, stop = encoded_chinese_bart.token_to_chars(unk_i)
  print(f""At {start}:{stop}: {paragraph_chinese[start:stop]}"")

Original:
BART Unknown Token count in Chinese Paragraph: 2 (7.407407407407407%)
At 3:8: Kočka
At 17:22: Kočka

",huggingface-transformers
llama2 7bhf repeats context of question directly from input prompt cuts off with newlines,"Context: I am trying to query Llama-2 7B, taken from HuggingFace (meta-llama/Llama-2-7b-hf). I give it a question and context (I would guess anywhere from 200-1000 tokens), and ask it to answer the question based on the context (context is retrieved from a vectorstore using similarity search). Here are my two problems:

The answer ends, and the rest of the tokens until it reaches max_new_tokens are all newlines. Or it just doesn't generate any text and the entire response is newlines. Adding a repetition_penalty of 1.1 or greater has solved infinite newline generation, but does not get me full answers.
For answers that do generate, they are copied word for word from the given context. This remains the same with repetition_penalty=1.1, and making the repetition penalty too high makes the answer nonsense.

I have only tried using temperature=0.4 and temperature=0.8, but from what I have done, tuning temperature and repetition_penalty both result in either the context being copied or a nonsensical answer.
Note about the ""context"": I am using a document stored in a Chroma vector store, and similarity search retrieves the relevant information before I pass it to Llama.
Example Problem:
My query is to summarize a certain Topic X.
query = ""Summarize Topic X""

The retrieved context from the vectorstore has 3 sources that looks something like this (I format the sources in my query to the LLM separated by newlines):
context = """"""When talking about Topic X, Scenario Y is always referred to. This is due to the relation of
Topic X is a broad topic which covers many aspects of life.
No one knows when Topic X became a thing, its origin is unknown even to this day.""""""

Then the response from Llama-2 directly mirrors one piece of context, and includes no information from the others. Furthermore, it produces many newlines after the answer. If the answer is 100 tokens, and max_new_tokens is 150, I have 50 newlines.
response = ""When talking about Topic X, Scenario Y is always referred to. This is due to the relation of \n\n\n\n""

One of my biggest issues is that in addition to copying one piece of context, if the context ends mid-sentence, so does the LLM response.
Is anyone else experiencing anything like this (newline issue or copying part of your input prompt)? Has anyone found a solution?
","This is a common issue with pre-trained base models like Llama.
My first thought would be to select a model that has some sort of instruction tuning done to it i.e https://huggingface.co/meta-llama/Llama-2-7b-chat. Instruction tuning impacts the model's ability to solve tasks reliably, as opposed to the base model, which is often just trained to predict the next token (which is often why the cutoff happens).
The second thing, in my experience, I have seen that has helped is using the same prompt format that was used during training. You can see in the source code the prompt format used in training and generation by Meta. Here is a thread about it.
Finally, for repetition, using a Logits Processor at generation-time has been helpful to reduce repetition.
",huggingface-transformers
continual pretraining vs finetuning a language model with mlm,"I have some custom data I want to use to further pre-train the BERT model. I’ve tried the two following approaches so far:

Starting with a pre-trained BERT checkpoint and continuing the pre-training with Masked Language Modeling (MLM) + Next Sentence Prediction (NSP) heads (e.g. using BertForPreTraining model)
Starting with a pre-trained BERT model with the MLM objective (e.g. using the BertForMaskedLM model assuming we don’t need NSP for the pretraining part.)

But I’m still confused that if using either BertForPreTraining or BertForMaskedLM actually does the continual pre-training on BERT or these are just two models for fine-tuning that use MLM+NSP and MLM for fine-tuning BERT, respectively. Is there even any difference between fine-tuning BERT with MLM+NSP or continually pre-train it using these two heads or this is something we need to test?
I've reviewed similar questions such as this one but still, I want to make sure that whether technically there's a difference between continual pre-training a model from an initial checkpoint and fine-tuning it using the same objective/head.
","The answer is a mere difference in the terminology used. When the model is trained on a large generic corpus, it is called 'pre-training'. When it is adapted to a particular task or dataset it is called as 'fine-tuning'.
Technically speaking, in either cases ('pre-training' or 'fine-tuning'), there are updates to the model weights.
For example, usually, you can just take the pre-trained model and then fine-tune it for a specific task (such as classification, question-answering, etc.). However, if you find that the target dataset is from a specific domain, and you have a few unlabled data that might help the model to adapt to the particular domain, then you can do a MLM or MLM+NSP 'fine-tuning' (unsupervised learning) (some researchers do call this as 'pre-training' especially when a huge corpus is used to train the model), followed by using the target corpus with target task fine-tuning.
",huggingface-transformers
deepspeed  attributeerror 39dummyoptim39 object has no attribute 39step39,"I want to use deepspeed for training LLMs along with Huggingface Trainer. But when I use deepspeed  along with trainer I get error ""AttributeError: 'DummyOptim' object has no attribute 'step'"". Below is my code
import argparse
import numpy as np
import torch
from datasets import load_dataset
from transformers import AutoTokenizer, AutoModelForCausalLM

from trl import DPOTrainer, DPOConfig
def preprocess_data(item):
    return {
        'prompt': 'Instruct: ' + item['prompt'] + '\n',
        'chosen': 'Output: ' + item['chosen'],
        'rejected': 'Output: ' + item['rejected']
    }        

def main():
    parser = argparse.ArgumentParser()
    parser.add_argument(""--epochs"", type=int, default=1)
    parser.add_argument(""--beta"", type=float, default=0.1)
    parser.add_argument(""--batch_size"", type=int, default=4)
    parser.add_argument(""--lr"", type=float, default=1e-6)
    parser.add_argument(""--seed"", type=int, default=2003)
    parser.add_argument(""--model_name"", type=str, default=""EleutherAI/pythia-14m"")
    parser.add_argument(""--dataset_name"", type=str, default=""jondurbin/truthy-dpo-v0.1"")
    parser.add_argument(""--local_rank"", type=int, default=0)

    args = parser.parse_args()

    # Determine device based on local_rank
    device = torch.device(""cuda"", args.local_rank) if torch.cuda.is_available() else torch.device(""cpu"")


    tokenizer = AutoTokenizer.from_pretrained(args.model_name)
    tokenizer.pad_token = tokenizer.eos_token
    model = AutoModelForCausalLM.from_pretrained(args.model_name).to(device)
    ref_model = AutoModelForCausalLM.from_pretrained(args.model_name).to(device)

    dataset = load_dataset(args.dataset_name, split=""train"")
    dataset = dataset.map(preprocess_data)

    # Split the dataset into training and validation sets
    dataset = dataset.train_test_split(test_size=0.1, seed=args.seed)
    train_dataset = dataset['train']
    val_dataset = dataset['test']

    training_args = DPOConfig(
        learning_rate=args.lr,
        num_train_epochs=args.epochs,
        per_device_train_batch_size=args.batch_size,
        logging_steps=10,
        remove_unused_columns=False,
        max_length=1024,
        max_prompt_length=512,
        deepspeed=""ds_config.json""       
    )

    

    # Verify and print embedding dimensions before finetuning
    print(""Base model embedding dimension:"", model.config.hidden_size)

    model.train()
    ref_model.eval()

    dpo_trainer = DPOTrainer(
        model,
        ref_model,
        beta=args.beta,
        train_dataset=train_dataset,
        eval_dataset=val_dataset,
        tokenizer=tokenizer,
        args=training_args,
    )

    dpo_trainer.train()
    # Evaluate
    evaluation_results = dpo_trainer.evaluate()
    print(""Evaluation Results:"", evaluation_results)

    save_model_name = 'finetuned_model'
    model.save_pretrained(save_model_name)

if __name__ == ""__main__"":
    main()

The config file used is the below one
{
""zero_optimization"": {
        ""stage"": 3,
        ""offload_optimizer"": {
            ""device"": ""cpu"",
            ""pin_memory"": true
        },
        ""offload_param"": {
            ""device"": ""cpu"",
            ""pin_memory"": true
        },
        ""overlap_comm"": true,
        ""contiguous_gradients"": true,
        ""sub_group_size"": 1e9,
        ""reduce_bucket_size"": ""auto"",
        ""stage3_prefetch_bucket_size"": ""auto"",
        ""stage3_param_persistence_threshold"": ""auto"",
        ""stage3_max_live_parameters"": 1e9,
        ""stage3_max_reuse_distance"": 1e9,
        ""stage3_gather_16bit_weights_on_model_save"": true
    },
""bf16"": {
    ""enabled"": ""auto""
},
""fp16"": {
    ""enabled"": ""auto"",
    ""loss_scale"": 0,
    ""initial_scale_power"": 32,
    ""loss_scale_window"": 1000,
    ""hysteresis"": 2,
    ""min_loss_scale"": 1
},

""gradient_accumulation_steps"": ""auto"",
""gradient_clipping"": ""auto"",
""train_batch_size"": ""auto"",
""train_micro_batch_size_per_gpu"": ""auto"",
""wall_clock_breakdown"": false,
""flops_profiler"": {
    ""enabled"": false,
    ""detailed"": false
},
""optimizer"": {
    ""type"": ""Lamb"",
    ""params"": {
    ""lr"": ""auto"",
    ""betas"": [0.9, 0.999],
    ""eps"": ""auto"",
    ""weight_decay"": ""auto""
    }
},
""zero_allow_untested_optimizer"": true
}

The code works with out deepspeed. I have torch=2.3.1, deepspeed                 =0.14.5, trl=0.9.4 and CUDA Version: 12.5.
Appreciate any hint on this !
","from accelerate.utils import DistributedType

training_args.distributed_state.distributed_type = DistributedType.DEEPSPEED

adding this solves the issue
",huggingface-transformers
size mismatch for embed_outweight copying a param with shape torchsize0 from checkpoint  huggingface pytorch,"I want to finetune an LLM. I am able to successfully finetune LLM. But when reload the model after save, gets error. Below is the code
import argparse
import numpy as np
import torch
from datasets import load_dataset
from transformers import AutoTokenizer, AutoModelForCausalLM

from trl import DPOTrainer, DPOConfig
def preprocess_data(item):
    return {
        'prompt': 'Instruct: ' + item['prompt'] + '\n',
        'chosen': 'Output: ' + item['chosen'],
        'rejected': 'Output: ' + item['rejected']
    }        

def main():
    parser = argparse.ArgumentParser()
    parser.add_argument(""--epochs"", type=int, default=1)
    parser.add_argument(""--beta"", type=float, default=0.1)
    parser.add_argument(""--batch_size"", type=int, default=4)
    parser.add_argument(""--lr"", type=float, default=1e-6)
    parser.add_argument(""--seed"", type=int, default=2003)
    parser.add_argument(""--model_name"", type=str, default=""EleutherAI/pythia-14m"")
    parser.add_argument(""--dataset_name"", type=str, default=""jondurbin/truthy-dpo-v0.1"")
    parser.add_argument(""--local_rank"", type=int, default=0)

    args = parser.parse_args()

    # Determine device based on local_rank
    device = torch.device(""cuda"", args.local_rank) if torch.cuda.is_available() else torch.device(""cpu"")


    tokenizer = AutoTokenizer.from_pretrained(args.model_name)
    tokenizer.pad_token = tokenizer.eos_token
    model = AutoModelForCausalLM.from_pretrained(args.model_name).to(device)
    ref_model = AutoModelForCausalLM.from_pretrained(args.model_name).to(device)

    dataset = load_dataset(args.dataset_name, split=""train"")
    dataset = dataset.map(preprocess_data)

    # Split the dataset into training and validation sets
    dataset = dataset.train_test_split(test_size=0.1, seed=args.seed)
    train_dataset = dataset['train']
    val_dataset = dataset['test']

    training_args = DPOConfig(
        learning_rate=args.lr,
        num_train_epochs=args.epochs,
        per_device_train_batch_size=args.batch_size,
        logging_steps=10,
        remove_unused_columns=False,
        max_length=1024,
        max_prompt_length=512,
        fp16=True        
    )

    

    # Verify and print embedding dimensions before finetuning
    print(""Base model embedding dimension:"", model.config.hidden_size)

    model.train()
    ref_model.eval()

    dpo_trainer = DPOTrainer(
        model,
        ref_model,
        beta=args.beta,
        train_dataset=train_dataset,
        eval_dataset=val_dataset,
        tokenizer=tokenizer,
        args=training_args,
    )

    dpo_trainer.train()
    # Evaluate
    evaluation_results = dpo_trainer.evaluate()
    print(""Evaluation Results:"", evaluation_results)

    save_model_name = 'finetuned_model'
    model.save_pretrained(save_model_name)

if __name__ == ""__main__"":
    main()

Error I was getting as below
    return model_class.from_pretrained(
    File ""/.local/lib/python3.10/site-packages/transformers/modeling_utils.py"", line 3838, in from_pretrained
        ) = cls._load_pretrained_model(
    File ""/.local/lib/python3.10/site-packages/transformers/modeling_utils.py"", line 4349, in _load_pretrained_model
        raise RuntimeError(f""Error(s) in loading state_dict for {model.__class__.__name__}:\n\t{error_msg}"")
        RuntimeError: Error(s) in loading state_dict for GPTNeoXForCausalLM:
            size mismatch for gpt_neox.embed_in.weight: copying a param with shape torch.Size([0]) from checkpoint, the shape in current model is torch.Size([50304, 128]).
            size mismatch for embed_out.weight: copying a param with shape torch.Size([0]) from checkpoint, the shape in current model is torch.Size([50304, 128]).
            You may consider adding `ignore_mismatched_sizes=True` in the model `from_pretrained` method.

After finetuning,  model works perfectly. But after reloading the saved trained model its not working. Any idea why gets this error when reloading the model ?
","Instead of
model.save_pretrained(save_model_name)

try this
dpo_trainer.save_model(save_model_name)

",huggingface-transformers
how to fix no token found error while downloading hugging face,"I am trying to test the hugging face's prithivida/parrot_paraphraser_on_T5 model but getting token not found error.
from parrot import Parrot
import torch
import warnings
warnings.filterwarnings(""ignore"")
parrot = Parrot(model_tag=""prithivida/parrot_paraphraser_on_T5"", use_gpu=False)

The error I am getting
OSError                                   Traceback (most recent call last)
Cell In [10], line 2
      1 #Init models (make sure you init ONLY once if you integrate this to your code)
----> 2 parrot = Parrot(model_tag=""prithivida/parrot_paraphraser_on_T5"", use_gpu=False)

File ~/.local/lib/python3.10/site-packages/parrot/parrot.py:10, in Parrot.__init__(self, model_tag, use_gpu)
      8 from parrot.filters import Fluency
      9 from parrot.filters import Diversity
---> 10 self.tokenizer = AutoTokenizer.from_pretrained(model_tag, use_auth_token=True)
     11 self.model     = AutoModelForSeq2SeqLM.from_pretrained(model_tag, use_auth_token=True)
     12 self.adequacy_score = Adequacy()

File ~/.local/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py:560, in AutoTokenizer.from_pretrained(cls, pretrained_model_name_or_path, *inputs, **kwargs)
    557     return tokenizer_class.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)
    559 # Next, let's try to use the tokenizer_config file to get the tokenizer class.
--> 560 tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)
    561 if ""_commit_hash"" in tokenizer_config:
    562     kwargs[""_commit_hash""] = tokenizer_config[""_commit_hash""]

File ~/.local/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py:412, in get_tokenizer_config(pretrained_model_name_or_path, cache_dir, force_download, resume_download, proxies, use_auth_token, revision, local_files_only, **kwargs)
    353 """"""
    354 Loads the tokenizer configuration from a pretrained model tokenizer configuration.
    355 
   (...)
    409 tokenizer_config = get_tokenizer_config(""tokenizer-test"")
    410 ```""""""
    411 commit_hash = kwargs.get(""_commit_hash"", None)
--> 412 resolved_config_file = cached_file(
    413     pretrained_model_name_or_path,
    414     TOKENIZER_CONFIG_FILE,
    415     cache_dir=cache_dir,
    416     force_download=force_download,
    417     resume_download=resume_download,
    418     proxies=proxies,
    419     use_auth_token=use_auth_token,
    420     revision=revision,
    421     local_files_only=local_files_only,
    422     _raise_exceptions_for_missing_entries=False,
    423     _raise_exceptions_for_connection_errors=False,
    424     _commit_hash=commit_hash,
    425 )
    426 if resolved_config_file is None:
    427     logger.info(""Could not locate the tokenizer configuration file, will try to use the model config instead."")

File ~/.local/lib/python3.10/site-packages/transformers/utils/hub.py:409, in cached_file(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, use_auth_token, revision, local_files_only, subfolder, user_agent, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash)
    406 user_agent = http_user_agent(user_agent)
    407 try:
    408     # Load from URL or cache if already cached
--> 409     resolved_file = hf_hub_download(
    410         path_or_repo_id,
    411         filename,
    412         subfolder=None if len(subfolder) == 0 else subfolder,
    413         revision=revision,
    414         cache_dir=cache_dir,
    415         user_agent=user_agent,
    416         force_download=force_download,
    417         proxies=proxies,
    418         resume_download=resume_download,
    419         use_auth_token=use_auth_token,
    420         local_files_only=local_files_only,
    421     )
    423 except RepositoryNotFoundError:
    424     raise EnvironmentError(
    425         f""{path_or_repo_id} is not a local folder and is not a valid model identifier ""
    426         ""listed on 'https://huggingface.co/models'\nIf this is a private repository, make sure to ""
    427         ""pass a token having permission to this repo with `use_auth_token` or log in with ""
    428         ""`huggingface-cli login` and pass `use_auth_token=True`.""
    429     )

File ~/.local/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py:124, in validate_hf_hub_args.<locals>._inner_fn(*args, **kwargs)
    119 if check_use_auth_token:
    120     kwargs = smoothly_deprecate_use_auth_token(
    121         fn_name=fn.__name__, has_token=has_token, kwargs=kwargs
    122     )
--> 124 return fn(*args, **kwargs)

File ~/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1052, in hf_hub_download(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, user_agent, force_download, force_filename, proxies, etag_timeout, resume_download, token, local_files_only, legacy_cache_layout)
   1048         return pointer_path
   1050 url = hf_hub_url(repo_id, filename, repo_type=repo_type, revision=revision)
-> 1052 headers = build_hf_headers(
   1053     token=token,
   1054     library_name=library_name,
   1055     library_version=library_version,
   1056     user_agent=user_agent,
   1057 )
   1059 url_to_download = url
   1060 etag = None

File ~/.local/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py:124, in validate_hf_hub_args.<locals>._inner_fn(*args, **kwargs)
    119 if check_use_auth_token:
    120     kwargs = smoothly_deprecate_use_auth_token(
    121         fn_name=fn.__name__, has_token=has_token, kwargs=kwargs
    122     )
--> 124 return fn(*args, **kwargs)

File ~/.local/lib/python3.10/site-packages/huggingface_hub/utils/_headers.py:117, in build_hf_headers(token, is_write_action, library_name, library_version, user_agent)
     44 """"""
     45 Build headers dictionary to send in a HF Hub call.
     46 
   (...)
    114         If `token=True` but token is not saved locally.
    115 """"""
    116 # Get auth token to send
--> 117 token_to_send = get_token_to_send(token)
    118 _validate_token_to_send(token_to_send, is_write_action=is_write_action)
    120 # Combine headers

File ~/.local/lib/python3.10/site-packages/huggingface_hub/utils/_headers.py:149, in get_token_to_send(token)
    147 if token is True:
    148     if cached_token is None:
--> 149         raise EnvironmentError(
    150             ""Token is required (`token=True`), but no token found. You""
    151             "" need to provide a token or be logged in to Hugging Face with""
    152             "" `huggingface-cli login` or `huggingface_hub.login`. See""
    153             "" https://huggingface.co/settings/tokens.""
    154         )
    155     return cached_token
    157 # Case implicit use of the token is forbidden by env variable

OSError: Token is required (`token=True`), but no token found. You need to provide a token or be logged in to Hugging Face with `huggingface-cli login` or `huggingface_hub.login`. See https://huggingface.co/settings/tokens.

I have the secret token downloaded but not sure where to pass and how?
The stack trace after updating the token inside class Parrot in ~/.local/lib/python3.10/site-packages/parrot/parrot.py
Traceback (most recent call last):
  File ""/media/chinmay/New Volume/myWorks/GIT_Hub/project_parrot_nlp/pp.py"", line 8, in <module>
    parrot = Parrot(model_tag=""prithivida/parrot_paraphraser_on_T5"", use_gpu=False)
  File ""/media/chinmay/New Volume/myWorks/GIT_Hub/project_parrot_nlp/vnv/lib/python3.10/site-packages/parrot/parrot.py"", line 10, in __init__
    self.tokenizer = AutoTokenizer.from_pretrained(model_tag, use_auth_token=True)
  File ""/media/chinmay/New Volume/myWorks/GIT_Hub/project_parrot_nlp/vnv/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py"", line 560, in from_pretrained
    tokenizer_config = get_tokenizer_config(pretrained_model_name_or_path, **kwargs)
  File ""/media/chinmay/New Volume/myWorks/GIT_Hub/project_parrot_nlp/vnv/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py"", line 412, in get_tokenizer_config
    resolved_config_file = cached_file(
  File ""/media/chinmay/New Volume/myWorks/GIT_Hub/project_parrot_nlp/vnv/lib/python3.10/site-packages/transformers/utils/hub.py"", line 409, in cached_file
    resolved_file = hf_hub_download(
  File ""/media/chinmay/New Volume/myWorks/GIT_Hub/project_parrot_nlp/vnv/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py"", line 124, in _inner_fn
    return fn(*args, **kwargs)
  File ""/media/chinmay/New Volume/myWorks/GIT_Hub/project_parrot_nlp/vnv/lib/python3.10/site-packages/huggingface_hub/file_download.py"", line 1052, in hf_hub_download
    headers = build_hf_headers(
  File ""/media/chinmay/New Volume/myWorks/GIT_Hub/project_parrot_nlp/vnv/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py"", line 124, in _inner_fn
    return fn(*args, **kwargs)
  File ""/media/chinmay/New Volume/myWorks/GIT_Hub/project_parrot_nlp/vnv/lib/python3.10/site-packages/huggingface_hub/utils/_headers.py"", line 117, in build_hf_headers
    token_to_send = get_token_to_send(token)
  File ""/media/chinmay/New Volume/myWorks/GIT_Hub/project_parrot_nlp/vnv/lib/python3.10/site-packages/huggingface_hub/utils/_headers.py"", line 149, in get_token_to_send
    raise EnvironmentError(
OSError: Token is required (`token=True`), but no token found. You need to provide a token or be logged in to Hugging Face with `huggingface-cli login` or `huggingface_hub.login`. See https://huggingface.co/settings/tokens.

","use generate token from https://huggingface.co/settings/tokens and past it
install python lib huggingface_hub
pip install huggingface_hub


python -c ""from huggingface_hub.hf_api import HfFolder; HfFolder.save_token('YOUR_TOKEN_HERE')""

if you are using notebooke
from huggingface_hub import notebook_login
notebook_login()

past your genrated token
",huggingface-transformers
different embeddings for same sentences with torch transformer,"Hey all and apologies in advance for what is probably a fairly basic question - I have a theory about what's causing the issue here, but would be great to confirm with people who know more about this than I do.
I've been trying to implement this python code snippet in Google colab. The snippet is meant to work out similarity for sentences. The code runs fine, but what I'm finding is that the embeddings and distances change every time I run it, which isn't ideal for my intended use case.
import torch
from scipy.spatial.distance import cosine
from transformers import AutoModel, AutoTokenizer

# Import our models. The package will take care of downloading the models automatically
tokenizer = AutoTokenizer.from_pretrained(""qiyuw/pcl-bert-base-uncased"")
model = AutoModel.from_pretrained(""qiyuw/pcl-bert-base-uncased"")

# Tokenize input texts
texts = [
    ""There's a kid on a skateboard."",
    ""A kid is skateboarding."",
    ""A kid is inside the house.""
]
inputs = tokenizer(texts, padding=True, truncation=True, return_tensors=""pt"")

# Get the embeddings
with torch.no_grad():
    embeddings = model(**inputs, output_hidden_states=True, return_dict=True).pooler_output

# Calculate cosine similarities
# Cosine similarities are in [-1, 1]. Higher means more similar
cosine_sim_0_1 = 1 - cosine(embeddings[0], embeddings[1])
cosine_sim_0_2 = 1 - cosine(embeddings[0], embeddings[2])

print(""Cosine similarity between \""%s\"" and \""%s\"" is: %.3f"" % (texts[0], texts[1], cosine_sim_0_1))
print(""Cosine similarity between \""%s\"" and \""%s\"" is: %.3f"" % (texts[0], texts[2], cosine_sim_0_2))

I think the issue must be model specific since I receive the warning about newly initialized pooler weights, and pooler_output is ultimately what the code reads to inform similarity:
Some weights of RobertaModel were not initialized from the model checkpoint at qiyuw/pcl-roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

Switching to an alternative model which does not give this warning (for example, sentence-transformers/all-mpnet-base-v2) makes the outputs reproducible, so I think it is because of the above warning about initialization of weights.  So here are my questions:

Can I make the output reproducible by initialising/seeding the model differently?
If I can't make the outputs reproducible, is there a way in which I can improve the accuracy to reduce the amount of variation between runs?
Is there a way to search huggingface models for those which will initialise the pooler weights so I can find a model which does suit my purposes?

Thanks in advance
","You are correct the model layer weights for bert.pooler.dense.bias and bert.pooler.dense.weight are initialized randomly. You can initialize these layers always the same way for a reproducible output, but I doubt the inference code that you have copied from there readme is correct. As already mentioned by you the pooling layers are not initialized and their model class also makes sure that the pooling_layer is not added:
...
self.bert = BertModel(config, add_pooling_layer=False)
...

The evaluation script of the repo should be called, according to the readme with the following command:
python evaluation.py --model_name_or_path qiyuw/pcl-bert-base-uncased --mode test --pooler cls_before_pooler

When you look into it, your inference code for qiyuw/pcl-bert-base-uncased should be the following way:
import torch
from scipy.spatial.distance import cosine
from transformers import AutoModel, AutoTokenizer

# Import our models. The package will take care of downloading the models automatically
tokenizer = AutoTokenizer.from_pretrained(""qiyuw/pcl-bert-base-uncased"")
model = AutoModel.from_pretrained(""qiyuw/pcl-bert-base-uncased"")

# Tokenize input texts
texts = [
    ""There's a kid on a skateboard."",
    ""A kid is skateboarding."",
    ""A kid is inside the house.""
]
inputs = tokenizer(texts, padding=True, truncation=True, return_tensors=""pt"")

# Get the embeddings
with torch.inference_mode():
    embeddings = model(**inputs)
    embeddings = embeddings.last_hidden_state[:, 0]

# Calculate cosine similarities
# Cosine similarities are in [-1, 1]. Higher means more similar
cosine_sim_0_1 = 1 - cosine(embeddings[0], embeddings[1])
cosine_sim_0_2 = 1 - cosine(embeddings[0], embeddings[2])

print(""Cosine similarity between \""%s\"" and \""%s\"" is: %.3f"" % (texts[0], texts[1], cosine_sim_0_1))
print(""Cosine similarity between \""%s\"" and \""%s\"" is: %.3f"" % (texts[0], texts[2], cosine_sim_0_2))

Output:
Cosine similarity between ""There's a kid on a skateboard."" and ""A kid is skateboarding."" is: 0.941
Cosine similarity between ""There's a kid on a skateboard."" and ""A kid is inside the house."" is: 0.779


Can I make the output reproducible by initialising/seeding the model differently?

Yes, you can. Use torch.maunal_seed:
import torch
from transformers import AutoModel, AutoTokenizer

model_random = AutoModel.from_pretrained(""qiyuw/pcl-bert-base-uncased"")
torch.manual_seed(42)
model_repoducible1 = AutoModel.from_pretrained(""qiyuw/pcl-bert-base-uncased"")

torch.manual_seed(42)
model_repoducible2 = AutoModel.from_pretrained(""qiyuw/pcl-bert-base-uncased"")

print(torch.allclose(model_random.pooler.dense.weight, model_repoducible1.pooler.dense.weight))
print(torch.allclose(model_random.pooler.dense.weight, model_repoducible2.pooler.dense.weight))
print(torch.allclose(model_repoducible1.pooler.dense.weight, model_repoducible2.pooler.dense.weight))

Output:
False
False
True

",huggingface-transformers
i load a float32 hugging face model cast it to float16 and save it how can i load it as float16,"I load a huggingface-transformers float32 model, cast  it to float16, and save it. How can I load it as float16?
Example:
# pip install transformers
from transformers import AutoModelForTokenClassification, AutoTokenizer

# Load model
model_path = 'huawei-noah/TinyBERT_General_4L_312D'
model = AutoModelForTokenClassification.from_pretrained(model_path)
tokenizer = AutoTokenizer.from_pretrained(model_path)

# Convert the model to FP16
model.half()

# Check model dtype
def print_model_layer_dtype(model):
    print('\nModel dtypes:')
    for name, param in model.named_parameters():
        print(f""Parameter: {name}, Data type: {param.dtype}"")

print_model_layer_dtype(model)
save_directory = 'temp_model_SE'
model.save_pretrained(save_directory)

model2 = AutoModelForTokenClassification.from_pretrained(save_directory, local_files_only=True)
print('\n\n##################')
print(model2)
print_model_layer_dtype(model2)

In this example, model2 loads as a float32 model (as shown by print_model_layer_dtype(model2)), even though model2 was saved as float16 (as shown in config.json). What is the proper way to load it as float16?
Tested with transformers==4.36.2 and Python 3.11.7 on Windows 10.
","Use torch_dtype='auto' in from_pretrained(). Example:
model2 = AutoModelForTokenClassification.from_pretrained(save_directory, 
                                                         local_files_only=True,
                                                         torch_dtype='auto')

Full example:
# pip install transformers
from transformers import AutoModelForTokenClassification, AutoTokenizer
import torch

# Load model
model_path = 'huawei-noah/TinyBERT_General_4L_312D'
model = AutoModelForTokenClassification.from_pretrained(model_path)
tokenizer = AutoTokenizer.from_pretrained(model_path)

# Convert the model to FP16
model.half()

# Check model dtype
def print_model_layer_dtype(model):
    print('\nModel dtypes:')
    for name, param in model.named_parameters():
        print(f""Parameter: {name}, Data type: {param.dtype}"")

print_model_layer_dtype(model)
save_directory = 'temp_model_SE'
model.save_pretrained(save_directory)

model2 = AutoModelForTokenClassification.from_pretrained(save_directory, local_files_only=True, torch_dtype='auto')
print('\n\n##################')
print(model2)
print_model_layer_dtype(model2)

It'll load model2 as torch.float16.
",huggingface-transformers
how to fix the learningrate for huggingface180s trainer,"I'm training model with the following parameters:
Seq2SeqTrainingArguments(
    output_dir                   = ""./out"", 
    overwrite_output_dir         = True,
    do_train                     = True,
    do_eval                      = True,
    
    per_device_train_batch_size  = 2, 
    gradient_accumulation_steps  = 4,
    per_device_eval_batch_size   = 8, 
    
    learning_rate                = 1.25e-5,
    warmup_steps                 = 1,
    
    save_total_limit             = 1,
       
    evaluation_strategy          = ""epoch"",
    save_strategy                = ""epoch"",
    logging_strategy             = ""epoch"",  
    num_train_epochs             = 5,   
    
    gradient_checkpointing       = True,
    fp16                         = True,    
        
    predict_with_generate        = True,
    generation_max_length        = 225,
          
    report_to                    = [""tensorboard""],
    load_best_model_at_end       = True,
    metric_for_best_model        = ""wer"",
    greater_is_better            = False,
    push_to_hub                  = False,
)

I assume that warmup_steps=1 fixes the learning rate.
However, after finished training I'm looking on the file trainer_state.json, and it seems that the learning rate is not fixed.
Here are the values of learning_rate and step:
learning_rate,     steps
1.0006 e-05       1033
7.5062 e-06       2066
5.0058 e-06       3099
2.5053 e-06       4132
7.2618 e-09       5165

It seems that the learning rate is not fixed on 1.25e-5 (after step 1). What am I missing? How to I fix the learning rate.
","A warm-up is in general an increase of the learning rate. It starts at 0 and then increases linearly over 1(here) step to the specified learning rate of 1.25e-5.
Afterwards by default a linear (in other cases a cosine) learning-rate scheduler decays your learning-rate.
To disable the decay add lr_scheduler_type='constant'.
If I recall correctly, this also disables the warmup.
If you want warmup and afterwards a constant rate use constant_with_warmup instead.
",huggingface-transformers
alternative to receptive field in transformers and what factors impact it,"I have two transformer networks. One with 3 heads per attention and 15 layers in total and second one with 5 heads per layer and 30 layers in total. Given an arbitrary set of documents (2048 tokens per each), how to find out, which network is going to be better to use and is less prone to overfitting?
In computer vision we have concept called: ""receptive field"", that allows us to understand how big or small network we need to use. For instance, if we have CNN with 120 layers and CNN with 70 layers, we can calculate their receptive fields and understand which one is going to perform better on a particular dataset of images.
Do you guys have something similar in NLP? How do you understand whether one architecture is more optimal to use versus another，having a set of text documents with unique properties?
","
How do you understand whether one architecture is more optimal to use versus another, having a set of text documents with unique properties?

For modern Transformer-based Language Models (LMs), there are some empirical ""scaling laws,"" such as the Chinchilla scaling laws (Wikipedia), that essentially say that larger (deeper) models with more layers, i.e., with more parameters tend to perform better. So far, most LMs seem to roughly follow Chinchilla scaling. There is another kind of scaling, which is closer to a ""receptive field"", that I talk about below.

Do you guys have something similar in NLP?

Kind of. Transformer-based LMs can be thought to have a ""receptive field"" similar to CNN layers, as the attention mechanism in the Transformer operates on a pre-defined ""context window"" or ""context length"", which is the maximum number of tokens the layer can look at (""attend to"") at any given time, similar to a CNN kernel. However, with the introduction of new positional encoding (PE) approaches, such as Rotary Positional Encoding (RoPE), and modified attention architectures, like Sliding Window Attention (SWA), this is not strictly accurate.
Scaling in terms of ""context length"" is of much interest, but usually, it is very difficult to scale Transformers this way, because of attention being a ($\mathcal{O}(N^2)$) (O(N^2)) operation. So, usually, researchers go towards deeper architectures with more parameters (""over-parameterization"") that can allow the model to ""memorize"" as much of the large training corpus as it can (""overfitting""), so that it can perform reasonably well, when fine-tuned for most down-stream tasks (that have at least some representative examples in the training corpus).
",huggingface-transformers
why aren39t my metrics showing in sagemaker cloudwatch,"I'm training a S-BERT model in SageMaker, using Huggins Face library. I've followed the HF tutorials on how to define metrics to be tracked in the huggingface_estimator, yet when my model is done training I cannot see any metric either in CloudWatch or by fetching the latest training job results:
`
from sagemaker.analytics import TrainingJobAnalytics
df = TrainingJobAnalytics(training_job_name=huggingface_estimator.latest_training_job.name).dataframe()

returns:
Warning: No metrics called loss found
Warning: No metrics called learning_rate found
Warning: No metrics called eval_loss found
Warning: No metrics called eval_accuracy found
Warning: No metrics called eval_f1 found
Warning: No metrics called eval_precision found
Warning: No metrics called eval_recall found
Warning: No metrics called eval_runtime found
Warning: No metrics called eval_samples_per_second found
Warning: No metrics called epoch found

Here's the code below
from sagemaker.huggingface import HuggingFace
from sagemaker import get_execution_role

from sagemaker import image_uris

role = get_execution_role() 

source_dir = 's3://...'
output_path = 's3://...'

metric_definitions = [{'Name': 'loss', 'Regex': ""'loss': ([0-9]+(.|e\-)[0-9]+),?""},
                      {'Name': 'learning_rate', 'Regex': ""'learning_rate': ([0-9]+(.|e\-)[0-9]+),?""},
                      {'Name': 'eval_loss', 'Regex': ""'eval_loss': ([0-9]+(.|e\-)[0-9]+),?""},
                      {'Name': 'eval_accuracy', 'Regex': ""'eval_accuracy': ([0-9]+(.|e\-)[0-9]+),?""},
                      {'Name': 'eval_f1', 'Regex': ""'eval_f1': ([0-9]+(.|e\-)[0-9]+),?""},
                      {'Name': 'eval_precision', 'Regex': ""'eval_precision': ([0-9]+(.|e\-)[0-9]+),?""},
                      {'Name': 'eval_recall', 'Regex': ""'eval_recall': ([0-9]+(.|e\-)[0-9]+),?""},
                      {'Name': 'eval_runtime', 'Regex': ""'eval_runtime': ([0-9]+(.|e\-)[0-9]+),?""},
                      {'Name': 'eval_samples_per_second', 'Regex': ""'eval_samples_per_second': ([0-9]+(.|e\-)[0-9]+),?""},
                      {'Name': 'epoch', 'Regex': ""'epoch': ([0-9]+(.|e\-)[0-9]+),?""}]

estimator_image = image_uris.retrieve(framework='pytorch',region='eu-west-1',version='1.13.1',py_version='py39',image_scope='training', instance_type='ml.p3.2xlarge')


huggingface_estimator = HuggingFace(
                            entry_point='script.py',
                            dependencies=['requirements.txt', 'model.py'],
                            instance_type='ml.p3.2xlarge',
                            base_job_name='...',
                            output_path=output_path,
                            role=role,
                            instance_count=1,
                            pytorch_version=None,
                            py_version=None,
                            metric_definitions = metric_definitions,
                            image_uri=estimator_image,
                            hyperparameters = {
                                'epochs': 1,
                                'train_batch_size': 64,
                                'eval_batch_size':64,
                                'learning_rate': 2e-5,
                                'model_name':'distilbert-base-uncased'})

huggingface_estimator.fit({'train': 's3://...',
                           'test': 's3://...'})

","Turns out the solution was available in this post
If using custom algorithms, and easy way to add metric is to print / log them in the script:
print('METRIC train_accuracy: {}'.format(accuracy))

And in the metric_definitions you have to use the exact same naming in the Regex:
{'Name': 'train_accuracy', 'Regex': ""METRIC train_accuracy: ([0-9]+(.|e\-)[0-9]+)\w+""}

",huggingface-transformers
huggingface  finetuning in tensorflow with custom datasets,"I have been battling with my own implementation on my dataset with a different transformer model than the tutorial, and I have been getting this error AttributeError: 'NoneType' object has no attribute 'dtype', when i was starting to train my model. I have been trying to debug for hours, and then I have tried the tutorial from hugging face as it can be found here https://huggingface.co/transformers/v3.2.0/custom_datasets.html. Running this exact code, so I could identify my mistake, also leads to the same error.
!wget http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz
!tar -xf aclImdb_v1.tar.gz

from pathlib import Path
def read_imdb_split(split_dir):
    split_dir = Path(split_dir)
    texts = []
    labels = []
    for label_dir in [""pos"", ""neg""]:
        for text_file in (split_dir/label_dir).iterdir():
            texts.append(text_file.read_text())
            labels.append(0 if label_dir is ""neg"" else 1)

    return texts, labels

train_texts, train_labels = read_imdb_split('aclImdb/train')
test_texts, test_labels = read_imdb_split('aclImdb/test')

from sklearn.model_selection import train_test_split
train_texts, val_texts, train_labels, val_labels = train_test_split(train_texts, train_labels, test_size=.2)

from transformers import DistilBertTokenizerFast
tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')

train_encodings = tokenizer(train_texts, truncation=True, padding=True)
val_encodings = tokenizer(val_texts, truncation=True, padding=True)
test_encodings = tokenizer(test_texts, truncation=True, padding=True)

import tensorflow as tf

train_dataset = tf.data.Dataset.from_tensor_slices((
    dict(train_encodings),
    train_labels
))
val_dataset = tf.data.Dataset.from_tensor_slices((
    dict(val_encodings),
    val_labels
))
test_dataset = tf.data.Dataset.from_tensor_slices((
    dict(test_encodings),
    test_labels
))

from transformers import TFDistilBertForSequenceClassification

model = TFDistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased')

optimizer = tf.keras.optimizers.Adam(learning_rate=5e-5)
model.compile(optimizer=optimizer, loss=model.compute_loss) # can also use any keras loss fn
model.fit(train_dataset.shuffle(1000).batch(16), epochs=3, batch_size=16)

My goal will be to perform multi-label text classification on my own custom dataset, which unfortunately I cannot share for privacy reasons. If anyone could point out what is wrong with this implementation, will be highly appreciated.
","There seems to be an error, when you are passing the loss parameter.
model.compile(optimizer=optimizer, loss=model.compute_loss) # can also use any keras loss fn

You don't need to pass the loss parameter, if you want to use the model's built-in loss function.
I was able to train the model with your provided source code by changing mentioned line to:
model.compile(optimizer=optimizer)

or by passing a loss function
loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)

model.compile(optimizer=optimizer, loss=loss_fn)

transformers version: 4.20.1
Hope it helps.
",huggingface-transformers
finetuning model39s classifier layer with new label,"I would like to fine-tune already fine-tuned BertForSequenceClassification model with new dataset containing just 1 additional label which hasn't been seen by model before.
By that, I would like to add 1 new label to the set of labels that model is currently able of classifying properly.
Moreover, I don't want classifier weights to be randomly initialized, I'd like to keep them intact and just update them accordingly to the dataset examples while increasing the size of classifier layer by 1.
The dataset used for further fine-tuning could look like this:
sentece,label
intent example 1,new_label
intent example 2,new_label
...
intent example 10,new_label

My model's current classifier layer looks like this:
Linear(in_features=768, out_features=135, bias=True)

How could I achieve it?
Is it even a good approach?
","You can just extend the weights and bias of your model with new values. Please have a look at the commented example below:
#This is the section that loads your model
#I will just use an pretrained model for this example
import torch
from torch import nn
from transformers import AutoModelForSequenceClassification, AutoTokenizer
tokenizer = AutoTokenizer.from_pretrained(""jpcorb20/toxic-detector-distilroberta"")
model = AutoModelForSequenceClassification.from_pretrained(""jpcorb20/toxic-detector-distilroberta"")
#we check the output of one sample to compare it later with the extended layer
#to verify that we kept the previous learnt ""knowledge""
f = tokenizer.encode_plus(""This is an example"", return_tensors='pt')
print(model(**f).logits)

#Now we need to find out the name of the linear layer you want to extend
#The layers on top of distilroberta are wrapped inside a classifier section
#This name can differ for you because it can be chosen randomly
#use model.parameters instead find the classification layer
print(model.classifier)

#The output shows us that the classification layer is called `out_proj`
#We can now extend the weights by creating a new tensor that consists of the
#old weights and a randomly initialized tensor for the new label 
model.classifier.out_proj.weight = nn.Parameter(torch.cat((model.classifier.out_proj.weight, torch.randn(1,768)),0))

#We do the same for the bias:
model.classifier.out_proj.bias = nn.Parameter(torch.cat((model.classifier.out_proj.bias, torch.randn(1)),0))

#and be happy when we compare the output with our expectation 
print(model(**f).logits)

Output:
tensor([[-7.3604, -9.4899, -8.4170, -9.7688, -8.4067, -9.3895]],
       grad_fn=<AddmmBackward>)
RobertaClassificationHead(
  (dense): Linear(in_features=768, out_features=768, bias=True)
  (dropout): Dropout(p=0.1, inplace=False)
  (out_proj): Linear(in_features=768, out_features=6, bias=True)
)
tensor([[-7.3604, -9.4899, -8.4170, -9.7688, -8.4067, -9.3895,  2.2124]],
       grad_fn=<AddmmBackward>)

Please note, that you should fine-tune your model. The new weights are randomly initialized and will therefore negatively impact the performance.
",huggingface-transformers
how to extract image hidden states in llava39s transformers huggingface implementation,"I am using the transformers library (Huggingface) to extract all hidden units of LLaVa 1.5. On the huggingface documentation, it shows that it is possible to extract image hidden states from the vision component.
Unfortunately, the outputs object has only these following keys available in the output dictionary:
odict_keys(['sequences', 'attentions', 'hidden_states', 'past_key_values'])
How do I also extract the image_hidden_states from this LLaVa implementation alongwith the exisiting outputs?
I have implemented the follow code in the hopes to do so.
import torch
from transformers import LlavaForConditionalGeneration, LlavaConfig, CLIPVisionConfig, LlamaConfig, AutoProcessor, LlavaProcessor
from PIL import Image
import requests
from torchinfo import summary

device = ""cuda:0"" if torch.cuda.is_available() else ""cpu""
model_id = 'llava-hf/llava-1.5-7b-hf'

# Initializing a CLIP-vision config
vision_config = CLIPVisionConfig(output_hidden_states=True, output_attentions=True, return_dict=True)

# Initializing a Llama config
text_config = LlamaConfig(output_hidden_states=True, output_attentions=True, return_dict=True)

# Initializing a Llava llava-1.5-7b style configuration
configuration = LlavaConfig(vision_config, text_config, output_hidden_states=True, output_attentions=True, return_dict=True)
cfg=LlavaConfig(vision_config, text_config, output_hidden_states=True, output_attentions=True, return_dict=True)

# Initializing a model from the llava-1.5-7b style configuration
model = LlavaForConditionalGeneration(configuration).from_pretrained(model_id, output_hidden_states=True, output_attentions=True, return_dict=True)

# Accessing the model configuration
configuration = model.config

model=model.to(device)
print(summary(model))

processor = LlavaProcessor.from_pretrained(""llava-hf/llava-1.5-7b-hf"", output_hidden_states=True, output_attentions=True, return_dict=True)
prompt = ""USER: <image>\nIs there sun in the image? ASSISTANT:""
url = ""https://www.ilankelman.org/stopsigns/australia.jpg""
image = Image.open(requests.get(url, stream=True).raw)
inputs = processor(text=prompt, images=image, return_tensors=""pt"")
inputs=inputs.to(device)

with torch.no_grad():
    outputs = model.generate(**inputs, 
                             output_hidden_states=True, 
                             return_dict_in_generate=True, 
                             max_new_tokens=1, 
                             min_new_tokens=1,
                            return_dict=True)

print(outputs.keys())


","Ok, I will try to answer my own question. The solution was quite not available directly with the transformers library. I do not know, why the functionality which is mentioned in their documentation doesn't work. However, I found a work-around by making use of the PyTorch pre-hooks and getting the values of the hidden-units.
",huggingface-transformers
autotokenizerfrom_pretrained took forever to load,"I used the following code to load my custom-trained tokenizer:
from transformers import AutoTokenizer
test_tokenizer = AutoTokenizer.from_pretrained('raptorkwok/cantonese-tokenizer-test')

It took forever to load. Even if I replace the AutoTokenizer with PreTrainedTokenizerFast, it still loads forever.
How to debug or fix this issue?
","The problem is resolved when downgrading transformers version to 4.28.1 from 4.41.0. Both pipeline() and from_pretrained() load the tokenizer successfully in seconds.
",huggingface-transformers
error runtimeerror cuda error operation not supported when tried to locate something into cuda,"Here is my code:
from transformers import AutoModelForCausalLM, AutoTokenizer, QuantoConfig
import torch
device = ""cuda:0""
model_id = ""bigscience/bloom-560m""
quantization_config = QuantoConfig(weights=""int8"")

model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.float32,  device_map=device)

tokenizer = AutoTokenizer.from_pretrained(model_id)

text = ""Hello my name is""
inputs = tokenizer(text, return_tensors=""pt"").to(device)
outputs = model.generate(**inputs, max_new_tokens=20)
print(tokenizer.decode(outputs[0], skip_special_tokens=True))

When I run, I obtain the next error:

RuntimeError: CUDA error: operation not supported CUDA kernel errors
might be asynchronously reported at some other API call, so the
stacktrace below might be incorrect. For debugging consider passing
CUDA_LAUNCH_BLOCKING=1. Compile with TORCH_USE_CUDA_DSA to enable
device-side assertions.

However, when I check if CUDA is available I obtain:
print('-------------------------------')
print(torch.cuda.is_available())
print(torch.cuda.device_count())
print(torch.cuda.current_device())
print(torch.cuda.device(0))
print(torch.cuda.get_device_name(0))
print('Memory Usage:')
print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')
print('Cached:   ', round(torch.cuda.memory_reserved(0)/1024**3,1), 'GB')


True 1 0 <torch.cuda.device object at 0x7f8bf6d4a9b0> GRID T4-16Q
Memory Usage: Allocated: 0.0 GB Cached: 0.0 GB

I run this code on Colab, and I do not have any issues. I also run the code on another machine with another GPU, and it runs as expected.
The configuration of the machine where I need to run it fails.

And the libraries:
accelerate 0.31.0 aiohttp 3.9.5 aiosignal 1.3.1 async-timeout 4.0.3 attrs 23.2.0 certifi 2024.6.2 charset-normalizer 3.3.2 datasets 2.20.0 dill 0.3.8 filelock 3.15.1 frozenlist 1.4.1 fsspec 2024.5.0 huggingface-hub 0.23.4 idna 3.7 Jinja2 3.1.4 MarkupSafe 2.1.5 mpmath
1.3.0 multidict 6.0.5 multiprocess 0.70.16 networkx 3.3 ninja 1.11.1.1 numpy 2.0.0 nvidia-cublas-cu12 12.1.3.1 nvidia-cuda-cupti-cu12
12.1.105 nvidia-cuda-nvrtc-cu12 12.1.105 nvidia-cuda-runtime-cu12 12.1.105 nvidia-cudnn-cu12 8.9.2.26 nvidia-cufft-cu12 11.0.2.54 nvidia-curand-cu12 10.3.2.106 nvidia-cusolver-cu12 11.4.5.107 nvidia-cusparse-cu12 12.1.0.106 nvidia-nccl-cu12 2.20.5 nvidia-nvjitlink-cu12 12.5.40 nvidia-nvtx-cu12 12.1.105 packaging 24.1 pandas 2.2.2 pip 24.0 psutil 5.9.8 pyarrow 16.1.0 pyarrow-hotfix 0.6 python-dateutil 2.9.0.post0 pytz 2024.1 PyYAML 6.0.1 quanto 0.2.0 regex 2024.5.15 requests 2.32.3 safetensors 0.4.3 setuptools 65.5.0 six 1.16.0 sympy 1.12.1 tokenizers 0.19.1 torch 2.3.1 tqdm 4.66.4 transformers 4.42.0.dev0 triton 2.3.1 typing_extensions 4.12.2 tzdata
2024.1 urllib3 2.2.2 xxhash 3.4.1 yarl 1.9.4

I do not know if this affect you, but the machine is a virtual machine with wmware under a vgpu. Also, I tried to run a simple nn, just for check if the problem was with the transformers library, but I obtained the same error when I tried to locate info on the GPU.
 import torch
    import torch.nn as nn
    dev = torch.device(""cuda"") if torch.cuda.is_available() else torch.device(""cpu"")
    t1 = torch.randn(1,2)
    t2 = torch.randn(1,2).to(dev)
    print(t1)  # tensor([[-0.2678,  1.9252]])
    print(t2)  # tensor([[ 0.5117, -3.6247]], device='cuda:0')
    t1.to(dev)
    print(t1)  # tensor([[-0.2678,  1.9252]])
    print(t1.is_cuda) # False
    t1 = t1.to(dev)
    print(t1)  # tensor([[-0.2678,  1.9252]], device='cuda:0')
    print(t1.is_cuda) # True

    class M(nn.Module):
        def __init__(self):        
            super().__init__()        
            self.l1 = nn.Linear(1,2)

        def forward(self, x):                      
            x = self.l1(x)
            return x
    model = M()   # not on cuda
    model.to(dev) # is on cuda (all parameters)
    print(next(model.parameters()).is_cuda) # True


Traceback (most recent call last): File
“/home/admin/llm/ModelsService/test.py”, line 14, in t2 =
torch.randn(1,2).to(dev) RuntimeError: CUDA error: operation not
supported CUDA kernel errors might be asynchronously reported at some
other API call, so the stacktrace below might be incorrect. For
debugging consider passing CUDA_LAUNCH_BLOCKING=1. Compile with
TORCH_USE_CUDA_DSA to enable device-side assertions.

by the way here info about my cuda

(test310) admin@appdev-llm-lnx1:~/llm/ModelsService$ nvcc --version
nvcc: NVIDIA (R) Cuda compiler driver Copyright (c) 2005-2019 NVIDIA
Corporation Built on Sun_Jul_28_19:07:16_PDT_2019 Cuda compilation
tools, release 10.1, V10.1.243

regards
","ok i will responds myself, if someone has a similar eror, just create the next environment variables:

export CUDA_HOME=/usr/local/cuda
export PATH=${CUDA_HOME}/bin:${PATH}
export LD_LIBRARY_PATH=${CUDA_HOME}/lib64:$LD_LIBRARY_PATH

after that pytorch start working correctly
",huggingface-transformers
how to use adapter transformers with a huggingface pipeline,"I tried to run the model ""AdapterHub/bert-base-uncased-pf-conll2003"" (Model description here) for token classification in NLP.
First I tried to install the adapter transformers
pip install -U adapter-transformers 

The output of the above command was
Collecting adapter-transformers

[... see edit history for skipped lines ...]

Installing collected packages: tokenizers, huggingface-hub, adapter-transformers
  Attempting uninstall: tokenizers
    Found existing installation: tokenizers 0.15.0
    Uninstalling tokenizers-0.15.0:
      Successfully uninstalled tokenizers-0.15.0
  Attempting uninstall: huggingface-hub
    Found existing installation: huggingface-hub 0.19.4
    Uninstalling huggingface-hub-0.19.4:
      Successfully uninstalled huggingface-hub-0.19.4
ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.
transformers 4.35.2 requires huggingface-hub<1.0,>=0.16.4, but you have huggingface-hub 0.13.4 which is incompatible.
transformers 4.35.2 requires tokenizers<0.19,>=0.14, but you have tokenizers 0.13.3 which is incompatible.
Successfully installed adapter-transformers-3.2.1.post0 huggingface-hub-0.13.4 tokenizers-0.13.3


I tried to load the model like this into the pipeline:
from transformers import AutoModelWithHeads
from transformers import pipeline
token_classification = pipeline(""token-classification"", model = ""AdapterHub/bert-base-uncased-pf-conll2003"")
res = token_classification(""Take out the trash bag from the bin and replace it."")
print(res)

I received the errors
EntryNotFoundError: 404 Client Error. (Request ID: Root=1-657e793c-0ce0c1936aff5e5741676650)

Entry Not Found for url: https://huggingface.co/AdapterHub/bert-base-uncased-pf-conll2003/resolve/main/config.json.

During handling of the above exception, another exception occurred:


OSError                                   Traceback (most recent call last)
<ipython-input-3-030dfe0e128d> in <cell line: 3>()
      1 from transformers import AutoModelWithHeads
      2 from transformers import pipeline
----> 3 token_classification = pipeline(""token-classification"", model = ""AdapterHub/bert-base-uncased-pf-conll2003"")
      4 res = token_classification(""Take out the trash bag from the bin and replace it."")
      5 print(res)

/usr/local/lib/python3.10/dist-packages/transformers/pipelines/__init__.py in pipeline(task, model, config, tokenizer, feature_extractor, framework, revision, use_fast, use_auth_token, device, device_map, torch_dtype, trust_remote_code, model_kwargs, pipeline_class, **kwargs)
    673         hub_kwargs[""_commit_hash""] = config._commit_hash
    674     elif config is None and isinstance(model, str):
--> 675         config = AutoConfig.from_pretrained(model, _from_pipeline=task, **hub_kwargs, **model_kwargs)
    676         hub_kwargs[""_commit_hash""] = config._commit_hash
    677 

 [... see edit history for skipped lines ...]

/usr/local/lib/python3.10/dist-packages/transformers/configuration_utils.py in _get_config_dict(cls, pretrained_model_name_or_path, **kwargs)
    624             try:
    625                 # Load from local folder or from cache or download from model Hub and cache
--> 626                 resolved_config_file = cached_file(
    627                     pretrained_model_name_or_path,
    628                     configuration_file,

/usr/local/lib/python3.10/dist-packages/transformers/utils/hub.py in cached_file(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, use_auth_token, revision, local_files_only, subfolder, user_agent, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash)
    452         if revision is None:
    453             revision = ""main""
--> 454         raise EnvironmentError(
    455             f""{path_or_repo_id} does not appear to have a file named {full_filename}. Checkout ""
    456             f""'https://huggingface.co/{path_or_repo_id}/{revision}' for available files.""

OSError: AdapterHub/bert-base-uncased-pf-conll2003 does not appear to have a file named config.json.
Checkout 'https://huggingface.co/AdapterHub/bert-base-uncased-pf-conll2003/main' for available files.

How do I correctly load this adapter model?
","# be sure you have the dependencies (NEW)
$ pip install adapters 

The old & legacy package is pip install -U adapter-transformers

Create the model outside of the pipeline
from transformers import AutoModelWithHeads
from transformers import pipeline
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained(""bert-base-uncased"")
model = AutoModelWithHeads.from_pretrained(""bert-base-uncased"")
adapter_name = model.load_adapter(""AdapterHub/bert-base-uncased-pf-conll2003"", source=""hf"")
model.active_adapters = adapter_name

token_classification = pipeline(""token-classification"", model=model, tokenizer=tokenizer)
res = token_classification(""Take out the trash bag from the bin and replace it."")
print(res)

",huggingface-transformers
how can i solve importerror using the trainer with pytorch requires accelerategt0201 when using huggingface39s trainarguments,"I'm using the transformers library in Google colab, and
When i am using TrainingArguments from transformers library i'm getting Import error with this  code:
from transformers import TrainingArguments

training_args = TrainingArguments(
    output_dir = ""/content/our-model"",
    learning_rate=2e-5,
    per_device_train_batch_size= 64,
    per_device_eval_batch_size = 16,
    num_train_epochs = 2,
    weight_decay = 0.01,
    evaluation_strategy = ""epoch"",
    save_strategy = ""epoch"",
    load_best_model_at_end = True,
    push_to_hub = False
)

This is the error i'm getting:
<ipython-input-28-0518ea5ff407> in <cell line: 2>()
      1 from transformers import TrainingArguments
----> 2 training_args = TrainingArguments(
      3     output_dir = ""/content/our-model"",
      4     learning_rate=2e-5,
      5     per_device_train_batch_size= 64,

4 frames
/usr/local/lib/python3.10/dist-packages/transformers/training_args.py in _setup_devices(self)
   1670         if not is_sagemaker_mp_enabled():
   1671             if not is_accelerate_available(min_version=""0.20.1""):
-> 1672                 raise ImportError(
   1673                     ""Using the `Trainer` with `PyTorch` requires `accelerate>=0.20.1`: Please run `pip install transformers[torch]` or `pip install accelerate -U`""
   1674                 )

ImportError: Using the `Trainer` with `PyTorch` requires `accelerate>=0.20.1`: Please run `pip install transformers[torch]` or `pip install accelerate -U 

I already tried pip install for 0.20.1 version of accelerate and pip install transformers[torch]
and both didn't worked.
","If you're not particular about which transformers and accelerate version to tie to, then do this to use the most up-to-date version in Google Colab:
! pip install -U accelerate
! pip install -U transformers

Then the issue you are having with accelerate should auto-resolve itself.
Note:

Underspecifying pip install -U transformers instead of pip install transformers[pytorch] might be easier since that's what most of the users do and the developers of the library will make sure that the basic pip works with the common functions and class like TrainingArguments

Instead of specifying accelerate to the pip install accelerate>=0.20.1, if you have no particular need to fixed the version, automatically upgrading to the latest version might get you more stability when using the library, esp. with ""hot""/""trending"" libraries that are constantly changing (almost) daily.



If further debugging is necessary, i.e. if the above didn't work. To check your transformers and accelerate version, do this:
import accelerate

accelerate.__version__

Most probably you might have an ImportError at the first line if accelerate is not already installed when you installed transformers.
And then if the first line works and the 2nd line is not outputting a version >=0.20.1, then that is the cause of your issue.
The current versions to-date (July 2023) are:
import accelerate
import transformers

transformers.__version__, accelerate.__version__

[out]:
('4.30.1', '0.21.0')

Here's an example notebook with the model that you wish to use as per the comments in your question, https://colab.research.google.com/drive/1D79AjHMeE6HAZC-g2S83baTgsHtDUu5i?usp=sharing

If the error persist after the pip install ..., try restarting the runtime.
If you can't find the buttons to press to restart, try this in the cell Restart kernel in Google Colab then re-run the cells for import ...
import os
os._exit(00)

",huggingface-transformers
embedding of llm vs custom embeddings,"I am new to topic of LLMs (been just 2-3 days) and I've encountered a potential issue in RAG Pipelines. Which assertion is wrong/right?

LLM models utilize the most fundamental units of processing as tokens. Tokens are created via tokenizers (will be specific to a model)

A token is passed into LLM sequentially (from the list of tokens at a time, which also determines the context window)

When ""training"", the ""embeddings"" are randomly initialized. After training , the embedding matrix is created such that there is an embedding for a particular token


Now in RAG, why is it that we are able to 'customize' our own embedding? I understand this helps with vectorsearch of already stored embeddings, but finally when you send all this to the model, does this ""bypass"" the model's embeddings and starts the inference process as it would? Also why don't RAG pipelines mention tokenizers often?
Went through multiple websites but process is abstracted everywhere
There's a mention of ""we create embeddings"" and then done!
","I found the answer....it was a silly little thing. The crux of the matter is , when you're extracting embeddings based on semantic search, the final result(no matter what comes) is ultimately going to be converted into text only, hence the name ""augmented"" generation - essentially a glorified mechanism of enhancing query context as far as this simple application is concerned
So essentially, the two processes (having some custom embedding in vector DB) vs embeddings in the model itself, are naturally separate and don't impact each other
",huggingface-transformers
huggingface pipeline  debug prompt,"I've defined a pipeline using Huggingface transformer library.
pipe = pipeline(
    ""text-generation"",
    model=myllm,
    tokenizer=tokenizer,
    max_new_tokens=512,
)

I'd like to test it:
result = pipe(""Some input prompt for the LLM"")

How can I debug the prompt actually sent to the LLM?
I expect the pipeline to apply the prompt template (tokenizer.default_chat_template) but how can I verify how the prompt is after the template has been applied?
","you may use preprocess method and check generated token_ids. Generally would suggest to more closely look on the code of the method, it will explain what is happening with the prompt before model forward pass.
params = pipe._preprocess_params

pipe.preprocess(""I can't believe you did such a "", **params)

# Returns:
# {'input_ids': tensor([[  40,  460,  470, 1975,  345,  750,  884,  257,  220]]), 
# 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1]]), 
# 'prompt_text': ""I can't believe you did such a ""}

Internally preprocess is calling either

for chats tokenizer.chat_template
for simple text prompts tokenizer(prompt_text).

For example for ""gpt-2"" model default tokenizer outputs token_ids and masks:
pipe.tokenizer(""I can't believe you did such a "")

# Returns:
# {'input_ids': [40, 460, 470, 1975, 345, 750, 884, 257, 220], 
# 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1]}


Another thing to consider during prompts debug is to look what will happen if you'll invert token ids:
pipe = pipeline(
    ""text-generation"",
    model=""openai-community/gpt2""
)

inputs = pipe.tokenizer(""I can't believe you did such a "")

pipe.tokenizer.convert_ids_to_tokens(inputs['input_ids'])

# ['I', 'Ġcan', ""'t"", 'Ġbelieve', 'Ġyou', 'Ġdid', 'Ġsuch', 'Ġa', 'Ġ']

",huggingface-transformers
speeding up load time of llms,"I am currently only able to play around with a V100 on GCP. I understand that I can load a LLM in 4bit quantization as shown below. However, (assuming due to the quantization) it is taking up to 10 minutes to load this model.
Is there a way to speed up this loading process?

I see that there is GGUF file format which may help in this regard (although I am not sure why/ how).
Would doing torch.compile somehow help me load the model next time in a fast manner. My hypothesis being that when compiled, I can save the resulting model in a binary format that can load faster?
Should I be baking the loaded model into the docker image somehow to speed this up? The downside being due to cuda the docker image is already at 4GB.

import torch
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig

device = ""cuda"" # the device to load the model onto
model_id = ""mistralai/Mistral-7B-Instruct-v0.2""
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type=""nf4"",
    bnb_4bit_compute_dtype=torch.bfloat16
)

model = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=bnb_config, device_map=""auto"")

","You havent provided much details about you docker setup. But yes every time you run this docker you will have to download files, until you build your own image which will copy model files into docker, then you can use cache_dir parameter in from_pretrained to point to location of your model.
I am able to load llama3 8b into Tesla M40 in few seconds.
",huggingface-transformers
running out of ram when finetuning model,"I am currently trying to finetune Wav2Vec2 model from: https://huggingface.co/dima806/bird_sounds_classification. But my RAM utilisation is running over the free tier on Google Colab.
The following is my code:
from transformers import TrainingArguments, Trainer

# Load model with ignore_mismatched_sizes=True
model = Wav2Vec2ForSequenceClassification.from_pretrained(
    ""dima806/bird_sounds_classification"",
    num_labels=len(label2id),
    ignore_mismatched_sizes=True
)

# Set up training with gradient accumulation
batch_size = 1  # Reduce batch size to manage memory
accumulation_steps = 4  # Accumulate gradients over 4 steps

training_args = TrainingArguments(
    output_dir=""./results"",
    evaluation_strategy=""epoch"",
    learning_rate=2e-5,
    per_device_train_batch_size=batch_size,
    per_device_eval_batch_size=batch_size,
    gradient_accumulation_steps=accumulation_steps,  # Gradient accumulation
    num_train_epochs=3,
    weight_decay=0.01,
    fp16=True,  # Enable mixed precision training
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=val_dataset,
    tokenizer=feature_extractor,
)

# Train the model
trainer.train()

What could be the reasons the RAM is going past 12.7GB? My dataset only contains 20 items. How can I address this issue?
","The sound inputs were too long, after resampling the audio into chunks, the problem was resolved.
",huggingface-transformers
how to fix error oserror ltmodelgt does not appear to have a file named configjson when loading custom finetuned model,"Preface
I am new to implementing the NLP model. I have successfully fine-tuned LLaMA 3-8B variants with QLORA and uploaded them to HuggingFace.
The directories are filled with these files:
-  .gitattributes
- adapter_config.json
- adapter_model.safetensors
- special_tokens_map.json
- tokenizer.json
- tokenizer_config.json
- training_args.bin

Implementation

I am trying to load this model through this:

model_id_1 = ""ferguso/llama-8b-pcl-v3""

tokenizer_1 = AutoTokenizer.from_pretrained(model_id_1)

quantization_config = BitsAndBytesConfig(
    load_in_8bit=True,
)

model_1 = AutoModelForCausalLM.from_pretrained(
    model_id_1,
    quantization_config=quantization_config,
)

But it shows the error OSError: ferguso/llama-8b-pcl-v3 does not appear to have a file named config.json. Checkout 'https://huggingface.co/ferguso/llama-8b-pcl-v3/tree/main' for available files.

So then I am trying to load the config.json from the original model which is meta-llama/Meta-Llama-3-8B:

original_model = ""meta-llama/Meta-Llama-3-8B""
model_id_1 = ""ferguso/llama-8b-pcl-v3""

tokenizer_1 = AutoTokenizer.from_pretrained(model_id_1)

quantization_config = BitsAndBytesConfig(
    load_in_8bit=True,
)

original_config = AutoConfig.from_pretrained(original_model)
original_config.save_pretrained(model_id_1)

model_1 = AutoModelForCausalLM.from_pretrained(
    model_id_1,
    quantization_config=quantization_config,
    config = original_config
)

But still, it shows another error OSError: Error no file named pytorch_model.bin, model.safetensors, tf_model.h5, model.ckpt.index or flax_model.msgpack found in directory ferguso/llama-8b-pcl-v3.
Questions
How to load the fine-tuned model properly?
","Your directory contains only the files of the peft-adapter and the files required to load the tokenizer, but the base model weights are missing. I assume you have used the save_pretrained method from peft. This method only saves the adapter weights and config (I use a smaller model for my answer and a different task type!):
from peft import LoraConfig, TaskType, get_peft_model, PeftModel
from transformers import AutoModelForTokenClassification
from pathlib import Path

# ferguso/llama-8b-pcl-v3 in your case 
adapter_path = 'bla'
# meta-llama/Meta-Llama-3-8B in your case
base_model_id = ""distilbert/distilbert-base-uncased""

peft_config = LoraConfig(task_type=TaskType.TOKEN_CLS, target_modules=""all-linear"")

# AutoModelForCausalLM in your case
model = AutoModelForTokenClassification.from_pretrained(base_model_id)
model = get_peft_model(model, peft_config)

model.save_pretrained(adapter_path)

print(*list(Path(adapter_path).iterdir()), sep='\n')

Output:
bla/adapter_config.json
bla/README.md
bla/adapter_model.safetensors

To load your pretrained model successfully, you need to load this base_model weights as well and use the peft model class to load the adapter:
model = AutoModelForTokenClassification.from_pretrained(base_model_id)
model = PeftModel.from_pretrained(model, adapter_path)

You can also merge the adapter weights back with merge_and_unload and save it:
model.merge_and_unload().save_pretrained('bla2')
print(*list(Path('bla2').iterdir()), sep='\n')

Output:
bla2/config.json
bla2/model.safetensors

This way you will be able to load the model without peft and only transformers as you tried in the example code of your question.
",huggingface-transformers
presidio transformers package not available despite being installed,"I'm trying to run the example code on this Microsoft documentation, but I am presented with a package not found error. I'm on a MAC and my friend had the same problem on his machine too. I'm sure that a I have installed the transformers package. I imported with no error. I'm on a virtual environment, on a jupyter notebook on vs code.
If I remove the config.yaml file, it runs with no errors, so maybe is something that's in it. But is kinda the same version that is on documentation.
Code:
from presidio_analyzer import AnalyzerEngine, RecognizerRegistry
from presidio_analyzer.nlp_engine import NlpEngineProvider

conf_file = 'config.yaml'

provider = NlpEngineProvider(conf_file=conf_file)
nlp_engine = provider.create_engine()

analyzer = AnalyzerEngine(
    nlp_engine=nlp_engine, 
    supported_languages=[""en""]
)

results_english = analyzer.analyze(text=""My name is Morris"", language=""en"")
print(results_english)

Error stack:
ValueError                                Traceback (most recent call last)
Cell In[3], line 6
      4 # Create NLP engine based on configuration
      5 provider = NlpEngineProvider(conf_file=conf_file)
----> 6 nlp_engine = provider.create_engine()
      8 # Pass the created NLP engine and supported_languages to the AnalyzerEngine
      9 analyzer = AnalyzerEngine(
     10     nlp_engine=nlp_engine, 
     11     supported_languages=[""en""]
     12 )

File ~/Projects/pii/lib/python3.12/site-packages/presidio_analyzer/nlp_engine/nlp_engine_provider.py:81, in NlpEngineProvider.create_engine(self)
     79 nlp_engine_name = self.nlp_configuration[""nlp_engine_name""]
     80 if nlp_engine_name not in self.nlp_engines:
---> 81     raise ValueError(
     82         f""NLP engine '{nlp_engine_name}' is not available. ""
     83         ""Make sure you have all required packages installed""
     84     )
     85 try:
     86     nlp_engine_class = self.nlp_engines[nlp_engine_name]

ValueError: NLP engine 'transformers' is not available. Make sure you have all required packages installed

My config.yaml:
nlp_engine_name: transformers
models:
  -
    lang_code: en
    model_name:
      spacy: en_core_web_sm
      transformers: StanfordAIMI/stanford-deidentifier-base

ner_model_configuration:
  labels_to_ignore:
  - O
  aggregation_strategy: simple # ""simple"", ""first"", ""average"", ""max""
  stride: 16
  alignment_mode: strict # ""strict"", ""contract"", ""expand""
  model_to_presidio_entity_mapping:
    PER: PERSON
    LOC: LOCATION
    EMAIL: EMAIL
    PHONE: PHONE_NUMBER

  low_confidence_score_multiplier: 0.4
  low_score_entity_names:
  - ID

","You will need to install the Presidio Analyzer package with the transformers extra dependency specifier:
pip install ""presidio-analyzer[transformers]""

This will install the extra dependencies needed for the transformers based NLP engine.
",huggingface-transformers
attributeerror 39trainingarguments39 object has no attribute 39model_init_kwargs39,"While finetuning Gemma2B model using QLoRA i'm getting error as AttributeError: 'TrainingArguments' object has no attribute 'model_init_kwargs'
Code:
Loading the libraries
from enum import Enum
from functools import partial
import pandas as pd
import torch

from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, set_seed, BitsAndBytesConfig
from datasets import load_dataset
from trl import SFTTrainer
from peft import get_peft_model, LoraConfig, TaskType

seed = 42
set_seed(seed)


Loading the dataset and preprocess it.

model_name = ""gg-hf/gemma-2b-it""
dataset_name = ""FinGPT/fingpt-fiqa_qa""
tokenizer = AutoTokenizer.from_pretrained(model_name)
template = """"""{% for message in messages %}\n{{'<|im_start|>' + message['role'] + '\n' + message['content'] + '<|im_end|>' + '\n'}}{% if loop.last and add_generation_prompt %}{{'<|im_start|>assistant\n' }}{% endif %}{% endfor %}""""""
tokenizer.chat_template = template

def preprocess(samples):
    batch = []
    for system_prompt, input, output in zip(samples[""instruction""], samples[""input""], samples[""output""]):
        conversation = [{""content"": system_prompt, ""role"": ""system""},
                        {""content"": input, ""role"": ""user""},
                         {""content"": output, ""role"": ""assistant""}]
        batch.append(tokenizer.apply_chat_template(conversation, tokenize=False))
    return {""content"": batch}

dataset = load_dataset(dataset_name)
dataset = dataset.map(
    preprocess,
    batched=True,
    remove_columns=dataset[""train""].column_names
)
dataset = dataset[""train""].train_test_split(0.1)
print(dataset)
print(dataset[""train""][0])


Create PEFT configurations

peft_config = LoraConfig(r=8,
                         lora_alpha=16,
                         lora_dropout=0.1,
                         target_modules=[""gate_proj"",""q_proj"",""lm_head"",""o_proj"",""k_proj"",""embed_tokens"",""down_proj"",""up_proj"",""v_proj""],
                         task_type=TaskType.CAUSAL_LM)

Create Quantization configurations
bnb_config = BitsAndBytesConfig(
            load_in_4bit=True,
            bnb_4bit_quant_type=""nf4"",
            bnb_4bit_compute_dtype=torch.bfloat16,
            bnb_4bit_use_double_quant=True,
        )


Load the model and tokenizer

class ChatmlSpecialTokens(str, Enum):
    user = ""<|im_start|>user""
    assistant = ""<|im_start|>assistant""
    system = ""<|im_start|>system""
    eos_token = ""<|im_end|>""
    bos_token = ""<s>""
    pad_token = ""<pad>""

    @classmethod
    def list(cls):
        return [c.value for c in cls]

tokenizer = AutoTokenizer.from_pretrained(
        model_name,
        pad_token=ChatmlSpecialTokens.pad_token.value,
        bos_token=ChatmlSpecialTokens.bos_token.value,
        eos_token=ChatmlSpecialTokens.eos_token.value,
        additional_special_tokens=ChatmlSpecialTokens.list(),
        trust_remote_code=True
    )
tokenizer.chat_template = template
model = AutoModelForCausalLM.from_pretrained(model_name)
model.resize_token_embeddings(len(tokenizer))
model = get_peft_model(model, peft_config)
model.print_trainable_parameters()
# cast non-trainable params in fp16
for p in model.parameters():
    if not p.requires_grad:
        p.data = p.to(torch.float16)


Training Configurations
output_dir = ""Gemma2B_finetune_QLoRA""
per_device_train_batch_size = 1
per_device_eval_batch_size = 1
gradient_accumulation_steps = 8
logging_steps = 5
learning_rate = 5e-4
max_grad_norm = 1.0
max_steps = 250
num_train_epochs=10
warmup_ratio = 0.1
lr_scheduler_type = ""cosine""
max_seq_length = 2048

training_arguments = TrainingArguments(
    output_dir=output_dir,
    per_device_train_batch_size=per_device_train_batch_size,
    per_device_eval_batch_size=per_device_eval_batch_size,
    gradient_accumulation_steps=gradient_accumulation_steps,
    save_strategy=""no"",
    evaluation_strategy=""epoch"",
    logging_steps=logging_steps,
    learning_rate=learning_rate,
    max_grad_norm=max_grad_norm,
    weight_decay=0.1,
    warmup_ratio=warmup_ratio,
    lr_scheduler_type=lr_scheduler_type,
    fp16=True,
    report_to=[""tensorboard"", ""wandb""],
    hub_private_repo=True,
    push_to_hub=True,
    num_train_epochs=num_train_epochs,
    gradient_checkpointing=True,
    gradient_checkpointing_kwargs={""use_reentrant"": False}
)


Create trainer
trainer = SFTTrainer(
    model=model,
    args=training_arguments,
    train_dataset=dataset[""train""],
    eval_dataset=dataset[""test""],
    tokenizer=tokenizer,
    packing=True,
    dataset_text_field=""content"",
    max_seq_length=max_seq_length,
    peft_config=peft_config,
    dataset_kwargs={
        ""append_concat_token"": False,
        ""add_special_tokens"": False,
    },
)



The error I'm getting is like :-
---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
Cell In[10], line 1
----> 1 trainer = SFTTrainer(
      2     model=model,
      3     args=training_arguments,
      4     train_dataset=dataset[""train""],
      5     eval_dataset=dataset[""test""],
      6     tokenizer=tokenizer,
      7     packing=True,
      8     dataset_text_field=""content"",
      9     max_seq_length=max_seq_length,
     10     peft_config=peft_config,
     11     dataset_kwargs={
     12         ""append_concat_token"": False,
     13         ""add_special_tokens"": False,
     14     },
     15 )

File /usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_deprecation.py:101, in _deprecate_arguments.<locals>._inner_deprecate_positional_args.<locals>.inner_f(*args, **kwargs)
     99         message += ""\n\n"" + custom_message
    100     warnings.warn(message, FutureWarning)
--> 101 return f(*args, **kwargs)

File /usr/local/lib/python3.10/dist-packages/trl/trainer/sft_trainer.py:154, in SFTTrainer.__init__(self, model, args, data_collator, train_dataset, eval_dataset, tokenizer, model_init, compute_metrics, callbacks, optimizers, preprocess_logits_for_metrics, peft_config, dataset_text_field, packing, formatting_func, max_seq_length, infinite, num_of_sequences, chars_per_token, dataset_num_proc, dataset_batch_size, neftune_noise_alpha, model_init_kwargs, dataset_kwargs, eval_packing)
    150     warnings.warn(
    151         ""You passed `model_init_kwargs` to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.""
    152     )
    153     args.model_init_kwargs = model_init_kwargs
--> 154 if args.model_init_kwargs is None:
    155     model_init_kwargs = {}
    156 elif not isinstance(model, str):

AttributeError: 'TrainingArguments' object has no attribute 'model_init_kwargs'

Do let me know if there's any solution for this?
Thanks.
","Just replace your TrainingArguments constructor with SFTConfig constructor, and pass this to SFTTrainer.
from trl import SFTConfig

training_arguments = SFTConfig(your training args ...)

trainer = SFTTrainer(args=training_arguments, rest of the args...) 

",huggingface-transformers
langchain runnableeach invoke error response is not valid json,"I am trying to use RunnableEach to make parallel requests to OpenAI. The responses are supposed to be ""yes/no"" plus a motivation. I want the response to be in JSON format, my code is like the following:
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables.base import RunnableEach
from langchain_openai import ChatOpenAI

class Classification(BaseModel):
   flag: bool = Field(description=""my descr"",enum=[True, False])
   answer: str = Field(description=""my descr"")

llm = ChatOpenAI(temperature=0, model=""gpt-4o-mini"", api_key=model_key).with_structured_output(
            Classification)

prompt = my_prompt

tagging_chain = prompt | llm
runnable_each = RunnableEach(bound=tagging_chain)
input_list = [{""input"": val} for val in mydata]
res = runnable_each.invoke(input_list)

I have around 25k elements in input list, that is 25k requests have to be processed. This has worked fine until today, it is failing because of an error in one request:
Function Classification arguments:

{""flag"":false,""motivation"":""The passage does not contain any information relevant to products

are not valid JSON. Received JSONDecodeError Unterminated string starting at: line 1 column 34 (char 33)

I understand that the call to the llm model returned a malformed string that when parsed as JSON (since I have constrained output), generates an error. I would like to know whether this kind of error can be handled in a way such that the entire process does not fail but only that request. Thanks in advance for your help.
Update
I saw in the langchain documentation that a retry logic is implemented in the library: https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.retry.RunnableRetry.html
By defining
tagging_chain = prompt | llm.with_retry()
runnable_each = RunnableEach(bound=tagging_chain)

Would it retry the single input that is failing or the entire input sequence?
","I found the below in the documentation
return_exceptions (bool) – Whether to return exceptions instead of raising them. Defaults to False.

So basically you just have to pass return_exceptions as True to the RunnableEach and it will just return it and not break the whole thing.
Reference
",langchain
cannot import name 39langchainembedding39 from 39llama_index39,"I'm trying to build a simple RAG, and I'm stuck at this code:
from langchain.embeddings.huggingface import HuggingFaceEmbeddings
from llama_index import LangchainEmbedding, ServiceContext

embed_model = LangchainEmbedding(
  HuggingFaceEmbeddings(model_name=""thenlper/gte-large"")
)
service_context = ServiceContext.from_defaults(
    chunk_size=256,
    llm=llm,
    embed_model=embed_model
)
index = VectorStoreIndex.from_documents(documents, service_context=service_context)

where I get ImportError: cannot import name 'LangchainEmbedding' from 'llama_index'
How can I solve? Is it related to the fact that I'm working on Colab?
","INFO 2024:
This answer worked in 2023 when question was asked
but it seems they moved all code and now you have to use other answers.

Not
from llama_index import LangchainEmbedding

but
from llama_index.embeddings import LangchainEmbedding

(See source code for llama_index/embeddings/__ init__.py)
",langchain
langchain workaround for with_structured_output using chatbedrock,"I'm working with the langchain library to implement a document analysis application.  Especifically I want to use the routing technique described in this documentation. i wanted to follow along the example, but my environment is restricted to AWS, and I am using ChatBedrock instead of ChatOpenAI due to limitations with my deployment.
According to this overview the with_structured_output method, which I need, is not (yet) implemented for models on AWS Bedrock, which is why I am looking for a workaround or any method to replicate this functionality.
The key functionality I am looking for is shown in this example:
from typing import List
from typing import Literal

from langchain_core.prompts import ChatPromptTemplate
from langchain_core.pydantic_v1 import BaseModel, Field
from langchain_openai import ChatOpenAI



class RouteQuery(BaseModel):
    """"""Route a user query to the most relevant datasource.""""""

    datasources: List[Literal[""python_docs"", ""js_docs"", ""golang_docs""]] = Field(
        ...,
        description=""Given a user question choose which datasources would be most relevant for answering their question"",
    )

system = """"""You are an expert at routing a user question to the appropriate data source.

Based on the programming language the question is referring to, route it to the relevant data source.""""""
prompt = ChatPromptTemplate.from_messages(
    [
        (""system"", system),
        (""human"", ""{question}""),
    ]
)

llm = ChatOpenAI(model=""gpt-3.5-turbo-0125"", temperature=0)
structured_llm = llm.with_structured_output(RouteQuery)
router = prompt | structured_llm
router.invoke(
    {
        ""question"": ""is there feature parity between the Python and JS implementations of OpenAI chat models""
    }
)

The output would be:
RouteQuery(datasources=['python_docs', 'js_docs'])

The most important fact for me is that it just selects items from the list without any additional overhead, which makes it possible to setup the right follow up questions.
Did anyone find a workaround how to resolve this issue?
","I found a solution in these two blog posts: here and here.
The key is to use the instructor package, which is a wrapper around pydantic. This means langchain is not necessary.
Here is an example based on the blog posts:
from typing import List
import instructor
from anthropic import AnthropicBedrock
from loguru import logger
from pydantic import BaseModel
import enum

class User(BaseModel):
    name: str
    age: int

class MultiLabels(str, enum.Enum):
    TECH_ISSUE = ""tech_issue""
    BILLING = ""billing""
    GENERAL_QUERY = ""general_query""

class MultiClassPrediction(BaseModel):
    """"""
    Class for a multi-class label prediction.
    """"""
    class_labels: List[MultiLabels]

if __name__ == ""__main__"":
    # Initialize the instructor client with AnthropicBedrock configuration
    client = instructor.from_anthropic(
        AnthropicBedrock(
            aws_region=""eu-central-1"",
        )
    )

    logger.info(""Hello World Example"")

    # Create a message and extract user data
    resp = client.messages.create(
        model=""anthropic.claude-instant-v1"",
        max_tokens=1024,
        messages=[
            {
                ""role"": ""user"",
                ""content"": ""Extract Jason is 25 years old."",
            }
        ],
        response_model=User,
    )

    print(resp)
    logger.info(""Classification Example"")

    # Classify a support ticket
    text = ""My account is locked and I can't access my billing info.""

    _class = client.chat.completions.create(
        model=""anthropic.claude-instant-v1"",
        max_tokens=1024,
        response_model=MultiClassPrediction,
        messages=[
            {
                ""role"": ""user"",
                ""content"": f""Classify the following support ticket: {text}"",
            },
        ],
    )

    print(_class)

",langchain
error with langchain chatprompttemplatefrom_messages,"As shown in LangChain Quickstart, I am trying the following Python code:
from langchain.prompts.chat import ChatPromptTemplate
template = ""You are a helpful assistant that translates {input_language} to {output_language}.""
human_template = ""{text}""

chat_prompt = ChatPromptTemplate.from_messages([
    (""system"", template),
    (""human"", human_template),
])

chat_prompt.format_messages(input_language=""English"", output_language=""French"", text=""I love programming."")

But when I run the above code, I get the following error:
Traceback (most recent call last):
   File ""/home/yser364/Projets/SinappsIrdOpenaiQA/promptWorkout.py"", line 6, in <module>
     chat_prompt = ChatPromptTemplate.from_messages([
   File ""/home/yser364/.local/lib/python3.10/site-packages/langchain/prompts/chat.py"", line 220, in from_messages
     return cls(input_variables=list(input_vars), messages=messages)
   File ""/home/yser364/.local/lib/python3.10/site-packages/langchain/load/serializable.py"", line 64, in __init__
     super().__init__(**kwargs)
   File ""pydantic/main.py"", line 341, in pydantic.main.BaseModel.__init__
 pydantic.error_wrappers.ValidationError: 4 validation errors for ChatPromptTemplate
 messages -> 0
   value is not a valid dict (type=type_error.dict)
 messages -> 0
   value is not a valid dict (type=type_error.dict)
 messages -> 1
   value is not a valid dict (type=type_error.dict)
 messages -> 1
   value is not a valid dict (type=type_error.dict)

I use Python 3.10.12.
","Your example is from the Prompt templates section of the LangChain Quickstart tutorial. I did not spot any differences, so it should work as given.
I tried out the example myself, with an additional loop to output the messages created by chat_prompt.format_messages:
from langchain.prompts.chat import ChatPromptTemplate
template = ""You are a helpful assistant that translates {input_language} to {output_language}.""
human_template = ""{text}""

chat_prompt = ChatPromptTemplate.from_messages([
    (""system"", template),
    (""human"", human_template),
])

messages = chat_prompt.format_messages(input_language=""English"", output_language=""French"", text=""I love programming."")
for message in messages:
    print(message.__repr__())

The example works without any errors. The result is very similar to what is shown in the tutorial, although not identical:
SystemMessage(content='You are a helpful assistant that translates English to French.', additional_kwargs={})
HumanMessage(content='I love programming.', additional_kwargs={}, example=False)

I ran the test with Python 3.9.5 and LangChain 0.0.300, which is the lastest version on PyPI. According to PyPI, it supports Python >=3.8.1 and <4.0.
Maybe your version of LangChain or one of its dependencies is outdated? Try to run it in a new venv with a fresh install of LangChain.
",langchain
langchain chatollama always produces invalid format expected quotjsonquot error,"Just trying to follow a basic langchain tutorial: https://python.langchain.com/v0.2/docs/tutorials/local_rag/
Super simple code:
from langchain_ollama import ChatOllama
import logging

logging.basicConfig(
    level=logging.DEBUG,
    format='%(asctime)s.%(msecs)03d [%(levelname)s]: %(message)s',
    datefmt='%H:%M:%S'
)

logging.info(""### Starting up"")

llm = ChatOllama(
    model=""llama3.1"",
)

response_message = llm.invoke(
    ""What are you?""
)

print(response_message.content)

No matter what I try I always get this error:
Exception has occurred: ResponseError
invalid format: expected ""json"" or a JSON schema
  File ""C:\XXXX\local_rag\main.py"", line 16, in <module>
    response_message = llm.invoke(
                       ^^^^^^^^^^^
ollama._types.ResponseError: invalid format: expected ""json"" or a JSON schema

Tried a few different approaches including messages[], PromptTemplate, streaming etc. from https://python.langchain.com/docs/integrations/chat/ollama/ but always getting the same error.
No issues going via the rest API i.e.
curl http://localhost:11434/api/chat -d '{
  ""model"": ""llama3.1"",
  ""messages"": [
    { ""role"": ""user"", ""content"": ""why is the sky blue?"" }
  ]
}'

Any help would be appreciated. Really hope I am just doing something daft here.
Edit:
The version of Ollama makes a difference but also setting llm.format = None works as suggested in the comment.
","I am seeing the same thing after upgrading Ollama. I just opened an issue here https://github.com/langchain-ai/langchain/issues/28753. For now it should work if you downgrade your version of ollama.
Edit:
This was fixed as of https://github.com/ollama/ollama/releases/tag/v0.5.3
",langchain
no connection because in target machine is actively refusing it,"Here is my code:
from langchain_core.prompts import ChatPromptTemplate
from langchain_ollama import ChatOllama
from langchain_core.output_parsers import StrOutputParser

llm = ChatOllama(
    model = 'llama3.2',
    temperature = 0
)

chat_template = ChatPromptTemplate.from_messages(
    [
        ('system', ""you have to give two line definition of the word given by user""),
        ('human', 'the word is {user_input}')
    ]
)

message = chat_template.format_messages(user_input = 'backlog')
llm.invoke(message)
chain = chat_template | llm | StrOutputParser()

chain.invoke({'user_input' : 'backlog'})

And it is showing connect error:
httpx.ConnectError: [WinError 10061] No connection could be made because the target machine actively refused it

How can I fix this?
I was trying to create basic word meaning chatbot using langchain.
","this means that Ollama is not running on you machine.
after Ollama pull <model name> you need to run Ollama serve
",langchain
no attribute called verbose,"Here is my code:
from langchain_core.prompts import ChatPromptTemplate
from langchain_ollama import ChatOllama

llm = ChatOllama(
    model = 'llama3.2',
    temperature = 0
)

chat_template = ChatPromptTemplate.from_messages(
    [
        ('system', ""you have to give two line definition of the word given by user""),
        ('human', 'the word is {user_input}')
    ]
)

message = chat_template.format_messages(user_input = 'backlog')
print(message)

response = llm.invoke(message)
print(message)

And it is showing Attribute error:
AttributeError: partially initialized module 'langchain' has no attribute 'verbose' (most likely due to a circular import)

How can I fix this?
I was trying to create basic word meaning chatbot using langchain.
","You are getting this error because you have a file named langchain.py
Please change your file name to some other name.
Also, please refrain from naming your file names to module names, keywords, etc.
https://github.com/langchain-ai/langchain/discussions/23071
",langchain
how to get intermediary chain step outputs in final output,"For simplicity sake I have the following chain:

extract names from a list
validate these names against a second list of names

What I want is to receive a JSON with all intermediary steps at the end as well as the input:
{""first_value"": ""Dave, John, carrot"", ""first_prompt_output"" ""Dave, John"", ""possible_values"": ""John""...}

But I am confused by the LC docs and I seem to be able to get all of the inputs using the runnable passthrough, but in a hard to read format. I fiddled with it for half an hour (tried RunnableParallel, RunnablePassthrough.assign(), ...) but I don't seem to be able to get it, there must be some key feature I'm missing.
{'result': {'third_prompt_output': 'Johny'},
 'first_prompt_output': {'first_prompt_output': {'first_prompt_output': ['Dave',
    'John']},
  'possible_values': {'first_value': 'Dave, John',
   'possible_values': ['John']}}}

first_prompt = PromptTemplate.from_template(""""""Find all names in the following text and extract it as JSON with a field `first_prompt_output`: {first_value}""
                                                    first_prompt_output:"""""")

second_prompt = PromptTemplate.from_template(""""""Here is a list of possible values: {possible_values} and a list of found value {first_prompt_output}. Find values that are in both lists. Return a JSON with the fields `first_prompt_output` and `second_prompt_output` and `possible_values`."""""")

first_value = ""Dave, John""
possible_values = [""John""]

first_chain = (
    first_prompt
    | llm
    | SimpleJsonOutputParser()
)

second_chain = (
    second_prompt
    | llm
    | SimpleJsonOutputParser()
)

chain = (
    {""first_prompt_output"": first_chain, ""possible_values"": RunnablePassthrough(), ""first_value"": RunnablePassthrough()} 
    | RunnableParallel(result={""second_prompt_output"": second_chain, ""first_value"": itemgetter(""first_value"")})
)

chain.invoke({""first_value"": first_value, ""possible_values"": possible_values})

I tried using the RunnableParallel or RunnablePassthrough.assign, but neither does what I expect it to do. What I basically need is a dict.update() but in the pipeline.
","I have not found a way to do this with LangChain, but I found a function that allows me to flatten the output and results in what I want, although it seems a bit clunky and I believe there must be a better solution.
The key is to add the following function to the chain:
def flatten_dict(*vars) -> dict:
    '''
    Flatten a dictionary by removing unnecessary mid-level keys.
    Returns a Runnable (chainable) function.
    '''
    flat = {}
    for var in vars:
        keys = [k for k in var]
        for key in keys:
            if isinstance(var[key], dict):
                flat.update(var[key])
            else:
                flat[key] = var[key]
    return flat

chain = (
    {""first_prompt_output"": first_chain, ""possible_values"": RunnablePassthrough(), ""first_value"": RunnablePassthrough()} 
    | RunnableParallel(result={""second_prompt_output"": second_chain, ""first_value"": itemgetter(""first_value"")})
)
| flatten_dict

",langchain
type quotvectorquot does not exist on postgresql  langchain,"I was trying to embed some documents on postgresql with the help of pgvector extension and langchain. Unfortunately I'm having trouble with the following error:
(psycopg2.errors.UndefinedObject) type ""vector"" does not exist
LINE 4:  embedding VECTOR(1536), 
                   ^

[SQL: 
CREATE TABLE langchain_pg_embedding (
    collection_id UUID, 
    embedding VECTOR(1536), 
    document VARCHAR, 
    cmetadata JSON, 
    custom_id VARCHAR, 
    uuid UUID NOT NULL, 
    PRIMARY KEY (uuid), 
    FOREIGN KEY(collection_id) REFERENCES langchain_pg_collection (uuid) ON DELETE CASCADE
)
]


My environment info:

pgvector docker image ankane/pgvector:v0.4.1
python 3.10.6, psycopg2 2.9.6, pgvector 0.1.6


List of installed extensions on postgres
  Name   | Version |   Schema   |                Description                 
---------+---------+------------+--------------------------------------------
 plpgsql | 1.0     | pg_catalog | PL/pgSQL procedural language
 vector  | 0.4.1   | public     | vector data type and ivfflat access method

I've tried the following ways to resolve:

Fresh installing the Postgres docker image with pgvector extension enabled.
Manually install the extension with the official instruction.
Manually install the extension on Postgres like the following:

CREATE EXTENSION IF NOT EXISTS vector
    SCHEMA public
    VERSION ""0.4.1"";

But no luck.
","Update 17th July 2023
As previously I mentioned my issue was somewhere else in my configuration, here is the other reason that may be responsible for the error,

The pgvector extension isn't enabled in the database you are using. Make sure you run CREATE EXTENSION vector; in each database you are using for storing vectors.
The vector schema is not in the search_path. Run SHOW search_path; to see the available schemas in the search path and \dx to see the list of installed extensions with schemas.


Unfortunately, the issue was somewhere else. My extension installation and search_path schema were totally okay for the defined database I was supposed to use. But my environment variable which was responsible for which database to use, got messed up and was using the default database postgres instead of my defined database, which didn't have the extension enabled.
",langchain
is there a way to load a saved sklearn vectorstore using langchain,"I created and saved a vectorstore using langchain_community.vectorstores SKLearnVectorStore and I can't load it.
I created and saved vectorstore as below:
from langchain_community.vectorstores import SKLearnVectorStore

vectorstore = SKLearnVectorStore.from_texts(
    texts=doc_splits,
    embedding=OllamaEmbeddings(model=""Gemma-2:9b""),
    persist_path=""assets/vectorstore"",
)
vectorstore.persist()

I want to use this vectorstore in another file how do I use it?
","Save the Vectors
vectorstore = SKLearnVectorStore.from_documents(
        documents=doc_splits,
        persist_path=PERSIST_PATH,
        embedding=OllamaEmbeddings(model=""Gemma-2:9b""),
        serializer=""parquet"",
    )

vectorstore.persist()

Load the Saved parquet file
vectorstore = SKLearnVectorStore(
            persist_path=PERSIST_PATH,
            embedding=OllamaEmbeddings(model=""Gemma-2:9b""),
            serializer=""parquet""
        )
docs = vectorstore.similarity_search(query)

*Note: PERSIST_PATH is the path where you would like to save the file and load it.
Refer: https://python.langchain.com/docs/integrations/vectorstores/sklearn/*
",langchain
change environment variable set in jupyter notebook with magics,"Hope this question is not already answered, I've searched many posts but none of them says how to change an environment variable.
I wanted to load environment variables in .env so followed the steps in this post.
%env ""OPENAI_API_KEY""=OPENAI_API_KEY

Then I wanted to see my variable with:
%env OPENAI_API_KEY

Everything worked well but my mistake was to erroneously copy the value for my environment variable. So I want to change it.
I tried dotenv to load this variable again but seems not to work.
Hope you can help me, please.
I tried doing again the same procedure so that it can be changed/updated.
","If you made a mistake writing your APIKEY:
Created a .env with your credentials:
OPENAI_API_KEY=""...erroneous-api-key...""

You verified:
%env OPENAI_API_KEY

And you realized it is wrong.
You can:
import os
del os.environ[""OPENAI_API_KEY""]

Then:
os.environ[""OPENAI_API_KEY""]=""...right-key...""

To verify this:
%env OPENAI_API_KEY

And the updated api-key should be prompted.
",langchain
langchain how to use a custom embedding model locally,"I am trying to use a custom embedding model in Langchain with chromaDB. I can't seem to find a way to use the base embedding class without having to use some other provider (like OpenAIEmbeddings or HuggingFaceEmbeddings). Am I missing something?
On the Langchain page it says that the base Embeddings class in LangChain provides two methods: one for embedding documents and one for embedding a query. so I figured there must be a way to create another class on top of this class and overwrite/implement those methods with our own methods. But how do I do that?
I tried to somehow use the base embeddings class but am unable to create a new embedding object/class on top of it.
","You can create your own class and implement the methods such as embed_documents. If you strictly adhere to typing you can extend the Embeddings class (from langchain_core.embeddings.embeddings import Embeddings) and implement the abstract methods there. You can find the class implementation here.
Below is a small working custom embedding class I used with semantic chunking.
from sentence_transformers import SentenceTransformer
from langchain_experimental.text_splitter import SemanticChunker
from typing import List


class MyEmbeddings:
    def __init__(self):
        self.model = SentenceTransformer(""sentence-transformers/all-MiniLM-L6-v2"")

    def embed_documents(self, texts: List[str]) -> List[List[float]]:
        return [self.model.encode(t).tolist() for t in texts]


embeddings = MyEmbeddings()

splitter = SemanticChunker(embeddings)

",langchain
how can i get llm to only respond in json strings,"This is how I am defining the executor
const executor = await initializeAgentExecutorWithOptions(tools, model, {
  agentType: 'chat-conversational-react-description',
  verbose: false,
});

Whenever I prompt the AI I have this statement at the end.
type SomeObject = {
  field1: number,
  field2: number,
}

- It is very critical that you answer only as the above object and JSON stringify it as a single string.
  Don't include any other verbose explanatiouns and don't include the markdown syntax anywhere.

The SomeObject is just an example. Usually it will have a proper object type.
When I use the executor to get a response from the AI, half the time I get the proper JSON string, but the other half the times are the AI completely ignoring my instructions and gives me a long verbose answer in just plain English...
How can I make sure I always get the structured data answer I want?
Maybe using the agentType: 'chat-conversational-react-description' isn't the right approach here?
","Update Nov. 6, 2023
OpenAI announced today a new “JSON Mode” at the DevDay Keynote. When activated the model will only generate responses using the JSON format.
You can refer to the official docs here.
Original Answer
That's a great question and LangChain provides an easy solution. Look at LangChain's Output Parsers if you want a quick answer. It is the recommended way to process LLM output into a specified format.
Here's the official link from the docs:

JavaScript: https://js.langchain.com/docs/how_to/output_parser_structured/
Python: https://python.langchain.com/docs/how_to/output_parser_structured/


Side note: I wrote an introductory tutorial about this particular issue but for Python, so if anyone else is interested in more details you can check it out here.
The example below does not use initializeAgentExecutorWithOptions, but will ensure that the output is processed as JSON without specifying this explicitly in your system prompt.
How it works
In order to tell LangChain that we'll need to convert the LLM response to a JSON output, we'll need to define a StructuredOutputParser and pass it to our chain.
Defining our parser:
Here's an example:
// Let's define our parser
const parser = StructuredOutputParser.fromZodSchema(
  z.object({
    field1: z.string().describe(""first field""),
    field2: z.string().describe(""second field"")
  })
);

Adding it to our Chain:
// We can then add it to our chain
const chain = RunnableSequence.from([
  PromptTemplate.fromTemplate(...),
  new OpenAI({ temperature: 0 }),
  parser, // <-- this line
]);

Invoking our chain with format_instructions:
// Finally, we'll pass the format instructions to the invoke method
const response = await chain.invoke({
  question: ""What is the capital of France?"",
  format_instructions: parser.getFormatInstructions(), // <-- this line
});

Go ahead and log the parser.getFormatInstructions() method before you call invoke if you'd like to see the output.
When we pass parser.getFormatInstructions() to the format_instructions property, this lets LangChain append the desired JSON schema that we defined in step 1 to our prompt before sending it to the large language model.
As a final point, it is absolutely critical to make sure your query/prompt is relevant and produces values that could be interpreted as the properties in your object SomeObject that are defined in the parser.
Please give this a try, and let me know if you're able to consistently output JSON.
",langchain
conversationsummarybuffermemory is not fully defined you should define basecache then call conversationsummarybuffermemorymodel_rebuild,"I am attempting to use LangChain's ConversationSummaryBufferMemory and running into this error:
pydantic.errors.PydanticUserError: `ConversationSummaryBufferMemory` is not fully defined; you should define `BaseCache`, then call `ConversationSummaryBufferMemory.model_rebuild()`.

This is what my code looks like:
memory = ConversationSummaryBufferMemory(
    llm=llm,
    input_key=""input"",
    output_key=""output"",
    max_token_limit=args.get(""max_tokens"", DEFAULT_MAX_TOKENS),
    memory_key=""chat_history"",
)

I am using langchain==0.3.7.
Has anyone else encountered this?
","pydantic library has been updated.
pip install pydantic==2.9.2 should help
",langchain
getting access denied error when hitting api although api is working fine when running on localhost via django,"I used GPT4all to create a RAG Pipeline with the help of langchain. When I use API for other purposes, it works fine but when I Hit the RAG pipeline, It gives access denied error.
Error getting: Permission Error
Root Cause: Unable to create model download directory in system profile. Access denied to 'C:\Windows\system32\config\systemprofile.cache'

Environment:
Python Version: 3.12
Operating System: Windows
Key Packages:

langchain
gpt4all
pydantic

Full Error:
{
    ""error"":""Error in Database Chain: Failed to create model download directory"",
    ""traceback"":""Traceback (most recent call last):

File ""C:\Python312\Lib\site-packages\gpt4all\gpt4all.py"", line 323, in retrieve_model
    os.makedirs(DEFAULT_MODEL_DIRECTORY, exist_ok=True)

File """", line 215, in makedirs

File """", line 225, in makedirs
    PermissionError: [WinError 5] Access is denied: 
    'C:\\Windows\\system32\\config\\systemprofile\\.cache'

The above exception was the direct cause of the following exception:

Traceback (most recent call last):

File ""C:\inetpub\wwwroot\hashmove-ai\HMAI\Controllers\ConversationalAIv2_Controller.py"", line 143, in post
    hmgpt_response = query_response(input_query, query_intent)

File ""C:\inetpub\wwwroot\hashmove-ai\HMAI_Business\ConversationalAIv2_Business.py"", line 281, in query_response
    return response_functions.get(intent_response, irrelevant)(query)

File ""C:\inetpub\wwwroot\hashmove-ai\HMAI_Business\ConversationalAIv2_Business.py"", line 251, in logistics
    chain = log_chain(load_llm(), vector_db(), memory(), log_prompt())

File ""C:\inetpub\wwwroot\hashmove-ai\HMAI_Business\ConversationalAIv2_Business.py"", line 90, in load_llm
    loaded_llm = GPT4All(

File ""C:\Python312\Lib\site-packages\langchain_core\load\serializable.py"", line 125, in init
    super().init(*args, **kwargs)

File ""C:\Python312\Lib\site-packages\pydantic\main.py"", line 212, in init
    validated_self = self.pydantic_validator.validate_python(data, self_instance=self)

File ""C:\Python312\Lib\site-packages\pydantic\_internal\_decorators_v1.py"", line 148, in _wrapper1
    return validator(values)

File ""C:\Python312\Lib\site-packages\langchain_core\utils\pydantic.py"", line 208, in wrapper
    return func(cls, values)

File ""C:\Python312\Lib\site-packages\langchain_community\llms\gpt4all.py"", line 145, in validate_environment
    values[""client""] = GPT4AllModel(

File ""C:\Python312\Lib\site-packages\gpt4all\gpt4all.py"", line 235, in init
    self.config: ConfigType = self.retrieve_model(model_name, model_path=model_path, allow_download=allow_download, verbose=verbose) 

File ""C:\Python312\Lib\site-packages\gpt4all\gpt4all.py"", line 325, in retrieve_model
    raise RuntimeError(""Failed to create model download directory"") from e

RuntimeError: Failed to create model download directory
""}

Tried giving manuall path to the cache:
os.environ[""TIKTOKEN_CACHE_DIR""] = ""embeddings-cache""

But didn't worked.
Note: I've deployed this on a server using IIS server.
","
I've deployed this on a server using IIS server

The error message is clear, it is an identity permission problem. What identity does your application access as? Anonymous identity or something else?
But based on the error message, it seems that your code related to some system driver folder permission. You can try to set admin permission in Application pool identity.

Open the Application Pools node underneath the machine node. Select the application pool you want to change.
Right click the application pool and select Advanced Settings.
Select the Identity list item and click the ellipsis (the button with the three dots).
Check Custom account and Set....
Enter Enter the local administrator name and password.


",langchain
how to stream from a server action,"I use Next.js server actions instead of API routes to make a request to OpenAI:
lib/gpt.ts:
export async function getAISuggestion(session: Session) {
  const prompt = ...

  const { stream, handlers } = LangChainStream();

  chatModel.predictMessages(prompt, {callbacks: [handlers]});

  return stream;
}

The server action calls it:
actions.ts:
export async function getSuggestion(): Promise<
  ServerActionResponse<ReadableStream<Uint8Array>>
> {
  try {
    [...]

    const stream = await GPT.getAISuggestion(session);
    
    return { data: stream };
  } catch (error) {
    console.error(error);
    [...]
  }
}

I'm trying to call this server action from a client component:
ClientComponent.tsx:
""use client"";

[...]

function foo {
    [...]

    const { data, error } = await getSugestion();
    
    if (data) {
      const reader = data.getReader(); 
      const decoder = new TextDecoder();
      let done = false;
      let text ="""";      
      while (!done) {
        const { value, done: readerDone } = await reader.read();
        console.log(""reading"", value);
        done = readerDone;
        text += decoder.decode(value);
        setAiSuggestionsText(text);
      }
    } else {
      setError(error);
    }
}

But apparently, you can't send a stream from a server action to a client component:

Warning: Only plain objects can be passed to Client Components from Server Components. ReadableStream objects are not supported.
{data: ReadableStream}

","Unfortunately, Next.js doesn't appear to support returning HTTP responses from server actions. You will need to use a Route-handler if you want to return a response object.
",langchain
how to custom stream events in langgraph,"I'm using langgraph astream_events. I defined a graph, just like:
call_llm = get_history_messages | llm.bind(tools)

def call_tool(messages: List[BaseMessage]):
    response = ...
    return ToolMessage(tool_call_id=uuid4(), content=json.dumps(response))

graph = StateGraph()
graph.add_node(Node.LLM, call_llm)
graph.add_node(Node.Tool, call_tool)
graph.add_edge(Node.LLM, Node.Tool)
graph.add_edge(Node.Tool, Node.End)
graph.compile()

human_message = HumanMessage(content=...)
async for event in graph.astream_events(human_message, version=""v2"", stream_mode=""values""):
    print(event)

When I call this method, I receive a large amount of event data, most of which I don't need. I wanna customize the output of LLM node and Tool node. How should I do this?
","Sounds like you're only interested in some event types, perhaps the one the contains the prompt and all the events for response stream chunks?
event_types_to_keep = {""on_chat_model_start"", ""on_chat_model_stream""} # customize as necessary

async def filter_events(event_stream):
    async for event in event_stream:
        if not event_types_to_keep or event.get(""event"") in event_types_to_keep:
            if ""input"" in event[""data""]:
                filtered_data = event[""data""][""input""][""messages""]
            if ""chunk"" in event[""data""]:
                filtered_data = event[""data""][""chunk""]
            
            # Yield a new event structure with the filtered data
            yield {
                ""event"": event[""event""],
                ""data"": filtered_data,
            }

async for event in filter_events(
    chain.astream_events({""topic"": ""parrot""}, version=""v2"")
):
    print(event)

which prints:
{'event': 'on_chat_model_start', 'data': [[HumanMessage(content='tell me a joke about parrot', additional_kwargs={}, response_metadata={})]]}
{'event': 'on_chat_model_stream', 'data': AIMessageChunk(content='', additional_kwargs={}, response_metadata={}, id='run-ffbf3d59-13f2-49ca-8988-b90c81cf3c9b')}
{'event': 'on_chat_model_stream', 'data': AIMessageChunk(content=""Here's a par"", additional_kwargs={}, response_metadata={}, id='run-ffbf3d59-13f2-49ca-8988-b90c81cf3c9b')}
{'event': 'on_chat_model_stream', 'data': AIMessageChunk(content='rot joke for', additional_kwargs={}, response_metadata={}, id='run-ffbf3d59-13f2-49ca-8988-b90c81cf3c9b')}

",langchain
impossible de find a chroma vector db collection created with langchain,"I am using this code to create a Chroma Vector DB (I have skipped non essential part for the code)
    import os
    from langchain_community.vectorstores import Chroma
    from langchain_openai import OpenAIEmbeddings    

    current_dir = os.path.dirname(os.path.abspath(__file__))
    file_path = os.path.join(current_dir, ""books"", ""ulysses.txt"")
    persistent_directory = os.path.join(current_dir, ""db"", ""chroma_db_Ulysse"")
    
    db = Chroma.from_documents(
            docs, embeddings, persist_directory=persistent_directory,collection_name=""CollectionUlysse"")

It works fine, except when I tried to access this database with Chroma, it can't find the collection :
    import chromadb
    import os
    from chromadb.config import Settings
    
    
    current_dir = os.path.dirname(os.path.abspath(__file__))
    persistent_directory = os.path.join(current_dir, ""db"", ""chroma_db_Ulysse_is_back"")
    
    print(f""Chemin du répertoire persistant : {persistent_directory}"")
    client = chromadb.Client(Settings(persist_directory=persistent_directory))
    
    collections = client.list_collections()
    collection_names = [col.name for col in collections]
    print(""Available collections :"", collection_names)

Available collections : []

Can it be differences between chromadb lib and Chroma lib frol langchain ?
","can you try using the PersistentClient instead of Client with config. The Client is meant for programatic configuration via env vars or settings. In recent versions new settings were introduces which may make supplying persistent_directory not enough to create a persistent client.
I have created a persistent dir with Langchain🦜🔗 ran your code and arrived at the same conclusion. After inspecting the sqlite3 file I can confirm that the collection is indeed created and using PersistentClient solved the problem:
import chromadb
import os
from chromadb.config import Settings


client = chromadb.PersistentClient(""db"")

collections = client.list_collections()
collection_names = [col.name for col in collections]
print(""Available collections :"", collection_names)

",langchain
ollama with python  chat is stuck on the first prompt,"I'm testing a local GPT with Ollama running on a Flask server. I've developed an interface to chat using Llama3.2 model.
I've managed to create the chat history and the chatbot answers according to the context, but for some reason it gets stuck on the instructions of the first prompt so when I tell it to stop, it doesn't. Let me illustrate:

User: We are gonna play a game in whic I'm gonna say some words and you have to tell me if the word represents an animal. I just want you to tell me if the word I'm saying is an animal or not
system: I'll respond with ""yes"" or ""no"" to let you know if the word represents an animal. Go ahead and give your first word!
User: horse
system: It's an animal!
User: elephant
system: Here are my answers: 1. No, ""horse"" is an animal. 2. Yes, ""elephant"" is an animal. Let's keep playing! What's the next word?
User: table
system: I'll let you know if each word represents an animal. - horse: Yes, it's an animal. - elephant: Yes, it's an animal. - table: No, it's not an animal. (it's a piece of furniture)
User: bottle
system: Let's get started with the game. Here are my answers: 1. horse - Yes, it's an animal. 2. elephant - Yes, it's an animal. 3. table - No, it's not an animal. 4. bottle - No, it's not an animal. Did I get them right?
User: dolphin
system: Here are my answers: - No (it's a piece of furniture) - Yes - No (it's an inanimate object) - No (it's an inanimate object, a container) - Yes
User: ok stop the game, let's do something else
system: Here are my responses: * horse: yes * elephant: yes * table: no * bottle: no * dolphin: yes

The thing is, when I try the same thing directly on cmd (ollama run llama3.2) it behaves as expected:

-> We are gonna play a game in whic I'm gonna say some words and you have to tell me if the word represents an animal.I just want you to tell me if the word I'm saying is an animal or not
I'm ready, let's play. What's the first word?
-> horse
Yes.
-> elephant
Yes.
-> table
No.
-> bottle
No.
-> dolphin
Yes.
-> ok stop the game, let's do something else
It was fun playing the animal game with you. What would you like to do next? Would you like to play a different game, have a conversation, or try something else? Let me know!

I don't know what I should change in my code or what is the parameter to control this behavior. My code:
from flask import Flask, render_template, request

from ollama import chat
from langchain_core.messages import AIMessage
from langchain_ollama import ChatOllama

app = Flask(__name__)
MODEL = ""llama3.2""
MODEL_MAX_TOKENS = 128000
messages = []

llm = ChatOllama(
    model=MODEL,
    temperature=1.1,
    stream=True
)

@app.route('/ask_gpt', methods=['POST'])
def ask_gpt():
    messages.append((""user"", request.form['text']))
    number_of_tokens = ut.get_number_of_tokens(messages)

    # Context accepts a max number of tokens
    while(number_of_tokens >= MODEL_MAX_TOKENS):
        del(messages[1])

    ai_msg = llm.invoke(messages)
    return ai_msg.content

Could anyone guide me through this? Thanks in advance.
","I found out where the problem was.
I was appending my messages to the history but not the GPT answers so it was unable to understand that questions were already answered.
There's is only one line to add just before the ""return"" statement:
messages.append({'role': 'assistant', 'content': ai_msg.content})
",langchain
kernel crash while excecuting vector embedding operation in chromadb,"I'm trying to build a chatbot using Ollama locally. And I'm stuck with my embedding process (chromadb).
When I provide a full pdf, the kernel crashes during the embedding process.
It works fine when I provide a single chapter from the book.
Problem block:
# Create the Chroma vector store
from langchain_chroma.vectorstores import Chroma
try:
    vector_db = Chroma.from_documents(
        documents=chunked_document,
        embedding=embedding_model,
        collection_name=""local-rag"",
        persist_directory=""./db/db_nomic""
    )
    print(""Embedded Documents stored in ChromaDB successfully!"")
except Exception as e:
    print(f""An error occurred: {e}"")

output
Note
embedding_model = OllamaEmbeddings(model=""nomic-embed-text"")

chunked_document = [Document(metadata={'source': 'xxx', 'page': 1, 'math_expressions': 'xxx'}, page_content=''), .... ]

Additional info:
Python version = 3.12.7
What I've tried so far:

I tried moving the code to a python file instead of using jupyter notebook. The execution stops at the same block.
execution stops after chunking

I tried to embed in batches. The same issue occurs


from langchain_chroma.vectorstores import Chroma
vector_db = Chroma(
    collection_name=""local-rag"",
    persist_directory=""./dtbs/db_nomic"",
    embedding_function=embedding_model
)
texts = [chunk.page_content for chunk in chunked_document]
metadatas = [chunk.metadata for chunk in chunked_document]
batch_size = 100
for i in range(0, len(texts), batch_size):
    batch_texts = texts[i:i+batch_size]
    batch_metadatas = metadatas[i:i+batch_size]
    vector_db.add_texts(texts=batch_texts, metadatas=batch_metadatas)


I tried to merge all the databases in a single database(as it is working when I provide single chapters). No luck there either

chapter_paths = [
    ""./partial_databases/db_nomic/ch1"",
    ""./partial_databases/db_nomic/ch2"",
    ...,
]
vector_db = Chroma(
    collection_name = ""local-rag"",
    persist_directory = ""./db/db_nomic"",
    embedding_function = embedding_model
)
# Merge documents from each chapter database into main_db
for path in chapter_paths:
    chapter_db = Chroma(
        collection_name = ""local-rag"",
        persist_directory=path,
        embedding_function=embedding_model
    )
    
    # Retrieve all documents (vectors) from the current chapter database
    chapter_data = chapter_db.get()    
    
    # Extract documents and metadatas
    docs = chapter_data['documents']
    metadatas = chapter_data['metadatas']
        
    vector_db.add_texts(texts=docs, metadatas=metadatas)

print(""Documents successfully merged into main database."")

I'm expecting to create a vector database using chromadb to store the whole pdf (246 pages)
","These steps solved my issue:

Created a Virtual Environment
Moved all the code from Jupyter Notebook to a python file
Installed necessary dependencies with pip
Ran the python file

As the problem was solved by fresh installation of the dependencies, Most probably I faced the issue because of some internal dependency conflict.
",langchain
langchain python with structured output ollama functions,"I am following this guide to set up a self-RAG.
I am not allowed to use OpenAI models at the moment, so I've been using ChatOllama models instead. I want to pipe outputs using the ""with_structured_output()"" function, with OllamaFunctions instead of ChatOllama. It is demonstrated here.
Essentially here is the code:
from langchain_experimental.llms.ollama_functions import OllamaFunctions


from langchain_core.prompts import PromptTemplate
from langchain_core.pydantic_v1 import BaseModel, Field


# Schema for structured response
class Person(BaseModel):
    name: str = Field(description=""The person's name"", required=True)
    height: float = Field(description=""The person's height"", required=True)
    hair_color: str = Field(description=""The person's hair color"")


# Prompt template
prompt = PromptTemplate.from_template(
    """"""Alex is 5 feet tall. 
Claudia is 1 feet taller than Alex and jumps higher than him. 
Claudia is a brunette and Alex is blonde.

Human: {question}
AI: """"""
)

# Chain
llm = OllamaFunctions(model=""phi3"", format=""json"", temperature=0)
structured_llm = llm.with_structured_output(Person)
chain = prompt | structured_llm

I get two errors that bring me to a dead end. The first one is:
ValidationError: 1 validation error for OllamaFunctions
__root__
  langchain_community.chat_models.ollama.ChatOllama() got multiple values for keyword argument 'format' (type=type_error)

so I changed
llm = OllamaFunctions(model=""phi3"", format=""json"", temperature=0)
to
llm = OllamaFunctions(model=""phi3"", temperature=0)
and that brings me to the next line at least. Then, the with_structured_output(Person) line fails with error:
File ~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/langchain_core/language_models/base.py:208, in BaseLanguageModel.with_structured_output(self, schema, **kwargs)
    204 def with_structured_output(
    205     self, schema: Union[Dict, Type[BaseModel]], **kwargs: Any
    206 ) -> Runnable[LanguageModelInput, Union[Dict, BaseModel]]:
    207     """"""Implement this if there is a way of steering the model to generate responses that match a given schema.""""""  # noqa: E501
--> 208     raise NotImplementedError()

NotImplementedError:

And I don't know where to go from here. Anything would help. Thanks!
","Hobakjuk found the issue: pip, github, webdoc versions of ollama_functions are out of sync. which requires a temp workaround until the pypi version is updated.
The Workaround involves:

ctrl+c copy code contents from github ollama_functions.py

make a local ollama_functions.py file, ctrl+v paste code into it

in your python code then import the 'patched' local library by replacing
from langchain_experimental.llms.ollama_functions import OllamaFunctions
with
from ollama_functions import OllamaFunctions


keep track of your code
",langchain
langchain chromadb chromafromdocuments returns typeerror cannot read properties of undefined reading 39data39,"I am running a langChain process on a node local server.
On my code :
     // Create docs with a loader
const loader = new TextLoader(""Documentation/hello.txt"");
const docs = await loader.load();

// Create vector store and index the docs
const vectorStore = await Chroma.fromDocuments(docs, new OpenAIEmbeddings(), {
collectionName: ""z-test-collection"",
});

// Search for the most similar document
const response = await vectorStore.similaritySearch(""hello"", 1);

console.log(response);

I get the following error message on
const vectorStore = await Chroma.fromDocuments(docs, new OpenAIEmbeddings(), {
collectionName: ""z-test-collection"",
});:
/home/alexandre/projects/langChain/ProcessGPT/node_modules/chromadb/dist/main/index.js:291
        return response.data;
                        ^
TypeError: Cannot read properties of undefined (reading 'data')
at /home/alexandre/projects/langChain/ProcessGPT/node_modules/chromadb/dist/main/index.js:291:29
at process.processTicksAndRejections (node:internal/process/task_queues:95:5)
at async ChromaClient.getOrCreateCollection (/home/alexandre/projects/langChain/ProcessGPT/node_modules/chromadb/dist/main/index.js:286:31)
at async Chroma.ensureCollection (/home/alexandre/projects/langChain/ProcessGPT/node_modules/langchain/dist/vectorstores/chroma.cjs:60:31)
at async Chroma.addVectors (/home/alexandre/projects/langChain/ProcessGPT/node_modules/langchain/dist/vectorstores/chroma.cjs:77:28)
at async Chroma.addDocuments (/home/alexandre/projects/langChain/ProcessGPT/node_modules/langchain/dist/vectorstores/chroma.cjs:52:9)
at async Chroma.fromDocuments (/home/alexandre/projects/langChain/ProcessGPT/node_modules/langchain/dist/vectorstores/chroma.cjs:121:9)
at async testChroma (/home/alexandre/projects/langChain/ProcessGPT/controllers/backendController.js:31:25)

The same error message appears regardless of the situation in which the method is called.
Are there other requirements appart from the ""npm install -S langchain"" and ""npm install -S chromadb"" ?
Thank you in advance
","For langchainJS v0.1 you have to install first ChromaDB (build it with docker-compose) as shown in the first step
https://js.langchain.com/v0.1/docs/integrations/vectorstores/chroma/#setup
",langchain
rag using langchain  chroma  unable to save more than 99 records to database,"I'm using the following code to load the content of markdown files (only one file, in my case), split it into chunks and then embed and store the chunks one by one. My file is split into 801 chunks. However, this code is unable to save the embeddings to disk in the vector db.
def load_documents():
loader = DirectoryLoader(DATA_PATH, glob=""*.md"")

documents = loader.load()

return documents

def split_text(documents: list[Document]):
text_splitter = RecursiveCharacterTextSplitter(

    chunk_size=300,

    chunk_overlap=100,

    length_function=len,

    add_start_index=True,

)

chunks = text_splitter.split_documents(documents)

print(f""Split {len(documents)} documents into {len(chunks)} chunks."")



document = chunks[10]

print(document.page_content)

print(document.metadata)



return chunks

def save_to_chroma(chunks: list[Document]):
# Clear out the database first.

if os.path.exists(CHROMA_PATH):

    shutil.rmtree(CHROMA_PATH)



# Create a new DB from the documents.

db = Chroma.from_documents(

    chunks, OpenAIEmbeddings(), persist_directory=CHROMA_PATH

)

While analysing this problem, I attempted to save the chunks one by one instead, using a for loop:
for i, chunk in enumerate(chunks):
db = Chroma.from_documents(
chunks, OpenAIEmbeddings(), persist_directory=CHROMA_PATH
)
I found that the code does save up to 99 chunks / embeddings, but always crashes when it tries to save further data. To investigate further, I opened the underlying database using DB Browser for SQLite and saw that Chroma was saving a max of 99 records in the 'embeddings' table. However, I was able to manually add more records to it i.e. exceeding 99.
I also tried a few variations i.e. by:

saving the chunks in batches of 10 or 30
not deleting the existing database already containing 99 records, and then attempting to write more data
playing around with the chunk size and chunk overlap

However, none of the above made any difference.
Does anybody know why this is happening and how to solve this problem?
","My problem was solved when I re-installed python on my pc due to some other problem. Now, when I run the code, it works like magic! All the chunks are being saved now regardless of whether I save them all in one go or in batches.
EDIT:
The problem was actually due to a conflict in the installed libraries. It gets resolved when you use a virtual environment, so no need to re-install python.
",langchain
how to remove items from langchain memory,"inside the langchain memory object there are different methods e.g. ConversationBufferMemory or ConversationBufferWindowMemory
Regardless, if the conversation get long at somepoint I get the following error
openai.error.InvalidRequestError: This model's maximum context length is 4097 tokens. However, your messages resulted in 4113 tokens (4063 in the messages, 50 in the functions). Please reduce the length of the messages or functions.

because the context get full. I wonder how I can remove the last object or apply some moving window strategy ?
","If memory is an instance of ConversationBufferMemory, for example, you can access past messages with memory.chat_memory.messages.
The method memory.clean() sets memory.chat_memory.messages to an empty list, so it deletes all memory. See here and here for the respective code parts. What you can do is
memory.chat_memory.messages = memory.chat_memory.messages[:-2]

to, for example, delete the last two messages.
To overwrite them, you could then use
memory.save_context({""input"": ""Servus""}, {""output"": ""Grüezi!""})

update Oct 2024
I could delete last 8 instances by
memory.messages = memory.messages[:-8]
where memory is created at the beginning using
memory = ChatMessageHistory()
",langchain
how do i fix this error related to langchain invoke ai chatbot in python with the chatgpt api and langchain for memory storage,"Here's my code:
import pickle, os
from langchain_openai.chat_models import ChatOpenAI
from langchain.schema import (
    AIMessage,
    HumanMessage,
    SystemMessage
)

def execute_prompt(text, history, jarvis_setup):
    print(f""You said: {text}"")
    history.append(HumanMessage(content = text))
    response = jarvis_setup(history)
    history.append(AIMessage(content = response.content))
    with open('JarvisMemory.txt', 'wb') as file:
        pickle.dump(history, file)
        
    print(response.content)

def main():
    jarvis_setup = ChatOpenAI(openai_api_key=""API_KEY"", model = ""gpt-3.5-turbo"", temperature = 0.7, max_tokens = 400)
    #history = [SystemMessage(content=""You are a human-like virtual assistant named Jarvis."", additional_kwargs={})]
    if os.path.exists(""JarvisMemory.txt""):
        with open(""JarvisMemory.txt"", ""rb"") as file:
            history = pickle.load(file)
    else:
        with open(""JarvisMemory.txt"", ""wb"") as file:
            history = [SystemMessage(content=""You are a human-like virtual assistant named Jarvis. Answer all questions as shortly as possible, unless a longer, more detailed response is requested."", additional_kwargs={})]
            pickle.dump(history, file)
    
    while True:
        print(""\n"")
        print(""Enter prompt."")
        text = input().lower()
        print(""Prompt sent."")
    
        if text:
            execute_prompt(text, history, jarvis_setup)
                        
        else:
            print(""No prompt given."")
            continue
                    
if __name__ == ""__main__"":
    main()

And I get this error:
LangChainDeprecationWarning: The method BaseChatModel.__call__ was deprecated in langchain-core 0.1.7 and will be removed in 0.3.0. Use invoke instead.
warn_deprecated(
Traceback (most recent call last):
File ""C:\Users\maste\Documents\Coding\Python\Jarvis\JarvisTextInpuhjhjghyjvjt.py"", line 44, in 
main()
File ""C:\Users\maste\Documents\Coding\Python\Jarvis\JarvisTextInpuhjhjghyjvjt.py"", line 37, in main
execute_prompt(text, history, jarvis_setup)
File ""C:\Users\maste\Documents\Coding\Python\Jarvis\JarvisTextInpuhjhjghyjvjt.py"", line 12, in execute_prompt
response = jarvis_setup(history)
File ""C:\Users\maste\AppData\Roaming\Python\Python310\site-packages\langchain_core_api\deprecation.py"", line 148, in warning_emitting_wrapper
return wrapped(*args, **kwargs)
File ""C:\Users\maste\AppData\Roaming\Python\Python310\site-packages\langchain_core\language_models\chat_models.py"", line 847, in call
generation = self.generate(
File ""C:\Users\maste\AppData\Roaming\Python\Python310\site-packages\langchain_core\language_models\chat_models.py"", line 456, in generate
raise e
File ""C:\Users\maste\AppData\Roaming\Python\Python310\site-packages\langchain_core\language_models\chat_models.py"", line 446, in generate
self._generate_with_cache(
File ""C:\Users\maste\AppData\Roaming\Python\Python310\site-packages\langchain_core\language_models\chat_models.py"", line 671, in _generate_with_cache
result = self._generate(
File ""C:\Users\maste\AppData\Roaming\Python\Python310\site-packages\langchain_openai\chat_models\base.py"", line 520, in _generate
message_dicts, params = self._create_message_dicts(messages, stop)
File ""C:\Users\maste\AppData\Roaming\Python\Python310\site-packages\langchain_openai\chat_models\base.py"", line 533, in _create_message_dicts
message_dicts = [_convert_message_to_dict(m) for m in messages]
File ""C:\Users\maste\AppData\Roaming\Python\Python310\site-packages\langchain_openai\chat_models\base.py"", line 533, in 
message_dicts = [_convert_message_to_dict(m) for m in messages]
File ""C:\Users\maste\AppData\Roaming\Python\Python310\site-packages\langchain_openai\chat_models\base.py"", line 182, in _convert_message_to_dict
if (name := message.name or message.additional_kwargs.get(""name"")) is not None:
AttributeError: 'SystemMessage' object has no attribute 'name'
I'm guessing I need to add "".invoke"" somewhere in the code based on some research I did on the issue, but I'm a beginner.
I found this website showcasing a very similar error and how to fix it: https://wikidocs.net/235780
You can translate the page to English with Google Translate and the translations are sufficient to understand. It says to add "".invoke"" in the place you can see shown on the website. Not sure how to implement this into my code though. Also, this might not be the right solution.
I also looked at the Langchain website and it also says to use ""invoke"" but I can't find examples of it being used in a full line of code.
","Here's  the solution! I just figured it out. Very simple mistake! When changing the langchain_community to langchain_openai, remove the "".chat_models""! That's all it was!
So this line:
from langchain_community.chat_models import ChatOpenAI
Should be this:
from langchain_openai import ChatOpenAI
This is how I figured it out:
https://python.langchain.com/v0.2/docs/versions/v0_2/#upgrade-to-new-imports
Also, at least in my code, I had to add "".invoke"" after jarvis_setup here:
response = jarvis_setup(history)
With those two changes, I get no warnings and no errors!
",langchain
aggregating output from langchain lcel elements,"I have two chains, one that generates a document and one that creates a short document resume. I want to chain them, using the output from the first on inside the other one. But I want to get both outputs in the result.
Before LCEL, I could do it using LLMChain's output_key parameter. With LCEL, there seems to be a RunnablePassthrough class, but I don't seem to get how to use it to aggregate the output.
Code example:
generate_document_chain = generate_document_prompt | llm | StrOutputParser()
resume_document_chain = resume_document_prompt | llm | StrOutputParser()

aggregated_chain = generate_document_chain | resume_document_chain 
content = aggregated_chain.invoke({""topic"": topic})

","Perhaps the following is what you want. It feeds the output of the first chain into second chain as input.
from langchain_core.runnables import RunnablePassthrough

aggregated_chain = generate_document_chain | {
    ""first_chain_output"": RunnablePassthrough(),
    ""second_chain_output"": resume_document_chain
}
content = aggregated_chain.invoke({""topic"": topic})

Then the output will be a dictionary with keys: ""first_chain_output"" and ""second_chain_output"".
You can also use RunnablePassthrough.assign. Unlike the case above, the key of generate_document_chain output should match the input variable name of the second chain. Below, the input variable of the second chain is  assumed to be ""input"" (btw, the input variable of the first chain is ""topic"").
aggregated_chain = (
    {""input"": generate_document_chain} 
    | RunnablePassthrough.assign(second_chain_output=resume_document_chain)
)

The output of this chain will be a dict with keys: ""input"" and ""second_chain_output"".
",langchain
unexpected value passed to langchain tool argument,"I'm new to LLMs, AI app development, and LangChain.
I'm trying to create a simple example tool that creates new user accounts in a hypothetical application when instructed to do so via a user prompt. The llm being used is llama3.1:8b via Ollama.
So far what I've written works, but it's very unreliable.
The reason why it's unreliable is because when LangChain calls on my tool, it provides unexpected/inconsistent values to the user creation tool's single username argument.
Sometime the argument will be a proper username and other times it will be a username with the value ""username="" prefixed to the username (eg: ""username=jDoe"" rather than simply ""jdoe"").
Also, if I ask for multiple users to be created, sometimes langchain will correctly invoke the tool multiple times while other times, it will invoke the tool once with a string in the format of an array (eg: ""['jDoe','jSmith']"")
What I've just explained is my problem.
My questions to everyone are:

Is the issue I'm encountering due to the limitations of LangChain or the Llama3.1:8b model that I'm using? Or is the issue something else?
Is there a way to get LangChain to more reliably call my user creation tool with a correctly formatted username?
Are there are other useful tips/recommendations that you can provide for a beginner like me?

Below is my code:
from dotenv import load_dotenv
from langchain.agents import AgentExecutor, create_react_agent
from langchain.tools import Tool
from langchain_core.prompts import PromptTemplate
from langchain_ollama.chat_models import ChatOllama

load_dotenv()


# Define the tool to create a user account
mock_user_db = [""jDoe"", ""jRogers"", ""jsmith""]


def create_user_tool(username: str):
    print(""USERNAME PROVIDED FOR CREATION: "" + username)
    if username in mock_user_db:
        return f""User {username} already exists.""
    mock_user_db.append(username)
    return f""User {username} created successfully.""


# Define the tool to delete a user account
def delete_user_tool(username: str):
    print(""USERNAME PROVIDED FOR DELETION: "" + username)
    if username not in mock_user_db:
        return f""User {username} does not exist.""
    
    mock_user_db.remove(username)
    return f""User {username} deleted successfully.""


def list_users_tool(ignore) -> list:
    return mock_user_db


# Wrap these functions as LangChain Tools
create_user = Tool(
    name=""Create User"",
    func=create_user_tool,
    description=""Creates a new user account in the company HR system.""
)

delete_user = Tool(
    name=""Delete User"",
    func=delete_user_tool,
    description=""Deletes an existing user account in company HR system.""
)

list_users = Tool(
    name=""List Users"",
    func=list_users_tool,
    description=""Lists all user accounts in company HR system.""
)

# Initialize the language model
llm = ChatOllama(model=""llama3.1:latest"", temperature=0)

# Create the agent using the tools
tools = [create_user, delete_user, list_users]

# Get the prompt to use
#prompt = hub.pull(""hwchase17/react"") # Does not work with ollama/llama3:8b
prompt = hub.pull(""hwchase17/react-chat"") # Kinda works with ollama/llama3:8b

agent = create_react_agent(llm, tools, prompt)

# Create an agent executor by passing in the agent and tools
agent_executor = AgentExecutor(agent=agent, tools=tools, handle_parsing_errors=True)

print(agent_executor.invoke({""input"": ""Please introduce yourself.""})['output'])

while True:
    user_prompt = input(""PROMPT: "")
    agent_response = agent_executor.invoke({""input"": user_prompt})
    print(agent_response['output'])


","Prompt engineering (what you are attempting here), is far from an exact science.  However, there are ways you can clarify the schema of the tool.
One example (from their docs) is getting it to parse your docstrings:
@tool(parse_docstring=True)
def create_user(username: str):
    """"""Creates a user

        Args:
            username: username of the user to be created. The exact string of the username, no longer than 20 characters long
    """"""
    ... # Rest of your code here

See docs here
But even more reliable would be to create your schema with Pydantic (great tool in general), again, from their docs:

class create_user(BaseModel):
    """"""Creates a user""""""

    username: str = Field(..., description=""username of the user to be created. The exact string of the username, no longer than 20 characters long""

In general, the more detail you provide, regarding the shape and nature of the tools and the data, the better results you can expect.
You may also want to consider setting your temperature to 0, so you get repeatable responses for any given prompt, which should help with debugging, but you need to test with a higher range of prompts to ensure reliable behaviour
",langchain
how to fix 39fs module not found39 error when using langchain document loaders in nextjs,"I am working on an AI project. I am using Langchain and Next.js 13.
I am trying to use the document loaders in langchain to load my PDF, however when I call a loader eg
import { PDFLoader } from ""langchain/document_loaders/fs/pdf"";

Immediately I get an error:
fs module not found

As per langchain documentation, this should not occur as it states that the APIs support Next.js enviroment.
I have tried using a package(pdf.js) to handle PDF file upload and parsing, but I get an error with this as well.
How can I use langchain document loaders in Next.js?
","I finally understand the issue here.
I was calling it in the browser context but once I moved my code to the API routes(Node) environment, everything worked as expected.
",langchain
importerror cannot import name 39gptsimplevectorindex39 from 39llama_index39,"I am getting an ImportError while using GPTSimpleVectorIndex from the llama-index library. Have installed the latest version of llama-index library and trying to run it on python 3.9.
from llama_index import GPTSimpleVectorIndex, SimpleDirectoryReader, LLMPredictor, PromptHelper, ServiceContext
ImportError: cannot import name 'GPTSimpleVectorIndex' from 'llama_index' (E:\Experiments\OpenAI\data anaysis\llama-index-main\venv\lib\site-packages\llama_index\__init__.py

The source code is given below,
import os, streamlit as st

from llama_index import GPTVectorStoreIndex, SimpleDirectoryReader, LLMPredictor, PromptHelper, ServiceContext
from langchain.llms.openai import OpenAI

","Try use GPTVectorStoreIndex instead of GPTSimpleVectorIndex:
from llama_index import GPTVectorStoreIndex, ..

",langchain
runtimeerror no validator found for ltclass 39langchain_communitytoolsjsontooljsonspec39gt see arbitrary_types_allowed in config,"I am getting RuntimeError: no validator found for <class 'langchain_community.tools.json.tool.JsonSpec'>, see arbitrary_types_allowed in Config even after to installing
import yaml
from langchain_community.agent_toolkits import JsonToolkit, create_json_agent
from langchain_community.tools.json.tool import JsonSpec
from langchain_openai import OpenAI

","The error you're encountering due to incorrect Pydantic configuration.
LangChain uses Pydantic models for validation, and this error can occur when Pydantic does know to handle certain types, such as the JsonSpec.
Pydantic's Config class has an option called arbitrary_types_allowed that needs to be set to True in such cases.
SOLUTIONS

always work in a virtual environment. (recommended)

check install pydantic and other modules related to langchain like langchain-community or langchain-openai. (recommended)

use the following code to set arbitrary_types_allowed to True
  from pydantic import BaseModel

  class Maintain(BaseModel):
        class Config:
              arbitrary_types_allowed = True



",langchain
splitting html file and saving chunks using langchain,"I'm very new to LangChain, and I'm working with around 100-150 HTML files on my local disk that I need to upload to a server for NLP model training. However, I have to divide my information into chunks because each file is only permitted to have a maximum of 20K characters. I'm trying to use the LangChain library to do so, but I'm not being successful in splitting my files into my desired chunks.
For reference, I'm using this URL:  http://www.hadoopadmin.co.in/faq/ Saved locally as HTML only.
It's a Hadoop FAQ page that I've downloaded as an HTML file onto my PC. There are many questions and answers there. I've noticed that sometimes, for some files, it gets split by a mere title, and another split is the paragraph following that title. But my desired output would be to have the title and the specific paragraph or following text from the body of the page, and as metadata, the title of the page.
I'm using this code:
from langchain_community.document_loaders import UnstructuredHTMLLoader
from langchain_text_splitters import HTMLHeaderTextSplitter
# Same Example with the URL http://www.hadoopadmin.co.in/faq/ Saved Locally as HTML Only
dir_html_file='FAQ – BigData.html'

data_html = UnstructuredHTMLLoader(dir_html_file).load()

headers_to_split_on = [
    (""h1"", ""Header 1"")]
html_splitter = HTMLHeaderTextSplitter(headers_to_split_on=headers_to_split_on)
html_header_splits = html_splitter.split_text(str(data_html))

But is returning a bunch of weird characters and not splitting the document at all.
This is an output:
[Document(page_content='[Document(page_content=\'BigData\\n\\n""You can have data without information, but you cannot have information without Big data.""\\n\\nsaurabhmcakiet@gmail.com\\n\\n+91-8147644946\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nToggle navigation\\n\\nHome\\n\\nBigData\\n\\n\\tOverview of BigData\\n\\tSources of BigData\\n\\tPros & Cons of BigData\\n\\tSolutions of BigData\\n\\nHadoop Admin\\n\\n\\tHadoop\\n\\t\\n\\t\\tOverview of HDFS\\n\\t\\tOverview of MapReduce\\n\\t\\tApache YARN\\n\\t\\tHadoop Architecture\\n\\t\\n\\n\\tPlanning of Hadoop Cluster\\n\\tAdministration and Maintenance\\n\\tHadoop Ecosystem\\n\\tSetup HDP cluster from scratch\\n\\tInstallation and Configuration\\n\\tAdvanced Cluster Configuration\\n\\tOverview of Ranger\\n\\tKerberos\\n\\t\\n\\t\\tInstalling kerberos/Configuring the KDC and Enabling Kerberos Security\\n\\t\\tConfigure SPNEGO Authentication for Hadoop\\n\\t\\tDisabled kerberos via ambari\\n\\t\\tCommon issues after Disabling kerberos via Ambari\\n\\t\\tEnable https for ambari Server\\n\\t\\tEnable SSL or HTTPS for Oozie Web UI\\n\\nHadoop Dev\\n\\n\\tSolr\\n\\t\\n\\t\\tSolr Installation\\n\\t\\tCommits and Optimizing in Solr and its use for NRT\\n\\t\\tSolr FAQ\\n\\t\\n\\n\\tApache Kafka\\n\\t\\n\\t\\tKafka QuickStart\\n\\t\\n\\n\\tGet last access time of hdfs files\\n\\tProcess hdfs data with Java\\n\\tProcess hdfs data with Pig\\n\\tProcess hdfs data with Hive\\n\\tProcess hdfs data with Sqoop/Flume\\n\\nBigData Architect\\n\\n\\tSolution Vs Enterprise Vs Technical Architect’s Role and Responsibilities\\n\\tSolution architect certification\\n\\nAbout me\\n\\nFAQ\\n\\nAsk Questions\\n\\nFAQ\\n\\nHome\\n\\nFAQ\\n\\nFrequently\\xa0Asked Questions about Big Data\\n\\nMany questions about big data have yet to be answered in a vendor-neutral way. With so many definitions, opinions run the gamut. Here I will attempt to cut to the heart of the matter by addressing some key questions I often get from readers, clients and industry analysts.\\n\\n1) What is Big Data?\\n\\n1) What is Big Data?\\n\\nBig data” is an all-inclusive term used to describe vast amounts of information. In contrast to traditional structured data which is typically stored in a relational database, big data varies in terms of volume, velocity, and variety.\\n\\nBig data\\xa0is characteristically generated in large volumes – on the order of terabytes or exabytes of data (starts with 1 and has 18 zeros after it, or 1 million terabytes) per individual data set.\\n\\nBig data\\xa0is also generated with high velocity – it is collected at frequent intervals – which makes it difficult to analyze (though analyzing it rapidly makes it more valuable).\\n\\nOr in simple words we can say “Big Data includes data sets whose size is beyond the ability of traditional software tools to capture, manage, and process the data in a reasonable time.”\\n\\n2) How much data does it take to be called Big Data?\\n\\nThis question cannot be easily answered absolutely. Based on the infrastructure on the market the lower threshold is at about 1 to 3 terabytes.\\n\\nBut using Big Data technologies can be sensible for smaller databases as well, for example if complex mathematiccal or statistical analyses are run against a database. Netezza offers about 200 built in functions and computer languages like Revolution R or Phyton which can be used in such cases.\\n\\

My Expected output will look something like this:
One chunk:

Frequently Asked Questions about Big Data

Many questions about big data have yet to be answered in a vendor-neutral way. With so many definitions, opinions run the gamut. Here I will attempt to cut to the heart of the matter by addressing some key questions I often get from readers, clients and industry analysts.

1) What is Big Data?
“Big data” is an all-inclusive term used to describe vast amounts of information. In contrast to traditional structured data which is typically stored in a relational database, big data varies in terms of volume, velocity, and variety. Big data is characteristically generated in large volumes – on the order of terabytes or exabytes of data (starts with 1 and has 18 zeros after it, or 1 million terabytes) per individual data set. Big data is also generated with high velocity – it is collected at frequent intervals – which makes it difficult to analyze (though analyzing it rapidly makes it more valuable).
Or in simple words we can say “Big Data includes data sets whose size is beyond the ability of traditional software tools to capture, manage, and process the data in a reasonable time.”
2) How much data does it take to be called Big Data?
This question cannot be easily answered absolutely. Based on the infrastructure on the market the lower threshold is at about 1 to 3 terabytes.
But using Big Data technologies can be sensible for smaller databases as well, for example if complex mathematical or statistical analyses are run against a database. Netezza offers about 200 built in functions and computer languages like Revolution R or Phyton which can be used in such cases.

Metadata: FAQ


Another Chunck
7) Where is the big data trend going?
Eventually the big data hype will wear off, but studies show that big data adoption will continue to grow. With a projected $16.9B market by 2015 (Wikibon goes even further to say $50B by 2017), it is clear that big data is here to stay. However, the big data talent pool is lagging behind and will need to catch up to the pace of the market. McKinsey & Company estimated in May 2011 that by 2018, the US alone could face a shortage of 140,000 to 190,000 people with deep analytical skills as well as 1.5 million managers and analysts with the know-how to use the analysis of big data to make effective decisions.
The emergence of big data analytics has permanently altered many businesses’ way of looking at data. Big data can take companies down a long road of staff, technology, and data storage augmentation, but the payoff – rapid insight into never-before-examined data – can be huge. As more use cases come to light over the coming years and technologies mature, big data will undoubtedly reach critical mass and will no longer be labeled a trend. Soon it will simply be another mechanism in the BI ecosystem.
8) Who are some of the BIG DATA users?
From cloud companies like Amazon to healthcare companies to financial firms, it seems as if everyone is developing a strategy to use big data. For example, every mobile phone user has a monthly bill which catalogs every call and every text; processing the sheer volume of that data can be challenging. Software logs, remote sensing technologies, information-sensing mobile devices all pose a challenge in terms of the volumes of data created. The size of Big Data can be relative to the size of the enterprise. For some, it may be hundreds of gigabytes, for others, tens or hundreds of terabytes to cause consideration.
9) Data visualization is becoming more popular than ever.
In my opinion, it is absolutely essential for organizations to embrace interactive data visualization tools. Blame or thank big data for that and these tools are amazing. They are helping employees make sense of the never-ending stream of data hitting them faster than ever. Our brains respond much better to visuals than rows on a spreadsheet.
Companies like Amazon, Apple, Facebook, Google, Twitter, Netflix and many others understand the cardinal need to visualize data. And this goes way beyond Excel charts, graphs or even pivot tables. Companies like Tableau Software have allowed non-technical users to create very interactive and imaginative ways to visually represent information.

Metadata: FAQ  

My thought process is being able to gather all the information and split it into chunks, but I don't want titles without their following paragraphs separated, and I also want as much info as possible (max 20K characters) before creating another chunk.
I would also like to save these chunks and their meta data. Is there a function in LangChain to do this?
I am open to hearing not to do this in LangChain for efficiency reasons.
","check this super amazing HTML chunking package :package:
pip install html_chunking

Our HTML chunking algorithm operates through a well-structured process that involves several key stages, each tailored to efficiently chunk and merge HTML content while adhering to a token limit. This approach is highly suitable for scenarios where token limitations are critical, and the need for accurate HTML parsing is paramount, especially in tasks like web automation or navigation where HTML content serves as input.

For those of you who are interested in this, here's a demo


from html_chunking import get_html_chunks
merged_chunks = get_html_chunks(your_html_string_here, max_tokens=1000, is_clean_html=True, attr_cutoff_len=25)
merged_chunks


The output should consists of several HTML chunks, where each chunk contains valid HTML code with preserved structure and attributes (from root node all the way down to current node), and any excessively long attributes are truncated to the specified length.

Check out the html_chunking PYPI page and our Github page for more example DEMO!!

For those who are investigating the BEST way of chunking HTML for web automation or any other web agent tasks, you should definitely try html_chunking!!

LangChain (HTMLHeaderTextSplitter & HTMLSectionSplitter) and LlamaIndex (HTMLNodeParser) split text at the element level and add metadata for each header relevant to the chunk. However, they extract only the text content and exclude the HTML structure, attributes, and other non-text elements, limiting their use for tasks requiring the full HTML context.

Check our Github repo below and star :star2:
https://github.com/KLGR123/html_chunking


",langchain
valueerror could not use apoc procedures please ensure the apoc plugin is installed in neo4j and that 39apocmetadata39 is allowed in neo4j,"I'm trying to use the Neo4jGraph class from the langchain_community.graphs module in my Python project to interact with a Neo4j database. My script here:
from langchain.chains import GraphCypherQAChain
from langchain_community.graphs import Neo4jGraph
from langchain_openai import ChatOpenAI

enhanced_graph = Neo4jGraph(
    url=""bolt://localhost:7687"",
    username=""neo4j"",
    password=""password"",
    enhanced_schema=True,
)
print(enhanced_graph.schema)

chain = GraphCypherQAChain.from_llm(
    ChatOpenAI(temperature=0), graph=enhanced_graph, verbose=True
)

chain.invoke({""query"": ""Who is Bob?""})

Error here:
ValueError: Could not use APOC procedures. Please ensure the APOC plugin is installed in Neo4j and that 'apoc.meta.data()' is allowed in Neo4j configuration
neo4j.exceptions.ClientError: {code: Neo.ClientError.Procedure.ProcedureNotFound} {message: There is no procedure with the name `apoc.meta.data` registered for this database instance. Please ensure you've spelled the procedure name correctly and that the procedure is properly deployed.}

How to solve the problem?
","This is a known error:

copy this file 'apoc-5.14.0-core.jar' from /var/lib/neo4j/labs/ to /var/lib/neo4j/plugins

update this file /var/lib/neo4j/conf/neo4j.conf
dbms.security.procedures.unrestricted=apoc.*
dbms.security.procedures.allowlist=apoc.*


Git link with solutions : https://github.com/langchain-ai/langchain/issues/12901
",langchain
pydantic is not compatible with langchain documents,"I am using LangChain 0.2.34 together with Python 3.12.5 to build a RAG architecture and Pydantic 2.8.2 for validation. It appears that some LangChain classes are not compatible with Pydantic although I explicitly allow arbitrary types. Or am I missing something?
Here is a code sample and the respective error.
from typing import List

from langchain_core.documents.base import Document
from pydantic import BaseModel, ConfigDict


class ResponseBody(BaseModel):
    message: List[Document]
    model_config = ConfigDict(arbitrary_types_allowed=True)

docs = [Document(page_content=""This is a document"")]
res = ResponseBody(message=docs)

Error:
TypeError: BaseModel.validate() takes 2 positional arguments but 3 were given

","Langchain is using the functionality in pydantic v1. Define your model with v1 syntax:
from pydantic.v1 import BaseModel, ConfigDict

...

You can read about their migration plan + pydantic compatibility here: How to use LangChain with different Pydantic versions
",langchain
embedding using the langchain_aws is giving none value,"I am trying to embed a text using the langchain_aws BedrockEmbeddings, but when I invoke the function, I get a list with the None values.
Here's the code:
from langchain_community.llms.bedrock import Bedrock 
from langchain_aws import BedrockEmbeddings
import boto3

# Initialize the Bedrock client
bedrock_client = boto3.client(service_name='bedrock-runtime')

# Initialize Bedrock Embeddings
bedrock_embeddings = BedrockEmbeddings(
    model_id=""amazon.titan-text-express-v1"",
    credentials_profile_name=""default"",
    client=bedrock_client,
    region_name=""ap-south-1""
)


embed_data=bedrock_embeddings.embed_documents([""This is a content of the document"", ""This is another document""])

print(embed_data)


Output:
[None, None]

","amazon.titan-text-express-v1 is not embedding model. Its a text generation model.
Use amazon.titan-embed-text-v2:0
",langchain
typeerror expected a runnable callable or dictinstead got an unsupported type ltclass 39list39gt,"I am experimenting with LLM development.
Here is my code:
import langchain, pydantic, transformers
from langchain import HuggingFacePipeline
from langchain_core.prompts import PromptTemplate
from langchain_core.runnables.base import RunnableSequence, RunnableMap, RunnableLambda

from langchain.callbacks import get_openai_callback
from pydantic import BaseModel, Field
from langchain.output_parsers import PydanticOutputParser
from transformers import pipeline

class MedicalSpecialty(BaseModel):
    medical_specialty: str = Field(description=""medical specialty the patient should go to"")
    urgent: bool = Field(description=""the patient should go to the hospital immediately"")

parser = PydanticOutputParser(pydantic_object=MedicalSpecialty)

queries = [""i have ache in my chest and in my left arm. Which medical specialty should I go to?""]

template = """"""
Question: {question}
""""""
prompt = PromptTemplate(template=template, input_variables=[""question""])


llm = HuggingFacePipeline.from_model_id(
    model_id=""bigscience/bloom-1b7"",
    task=""text-generation"",
    model_kwargs={""max_length"": 1024},
    device=-1  # Ensure it runs on CPU for macOS M1
)

# Wrap the prompt in a RunnableLambda to make it a Runnable
prompt_runnable = RunnableLambda(lambda x: prompt.format(**x))

# Define the sequence that includes the prompt and LLM
sequence = RunnableSequence([
    prompt_runnable,
    llm
])

with get_openai_callback() as CB:
    for query in queries:
        result = sequence.invoke({""question"": query})
        print(query)
        print(result)
        print(""===================================="")
    
    # Print the costs of the requests
    print(cb)

Unfortunately, after several iterations, I keep getting this error:
TypeError                                 Traceback (most recent call last)
Cell In[6], line 19
     16 prompt_runnable = RunnableLambda(lambda x: prompt.format(**x))
     18 # Define the sequence that includes the prompt and LLM
---> 19 sequence = RunnableSequence([
     20     prompt_runnable,
     21     llm
     22 ])
     24 with get_openai_callback() as CB:
     25     for query in queries:

File /opt/anaconda3/envs/LLM/lib/python3.11/site-    packages/langchain_core/runnables/base.py:2632, in RunnableSequence.__init__(self,     name, first, middle, last, *steps)
   2630         steps_flat.extend(step.steps)
   2631     else:
-> 2632         steps_flat.append(coerce_to_runnable(step))
   2633 if len(steps_flat) < 2:
   2634     raise ValueError(
   2635         f""RunnableSequence must have at least 2 steps, got     {len(steps_flat)}""
   2636     )

File /opt/anaconda3/envs/LLM/lib/python3.11/site-    packages/langchain_core/runnables/base.py:5554, in coerce_to_runnable(thing)
   5552     return cast(Runnable[Input, Output], RunnableParallel(thing))
   5553 else:
-> 5554     raise TypeError(
   5555         f""Expected a Runnable, callable or dict.""
   5556         f""Instead got an unsupported type: {type(thing)}""
   5557     )

TypeError: Expected a Runnable, callable or dict.Instead got an unsupported type:     <class 'list'>

Please, someone help!
","No need to use lambda for this simple prompt with one input.
Try to use below and report back here on what happens
prompt_runnable = PromptRunnable(prompt)

",langchain
implement a search engine chain using tavily in langchain,"I want to implement a search engine chain using tavily in langchain. This chain gives user's query as an input and returns up to 5 related documents. Each retrieved document must have the content of the document as page_content and the url of the corresponding site as metadata under the definition of LangChain Documents. I must use langchain_core.documents.base.Document class to define documents. So this chain will have two main parts:

Tavily search platform
Parser with the aim of converting search output data into standard LangChai documents.

I wrote this code but I don't know how to change tavily output format into standard form of document:
from langchain_core.documents.base import Document
from langchain_community.tools.tavily_search import TavilySearchResults

search = TavilySearchResults(max_results=5)

class ParsedDocument(BaseModel):
    content: str = Field(description=""This refers to the content of the search."")
    url: str = Field(description=""This refers to the url of the search."")

search_parser = PydanticOutputParser(pydantic_object=ParsedDocument)
search_engine_chain = search | search_parser


I would be grateful if you could help me how to change this code.
","I finally found the answer:
class ParsedDocument(BaseModel):
    content: str = Field(description=""This refers to the content of the search."")
    url: str = Field(description=""This refers to the url of the search."")

# Define a custom parser
def custom_parser(search_results):
    parsed_documents = []
    for result in search_results:  # Adjust this line based on the actual structure of search_results
        parsed_document = ParsedDocument(content=result['content'], url=result['url'])
        document = Document(page_content=parsed_document.content, metadata={'url': parsed_document.url})
        parsed_documents.append(document)
    return parsed_documents

search_engine_chain = search | custom_parser

",langchain
how can i add a progress barstatus when creating a vector store with langchain,"Creating a vector store with the Python library langchain may take a while. How can I add a progress bar?

Example of code where a vector store is created with langchain:
import pprint
from langchain_community.vectorstores import FAISS
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain.docstore.document import Document

model = ""sentence-transformers/multi-qa-MiniLM-L6-cos-v1""
embeddings = HuggingFaceEmbeddings(model_name = model)

def main():
    doc1 = Document(page_content=""The sky is blue."",    metadata={""document_id"": ""10""})
    doc2 = Document(page_content=""The forest is green"", metadata={""document_id"": ""62""})
    docs = []
    docs.append(doc1)
    docs.append(doc2)

    for doc in docs:
        doc.metadata['summary'] = 'hello'

    pprint.pprint(docs)
    db = FAISS.from_documents(docs, embeddings)
    db.save_local(""faiss_index"")
    new_db = FAISS.load_local(""faiss_index"", embeddings)

    query = ""Which color is the sky?""
    docs = new_db.similarity_search_with_score(query)
    print('Retrieved docs:', docs)
    print('Metadata of the most relevant document:', docs[0][0].metadata)

if __name__ == '__main__':
    main()

Tested with Python 3.11 with:
pip install langchain==0.1.1 langchain_openai==0.0.2.post1 sentence-transformers==2.2.2 langchain_community==0.0.13 faiss-cpu==1.7.4

The vector store is created with db = FAISS.from_documents(docs, embeddings).
","Langchain does not natively support any progress bar for this at the moment with release of 1.0.0
I also had similar case, so instead of sending all the documents, I send independent document for ingestion and tracked progress at my end. This was helpful for me.
You can do the ingestion in the following way
    with tqdm(total=len(docs), desc=""Ingesting documents"") as pbar:
        for d in docs:
            if db:
                db.add_documents([d])
            else:
                db = FAISS.from_documents([d], embeddings)
            pbar.update(1)  


From what I checked from langchain code https://github.com/langchain-ai/langchain/blob/master/libs/core/langchain_core/retrievers.py#L31 they are making call to add_texts as well, so no major operation is being performed here other than parsing.
I had simple documents, and I didn't observe much difference. Probably others who has tried on huge documents can add if it adds latency in their usecase.
Below is your updated code
import pprint
from tqdm import tqdm
from langchain_community.vectorstores import FAISS
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain.docstore.document import Document

model = ""sentence-transformers/multi-qa-MiniLM-L6-cos-v1""
embeddings = HuggingFaceEmbeddings(model_name = model)

def main():
    doc1 = Document(page_content=""The sky is blue."",    metadata={""document_id"": ""10""})
    doc2 = Document(page_content=""The forest is green"", metadata={""document_id"": ""62""})
    docs = []
    docs.append(doc1)
    docs.append(doc2)

    for doc in docs:
        doc.metadata['summary'] = 'hello'

    db = None
    with tqdm(total=len(docs), desc=""Ingesting documents"") as pbar:
        for d in docs:
            if db:
                db.add_documents([d])
            else:
                db = FAISS.from_documents([d], embeddings)
            pbar.update(1)  

    # pprint.pprint(docs)
    # db = FAISS.from_documents(docs, embeddings)
    db.save_local(""faiss_index"")
    new_db = FAISS.load_local(""faiss_index"", embeddings)

    query = ""Which color is the sky?""
    docs = new_db.similarity_search_with_score(query)
    print('Retrieved docs:', docs)
    print('Metadata of the most relevant document:', docs[0][0].metadata)

if __name__ == '__main__':
    main()

",langchain
why am i receiving quotattributeerror 39str39 object has no attribute 39page_content39quot when trying to add my embeddings to azure cognitive search,"I am extracting text from pdf documents and load it to Azure Cognitive Search for a RAG approach. Unfortunately this does not work. I am receiving the error message

AttributeError: 'str' object has no attribute 'page_content'

What I want to do is

Extract text from pdf via pymupdf - works
Upload it to Azuer Vector search as embeddings with vectors and `filename``
Query this through ChatGPT model

This is my code:
!pip install cohere tiktoken
!pip install openai==0.28.1
!pip install pymupdf
!pip install azure-storage-blob azure-identity
!pip install azure-search-documents --pre --upgrade
!pip install langchain

import fitz
import time
import uuid
import os
import openai

from PIL import Image
from io import BytesIO
from IPython.display import display

from azure.identity import DefaultAzureCredential
from azure.storage.blob import BlobServiceClient, BlobClient, ContainerClient

from langchain.embeddings import OpenAIEmbeddings
from langchain.text_splitter import RecursiveCharacterTextSplitter

from langchain.chat_models import AzureChatOpenAI
from langchain.vectorstores import AzureSearch
from langchain.document_loaders import DirectoryLoader
from langchain.document_loaders import TextLoader
from langchain.text_splitter import TokenTextSplitter
from langchain.chains import ConversationalRetrievalChain
from langchain.prompts import PromptTemplate

from google.colab import drive

OPENAI_API_BASE = ""https://xxx.openai.azure.com""
OPENAI_API_KEY = ""xxx""
OPENAI_API_VERSION = ""2023-05-15""

openai.api_type = ""azure""
openai.api_key = OPENAI_API_KEY
openai.api_base = OPENAI_API_BASE
openai.api_version = OPENAI_API_VERSION

AZURE_COGNITIVE_SEARCH_SERVICE_NAME = ""https://xxx.search.windows.net""
AZURE_COGNITIVE_SEARCH_API_KEY = ""xxx""
AZURE_COGNITIVE_SEARCH_INDEX_NAME = ""test""

llm = AzureChatOpenAI(deployment_name=""gpt35"", openai_api_key=OPENAI_API_KEY, openai_api_base=OPENAI_API_BASE, openai_api_version=OPENAI_API_VERSION)
embeddings = OpenAIEmbeddings(deployment_id=""ada002"", chunk_size=1, openai_api_key=OPENAI_API_KEY, openai_api_base=OPENAI_API_BASE, openai_api_version=OPENAI_API_VERSION)

acs = AzureSearch(azure_search_endpoint=AZURE_COGNITIVE_SEARCH_SERVICE_NAME,
                  azure_search_key = AZURE_COGNITIVE_SEARCH_API_KEY,
                  index_name = AZURE_COGNITIVE_SEARCH_INDEX_NAME,
                  embedding_function = embeddings.embed_query)
    
def generate_tokens(s):
  text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)
  splits = text_splitter.split_text(s)

  return splits

drive.mount('/content/drive')
folder = ""/content/drive/.../pdf/""

page_content = ''
doc_content = ''
    
for filename in os.listdir(folder):
    file_path = os.path.join(folder, filename)
    if os.path.isfile(file_path):
        print(f""Processing file: {file_path}"")

        doc = fitz.open(file_path)
        for page in doc: # iterate the document pages
          page_content += page.get_text() # get plain text encoded as UTF-8
          doc_content += page_content

          d = generate_tokens(doc_content)

          # the following line throws the error
          # how can i add the chunks + filename to 
          # Azure Cognitive Search?

          acs.add_documents(documents=d)
    
        print(metadatas)
        print(""----------"")
        print(doc_content)
        count = len(doc_content.split())
        print(""Number of tokens: "", count)


---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
<ipython-input-33-d9eaff7ee027> in <cell line: 10>()
     31           all_texts.extend(d)
     32 
---> 33           acs.add_documents(documents=d)
     34 
     35           metadatas = [{""Source"": f""{i}-pl""} for i in range(len(all_texts))]

1 frames
/usr/local/lib/python3.10/dist-packages/langchain/schema/vectorstore.py in <listcomp>(.0)
    118         """"""
    119         # TODO: Handle the case where the user doesn't provide ids on the Collection
--> 120         texts = [doc.page_content for doc in documents]
    121         metadatas = [doc.metadata for doc in documents]
    122         return self.add_texts(texts, metadatas, **kwargs)

AttributeError: 'str' object has no attribute 'page_content'

","I believe the issue is in your generate_tokens method. Instead of returning a list of documents, it is returning a list of string. Please see the documentation for split_text here: https://api.python.langchain.com/en/latest/text_splitter/langchain.text_splitter.RecursiveCharacterTextSplitter.html#langchain.text_splitter.RecursiveCharacterTextSplitter.split_text.
I believe the fix should be to convert this list of string into a list of Document objects. Can you please try the following:
def generate_tokens(s):
  text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)
  splits = text_splitter.split_text(s)

  return text_splitter.create_documents(splits) #this should return the list of documents.

",langchain
i get an empty array from vector serch in mongodb with langchain,"I have the code:
loader = PyPDFLoader(“https://arxiv.org/pdf/2303.08774.pdf”)
data = loader.load()
docs = text_splitter1.split_documents(data)
vector_search_index = “vector_index”

vector_search = MongoDBAtlasVectorSearch.from_documents(
  documents=docs,
  embedding=OpenAIEmbeddings(disallowed_special=()),
  collection=atlas_collection,
  index_name=vector_search_index,
)

query = ""What were the compute requirements for training GPT 4""
results = vector_search1.similarity_search(query)
print(""result: "", results)

And in results I have every time only empty array. I don't understand what I do wrong. This is the link  on the langchain documentation with examples. Information is saved normally in database, but I cannot search info in this collection.
","So I was able to get this to work in MongoDB with the following code:
text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=150)

loader = PyPDFLoader(""https://arxiv.org/pdf/2303.08774.pdf"")
data = loader.load()
docs = text_splitter.split_documents(data)

DB_NAME = ""langchain_db""
COLLECTION_NAME = ""atlas_collection""
ATLAS_VECTOR_SEARCH_INDEX_NAME = ""vector_index""
MONGODB_ATLAS_CLUSTER_URI = uri = os.environ.get(""MONGO_DB_ENDPOINT"")

client = MongoClient(MONGODB_ATLAS_CLUSTER_URI)
MONGODB_COLLECTION = client[DB_NAME][COLLECTION_NAME]

vector_search = MongoDBAtlasVectorSearch.from_documents(
    documents=docs,
    embedding=OpenAIEmbeddings(disallowed_special=()),
    collection=MONGODB_COLLECTION,
    index_name=ATLAS_VECTOR_SEARCH_INDEX_NAME,
)

query = ""What were the compute requirements for training GPT 4""
results = vector_search.similarity_search(query)
print(""result: "", results)

At this point, I did get the same results that you did. Before it would work, I had to create the vector search index and I made sure it was named the same as what is specified in ATLAS_VECTOR_SEARCH_INDEX_NAME:

FWIW - It was easier for me to do in Astra DB (I tried this first, because I am a DataStax employee):
text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=150)

loader = PyPDFLoader(""https://arxiv.org/pdf/2303.08774.pdf"")
data = loader.load()
docs = text_splitter.split_documents(data)
atlas_collection = ""atlas_collection""

ASTRA_DB_API_ENDPOINT = os.environ.get(""ASTRA_DB_API_ENDPOINT"")
ASTRA_DB_APPLICATION_TOKEN = os.environ.get(""ASTRA_DB_APPLICATION_TOKEN"")

vector_search = AstraDBVectorStore.from_documents(
  documents=docs,
  embedding=OpenAIEmbeddings(disallowed_special=()),
  collection_name=atlas_collection,
  api_endpoint=ASTRA_DB_API_ENDPOINT,
  token=ASTRA_DB_APPLICATION_TOKEN,
)

query = ""What were the compute requirements for training GPT 4""
results = vector_search.similarity_search(query)
print(""result: "", results)

Worth noting, that Astra DB will create your vector index automatically based on the dimensions of the embedding model.
",langchain
how to develop a generalized rag pipeline for text images and structured data,"I'm trying to find a general solution for RAG to solve problems involving both text, images, chart, tables,.., they are in many different formats such as .docx, .xlsx, .pdf.
The requirement for the answer:

Some answers are just images
Some answers only contain text and need to be absolutely accurate because it relates to a process,...
On the other hand, the answers may not need to be absolutely accurate but should still ensure logical consistency; this is something I am already working on

The features of the documents:

Some documents in DOCX and Excel formats contain only text; this is the simplest form. My task is to determine the embedding model and LLM, in addition to selecting hyperparameters such as chunk size, chunk overlap, etc., and experimenting to find the appropriate values
If the documents have more complex content, such as DOCX files containing text and images, or PDF files containing text, images, charts, tables, etc., I haven't found a general solution to handle them yet.

Below are some documents I have read but feel I don't fully understand, I'm not sure how it can help me.

https://medium.com/kx-systems/guide-to-multimodal-rag-for-images-and-text-10dab36e3117
https://blog.langchain.dev/semi-structured-multi-modal-rag/

I want to be able to outline a pipeline to answer questions according to the requirements of my system. Any help would be greatly appreciated!
System:

LLM was run locally (Llama 3.1 13N Instruct, Qwen2-7B-Instruct,...)

","Here is a sample of the code you will need to implement a RAG-FUSION. You would have to structure your requirements with this code, this serves as a guide for json files, you can implement others such as pdf, images following the same procedure.
def determine_extension(file):
    if file.endswith("".jpg"", "".png""):
       send_image_to_rag_classifier(file)
    elif ...
    else ...


"""""" Implement the RAG fusion using the langchain library""""""

import asyncio
import json
import logging
import os
import pathlib as path
from operator import itemgetter
from typing import Any

from dotenv import find_dotenv, load_dotenv
from langchain.prompts import ChatPromptTemplate
from langchain_community.vectorstores import chroma
from langchain_core.documents import Document
from langchain_core.output_parsers import StrOutputParser
from langchain_core.vectorstores import VectorStoreRetriever
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_text_splitters import RecursiveJsonSplitter

logger = logging.getLogger(__name__)


# read OPENAI_API_KEY from the environment
load_dotenv(find_dotenv())

# Define a prompt for the RAG model
SYSTEM_PROMPT = """""" 
                  Your prompt
                """"""


# recursive pass data in the retriever_data
def collect_data_files(filepath: path) -> list:
    """"""Walk through the file path and collect the files

    Args: filepath: The file path to be walked through

    Returns:
    list: List of files
    """"""
    
    return store_file


# Create a recursive json splitter to split the data into chunks
def retrieve_data(data) -> list[chroma.Document]:
    """"""
    Retrieve the data from the file

    Args: data: The data to be retrieved

    Returns: list: List of documents
    """"""

    docs = collect_data_files(data)

    for file in docs:
        with open(file, ""r"") as f:
            data = json.loads(f.read())
            # Split the data into chunks
            splitter = RecursiveJsonSplitter(max_chunk_size=300)

            # create documents from the vector database
            documents = splitter.create_documents(texts=data, convert_lists=True)

    return documents


# vectorstore database from chroma
def vectorstore_db(data) -> VectorStoreRetriever:
    """"""
    Create a vectorstore database from the data
    Args: data: The data to be indexed

    Returns: VectorStoreRetriever: The vectorstore retriever
    """"""


    return vector_retriever


# create a function to generate queries from the RAG model
def get_unique_union_of_documents(docs: list[list]) -> list[Any]:
    """"""
    Get the unique union of the documents
    Args:
    docs: The documents to be processed

    Returns:
    list: The unique union of the documents""""""

    return [json.loads(doc) for doc in unique_union]


# RAG FUSION
class RAGFusion:
    """"""
    Implement the RAG fusion
    Args:
    data: The data to be used for the RAG fusion
    """"""

    def __init__(self, data) -> None:

        self.data = data

    def __call__(self, question: str) -> str:
        """"""
        Implement the RAG fusion
        Args:
        question: The question to be answered

        Returns:
        str: The answer to the question
        """"""

        try:
            # create a retrieval chain
            prompt_for_rag_fusion = ChatPromptTemplate.from_template(SYSTEM_PROMPT)

            generate_query = (
                prompt_for_rag_fusion
                | ChatOpenAI(temperature=0.5, max_tokens=4096)
                | StrOutputParser()
                | (lambda x: x.split(""\n""))
            )

            vb = vectorstore_db(self.data)

            # create a retrieval chain
            retrieval_chain = generate_query | vb.map() | get_unique_union_of_documents

            chat_template = """"""
                        Answer the following questions{question} \n
                        Based on the data and context provided {context} \n
                        Question: {question} \n
                    """"""

            # get the chat prompt template
            prompt = ChatPromptTemplate.from_template(chat_template)

            # use this llm
            llm = ChatOpenAI(temperature=0.5, max_tokens=4096)

            # implement the final rag fusion
            final_rag_fusion = (
                {""context"": retrieval_chain, ""question"": itemgetter(""question"")}
                | prompt
                | llm
                | StrOutputParser()
            )

            return final_rag_fusion.invoke({""question"": question})
        except Exception as e:
            logger.error(f""An error occurred: {e}"")

",langchain
langchain ollama and llama 3 prompt and response,"Currently, I am getting back multiple responses, or the model doesn't know when to end a response, and it seems to repeat the system prompt in the response(?). I simply want to get a single response back. My setup is very simple, so I imagine I am missing implementation details, but what can I do to only return the single response?
from langchain_community.llms import Ollama

llm = Ollama(model=""llama3"")

def get_model_response(user_prompt, system_prompt):
    prompt = f""""""
        <|begin_of_text|>
        <|start_header_id|>system<|end_header_id|>
        { system_prompt }
        <|eot_id|>
        <|start_header_id|>user<|end_header_id|>
        { user_prompt }
        <|eot_id|>
        <|start_header_id|>assistant<|end_header_id|>
        """"""
    response = llm.invoke(prompt)
    return response

","Using a PromptTemplate from Langchain, and setting a stop token for the model, I was able to get a single correct response.
from langchain_community.llms import Ollama
from langchain import PromptTemplate # Added

llm = Ollama(model=""llama3"", stop=[""<|eot_id|>""]) # Added stop token

def get_model_response(user_prompt, system_prompt):
    # NOTE: No f string and no whitespace in curly braces
    template = """"""
        <|begin_of_text|>
        <|start_header_id|>system<|end_header_id|>
        {system_prompt}
        <|eot_id|>
        <|start_header_id|>user<|end_header_id|>
        {user_prompt}
        <|eot_id|>
        <|start_header_id|>assistant<|end_header_id|>
        """"""

    # Added prompt template
    prompt = PromptTemplate(
        input_variables=[""system_prompt"", ""user_prompt""],
        template=template
    )
    
    # Modified invoking the model
    response = llm(prompt.format(system_prompt=system_prompt, user_prompt=user_prompt))
    
    return response

",langchain
how to return used context to answer using langchain in python,"I have built a RAG system like this:
loader = PyPDFLoader(pdf_file_name)
raw_documents = loader.load()

text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
documents = text_splitter.split_documents(raw_documents)
print(documents[-1])

Document(
   metadata={'source': '/Appraisal.pdf', 'page': 37},
   page_content='File No.\nProperty Address\nCity County State Zip Code\nClient10828\nBorrower or Owner John Smith & Kitty Smith\n29 Dream St\nDreamTown SC 99999\nSouthern First Bank\nBB Appraisals, LLC'
)

compressor = CohereRerank(
    top_n=top_n,
    model=""rerank-english-v3.0"",
    cohere_api_key=""""
)

retriever = vectorstore.as_retriever(
    search_type=""similarity"", 
    search_kwargs={""k"": top_n}
)

compression_retriever = ContextualCompressionRetriever(
    base_compressor=compressor, base_retriever=retriever
)

def format_docs(docs):
    return ""\n\n"".join(doc.page_content for doc in docs)

response_schemas = [
    ResponseSchema(name=""price"", description=""Price"", type=""float""),
    ResponseSchema(name=""unit"", description=""Unit"", type=""int""),
]
output_parser = StructuredOutputParser.from_response_schemas(response_schemas)

rag_prompt = PromptTemplate(
    input_variables=[""context"",""question""],
    template=template,
    partial_variables={""format_instructions"": output_parser.get_format_instructions()},
)

rag_chain = (
    {""context"": compression_retriever | format_docs, ""question"": RunnablePassthrough()}
    | rag_prompt
    | llm
    | output_parser
)

query = ""What is the price? How many units?""

response = rag_chain.invoke(query, config={""configurable"": {""session_id"": ""abc123""}},)

But then my response is a JSON with my price and unit as keys only. And I would like to be able to have a ""context"" variable that stores the paragraphs used in my document that the algo relied upon to answer the questions.
Any idea how I could do that please?
","There are two ways to do this

For pictorial rep. of the information, as to which document the LLM used,  You would need to visit langchain_smith, you must also understand some methodologies in RAG - LIKE RAG-FUSION, this will help you create a RAG-FUSION used by the llm to get the documents it used to retrieve the information.

I am not sure about this, but you can try this function, pipe it with your compression_retriever chain. The idea of this function is for you to pass it along with the llm or the retriever as the case maybe, so as to make retrieving the docs easier


def unique_union_of_documents(docs: list) -> list[Any]:
    """"""
    Get the unique union of the documents
    Args:
    docs: The documents to be processed

    Returns:
    list: The unique union of the documents""""""

    doc_news = [json.dumps(doc.page_content) for _ in docs]
    # find the unique union of the documents
    unique_union = list(set(doc_news))

    return [json.loads(doc) for doc in unique_union]

You can now call this function after your variable response
",langchain
langchain chat history,"I am struggling with passing context to conversational rag chain when using RunnableWithMessageHistory.
I have the following query function:
def query(query_text, prompt, session_id, metadata_context):
# History retrieval test
contextualize_q_prompt = ChatPromptTemplate.from_messages(
    [
        (""system"", contextualize_q_system_prompt),
        (""system"", ""{context}""),
        (""system"", prompt),
        MessagesPlaceholder(""chat_history""),
        (""human"", ""{input}""),
    ]
)
history_aware_retriever = create_history_aware_retriever(
    llm, retriever, contextualize_q_prompt
)

qa_prompt = ChatPromptTemplate.from_messages(
    [   
        (""system"", PROMPT_TEMPLATE),
        (""system"", ""{context}""),
        (""system"", prompt),
        MessagesPlaceholder(""chat_history""),
        (""human"", ""{input}""),
    ]
)

question_answer_chain = create_stuff_documents_chain(llm, qa_prompt)
rag_chain = create_retrieval_chain(history_aware_retriever, question_answer_chain)

conversational_rag_chain = RunnableWithMessageHistory(
    rag_chain,
    get_session_history,
    input_messages_key=""input"",
    history_messages_key=""chat_history"",
    output_messages_key=""answer"",
)
try:
    logger.info(f""Model: {LLM_MODEL} assigned. Generation of response has started."")
    response = conversational_rag_chain.invoke({""input"": query_text, ""context"": metadata_context}, config={""configurable"": {""session_id"": f""{session_id}""}},)
    logger.info(f""Response generated."")
except Exception as e:
    return ({'Generation of response failed: ': str(e)})
return response[""answer""]

I want to pass my own 'context' that is prepared and parsed from retriever. I do not want retriever to be called again but from what I've read - retrieving happens by itself if chat_history does not contain the answer.
prompt variable is created:
prompt_template = ChatPromptTemplate.from_template(PROMPT_TEMPLATE)
prompt = prompt_template.format(context=metadata_context, input=query_text)

As you can see I am trying to put the context everywhere but no success.
The 'context' I can see when calling
conversational_rag_chain.invoke({""input"": query_text, ""context"": metadata_context}, config={""configurable"": {""session_id"": f""{session_id}""}},)
        logger.info(f""Response generated."")

is the result of retriever:
Document(metadata={'number_of_reviews': '16', 'price': 18999, 'product_name': 'product', 'rating': '4')

The code I'm using is as follows:
chroma_client = chromadb.HttpClient(host=DB_HOST, port=DB_PORT)
chroma_collection = chroma_client.get_collection(os.getenv(""DB_COLLECTION""))

vectorstore = VStoreChroma(DB_COLLECTION, embedding_function, client=client)


llm = ChatOpenAI(model=""gpt-4o-mini"",temperature=0)

retriever = SelfQueryRetriever.from_llm(
    llm,
    vectorstore,
    document_content_description,
    metadata_field_info,
    search_kwargs = {""k"": 10}
)

def self_query(query_text):
    model = llm
    logger.info(""Data retrieval has started."")
    try:
        result = retriever.invoke(query_text)
        logger.info(""Data retrieved from database."")
        if len(result) == 0:
            logger.info(f""Unable to find matching results."")
    except Exception as e:
        return ({'Retrieval failed: ': str(e)})
    return result

Retrieving is alright I get correct results. The problem is that the context I prepare from metadata by parsing it with the function like the one you mention in your snippet. It is string and I do not get it where I can pass it so the context is used properly. The rest is as I mentioned before.
","You can pass your context or your question however you decide but i think fundamentally, your context should be separate from question
You can also pass your context easily using this template I have provided
    chat_template = """"""
                Answer the following questions{question} \n
                Based on the data and context provided {context} \n
                Question: {question} \n
            """"""

    # get the chat prompt template
    prompt = ChatPromptTemplate.from_template(chat_template)

To something like this below.
from langchain.prompts import ChatPromptTemplate
from langchain.chat_models import ChatOpenAI
from langchain.chains import LLMChain
from langchain_core.output_parsers import StrOutputParser
from operator import itemgetter

prompt = ChatPromptTemplate.from_template(chat_template)

# Initialize the LLM
llm = ChatOpenAI(temperature=0.5, max_tokens=4096)

# Initialize the output parser
output_parser = StrOutputParser()

# Create the LLMChain
final_rag = LLMChain(prompt=prompt, llm=llm, output_parser=output_parser)

# Invoke the chain with the input
question = ""Your question here""  # Replace this with your actual question
response = final_rag.invoke({""input"": question, ""context"": itemgetter(metadata_context)})

print(response)

",langchain
why langchain huggingfaceembeddings model dimension is not the same as stated on huggingface,"I was using langchain HuggingFaceEmbeddings model: dunzhang/stella_en_1.5B_v5.
When I look at https://huggingface.co/spaces/mteb/leaderboard, I can see that the model is 8192.
But when I do
len(embed_model.embed_query(""hey you""))

It gives me 1024.
Why this difference please ?
","According to the documentation at dunzhang/stella_en_1.5B_v5

The default dimension is 1024, if you need other dimensions, please clone the model and modify modules.json to replace 2_Dense_1024 with another dimension, e.g. 2_Dense_256 or 2_Dense_8192

You will need to download the model and use the local model after making the config changes
",langchain
how to print input requests and output responses in ollama server,"I'm working with Langchain and CrewAI libraries to gain an in-depth understanding of system prompting. Currently, I'm running the Ollama server manually (ollama serve) and trying to intercept the messages flowing through using a proxy server I've created.
The goal is to log or print the input requests and output responses for debugging and analysis purposes.
Can anyone suggest a better way to achieve this?
","For Ubuntu Users:
To print out the input request on the server side, you need to enable Debug mode. Follow these steps:

Open Ollama's service file:
sudo systemctl edit --full ollama.service

Add the following line in the [Service] section:
Environment=""OLLAMA_DEBUG=1""

Restart the Ollama service:
sudo systemctl restart ollama.service

Read the service logs to view debug information:
journalctl -f -b -u ollama


This will enable Debug mode and allow you to see detailed logs for input requests.
Additional info.
",langchain
sagemaker and langchain valueerror when calling invokeendpoint operation for llama 2 model,"I am trying to deploy a Llama 2 model for text generation inference using Sagemaker and LangChain. I am writing code in Notebook instances and deploying SageMaker instances from the code.
I followed the documentation from https://python.langchain.com/docs/integrations/llms/sagemaker. I used the following code to create a chain for question answering:
from langchain.docstore.document import Document
example_doc_1 = """"""
Peter and Elizabeth took a taxi to attend the night party in the city. While in the party, Elizabeth collapsed and was rushed to the hospital.
Since she was diagnosed with a brain injury, the doctor told Peter to stay besides her until she gets well.
Therefore, Peter stayed with her at the hospital for 3 days without leaving.
""""""

docs = [
    Document(
        page_content=example_doc_1,
    )
]

from typing import Dict

from langchain import PromptTemplate, SagemakerEndpoint
from langchain.llms.sagemaker_endpoint import LLMContentHandler
from langchain.chains.question_answering import load_qa_chain
import json

query = """"""How long was Elizabeth hospitalized?
""""""

prompt_template = """"""Use the following pieces of context to answer the question at the end.

{context}

Question: {question}
Answer:""""""
PROMPT = PromptTemplate(
    template=prompt_template, input_variables=[""context"", ""question""]
)


class ContentHandler(LLMContentHandler):
    content_type = ""application/json""
    accepts = ""application/json""

    def transform_input(self, prompt: str, model_kwargs: Dict) -> bytes:
        input_str = json.dumps({prompt: prompt, **model_kwargs})
        return input_str.encode(""utf-8"")

    def transform_output(self, output: bytes) -> str:
        response_json = json.loads(output.read().decode(""utf-8""))
        return response_json[0][""generated_text""]


content_handler = ContentHandler()

chain = load_qa_chain(
    llm=SagemakerEndpoint(
        endpoint_name=""XYZ"",
        credentials_profile_name=""XYZ"",
        region_name=""XYZ"",
        model_kwargs={""temperature"": 1e-10},
        content_handler=content_handler,
    ),
    prompt=PROMPT,
)

chain({""input_documents"": docs, ""question"": query}, return_only_outputs=True)

But I got an error
ValueError: Error raised by inference endpoint: 
An error occurred (ModelError) when calling the InvokeEndpoint operation: 
Received client error (422) from primary with message 
""Failed to deserialize the JSON body into the target type: missing field `inputs` at line 1 column 966"".

In multiple tutorials there isn't any inputs field. I have no idea if they updated the documentation, which I have been referring to but can't resolve this problem.
My question is:

Why am I getting this error and how can I fix it?
What am I missing in my code or configuration?
Any help or guidance would be appreciated. Thanks in advance.

","Looks like it is a known issues with langchain documentation, @sigvamo mentioned this error can be workaround by updating ContentHandler to include inputs in its transform_input method
from typing import Dict, List
from langchain.embeddings import SagemakerEndpointEmbeddings
from langchain.embeddings.sagemaker_endpoint import EmbeddingsContentHandler
import json

class ContentHandler(EmbeddingsContentHandler):
   content_type = ""application/json""
   accepts = ""application/json""

   def transform_input(self, inputs: list[str], model_kwargs: Dict) -> bytes:
       input_str = json.dumps({""text_inputs"": inputs, **model_kwargs})
       return input_str.encode(""utf-8"")

   def transform_output(self, output: bytes) -> List[List[float]]:
       response_json = json.loads(output.read().decode(""utf-8""))
       return response_json[""embedding""]

",langchain
langchain_community amp langchain packages giving error missing 1 required keywordonly argument 39recursive_guard39,"All of sudden langchain_community & langchain packages started throwing error:
TypeError: ForwardRef._evaluate() missing 1 required keyword-only argument: 'recursive_guard'
The error getting generated somewhere in pydantic
I strongly suspect it is version mismatch. So I tried upgrading packages langchain, langchain_community, pydantic, langsmith etc. But no luck.
My current installed versions shows as under:
Python 3.12.4

langchain: 0.2.3
langchain_community: 0.2.4
langsmith: 0.1.75
pydantic: 2.7.3
typing_extensions: 4.11.0

Pip check also not showing any conflict.
Here is complete trace of error. Any help would be really appreciated.
TypeError: ForwardRef._evaluate() missing 1 required keyword-only argument: 'recursive_guard'

File ""C:\Users\lenovo\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\LocalCache\local-packages\Python312\site-packages\streamlit\runtime\scriptrunner\script_runner.py"", line 600, in _run_script
    exec(code, module.__dict__)
File ""C:\MyProject\MyScript.py"", line 20, in <module>
    from langchain_community.vectorstores import Chroma
File ""<frozen importlib._bootstrap>"", line 1412, in _handle_fromlist
File ""C:\Users\lenovo\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\LocalCache\local-packages\Python312\site-packages\langchain_community\vectorstores\__init__.py"", line 509, in __getattr__
    module = importlib.import_module(_module_lookup[name])
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
File ""C:\Program Files\WindowsApps\PythonSoftwareFoundation.Python.3.12_3.12.1264.0_x64__qbz5n2kfra8p0\Lib\importlib\__init__.py"", line 90, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
File ""C:\Users\lenovo\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\LocalCache\local-packages\Python312\site-packages\langchain_community\vectorstores\chroma.py"", line 20, in <module>
    from langchain_core.documents import Document
File ""C:\Users\lenovo\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\LocalCache\local-packages\Python312\site-packages\langchain_core\documents\__init__.py"", line 6, in <module>
    from langchain_core.documents.compressor import BaseDocumentCompressor
File ""C:\Users\lenovo\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\LocalCache\local-packages\Python312\site-packages\langchain_core\documents\compressor.py"", line 6, in <module>
    from langchain_core.callbacks import Callbacks
File ""C:\Users\lenovo\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\LocalCache\local-packages\Python312\site-packages\langchain_core\callbacks\__init__.py"", line 22, in <module>
    from langchain_core.callbacks.manager import (
File ""C:\Users\lenovo\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\LocalCache\local-packages\Python312\site-packages\langchain_core\callbacks\manager.py"", line 29, in <module>
    from langsmith.run_helpers import get_run_tree_context
File ""C:\Users\lenovo\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\LocalCache\local-packages\Python312\site-packages\langsmith\run_helpers.py"", line 40, in <module>
    from langsmith import client as ls_client
File ""C:\Users\lenovo\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\LocalCache\local-packages\Python312\site-packages\langsmith\client.py"", line 52, in <module>
    from langsmith import env as ls_env
File ""C:\Users\lenovo\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\LocalCache\local-packages\Python312\site-packages\langsmith\env\__init__.py"", line 3, in <module>
    from langsmith.env._runtime_env import (
File ""C:\Users\lenovo\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\LocalCache\local-packages\Python312\site-packages\langsmith\env\_runtime_env.py"", line 10, in <module>
    from langsmith.utils import get_docker_compose_command
File ""C:\Users\lenovo\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\LocalCache\local-packages\Python312\site-packages\langsmith\utils.py"", line 31, in <module>
    from langsmith import schemas as ls_schemas
File ""C:\Users\lenovo\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\LocalCache\local-packages\Python312\site-packages\langsmith\schemas.py"", line 69, in <module>
    class Example(ExampleBase):
File ""C:\Users\lenovo\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\LocalCache\local-packages\Python312\site-packages\pydantic\v1\main.py"", line 286, in __new__
    cls.__try_update_forward_refs__()
File ""C:\Users\lenovo\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\LocalCache\local-packages\Python312\site-packages\pydantic\v1\main.py"", line 807, in __try_update_forward_refs__
    update_model_forward_refs(cls, cls.__fields__.values(), cls.__config__.json_encoders, localns, (NameError,))
File ""C:\Users\lenovo\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\LocalCache\local-packages\Python312\site-packages\pydantic\v1\typing.py"", line 554, in update_model_forward_refs
    update_field_forward_refs(f, globalns=globalns, localns=localns)
File ""C:\Users\lenovo\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\LocalCache\local-packages\Python312\site-packages\pydantic\v1\typing.py"", line 520, in update_field_forward_refs
    field.type_ = evaluate_forwardref(field.type_, globalns, localns or None)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
File ""C:\Users\lenovo\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\LocalCache\local-packages\Python312\site-packages\pydantic\v1\typing.py"", line 66, in evaluate_forwardref
    return cast(Any, type_)._evaluate(globalns, localns, set())
       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

","I am having the same issue. The stack is different, but the error comes from the same line pydantic\v1\typing.py"", line 66
This is referring to the python typing module (v3.12.4) that has an additional mandatory parameter 'recursive_guard'. There are other areas of the code in pydantic where this has been fixed (recursive_gurard=set()).
Check this out --> https://github.com/pydantic/pydantic-core/issues/1292
Within this thread, they mention that using python v3.12.3 could temporarily solve the issue in 1292, probably because this additional attribute in v3.12.4 (I am guessing here). This is not an option for me as my google alpha functions local deploy is not recognizing the --runtime=python311 and always take the latest runtime (v3.12.4).
I hope that they fix this too
",langchain
how to view the final prompt in a multiqueryretriever pipeline using langchain,"I am currently working on a project using the LangChain library where I want to retrieve relevant documents from a vector database and then generate answers based on these documents using the Ollama LLM.
Below is my current implementation:
import logging

logging.basicConfig()
logging.getLogger(""langchain.retrievers.multi_query"").setLevel(logging.INFO)

# Define the prompt template for generating multiple query versions
QUERY_PROMPT = PromptTemplate(
    input_variables=[""question""],
    template=""""""You are an AI language model assistant. Your task is to generate five
    different versions of the given user question to retrieve relevant documents from
    a vector database. By generating multiple perspectives on the user question, your
    goal is to help the user overcome some of the limitations of the distance-based
    similarity search. Provide these alternative questions separated by newlines.
    Original question: {question}"""""",
)

# Initialize the MultiQueryRetriever
retriever = MultiQueryRetriever.from_llm(
    vectordb.as_retriever(), 
    ollama,
    prompt=QUERY_PROMPT
)

# Modified RAG prompt for generating the final response
template = """"""Answer the question based ONLY on the following context:
{context}
Question: {question}
""""""

# Create the final QA chain
prompt = ChatPromptTemplate.from_template(template)

from langchain_core.runnables import RunnableLambda


def inspect(state):
    """"""Print the state passed between Runnables in a langchain and pass it on""""""
    print(state)
    return state


qa_chain = (
    {""context"": retriever, ""question"": RunnablePassthrough()}
    | RunnableLambda(inspect)  # Add the inspector here to print the intermediate results
    | prompt
    | ollama
    | StrOutputParser()
)

# Invoke the QA chain with a sample query
qa_chain.invoke(""Give 10 quotes from this articles related to love?"")


How can I view the final prompt that is generated by the qa_chain before it is sent to the Ollama LLM for processing? I would like to see the exact prompt that includes the context and the user's question.
","Enable verbose and debug mode. For example,
...
from langchain_core.runnables import RunnableLambda, RunnablePassthrough
from langchain.schema.output_parser import StrOutputParser

from langchain.globals import set_verbose, set_debug

set_debug(True)
set_verbose(True)

",langchain
how to specify the langsmith project name for each chain,"According to the LangSmith documentation you need to set the LANGCHAIN_PROJECT environment variable to specify the project name in langsmith.
However, if we want to execute multiple chains within a single service but under different project names, how do we specify the project name at runtime per-chain?
We think we have a solution, but it required looking into the LangChain code, so we are concerned this is not the idomatic way to do it and it might cause issues when their API changes.
This is what we think we should do.

Instantiate the chain.
Instantiate a custom langchain tracer but with a specific project name defined at runtime.
Replace the chain's callbacks with the tracer.

        chain = LLMChain(llm=self._chat_model, prompt=prompt, verbose=True)
        tracer = LangChainTracer(project_name=""whatever"")
        chain.callbacks = CallbackManager(handlers=[tracer])

Thank you in advance for any help.
","You can specify project name in the @traceable decorator as an argument.
@traceable(
    run_type=""llm"",
    project_name=""My Project""
)
def my_llm_call(
    ...
) -> str:

You can also put it as a part of langsmith_extra when calling the traced function. This takes precedence over the traceable project:
my_llm_call(
   ...,
   langsmith_extra={""project_name"": ""My Project 2""},
)

",langchain
importerror cannot import name 39pre_init39 from 39langchain_coreutils39,"When I want to install langchain libraries from requirements.txt  I'm getting
ImportError: cannot import name 'pre_init' from 'langchain_core.utils'

I've tried to install libraries from terminal using these commands :
pip install gigachain
pip install gigachat
pip install -U langchain-community

and it's working, so I used
pip freeze

And pasted all libraries from the terminal to requirements.txt and it's doesn't work. It would be nice if someone could help
","Your import error suggests that the langchain-core module has not been installed. You can confirm whether this is the case by checking the output from the following command:
pip show langchain-core

If it hasn't been installed, ensure it is installed via the following command:
pip install langchain-core

As for using a requirements.txt file, simply creating it doesn't automatically install the packages listed within. You still have to run the following command in your terminal to install the listed packages:
python -m pip install -r /path/to/requirements.txt

Finally, you don't have to manually create the requirements.txt file by coping and pasting the output from pip freeze. You can simply run:
python -m pip freeze > /path/to/requirements.txt

You can read more about requirement files under the requirements files section of the pip user guide.
",langchain
why is embedding csv file taking much longer than pdf embedding in langchain,"I successfully embedded a 400-page PDF document within 1-2 hours. However, when I tried to embed a CSV file with about 40k rows and only one column, the estimated embedding time is approximately 24 hours.
Here is the code I used:

embedder = OllamaEmbeddings(model=""nomic-embed-text"", show_progress=True)

file_path = 'filtered_combined_info.csv'

loader = CSVLoader(
    file_path=file_path,
    encoding='utf-8',  # or 'ISO-8859-1' if utf-8 doesn't work
    autodetect_encoding=False  # Set to True if you want to attempt autodetection
)
data = loader.load()

text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=0)
docs = text_splitter.split_documents(data)

persist_directory = 'db'

vectordb = Chroma.from_documents(documents=docs, 
                                 embedding=embedder,
                                 persist_directory=persist_directory)

Why is the embedding process for the CSV file taking significantly longer than for the PDF file? Are there any optimizations or changes I can make to reduce the embedding time for the CSV file?
Additionally, is there anything I am doing wrong that might be causing it to take so much time?

","I removed everything of Ollama that i installed in my local machine, and moved the installation to the docker.
First start the docker and run the following:
docker run -d --rm -v ./ollama:/root/.ollama -p 11434:11434 --name ollama ollama/ollama

and then install
docker exec -it ollama ollama run nomic-embed-text

Now use same like this before:
embedder = OllamaEmbeddings(model=""nomic-embed-text"",
                            show_progress=True)

Check the difference:

I don't know how installing on the docker seems increasing the speed but my guess is changing from windows (my machine) to docker Linux worked?
",langchain
possibility to load documents with azure document intelligence from azure container storage,"I have a storage account with Azure Container Storage configured consisting of multiple pdf/word/excel files. I would like to use Azure Document Intelligence to semantically chunk these files.
Is there a possibility to load the files directly from Container Storage to Azure Document Intelligence using langchain? According to the langchain docs it seems like either file has to be locally available or public url has to be handed over.
Attempt:
# Prerequisite: An Azure AI Document Intelligence resource in one of the 3 preview regions: East US, West US2, West Europe

import os
from langchain_community.document_loaders import AzureAIDocumentIntelligenceLoader

file_path = ""storage-path-to-file""
endpoint = os.getenv(""DOCUMENTINTELLIGENCE_ENDPOINT"")
key = os.getenv(""DOCUMENTINTELLIGENCE_API_KEY"")

loader = AzureAIDocumentIntelligenceLoader(
    api_endpoint=endpoint, api_key=key, file_path=file_path, api_model=""prebuilt-layout""
)

documents = loader.load()

# Returns:
# Message: Invalid request.
# Inner error: {
#    ""code"": ""InvalidManagedIdentity"",
#    ""message"": ""The managed identity configuration is invalid: Managed identity is not enabled  # for the current resource.""
# }

","
Is there a possibility to load the files directly from Container Storage to Azure Document Intelligence using langchain? According to the langchain  docs it seems like either file has to be locally available or public url has to be handed over.

You can use the below code which loads the files directly from Azure Blob storage using Azure Blob URL + SAS token.
Code:
import os
from langchain_community.document_loaders import AzureAIDocumentIntelligenceLoader

#bloburl + ?Sastoken
URL = ""https://venkat78932.blob.core.windows.net/test/24005356.pdf?sp=r&st=2024-07-09T12:41:41Z&se=2024-07-09T20:41:41Z&spr=https&sv=2022-11-02&sr=b&sig=eupT8WGH5ojQpXYd%2xxxxxD""
endpoint = os.getenv(""DOCUMENTINTELLIGENCE_ENDPOINT"")
key = os.getenv(""DOCUMENTINTELLIGENCE_API_KEY"")


loader = AzureAIDocumentIntelligenceLoader(
    api_endpoint=endpoint, api_key=key, url_path=URL, api_model=""prebuilt-layout""
)

documents = loader.load()

print(len(documents))
print(documents[0])

Output:
1
page_content='Word Documents Template\n===\n\n\n## Main heading:\n\nUse the Heading 1 style for primary headings so that screen readers can identify them as such.\n\nIf not already, manually change your heading 1 style to be:\n\n\\- sans serif (e.g. Arial, Verdana, Trebuchet or Calibri),\n\n\\- 16 pt, and\n\n\\- Bold\n\nThen set this formatting as your default for this style.\n\n\n## Sub Headings:\n\nUse Heading 2 style for sub headings.\n\nIf not already, manually change your heading 2 style to be:\n\n\\- sans serif (e.g. Arial, Verdana, Trebuchet or Calibri...................tands out, and does not distort the shape of text as italics and underlining do. Finally, block capitals can be difficult to follow as block capitals remove the natural shape of words, turning them into blocks. Clear layout allows one to focus on the content of visual materials rather than the format.\n\n\n## Furthermore\n\nIf you use headings it makes the creation and upkeep of tables of contents easier (For automatic creation and updating go to: Insert - Reference - Index and Tables - Table of contents).\n'


You can get the Azure Blob URL + SAS token from portal.
Portal -> Storage account -> Container -> your file -> Generate sas token -> click Generate sas token and url
Portal:

Reference:
Azure AI Document Intelligence | 🦜️🔗 LangChain
",langchain
how do i persist faiss indexes,"In the langchain wiki of FAISS, https://python.langchain.com/v0.2/docs/integrations/vectorstores/faiss/, it only talks about saving indexes to files.
db.save_local(""faiss_index"")

new_db = FAISS.load_local(""faiss_index"", embeddings)

docs = new_db.similarity_search(query)

How can I save the indexes to databases, such that we can organize and concurrently access multiple indexes?
Searched online but could not get much info on this.
Can FAISS be used with any kind of distributed databases?
","In fact, FAISS is considered as an in-memory database itself in order to vector search based on similarity that you can serialize and deserialize the indexes using functions like write_index and read_index within the FAISS interface directly or using save_local and load_local within the LangChain integration which typically uses the pickle for serialization.
If you need to store serialized files, you could manually save them in a NoSQL database like MongoDB as binary data, and then deserialize and retrieve them when needed, however, it is not the best practice!
If you are looking for a vector database that is not in-memory and capable in a scalable system, you might want to consider using Milvus which is designed for this purpose.
",langchain
outputparserexception could not parse llm output on jupyter notebook,"I am facing the following error on conda notebook
   File ~\.conda\envs\LLMS\lib\site-packages\langchain\agents\conversational\output_parser.py:26, in ConvoOutputParser.parse(self, text) 24 match = re.search(regex, text) 25 if not match: ---> 26     raise OutputParserException(f""Could not parse LLM output:{text}`"")
27 action = match.group(1)
28 action_input = match.group(2)
OutputParserException: Could not parse LLM output: `
Answer: ""Hello, good morning. I am a helpful assistant. 
Have a normal`

morning"")`
I also checked the https://python.langchain.com/docs/modules/agents/how_to/handle_parsing_errors
I was tryong with ConversationalAgent vs initialize_agent which has some limitation for my purpose.
This  is the code I tried
`
import os
from langchain.llms.huggingface_endpoint import HuggingFaceEndpoint
from langchain.llms import LlamaCpp
from langchain import PromptTemplate, LLMChain
from langchain.callbacks.manager import CallbackManager
from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler
import pandas as pd
from utils import *
llm_hf = HuggingFaceEndpoint(
            endpoint_url=""https://xxx"",
            huggingfacehub_api_token=""xxx"", task=""text-generation""
        )
from langchain.agents import create_sql_agent
from langchain.agents.agent_toolkits import SQLDatabaseToolkit
from langchain.sql_database import SQLDatabase
from langchain.llms.openai import OpenAI
from langchain.agents import AgentExecutor
from langchain.agents.agent_types import AgentType
from langchain.chat_models import ChatOpenAI
# Connect to the SQLite database (it will create a new one if it doesn't exist)
conn = sqlite3.connect('doctors.db')

# Replace 'table_name' with the name of the table you want to create in the database
table_name = 'Doctors'

# Use the `to_sql` method to save the DataFrame to the database
clean_df.to_sql(table_name, conn, if_exists='replace', index=False)


llm = llm_hf



db = SQLDatabase.from_uri(""sqlite:///doctors.db"")
db.get_table_names()
toolkit = SQLDatabaseToolkit(db=db,
                             llm=llm,
                            )
sql_executor = create_sql_agent(
    llm=llm,
    toolkit=toolkit,
    verbose=True,
    agent_type=AgentType.ZERO_SHOT_REACT_DESCRIPTION,
    top_k = 10000,
    #agent_executor_kwargs={""return_intermediate_steps"": True},
)

from langchain.agents import initialize_agent, Tool
tools = [
    Tool(
        name=""Doctors Database System"",
        func=sql_executor.run,
        return_direct=True,
        description=""useful to give information about doctors names, specialities and locations. Input should be a fully formed question."",
    )
]
from langchain.agents import ZeroShotAgent, Tool, AgentExecutor, ConversationalAgent
from langchain.memory import ConversationBufferWindowMemory
from langchain.llms import OpenAI
from langchain.chains import LLMChain

prefix = """"""You are a helpful assistant. 
Have a normal conversation with a human. 
You can offer to answer questions about a database with doctor information.
You have access to the following tools:""""""
suffix = """"""Begin!""

{chat_history}
Question: {input}
{agent_scratchpad}""""""

prompt = ConversationalAgent.create_prompt(
    tools,
    prefix=prefix,
    suffix=suffix,
    input_variables=[""input"", ""chat_history"", ""agent_scratchpad""],
)
memory = ConversationBufferWindowMemory(memory_key=""chat_history"", k = 5)

llm_chain = LLMChain(llm=llm, prompt=prompt)

agent = ConversationalAgent(llm_chain=llm_chain,
                            #output_parser= lambda **kwargs : dict(kwargs),
                            verbose=True)

agent_chain = AgentExecutor.from_agent_and_tools(
    agent=agent, tools=tools, verbose=True, memory=memory
)
agent_chain.run(""Hello, good `

Would really appreciate. Any suggestion thanks!
","Temporary work around would be as below
     try:
             response= agent_chain.run(""how many unique statuses are there?"")
     except Exception as e:
             response = str(e)
             if response.startswith(""Could not parse LLM output: `""):
                 response = response.removeprefix(""Could not parse LLM output: `"").removesuffix(""`"")
                 print(response)

",langchain
langchain cannot connect with azure sql database,"Trying to connect with my Azure SQL Database, it contains multiple tables.
Here's a sample of my code
from sqlalchemy import create_engine

driver = '{ODBC Driver 17 for SQL Server}'
odbc_str = 'mssql+pyodbc:///?odbc_connect=' \
                'Driver='+driver+ \
                ';Server=tcp:' + os.getenv(""SQL_SERVER"")+'.database.windows.net;PORT=1433' + \
                ';DATABASE=' + os.getenv(""SQL_DB"") + \
                ';Uid=' + os.getenv(""SQL_USERNAME"")+ \
                ';Pwd=' + os.getenv(""SQL_PWD"") + \
                ';Encrypt=yes;TrustServerCertificate=no;Connection Timeout=30;'

db_engine = create_engine(odbc_str)

from langchain.agents import AgentType, create_sql_agent
from langchain.sql_database import SQLDatabase
from langchain.agents.agent_toolkits.sql.toolkit import SQLDatabaseToolkit

db = SQLDatabase(db_engine)

And this is the error that I get thrown:
Error: ('HY104', '[HY104] [Microsoft][ODBC SQL Server Driver]Invalid precision value (0) (SQLBindParameter)')

The above exception was the direct cause of the following exception:

DBAPIError                                Traceback (most recent call last)
Cell In[16], line 5
      2 from langchain.sql_database import SQLDatabase
      3 from langchain.agents.agent_toolkits.sql.toolkit import SQLDatabaseToolkit
----> 5 db = SQLDatabase(db_engine)
      7 sql_toolkit = SQLDatabaseToolkit(db=db, llm=llm)
      8 sql_toolkit.get_tools()
...
[SQL: SELECT [INFORMATION_SCHEMA].[TABLES].[TABLE_NAME] 
FROM [INFORMATION_SCHEMA].[TABLES] 
WHERE [INFORMATION_SCHEMA].[TABLES].[TABLE_SCHEMA] = CAST(? AS NVARCHAR(max)) AND [INFORMATION_SCHEMA].[TABLES].[TABLE_TYPE] = CAST(? AS NVARCHAR(max)) ORDER BY [INFORMATION_SCHEMA].[TABLES].[TABLE_NAME]]
[parameters: ('dbo', 'BASE TABLE')]
(Background on this error at: https://sqlalche.me/e/20/dbapi)
Output is truncated. View as a scrollable element or open in a text editor. Adjust cell output settings...

Does anyone have an idea?
I want to connect to my azure SQL database, but it is not working and throwing me an error. I tried changing the driver, and encryption and such but it all does not seem to work.
","Error: ('HY104', '[HY104] [Microsoft][ODBC SQL Server Driver]Invalid precision value (0) (SQLBindParameter)')

According to this the issue may get lower version of SQL alchemy package, use compatible version of SQL alchemy. You can use below code to connect Azure SQL database to Lang chain:
import urllib
import sqlalchemy
from langchain.agents import AgentType, create_sql_agent
from langchain.sql_database import SQLDatabase
from langchain.agents.agent_toolkits.sql.toolkit import SQLDatabaseToolkit

server = '<serverName>.database.windows.net'
database = '<dbName>'
username = '<userName>'
password = '<password>'
driver = '{ODBC Driver 17 for SQL Server}'
odbc_str = 'DRIVER='+driver+';SERVER='+server+';PORT=1433;UID='+username+';DATABASE='+ database + ';PWD='+ password
connect_str = 'mssql+pyodbc:///?odbc_connect=' + urllib.parse.quote_plus(odbc_str)
engine = sqlalchemy.create_engine(connect_str)
db = SQLDatabase(engine)
db.run(""SELECT [TABLE_NAME] FROM [INFORMATION_SCHEMA].[TABLES] WHERE [TABLE_SCHEMA] = 'dbo' AND [TABLE_TYPE] = 'BASE TABLE' ORDER BY [TABLE_NAME]"")   

You will get the output as shown below:

",langchain
get all documents from chromadb using python and langchain,"I'm using langchain to process a whole bunch of documents which are in an Mongo database.
I can load all documents fine into the chromadb vector storage using langchain. Nothing fancy being done here. This is my code:

from langchain.embeddings.openai import OpenAIEmbeddings
embeddings = OpenAIEmbeddings()

from langchain.vectorstores import Chroma
db = Chroma.from_documents(docs, embeddings, persist_directory='db')
db.persist()


Now, after storing the data, I want to get a list of all the documents and embeddings WITH id's.
This is so I can store them back into MongoDb.
I also want to put them through Bertopic to get the topic categories.
Question 1 is: how do I get all documents I've just stored in the Chroma database? I want the documents, and all the metadata.
Many thanks for your help!
","Looking at the source code (https://github.com/hwchase17/langchain/blob/master/langchain/vectorstores/chroma.py)
You can just call below
db.get()

and you will get a json output with the id's, embeddings and docs data.
",langchain
langchain error  notimplementederror need to determine which default deprecation schedule to use within  minor releases,"I ran the bellow code. However, currently, it shows an error massage.
from langchain.llms import GooglePalm

api_key = 'my_API'


llm = GooglePalm(google_api_key=api_key,
                 temperature=0.1)

This is the error I got.
NotImplementedError                       Traceback (most recent call last)
<ipython-input-2-a3e32679669c> in <cell line: 7>()
      5 
      6 # Create llm variable here
----> 7 llm = GooglePalm(google_api_key=api_key,
      8                  temperature=0.1)

2 frames
/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py in warn_deprecated(since, message, name, alternative, pending, obj_type, addendum, removal)
    293         if not removal:
    294             removal = f""in {removal}"" if removal else ""within ?? minor releases""
--> 295             raise NotImplementedError(
    296                 f""Need to determine which default deprecation schedule to use. ""
    297                 f""{removal}""

NotImplementedError: Need to determine which default deprecation schedule to use. within ?? minor releases

Can someone please help me to solve this?
I need to create the large language model variable.
","I also had a similar error, I'm using langchain==0.1.4 in which its google_palm file suggests deprecation of GooglePalm and it is replaced with
langchain_google_genai.GoogleGenerativeAI
Make use of GoogleGenerativeAI instead;
for its installation follow these steps:

pip install --upgrade --quiet langchain-google-genai
pip install -q -U google-generativeai

Then use this code:
from langchain_google_genai import GoogleGenerativeAI

llm = GoogleGenerativeAI(model=""models/text-bison-001"", google_api_key=SECRET_KEY, temperature=0.1)

Use Google Palm API key.
Article Links:

Langchain GoogleGenerativeAI
GoogleGenerativeAI Quick Start guide

I hope you find this useful.
",langchain
what does quotiquot in the section quot_iqquot and quot_mquot mean in this name quotmetallama38binstructiq3_mggufquot,"Appreciate if someone could let me know what does ""I"" in the section ""_IQ"" and ""_M"" mean in this name ""Meta-Llama-3-8B-Instruct-IQ3_M.gguf""???
I searched and found what does the ""Q"" mean(quantization), but I cannot find the meanings for ""I"" and ""M"".
","IQ quantization uses an Importance Matrix (Imatrix) to determine the importance of different model activations during the quantization process.
This is an alternate quantization method to K quantization. The IQ quantization is generally a more advanced and higher-quality quantization technique than the legacy K-quant methods. Still, the optimal choice depends on the target hardware and performance requirements.
The ""M"", ""S"", ""XS"" and ""XXS"" suffixes in IQ quantization names refer to the model size, with ""M"" being the largest and ""XXS"" being the smallest. For example, the bitness is not exactly 3, as de M uses ~3.6 bits per parameter and XXS uses ~3.2 bits.
",llama
why rag is slower than llm,"I used RAG with LLAMA3 for AI bot. I find RAG with chromadb is much slower than call LLM itself.
Following the test result, with just one simple web page about 1000 words, it takes more than 2 seconds for retrieving:
Time used for retrieving: 2.245511054992676
Time used for LLM: 2.1182022094726562

Here is my simple code:
embeddings = OllamaEmbeddings(model=""llama3"")
vectorstore = Chroma.from_documents(documents=splits, embedding=embeddings)
retriever = vectorstore.as_retriever()
question = ""What is COCONut?""
start = time.time()
retrieved_docs = retriever.invoke(question)
formatted_context = combine_docs(retrieved_docs)
end = time.time()
print(f""Time used for retrieving: {end - start}"")

start = time.time()
answer = ollama_llm(question, formatted_context)
end = time.time()
print(f""Time used for LLM: {end - start}"")

I found when my chromaDB size just about 1.4M, it takes more than 20 seconds for retrieving and still only takes about 3 or 4 seconds for LLM. Is there anything I missing? or RAG tech itself is so slow?
","
Retrieval-Augmented Generation (RAG) models are slower as compared to Large Language Models (LLMs) due to an extra retrieval step.
Since RAG models search a database for relevant information, which can be time-consuming, especially with large databases, it is tend to be slower. Versus LLMs respond faster as they rely on pre-trained information and skip the said database retrieval step.
You must also note that LLMs may lack the most current or specific information compared to RAG models, which usually access external data sources and can provide more detailed responses using the latest information.
Thus, Despite being slower, RAG models have the advantage in response quality and relevance for complex, information-rich queries. Hope I am able to help.

Please mark this as a answer, if you think this sets a perfect context for your case.
",llama
why does running llama 31 70b model underutilises the gpu,"I have deployed Llama 3.1 70B and Llama 3.1 8B on my system and it works perfectly for the 8B model. When I tested it for 70B, it underutilized the GPU and took a lot of time to respond. Here are the system details:
CPU: Ryzen 7 3700x, RAM: 48g ddr4 2400, SSD: NVME m.2, GPU: RTX 3060 ti, Motherboard: B550 M:
sudo docker logs cybersage-lama
time=2024-12-05T09:04:12.081Z level=INFO source=server.go:105 msg=""system memory"" total=""47.0 GiB"" free=""45.8 GiB"" free_swap=""3.9 GiB""
time=2024-12-05T09:04:12.082Z level=INFO source=memory.go:343 msg=""offload to cuda"" layers.requested=-1 layers.model=81 layers.offload=10 layers.split="""" memory.available=""[7.5 GiB]"" memory.gpu_overhead=""0 B"" memory.required.full=""44.0 GiB"" memory.required.partial=""7.2 GiB"" memory.required.kv=""640.0 MiB"" memory.required.allocations=""[7.2 GiB]"" memory.weights.total=""38.9 GiB"" memory.weights.repeating=""38.1 GiB"" memory.weights.nonrepeating=""822.0 MiB"" memory.graph.full=""324.0 MiB"" memory.graph.partial=""1.1 GiB""
time=2024-12-05T09:04:12.085Z level=INFO source=server.go:380 msg=""starting llama server"" cmd=""/usr/lib/ollama/runners/cuda_v12/ollama_llama_server --model /root/.ollama/models/blobs/sha256-de20d2cf2dc430b1717a8b07a9df029d651f3895dbffec4729a3902a6fe344c9 --ctx-size 2048 --batch-size 512 --n-gpu-layers 10 --threads 8 --parallel 1 --port 44611""
time=2024-12-05T09:04:12.086Z level=INFO source=sched.go:449 msg=""loaded runners"" count=1
time=2024-12-05T09:04:12.086Z level=INFO source=server.go:559 msg=""waiting for llama runner to start responding""
time=2024-12-05T09:04:12.087Z level=INFO source=server.go:593 msg=""waiting for server to become available"" status=""llm server error""
time=2024-12-05T09:04:12.150Z level=INFO source=runner.go:939 msg=""starting go runner""
time=2024-12-05T09:04:12.150Z level=INFO source=runner.go:940 msg=system info=""AVX = 1 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 0 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 0 | FP16_VA = 0 | RISCV_VECT = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | cgo(gcc)"" threads=8
time=2024-12-05T09:04:12.150Z level=INFO source=.:0 msg=""Server listening on 127.0.0.1:44611""
llama_model_loader: loaded meta data with 29 key-value pairs and 724 tensors from /root/.ollama/models/blobs/sha256-de20d2cf2dc430b1717a8b07a9df029d651f3895dbffec4729a3902a6fe344c9 (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = Meta Llama 3.1 70B Instruct
llama_model_loader: - kv   3:                           general.finetune str              = Instruct
llama_model_loader: - kv   4:                           general.basename str              = Meta-Llama-3.1
llama_model_loader: - kv   5:                         general.size_label str              = 70B
llama_model_loader: - kv   6:                            general.license str              = llama3.1
llama_model_loader: - kv   7:                               general.tags arr[str,6]       = [""facebook"", ""meta"", ""pytorch"", ""llam...
llama_model_loader: - kv   8:                          general.languages arr[str,8]       = [""en"", ""de"", ""fr"", ""it"", ""pt"", ""hi"", ...
llama_model_loader: - kv   9:                          llama.block_count u32              = 80
llama_model_loader: - kv  10:                       llama.context_length u32              = 131072
llama_model_loader: - kv  11:                     llama.embedding_length u32              = 8192
llama_model_loader: - kv  12:                  llama.feed_forward_length u32              = 28672
llama_model_loader: - kv  13:                 llama.attention.head_count u32              = 64
llama_model_loader: - kv  14:              llama.attention.head_count_kv u32              = 8
llama_model_loader: - kv  15:                       llama.rope.freq_base f32              = 500000.000000
llama_model_loader: - kv  16:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  17:                          general.file_type u32              = 15
llama_model_loader: - kv  18:                           llama.vocab_size u32              = 128256
llama_model_loader: - kv  19:                 llama.rope.dimension_count u32              = 128
llama_model_loader: - kv  20:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  21:                         tokenizer.ggml.pre str              = llama-bpe
llama_model_loader: - kv  22:                      tokenizer.ggml.tokens arr[str,128256]  = [""!"", ""\"""", ""#"", ""$"", ""%"", ""&"", ""'"", ...
llama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
time=2024-12-05T09:04:12.341Z level=INFO source=server.go:593 msg=""waiting for server to become available"" status=""llm server loading model""
llama_model_loader: - kv  24:                      tokenizer.ggml.merges arr[str,280147]  = [""Ġ Ġ"", ""Ġ ĠĠĠ"", ""ĠĠ ĠĠ"", ""...
llama_model_loader: - kv  25:                tokenizer.ggml.bos_token_id u32              = 128000
llama_model_loader: - kv  26:                tokenizer.ggml.eos_token_id u32              = 128009
llama_model_loader: - kv  27:                    tokenizer.chat_template str              = {{- bos_token }}\n{%- if custom_tools ...
llama_model_loader: - kv  28:               general.quantization_version u32              = 2
llama_model_loader: - type  f32:  162 tensors
llama_model_loader: - type q4_K:  441 tensors
llama_model_loader: - type q5_K:   40 tensors
llama_model_loader: - type q6_K:   81 tensors
llm_load_vocab: special tokens cache size = 256
llm_load_vocab: token to piece cache size = 0.7999 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 128256
llm_load_print_meta: n_merges         = 280147
llm_load_print_meta: vocab_only       = 0
llm_load_print_meta: n_ctx_train      = 131072
llm_load_print_meta: n_embd           = 8192
llm_load_print_meta: n_layer          = 80
llm_load_print_meta: n_head           = 64
llm_load_print_meta: n_head_kv        = 8
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_swa            = 0
llm_load_print_meta: n_embd_head_k    = 128
llm_load_print_meta: n_embd_head_v    = 128
llm_load_print_meta: n_gqa            = 8
llm_load_print_meta: n_embd_k_gqa     = 1024
llm_load_print_meta: n_embd_v_gqa     = 1024
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 28672
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 500000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_ctx_orig_yarn  = 131072
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: ssm_dt_b_c_rms   = 0
llm_load_print_meta: model type       = 70B
llm_load_print_meta: model ftype      = Q4_K - Medium
llm_load_print_meta: model params     = 70.55 B
llm_load_print_meta: model size       = 39.59 GiB (4.82 BPW) 
llm_load_print_meta: general.name     = Meta Llama 3.1 70B Instruct
llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'
llm_load_print_meta: EOS token        = 128009 '<|eot_id|>'
llm_load_print_meta: LF token         = 128 'Ä'
llm_load_print_meta: EOT token        = 128009 '<|eot_id|>'
llm_load_print_meta: EOM token        = 128008 '<|eom_id|>'
llm_load_print_meta: EOG token        = 128008 '<|eom_id|>'
llm_load_print_meta: EOG token        = 128009 '<|eot_id|>'
llm_load_print_meta: max token length = 256
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: NVIDIA GeForce RTX 3060 Ti, compute capability 8.6, VMM: yes
llm_load_tensors: ggml ctx size =    0.68 MiB
llm_load_tensors: offloading 10 repeating layers to GPU
llm_load_tensors: offloaded 10/81 layers to GPU
llm_load_tensors:        CPU buffer size = 40543.11 MiB
llm_load_tensors:      CUDA0 buffer size =  5188.75 MiB
llama_new_context_with_model: n_ctx      = 2048
llama_new_context_with_model: n_batch    = 512
llama_new_context_with_model: n_ubatch   = 512
llama_new_context_with_model: flash_attn = 0
llama_new_context_with_model: freq_base  = 500000.0
llama_new_context_with_model: freq_scale = 1
llama_kv_cache_init:  CUDA_Host KV buffer size =   560.00 MiB
llama_kv_cache_init:      CUDA0 KV buffer size =    80.00 MiB
llama_new_context_with_model: KV self size  =  640.00 MiB, K (f16):  320.00 MiB, V (f16):  320.00 MiB
llama_new_context_with_model:  CUDA_Host  output buffer size =     0.52 MiB
llama_new_context_with_model:      CUDA0 compute buffer size =  1088.45 MiB
llama_new_context_with_model:  CUDA_Host compute buffer size =    20.01 MiB
llama_new_context_with_model: graph nodes  = 2566
llama_new_context_with_model: graph splits = 914
time=2024-12-05T09:04:19.620Z level=INFO source=server.go:598 msg=""llama runner started in 7.53 seconds""

Here is the output of nvidia-smi when a request is sent to the model using 70B:
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 560.35.03              Driver Version: 560.35.03      CUDA Version: 12.6     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA GeForce RTX 3060 Ti     On  |   00000000:0A:00.0 Off |                  N/A |
| 30%   57C    P0             74W /  225W |    6534MiB /   8192MiB |      5%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
                                                                                     
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|    0   N/A  N/A   3129822      C   ...unners/cuda_v12/ollama_llama_server       6524MiB |
+-----------------------------------------------------------------------------------------+

Here is how I deployed Llama 3.1 on the machine:

Pull the LLaMA Docker Image: Pull the LLaMA Docker image (in this case, ollama/ollama):
sudo docker pull ollama/ollama

This test was successful.

Test GPU Access: You can test GPU access by running a CUDA base image to confirm that Docker recognizes your GPU:
sudo docker run --rm nvidia/cuda:11.8.0-base-ubuntu22.04 nvidia-smi


Run the LLaMA Container: Run the LLaMA container with GPU access, mapping the host port to the container’s port without additional environment variables:
sudo docker run -d --gpus all -p 11434:11434 --name cybersage-lama ollama/ollama



I am not sure why it is underutilizing the GPU and everything is going slow.
","You need ~148GB of VRAM to run a 70B unquantised model (16FP). And ~ 48GB to run an INT4 quantised model.
You can see from the logs that 10 out of the 81 layers are in the GPU.
llm_load_tensors: offloading 10 repeating layers to GPU
llm_load_tensors: offloaded 10/81 layers to GPU

The other layers will run in the CPU, and thus the slowness and low GPU use.
I can see that the total model is using ~45GB of ram (5 in the GPU and 40 on the CPU), so I reckon you are running an INT4 quantised model). You can see some of this in the logs you shared
llm_load_tensors:        CPU buffer size = 40543.11 MiB
llm_load_tensors:      CUDA0 buffer size =  5188.75 MiB

To know the amount of memory required multiply the number of parameters by the size of weight.
16FP is 2 bytes so ~ 140GB. INT4 is 0.5 bytes = ~35GB. In practice one needs more than the bare minimum to run the model.
",llama
running rest api ollama and nginx reverse proxy,"I want to run Ollama docker image. This exposes entire API with its endpoints.
I want to secure it by adding reverse proxy on top of it and i have chosen Nginx for the job.
So I need to add Nginx to my docker image with Ollama.
So i have constructed such Dockerfile:
FROM ollama/ollama:latest

FROM nginx:latest

COPY /.nginx/nginx.conf /etc/nginx/conf.d/default.conf

My Nginx configuration:
server {
    listen 80;
    
    location /api/tags {
        proxy_pass http://localhost:11434/api/tags;
        proxy_set_header Host localhost:11434;
    }
    
    # Default: Return 404 for all other endpoints
    location / {
        return 404;
    }
}

Here I allow just one endpoint to be proxied to Ollama.
In order to test it, I tried some endpoints and they returned correctly 404.
But when I have tried api/tags path, I got ""Bad gateway"" 502 error.
In docker logs I could see:

2024-12-01 22:44:41 2024/12/01 21:44:41 [warn] 29#29: *1 upstream server temporarily disabled while connecting to upstream, client: 172.17.0.1, server: , request: ""GET /api/tags HTTP/1.1"", upstream: ""http://127.0.0.1:11434/api/tags"", host: ""localhost:8000""


2024-12-01 22:44:41 2024/12/01 21:44:41 [error] 29#29: *1 connect() failed (111: Connection refused) while connecting to upstream, client: 172.17.0.1, server: , request: ""GET /api/tags HTTP/1.1"", upstream: ""http://127.0.0.1:11434/api/tags"", host: ""localhost:8000""


2024-12-01 22:44:41 2024/12/01 21:44:41 [warn] 29#29: *1 upstream server temporarily disabled while connecting to upstream, client: 172.17.0.1, server: , request: ""GET /api/tags HTTP/1.1"", upstream: ""http://127.0.0.1:11434/api/tags"", host: ""localhost:8000""


2024-12-01 22:44:41 172.17.0.1 - - [01/Dec/2024:21:44:41 +0000] ""GET /api/tags HTTP/1.1"" 502 559 ""-"" ""Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/128.0.0.0 Safari/537.36 OPR/114.0.0.0"" ""-""

It seems that Ollama server is not running, but I don't know:

How to check running processes in Docker, ps command is not found in docker.
How to run Ollama server?
How to define in Docker to start Ollama server automatically?

","You can't create a Docker image containing multiple programs the way you're trying. The Dockerfile you have will result in an image that only contains Nginx and not Ollama. It's also generally recommended to only have one process running in a container.
I'd recommend running Ollama and Nginx in each their own containers and orchestrate them using Docker Compose.
Create a docker-compose.yml file containing
services:
  ollama:
    image: ollama/ollama:latest
  nginx:
    build: .
    ports:
      - 80:80

Then change your nginx.conf file so it passes the request to the Ollama container by using the ollama service name instead of localhost like this:
server {
    listen 80;
    
    location /api/tags {
        proxy_pass http://ollama:11434/api/tags;
        proxy_set_header Host localhost:11434;
    }
    
    # Default: Return 404 for all other endpoints
    location / {
        return 404;
    }
}

Your Dockerfile will work as is, since the first line is effectively ignored. But to keep things tidy, you should change it to
FROM nginx:latest

COPY /.nginx/nginx.conf /etc/nginx/conf.d/default.conf

docker compose up -d will start both containers. When a request comes in to Nginx on http://localhost/api/tags it'll pass it on to the Ollama container on /api/tags on port 11434.
",llama
llamacpppython not using nvidia gpu cuda,"I have been playing around with oobabooga text-generation-webui  on my Ubuntu 20.04 with my NVIDIA GTX 1060 6GB for some weeks without problems. I have been using llama2-chat models sharing memory between my RAM and NVIDIA VRAM. I installed without much problems following the intructions on its repository.
So what I want now is to use the model loader llama-cpp with its package llama-cpp-python bindings to play around with it by myself. So using the same miniconda3 environment that oobabooga text-generation-webui uses I started a jupyter notebook and I could make inferences and everything is working well BUT ONLY for CPU.
A working example bellow,
from llama_cpp import Llama

llm = Llama(model_path=""/mnt/LxData/llama.cpp/models/meta-llama2/llama-2-7b-chat/ggml-model-q4_0.bin"", 
            n_gpu_layers=32, n_threads=6, n_ctx=3584, n_batch=521, verbose=True), 

prompt = """"""[INST] <<SYS>>
Name the planets in the solar system? 
<</SYS>>
[/INST] 
""""""
output = llm(prompt, max_tokens=350, echo=True)
print(output['choices'][0]['text'].split('[/INST]')[-1])


Of course! Here are the eight planets in our solar system, listed in order from closest to farthest from the Sun:

Mercury
Venus
Earth
Mars
Jupiter
Saturn
Uranus
Neptune



Note that Pluto was previously considered a planet but is now classified as a dwarf planet due to its small size and unique orbit.

I want to make inference using GPU as well. What is wrong?
Why can't I offload to gpu like the parameter n_gpu_layers=32 specifies and also like oobabooga text-generation-webui already does on the same miniconda environment whithout any problems?
","After searching around and suffering quite for 3 weeks I found out this issue on its repository.
The llama-cpp-python needs to known where is the libllama.so shared library. So exporting it before running my python interpreter, jupyter notebook etc. did the trick.
For using the miniconda3 installation used by oobabooga text-generation-webui I exported it like bellow:
export LLAMA_CPP_LIB=/yourminicondapath/miniconda3/lib/python3.10/site-packages/llama_cpp_cuda/libllama.so

Voilà!!!!
On importing  from llama_cpp import Llama I get

ggml_init_cublas: found 1 CUDA devices:
Device 0: NVIDIA GeForce GTX 1060, compute capability 6.1

And on
llm = Llama(model_path=""/mnt/LxData/llama.cpp/models/meta-llama2/llama-2-7b-chat/ggml-model-q4_0.bin"", 
            n_gpu_layers=28, n_threads=6, n_ctx=3584, n_batch=521, verbose=True), 

...

llama_model_load_internal: using CUDA for GPU acceleration
llama_model_load_internal: mem required  = 2381.32 MB (+ 1026.00 MB per state)
llama_model_load_internal: allocating batch_size x (512 kB + n_ctx x 128 B) = 480 MB VRAM for the scratch buffer
llama_model_load_internal: offloading 28 repeating layers to GPU
llama_model_load_internal: offloaded 28/35 layers to GPU
llama_model_load_internal: total VRAM used: 3521 MB
...

",llama
issue with llama 27b model producing output limited to 511 tokens,"I am facing an issue with the Llama 2-7B model where the output is consistently limited to only 511 tokens, even though the model should theoretically be capable of producing outputs up to a maximum of 4096 tokens.
I’ve tried setting the max_tokens parameter to higher values, such as 3000, and have calculated the available tokens by subtracting the prompt tokens from the model’s total token limit (4096 tokens). However, despite these adjustments, I continue to receive outputs capped at 511 tokens.
Here’s a snippet of the code I am using to interact with the model:
import psutil
import os
import warnings
from llama_cpp import Llama

# Suppress warnings
warnings.filterwarnings(""ignore"")

# Path to the model
model_path = ""C:/Llama_project/models/llama-2-7b-chat.Q2_K.gguf""

# Load the model
llm = Llama(model_path=model_path)

# System message to set the behavior of the assistant
system_message = ""You are a helpful assistant.""

# Function to ask questions
def ask_question(question):
    # Use user input for the question prompt
    prompt = f""Answer the following question: {question}""

    # Calculate the remaining tokens for output based on the model's 4096 token limit
    prompt_tokens = len(prompt.split())  # Rough token count estimate
    max_output_tokens = 4096 - prompt_tokens  # Tokens left for output
    
    # Monitor memory usage before calling the model
    process = psutil.Process(os.getpid())
    mem_before = process.memory_info().rss / 1024 ** 2  # Memory in MB

    # Get the output from the model with the calculated max tokens for output
    output = llm(prompt=prompt, max_tokens=max_output_tokens, temperature=0.7, top_p=1.0)

    # Monitor memory usage after calling the model
    mem_after = process.memory_info().rss / 1024 ** 2  # Memory in MB
    
    # Clean the output and return only the answer text
    return output[""choices""][0][""text""].strip()

# Main loop for user interaction
while True:
    user_input = input(""Ask a question (or type 'exit' to quit): "")
    
    if user_input.lower() == 'exit':
        print(""Exiting the program."")
        break
    
    # Get the model's response
    answer = ask_question(user_input)
    
    # Print only the answer
    print(f""Answer: {answer}"")

Problem Details:

Model: Llama 2-7B (Q2_K version)
Expected Output: I was expecting a response close to the maximum token limit (3000 or more tokens).
Actual Output: The output is capped at 511 tokens, regardless of the prompt length.

Tried:

Setting max_tokens to 3000 or higher.
Calculating the available tokens by subtracting the prompt length from the model’s total token limit.

I would expect the model to generate responses that are close to the token limit (ideally closer to 3000 tokens or more, depending on the input), but it keeps producing output limited to 511 tokens.
","Try to add n_ctx to 2048 in Llama constructor, so:
Llama(n_ctx=2048, model_path=model_path)

This parameters tells model what is the maximum length of the prompt and response combined.
",llama
error when pushing llama31 7b finetuned model to huggingface,"I'm having an issue pushing a fine-tuned Llama 3.1 model to Huggingface, getting the error below.  All of the literature that I've read suggests that the code below that I'm using to push is correct, and the model exists on my page, there are just no files or model card associated with it.
model.push_to_hub(NEW_MODEL, token=token, max_shard_size=""10GB"")
The loading of said model code is below, done before model.push:
tokenizer = AutoTokenizer.from_pretrained(NEW_MODEL)

model = AutoModelForCausalLM.from_pretrained(
    MODEL_NAME,
    torch_dtype=torch.float16,
    device_map=""auto"",
)

model.resize_token_embeddings(len(tokenizer), pad_to_multiple_of=8)
model = PeftModel.from_pretrained(model, NEW_MODEL)
model = model.merge_and_unload()


  

Does anyone have a word of advice here?
Cheers!
---------------------------------------------------------------------------
IsADirectoryError                         Traceback (most recent call last)
<ipython-input-86-cac150fe4ba1> in <cell line: 1>()
----> 1 model.push_to_hub(NEW_MODEL, token=token, max_shard_size=""10GB"")

3 frames
/usr/local/lib/python3.10/dist-packages/transformers/utils/hub.py in push_to_hub(self, repo_id, use_temp_dir, commit_message, private, token, max_shard_size, create_pr, safe_serialization, revision, commit_description, tags, **deprecated_kwargs)
    917 
    918         # Create a new empty model card and eventually tag it
--> 919         model_card = create_and_tag_model_card(
    920             repo_id, tags, token=token, ignore_metadata_errors=ignore_metadata_errors
    921         )

/usr/local/lib/python3.10/dist-packages/transformers/utils/hub.py in create_and_tag_model_card(repo_id, tags, token, ignore_metadata_errors)
   1172     try:
   1173         # Check if the model card is present on the remote repo
-> 1174         model_card = ModelCard.load(repo_id, token=token, ignore_metadata_errors=ignore_metadata_errors)
   1175     except EntryNotFoundError:
   1176         # Otherwise create a simple model card from template

/usr/local/lib/python3.10/dist-packages/huggingface_hub/repocard.py in load(cls, repo_id_or_path, repo_type, token, ignore_metadata_errors)
    185 
    186         # Preserve newlines in the existing file.
--> 187         with card_path.open(mode=""r"", newline="""", encoding=""utf-8"") as f:
    188             return cls(f.read(), ignore_metadata_errors=ignore_metadata_errors)
    189 

/usr/lib/python3.10/pathlib.py in open(self, mode, buffering, encoding, errors, newline)
   1117         if ""b"" not in mode:
   1118             encoding = io.text_encoding(encoding)
-> 1119         return self._accessor.open(self, mode, buffering, encoding, errors,
   1120                                    newline)
   1121 

IsADirectoryError: [Errno 21] Is a directory: 'Mottzerella/Llama-3-8B-Instruct-Finance-RAG'

Tried to push a saved model to Huggingface while logged into Google Colab.  Tried different versions of the push including use_temp_dir=True, all with the same result.  The model itself is listed in my Huggingface but none of the files are associated with it.  Expected the model to be pushed, but got the above error.
","according to https://github.com/huggingface/transformers/issues/28543
the issue stems from having a local path that has the same name as NEW_MODEL and a remote repository having the id NEW_MODEL as well with no model card.
to fix this either :

rename the local path to something else
push_to_hub to a repository with a new ID something like NEW_MODEL_2

",llama
could not parse modelproto from metallama318binstructtokenizermodel,"I tried to use Llama 3.1 without relying on external programs, but I was not successful. I downloaded the Meta-Llama-3.1-8B-Instruct model, which includes only the files consolidated.00.pth, params.json, and tokenizer.model.
The params.json file contains the following configuration:
{
  ""dim"": 4096,
  ""n_layers"": 32,
  ""n_heads"": 32,
  ""n_kv_heads"": 8,
  ""vocab_size"": 128256,
  ""ffn_dim_multiplier"": 1.3,
  ""multiple_of"": 1024,
  ""norm_eps"": 1e-05,
  ""rope_theta"": 500000.0,
  ""use_scaled_rope"": true
}

Can you guide me on how to use this model?
I have tried the following code:
import torch
from transformers import LlamaTokenizer, LlamaForCausalLM, LlamaConfig

model_path = 'Meta-Llama-3.1-8B-Instruct'
tokenizer_path = f'{model_path}/tokenizer.model'

# Load tokenizer
tokenizer = LlamaTokenizer.from_pretrained(tokenizer_path)

# Configure the model
model_config = LlamaConfig(
    hidden_size=4096,
    num_hidden_layers=32,
    num_attention_heads=32,
    intermediate_size=5324.8,  # This value is calculated as 4096 * 1.3
    vocab_size=128256,
    use_scaled_rope=True
)

# Load the model
model = LlamaForCausalLM(config=model_config)
model.load_state_dict(torch.load(f'{model_path}/consolidated.00.pth'))

model.eval()

# Tokenize and generate output
input_text = ""Hello, how are you?""
inputs = tokenizer(input_text, return_tensors='pt')
outputs = model.generate(inputs['input_ids'])

# Decode and print the output
decoded_output = tokenizer.decode(outputs[0], skip_special_tokens=True)
print(decoded_output)

However, I got the following error:
(venv) PS C:\Users\Main\Desktop\mygguf> python app.py
C:\Users\Main\Desktop\mygguf\venv\Lib\site-packages\transformers\tokenization_utils_base.py:2165: FutureWarning: Calling LlamaTokenizer.from_pretrained() with the path to a single file or url is deprecated and won't be possible anymore in v5. Use a model identifier or the path to a directory instead.
  warnings.warn(
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message
Traceback (most recent call last):
  File ""C:\Users\Main\Desktop\mygguf\app.py"", line 9, in <module>
    tokenizer = LlamaTokenizer.from_pretrained(tokenizer_path)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""C:\Users\Main\Desktop\mygguf\venv\Lib\site-packages\transformers\tokenization_utils_base.py"", line 2271, in from_pretrained
    return cls._from_pretrained(
           ^^^^^^^^^^^^^^^^^^^^^
  File ""C:\Users\Main\Desktop\mygguf\venv\Lib\site-packages\transformers\tokenization_utils_base.py"", line 2505, in _from_pretrained
    tokenizer = cls(*init_inputs, **init_kwargs)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""C:\Users\Main\Desktop\mygguf\venv\Lib\site-packages\transformers\models\llama\tokenization_llama.py"", line 171, in __init__
    self.sp_model = self.get_spm_processor(kwargs.pop(""from_slow"", False))
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""C:\Users\Main\Desktop\mygguf\venv\Lib\site-packages\transformers\models\llama\tokenization_llama.py"", line 198, in get_spm_processor
    tokenizer.Load(self.vocab_file)
  File ""C:\Users\Main\Desktop\mygguf\venv\Lib\site-packages\sentencepiece\__init__.py"", line 961, in Load
    return self.LoadFromFile(model_file)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""C:\Users\Main\Desktop\mygguf\venv\Lib\site-packages\sentencepiece\__init__.py"", line 316, in LoadFromFile
    return _sentencepiece.SentencePieceProcessor_LoadFromFile(self, arg)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
RuntimeError: Internal: could not parse ModelProto from Meta-Llama-3.1-8B-Instruct/tokenizer.model

","The way you should think about using llm model is that you have to pass it information systematically.
Since you are using a publicly available model they come with things like weights, cfg etc... so you don't need to declare yours.
All you need do is to start by declaring the file-paths of your model(i.e where you downloaded it).
Also there is tokenism (tokens are simply vectors which models understand they usually map it with the given words you ask it).
If the output is not the desired, You can use different tokenizers
You can look up the process of using different tokens or tokenizers such as BERT, All-net etc here is a link to a blog
You should also spend sometime on Huggingface website here is the link
hugging_face
Here is a snippet of how to use the model, I have provided comments on what each line does. I hope it helps you!
import torch
from transformers import AutoTokenizer, AutoModel
from transformers import LlamaTokenizer, LlamaForCausalLM, LlamaConfig

model_path = 'Meta-Llama-3.1-8B-Instruct'


# Load the tokenizer directly from the model path
tokenizer = AutoTokenizer.from_pretrained(model_path)

# Load model configuration from params.json
config = LlamaConfig.from_json_file(f'{model_path}/params.json')

# load the model with the specific configs. 
model = LlamaForCausalLM(config=config)

# Load the weights of the model
state_dict = torch.load(f'{model_path}/consolidated.00.pth', map_location=torch.device('cpu'))
model.load_state_dict(state_dict)

model.eval()

# generate tokens and generate output
input_text = ""Hello, how are you?""
inputs = tokenizer(input_text, return_tensors='pt')
outputs = model.generate(inputs['input_ids'])

# print the output you asked it 
output = tokenizer.decode(outputs[0], skip_special_tokens=True)
print(output)

",llama
finding configjson for llama 31 8b,"I installed the Llama 3.1 8B model through Meta's Github page, but I can't get their example code to work. I'm running the following code in the same directory as the Meta-Llama-3.1-8B folder:
import transformers
import torch

pipeline = transformers.pipeline(
  ""text-generation"",
  model=""Meta-Llama-3.1-8B"",
  model_kwargs={""torch_dtype"": torch.bfloat16},
  device=""cuda""
)

The error is
OSError: Meta-Llama-3.1-8B does not appear to have a file named config.json

Where can I get config.json?
I've installed the latest transformers module, and I understand that I can access the remote model on HuggingFace. But I'd rather use my local model. Is this possible?
","The issue isn't on your end. The confusion arises from Meta not clearly distinguishing between the distributions via Hugging Face and download.sh.
To resolve this, you can download the model files using the Hugging Face CLI:
!huggingface-cli download meta-llama/Meta-Llama-3-8B-Instruct --local-dir meta-llama/Meta-Llama-3-8B-Instruct

This method will provide you with the config.json and tokenizer.json files.
Additionally, you can try downloading other versions manually. For instance, someone shared a link to the configuration file on Hugging Face:
llama-3-8b/config.json
",llama
getting peft version error while autotrain finetune on llama 2,"i did some Llama 2 finetuning with autotrain, on google colab. this is a sample text column, for fine tuning
###Human:
Here is the OCR Text extracted from a VHS tape cover. Yes, the text is surely extracted from a VHS tape, but it may have some typos and wrong characters
hi fi EMBASSY HOME ENTERTAINMENT PSYCHIC KILLER A WHOLE NEW KIND OF MURDER Mardi Rustam presents PSYCHIC KILLER Starring PAUL BURKE JIM HUTTON JULIE ADAMS NEHEMIAH PERSOFF NEVILLE BRAND Co starring ALDO RAY WHIT BISSELL ROD CAMERON and DELLA REESE Executive producer MOHAMMED RUSTAM Written by GREYDON CLARK MIKE ANGEL RAYMOND DANTON Produced by MARDI RUSTAM Directed by RAYMOND DANTON An E Embassy Pictures Release
And here is the possibly matched database entry
title Psychic killer, distributor Embassy Home Entertainment, vhs release year 1975, crew Jim Hutton Paul Burke Neville Brand, synopsis A man wrongly accused of murder and locked in a mental institution seeks revenge through his newly learned psychic gifts His deadly revenge is terrifying Jim Hutton stars

Instructions for Matching VHS Tape Details to Database Entry
~*~SURELY~*~  Confirm if both the title and VHS home video distributor from the OCR text match the database entry.
~*~POSSIBLE~*~  Indicate if only the title matches, and you're uncertain about the distributor.
~*~IMPOSSIBLE~*~  Indicate if neither the title nor the distributor matches, or if the tape and distributor are clearly not the same.
Guidelines
Focus on the title of the movie, VHS home video distributor (not theatrical distributor), VHS release year (not theatrical release year), and cast and crew members.
Consider that the synopsis was manually added to the database and may have discrepancies, and some missing information such the crew.
Explain your reasoning briefly, optimized for ML data training enrichment, but avoid rewriting details of all possible matches.

###Assistant:
- ~*~SURELY~*~ The title of the movie ""Psychic Killer"" match both in OCR extracted text and database entry. The distributor ""Embassy Home Entertainment"" also matches in both the sources. Although the release year is not mentioned in the VHS tape, it matches the common names of the cast members such as Jim Hutton and Paul Burke. Therefore, it is a confirmed match.

this is how i do autotrain
!autotrain llm --train --project_name ""llm-vhsfinetune-200s"" \
--model abhishek/llama-2-7b-hf-small-shards \
--data_path . \
--text_column text \
--use_peft \
--use_int4 \
--learning_rate 2e-4 \
--train_batch_size 2 \
--num_train_epochs 1 \
--model_max_length 1024 \
--trainer sft \
--push_to_hub \
--repo_id ""soajan/llm2-vhsfinetune-200s"" \
--block_size 1024 > training.log

after training is done, trying to load & test the model from huggingface:
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig

model_id = ""soajan/llm2-vhsfinetune-200s""
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type=""nf4"",
    bnb_4bit_compute_dtype=torch.bfloat16
)

tokenizer = AutoTokenizer.from_pretrained(model_id)
model = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=bnb_config, device_map={"""":0})

but getting the below error message, stating
ValueError: The version of PEFT you are using is not compatible, 
please use a version that is greater than 0.4.0

Full output & error trace:
Downloading (…)okenizer_config.json: 100%
705/705 [00:00<00:00, 52.3kB/s]
Downloading tokenizer.model: 100%
500k/500k [00:00<00:00, 507kB/s]
Downloading (…)/main/tokenizer.json: 100%
1.84M/1.84M [00:00<00:00, 3.73MB/s]
Downloading (…)in/added_tokens.json: 100%
21.0/21.0 [00:00<00:00, 1.21kB/s]
Downloading (…)cial_tokens_map.json: 100%
435/435 [00:00<00:00, 33.4kB/s]
Downloading (…)/adapter_config.json: 100%
458/458 [00:00<00:00, 35.5kB/s]
Loading checkpoint shards: 100%
10/10 [02:20<00:00, 10.89s/it]
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
<ipython-input-69-1fbd90a0393c> in <cell line: 13>()
     11 
     12 tokenizer = AutoTokenizer.from_pretrained(model_id)
---> 13 model = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=bnb_config, device_map={"""":0})

3 frames
/usr/local/lib/python3.10/dist-packages/transformers/models/auto/auto_factory.py in from_pretrained(cls, pretrained_model_name_or_path, *model_args, **kwargs)
    533         elif type(config) in cls._model_mapping.keys():
    534             model_class = _get_model_class(config, cls._model_mapping)
--> 535             return model_class.from_pretrained(
    536                 pretrained_model_name_or_path, *model_args, config=config, **hub_kwargs, **kwargs
    537             )

/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py in from_pretrained(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)
   3223 
   3224         if has_adapter_config:
-> 3225             model.load_adapter(
   3226                 adapter_model_id,
   3227                 adapter_name=adapter_name,

/usr/local/lib/python3.10/dist-packages/transformers/lib_integrations/peft/peft_mixin.py in load_adapter(self, peft_model_id, adapter_name, revision, token, device_map, max_memory, offload_folder, offload_index)
    114                 `offload_index` argument to be passed to `accelerate.dispatch_model` method.
    115         """"""
--> 116         check_peft_version(min_version=""0.4.0"")
    117 
    118         adapter_name = adapter_name if adapter_name is not None else ""default""

/usr/local/lib/python3.10/dist-packages/transformers/utils/peft_utils.py in check_peft_version(min_version)
     93 
     94     if not is_peft_version_compatible:
---> 95         raise ValueError(
     96             f""The version of PEFT you are using is not compatible, please use a version that is greater""
     97             f"" than {min_version}""

ValueError: The version of PEFT you are using is not compatible, please use a version that is greater than 0.4.0

and i'm checking peft version, it is 0.5.0.dev0. Why may this be happening?
","In  Tranasformers.utils.peft_utils.py ,
 is_peft_version_compatible = version.parse(importlib.metadata.version(""peft"")) <= version.parse(min_version)

The <= should be >=.
enter image description here
",llama
error saying quotattributeerror 39document39 object has no attribute 39get_doc_id39quot,"I'm working on using Predibase and LlamaIndex to set up all the moving parts of an RAG system, with Predibase as the LLM provider and currently I'm trying to create the index so that any query I make will pull the relevant context from my Pinecone Vector Store.
The following Python code is an example of achieving this objective.
index = VectorStoreIndex.from_documents(documents, storage_context=pinecone_storage_context)

During this specific process, in this case, the type of the variable 'documents' that I'm passing over is 'list' but this makes an error saying ""AttributeError: 'Document' object has no attribute 'get_doc_id'"".
So is there any way to solve this issue?
This is the resource that I'm currently following right now.
Colab Python code
# Extract Filings Function
def get_filings(ticker):
    global sec_api_key

    # Finding Recent Filings with QueryAPI
    queryApi = QueryApi(api_key=sec_api_key)
    query = {
      ""query"": f""ticker:{ticker} AND formType:\""10-K\"""",
      ""from"": ""0"",
      ""size"": ""1"",
      ""sort"": [{ ""filedAt"": { ""order"": ""desc"" } }]
    }
    filings = queryApi.get_filings(query)

    # Getting 10-K URL
    filing_url = filings[""filings""][0][""linkToFilingDetails""]

    # Extracting Text with ExtractorAPI
    extractorApi = ExtractorApi(api_key=sec_api_key)
    onea_text = extractorApi.get_section(filing_url, ""1A"", ""text"") # Section 1A - Risk Factors
    seven_text = extractorApi.get_section(filing_url, ""7"", ""text"") # Section 7 - Management’s Discussion and Analysis of Financial Condition and Results of Operations

    # Joining Texts
    combined_text = onea_text + ""\n\n"" + seven_text

    return combined_text

# construct vector store and custom storage context
pinecone_index = pc.Index(""predibase-demo-hf"")
pincone_vector_store = PineconeVectorStore(pinecone_index=pinecone_index)
pinecone_storage_context = StorageContext.from_defaults(vector_store=pincone_vector_store)


# Prompt the user to input the stock ticker they want to analyze
ticker = input(""What Ticker Would you Like to Analyze? ex. AAPL: "")

print(""-----"")
print(""Getting Filing Data"")
# Retrieve the filing data for the specified ticker
filing_data = get_filings(ticker)

print(""-----"")
print(""Initializing Vector Database"")
# Initialize a text splitter to divide the filing data into chunks
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size = 1000,         # Maximum size of each chunk
    chunk_overlap = 500,       # Number of characters to overlap between chunks
    length_function = len,     # Function to determine the length of the chunks
    is_separator_regex = False # Whether the separator is a regex pattern
)
# Split the filing data into smaller, manageable chunks
split_data = text_splitter.create_documents([filing_data])


# Load in the documents you want to index
documents = split_data

index = VectorStoreIndex.from_documents(documents, storage_context=pinecone_storage_context)

Error

","Solution
from llama_index.core import Document, VectorStoreIndex

# Split the filing data into smaller, manageable chunks
split_data = str(text_splitter.create_documents([filing_data]))


# Here we create the index so that any query you make will pull the relevant context from your Vector Store.

# text_list = [text1, text2, ...]
documents = [Document(text=t) for t in split_data]

# build index
index = VectorStoreIndex.from_documents(documents, storage_context=pinecone_storage_context)

The problem was VectorStoreIndex wanted me to pass certain types that can be processed.
I was working on finding any way to make the split_data into a certain format that VectorStoreIndex can utilize and in the end, I've just wrapped the  text_splitter.create_documents([filing_data]) with the str() function and created documents manually with adding the following code.
documents = [Document(text=t) for t in split_data]
Hope this helps somebody facing a similar issue like me.
",llama
ollama not saving anything in context,"I am messing around with ollama in C# coding, and managed to get it to give me output, and even interacting with my code, however, Using the example they provided, I am running into an issue where the LLM is not retaining the previous conversations, and i ended up with the sloppy method of injecting the instructions alongside my prompts.
Here is my Code:
public async void Prompt(string prompt)
{
    output = """";
    string input = instructions + prompt;
    context = await ollama.StreamCompletion(input, context, stream => output += (stream.Response));
    Console.WriteLine(output);
}

Output is a string, I might change this to something that can save more variables later.
Context is a ConversationContext variable.
The idea is to give it a prompt that it should follow the whole conversation through, however currently, that needs to be injected through instructions being added Before my prompt. This is not Ideal, as it defeats the purpose of it being an LLM.
What i want to do is:
Inject instruction on launch only.
Call prompts as normal.
Have it follow through with the instruction when needed.
I tried context += But thats not a valid method. I have looked for a solution to this, but I only found one other question regarding this and it was in feb.
","Thanks To Conway i was able to fix my Code. Due to the fact that I am used to programming in Unity I always used Void instead of Task cause I didn't know about it, so to fix what I had, I did:
`
public async Task Prompt(string prompt)
{
    output = """";
    string input = prompt;
    context = await ollama.StreamCompletion(input, context, stream => output += (stream.Response));
    Console.WriteLine(output);
    ConsoleCommands.aIResponse = true;
    ConsoleCommands.checkCommand(output);
}
`

Thanks to this, I now have an AI integrated program, that Remembers Prior prompts and conversations.
I hope that this will help someone else in the future!
",llama
how to load vectors from stored chroma db,"I'm working on creating a RAG-based LLM. The system is working correctly, i.e., the vector embeddings are successfully created and stored in the respective directory. However, the issue i'm facing is loading back those vectors from the stored chroma db file. Following is my function that handles the creation and retrieval of vectors:
def vector_embedding():
    persist_directory = ""./chroma_db""

    if os.path.exists(persist_directory):
        st.write(""Loading vectors from disk..."")
        st.session_state.vectors = Chroma(persist_directory=persist_directory, embedding_function=OllamaEmbeddings(model=""nomic-embed-text""))
        st.write(""Loaded vectors from disk."")
        return
    
    st.write(""Creating new vectors..."")
    
    st.session_state.embeddings = OllamaEmbeddings(model=""nomic-embed-text"", show_progress=True)
    
    file_path = ""data2.pdf""
    st.session_state.loader = UnstructuredPDFLoader(file_path)
    
    if not os.path.exists(file_path):
        st.write(f""File does not exist: {file_path}"")
        return

    st.session_state.docs = st.session_state.loader.load()
    
    if not st.session_state.docs:
        st.write(""No doduments loaded."")
        return
    
    st.write(f""Loaded {len(st.session_state.docs)} documents."")
    
    st.session_state.text_splitter = RecursiveCharacterTextSplitter(chunk_size=7500, chunk_overlap=200)
    st.session_state.final_documents = st.session_state.text_splitter.split_documents(st.session_state.docs[:200])

    if not st.session_state.final_documents:
        st.write(""No final documents after splitting."")
        return
    
    st.write(f""Created {len(st.session_state.final_documents)} document chunks."")
    
    for idx, doc in enumerate(st.session_state.final_documents):
        if 'id' not in doc.metadata or not doc.metadata['id']:
            doc.metadata['id'] = f""doc_{idx}""
    
    ids = [doc.metadata['id'] for doc in st.session_state.final_documents]
    
    
    st.session_state.vectors = Chroma.from_documents(
        documents=st.session_state.final_documents,
        embedding=st.session_state.embeddings,
        collection_name=""local-rag"",
        persist_directory=persist_directory
    )
    
    st.session_state.vectors.persist()
    st.write(""Vectors saved to disak."")

I have tried loading vectors by:
    if os.path.exists(persist_directory):
        st.write(""Loading vectors from disk..."")
        st.session_state.vectors = Chroma(persist_directory=persist_directory, embedding_function=OllamaEmbeddings(model=""nomic-embed-text""))
        st.write(""Loaded vectors from disk."")
        return

However, when I run the app, the model does not have any context, and therefore doesn't answer appropriately.
","Turns out I did not specified the collection name while loading the vectors, following solved my problem:
st.session_state.vectors = Chroma(collection_name=""local-rag"",persist_directory=persist_directory, embedding_function=OllamaEmbeddings(model=""nomic-embed-text""))

",llama
sample request json for vertex ai endpoint,"I've deployed the Llama 3 model using the Deploy button on the Vertex AI model garden  Llama 3 card:
https://pantheon.corp.google.com/vertex-ai/publishers/meta/model-garden/llama3

I can make a request using the ""Try out Llama 3"" side panel on that page & it seems to be working with my deployed model + endpoint. I'd like to try making a request using Curl or python next. The endpoint UI page also has a ""sample request"" feature, but it's much less helpful / very generic rather than customized.

So does anyone have an example request (for this model or another)?
Specifically for the JSON instances & parameters. Parameters I also may be able to figure out, but I have absolutely no idea what an instance is in this context?
This seems like the closest related question: Sending http request Google Vertex AI end point
..Google Cloud loves naming something generically, not giving that many details on what it is, & then expecting something very specific as a value.
edit: Found the docs on this GCP method:
https://cloud.google.com/vertex-ai/docs/reference/rest/v1/projects.locations.endpoints/predict
which gives some description but ""The instances that are the input to the prediction call."" is not really that helpful.
","Apologies for the poor experience. For now, the best reference is the notebook.
Here's the relevant snippet:
prompt = ""What is a car?""  # @param {type: ""string""}
max_tokens = 50  # @param {type:""integer""}
temperature = 1.0  # @param {type:""number""}
top_p = 1.0  # @param {type:""number""}
top_k = 1.0  # @param {type:""number""}
raw_response = False  # @param {type:""boolean""}

# Overides parameters for inferences.
# If you encounter the issue like `ServiceUnavailable: 503 Took too long to respond when processing`,
# you can reduce the max length, such as set max_tokens as 20.
instances = [
    {
        ""prompt"": prompt,
        ""max_tokens"": max_tokens,
        ""temperature"": temperature,
        ""top_p"": top_p,
        ""top_k"": top_k,
        ""raw_response"": raw_response
    }
]

But please note that the full JSON (e.g. to send using curl) is:
{
  ""instances"": [
    {
        ""prompt"": prompt,
        ""max_tokens"": max_tokens,
        ""temperature"": temperature,
        ""top_p"": top_p,
        ""top_k"": top_k,
        ""raw_response"": raw_response
    }
  ]
}

",llama
running ollama on kaggle,"I downloaded ollama on a kaggle notebook (linux). I want to interact with it using a python script. On following the instructions on the github repo and running: ollama run llama3 I got the output: Error: could not connect to ollama app, is it running?.
It appears that I need to run ollama serve before running llama3. However the entire main thread is taken up by ollama serve so you cannot run anything else after it.
The work arounds I tried:

Trying to create a background process: ollama serve & which returned OSError: Background processes not supported.
Trying to run it via python using subprocess.run('ollama', 'serve') which returns TypeError: bufsize must be an integer

Full logs of second method:
TypeError                                 Traceback (most recent call last)
Cell In[29], line 1
----> 1 subprocess.run('ollama', 'serve')

File /opt/conda/lib/python3.10/subprocess.py:503, in run(input, capture_output, timeout, check, *popenargs, **kwargs)
    500     kwargs['stdout'] = PIPE
    501     kwargs['stderr'] = PIPE
--> 503 with Popen(*popenargs, **kwargs) as process:
    504     try:
    505         stdout, stderr = process.communicate(input, timeout=timeout)

File /opt/conda/lib/python3.10/subprocess.py:780, in Popen.__init__(self, args, bufsize, executable, stdin, stdout, stderr, preexec_fn, close_fds, shell, cwd, env, universal_newlines, startupinfo, creationflags, restore_signals, start_new_session, pass_fds, user, group, extra_groups, encoding, errors, text, umask, pipesize)
    778     bufsize = -1  # Restore default
    779 if not isinstance(bufsize, int):
--> 780     raise TypeError(""bufsize must be an integer"")
    782 if pipesize is None:
    783     pipesize = -1  # Restore default

TypeError: bufsize must be an integer

I chose ollama because the setup was simple (just running a single command). I am ok with using another method besides ollama, however I want to run it on python without much setup as it is running on kaggle
","This is what I did to get it to work:
#Download ollama
!curl -fsSL https://ollama.com/install.sh | sh
import subprocess
process = subprocess.Popen(""ollama serve"", shell=True) #runs on a different thread
#Download model
!ollama pull llama3
!pip install ollama
import ollama

#Then everytime you want to chat
response = ollama.chat(model='llama3', messages=[
  {
    'role': 'user',
    'content': 'Why is the sky blue?',
  },
])
print(response['message']['content'])

",llama
how to instantly terminate a thread using ollama python api with tkinter to stream a response from llama2,"I'm using ollama to stream a response from llama2 large language model. The functionality I need is, when I click the stop button, it should stop the thread immediately. The code below works but the problem is, it always waits for the first chunk to be available before it checks if stop is True. Sometimes the response can take some time before the first chunk is ready. It shouldn't wait for the first chunk. It should immediately stop the thread when stop button is clicked.
My code:
import tkinter as tk
import ollama
import threading

stop = False


# Create the main window
root = tk.Tk()
root.title(""Tkinter Button Example"")

def get_answer():
    print(""Start"")
    stream = ollama.chat(
        model='llama2',
        messages=[{'role': 'user', 'content': 'write a story about earth'}],
        stream=True,
    )
    for chunk in stream:
        if stop is False:
            print(chunk['message']['content'], end='', flush=True)

        else:
            print(""Stopped"")   
            return
# Define the function to be called when the button is pressed
def on_button_click():
    global stop
    stop = True
    print(""Button clicked!"")

# Create the button
button = tk.Button(root, text=""Stop"", command=on_button_click)

# Place the button on the window
button.pack(pady=20)

thread = threading.Thread(target=get_answer)
thread.start()

# Start the Tkinter event loop
root.mainloop()

","You cannot instantly terminate a thread in python.
This ollama API currently offers an async client, you can use the async client and cancel the Task, this should close the async connection almost instantly.
import tkinter as tk
import ollama
import threading
import asyncio
from typing import Optional

# Create the main window
root = tk.Tk()
root.title(""Tkinter Button Example"")

client = ollama.AsyncClient()
async def get_answer():
    print(""Start"")
    stream = await client.chat(
        model='llama2',
        messages=[{'role': 'user', 'content': 'write a story about earth'}],
        stream=True,
    )
    async for chunk in stream:
        print(chunk['message']['content'], end='', flush=True)


worker_loop: Optional[asyncio.AbstractEventLoop] = None
task_future: Optional[asyncio.Future] = None
def worker_function():
    global worker_loop, task_future
    worker_loop = asyncio.new_event_loop()
    task_future = worker_loop.create_task(get_answer())
    worker_loop.run_until_complete(task_future)

# Define the function to be called when the button is pressed
def on_button_click():
    # the loop and the future are not threadsafe
    worker_loop.call_soon_threadsafe(
        lambda: task_future.cancel()
    )
    print(""Button clicked!"")

# Create the button
button = tk.Button(root, text=""Stop"", command=on_button_click)

# Place the button on the window
button.pack(pady=20)

thread = threading.Thread(target=worker_function)
thread.start()

# Start the Tkinter event loop
root.mainloop()

this does raise an asyncio.CancelledError in the worker thread, so you may want to catch it if you don't want errors on your screen, and you may want to wrap the whole thing in a class rather than relying on globals.
",llama
error while installing python package llamacpppython,"I am using Llama to create an application. Previously I used openai but am looking for a free alternative. Based on my limited research, this library provides openai-like api access making it quite easy to add into my prexisting code. However this library has errors while downloading. I tried installing cmake which did not help.
Building wheels for collected packages: llama-cpp-python
  Building wheel for llama-cpp-python (pyproject.toml) ... error
  error: subprocess-exited-with-error

  × Building wheel for llama-cpp-python (pyproject.toml) did not run successfully.
  │ exit code: 1
  ╰─> [20 lines of output]
      *** scikit-build-core 0.5.1 using CMake 3.27.7 (wheel)
      *** Configuring CMake...
      2023-10-10 21:23:02,749 - scikit_build_core - WARNING - Can't find a Python library, got libdir=None, ldlibrary=None, multiarch=None, masd=None
      loading initial cache file C:\Users\ARUSHM~1\AppData\Local\Temp\tmpf1bzj6ul\build\CMakeInit.txt
      -- Building for: NMake Makefiles
      CMake Error at CMakeLists.txt:3 (project):
        Running

         'nmake' '-?'

        failed with:

         The system cannot find the file specified


      CMake Error: CMAKE_C_COMPILER not set, after EnableLanguage
      CMake Error: CMAKE_CXX_COMPILER not set, after EnableLanguage
      -- Configuring incomplete, errors occurred!

      *** CMake configuration failed
      [end of output]

  note: This error originates from a subprocess, and is likely not a problem with pip.
  ERROR: Failed building wheel for llama-cpp-python
Failed to build llama-cpp-python
ERROR: Could not build wheels for llama-cpp-python, which is required to install pyproject.toml-based projects

Although not directly related to this question, these are other questions I am unable to get answers for:

Does this library use Llama or Llama 2?
Will this be secure on a Python Flask Application?

","You need to install the desktop c++ block with visual studio to get cmake properly installed.Open the Visual Studio Installer and click Modify, then check Desktop development with C++ and click Modify to start the install.  I also recommend the Windows 10 SDK.
https://learn.microsoft.com/en-us/cpp/build/cmake-projects-in-visual-studio?view=msvc-170
https://developer.microsoft.com/en-us/windows/downloads/windows-sdk/
After that, !pip install llama-cpp-python should build just fine.
",llama
error installing metallama370b model from hugging face hub,"I'm trying to load the Meta-Llama-3-70B model from the Hugging Face Hub using the Transformers library in Python, but I'm encountering the following error:
OSError: We couldn't connect to 'https://huggingface.co' to load this file, couldn't find it in the cached files and it looks like meta-llama/Meta-Llama-3-70B is not the path to a directory containing a file named config.json.  Checkout your internet connection or see how to run the library in offline mode at 'https://huggingface.co/docs/transformers/installation#offline-mode'.

Here's the code I'm using:
import torch
import transformers

model_id = ""meta-llama/Meta-Llama-3-70B""
pipeline = transformers.pipeline(
    ""text-generation"", model=model_id, model_kwargs={""torch_dtype"": torch.bfloat16}, device_map=""auto""
)
pipeline(""Hey how are you doing today?"")

I've granted access to the Meta-Llama-3-70B model on the Hugging Face website, but I'm still encountering this error. I've checked my internet connection, and it seems to be working fine.
Can someone help me understand what might be causing this issue and how to resolve it? Are there any additional steps I need to take to successfully load and use the Meta-Llama-3-70B model from the Hugging Face Hub?
","In case you are facing the same problem even after getting the permission for the gated model, then follow these:
First, get the Hugging Face access token from here
then run this code:
from huggingface_hub import login

login(token='xxxxxxxxxxxxxxxxxxxxxxx')

Replace those x's with your access token
And then run the model
",llama
deploying llm on sagemaker endpoint  cuda out of memory,"I am trying to deploy huggingface LLM (for inference) to Sagemaker Endpoint using custom scripts (Using Pytorch framework with model and inference script zipped as .tar.gz file).  The tar.gz file structure is:
model.tar.gz/
|- pytorch_model.bin
|- ....
|- code/
  |- inference.py
  |- requirements.txt 

In inference.py, I have defined functions model_fn and predict_fn.
This tar.gz file is uploaded to S3 and the model while deployment is being picked from this S3 location.
I have followed the process defined in https://huggingface.co/docs/sagemaker/en/inference --> Sections: Create a model artifact for deployment and User defined code and modules
After following all these steps, I am getting an error :

CUDA out of memory. Tried to allocate 20.00 MiB. GPU 1 has a total
capacty of 22.20 GiB of which 13.12 MiB is free. Process 13234 has
2.25 GiB memory in use. Process 13238 has 3.82 GiB memory in use. Process 13236 has 8.06 GiB memory in use. Process 13239 has 8.06 GiB
memory in use. Of the allocated memory 6.93 GiB is allocated by
PyTorch, and 49.59 MiB is reserved by PyTorch but unallocated. If
reserved but unallocated memory is large try setting max_split_size_mb
to avoid fragmentation.  See documentation for Memory Management and
PYTORCH_CUDA_ALLOC_CONF : 400

My model is an LLM with 7b parameters and compute is ml.g5.12x (192 GB  and GPU 24 GB x 4). The memory is more than sufficient (as I was getting this error, I tried such a large compute) and the code I have tried is using AutoModelForCausalLM.from_pretrained and Autotokenizer.from_pretrained.  I have tried device maps of ""auto"", balanced_low_0, and balanced. The memory on GPU is sufficient to start with (as checked by me from memory summary)
The thing is I was able to get a response for a couple of pings and then I started getting this error. I am clearing the cache in my predict function but still I am getting this error.
How can I resolve my out-of-memory error? I get out of memory error either right at the start or my memory of GPU fills incrementally with each inference.
","I had tried a lot of different things as mentioned on various locations on web but none worked for me. The error was due to inefficient GPU memory allocation strategy for LLM (device_map=""auto"" was not working well for me) and some variables that were getting stored on GPU. I am mentioning variables because out-of-memory was appearing within first four inferences (which meant that empty memory available on GPU was very less to start with and the reason why I mentioned that the strategy of GPU memory allocation was not working for me).
Before I elaborate on my answer, I will list the various things I found at various locations and what finally worked for me (and I believe, most of the users who had faced this issue will most likely benefit from what I tried rather than the things mentioned below, even during training)

Update PYTORCH_CUDA_ALLOC_CONF max_split_size_mb. But this may be the least helpful option.
os.environ[""PYTORCH_CUDA_ALLOC_CONF""] = ""max_split_size_mb:enter-size-here""

A user at a different forum had mentioned that one needs to install following packages: transformers==4.28.1, sentencepiece==0.1.97, accelerate==0.18.0, bitsandbytes==0.37.2 and torch 1.13.1 . But I believe out of memory issues mostly need to be handled by memory management. Package issues might be temporary with new releases but are resolved with certaininity.

Related to Training ONLY - While training for vision models, the images might not fit in GPU solely and so you should adjust it’s size as well as release them from GPU memory.

Related to Training ONLY - Reduce training batch sizes to as small as 1

Garbage collection gc.collect()

Empty cache torch.cuda.empty_cache()

Increase system RAM/larger compute instance


What really helped me was distributing the LLM across GPU’s by defining max_memory of GPU that can be used for storing model. This meant that my GPU was not fully booked by the LLM. It is a three step process:

Load model with no weights/empty model on GPU.  While inference set
no_grad to avoid any calculations weight updates even though no
weights will be updated. Also, Set device map to fix the max memory
loaded model weights can take.

Load model weights on CPU

The weights for each layer are loaded to GPU, execution/calculation
is done and then, weights are removed from GPU.



    with torch.no_grad():
    
            with init_empty_weights():
                old_prediction_model = AutoModelForCausalLM.from_pretrained(
                    model_dir,
                    torch_dtype=torch.bfloat16,
                    quantization_config=quantization_config
                    )
            model = load_checkpoint_and_dispatch(
            old_prediction_model, offload_folder=""/offload_folder_name_or_location"",
                checkpoint=model_dir, device_map=infer_auto_device_map(old_prediction_model, max_memory={0: ""10GiB""}),
                dtype=torch.bfloat16
        )

NOTE: Along with all this, another big cause of out of memory errors is leaving your variables on GPU i.e. since, execution is happening on GPU and during that course you try to create a list of model inferences or evaluations, the GPU memory will continue to fill up as your new inferences are made. To avoid it, with each inference take your variables off the GPU and to cpu memory
",llama
how to load a quantized finetuned llama 38b model in vllm for faster inference,"I am working on deploying a quantized fine-tuned LLaMA 3-8B model and I aim to use vLLM  to achieve faster inference. I am currently using the following Python code to load the model:
import torch
import transformers
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig
import bitsandbytes as bnb
import accelerate

# model_id = ""meta-llama/Meta-Llama-3-8B"" #""mistralai/Mistral-7B-Instruct-v0.1""
model_id = ""meta-llama/Meta-Llama-3-8B-Instruct""
quantization_config = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_compute_dtype=torch.bfloat16)

base_model = AutoModelForCausalLM.from_pretrained(
    model_id,
    trust_remote_code=True,
    quantization_config=quantization_config, #load_in_8bit=True,#
    device_map='auto',
    token=MYTOKEN
)
peft_model = ""BojanaBas/Meta-Llama-3-8B-Instruct-pqa-10""
model = PeftModel.from_pretrained(base_model, peft_model)

The code successfully loads the model, but I am not sure how to integrate this with vLLM to optimize for faster inference. I read that it is not possible to load a model using PEFT in vLLM; instead, the PEFT model needs to be merged and loaded on Hugging Face.
I have merged and loaded the model on Hugging Face as described in the
article, after that, I am trying to use the model pushed to Hugging Face to load it on vLLM  with the following code:

from vllm import LLM

merged_peft_model_name=""lcass00/Meta-Llama-3-8B-Instruct-pqa-10-merged-peft""
model_id = ""meta-llama/Meta-Llama-3-8B-Instruct""

llm = LLM(model=merged_peft_model_name, tokenizer=model_id)

but when I try to load the model on vLLM I get the following error:
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
<ipython-input-14-c306a36d9c21> in <cell line: 3>()
      1 from vllm import LLM
      2 
----> 3 llm = LLM(model=merged_peft_model_name, tokenizer=model_id)

4 frames
/usr/local/lib/python3.10/dist-packages/vllm/entrypoints/llm.py in __init__(self, model, tokenizer, tokenizer_mode, skip_tokenizer_init, trust_remote_code, tensor_parallel_size, dtype, quantization, revision, tokenizer_revision, seed, gpu_memory_utilization, swap_space, enforce_eager, max_context_len_to_capture, max_seq_len_to_capture, disable_custom_all_reduce, **kwargs)
    142             **kwargs,
    143         )
--> 144         self.llm_engine = LLMEngine.from_engine_args(
    145             engine_args, usage_context=UsageContext.LLM_CLASS)
    146         self.request_counter = Counter()

/usr/local/lib/python3.10/dist-packages/vllm/engine/llm_engine.py in from_engine_args(cls, engine_args, usage_context)
    333         """"""Creates an LLM engine from the engine arguments.""""""
    334         # Create the engine configs.
--> 335         engine_config = engine_args.create_engine_config()
    336         distributed_executor_backend = (
    337             engine_config.parallel_config.distributed_executor_backend)

/usr/local/lib/python3.10/dist-packages/vllm/engine/arg_utils.py in create_engine_config(self)
    557     def create_engine_config(self, ) -> EngineConfig:
    558         device_config = DeviceConfig(self.device)
--> 559         model_config = ModelConfig(
    560             self.model, self.tokenizer, self.tokenizer_mode,
    561             self.trust_remote_code, self.dtype, self.seed, self.revision,

/usr/local/lib/python3.10/dist-packages/vllm/config.py in __init__(self, model, tokenizer, tokenizer_mode, trust_remote_code, dtype, seed, revision, code_revision, rope_scaling, tokenizer_revision, max_model_len, quantization, quantization_param_path, enforce_eager, max_context_len_to_capture, max_seq_len_to_capture, max_logprobs, disable_sliding_window, skip_tokenizer_init, served_model_name)
    141             self._verify_tokenizer_mode()
    142         self._verify_embedding_mode()
--> 143         self._verify_quantization()
    144         self._verify_cuda_graph()
    145 

/usr/local/lib/python3.10/dist-packages/vllm/config.py in _verify_quantization(self)
    201         if self.quantization is not None:
    202             if self.quantization not in supported_quantization:
--> 203                 raise ValueError(
    204                     f""Unknown quantization method: {self.quantization}. Must ""
    205                     f""be one of {supported_quantization}."")

ValueError: Unknown quantization method: bitsandbytes. Must be one of ['aqlm', 'awq', 'deepspeedfp', 'fp8', 'marlin', 'gptq_marlin_24', 'gptq_marlin', 'gptq', 'squeezellm', 'sparseml'].

How can I load a quantized finetuned model on vLLM?
","Unfortunately vLLM does not support bitsandbytes quantization technique yet. You may want to use Mixtral-8x7B-Instruct-v0.1-GPTQ tough, as GPTQ and  AWQ quantization techniques are already supported.
",llama
could not find orgspringframeworkai,"I'm trying to integrate Llama3 in my spring application by using the following documentation:
https://docs.spring.io/spring-ai/reference/1.0-SNAPSHOT/api/chat/ollama-chat.html
When building the application with the added dependency:
 implementation 'org.springframework.ai:spring-ai-ollama-spring-boot-starter'

I got the following error:
Execution failed for task ':compileJava'.
> Could not resolve all files for configuration ':compileClasspath'.
   > Could not find org.springframework.ai:spring-ai-ollama-spring-boot-starter:.

This github issue https://github.com/spring-projects/spring-ai/issues/194 also doesn't help. After changing the path based on a comment there to:
    implementation 'org.springframework.experimental.ai:spring-ai-ollama-spring-boot-starter'

I got the following error:
Execution failed for task ':compileJava'.
> Could not resolve all files for configuration ':compileClasspath'.
   > Could not find org.springframework.experimental.ai:spring-ai-ollama-spring-boot-starter:

","If you go to the maven repository of the mentioned dependency, you will see that the dependency is relocated. To solve this problem, simply change the dependency location to the correct one:
implementation 'io.springboot.ai:spring-ai-ollama-spring-boot-starter:1.0.3'

It is also worth considering to include Spring AI Bill of Materials (BOM) as noted by @M.Deinum's comment to avoids the need for you to specify and maintain the dependency versions yourself:
    implementation platform(""io.springboot.ai:spring-ai-bom:1.0.3"")
    // Replace the following with the starter dependencies of specific modules you wish to use
    implementation 'io.springboot.ai:spring-ai-ollama-spring-boot-starter'

",llama
typeerror in python 311 when using basicmodelrunner from llamacpppython,"I'm currently taking the DeepAI's Finetuning Coursera course and encountered a bug while trying to run one of their demonstrations locally in a Jupyter notebook.
Environment:

Python version: 3.11
Required packages:

notebook
lamini
llama-cpp-python (> 0.1.53)



Issue:
When attempting to run the following code:
import os
import lamini

lamini.api_url = os.getenv(""POWERML__PRODUCTION__URL"")
lamini.api_key = os.getenv(""POWERML__PRODUCTION__KEY"")

from llama import BasicModelRunner

non_finetuned = BasicModelRunner(""meta-llama/Llama-2-7b-hf"")

non_finetuned_output = non_finetuned(""Tell me how to train my dog to sit"")

I receive a TypeError at the last line:
TypeError: can only concatenate str (not ""NoneType"") to str

This error occurs when I try to pass a string to the non_finetuned BasicModelRunner object.
","I had the same problem. You have to create an account in lamini then you can create a key copy/past to POWERML__PRODUCTION__KEY
for POWERML__PRODUCTION__URL let it as empty string """"
",llama
llama3 response incompleteness,"I am getting incomplete answer from meta-llama/Meta-Llama-3-8B-Instruct what can i do to improve the response completeness
import requests
import os
from dotenv import load_dotenv
import streamlit as st
import PyPDF2 as pdf
import warnings
warnings.filterwarnings('ignore')

load_dotenv()
key = os.getenv('HF_TOKEN')

def input_pdf_text(uploaded_file):
    reader=pdf.PdfReader(uploaded_file)
    text=""""
    for page in range(len(reader.pages)):
        page=reader.pages[page]
        text+=str(page.extract_text())
    return text

input_prompt = """"""You are an skilled ATS (Applicant Tracking System) scanner with a deep understanding of Human Resources and ATS functionality, your task is to evaluate the resume against the provided job description. give me the percentage of match if the resume matches
the job description. First the output should come as percentage and then keywords missing and last final thoughts."""""" 

if uploaded_file is not None:
    pdf_content = input_pdf_text(uploaded_file)

if uploaded_file is not None:
    st.write(""PDF Uploaded Successfully"") 

submit3 = st.button(""Percentage match"")

API_URL = ""https://api-inference.huggingface.co/models/meta-llama/Meta-Llama-3-8B-Instruct""
headers = {""Authorization"": f""Bearer {key}""}

def query(payload):
    response = requests.post(API_URL, headers=headers, json=payload) 
    return response.json()

if submit3:
    if uploaded_file is not None:
        input_data = f""""""
            input prompt: {input_prompt}
            job description: {input_text}
            resume: {pdf_content}
        """"""
        response1 = query({
            ""inputs"": input_data
        })
        data = response1[0][""generated_text""]
        st.subheader(""Analysis"")
        st.write(data)        


I need to improve the completeness and accuracy of responses generated by the Llama 3 model
","Add this to your query:
max_new_tokens = 4096
And, use this prompt technique.
",llama
vectorstoreindex api key while using azureopenai service,"I'm trying to use VectorStoreIndex from llama_index solving the RAG problem for chatbot just the following way:
import openai
from llama_index import VectorStoreIndex

index = VectorStoreIndex.from_documents(docs)
index.storage_context.persist()

When I read documentation, it is recommended to use openai.api_key = os.getenv('OPENAI_API_KEY') to be able to connect model with OpenAI. But what should I do if I use AzureOpenAI (meaning, that I have api_key, azure_endpoint and api_version and don't have openai api_key)? If I use openai.api_key = my_azure_api_key this also does not work because apparently the model refers to https://platform.openai.com/account/api-keys and not to AzureAI Services..
Sorry if the question looks dubious, was unable to find the information on web.
Thanks!
","Have solved it. Actually all you need is to add context parameter into VectorStoreIndex.from_documents(). In this parameter one can specify AzureOpenAI or AzureOpenAIEmbedding and obviously use api_key, azure_endpoint and api_version.
index = VectorStoreIndex.from_documents(documents, service_context=service_context)

",llama
metallamallama27bhf returning tensor instead of modeloutput,"I am using the Llama-2-7b-hf model with the model.generate function from the transformers library (v4.38.2) and it's returning the outputs as a single tensor, instead of the ModelOutput I expected.
I have a copy of the model stored locally:
[Llama-2-7b-hf]$ ls -1
config.json
generation_config.json
LICENSE.txt
model-00001-of-00002.safetensors
model-00002-of-00002.safetensors
model.safetensors.index.json
README.md
Responsible-Use-Guide.pdf
special_tokens_map.json
tokenizer_config.json
tokenizer.json
tokenizer.model
USE_POLICY.md

This is the code where the model is initialized and then called:
model_path = ""Llama-2-7b-hf""
model = AutoModelForCausalLM.from_pretrained(model_path, return_dict_in_generate=True, local_files_only=True).to(device)
tokenizer = AutoTokenizer.from_pretrained(engine, local_files_only=True)

input_ids = tokenizer(model_path, return_tensors=""pt"").input_ids.to(device)
outputs = model.generate(input_ids, top_k=1, max_length=max_len, num_return_sequences=1, output_scores=True)
sequences, scores = outputs.sequences, outputs.scores

I have used this code with several other models like Mistral and Occiglot and they return ModelOutput objects with these attributes, sequences and scores, but not Llama. Can anyone help me understand what is wrong?
","I managed to solve it by passing the return_dict_in_generate and output_scores parameters in the call to model.generate instead of in the initialization of the model.
model = AutoModelForCausalLM.from_pretrained(engine, local_files_only=True).to(device)
outputs = model.generate(input_ids, top_k=1, max_length=max_len, num_return_sequences=1, output_scores=True, return_dict_in_generate=True)

",llama
how to make sense of the output of the reward model how do we know what string it is preferring,"In the process of doing RLHF I made a reward model using a dataset of chosen and rejected string pairs. It is very similar to the example that's there in the official TRL library - Reward Modeling
I used LLaMA 2 7b model (tried both the chat and non-chat versions - the behavior is the same).
Now what I would like to do is to actually pass an input and see the output of the Reward model. However I can’t seem to make any sense of what the reward model outputs.
For example: I tried to make the input as follows -
chosen = ""This is the chosen text.""
rejected = ""This is the rejected text.""
test = {""chosen"": chosen, ""rejected"": rejected}

Then I try -
import torch
import torch.nn as nn

from transformers import AutoModelForSequenceClassification, AutoTokenizer, AutoModelForCausalLM
base_model_id = ""./llama2models/Llama-2-7b-chat-hf""
model_id = ""./reward_models/Llama-2-7b-chat-hf_rm_inference/checkpoint-500""

model = AutoModelForSequenceClassification.from_pretrained(
    model_id,
    # num_labels=1, #gives an error since the model always outputs a tensor of [2, 4096]
)

tokenizer = AutoTokenizer.from_pretrained(base_model_id)
rewards_chosen = model(
            **tokenizer(chosen, return_tensors='pt')
        ).logits
print('reward chosen is ', rewards_chosen)

rewards_rejected = model(
           **tokenizer(rejected, return_tensors='pt')
        ).logits

print('reward rejected is ', rewards_rejected)
loss = -nn.functional.logsigmoid(rewards_chosen - rewards_rejected).mean()
print(loss)

And the output looks something like this -
reward chosen is  tensor([[ 2.1758, -8.8359]], dtype=torch.float16)
reward rejected is  tensor([[ 1.0908, -2.2168]], dtype=torch.float16)
tensor(0.0044)

Printing loss wasn’t helpful. I mean I do not see any trend (for example positive loss turning negative) even if I switch rewards_chosen and rewards_rejected in the formula.
Also the outputs did not yield any insights. I do not understand how to make sense of rewards_chosen and rewards_rejected. Why are they a tensor with two elements instead of one?
I tried rewards_chosen > rewards_rejected but that is also not helpful since it outputs tensor([[ True, False]])
When I try some public reward model (its just a few megabytes since its just the adapter - https://huggingface.co/vincentmin/llama-2-13b-reward-oasst1) then I get outputs that make more sense since its outputs a single element tensor -
Code -
import torch
import torch.nn as nn

from peft import PeftModel, PeftConfig
from transformers import AutoModelForSequenceClassification, AutoTokenizer, AutoModelForCausalLM

peft_model_id = ""./llama-2-13b-reward-oasst1""
base_model_id = ""/cluster/work/lawecon/Work/raj/llama2models/13b-chat-hf""
config = PeftConfig.from_pretrained(peft_model_id)
model = AutoModelForSequenceClassification.from_pretrained(
    base_model_id,
    num_labels=1,
    # torch_dtype=torch.float16,
)
model = PeftModel.from_pretrained(model, peft_model_id)
tokenizer = AutoTokenizer.from_pretrained(base_model_id)
chosen = ""prompter: What is your purpose? assistant: My purpose is to assist you.""
rejected = ""prompter: What is your purpose? assistant: I do not understand you.""
test = {""chosen"": chosen, ""rejected"": rejected}

model.eval()
with torch.no_grad():
    rewards_chosen = model(
                **tokenizer(chosen, return_tensors='pt')
            ).logits
    print('reward chosen is ', rewards_chosen)

    rewards_rejected = model(
               **tokenizer(rejected, return_tensors='pt')
            ).logits

    print('reward rejected is ', rewards_rejected)
    loss = -nn.functional.logsigmoid(rewards_chosen - rewards_rejected).mean()
    print(loss)

Output -
reward chosen is  tensor([[0.6876]])
reward rejected is  tensor([[-0.9243]])
tensor(0.1819)

This output makes more sense to me. But why do I get the outputs that have two values with my reward model?
","I've been facing the exact same issue myself, after following the same example on the TRL library! I think there's a mistake in that example; reward models should output single-element tensors as you suggest, rather than two-element tensors.
I believe that setting num_labels=1 when calling AutoModelForSequenceClassification.from_pretrained is the solution here. This instantiates a model with a single-element output.
I can see that you've commented this out in your example, saying that it ""gives an error since the model always outputs a tensor of [2, 4096]"". I get no such error, so I'm not sure what's going on for you there.
",llama
unknown document type error while using llamaindex with azure openai,"I'm trying to reproduce the code from documentation: https://docs.llamaindex.ai/en/stable/examples/customization/llms/AzureOpenAI.html and receive the following error after index = VectorStoreIndex.from_documents(documents):
raise ValueError(f""Unknown document type: {type(document)}"")
ValueError: Unknown document type: <class 'llama_index.legacy.schema.Document'>

Due to the fact that all these generative ai libraries are being constantly updated, I have to switch the import of SimpleDirectoryReader and make it like from llama_index.legacy.readers.file.base import SimpleDirectoryReader
All the rest is actually the same with tutorial (using llama_index==0.10.18 and python of version 3.9.16). I have spent already several hours on that and actually don't have ideas how should I proceed. So if somebody can assist with that - it would be super helpful :)
Many thanks in advance.
","The error occurs because of the type of document you are passing for VectorStoreIndex.from_documents().
When you import SimpleDirectoryReader from legacy modules, the type of document is llama_index.legacy.schema.Document.

You are passing that to VectorStoreIndex, which is imported from core modules: from llama_index.core import VectorStoreIndex.
The document you referred to is correct for core modules, and you can import SimpleDirectoryReader as from llama_index.core import VectorStoreIndex, SimpleDirectoryReader, and everything will work fine.
If you wish to use legacy modules, then use the code below.
from llama_index.legacy.llms.azure_openai import AzureOpenAI
from llama_index.legacy.embeddings.azure_openai import AzureOpenAIEmbedding
from llama_index.legacy import SimpleDirectoryReader, VectorStoreIndex, ServiceContext
import logging
import sys

logging.basicConfig(
    stream=sys.stdout, level=logging.INFO
)  # logging.DEBUG for more verbose output
logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))

api_key = ""3c9xxxyyyyzzzzzssssssdb9""
azure_endpoint = ""https://<resource_name>.openai.azure.com/""
api_version = ""2023-07-01-preview""

llm = AzureOpenAI(
    model=""gpt-4"",
    deployment_name=""gpt4"",
    api_key=api_key,
    azure_endpoint=azure_endpoint,
    api_version=api_version,
)

# You need to deploy your own embedding model as well as your own chat completion model
embed_model = AzureOpenAIEmbedding(
    model=""text-embedding-ada-002"",
    deployment_name=""embeding1"",
    api_key=api_key,
    azure_endpoint=azure_endpoint,
    api_version=api_version,
)

documents = SimpleDirectoryReader(input_files=[""./data/s1.txt""]).load_data()
type(documents[0])

service_context = ServiceContext.from_defaults(
    llm=llm, embed_model=embed_model
)

index = VectorStoreIndex.from_documents(documents, service_context=service_context)

Output:
query = ""What is the model name and who updated it last?""
query_engine = index.as_query_engine()
answer = query_engine.query(query)
print(""query was:"", query)
print(""answer was:"", answer)


Here, when using legacy modules, all tools and models should be imported from the same legacy modules, and an additional service context is used for the vector store index.
",llama
databricks importerror cannot import name 39override39 from 39typing_extensions39,"I am trying to use a databricks notebook to finetune the Llama2 model. The code for this is here. I'm running into an error at lines 219-231:
from trl import SFTTrainer

max_seq_length = 512

trainer = SFTTrainer(
    model=model,
    train_dataset=dataset,
    peft_config=peft_config,
    dataset_text_field=""text"",
    max_seq_length=max_seq_length,
    tokenizer=tokenizer,
    args=training_arguments,
)

I am getting the error

ImportError: cannot import name 'override' from 'typing_extensions' (/databricks/python/lib/python3.10/site-packages/typing_extensions.py)

The full stack trace is below.
---------------------------------------------------------------------------
ImportError                               Traceback (most recent call last)
File <command-3349581073491723>, line 1
----> 1 from trl import SFTTrainer
      3 max_seq_length = 512
      5 trainer = SFTTrainer(
      6     model=model,
      7     train_dataset=dataset,
   (...)
     12     args=training_arguments,
     13 )

File /local_disk0/.ephemeral_nfs/envs/pythonEnv-8a237d14-25f4-4066-b47f-e8a95f2342d9/lib/python3.10/site-packages/trl/__init__.py:15
      8 from .import_utils import is_diffusers_available, is_peft_available, is_wandb_available, is_xpu_available
      9 from .models import (
     10     AutoModelForCausalLMWithValueHead,
     11     AutoModelForSeq2SeqLMWithValueHead,
     12     PreTrainedModelWrapper,
     13     create_reference_model,
     14 )
---> 15 from .trainer import (
     16     DataCollatorForCompletionOnlyLM,
     17     DPOTrainer,
     18     IterativeSFTTrainer,
     19     PPOConfig,
     20     PPOTrainer,
     21     RewardConfig,
     22     RewardTrainer,
     23     SFTTrainer,
     24 )
     27 if is_diffusers_available():
     28     from .models import (
     29         DDPOPipelineOutput,
     30         DDPOSchedulerOutput,
     31         DDPOStableDiffusionPipeline,
     32         DefaultDDPOStableDiffusionPipeline,
     33     )

File /local_disk0/.ephemeral_nfs/envs/pythonEnv-8a237d14-25f4-4066-b47f-e8a95f2342d9/lib/python3.10/site-packages/trl/trainer/__init__.py:40
     38 from .dpo_trainer import DPOTrainer
     39 from .iterative_sft_trainer import IterativeSFTTrainer
---> 40 from .ppo_config import PPOConfig
     41 from .ppo_trainer import PPOTrainer
     42 from .reward_trainer import RewardTrainer, compute_accuracy

File /local_disk0/.ephemeral_nfs/envs/pythonEnv-8a237d14-25f4-4066-b47f-e8a95f2342d9/lib/python3.10/site-packages/trl/trainer/ppo_config.py:22
     19 from typing import Literal, Optional
     21 import numpy as np
---> 22 import tyro
     23 from typing_extensions import Annotated
     25 from trl.trainer.utils import exact_div

File /local_disk0/.ephemeral_nfs/envs/pythonEnv-8a237d14-25f4-4066-b47f-e8a95f2342d9/lib/python3.10/site-packages/tyro/__init__.py:4
      1 from typing import TYPE_CHECKING
      3 from . import conf as conf
----> 4 from . import extras as extras
      5 from ._cli import cli as cli
      6 from ._fields import MISSING as MISSING

File /local_disk0/.ephemeral_nfs/envs/pythonEnv-8a237d14-25f4-4066-b47f-e8a95f2342d9/lib/python3.10/site-packages/tyro/extras/__init__.py:5
      1 """"""The :mod:`tyro.extras` submodule contains helpers that complement :func:`tyro.cli()`.
      2 
      3 Compared to the core interface, APIs here are more likely to be changed or deprecated. """"""
----> 5 from .._argparse_formatter import set_accent_color as set_accent_color
      6 from .._cli import get_parser as get_parser
      7 from ._base_configs import (
      8     subcommand_type_from_defaults as subcommand_type_from_defaults,
      9 )

File /local_disk0/.ephemeral_nfs/envs/pythonEnv-8a237d14-25f4-4066-b47f-e8a95f2342d9/lib/python3.10/site-packages/tyro/_argparse_formatter.py:37
     35 from rich.text import Text
     36 from rich.theme import Theme
---> 37 from typing_extensions import override
     39 from . import _arguments, _strings, conf
     40 from ._parsers import ParserSpecification

ImportError: cannot import name 'override' from 'typing_extensions' (/databricks/python/lib/python3.10/site-packages/typing_extensions.py)

I have tried installing multiple versions of typing_extensions, both the most updated version (4.8.0), as well as (4.7.1) as was suggested in this stackoverflow post. I also tried the solution posted here, as well as installing the dependencies with '%' instead of '!', as suggested here. None of this has worked.
Here's a full list of my installed packages:
Package                      Version
---------------------------- -------------
absl-py                      1.0.0
accelerate                   0.25.0.dev0
aiohttp                      3.8.5
aiosignal                    1.3.1
anyio                        3.5.0
appdirs                      1.4.4
argon2-cffi                  21.3.0
argon2-cffi-bindings         21.2.0
astor                        0.8.1
asttokens                    2.2.1
astunparse                   1.6.3
async-timeout                4.0.3
attrs                        21.4.0
audioread                    3.0.0
azure-core                   1.29.1
azure-cosmos                 4.3.1
azure-storage-blob           12.17.0
azure-storage-file-datalake  12.12.0
backcall                     0.2.0
bcrypt                       3.2.0
beautifulsoup4               4.11.1
bitsandbytes                 0.41.2.post2
black                        22.6.0
bleach                       4.1.0
blinker                      1.4
blis                         0.7.10
boto3                        1.24.28
botocore                     1.27.28
cachetools                   4.2.4
catalogue                    2.0.9
category-encoders            2.6.1
certifi                      2022.9.14
cffi                         1.15.1
chardet                      4.0.0
charset-normalizer           2.0.4
click                        8.0.4
cloudpickle                  2.0.0
cmdstanpy                    1.1.0
confection                   0.1.1
configparser                 5.2.0
convertdate                  2.4.0
cryptography                 37.0.1
cycler                       0.11.0
cymem                        2.0.7
Cython                       0.29.32
dacite                       1.8.1
databricks-automl-runtime    0.2.17
databricks-cli               0.17.7
databricks-feature-store     0.14.1
databricks-sdk               0.1.6
dataclasses-json             0.5.14
datasets                     2.13.1
dbl-tempo                    0.1.23
dbus-python                  1.2.18
debugpy                      1.6.0
decorator                    5.1.1
defusedxml                   0.7.1
dill                         0.3.4
diskcache                    5.6.1
distlib                      0.3.7
distro                       1.7.0
distro-info                  1.1+ubuntu0.1
docstring-parser             0.15
docstring-to-markdown        0.12
einops                       0.6.1
entrypoints                  0.4
ephem                        4.1.4
evaluate                     0.4.0
executing                    1.2.0
facets-overview              1.0.3
fastapi                      0.98.0
fastjsonschema               2.18.0
fasttext                     0.9.2
filelock                     3.6.0
flash-attn                   1.0.7
Flask                        1.1.2+db1
flatbuffers                  23.5.26
fonttools                    4.25.0
frozenlist                   1.4.0
fsspec                       2022.7.1
future                       0.18.2
gast                         0.4.0
gitdb                        4.0.10
GitPython                    3.1.27
google-api-core              2.8.2
google-auth                  1.33.0
google-auth-oauthlib         0.4.6
google-cloud-core            2.3.3
google-cloud-storage         2.10.0
google-crc32c                1.5.0
google-pasta                 0.2.0
google-resumable-media       2.5.0
googleapis-common-protos     1.56.4
greenlet                     1.1.1
grpcio                       1.48.1
grpcio-status                1.48.1
gunicorn                     20.1.0
gviz-api                     1.10.0
h11                          0.14.0
h5py                         3.7.0
holidays                     0.27.1
horovod                      0.28.1
htmlmin                      0.1.12
httplib2                     0.20.2
httptools                    0.6.0
huggingface-hub              0.16.4
idna                         3.3
ImageHash                    4.3.1
imbalanced-learn             0.10.1
importlib-metadata           4.11.3
importlib-resources          6.0.1
ipykernel                    6.17.1
ipython                      8.10.0
ipython-genutils             0.2.0
ipywidgets                   7.7.2
isodate                      0.6.1
itsdangerous                 2.0.1
jedi                         0.18.1
jeepney                      0.7.1
Jinja2                       2.11.3
jmespath                     0.10.0
joblib                       1.2.0
joblibspark                  0.5.1
jsonschema                   4.16.0
jupyter-client               7.3.4
jupyter_core                 4.11.2
jupyterlab-pygments          0.1.2
jupyterlab-widgets           1.0.0
keras                        2.11.0
keyring                      23.5.0
kiwisolver                   1.4.2
langchain                    0.0.217
langchainplus-sdk            0.0.20
langcodes                    3.3.0
launchpadlib                 1.10.16
lazr.restfulclient           0.14.4
lazr.uri                     1.0.6
lazy_loader                  0.3
libclang                     15.0.6.1
librosa                      0.10.0
lightgbm                     3.3.5
llvmlite                     0.38.0
LunarCalendar                0.0.9
Mako                         1.2.0
Markdown                     3.3.4
markdown-it-py               3.0.0
MarkupSafe                   2.0.1
marshmallow                  3.20.1
matplotlib                   3.5.2
matplotlib-inline            0.1.6
mccabe                       0.7.0
mdurl                        0.1.2
mistune                      0.8.4
mleap                        0.20.0
mlflow-skinny                2.5.0
more-itertools               8.10.0
msgpack                      1.0.5
multidict                    6.0.4
multimethod                  1.9.1
multiprocess                 0.70.12.2
murmurhash                   1.0.9
mypy-extensions              0.4.3
nbclient                     0.5.13
nbconvert                    6.4.4
nbformat                     5.5.0
nest-asyncio                 1.5.5
networkx                     2.8.4
ninja                        1.11.1
nltk                         3.7
nodeenv                      1.8.0
notebook                     6.4.12
numba                        0.55.1
numexpr                      2.8.4
numpy                        1.21.5
oauthlib                     3.2.0
openai                       0.27.8
openapi-schema-pydantic      1.2.4
opt-einsum                   3.3.0
packaging                    21.3
pandas                       1.4.4
pandocfilters                1.5.0
paramiko                     2.9.2
parso                        0.8.3
pathspec                     0.9.0
pathy                        0.10.2
patsy                        0.5.2
peft                         0.4.0
petastorm                    0.12.1
pexpect                      4.8.0
phik                         0.12.3
pickleshare                  0.7.5
Pillow                       9.2.0
pip                          23.3.1
platformdirs                 2.5.2
plotly                       5.9.0
pluggy                       1.0.0
pmdarima                     2.0.3
pooch                        1.7.0
preshed                      3.0.8
prompt-toolkit               3.0.36
prophet                      1.1.4
protobuf                     3.19.4
psutil                       5.9.0
psycopg2                     2.9.3
ptyprocess                   0.7.0
pure-eval                    0.2.2
pyarrow                      8.0.0
pyasn1                       0.4.8
pyasn1-modules               0.2.8
pybind11                     2.11.1
pycparser                    2.21
pydantic                     1.10.6
pyflakes                     3.0.1
Pygments                     2.16.1
PyGObject                    3.42.1
PyJWT                        2.3.0
PyMeeus                      0.5.12
PyNaCl                       1.5.0
pyodbc                       4.0.32
pyparsing                    3.0.9
pyright                      1.1.294
pyrsistent                   0.18.0
pytesseract                  0.3.10
python-apt                   2.4.0+ubuntu2
python-dateutil              2.8.2
python-dotenv                1.0.0
python-editor                1.0.4
python-lsp-jsonrpc           1.0.0
python-lsp-server            1.7.1
pytoolconfig                 1.2.2
pytz                         2022.1
PyWavelets                   1.3.0
PyYAML                       6.0
pyzmq                        23.2.0
regex                        2022.7.9
requests                     2.28.1
requests-oauthlib            1.3.1
responses                    0.18.0
rich                         13.6.0
rope                         1.7.0
rsa                          4.9
s3transfer                   0.6.0
safetensors                  0.3.2
scikit-learn                 1.1.1
scipy                        1.9.1
seaborn                      0.11.2
SecretStorage                3.3.1
Send2Trash                   1.8.0
sentence-transformers        2.2.2
sentencepiece                0.1.99
setuptools                   63.4.1
shap                         0.41.0
shtab                        1.6.4
simplejson                   3.17.6
six                          1.16.0
slicer                       0.0.7
smart-open                   5.2.1
smmap                        5.0.0
sniffio                      1.2.0
soundfile                    0.12.1
soupsieve                    2.3.1
soxr                         0.3.6
spacy                        3.5.3
spacy-legacy                 3.0.12
spacy-loggers                1.0.4
spark-tensorflow-distributor 1.0.0
SQLAlchemy                   1.4.39
sqlparse                     0.4.2
srsly                        2.4.7
ssh-import-id                5.11
stack-data                   0.6.2
starlette                    0.27.0
statsmodels                  0.13.2
tabulate                     0.8.10
tangled-up-in-unicode        0.2.0
tenacity                     8.1.0
tensorboard                  2.11.0
tensorboard-data-server      0.6.1
tensorboard-plugin-profile   2.11.2
tensorboard-plugin-wit       1.8.1
tensorflow                   2.11.1
tensorflow-estimator         2.11.0
tensorflow-io-gcs-filesystem 0.33.0
termcolor                    2.3.0
terminado                    0.13.1
testpath                     0.6.0
thinc                        8.1.12
threadpoolctl                2.2.0
tiktoken                     0.4.0
tokenize-rt                  4.2.1
tokenizers                   0.13.3
tomli                        2.0.1
torch                        1.13.1+cu117
torchvision                  0.14.1+cu117
tornado                      6.1
tqdm                         4.64.1
traitlets                    5.1.1
transformers                 4.30.2
trl                          0.7.4
typeguard                    2.13.3
typer                        0.7.0
typing_extensions            4.7.1
typing-inspect               0.9.0
tyro                         0.5.14
ujson                        5.4.0
unattended-upgrades          0.1
urllib3                      1.26.11
uvicorn                      0.23.2
uvloop                       0.17.0
virtualenv                   20.16.3
visions                      0.7.5
wadllib                      1.3.6
wasabi                       1.1.2
watchfiles                   0.19.0
wcwidth                      0.2.5
webencodings                 0.5.1
websocket-client             0.58.0
websockets                   11.0.3
Werkzeug                     2.0.3
whatthepatch                 1.0.2
wheel                        0.37.1
widgetsnbextension           3.6.1
wordcloud                    1.9.2
wrapt                        1.14.1
xgboost                      1.7.6
xxhash                       3.3.0
yapf                         0.31.0
yarl                         1.9.2
ydata-profiling              4.2.0
zipp                         3.8.0

If anyone has any idea how to fix this, please let me know!
","might related to Databricks bug. see this, https://github.com/openai/openai-python/issues/751
",llama
check the difference in pretrained and finetuned model,"So I am finetuning a pretrained LLaMa2 model. I want to check if the model that I have finetuned is different from the original. I want to check the difference between base_model and model. Is there a way to check if there is a difference in weights or parameters after training?
from google.colab import drive

# Mount Google Drive
drive.mount('/content/drive')

# Path to your saved model in Google Drive
model_path_in_drive = '/content/drive/MyDrive/Mod/llama-2-7b-miniguanaco'

# Reload model in FP16 and merge it with LoRA weights
base_model = AutoModelForCausalLM.from_pretrained(
    model_name,
    low_cpu_mem_usage=True,
    return_dict=True,
    torch_dtype=torch.float16,
    device_map=device_map,
)

# Load your PeftModel from the saved checkpoint in Google Drive
model = PeftModel.from_pretrained(base_model, model_path_in_drive)
model = model.merge_and_unload()
#mark_only_lora_as_trainable(lora_model)
# Reload tokenizer to save it
tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
tokenizer.pad_token = tokenizer.eos_token
tokenizer.padding_side = ""right""

I tried some methods in this article but it didn't help me at all.
","Just loop over the parameters and compare them with torch.allclose. I used DistilBertModel for the answer, please use the respective classes from your example that are also mentioned in the comments:
import torch
from transformers import  DistilBertModel

# AutoModelForCausalLM in your case
base_model =  DistilBertModel.from_pretrained(""distilbert/distilbert-base-uncased"")
# PeftModel.merge_and_unload() in your case
finetuned_model = DistilBertModel.from_pretrained(""distilbert/distilbert-base-uncased-finetuned-sst-2-english"")

for base_param, finetuned_param in zip(base_model.named_parameters(), finetuned_model.named_parameters()):
  if not torch.allclose(base_param[1], finetuned_param[1]):
    print(base_param[0])

Output:
embeddings.word_embeddings.weight
embeddings.position_embeddings.weight
embeddings.LayerNorm.weight
embeddings.LayerNorm.bias
transformer.layer.0.attention.q_lin.weight
transformer.layer.0.attention.q_lin.bias
transformer.layer.0.attention.k_lin.weight
transformer.layer.0.attention.k_lin.bias
transformer.layer.0.attention.v_lin.weight
transformer.layer.0.attention.v_lin.bias
transformer.layer.0.attention.out_lin.weight
transformer.layer.0.attention.out_lin.bias
transformer.layer.0.sa_layer_norm.weight
transformer.layer.0.sa_layer_norm.bias
transformer.layer.0.ffn.lin1.weight
transformer.layer.0.ffn.lin1.bias
transformer.layer.0.ffn.lin2.weight
transformer.layer.0.ffn.lin2.bias
transformer.layer.0.output_layer_norm.weight
transformer.layer.0.output_layer_norm.bias
...
transformer.layer.5.attention.q_lin.weight
transformer.layer.5.attention.q_lin.bias
transformer.layer.5.attention.k_lin.weight
transformer.layer.5.attention.k_lin.bias
transformer.layer.5.attention.v_lin.weight
transformer.layer.5.attention.v_lin.bias
transformer.layer.5.attention.out_lin.weight
transformer.layer.5.attention.out_lin.bias
transformer.layer.5.sa_layer_norm.weight
transformer.layer.5.sa_layer_norm.bias
transformer.layer.5.ffn.lin1.weight
transformer.layer.5.ffn.lin1.bias
transformer.layer.5.ffn.lin2.weight
transformer.layer.5.ffn.lin2.bias
transformer.layer.5.output_layer_norm.weight
transformer.layer.5.output_layer_norm.bias

",llama
runtimeerror cuda error no kernel image is available for execution on the device for cuda 118 and torch 200,"I wanted to use meta-llama/Llama-2-13b-chat-hf, but I am having this error:
RuntimeError: CUDA error: no kernel image is available for execution on the device
CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.

For debugging consider passing CUDA_LAUNCH_BLOCKING=1.

Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.

The output of nvidia-smi is:
| NVIDIA-SMI 465.19.01    Driver Version: 465.19.01    CUDA Version: 11.3     |

NVCC:
nvcc: NVIDIA (R) Cuda compiler driver
Copyright (c) 2005-2022 NVIDIA Corporation

Built on Wed_Sep_21_10:33:58_PDT_2022

Cuda compilation tools, release 11.8, V11.8.89

Build cuda_11.8.r11.8/compiler.31833905_0

Torch Version:
torch==2.0.0+cu118
torchaudio==2.0.1+cu118
torchvision==0.15.1+cu118

Transformers Version:
transformers==4.37.2

I have a some 2080ti and a 710 and am using Ubuntu 16.
I also got this in my output:
Found GPU9 NVIDIA GeForce GT 710 which is of cuda capability 3.5.
    PyTorch no longer supports this GPU because it is too old.
    The minimum cuda capability supported by this library is 3.7.

I was downloading torch versions from here.
I tried building from the source as well but it gave me the same output.
The output from bitsandbytes:

++++++++++++++++++++++++++ OTHER +++++++++++++++++++++++++++
COMPILED_WITH_CUDA = True
COMPUTE_CAPABILITIES_PER_GPU = ['7.5', '7.5', '7.5', '7.5', '7.5', '7.5', '7.5', '7.5', '7.5', '3.5']
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
++++++++++++++++++++++ DEBUG INFO END ++++++++++++++++++++++
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

Running a quick check that:
    + library is importable
    + CUDA function is callable


WARNING: Please be sure to sanitize sensible info from any such env vars!

SUCCESS!
Installation was successful!


I ran this code to test torch
import torch
import sys
print('A', sys.version)
print('B', torch.__version__)
print('C', torch.cuda.is_available())
print('D', torch.backends.cudnn.enabled)
device = torch.device('cuda')
print('E', torch.cuda.get_device_properties(device))
print('F', torch.tensor([1.0, 2.0]).cuda())

It gave me this output:
A 3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0]
B 2.0.0+cu118
C True
D True
    UserWarning: 
    Found GPU9 NVIDIA GeForce GT 710 which is of cuda capability 3.5.
    PyTorch no longer supports this GPU because it is too old.
    The minimum cuda capability supported by this library is 3.7.
    
  warnings.warn(old_gpu_warn % (d, name, major, minor, min_arch // 10, min_arch % 10))
E _CudaDeviceProperties(name='NVIDIA GeForce RTX 2080 Ti', major=7, minor=5, total_memory=11019MB, multi_processor_count=68)
F tensor([1., 2.], device='cuda:0')

What should I do to fix this?
","As mentioned in the warning:
    UserWarning: 
    Found GPU9 NVIDIA GeForce GT 710 which is of cuda capability 3.5.
    PyTorch no longer supports this GPU because it is too old.
    The minimum cuda capability supported by this library is 3.7.


The main issue was that torch was trying to run code on the GT710 and due to it being not supported, the program crashed. To fix this I added an environment variable:
export CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7,8

So torch just ignores the GT710 now and it runs perfectly fine for me now.
",llama
typeerror llama_tokenize missing 2 required positional arguments 39add_bos39 and 39special39,"I am running python 3.11 and the latest version of llama-cpp-python with a gguf model
I expect the code to run normally like a chatbot but instead I get this error:
Traceback (most recent call last):
  File ""d:\AI Custom\AI Arush\server.py"", line 223, in <module>
    init()
  File ""d:\AI Custom\AI Arush\server.py"", line 57, in init
    m_eval(model, m_tokenize(model, PROMPT_INIT, True), False, ""Starting up..."")
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""d:\AI Custom\AI Arush\server.py"", line 182, in m_tokenize
    n_tokens = llama_cpp.llama_tokenize(
               ^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: llama_tokenize() missing 2 required positional arguments: 'add_bos' and 'special'

This is my tokenize code:
def m_tokenize(model: llama_cpp.Llama, text: bytes, add_bos=False, special=False):
    assert model.ctx is not None
    n_ctx = llama_cpp.llama_n_ctx(model.ctx)
    tokens = (llama_cpp.llama_token * int(n_ctx))()
    n_tokens = llama_cpp.llama_tokenize(
        model.ctx,
        text,
        tokens,
        n_ctx,
        llama_cpp.c_bool(add_bos),
    )
    if int(n_tokens) < 0:
        raise RuntimeError(f'Failed to tokenize: text=""{text}"" n_tokens={n_tokens}')
    return list(tokens[:n_tokens])


","TypeError: llama_tokenize() missing 2 required positional arguments: 'add_bos' and 'special'

To resolve the error you need to include the arguments add_bos and special to the llama_tokenize() function.
def m_tokenize(model: llama_cpp.Llama, text: bytes, add_bos=False, special=False):
    assert model.ctx is not None
    n_ctx = llama_cpp.llama_n_ctx(model.ctx)
    tokens = (llama_cpp.llama_token * int(n_ctx))()
    
    # Include the missing arguments in the function call
    n_tokens = llama_cpp.llama_tokenize(
        model.ctx,
        text,
        tokens,
        n_ctx,
        # You should check if llama_cpp.c_bool(add_bos) is returning a c_boo value also you have the arguments add_bos=False and special=False in this function 
        # If I am right all you need is:
        add_bos
        # Not
        # llama_cpp.c_bool(add_bos),
        # You should check if llama_cpp.c_bool(special) is returning a c_boo value
        # If I am right all you need is:
        special  # Include the special argument
        # Not 
        # llama_cpp.c_bool(special) 
    )
    
    if int(n_tokens) < 0:
        raise RuntimeError(f'Failed to tokenize: text=""{text}"" n_tokens={n_tokens}')
    
    return list(tokens[:n_tokens])

From llama_cpp.py (GitHub), code lines starting at 1817
def llama_tokenize(
    model: llama_model_p,
    text: bytes,
    text_len: Union[c_int, int],
    tokens,  # type: Array[llama_token]
    n_max_tokens: Union[c_int, int],
    add_bos: Union[c_bool, bool],
    special: Union[c_bool, bool],
) -> int:
    """"""Convert the provided text into tokens.""""""
    return _lib.llama_tokenize(
        model, text, text_len, tokens, n_max_tokens, add_bos, special
    )


_lib.llama_tokenize.argtypes = [
    llama_model_p,
    c_char_p,
    c_int32,
    llama_token_p,
    c_int32,
    c_bool,
    c_bool,
]
_lib.llama_tokenize.restype = c_int32


",llama
running through this error  attributeerror can39t set attribute when finetuning llama2,"I am trying to fine-tune llama2 on my M2 macos, when I run this snippet of code
tokenizer = AutoTokenizer.from_pretrained(base_model, use_fast=True)
tokenizer.pad_token = tokenizer.unk_token
tokenizer.padding_side = ""right""

I get this error:
Traceback (most recent call last):
  File ""/Users/salmakhaled/Pobot/venv/lib/python3.9/site-packages/IPython/core/interactiveshell.py"", line 3550, in run_code
    exec(code_obj, self.user_global_ns, self.user_ns)
  File ""/var/folders/67/wk5jj9q91n3g7llmbclsf6wm0000gp/T/ipykernel_24144/1335867970.py"", line 2, in <module>
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
  File ""/Users/salmakhaled/Pobot/venv/lib/python3.9/site-packages/transformers/models/auto/tokenization_auto.py"", line 787, in from_pretrained
  File ""/Users/salmakhaled/Pobot/venv/lib/python3.9/site-packages/transformers/tokenization_utils_base.py"", line 2028, in from_pretrained
    if isinstance(value, dict):
  File ""/Users/salmakhaled/Pobot/venv/lib/python3.9/site-packages/transformers/tokenization_utils_base.py"", line 2260, in _from_pretrained
    save_directory, (filename_prefix + ""-"" if filename_prefix else """") + ADDED_TOKENS_FILE
  File ""/Users/salmakhaled/Pobot/venv/lib/python3.9/site-packages/transformers/models/llama/tokenization_llama_fast.py"", line 129, in __init__
    self.can_save_slow_tokenizer = False if not self.vocab_file else True
AttributeError: can't set attribute

My model name is llama2-7b-chat-hf
Python 3.9.6
transformers 4.36.1
accelerate 0.25.0
torch 2.3.0.dev20240118
I tried to upgrade the libraries versions but I still encounter this error. So I don't know is this issue with a dependencies conflict or another thing.
","The error AttributeError: can't set attribute is raised when you attempt to change a property (see more here)
From the error message this line is the cause of the problem :
self.can_save_slow_tokenizer = False if not self.vocab_file else True

The can_save_slow_tokenizer is updated to be a property in this commit, The line does not exist in transformers >= 4.33.0, it was replaced with :
    @property
    def can_save_slow_tokenizer(self) -> bool:
        return os.path.isfile(self.vocab_file) if self.vocab_file else False

as you mentioned you may have dependencies conflict, consider creating a new virtual environment and install transformers.
",llama
assertionerror when using llamacpppython in google colab,"I'm trying to use llama-cpp-python (a Python wrapper around llama.cpp) to do inference using the Llama LLM in Google Colab. My code looks like this:
!pip install llama-cpp-python
from llama_cpp import ChatCompletionMessage, Llama

model = Llama(
    ""/content/drive/MyDrive/<weights-file>.bin"",
)

However, when running it, I get this error:
AssertionError                            Traceback (most recent call last)
<ipython-input-13-652eb650093d> in <cell line: 9>()
      7 }
      8 
----> 9 model = Llama(
     10     model_path=""/content/drive/MyDrive/careo/Wizard-Vicuna-13B-Uncensored.ggmlv3.q4_1.bin"",
     11 )

/usr/local/lib/python3.10/dist-packages/llama_cpp/llama.py in __init__(self, model_path, n_ctx, n_parts, n_gpu_layers, seed, f16_kv, logits_all, vocab_only, use_mmap, use_mlock, embedding, n_threads, n_batch, last_n_tokens_size, lora_base, lora_path, low_vram, tensor_split, rope_freq_base, rope_freq_scale, n_gqa, rms_norm_eps, mul_mat_q, verbose)
    321                     self.model_path.encode(""utf-8""), self.params
    322                 )
--> 323         assert self.model is not None
    324 
    325         if verbose:

AssertionError: 

I have tried running this code on my local machine and it works without problem.
Do you have any idea what might be causing the error in Google Colab?
","From the model path - model_path=""/content/drive/MyDrive/careo/Wizard-Vicuna-13B-Uncensored.ggmlv3.q4_1.bin"", I can see you are using ggmlv3 model format, as per the new commit for llama-cpp-python repo, the new model format has been changed from ggmlv3 to gguf.
The author also mentioned that ggmlv3 weights are still gonna work for versions before 0.1.79(new version)
so you either mention the version while installing the package pip install llama-cpp-python==0.1.78 or change model format for new version gguf - refer.
If CodeLlama model weights are useful for you, then there are so many model weights published in hugging face like TheBloke/CodeLlama-13B-GGUF
",llama
langchain  local llama compatible model,"I'm trying to setup a local chatbot demo for testing purpose. I wanted to use LangChain as the framework and LLAMA as the model. Tutorials I found all involve some registration, API key, HuggingFace, etc, which seems unnecessary for my purpose.
Is there a way to use a local LLAMA comaptible model file just for testing purpose? And also an example code to use the model with LangChain would be appreciated. Thanks!
UPDATE: I wrote a blog post based on the accepted answer.
","No registration is required to utilize on-prem local models within ecosystems like Hugging Face (HF). Similarly, using Langchain does not involve any registration requirements. Various model formats, such as GGUF and GGML, are employed for storing models for inference and can be found on HF. It is crucial to consider these formats when attempting to load and run a model locally.
For instance, consider TheBloke's Llama-2-7B-Chat-GGUF model, which is a relatively compact 7-billion-parameter model suitable for execution on a modern CPU/GPU. To run the model, we can use Llama.cpp from Langchain:
def llamacpp():
    from langchain_community.llms import LlamaCpp
    from langchain.prompts import PromptTemplate
    from langchain.chains import LLMChain
    
    llm = LlamaCpp(
        model_path=""models/Llama-2-7B-Chat-GGUF/llama-2-7b-chat.Q4_0.gguf"",
        n_gpu_layers=40,
        n_batch=512,
        verbose=True,
    )
    
    template = """"""Question: {question}

    Answer: Let's work this out in a step by step way to be sure we have the right answer.""""""

    prompt = PromptTemplate(template=template, input_variables=[""question""])
    
    llm_chain = LLMChain(prompt=prompt, llm=llm)
    question = ""Who is Bjarne Stroustrup and how is he related to programming?""
    print(llm_chain.run(question))

And get output from the LLM:


Bjarne Stroustrup is a Danish computer scientist who created C++.
- He was born in Aarhus, Denmark on August 5, 1950 and earned his PhD from Cambridge University in 1983.
- In 1979 he began developing the programming language C++, which was initially called ""C with Classes"".
- C++ was first released in 1983 and has since become one of the most popular programming languages in use today.

Bjarne Stroustrup is known for his work on the C programming
language and its extension to C++.
- He wrote The C Programming Language, a book that helped establish C as a widely used language.
- He also wrote The Design and Evolution of C++, a detailed explanation of how he created C++ and why he made certain design
choices.


...

In this instance, I cloned TheBloke's model repository from HF and positioned it in a directory named models/. The final path for the model became models/Llama-2-7B-Chat-GGUF/llama-2-7b-chat.Q4_0.gguf:
# Make sure you have git-lfs installed (https://git-lfs.com)
git lfs install
git clone https://huggingface.co/TheBloke/Llama-2-7B-Chat-GGUF

Although the model can be run on a CPU, this was locally run on my Windows PC equipped with an RTX 4070 card with good performance during inference.
",llama
making an exe of a llama index application,"I got a strange error when tying to build an exe using pyinstaller. Im using a conda virtual environment, and smaller apps compile ok. However it is giving me an error:
FileNotFoundError: [Errno 2] No such file or directory: 'C:\Users\me\AppData\Local\Temp\_MEI140402\llama_index\VERSION'
The strange thing about it is that the program runs all okay on my system, and I'm having many issues making this exe. When I run locally there is no file like this in the temp folder.
Are there any other options to distribute my code?
# -*- mode: python ; coding: utf-8 -*-


a = Analysis(
    ['run.py'],
    pathex=[ all extra paths ],
    binaries=[],
    datas=[('added_hook.py', '.'),
    ],
    hiddenimports=['docmaker'],
    hookspath=['all hooks' ],
    hooksconfig={},
    runtime_hooks=[],
    excludes=[],
    noarchive=False,
)
pyz = PYZ(a.pure)

exe = EXE(
    pyz,
    a.scripts,
    a.binaries,
    a.datas,
    [],
    name='run',
    debug=False,
    bootloader_ignore_signals=False,
    strip=False,
    upx=True,
    upx_exclude=[],
    runtime_tmpdir=None,
    console=True,
    disable_windowed_traceback=False,
    argv_emulation=False,
    target_arch=None,
    codesign_identity=None,
    entitlements_file=None,
)

coll = COLLECT(
    exe,
    a.binaries,
    a.datas,
    strip=False,
    upx=True,
    upx_exclude=[],
    name='run',
)

I added all paths to my pathex so pyinstaller would pick it up, even the temp folder.
Also i added a py file called added_hook.py that adds more resources with the code. I attempted this to make sure pyinstaller would collect more files to compile the .exe
from PyInstaller.utils.hooks import collect_data_files
Instruct PyInstaller to collect data files from the 'resources' package.
datas = collect_data_files('resources')
","Having the same problem. The file that is missing is in the llama_index/ package install directory itself. Not sure how to get this file included via pyinstaller
Nasty hack that works for me: (but then blows up thinking transformers isn't installed...)
datas += [('.venv/lib/python3.11/site-packages/llama_index/VERSION', 'llama_index/')]
",llama
how to run llamacpp with cublas on windows,"I have been using Llama.cpp and running the model on my Mac (only CPU) but now I wanted to switch to Windows and run it on a GPU but when I try CuBlas build, I cannot seem to execute ./main or ./server file at all. Any idea what might be wrong? or want can be done? Here is what I am facing when I build with CuBlas,
./main.exe -m ./models/7B/llama-2-7b-chat.Q4_K_M.gguf -n 128 -ngl 40

./main.exe : The term './main.exe' is not recognized as the name of a cmdlet, function, script file, or operable program. Check the spelling of the name, or if a path was 
included, verify that the path is correct and try again.
At line:1 char:1
+ ./main.exe -m ./models/7B/llama-2-7b-chat.Q4_K_M.gguf -n 128 -ngl 40
+ ~~~~~~~~~~
    + CategoryInfo          : ObjectNotFound: (./main.exe:String) [], CommandNotFoundException
    + FullyQualifiedErrorId : CommandNotFoundException

","So, I found the solution to this question. The way I had built it was wrong. I used the method that was supposed to be used for Mac. Generally, I should follow a completely different approach for building on Windows. Here is the link to the GitHub repo for llama.cpp, which has steps to build on Windows.
Also, cuBlash has to be made for Windows but do not do it in the way you would do it for Mac. Windows have a different approach. Once you build it correctly you will see the main.exe and server.exe files in the main directory.
",llama
ollama with docker,"Hi Can you help me to debug this:
I pushed llama2 docker and i'm calling it using a link https and it is up
I pulled llama2:70b to this instance.
I have ""Ollama is running"" in my browser i can do the get using api/tags
Now when i call my model like this:
chat_model = ChatOllama(
            base_url=""https://mylink"", model=""llama2:70b"", verbose=True, callback_manager=callback_manager, 
        )

Then i do
chain = RetrievalQA.from_chain_type(llm=chat_model_2...)

and try to do
chain(MYQUERY)

i have an error.
if i do ChatOllama and specify my local it is wokring fine.
Any help please ?
Using Ollama with Docker on another instance.
Expected to work like you deploy the model on your local and it is not the case
","It seems that there is an issue with https, I made it work by replacing https with http and my server took care of the redirection:
chat_model = ChatOllama(
    base_url=""http://mylink"", model=""llama2:70b"", verbose=True, callback_manager=callback_manager, 
)

",llama
running llama2 on a geforce 1080 8gb machine,"I am trying to run LLama2 on my server which has mentioned nvidia card. It's a simple hello world case you can find here. However I am constantly running into memory issues:
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 250.00 MiB (GPU 0; 7.92 GiB total capacity; 7.12 GiB already allocated; 241.62 MiB free; 7.18 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

I tried
export PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:128
but same effect. Is there anything I can do?
","According to this source:

The model you use will vary depending on your hardware. For good
results, you should have at least 10GB VRAM at a minimum for the 7B
model, though you can sometimes see success with 8GB VRAM.

In order to lower the memory footprint of the model, I first recommend that you try running the model in half precision (if supported) with a batch size of one. If you still experience CUDA out of memory I suggest you to try with a quantized version of the model, like for example these ones.
",llama
llama2 running pytorch produces a quotfailed to create processquot,"CONTEXT

I am trying to run llama2 on my local machine.

I have followed the documentation available on the github repository


https://github.com/facebookresearch/llama
thank you in advance for your support
what did I do?

install anaconda
clone the llama repository

https://github.com/facebookresearch/llama

download the models

create a virtual environment named llama2

install pytorch on Anaconda


conda install pytorch torchvision torchaudio pytorch-cuda=12.1 -c pytorch -c nvidia

install local components

pip install -e .

executed the following command line

torchrun 
--nproc_per_node 1 example_text_completion.py 
--ckpt_dir llama-2-7b-chat/
--tokenizer_path tokenizer.model 
--max_seq_len 512 --max_batch_size 6

the error
failed to create process.
","root cause
the error comes from an incorrect-or-missing update of the torchrun-script.py
how to correct the issue

edit the file

C:\Apps\Anaconda3\envs\[YOUR ENVIRONMENT]\Scripts\torchrun-script.py

replace the first line with

#!c:\Apps\Anaconda3\envs\[YOUR ENVIRONMENT]\python.exe

save the file

voilà, it works!
",llama
why there is a quotropefreqsquot variable in llama27b weights,"I noticed a weight called “rope.freqs” in the weights of llama2 model（e.g. llama-2-7b or llama-2-7b-chat）. What is the function of this weight, and which part of the model does it correspond to?
IN [14]: checkpoint = torch.load('llama-2-7b-chat/consolidated.00.pth',map_location='cpu')
In [15]: checkpoint['rope.freqs']
Out[15]:
tensor([1.0000e+00, 8.6719e-01, 7.5000e-01, 6.4844e-01, 5.6250e-01, 4.8633e-01,
        4.2188e-01, 3.6523e-01, 3.1641e-01, 2.7344e-01, 2.3730e-01, 2.0508e-01,
        1.7773e-01, 1.5430e-01, 1.3379e-01, 1.1523e-01, 1.0010e-01, 8.6426e-02,
        7.5195e-02, 6.4941e-02, 5.6152e-02, 4.8584e-02, 4.2236e-02, 3.6621e-02,
        3.1738e-02, 2.7344e-02, 2.3682e-02, 2.0508e-02, 1.7822e-02, 1.5381e-02,
        1.3306e-02, 1.1536e-02, 1.0010e-02, 8.6670e-03, 7.5073e-03, 6.5002e-03,
        5.6152e-03, 4.8828e-03, 4.2114e-03, 3.6469e-03, 3.1586e-03, 2.7313e-03,
        2.3651e-03, 2.0599e-03, 1.7776e-03, 1.5411e-03, 1.3351e-03, 1.1520e-03,
        9.9945e-04, 8.6594e-04, 7.5150e-04, 6.4850e-04, 5.6076e-04, 4.8637e-04,
        4.2152e-04, 3.6430e-04, 3.1662e-04, 2.7466e-04, 2.3746e-04, 2.0504e-04,
        1.7738e-04, 1.5354e-04, 1.3351e-04, 1.1539e-04], device='cpu',
       dtype=torch.bfloat16)

In [16]: checkpoint['rope.freqs'].shape
Out[16]: torch.Size([64])


","It is the parameter in RoPE, a kind of position embedding. Check the paper ""RoFormer: Enhanced Transformer with Rotary Position Embedding"" if you want. In the code of LLaMA, it uses precomputed cosine and sine frequencies instead.
",llama
number of tokens exceeded maximum limit,"I am using the llama2 quantized model from Huggingface and loading it using ctransformers from langchain. When I run the query, I got the below warning
Number of tokens (512) exceeded maximum context length (512)
Below is my code:
from langchain.llms import CTransformers
llm = CTransformers(model='models_k/llama-2-7b-chat.ggmlv3.q2_K.bin',
                      model_type='llama',
                      config={'max_new_tokens': 512,
                              'temperature': 0.01}
                      )

B_INST, E_INST = ""[INST]"", ""[/INST]""
B_SYS, E_SYS = ""<<SYS>>\n"", ""\n<</SYS>>\n\n""

DEFAULT_SYSTEM_PROMPT=""""""\
You are a helpful, respectful and honest assistant. Always answer as helpfully as possible. 
Please ensure that your responses are socially unbiased and positive in nature.

If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. 
If you don't know the answer to a question, please don't share false information.""""""

instruction = db_schema + "" Based on the database schema provided to you \n Convert the following text from natural language to sql query: \n\n {text} \n only display the sql query""

SYSTEM_PROMPT = B_SYS + DEFAULT_SYSTEM_PROMPT + E_SYS

template = B_INST + SYSTEM_PROMPT + instruction + E_INST

prompt = PromptTemplate(template=template, input_variables=[""text""])
LLM_Chain=LLMChain(prompt=prompt, llm=llm)
print(LLM_Chain.run(""List the names and prices of electronic products that cost less than $500.""))

Can anyone tell me why am i getting this error? Do I have to change the settings?
","You can fix this by the suggestion: context length.
Code like here:
llm = CTransformers(model='models_k/llama-2-7b-chat.ggmlv3.q2_K.bin',
                      model_type='llama',
                      config={'max_new_tokens': 600,
                              'temperature': 0.01,
                              'context_length': 700}
                      )

",llama
how to install llama 20 on virtual envirnoment,"I am trying to run llama on my local server. After running Quickstart.ipynb Iam getting error llama_recipes not found. I tried to install llama_recipes from directory but didn't work. Also while loading, I am getting error about accelerator
","
Create a virtual environment
Clone repo:
https://github.com/facebookresearch/llama-recipes
Install requirements from dev_requirement.txt an requirements.txt using command pip install -r [filename].txt
Move llama_recipes folder from src directory and quickstart.ipynb from examples directory to same directory.
Install/upgrade jupyter and ipywidgets if you are running jupyter notebook on VSCode.
if you still face dependecy error about accelerator/bitsandbytes
edit load_in_8bit=True to load_in_8bit=False.

",llama
can we control number of documents to return in retrievalqa langchain,"Can we control the document query parameter in RetrievalQA() like we could do in vectorDBQA() in langchain before? Also, shall I use map_reduce chain type instead for my large documents?
I tried to look into the source code but could not find it.
","You can customized the Retrieval and add your requirement
   class URRetrival(BaseRetriever):

    def __int__(self):
        pass

    def setparms(self, count):
        self.count = count


    def _get_relevant_documents(
            self, query: str, *, run_manager: CallbackManagerForRetrieverRun
    ) -> List[Document]:
        
        # need to add your similarity search here and add set self.count as limit
    #results = your vectordb.similarity(limit_or_k = self.count)
        return results

    async def _aget_relevant_documents(
            self,
            query: str,
            *,
            run_manager: AsyncCallbackManagerForRetrieverRun,
            **kwargs: Any,
    ) -> List[Document]:
        raise NotImplementedError()

in above example you need to add your vector db , its depend on you which you are using, and return the similarity results, the vector db supported for k/limit of relative docs
_get_relevant_documents : method call when inside the chain, you need to pass this costume retrieval as chain retrieval
Example:
retrival = URRetrival()
retrival.setparms(5)
qa_with_sources_chain = RetrievalQA.from_chain_type(
    llm=llm,
    retriever=retrival,
    callbacks=[handler],

",llama
trl sfttrainer  llama2 finetuning on alpaca  datasettext field,"I am trying to finetune the Llama2 model using the alpaca dataset.  I have loaded the model in 4-bit and apply the peft config to the model for Lora training. Then I am trying to do TRL’s SFTTrainer to fine-tune the model.
The train_dataset is
Dataset({
    features: ['instruction', 'input', 'output', 'input_ids', 'attention_mask'],
    num_rows: 50002
})

This is the error that I get:
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
Cell In[28], line 3
      1 # Step 8 :Set supervised fine-tuning parameters
      2 from transformers import DataCollatorForLanguageModeling
----> 3 trainer = SFTTrainer(
      4     model=model,
      5     train_dataset=train_data,
      6     #eval_dataset=val_data,
      7     #peft_config=peft_config,
      8     #dataset_text_field=""train"",
      9     max_seq_length=max_seq_length,
     10     tokenizer=tokenizer,
     11     args=training_arguments,
     12     #packing=True,
     13     #data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False),
     14 )


ValueError: You passed `packing=False` to the SFTTrainer, but you didn't pass a `dataset_text_field` or `formatting_func` argument.

If I try to pass the packing = True then I get this error:
ValueError: You need to pass a `dataset_text_field` or `formatting_func` argument to the SFTTrainer if you want to use the `ConstantLengthDataset`.

If I provide the dataset_text_field, which I do not know what it is.  I tried with ""train"" or ""text"" keywords and I am getting this error:
ValueError: the `--group_by_length` option is only available for `Dataset`, not `IterableDataset

I appreciate if someone can help me to understand the ""dataset_text_filed"", where do I set ConstantLengthDataset (does it come from packing?).  I also tried with packing = False and provide the dataset_text_field with 'train' and 'text' and they are incorrect.
based on the documentation:
dataset_text_field (Optional[str]): The name of the text field of the dataset, in case this is passed by a user, the trainer will automatically create a ConstantLengthDataset based on the dataset_text_field argument.

","dataset_text_field (Optional[str]) is the name of the field in the training dataset that contains the text that will be used for training only if formatting_func is None.
You should be careful because if you do this:
`dataset_text_field='instruction'

SFTTrainer will only read the text saved in train_dataset['instruction'].
So that Llama2 will only learn to predict the instructions without the answers.
Each line of your dataset(train_data) should have a string saved in 'text' field like this:
train_data[0]['text']=""<s>[INST]<<SYS>>You are an expert in math.<</SYS>>Compute 2+2 [/INST]4 </s>""

And you should do this:
`dataset_text_field='text'

",llama
how to debug the llama2 inference command with vscode,"I am trying to run the LLAMA2 inference script (shown below) with vscode debugging mode:
    torchrun --nproc_per_node 1 example_text_completion.py \
    --ckpt_dir models/7B-Chat \
    --tokenizer_path tokenizer.model \
    --max_seq_len 128 --max_batch_size 4

Before this, I can successfully run it with my command line interface, which shows my python environment is correct.
I have tried these two debug configs below:


 {
     ""name"": ""Python: run_llama2_inference"",
     ""type"": ""python"",
     ""request"": ""launch"",
     ""module"": ""torchrun"",
     ""args"": [
         ""--nproc_per_node=1"",
         ""example_chat_completion.py"",
         ""--ckpt_dir=models/7B-Chat/"",
         ""--tokenizer_path=tokenizer.model"",
         ""--max_seq_len=512"",
         ""--max_batch_size=4"",
     ],
     ""console"": ""integratedTerminal"",
     ""justMyCode"": true,
     ""env"": {
         ""PYTHONPATH"": ""${workspaceFolder}""
     }
 },

Corresponding Error Messsage: ""No module named torchrun""


 {
     ""name"": ""Python: run_llama2_inference"",
     ""type"": ""python"",
     ""request"": ""launch"",
     ""module"": ""torch.distributed.launch"",
     ""args"": [
         ""--use-env"",
         ""example_chat_completion.py"",
         ""--nproc_per_node=1"",
         ""--ckpt_dir=models/7B-Chat/"",
         ""--tokenizer_path=tokenizer.model"",
         ""--max_seq_len=512"",
         ""--max_batch_size=4"",
     ],
     ""console"": ""integratedTerminal"",
     ""justMyCode"": true,
     ""env"":
         ""PYTHONPATH"": ""${workspaceFolder}""
     }
 },

Corresponding Error Messsage: ""Could not consume arg: --nproc_per_node=1""


Both configs do not work as expected. I would like to seek the advice from online experts. Appreciate your ideas or advice in advance!
","You can use the ""program"" field to specify the Python script you want to run (example_text_completion.py), and pass the rest of the arguments using the ""args"" field.
Here's an example of how you can modify your launch configuration:
{
    ""name"": ""Python: run_llama2_inference"",
    ""type"": ""python"",
    ""request"": ""launch"",
    ""program"": ""${workspaceFolder}/example_chat_completion.py"",
    ""args"": [
        ""--ckpt_dir=models/7B-Chat/"",
        ""--tokenizer_path=tokenizer.model"",
        ""--max_seq_len=512"",
        ""--max_batch_size=4"",
    ],
    ""console"": ""integratedTerminal"",
    ""justMyCode"": true,
    ""env"": {
        ""PYTHONPATH"": ""${workspaceFolder}""
    }
}

",llama
performing inference on llama2 from vertex ai model garden,"I deployed llama2-chat-13b from model garden. However, I am getting error while trying to perform inference.
Configuration:
project=""X"";
endpoint_id=""Y"";
location=""us-east1"";
64 VCPUs, 57.6 GB RAM;
GPU= 4 T4;

I tried three approaches, but all of them are returning some kind of error:
Approach 1:
from typing import Dict, List, Union

from google.cloud import aiplatform
from google.protobuf import json_format
from google.protobuf.struct_pb2 import Value


def predict_custom_trained_model_sample(
    project: str,
    endpoint_id: str,
    instances: Union[Dict, List[Dict]],
    location: str = ""us-east1"",
    api_endpoint: str = ""us-east1-aiplatform.googleapis.com"",
):
    """"""
    `instances` can be either single instance of type dict or a list
    of instances.
    """"""
    # The AI Platform services require regional API endpoints.
    client_options = {""api_endpoint"": api_endpoint}
    # Initialize client that will be used to create and send requests.
    # This client only needs to be created once, and can be reused for multiple requests.
    client = aiplatform.gapic.PredictionServiceClient(client_options=client_options)
    # The format of each instance should conform to the deployed model's prediction input schema.
    instances = instances if isinstance(instances, list) else [instances]
    instances = [
        json_format.ParseDict(instance_dict, Value()) for instance_dict in instances
    ]
    parameters_dict = {}
    parameters = json_format.ParseDict(parameters_dict, Value())
    endpoint = client.endpoint_path(
        project=project, location=location, endpoint=endpoint_id
    )
    response = client.predict(
        endpoint=endpoint, instances=instances, parameters=parameters
    )
    print(""response"")
    print("" deployed_model_id:"", response.deployed_model_id)
    # The predictions are a google.protobuf.Value representation of the model's predictions.
    predictions = response.predictions
    for prediction in predictions:
        print("" prediction:"", dict(prediction))


# [END aiplatform_predict_custom_trained_model_sample]


predict_custom_trained_model_sample(
    project="""",
    endpoint_id="""",
    location=""us-east1"",
    instances = [
      {
         ""prompt"":[""hello""]
      },
   ]
)

I get the following error:
_InactiveRpcError                         Traceback (most recent call last)
/usr/local/lib/python3.10/dist-packages/google/api_core/grpc_helpers.py in error_remapped_callable(*args, **kwargs)
     71         try:
---> 72             return callable_(*args, **kwargs)
     73         except grpc.RpcError as exc:

6 frames
_InactiveRpcError: <_InactiveRpcError of RPC that terminated with:
    status = StatusCode.INTERNAL
    details = ""Internal Server Error""
    debug_error_string = ""UNKNOWN:Error received from peer ipv4:172.217.15.234:443 {created_time:""2023-10-31T20:13:00.233826088+00:00"", grpc_status:13, grpc_message:""Internal Server Error""}""
>

The above exception was the direct cause of the following exception:

InternalServerError                       Traceback (most recent call last)
/usr/local/lib/python3.10/dist-packages/google/api_core/grpc_helpers.py in error_remapped_callable(*args, **kwargs)
     72             return callable_(*args, **kwargs)
     73         except grpc.RpcError as exc:
---> 74             raise exceptions.from_grpc_error(exc) from exc
     75 
     76     return error_remapped_callable

InternalServerError: 500 Internal Server Error

Approach 2:
import vertexai
from vertexai.language_models import TextGenerationModel
vertexai.init(project=""X"", location=""us-east1"")
parameters = {
    ""candidate_count"": 1,
    ""max_output_tokens"": 1024,
    ""temperature"": 0.2,
    ""top_p"": 0.8,
    ""top_k"": 40
}
model = TextGenerationModel.from_pretrained(""Llama2-13B-chat-001"")

I get the following error:
_InactiveRpcError                         Traceback (most recent call last)
/usr/local/lib/python3.10/dist-packages/google/api_core/grpc_helpers.py in error_remapped_callable(*args, **kwargs)
     71         try:
---> 72             return callable_(*args, **kwargs)
     73         except grpc.RpcError as exc:

11 frames
_InactiveRpcError: <_InactiveRpcError of RPC that terminated with:
    status = StatusCode.INVALID_ARGUMENT
    details = ""Invalid publisher model resource format.""
    debug_error_string = ""UNKNOWN:Error received from peer ipv4:172.217.0.74:443 {created_time:""2023-10-31T20:23:51.049594923+00:00"", grpc_status:3, grpc_message:""Invalid publisher model resource format.""}""
>

The above exception was the direct cause of the following exception:

InvalidArgument                           Traceback (most recent call last)
/usr/local/lib/python3.10/dist-packages/google/api_core/grpc_helpers.py in error_remapped_callable(*args, **kwargs)
     72             return callable_(*args, **kwargs)
     73         except grpc.RpcError as exc:
---> 74             raise exceptions.from_grpc_error(exc) from exc
     75 
     76     return error_remapped_callable

InvalidArgument: 400 Invalid publisher model resource format.

Finally, I tried with langchain.
Approach 3:
from langchain.llms.vertexai import VertexAIModelGarden, VertexAI

llm = VertexAIModelGarden(
    project=""X"",
    endpoint_id=""Y"",
    location='us-east1'
)
llm(""hello"")

I get the following error:
TypeError: string indices must be integers
","Well, this is working for me.
from typing import Dict, List, Union

from google.cloud import aiplatform
from google.protobuf import json_format
from google.protobuf.struct_pb2 import Value


def predict_custom_trained_model_sample(
    project: str,
    endpoint_id: str,
    instances: Union[Dict, List[Dict]],
    location: str = ""us-central1"",
):
    """"""
    `instances` can be either single instance of type dict or a list
    of instances.
    """"""
    api_endpoint = f""{location}-aiplatform.googleapis.com""
    # The AI Platform services require regional API endpoints.
    client_options = {""api_endpoint"": api_endpoint}
    # Initialize client that will be used to create and send requests.
    # This client only needs to be created once, and can be reused for multiple requests.
    client = aiplatform.gapic.PredictionServiceClient(client_options=client_options)
    # The format of each instance should conform to the deployed model's prediction input schema.
    instances = instances if isinstance(instances, list) else [instances]
    instances = [
        json_format.ParseDict(instance_dict, Value()) for instance_dict in instances
    ]
    parameters_dict = {}
    parameters = json_format.ParseDict(parameters_dict, Value())
    endpoint = client.endpoint_path(
        project=project, location=location, endpoint=endpoint_id
    )
    response = client.predict(
        endpoint=endpoint, instances=instances, parameters=parameters
    )
    print(""Response"")
    print(""Deployed Model ID:"", response.deployed_model_id)
    # The predictions are a google.protobuf.Value representation of the model's predictions.
    predictions = response.predictions
    for prediction in predictions:
        print(""prediction:"", prediction)

query = ""Who is Albert Einstein?""
predict_custom_trained_model_sample(
    project=""X"",
    endpoint_id=""Y"",
    location=""us-east1"",
    instances = [
      {
         ""prompt"": query, ""temperature"": 0.0
      },
   ]
)

",llama
how can i retrain a llama 2 text generation model into a sequencetosequence model,"LLaMA 2 is a Text Generation Model. Is it possible to re-train the model to make it capable of doing sequence-to-sequence generation, such as translation? I can access LLaMA 2 via the HuggingFace platform.
Alternatively, should I write a prompt to ask LLaMA 2 to translate words and train its translation ability with Q & A style?
Thanks.
","It is definitely possible and certainly one of the better capabilities of such models. Many translation models have been trained in an encoder-decoder transformer structure, including the very first transformer paper.
While you can finetune a model to do specifically translation, you could try to zero-shot or few-shot the inference and it probably should return good results still. Did you know have good enough performance from that?
",llama
run code llama from hugging face locally with gpu,"I have trying to host the Code Llama from Hugging Face locally and trying to run it. It runs soley on CPU and it is not utilizing GPU available in the machine despite having Nvidia Drivers and Cuda toolkit.
from transformers import AutoTokenizer
import transformers

model = ""codellama/CodeLlama-7b-hf""

tokenizer = AutoTokenizer.from_pretrained(model)
pipeline = transformers.pipeline(
    ""text-generation"",
    model=model,
    torch_dtype=None,
    device_map = ""cuda:0""
)

prompt = ""Write python code to reverse a string""

sequences = pipeline(
    prompt,
    do_sample=True,
    top_k=10,
    temperature=0.1,
    top_p=0.95,
    num_return_sequences=1,
    eos_token_id=tokenizer.eos_token_id,
    max_length=200,
)
for seq in sequences:
    print(f""Result: {seq['generated_text']}"")


The code above runs the LLM locally but in case we use cuda for the device, it gives the following error
File ""C:\Users\winuser3\Desktop\GENAI-App\venv\lib\site-packages\transformers\modeling_utils.py"", line 3333, in from_pretrained
    ) = cls._load_pretrained_model(
  File ""C:\Users\winuser3\Desktop\GENAI-App\venv\lib\site-packages\transformers\modeling_utils.py"", line 3723, in _load_pretrained_model
    new_error_msgs, offload_index, state_dict_index = _load_state_dict_into_meta_model(
  File ""C:\Users\winuser3\Desktop\GENAI-App\venv\lib\site-packages\transformers\modeling_utils.py"", line 744, in _load_state_dict_into_meta_model
    set_module_tensor_to_device(model, param_name, param_device, **set_module_kwargs)
  File ""C:\Users\winuser3\Desktop\GENAI-App\venv\lib\site-packages\accelerate\utils\modeling.py"", line 317, in set_module_tensor_to_device
    new_value = value.to(device)
  File ""C:\Users\winuser3\Desktop\GENAI-App\venv\lib\site-packages\torch\cuda\__init__.py"", line 289, in _lazy_init
    raise AssertionError(""Torch not compiled with CUDA enabled"")
AssertionError: Torch not compiled with CUDA enabled


","You need to install a version of Torch that is CUDA enabled; see https://pytorch.org/get-started/locally.
Namely, You click on ""Windows"", ""CUDA 11.8"" and you get the installation instruction pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118.
If you had already installed torch or the other packages, you may need to pip uninstall them first.
",llama
theblokellama27b does not appear to have a file named pytorch_modelbin tf_modelh5 modelckpt or flax_modelmsgpack,"As you can guess from the title, this is the error I get. I only changed the model in AutoModelForCausalLM, Older version was

model = AutoModelForCausalLM.from_pretrained(""meta-llama/Llama-2-7b-chat-hf"",

device_map ='auto',

torch_dtype = torch.float16,

use_auth_token = True)

However, since my GPU is NVIDIA GeForce RTX 2080 TI, it answers a simple question in 20 mins. Then I changed it to:

model = AutoModelForCausalLM.from_pretrained(""TheBloke/Llama-2-7b-Chat-GGUF"",

model_file = ""llama-2-7b-chat.q4_K_M.gguf"",

device_map ='auto',

torch_dtype = torch.float16,

use_auth_token = True)

However, this is not working, and giving the error. Below is the full code, if it is needed to solve.
Before the full code: Also, I have the file ""llama-2-7b.Q5_K_m.gguf"" downloaded from HF in my local env, but not virtual env. I am not using this local file in the code, but saying if it helps.
from langchain.document_loaders import JSONLoader

from langchain.text_splitter import CharacterTextSplitter, TokenTextSplitter, RecursiveCharacterTextSplitter

from langchain.embeddings import HuggingFaceEmbeddings

from langchain.vectorstores import Chroma

from langchain import HuggingFacePipeline

from langchain.chains import ConversationalRetrievalChain

from langchain.memory import ConversationBufferMemory

from langchain.embeddings.openai import OpenAIEmbeddings

from langchain.embeddings.huggingface import HuggingFaceEmbeddings

from langchain.chat_models import ChatOpenAI

import os

import sys

import huggingface_hub

from huggingface_hub import notebook_login

import torch

import transformers

from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline

from torch import cuda, bfloat16

import chromadb

from pathlib import Path

from pprint import pprint

import json

from loader import JSONLoader

from langchain.prompts.chat import PromptTemplate, HumanMessagePromptTemplate, SystemMessagePromptTemplate, ChatPromptTemplate

import json

from langchain.docstore.document import Document



def parse_json(json_data):

""""""Parse JSON data into a Python dictionary.""""""

return json.loads(json_data)



def create_doc(json_data):

""""""Create a Document object from JSON data.""""""

data = parse_json(json_data)

content_value = """"



# Collect values of keys that contain ""item"" in their name

for key, value in data.items():

if ""item"" in key.lower():

content_value += value + ""\n""



return Document(page_content=content_value, metadata={""company"": data[""company""]})





##embed_model_id = 'BAAI/bge-base-en' ## CHANGE



embed_model_id = 'sentence-transformers/all-mpnet-base-v2'







device = f'cuda:{cuda.current_device()}' if cuda.is_available() else 'cpu' ## NVIDIA GeForce RTX 2080 TI



embed_model = HuggingFaceEmbeddings(

model_name=embed_model_id,

model_kwargs={'device': device},

encode_kwargs={'device': device, 'batch_size': 32}

)



docs = []





for file in os.listdir(""lessdata""):

if file.endswith("".json""):

file_path = ""./lessdata/""+file

with open(file_path) as file:

json_data = file.read()

document = create_doc(json_data)

docs.append(document)





document_splitter = RecursiveCharacterTextSplitter(separators=['\n'], chunk_size = 500, chunk_overlap = 100)

document_chunks = document_splitter.split_documents(docs)





vectordb = Chroma.from_documents(document_chunks,embedding=embed_model, persist_directory='./database')



##vectordb.persist()

'''

vectordb = Chroma.from_documents(document_chunks,embedding=embed_model, persist_directory='./database')

vectordb.persist('./database')





'''







### PLEASE DO NOT TOUCH THE VSCODE





tokenizer = AutoTokenizer.from_pretrained(""meta-llama/Llama-2-7b-chat-hf"", use_auth_token = True,)





model = AutoModelForCausalLM.from_pretrained(""TheBloke/Llama-2-7b-Chat-GGUF"",

model_file = ""llama-2-7b-chat.q4_K_M.gguf"",

device_map ='auto',

torch_dtype = torch.float16,

use_auth_token = True)









'''

model = AutoModelForCausalLM.from_pretrained(""meta-llama/Llama-2-7b-chat-hf"",

device_map ='auto',

torch_dtype = torch.float16,

use_auth_token = True)





'''







pipe = pipeline(""text-generation"",

model = model,

tokenizer = tokenizer,

device_map='auto',

max_new_tokens = 512,

min_new_tokens = 1,

top_k = 5) ##see it



## In vectorstore, take top 5 closest vectors-inputs-contexts, whatever you wanna call.



llm = HuggingFacePipeline(pipeline=pipe, model_kwargs= {'temperature':0.7})



memory = ConversationBufferMemory(memory_key=""chat_history"", input_key='question', output_key='answer', return_messages=True)



system_template = r""""""

Given a context, use your knowledge and answer the question. Be flexible, and try everything to answer in the format asked by query.

----

{context}

----

""""""





user_template = ""Question:```{question}```""



messages = [

SystemMessagePromptTemplate.from_template(system_template),

HumanMessagePromptTemplate.from_template(user_template)

]





qa_prompt = ChatPromptTemplate.from_messages(messages)







jsonExpert = ConversationalRetrievalChain.from_llm(llm = llm,

retriever=vectordb.as_retriever(search_kwargs = {'k': 1}), ## whats it

verbose = True, memory = memory, combine_docs_chain_kwargs={'prompt': qa_prompt},

return_source_documents = True

)



##retriever returns 1 output object.



chat_history = []

query = ""Consider the financials and progress of companies who is in the tech business.""

result = jsonExpert({""question"": query}, {""chat_history"": chat_history})

#result = jsonExpert({""question"": query})





sources = result[""source_documents""][0]

print(result['answer'])

pprint(sources)

pprint(memory)

","Basically you have to convert your downloaded weights to Hugging Face Transformers format using this
python src/transformers/models/llama/convert_llama_weights_to_hf.py \ --input_dir /path/to/downloaded/llama/weights --model_size 7B --output_dir /output/path
Then you would be able to get the config.json.
Source: https://huggingface.co/docs/transformers/main/en/model_doc/llama
",llama
llama 70b on hugging face inference api endpoint short responses,"I just deployed the Nous-Hermes-Llama2-70b parameter on a 2x Nvidia A100 GPU through the Hugging Face Inference endpoints.
When I tried the following code, the response generations were incomplete sentences that were less than 1 line long.
import requests

API_URL = 'https://myendpoint.us-east-1.aws.endpoints.huggingface.cloud'
headers = {
  ""Authorization"": ""Bearer mytoken1234"",
  ""Content-Type"": ""application/json""
}

def query(payload):
  response = requests.post(API_URL, headers=headers, json=payload)
  return response.json()
 
output = query({
  ""inputs"": ""### Instruction:\r\nCome up with a joke about cats\r\n### Response:\r\n"",
})

The output in this case was:
""Why don't cats play poker in the jungle?

 Because ""

As you see, the response stopped after 9 words.
Do I need to add more headers to the request like temperature and max token length? How would I do that? What do I need to do to get normal, long responses?
Here is the model I'm using: https://huggingface.co/NousResearch/Nous-Hermes-Llama2-70b
","Added ""max_new_tokens"" => 256 as a parameter, fixed it.
",llama
using langchain for text to sql using custom llm api,"I am trying to use my llama2 model (exposed as an API using ollama). I want to chat with the llama agent and query my Postgres db (i.e. generate text to sql). I was able to find langchain code that uses open AI to do this. However, I am unable to find anything out there which fits my situation.
Any pointers will be of great help.
Code with openai
# Create connection to postgres
import psycopg2  # Import the library

database = 'postgres'
username = 'postgres'
password = 'password'
server = 'localhost'
port = '5432'

# Establish the connection
conn = psycopg2.connect(
    dbname=database,
    user=username,
    password=password,
    host=server,
    port=port
)

db = SQLDatabase.from_uri(
    ""postgresql://postgres:password@localhost:5432/postgres"")
toolkit = SQLDatabaseToolkit(db=db, llm=OpenAI(temperature=0))

agent_executor = create_sql_agent(
    llm=OpenAI(temperature=0),
    toolkit=toolkit,
    verbose=True,
    agent_type=AgentType.ZERO_SHOT_REACT_DESCRIPTION,
)

agent_executor.run(""Describe the transaction table"")

I want to make the above code work for my llama2 model exposed via an API at localhost:11434/api/generate
","Load your llm like mentioned here https://python.langchain.com/docs/integrations/llms/ollama
and then use that inplace of openai. You'll most probably have to change the prompts to fit llama2 desired format
",llama
repositorynotfounderror while using llama27bchatggmlv3q8_0bin,"I am using colab to interact with CSV.
# Load the locally downloaded model
def load_llm():
    llm = CTransformers(
        model = ""llama-2-7b-chat.ggmlv3.q8_0.bin"",
        model_type=""llama"",
        max_new_tokens = 512,
        temperature = 0.5
    )
    return llm


uploaded_file = '/content/2019.csv'
loader = CSVLoader(file_path=uploaded_file, encoding=""utf-8"", csv_args={'delimiter': ','})
data = loader.load()

# Initialize embeddings and vector store
embeddings = HuggingFaceEmbeddings(model_name='sentence-transformers/all-MiniLM-L6-v2', model_kwargs={'device': 'cpu'})
db = FAISS.from_documents(data, embeddings)
llm = load_llm()
chain = ConversationalRetrievalChain.from_llm(llm=llm, retriever=db.as_retriever())


    The above exception was the direct cause of the following exception:
    
    RepositoryNotFoundError                   Traceback (most recent call last)
    /usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_errors.py in hf_raise_for_status(response, endpoint_name)
        291                 "" make sure you are authenticated.""
        292             )
    --> 293             raise RepositoryNotFoundError(message, response) from e
        294 
        295         elif response.status_code == 400:
    
    RepositoryNotFoundError: 401 Client Error. (Request ID: Root=1-64e367a0-03a4565e09b69d8b0d3624fb;5108be96-cf03-475c-a68f-c6481f96b567)

Repository Not Found for url: https://huggingface.co/api/models/llama-2-7b-chat.ggmlv3.q8_0.bin/revision/main.
Please make sure you specified the correct `repo_id` and `repo_type`.
If you are trying to access a private or gated repo, make sure you are authenticated.
Invalid username or password.

https://huggingface.co/api/models/llama-2-7b-chat.ggmlv3.q8_0.bin/revision/main is not found and returning 404 so i am getting this error. what is the resolution to get this model in colab- any suggestion would be really helpful
","In this way the download of the model is carried out without problem.
from huggingface_hub import hf_hub_download

# https://huggingface.co/docs/huggingface_hub/v0.16.3/en/package_reference/file_download#huggingface_hub.hf_hub_download
hf_hub_download(
    repo_id=""TheBloke/Llama-2-7B-Chat-GGML"",
    filename=""llama-2-7b-chat.ggmlv3.q8_0.bin"",
    local_dir=""./models""
)

Or with ctransformers, you can load from Hugging Face Hub directly and specify a model file (.bin or .gguf files) using:
from ctransformers import AutoModelForCausalLM

llm = AutoModelForCausalLM.from_pretrained(""TheBloke/Llama-2-7B-Chat-GGML"", model_file=""llama-2-7b-chat.ggmlv3.q8_0.bin"")

",llama
transformers code works on its own but breaks when using gradio device mismatch,"I am attempting to make a gradio demo for nanoLLaVA by @stablequan. I am porting over just the structure of Apache 2.0 licensed code in the Moondream repo.
The nanoLLaVA repo has example code in the repo, which I used to make this script. This works and provides a reasonable output.
enter image description here
When I use the same code but in gradio here,  I get this error regarding a mismatch in devices.
Traceback (most recent call last):
  File ""C:\Users\Moo\AppData\Local\Programs\Python\Python311\Lib\site-packages\gradio\queueing.py"", line 495, in call_prediction
    output = await route_utils.call_process_api(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""C:\Users\Moo\AppData\Local\Programs\Python\Python311\Lib\site-packages\gradio\route_utils.py"", line 232, in call_process_api
    output = await app.get_blocks().process_api(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""C:\Users\Moo\AppData\Local\Programs\Python\Python311\Lib\site-packages\gradio\blocks.py"", line 1561, in process_api
    result = await self.call_function(
             ^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""C:\Users\Moo\AppData\Local\Programs\Python\Python311\Lib\site-packages\gradio\blocks.py"", line 1179, in call_function
    prediction = await anyio.to_thread.run_sync(
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""C:\Users\Moo\AppData\Local\Programs\Python\Python311\Lib\site-packages\anyio\to_thread.py"", line 33, in run_sync
    return await get_asynclib().run_sync_in_worker_thread(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""C:\Users\Moo\AppData\Local\Programs\Python\Python311\Lib\site-packages\anyio\_backends\_asyncio.py"", line 877, in run_sync_in_worker_thread
    return await future
           ^^^^^^^^^^^^
  File ""C:\Users\Moo\AppData\Local\Programs\Python\Python311\Lib\site-packages\anyio\_backends\_asyncio.py"", line 807, in run
    result = context.run(func, *args)
             ^^^^^^^^^^^^^^^^^^^^^^^^
  File ""C:\Users\Moo\AppData\Local\Programs\Python\Python311\Lib\site-packages\gradio\utils.py"", line 678, in wrapper
    response = f(*args, **kwargs)
               ^^^^^^^^^^^^^^^^^^
  File ""C:\Users\Moo\Downloads\llm\nanollava\nanollava_gradio_demo.py"", line 46, in answer_question
    output_ids = model.generate(
                 ^^^^^^^^^^^^^^^
  File ""C:\Users\Moo\AppData\Local\Programs\Python\Python311\Lib\site-packages\torch\utils\_contextlib.py"", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File ""C:\Users\Moo\AppData\Local\Programs\Python\Python311\Lib\site-packages\transformers\generation\utils.py"", line 1575, in generate
    result = self._sample(
             ^^^^^^^^^^^^^
  File ""C:\Users\Moo\AppData\Local\Programs\Python\Python311\Lib\site-packages\transformers\generation\utils.py"", line 2697, in _sample
    outputs = self(
              ^^^^^
  File ""C:\Users\Moo\AppData\Local\Programs\Python\Python311\Lib\site-packages\torch\nn\modules\module.py"", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""C:\Users\Moo\AppData\Local\Programs\Python\Python311\Lib\site-packages\torch\nn\modules\module.py"", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""C:\Users\Moo\.cache\huggingface\modules\transformers_modules\qnguyen3\nanoLLaVA\4a1bd2e2854c6df9c4af831a408b14f7b035f4c0\modeling_llava_qwen2.py"", line 2267, in forward
    ) = self.prepare_inputs_labels_for_multimodal(
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""C:\Users\Moo\.cache\huggingface\modules\transformers_modules\qnguyen3\nanoLLaVA\4a1bd2e2854c6df9c4af831a408b14f7b035f4c0\modeling_llava_qwen2.py"", line 687, in prepare_inputs_labels_for_multimodal
    image_features = self.encode_images(images).to(self.device)
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""C:\Users\Moo\.cache\huggingface\modules\transformers_modules\qnguyen3\nanoLLaVA\4a1bd2e2854c6df9c4af831a408b14f7b035f4c0\modeling_llava_qwen2.py"", line 661, in encode_images
    image_features = self.get_model().mm_projector(image_features)
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""C:\Users\Moo\AppData\Local\Programs\Python\Python311\Lib\site-packages\torch\nn\modules\module.py"", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""C:\Users\Moo\AppData\Local\Programs\Python\Python311\Lib\site-packages\torch\nn\modules\module.py"", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""C:\Users\Moo\AppData\Local\Programs\Python\Python311\Lib\site-packages\torch\nn\modules\container.py"", line 217, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File ""C:\Users\Moo\AppData\Local\Programs\Python\Python311\Lib\site-packages\torch\nn\modules\module.py"", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""C:\Users\Moo\AppData\Local\Programs\Python\Python311\Lib\site-packages\torch\nn\modules\module.py"", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""C:\Users\Moo\AppData\Local\Programs\Python\Python311\Lib\site-packages\torch\nn\modules\linear.py"", line 116, in forward
    return F.linear(input, self.weight, self.bias)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument mat2 in method wrapper_CUDA_mm)

","Now if you've seen this before, and read through the error message, you might be able to tell me immediately, that ""oh obviously its running on a different thread and set_default_device() doesn't carry over to that thread"". This issue here is related to this. I'm not sure which version the fix applies to. But either way, since default is ""cpu"", if everything is on cpu that should be fine right?
So I modified the AutoModelForCausalLM call to use device_map='cpu', and printed the .device of both inputs and the model before executing model.generate:
<|im_start|>system
Answer the questions.<|im_end|><|im_start|>user
<image>
What do you see?<|im_end|><|im_start|>assistant

cpu
cpu
cpu

So what you need to do is figure out a way to execute set_default_device on that thread, or as seen here, you can try set_default_tensor_type, WHICH WORKS!!
# set device
torch.set_default_device('cuda')  # or 'cpu'
torch.set_default_tensor_type('torch.cuda.FloatTensor')

",multimodal
prediction logits using lxmert with hugging face library,"how can we get the prediction logits in the lxmert model using hugging face library? It's fairly easy to get in visualbert, but I'm not able to get it with the lxmert model. In case of visualbert model, the keys I'm getting are :
['prediction_logits', 'seq_relationship_logits', 'attentions']

and with the help of lxmert mode, the keys are :
['language_output', 'vision_output', 'pooled_output', 'language_attentions', 'vision_attentions', 'cross_encoder_attentions']

Even though there's a mention of prediction logits in the documentation I am not able to get them, if someone can help that would be great.
EDIT : Link to colab notebook for lxmert.
","Use LxmertForPreTraining instead of LxmertModel:
###Colab commands
#pip install transformers
#!git clone https://github.com/huggingface/transformers
#cd transformers
#cd examples/research_projects/lxmert
#pip install wget

from IPython.display import clear_output, Image, display
import PIL.Image
import io
import json
import torch
import numpy as np
from processing_image import Preprocess
from visualizing_image import SingleImageViz
from modeling_frcnn import GeneralizedRCNN
from utils import Config
import utils
import wget
import pickle
import os
import cv2
from copy import deepcopy

torch.cuda.is_available()

URL = ""https://github.com/jacobgil/vit-explain/raw/main/examples/both.png""

frcnn_cfg = Config.from_pretrained(""unc-nlp/frcnn-vg-finetuned"")
frcnn = GeneralizedRCNN.from_pretrained(""unc-nlp/frcnn-vg-finetuned"", config=frcnn_cfg)
image_preprocess = Preprocess(frcnn_cfg)

# run frcnn
images, sizes, scales_yx = image_preprocess(URL)
output_dict = frcnn(
    images,
    sizes,
    scales_yx=scales_yx,
    padding=""max_detections"",
    max_detections=frcnn_cfg.max_detections,
    return_tensors=""pt"",
)

# Very important that the boxes are normalized
normalized_boxes = output_dict.get(""normalized_boxes"")
features = output_dict.get(""roi_features"")

from transformers import LxmertTokenizer, LxmertForPreTraining
import torch

tokenizer = LxmertTokenizer.from_pretrained(""unc-nlp/lxmert-base-uncased"")
model = LxmertForPreTraining.from_pretrained(""unc-nlp/lxmert-base-uncased"")

text_sentence = ""dog and cat are in the room and "" + tokenizer.mask_token + "" is laying on the ground""

inputs = tokenizer(text_sentence, return_token_type_ids=True, return_attention_mask=True, add_special_tokens=True, return_tensors=""pt"")

visual_feats = features
visual_attention_mask = torch.ones(features.shape[:-1], dtype=torch.long)
visual_pos=normalized_boxes

inputs.update(
    {
        ""visual_feats"": visual_feats,
        ""visual_pos"": visual_pos,
        ""visual_attention_mask"": visual_attention_mask,
    }
)

model_outputs = model(**inputs, output_attentions=True)

model_outputs.keys()

Output:
odict_keys(['prediction_logits', 'cross_relationship_score', 'question_answering_score', 'language_attentions', 'vision_attentions', 'cross_encoder_attentions'])

P.S.: You can control the pertaining task heads via the configuration fields task_matched, task_mask_lm, task_obj_predict, and task_qa. I assume you are only interested in mask_lm following your comment. That means you should initialize your model as follows:
from transformers import LxmertConfig, LxmertForPreTraining

config = LxmertConfig.from_pretrained(""unc-nlp/lxmert-base-uncased"")
config.task_matched = False
config.task_obj_predict=False
config.task_qa= False
model = LxmertForPreTraining.from_pretrained(""unc-nlp/lxmert-base-uncased"", config=config)

",multimodal
get contrastive_logits_per_image with flava model using huggingface library,"I have used a code of Flava model from this link:
https://huggingface.co/docs/transformers/model_doc/flava#transformers.FlavaModel.forward.example

But I am getting the following error:
'FlavaModelOutput' object has no attribute 'contrastive_logits_per_image'

I tried using FlavaForPreTraining model instead, so updated code was :
from PIL import Image
import requests
from transformers import FlavaProcessor, FlavaForPreTraining

model = FlavaForPreTraining.from_pretrained(""facebook/flava-full"")
processor = FlavaProcessor.from_pretrained(""facebook/flava-full"")

url = ""http://images.cocodataset.org/val2017/000000039769.jpg""
image = Image.open(requests.get(url, stream=True).raw)

inputs = processor(text=[""a photo of a cat""], images=image, return_tensors=""pt"", padding=True, return_codebook_pixels = True)

inputs.update(
    {
        ""input_ids_masked"": inputs.input_ids,
    }
)

outputs = model(**inputs)

logits_per_image = outputs.contrastive_logits_per_image  # this is the image-text similarity score
probs = logits_per_image.softmax(dim=1)  # we can take the softmax to get the label probabilities

but I'm still getting this as error:
/usr/local/lib/python3.7/dist-packages/transformers/modeling_utils.py:714: FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.
  ""The `device` argument is deprecated and will be removed in v5 of Transformers."", FutureWarning

---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
<ipython-input-44-bdb428b8184a> in <module>()
----> 1 outputs = model(**inputs)

2 frames
/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py in _call_impl(self, *input, **kwargs)
   1128         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks
   1129                 or _global_forward_hooks or _global_forward_pre_hooks):
-> 1130             return forward_call(*input, **kwargs)
   1131         # Do not call functions when jit is used
   1132         full_backward_hooks, non_full_backward_hooks = [], []

/usr/local/lib/python3.7/dist-packages/transformers/models/flava/modeling_flava.py in forward(self, input_ids, input_ids_masked, pixel_values, codebook_pixel_values, attention_mask, token_type_ids, bool_masked_pos, position_ids, image_attention_mask, skip_unmasked_multimodal_encoder, mlm_labels, mim_labels, itm_labels, output_attentions, output_hidden_states, return_dict, return_loss)
   1968             if mim_labels is not None:
   1969                 mim_labels = self._resize_to_2d(mim_labels)
-> 1970                 bool_masked_pos = self._resize_to_2d(bool_masked_pos)
   1971                 mim_labels[bool_masked_pos.ne(True)] = self.ce_ignore_index
   1972 

/usr/local/lib/python3.7/dist-packages/transformers/models/flava/modeling_flava.py in _resize_to_2d(self, x)
   1765 
   1766     def _resize_to_2d(self, x: torch.Tensor):
-> 1767         if x.dim() > 2:
   1768             x = x.view(x.size(0), -1)
   1769         return x

AttributeError: 'NoneType' object has no attribute 'dim'

Can anyone provide suggestions with what's going wrong?
","FLAVA's author here.
Can you please add the following arguments to your processor call:
return_codebook_pixels=True, return_image_mask=True

Here is an example colab if you want to see how to call FLAVA model: https://colab.research.google.com/drive/1c3l4r4cEA5oXfq9uXhrJibddwRkcBxzP?usp=sharing#scrollTo=xtkrSjfhCdv-
",multimodal
modelling and fitting bimodal lognormal distributions in a loop using lmfit,"I have been spending FAR too much time trying to figure this out - so time to seek help. I am attempting to use lmfit to fit two lognormals (a and c) as well as the sum of these two lognormals (a+c) to a size distribution. Mode a centers around x=0.2, y=1, mode c centers around x=1.2, y=<<<1. There are numerous size distributions (>200) which are all slightly different and are passed in to the following code from an outside loop. For this example, I have provided a real life distribution and have not included the loop. Hopefully my code is sufficiently annotated to allow understanding of what I am trying to achieve.
I must be missing some fundamental understanding of lmfit (spoiler alert - I'm not great at Maths either) as I have 2 problems:

the fits (a, c and a+c) do not accurately represent the data. Note how the fit (red solid line) diverts away from the data (blue solid line). I assume this is something to do with the initial guess parameters. I have tried LOTS and have been unable to get a good fit.
re-running the model with ""new"" best fit values (results2, results3) doesn't appear to significantly improve the fit at all. Why?

Example result using provided x and y data:

Here is one-I-made-earlier showing the type of fit I am after (produced using the older mpfit module, using different data than provided below and using unique initial best guess parameters (not in a loop). Excuse the legend format, I had to remove certain information):

Any assistance is much appreciated. Here is the code with an example distribution:
from lmfit import models
import matplotlib.pyplot as plt
import numpy as np

# real life data example
y = np.array([1.000000, 0.754712, 0.610303, 0.527856, 0.412125, 0.329689, 0.255756, 0.184424, 0.136819,
              0.102316, 0.078763, 0.060896, 0.047118, 0.020297, 0.007714, 0.010202, 0.008710, 0.005579,
              0.004644, 0.004043, 0.002618, 0.001194, 0.001263, 0.001043, 0.000584, 0.000330, 0.000179,
              0.000117, 0.000050, 0.000035, 0.000017, 0.000007])

x = np.array([0.124980, 0.130042, 0.135712, 0.141490, 0.147659, 0.154711, 0.162421, 0.170855, 0.180262,
              0.191324, 0.203064, 0.215738, 0.232411, 0.261810, 0.320252, 0.360761, 0.448802, 0.482528,
              0.525526, 0.581518, 0.658988, 0.870114, 1.001815, 1.238899, 1.341285, 1.535134, 1.691963,
              1.973359, 2.285620, 2.572177, 2.900414, 3.342739])

# create the joint model using prefixes for each mode
model = (models.LognormalModel(prefix='p1_') +
         models.LognormalModel(prefix='p2_'))

# add some best guesses for the model parameters
params = model.make_params(p1_center=0.1, p1_sigma=2, p1_amplitude=1,
                           p2_center=1, p2_sigma=2, p2_amplitude=0.000000000000001)

# bound those best guesses
# params['p1_amplitude'].min = 0.0
# params['p1_amplitude'].max = 1e5
# params['p1_sigma'].min = 1.01
# params['p1_sigma'].max = 5
# params['p1_center'].min = 0.01
# params['p1_center'].max = 1.0
# 
# params['p2_amplitude'].min = 0.0
# params['p2_amplitude'].max = 1
# params['p2_sigma'].min = 1.01
# params['p2_sigma'].max = 10
# params['p2_center'].min = 1.0
# params['p2_center'].max = 3

# actually fit the model
result = model.fit(y, params, x=x)

# ====================================
# ================================
# re-run using the best-fit params derived above
params2 = model.make_params(p1_center=result.best_values['p1_center'], p1_sigma=result.best_values['p1_sigma'],
                            p1_amplitude=result.best_values['p1_amplitude'],
                            p2_center=result.best_values['p2_center'], p2_sigma=result.best_values['p2_sigma'],
                            p2_amplitude=result.best_values['p2_amplitude'], )
# re-fit the model
result2 = model.fit(y, params2, x=x)

# ================================
# re-run again using the best-fit params derived above
params3 = model.make_params(p1_center=result2.best_values['p1_center'], p1_sigma=result2.best_values['p1_sigma'],
                            p1_amplitude=result2.best_values['p1_amplitude'],
                            p2_center=result2.best_values['p2_center'], p2_sigma=result2.best_values['p2_sigma'],
                            p2_amplitude=result2.best_values['p2_amplitude'], )
# re-fit the model
result3 = model.fit(y, params3, x=x)

# ================================
# add individual fine and coarse modes using the revised fit parameters
model_a = models.LognormalModel()
params_a = model_a.make_params(center=result3.best_values['p1_center'], sigma=result3.best_values['p1_sigma'],
                               amplitude=result3.best_values['p1_amplitude'])
result_a = model_a.fit(y, params_a, x=x)

model_c = models.LognormalModel()
params_c = model_c.make_params(center=result3.best_values['p2_center'], sigma=result3.best_values['p2_sigma'],
                               amplitude=result3.best_values['p2_amplitude'])
result_c = model_c.fit(y, params_c, x=x)

# ====================================
plt.plot(x, y, 'b-', label='data')
plt.plot(x, result.best_fit, 'r-', label='best_fit_1')
plt.plot(x, result.init_fit, 'lightgrey', ls=':', label='ini_fit_1')
plt.plot(x, result2.best_fit, 'r--', label='best_fit_2')
plt.plot(x, result2.init_fit, 'lightgrey', ls='--', label='ini_fit_2')
plt.plot(x, result3.best_fit, 'r.-', label='best_fit_3')
plt.plot(x, result3.init_fit, 'lightgrey', ls='--', label='ini_fit_3')

plt.plot(x, result_a.best_fit, 'grey', ls=':', label='best_fit_a')
plt.plot(x, result_c.best_fit, 'grey', ls='--', label='best_fit_c')
plt.xscale(""log"")
plt.yscale(""log"")
plt.legend()
plt.show()

","There are three main pieces of advice I can give:

initial values matter and should not be so far off as to make
portions of the model completely insensitive to the parameter
values.  Your initial model is sort of off by several orders of
magnitude.
always look at the fit result. This is the primary
result -- the plot of the fit is a representation of the actual
numerical results. Not showing that you printed out the fit
report is a good indication that you did not look at the actual
result. Really, always look at the results.
if you are judging the quality of the fit based on a plot of
the data and model, use how you choose to plot the data to guide
how you fit the data.  Specifically in your case, if you are
plotting on a log scale, then fit the log of the data to the log
of the model: fit in ""log space"".

Such a fit might look like this:
from lmfit import models, Model
from lmfit.lineshapes import lognormal
import matplotlib.pyplot as plt
import numpy as np


y = np.array([1.000000, 0.754712, 0.610303, 0.527856, 0.412125, 0.329689, 0.255756, 0.184424, 0.136819,
              0.102316, 0.078763, 0.060896, 0.047118, 0.020297, 0.007714, 0.010202, 0.008710, 0.005579,
              0.004644, 0.004043, 0.002618, 0.001194, 0.001263, 0.001043, 0.000584, 0.000330, 0.000179,
              0.000117, 0.000050, 0.000035, 0.000017, 0.000007])

x = np.array([0.124980, 0.130042, 0.135712, 0.141490, 0.147659, 0.154711, 0.162421, 0.170855, 0.180262,
              0.191324, 0.203064, 0.215738, 0.232411, 0.261810, 0.320252, 0.360761, 0.448802, 0.482528,
              0.525526, 0.581518, 0.658988, 0.870114, 1.001815, 1.238899, 1.341285, 1.535134, 1.691963,
              1.973359, 2.285620, 2.572177, 2.900414, 3.342739])

# use a model that is the log of the sum of two log-normal functions
# note to be careful about log(x) for x < 0.
def log_lognormal(x, amp1, cen1, sig1, amp2, cen2, sig2):
    comp1 = lognormal(x, amp1, cen1, sig1)
    comp2 = lognormal(x, amp2, cen2, sig2)
    total = comp1 + comp2
    total[np.where(total<1.e-99)] = 1.e-99
    return np.log(comp1+comp2)

model = Model(log_lognormal)
params = model.make_params(amp1=5.0, cen1=-4, sig1=1,
                           amp2=0.1, cen2=-1, sig2=1)

# part of making sure that the lognormals are strictly positive 
params['amp1'].min = 0
params['amp2'].min = 0

result = model.fit(np.log(y), params, x=x)
print(result.fit_report())      # <-- HERE IS WHERE THE RESULTS ARE!!

# also, make a plot of data and fit
plt.plot(x, y, 'b-', label='data')
plt.plot(x, np.exp(result.best_fit), 'r-', label='best_fit')
plt.plot(x, np.exp(result.init_fit), 'grey',  label='ini_fit')
plt.xscale(""log"")
plt.yscale(""log"")
plt.legend()
plt.show()

This will print out
[[Model]]
    Model(log_lognormal)
[[Fit Statistics]]
    # fitting method   = leastsq
    # function evals   = 211
    # data points      = 32
    # variables        = 6
    chi-square         = 0.91190970
    reduced chi-square = 0.03507345
    Akaike info crit   = -101.854407
    Bayesian info crit = -93.0599914
[[Variables]]
    amp1:  21.3565856 +/- 193.951379 (908.16%) (init = 5)
    cen1: -4.40637490 +/- 3.81299642 (86.53%) (init = -4)
    sig1:  0.77286862 +/- 0.55925566 (72.36%) (init = 1)
    amp2:  0.00401804 +/- 7.5833e-04 (18.87%) (init = 0.1)
    cen2: -0.74055538 +/- 0.13043827 (17.61%) (init = -1)
    sig2:  0.64346873 +/- 0.04102122 (6.38%) (init = 1)
[[Correlations]] (unreported correlations are < 0.100)
    C(amp1, cen1) = -0.999
    C(cen1, sig1) = -0.999
    C(amp1, sig1) = 0.997
    C(cen2, sig2) = -0.964
    C(amp2, cen2) = -0.940
    C(amp2, sig2) = 0.849
    C(sig1, amp2) = -0.758
    C(cen1, amp2) = 0.740
    C(amp1, amp2) = -0.726
    C(sig1, cen2) = 0.687
    C(cen1, cen2) = -0.669
    C(amp1, cen2) = 0.655
    C(sig1, sig2) = -0.598
    C(cen1, sig2) = 0.581
    C(amp1, sig2) = -0.567

and generate a plot like

",multimodal
what method and tool for regression analysis for a multimodal distribution in r,"I have a set of variables X1 and X2 and Y with relationship plot as shown below. X2 values are used for color coding.
X1, X2, and X3 are integer variables.

The observed pattern is multimodal.
What is the best way to predict Y based on X1 and X2?
Can we use non-linear or hurdle models for this?
Also what are the tools available to achieve this in R?
","Generally speaking, there is no need to worry about the distribution of the response. Although you are showing a bivariate plot, it is possible that the multi-modality is explained by X2 (or other, missing variables)
It is the distribution of the model residuals that matters (if it matters at all).
If the residuals are non-normal, then certain inferences may be invalid, although this may not be a problem at all if the model is used for prediction.
If you really do have a curvilinear association then you could consider:

transformations
non-linear terms
splines
generalised additive models (GAMs)
non-linear models

Of course, if the underlying problem is that you have missing explanatory variables, then some of these approaches may lead to an overfitted model.
",multimodal
multistart in openmdao,"Is there any straightforward way to use a multi-start option for solving optimization problems in OpenMDAO? I am currently using the SLSQP driver of ScipyOptimize, and would like to start the search from multiple initial points (either in a sequence of in parallel, although that is not necessary), and choose the best of the results, to increase confidence that the minimum found is a good minimum (or likely global). Is there any clever way to achieve this in OpenMDAO rather than implementing a loop ""by hand""?
","The ""by hand"" method (i.e. putting a for-loop around a call to run_driver()) is the way to do it. No fancy tricks needed.
One feature  you might find helpful is the manual case recording at the problem level. At the end of each point, you can manually record a case with ALL the variables in it. You get to name that case yourself, so you can give it a descriptive name if keeping track of which point is which would be useful to you.
",multimodal
how to use the modal in the list in react native a specific modal for each list item,"I made a customized list component (in React Native) which shows touchable images with some description texts.
I need each images open a specific Modal; but I don't  know how!! where & how I should code the Modal??
... here is my photo list component:
export class CustomGallery extends Component {

  render() {
    let {list} = this.props;
    return (
      <View style={styles.container}>
        <FlatList
          numColumns={4}
          data={list}
          renderItem={({ item}) => (
            <View style={styles.views}>
              <TouchableOpacity style={styles.touch} >
                <ImageBackground
                  style={styles.img}
                  source={{ uri: item.photo }}
                >
                  <Text style={styles.txt}>{item.name}</Text>
                  <Text style={styles.txt}>{item.key}</Text>
                  <Text style={styles.txt}>{item.describtion}</Text>
                </ImageBackground>
              </TouchableOpacity>
            </View>
          )}
        />
      </View>
    );
  }
}

","For Modal you can use modal from material-ui - https://material-ui.com/components/modal/
The Modal component renders its children node infront of a backdrop component. Simple and basic example would be like a confirmation message that pops up asking whether you surely want to delete particular information or not.
From your code I am guessing you want to display information regarding the image using modal when you click on the image.
Here I have added Modal component:
import React from 'react';
import Modal from '@material-ui/core/Modal';

export class CustomGallery extends Component {
    constructor() {
        super();
        this.state = {
          modalOpen: false,
          snackOpen: false,
          modalDeleteOpen: false,
        };
      }

      
  handleModalOpen = () => {
    this.setState({ modalOpen: true });
  }

  handleModalClose = () => {
    this.setState({ modalOpen: false });
  }

    render() {
      let {list} = this.props;
      return (
        <View style={styles.container}>
          <FlatList
            numColumns={4}
            data={list}
            renderItem={({ item}) => (
              <View style={styles.views}>
                <TouchableOpacity style={styles.touch} >
                   
                    <ImageBackground
                    style={styles.img}
                    onClick={() => this.handleModalOpen()}
                    >
                    { item.photo }
                    </ImageBackground>
                    <Modal
                     open={this.state.modalOpen}
                     onClose={this.handleModalClose}
                     closeAfterTransition
                    >
                        <Text style={styles.txt}>{item.name}</Text>
                        <Text style={styles.txt}>{item.key}</Text>
                        <Text style={styles.txt}>{item.describtion}</Text>
                    </Modal>
                </TouchableOpacity>
              </View>
            )}
          />
        </View>
      );
    }
  }

",multimodal
plot unimodal distributions determined from a multimodal distribution,"I've used GaussianMixture to analyze a multimodal distribution. From the GaussianMixture class I can access the means and covariances using the attributes means_ and covariances_. How can I use them to now plot the two underlying unimodal distributions?
I thought of using scipy.stats.norm but I don't know what to select as parameters for loc and scale. The desired output would be analogously as shown in the attached figure.

The example code of this question was modified from the answer here.
import numpy as np
import matplotlib.pyplot as plt
from sklearn import mixture
from scipy.stats import norm

ls = np.linspace(0, 60, 1000)
multimodal_norm = norm.pdf(ls, 0, 5) + norm.pdf(ls, 20, 10)
plt.plot(ls, multimodal_norm)

# concatenate ls and multimodal to form an array of samples
# the shape is [n_samples, n_features]
# we reshape them to create an additional axis and concatenate along it
samples = np.concatenate([ls.reshape((-1, 1)), multimodal_norm.reshape((-1,1))], axis=-1)
print(samples.shape)

gmix = mixture.GaussianMixture(n_components = 2, covariance_type = ""full"")
fitted = gmix.fit(samples)

print(fitted.means_)
print(fitted.covariances_)

# The idea is something like the following (not working):
new_norm1 = norm.pdf(ls, fitted.means_, fitted.covariances_)
new_norm2 = norm.pdf(ls, fitted.means_, fitted.covariances_)
plt.plot(ls, new_norm1, label='Norm 1')
plt.plot(ls, new_norm2, label='Norm 2')

","It is not entirely clear what you are trying to accomplish. You are fitting a GaussianMixture model to the concatenation of the sum of the values of pdfs of two gaussians sampled on a uniform grid, and the unifrom grid itself. This is not how a Gaussian Mixture model is meant to be fitted. Typically one fits a model to random observations drawn from some distribution (typically unknown but could be a simulated one).
Let me assume that you want to fit the GaussianMixture model to a sample drawn from a Gaussian Mixture distribution. Presumably to test how well the fit works given you know what the expected outcome is. Here is the code for doing this, both to simulate the right distribution and to fit the model. It prints the parameters that the fit recovered from the sample -- we observe that they are indeed close to the ones we used to simulate the sample. Plot of the density of the GaussianMixture distribution that fits to the data is generated at the end
import numpy as np
import matplotlib.pyplot as plt
from sklearn import mixture
from scipy.stats import norm

# set simulation parameters
mean1, std1, w1 = 0,5,0.5
mean2, std2, w2 = 20,10,1-w1

# simulate constituents
n_samples = 100000
np.random.seed(2021)
gauss_sample_1 = np.random.normal(loc = mean1,scale = std1,size = n_samples)
gauss_sample_2 = np.random.normal(loc = mean2,scale = std2,size = n_samples)
binomial = np.random.binomial(n=1, p=w1, size = n_samples)

# simulate gaussian mixture
mutlimodal_samples = (gauss_sample_1 * binomial + gauss_sample_2 * (1-binomial)).reshape(-1,1)

# define and fit the mixture model
gmix = mixture.GaussianMixture(n_components = 2, covariance_type = ""full"")
fitted = gmix.fit(mutlimodal_samples)

print('fitted means:',fitted.means_[0][0],fitted.means_[1][0])
print('fitted stdevs:',np.sqrt(fitted.covariances_[0][0][0]),np.sqrt(fitted.covariances_[1][0][0]))
print('fitted weights:',fitted.weights_)

# Plot component pdfs and a joint pdf
ls = np.linspace(-50, 50, 1000)
new_norm1 = norm.pdf(ls, fitted.means_[0][0], np.sqrt(fitted.covariances_[0][0][0]))
new_norm2 = norm.pdf(ls, fitted.means_[1][0], np.sqrt(fitted.covariances_[1][0][0]))
multi_pdf = w1*new_norm1 + (1-w1)*new_norm2
plt.plot(ls, new_norm1, label='Norm pdf 1')
plt.plot(ls, new_norm2, label='Norm pdf 2')
plt.plot(ls, multi_pdf, label='multi-norm pdf')
plt.legend(loc = 'best')
plt.show()


The results are
fitted means: 22.358448018824642 0.8607494960575028
fitted stdevs: 8.770962351118127 5.58538485134623
fitted weights: [0.42517515 0.57482485]

as we see they are close (up to their ordering, which of course the model cannot recover but it is irrelevant anyway) to what went into the simulation:
mean1, std1, w1 = 0,5,0.5
mean2, std2, w2 = 20,10,1-w1

And the plot of the density and its parts. Recall that the pdf of the GaussianMixture is not the sum of the pdfs but a weighted average with weights w1, 1-w1:

",multimodal
how to define kronecker product layer of 2 keras layers of shape none 4096 is performed,"Let's say there are 2 different/separate keras layers,
encoder_1 & encoder_2 with both having output shape of (None, 4096).
Now how to define keras multiply layer which gives (None, 4096, 4096) as it's output shape.
Is this same as Kronecker product?
If not the same please show how to implement Kronecker product of 2 layers named, encoder_1 & encoder_2?
","So you should be able to achieve this simply using either the Dot layer or dot method of Keras, after inserting dimensions of length 1:
import tensorflow as tf
from tensorflow.keras.layers import dot

encoder_1 = tf.expand_dims(encoder_1, axis=2)
encoder_2 = tf.expand_dims(encoder_2, axis=1)
outer = dot([encoder_1, encoder_2], axes=(2, 1))

outer should be a tensor of shape (None, 4096, 4096).
",multimodal
markdown with code blocks appearing as frontend ui issue in chatgpt responses,"I’m encountering a problem when using ChatGPT to generate Markdown that includes code blocks. Instead of returning proper Markdown output, the content appears to break and render as part of the frontend UI. This is particularly problematic when I need the response in strict Markdown format for documentation purposes.
Example Issue:
I ask ChatGPT for Markdown output like this:
Instead of receiving this as clean Markdown, the response appears to break into a frontend-rendered format where the code sections lose their Markdown formatting.
Attempts to Resolve:

Reformatted the input request to clarify I need the response in Markdown, but the problem persists.
Checked for any embedded rendering settings that might be triggering this, but couldn’t find any.

Expected Behavior:
Markdown output with properly formatted code blocks, such as:
**Description**: Authenticate users and provide a JWT token.  
**Required Role**: Public (No authentication required)  
**Request Body**:  

{
  ""email"": ""user@example.com"",
  ""password"": ""password123""
}


**Response**:
{
  ""token"": ""jwt-token-string"",
  ""user"": {
    ""id"": ""UUID"",
    ""email"": ""user@example.com""
  }
}


Actual Behavior:
The Markdown structure is broken, and the response seems to mix in UI rendering, which makes it unusable in raw Markdown format.

Question:
Has anyone else faced this issue with ChatGPT or other Markdown generators? Is there a specific prompt I should use to ensure the output is pure Markdown? Are there ways to handle this problem effectively, or is this a limitation of the AI?
Any help would be appreciated!
","It's because ChatGPT is wrapping the whole Markdown code into a codeblocks using ```, so the closing ``` in the code close the outer codeblock. To prevent this, you can ask ChatGPT to wrap the Markdown code into ~~~ instead, this will make a codeblock, but since it starts with a different ""tag"" than those in it, it won't break the rendering.
",prompt
how can i read a single line user input with bunjs,"Bun.js has a nice native API to read periodical user input:
const prompt = ""Type something: "";
process.stdout.write(prompt);
for await (const line of console) {
  console.log(`You typed: ${line}`);
  process.stdout.write(prompt);
}

But...
I want to read user inputs, outside a loop!

I found this solution:
const stdinIterator = process.stdin.iterator()

console.write('What is your first name?\n> ')
const firstName = (await stdinIterator.next()).value.toString().trimEnd()
console.log('Hello,', firstName)

console.write('What is your last name?\n> ')
const lastName = (await stdinIterator.next()).value.toString().trimEnd()
console.log('Welcome', `${firstName} ${lastName}`)

It works. But then the process is not terminated automatically (I need to press Ctrl+C manually).
","Bun.js has implemented prompt API.
Use it and enjoy:
const firstName = prompt('What is your first name?')
console.log('Hello,', firstName)

const lastName = prompt('What is your last name?')
console.log('Welcome', `${firstName} ${lastName}`)

See: https://bun.sh/docs/api/globals
",prompt
how to hide the hostname on the terminal on linux,"Is there a way I can hide the hostname on the terminal without updating the /etc/hosts or related files.
Generally we see the server details on screen as below
[root@ServernamE /]#

So, I don't want the hostname = ServernamE to be displayed on terminal. The reason for doing this is, I will be recording a web session. I know that I can morph the desired part but that is time consuming and depend on other softwares.
Thanks
","If you want a temporary solution(since you are talking about a screencast), you can set your PS1 variable to change the prompt.
For example, if you want your prompt to be:
$

Then set your PS1 variable as follows on the terminal:
export PS1='$ '

Likewise, you can have it to whatever you want the prompt to look like. If you want the path to be displayed, set it as:
export PS1='\w '

For a permanent solution, you can set this in your shell configuration script, which is your ~/.bashrc file in case you have bash as your shell.
",prompt
pydanticusererror if you use root_validator with prefalse the default you must specify skip_on_failuretrue,"I want to execute this code in google colab but I get following error:
from llama_index.prompts.prompts import SimpleInputPrompt

# Create a system prompt 
system_prompt = """"""[INST] <>
more string here.<>
""""""

query_wrapper_prompt = SimpleInputPrompt(""{query_str} [/INST]"")

Error:
/usr/local/lib/python3.10/dist-packages/pydantic/_internal/_config.py:269: UserWarning: Valid config keys have changed in V2:
* 'allow_population_by_field_name' has been renamed to 'populate_by_name'
  warnings.warn(message, UserWarning)
---------------------------------------------------------------------------
PydanticUserError                         Traceback (most recent call last)
<ipython-input-36-c45796b371fe> in <cell line: 3>()
      1 # Import the prompt wrapper...
      2 # but for llama index
----> 3 from llama_index.prompts.prompts import SimpleInputPrompt
      4 # Create a system prompt
      5 system_prompt = """"""[INST] <>

6 frames
/usr/local/lib/python3.10/dist-packages/pydantic/deprecated/class_validators.py in root_validator(pre, skip_on_failure, allow_reuse, *__args)
    226     mode: Literal['before', 'after'] = 'before' if pre is True else 'after'
    227     if pre is False and skip_on_failure is not True:
--> 228         raise PydanticUserError(
    229             'If you use `@root_validator` with pre=False (the default) you MUST specify `skip_on_failure=True`.'
    230             ' Note that `@root_validator` is deprecated and should be replaced with `@model_validator`.',

PydanticUserError: If you use `@root_validator` with pre=False (the default) you MUST specify `skip_on_failure=True`. Note that `@root_validator` is deprecated and should be replaced with `@model_validator`.

For further information visit https://errors.pydantic.dev/2.1.1/u/root-validator-pre-skip

If I follow  the link, there is no solution for my case.
How can I solve that problem?
Thanks in forward.
","In my env, I have
pip list | grep pydantic
pydantic                     2.2.1

I fix the problem, by downgrading pydantic version
pip install pydantic==1.10.9

",prompt
bash prompt with the last exit code,"I've been trying to customize my Bash prompt so that it will look like
[feralin@localhost ~]$ _

with colors. I managed to get constant colors (the same colors every time I see the prompt), but I want the username ('feralin') to appear red, instead of green, if the last command had a nonzero exit status. I came up with:
\e[1;33m[$(if [[ $? == 0  ]]; then echo ""\e[0;31m""; else echo ""\e[0;32m""; fi)\u\e[m@\e[1;34m\h \e[0;35m\W\e[1;33m]$ \e[m

However, from my observations, the $(if ...; fi) seems to be evaluated once, when the .bashrc is run, and the result is substituted forever after. This makes the name always green, even if the last exit code is nonzero (as in, echo $?). Is this what is happening? Or is it simply something else wrong with my prompt? Long question short, how do I get my prompt to use the last exit code?
","As you are starting to border on a complex PS1, you might consider using PROMPT_COMMAND. With this, you set it to a function, and it will be run after each command to generate the prompt.
You could try the following in your ~/.bashrc file:
PROMPT_COMMAND=__prompt_command    # Function to generate PS1 after CMDs

__prompt_command() {
    local EXIT=""$?""                # This needs to be first
    PS1=""""

    local RCol='\[\e[0m\]'

    local Red='\[\e[0;31m\]'
    local Gre='\[\e[0;32m\]'
    local BYel='\[\e[1;33m\]'
    local BBlu='\[\e[1;34m\]'
    local Pur='\[\e[0;35m\]'

    if [ $EXIT != 0 ]; then
        PS1+=""${Red}\u${RCol}""        # Add red if exit code non 0
    else
        PS1+=""${Gre}\u${RCol}""
    fi

    PS1+=""${RCol}@${BBlu}\h ${Pur}\W${BYel}$ ${RCol}""
}

This should do what it sounds like you want. Take a look a my bashrc's sub file if you want to see all the things I do with my __prompt_command function.
",prompt
is there a way to place the prompt cursor at the bottom of the console,"In MSDOS, I wrote a prompt (%PROMPT%) that output a reverse color (white) line at the top of the console with information like MSDOS version and time/date. Then the prompt text ($P$G) and cursor moved to the bottom of the console.  Standard output color was Cyan while the prompt string itself was white.
I don't care about a ""title"" since all windows now have titles. I don't care about the color of the output. I do want the prompt to always appear at the bottom of the console (even after clearing the screen or whatever).
I tried this:
function prompt
{
    $windowHeigth = [Console]::WindowHeight
    $windowWidth  = [Console]::WindowWidth
    [Console]::SetCursorPosition(0, $windowHeigth-1)

    ""PS $($ExecutionContext.SessionState.Path.CurrentLocation)$('>' * ($NestedPromptLevel + 1)) ""
}

This does place the prompt at the bottom of the console, but when I clear the screen it reverts to the default style prompt location. I can't find out how to find the coordinate of the first row of the last line in the console.
","After some experimentation this is what I came up with:
function prompt
{
    $rows = [Console]::WindowHeight
    $setCursorBottom = ""`e[${rows};0H""
    ""${setCursorBottom}PS $($executionContext.SessionState.Path.CurrentLocation)$('>' * ($nestedPromptLevel + 1)) ""
}

",prompt
color for the prompt just the prompt proper in cmdexe and powershell,"So in Bash you just configure PS1 to add colors to your prompt. I'm talking about the prompt proper, not the color of the foreground (text) or the background. And it's really easy in Bash and it helps a lot if you need to find your commands in a sea of messy text output.
Can you achieve the same for cmd.exe, or as a fallback, for PowerShell? A colored prompt?
I don't know if it could be done in the old days before Win32 by loading ANSI.SYS. I think that was just to make the foreground and the background colorful. But I might be wrong. And anyway, those days are gone, and in our modern times (I know), we're using cmd.exe, or PowerShell.
I know both cmd.exe and PowerShell are capable of colored output. For cmd.exe, just run color /? to find out. But my question is not about the foreground and the background, that's all known to humankind - it's about just changing the prompt color for cmd.exe, probably via the PROMPT environment variable as via the PS1 variable for Bash? Is it possible?
And no, Cygwin is not an alternative for this. I'm a Cygwin user with MinTTY and all, and I love it. But I still want my cmd.exe prompt colored, too.
","Building on @KriZ's answer, the ANSI escape sequences work perfectly in Windows 10 cmd.exe as of 2019. Didn't need to explicitly call out ansi.sys or copy any files. It just worked out of the box in Windows 10.
For example,
set PROMPT=$E[1;37m[user@machine:$E[1;35m$P ]$$ $E[1;37m

Produces: 

(Notice the space after the final $)
Everything before the drive is colored in bold white and the drive/folder is bold pink, and everything after the final $ is bold white.
The format for the colors is:
$E[bold_or_not;colorm

With m always following the color number. bold_or_not = 0 or 1. Here's a guide for the colors:

0     Turn Off Attributes
1     High Intensity
2     Normal Intensity
4     Underline (mono only)
5     Blink
7     Reverse Video
8     Invisible
30    Black
31    Red
32    Green
33    Yellow
34    Blue
35    Magenta
36    Cyan
37    White
40    Black
41    Red
42    Green
43    Yellow
44    Blue
45    Magenta
46    Cyan
47    White


Colors Source: https://kb.iu.edu/d/aamm
",prompt
how to make sure user prompted response equals at least one value from a provided vector,"I am developing a QC check script for my job. I want it to be all within one function, and therefore prompts are asked to the user to determine the arguments used within the function. Here is an example of a splice of function I have developed.
askforQC_type <- \(qctype){
  answer <- readline(""What QC types would you like to run?"") 
  qualitycontroltypes <- c('BLK', 'DDL', 'DUP', 'FBLK', 'LCS', 'LCSD', 'LDUP', 'LMB', 'MS', 'MSD', 'NO3', 'RNS', 'SO4', 'SPI', 'SPL', 'TBLK')
  if(answer==qualitycontroltypes) {
    writeLines(paste(""QC types chosen:"", qctype))
  } else {
      writeLines(paste(""Sorry,"", qctype, ""is not an applicable QC type""))
    }
  }

I am wanting the user to provide an answer that at least fits one of the values within the qualitycontroltypes vector. If it does not fit any of those values, I want it to spit out the message. Then, I want to use the values provided by the user to tell the script to paste those values into a subset function later in my script.
Preferable if the user can answer in a format such as BLK, LCS, LDUP, etc. without needing to put """" or c(...) in the answer. Also, if ALL is entered by the user, then run the script using all values in the qualitycontroltypes vector. I am trying to make the script as simple as possible for the user in case they do not know how to operate r at all. Thanks.
UPDATE
As @MrFlick pointed out, the ""answer %in% qualitycontroltypes is the correct way to go about this. Now what I need to figure out it how to let the user provide a comma and space before each of the answers until the last answer provided. I think this is what @Limey is getting at in his comment. Not sure how to do this though.
","Here is what I think you are looking for with regard to user input:
askforQC_type <- \(qctype){
  
  assign(""qctype"",readline(""What QC types would you like to run?""))
  qctype <- c(unlist(strsplit(qctype,split = c("",""))))
  qctype <- trimws(qctype)
  qualitycontroltypes <- c('BLK', 'DDL', 'DUP', 'FBLK', 'LCS', 'LCSD', 'LDUP', 'LMB', 'MS', 'MSD', 'NO3', 'RNS', 'SO4', 'SPI', 'SPL', 'TBLK')
  if(all(qctype %in% qualitycontroltypes) == TRUE) {
    writeLines(paste(""QC types chosen:"", qctype))
  } else if(qctype == ""ALL""){
    writeLines(paste(""QC types chosen:"", qualitycontroltypes))
  } else {
    writeLines(paste(""Sorry,"", qctype, ""contains a QC type that is not applicable""))
  }
  
}


askforQC_type()

What QC types would you like to run? DDL,DUP

QC types chosen: DDL
QC types chosen: DUP

What QC types would you like to run? ALL
QC types chosen: BLK
QC types chosen: DDL
QC types chosen: DUP
QC types chosen: FBLK
QC types chosen: LCS
QC types chosen: LCSD
QC types chosen: LDUP
QC types chosen: LMB
QC types chosen: MS
QC types chosen: MSD
QC types chosen: NO3
QC types chosen: RNS
QC types chosen: SO4
QC types chosen: SPI
QC types chosen: SPL
QC types chosen: TBLK

What I did was to use the assign() function to assign the user input value(s) to qctype, then split that into a vector string using strsplit() and unlist() and then remove any unwanted whitespace using trimws().
Then, in the if statement, I used the all() function to check if qctype %in% qualitycontroltypes returns TRUE in case a user supplies more than one input value. If that condition is met, then the program runs as expected.
I hope this helps!
",prompt
to get a prompt which indicates gitbranch in zsh,"I run the following codes separately as my prompt unsuccessfully in .zshrc. This suggests me that apparently I do not have a program called __git_ps1. It is not in MacPorts.
#1
PROMPT=""$(__git_ps1 "" \[\033[1;32m\] (%s)\[\033[0m\]"")\$""$

#2
PROMPT=""$(__git_ps1 "" (%s)"")\$""$

#3
# Get the name of the branch we are on
git_prompt_info() {
  branch_prompt=$(__git_ps1)
  if [ -n ""$branch_prompt"" ]; then
    status_icon=$(git_status)
    echo $branch_prompt $status_icon
  fi
}

# Show character if changes are pending
git_status() {
  if current_git_status=$(git status | grep 'added to commit' 2> /dev/null); then
    echo ""☠""
  fi
}
autoload -U colors
colors
setopt prompt_subst
PROMPT='
%~%{$fg_bold[black]%}$(git_prompt_info)
→ %{$reset_color%}'

How can you get a prompt which shows the name of a Git-branch?
","__git_ps1 is from git-completion.bash. In zsh you probably have to provide your own function to determine the current directories git branch. There are quite a few blog posts about a git prompt for zsh.
You just need:

a function to provide the branch name
enable prompt (command) substitution
add the function to your prompt

For example
git_prompt() {
 ref=$(git symbolic-ref HEAD | cut -d'/' -f3)
 echo $ref
}
setopt prompt_subst
PS1=$(git_prompt)%#
autoload -U promptinit
promptinit

Update: use the zsh vcs_info module instead of git_prompt()
setopt prompt_subst
autoload -Uz vcs_info
zstyle ':vcs_info:*' actionformats \
    '%F{5}(%f%s%F{5})%F{3}-%F{5}[%F{2}%b%F{3}|%F{1}%a%F{5}]%f '
zstyle ':vcs_info:*' formats       \
    '%F{5}(%f%s%F{5})%F{3}-%F{5}[%F{2}%b%F{5}]%f '
zstyle ':vcs_info:(sv[nk]|bzr):*' branchformat '%b%F{1}:%F{3}%r'

zstyle ':vcs_info:*' enable git cvs svn

# or use pre_cmd, see man zshcontrib
vcs_info_wrapper() {
  vcs_info
  if [ -n ""$vcs_info_msg_0_"" ]; then
    echo ""%{$fg[grey]%}${vcs_info_msg_0_}%{$reset_color%}$del""
  fi
}
RPROMPT=$'$(vcs_info_wrapper)'

",prompt
automating a script for easier  future use,"New user to R here. I have just completed a quality control script for some 2023 data I have for my job. I have attached the script below. What I want to do is make this a more automated process so someone in the future can easily use this script. The only thing that should be changing is the year selected. Ideally (not sure if this is possible / relatively easy) what I would want is to open the script, have a prompt asking for the year they are wanting to do QC on (i.e. enter for example 2021, 2019, etc.) and then run the script, plugging in the year into the appropriate section of the code (which I have put in asterix). I am new to R so not sure how easy this is to go about, but any help would be much much appreciated (along with some explanation if you don't mind).
# QC Check Function Building Blocks ---------------------------------------

#Bring in QC Results, QC Samples, and Results Tables and Filter Out Unneeded Columns
fn.importData(MDBPATH=""C:/Users/h2edhmrs/Desktop/DASLER_TEST_COPY.mdb"",
              TABLES=c(""Analytes""))

""QC-Samples"" <- `QC Samples` %>% select(LOC_ID, QC_SAMPLE, SAMPLE_DEPTH, QC_TYPE, ASSOC_SAMP)
""QC-Results"" <- `QC Results` %>%  select(Loc_ID, QC_Sample, Units, Value, Text_Value, QC_Type, Storet_Num)
'Results_' <- `Results` %>% select(Loc_ID, Sample_Num, Units, Value, Storet_Num, Text_Value)

**#Only get 2023 records from QC Samples table
'QC-Samples' <- `QC-Samples` %>% filter(substr(QC_SAMPLE,1,4)==""2023"")**

#Rename QC-Results QC_Sample column to match name in QC-Samples table
colnames(`QC-Results`)[2] <- 'QC_SAMPLE'

#Merge Results and Samples table to get full 2023 QC records
QC_Results <- merge(`QC-Results`, `QC-Samples`[ ,c(""QC_SAMPLE"", ""ASSOC_SAMP"")], by = ""QC_SAMPLE"")

#Now must get associated samples into table
#To do this, I will rename ""Sample_Num"" column in Results_ table to ASSOC_SAMP and then merge the two
colnames(Results_)[2] <- ""ASSOC_SAMP""
QCandResults <- merge(QC_Results, Results_[,c(""ASSOC_SAMP"",""Storet_Num"", ""Units"", ""Value"", ""Text_Value"")], by = c(""ASSOC_SAMP"", ""Storet_Num""))

#rename columns of QCandResults for clarity
colnames(QCandResults)[c(1,5,6,7,9,10,11)] <- c(""Sample_Num"", ""Units_QC"", ""Value_QC"", ""Text_Value_QC"", ""Units_Results"", ""Value_Results"", ""Text_Value_Results"")

#matching Storet_num to display the parameter name in the QCandResults table
colnames(Analytes)[1] <- ""Storet_Num""
QCandResults <- (merge(Analytes[,c(""Storet_Num"", ""anl_short"")], QCandResults, by = ""Storet_Num""))
QCandResults <- QCandResults[-1]

#Making only dups and splits in the table
QCandResults <- subset(QCandResults, QC_Type ==  c(""DUP"", ""SPL""))


# Relative Percent Difference Function ------------------------------------

#Developing Relative Percent Difference Function
RPD = \(x1, x2) {
  x1[is.na(x1)] = 0L; x2[is.na(x2)] = 0L
  abs((x1 - x2) / ((x1 + x2) * 0.5)) * 100
}

QCandResults <- transform(QCandResults, RPD = RPD(Value_Results, Value_QC))

#Creating pass column and then creating stat for how many QC failed
QCandResults <- transform(QCandResults, Pass = if_else(RPD > 20, ""N"", """"))

(sum(QCandResults$Pass == ""N"", na.rm=T) / nrow(QCandResults))

#export to xl
write.xlsx(QCandResults, ""QC2023.xlsx"")

","Lets take this example from the gapminder dataset. I am just creating a string to extract the year from since that seems to be what you are doing in your code. What you can do is just all your existing code in a function. The only thing that you would need to modify is what you are passing to filter. I added {{}} to  sub where your variable name normally goes. The {{}} tell dplyr to evaluate whatever column you provide as an argument to year_col instead of the value of year_col. paste is going literally just copy and pasting the argument provided to year select.
library(gapminder)
library(dplyr)

make_string = gapminder |>
  mutate(year_string = paste(country, ""in"", year))


filter_year = \(data, year_col, year_to_select){
   
  filter_data = data |>
    filter(sub("".*([0-9]{4}).*"", '\\1' , {{year_col}}) == paste(year_to_select)

  filter_data
}

examp = filter_year(make_string, year_col = year_string, year_to_select = '1952')
  
check = make_string |>
  filter(year == 1952)

identical(examp, check)
#> [1] TRUE

So in your example code it would look something like this

cleaning_fun = \(year_select, year_col){
  ""QC-Samples"" <- `QC Samples` %>% 
select(LOC_ID, QC_SAMPLE, SAMPLE_DEPTH, QC_TYPE, ASSOC_SAMP)
""QC-Results"" <- `QC Results` %>%  select(Loc_ID, QC_Sample, Units, Value, Text_Value, QC_Type, Storet_Num)
'Results_' <- `Results` %>% 
select(Loc_ID, Sample_Num, Units, Value, Storet_Num, Text_Value)


'QC-Samples' <- `QC-Samples` %>% 
   filter(substr({{year_col}},1,4)== paste(year_select))

## the rest of your code goes below here 

QCandResults
}


Created on 2024-07-31 with reprex v2.1.1
",prompt
how to render prompt string in semantic kernel for evaluation with prompt flow sdk in python,"I am currently working on a project where I utilize the Semantic Kernel with a plugins directory and multiple skills. My ultimate goal is to develop a GitHu. workflow to evaluate my prompts using the Prompt Flow SDK. To achieve this, I need a way to access the final prompt string value before it is sent to the AI model.
My Question:
How can I retrieve the rendered prompt string in Semantic Kernel before it's sent to the AI model? Is there any existing functionality, method, or workaround to achieve this.
","In C# you can use filters. Search for IPromptRenderFilter ... I am not using Python so your mileage may vary.
This is an example of how to display the Rendered Template.
internal class PromptRenderFilter : IPromptRenderFilter
{
    private readonly AppSettings _settings;

    public PromptRenderFilter(AppSettings settings)
    {
        _settings = settings;
    }

    public async Task OnPromptRenderAsync(PromptRenderContext context, Func<PromptRenderContext, Task> next)
    {
        AnsiConsole.WriteLine($""Rendering prompt for {context.Function.Name}"");

        await next(context);

        if(_settings.DisplayRenderedPrompt)
            AnsiConsole.WriteLine($""Rendered prompt: {context.RenderedPrompt}"");
    }
}

",prompt
sas enterprise guide creating date range prompt,"I created a date range prompt using SAS Enterprise Guide Prompt Manager. I right click the prompt to see its related macro variables.
1- I just want to run the program and select the prompt values. When I run the project, prompt screen appears. I had an excel prompt already which appears, but my second date range prompt doesn't seem.
2- I want to use the date range related macros in my code. When I right click the prompt to see its macros, a lot of options appear. I selected the first one to see what happens. (which was date_pmt_min). I chose the date type as month by the way (example: July 2024)
%let str_date = ""&date_pmt_min""d;
%put start date: &str_date ;

The code above doesnt print a value to the log.
How can I see the result in log & see the prompt in prompt screen?
Thanks in advance.
","after you created the prompt you need to assign it to a program.
Follow the steps below to do so.
Open the properties on the program

Assign the prompt via the Add Button

If you run the program the prompt automatically create the following variables

To show the values you can just use the %put function without the need to assign a new local variable.
Here yu see the code and the result in the log file

",prompt
ensuring consistent response structure with gemini api,"I'm building a project that leverages the Gemini API, but I'm facing an inconsistency issue. Every time I send the same prompt, the response structure changes, making backend processing difficult.
The desired behavior is to receive responses with a consistent structure, regardless of content variations. This structure should ideally include:
An introductory paragraph 3-4 paragraphs containing the main content A concluding paragraph However, subsequent requests with the same prompt sometimes lack the title or begin with different elements, disrupting my processing pipeline.
Could you advise on how to achieve consistent response structure from Gemini API, even when the content varies?
","You don't give the prompt you're using to generate the reply, but in general, Gemini is better at following examples rather than following instructions.
So your prompt should probably look something like:
You will be asked a question. Your reply should include a title, a 
descriptive paragraph, and a concluding paragraph as illustrated below.

Example question: What is the meaning of life?
Example reply:
Title: About life
Description: Life. Don't talk to me about life.
Conclusion: All the circuits down my left arm

Question: [put the question here]
Reply:

",prompt
bash pwd shortening,"I'm looking for a bash function that will shorten long path names to keep my PS1 variable from getting excessively long.  Something along the lines of:
/this/is/the/path/to/a/really/long/directory/i/would/like/shortened

might end up as:
/t../i../t../p../to/a/r../l../d../i/w../like/shortened

something that the took the path and a maximum acceptable number of characters to shorten to would be perfect for my .bashrc file.
","How about a Python script? This shortens the longest directory names first, one character at a time until it meets its length goal or cannot get the path any shorter. It does not shorten the last directory in the path.
(I started writing this in plain shell script but man, bash stinks at string manipulation.)
#!/usr/bin/env python
import sys

try:
    path   = sys.argv[1]
    length = int(sys.argv[2])
except:
    print >>sys.stderr, ""Usage: $0 <path> <length>""
    sys.exit(1)

while len(path) > length:
    dirs = path.split(""/"");

    # Find the longest directory in the path.
    max_index  = -1
    max_length = 3

    for i in range(len(dirs) - 1):
        if len(dirs[i]) > max_length:
            max_index  = i
            max_length = len(dirs[i])

    # Shorten it by one character.    
    if max_index >= 0:
        dirs[max_index] = dirs[max_index][:max_length-3] + ""..""
        path = ""/"".join(dirs)

    # Didn't find anything to shorten. This is as good as it gets.
    else:
        break

print path

Example output:
$ echo $DIR
/this/is/the/path/to/a/really/long/directory/i/would/like/shortened
$ ./shorten.py $DIR 70
/this/is/the/path/to/a/really/long/directory/i/would/like/shortened 
$ ./shorten.py $DIR 65
/this/is/the/path/to/a/really/long/direc../i/would/like/shortened
$ ./shorten.py $DIR 60
/this/is/the/path/to/a/re../long/di../i/would/like/shortened
$ ./shorten.py $DIR 55
/t../is/the/p../to/a/r../l../di../i/wo../like/shortened
$ ./shorten.py $DIR 50
/t../is/the/p../to/a/r../l../d../i/w../l../shortened

",prompt
my zsh prompt is not bold,"I am customizing my zsh terminal but I face difficulties when it comes to turn my prompt into bold. I tried several different things, but none worked, whereas it did work for people on the Internet...

So this is my test prompt. Both ""HEY"" are in color 1, but the second ""HEY"" is supposed to be bold. It is obviously not, and instead of that, I get some flashy red, like the first part of my prompt, which is in color 9. wtf :)
Here is the code that I use in this case:
HEY%BHEY%b

I also noticed that it does the same thing for every one of the 8 first colors: black becomes grey, grey becomes white, night blue becomes dark blue, leaf green becomes flashy green, same for pink, yellow...
I also tried to use some escape sequences with the modulo, but none worked: each time, the modulo took the character next to it as its parameter and the rest of the code would just be displayed like normal characters.
For example, this:
PS1=""%{\e[1;33%}test""

would display this:
\e[1;33test

Could someone explain me what is going wrong here and how I could fix it to have some (real) bold prompt text?
","""ANSI"" colors (escape codes) has a ""bold"" attribute, but it is often interpreted as ""bright"" instead. You may have to look in your terminal application's options to see if you can reconfigure it to interpret bold as bold instead of bright. I think you have to choose between supporting ""bold"" and ""bright colors"". See also https://en.wikipedia.org/wiki/ANSI_escape_code#Colors
",prompt
ohmyposh themes not working correctly with powerline font and conemu,"I use PowerShell as my shell on Windows 7. I find that ConEmu is a really good terminal. Since I am not on Windows 10, I cannot use Windows Terminal but ConEmu is not bad at all. I found out about posh-git and oh-my-posh and how they can customize your PowerShell experience. I installed oh-my-posh and the Paradox theme looked nice. But I saw some empty boxes in random places in my Prompt. 
And this is an issue that I face on all the themes. The colors and the design are beautiful and I want to use it but those weird boxes are keeping me from doing that. I would also like to tell that I am using Cascadia Code as my font and also this is the Powerline version of Cascadia Code. So, I think it should work as excepted. Next, trying to fix this, I went to nerdfont.com and I installed Cascadia Code from there as opposed to Microsoft's official GitHub repository. Then I set ConEmu's font to Cascadia which I installed from nerdfonts and this happened:

It's better in a way that I can see the Python symbol and some more symbols but still there is one box that cannot be rendered. But it does get worse if I change repository:

There is a weird question mark after ""master"". I think I have met all the prerequisites to use oh-my-posh like install posh-git and PSReadLine and having powerline Cascadia Code font and also using ConEmu as they officially suggest. I would absolutely appreciate it a lot if anyone can help me out of this mess and suggest what to do to fix my prompt. 
P.S I am using PowerShell 7 Core.
","When you see boxes, that means that the font doesn't have that specified character. e.g. there are a lot of specialized fonts that don't have every character location defined.
Right on the oh-my-posh GitHub page, Quote:

In case you notice weird glyphs after installing a font of choice,
make sure the glyphs are available (maybe they have a different
location in the font, if so, adjust the correct $ThemeSettings icon).
If it turns out the character you want is not supported, select a
different font.

Also on the oh-my-posh GitHub page, the font used is:

The fonts I use are Powerline fonts, there is a great repository
containing them. I use Meslo LG M Regular for Powerline Nerd Font

If using Meslo LG M Regular doesn't solve your problem, then you have to manually remap the icons to the correct unicode locations in your chosen font.
For Version 2 of Oh My Posh, you have to edit the $ThemeSettings variable. Follow the instructions on the GitHub on configuring Theme Settings. e.g.:
$ThemeSettings.GitSymbols.BranchSymbol = [char]::ConvertFromUtf32(0xE0A0) 

For Version 3+ of Oh My Posh, you have to edit the JSON configuration file to make the changes, e.g.:
...
{
    ""type"": ""git"",
    ""style"": ""powerline"",
    ""powerline_symbol"": ""\uE0B0"",
....

",prompt
sweetalert prompt issue in bootstrap modal,"I have been trying for more than two days to run SweetAlert prompt in a modal bootstrap without success, the input is not accessible and I don't understand why. I need help please.


$(""#openSWAL"").click(function(){_x000D_
	swal({_x000D_
    title: ""An input!"",_x000D_
    text: ""Write something interesting:"",_x000D_
    type: ""input"",_x000D_
    showCancelButton: true,_x000D_
    closeOnConfirm: false,_x000D_
    animation: ""slide-from-top"",_x000D_
    inputPlaceholder: ""Write something""_x000D_
  },_x000D_
       function(inputValue){_x000D_
    if (inputValue === false) return false;_x000D_
_x000D_
    if (inputValue === """") {_x000D_
      swal.showInputError(""You need to write something!"");_x000D_
      return false_x000D_
    }_x000D_
_x000D_
    swal(""Nice!"", ""You wrote: "" + inputValue, ""success"");_x000D_
  });_x000D_
});
<script src=""https://ajax.googleapis.com/ajax/libs/jquery/3.2.0/jquery.min.js""></script>_x000D_
<script src=""https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/js/bootstrap.min.js""></script>_x000D_
<script src=""https://cdnjs.cloudflare.com/ajax/libs/sweetalert/1.1.3/sweetalert.min.js""></script>_x000D_
_x000D_
<link rel=""stylesheet"" href=""https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap.min.css"">_x000D_
<link rel=""stylesheet"" href=""https://cdnjs.cloudflare.com/ajax/libs/sweetalert/1.1.3/sweetalert.min.css"">_x000D_
_x000D_
<!-- Button trigger modal -->_x000D_
<button type=""button"" class=""btn btn-primary btn-lg"" data-toggle=""modal"" data-target=""#myModal"">_x000D_
  Open modal_x000D_
</button>_x000D_
_x000D_
<!-- Modal -->_x000D_
<div class=""modal fade"" id=""myModal"" tabindex=""-1"" role=""dialog"" aria-labelledby=""myModalLabel"">_x000D_
  <div class=""modal-dialog"" role=""document"">_x000D_
    <div class=""modal-content"">_x000D_
      <div class=""modal-header"">_x000D_
        <button type=""button"" class=""close"" data-dismiss=""modal"" aria-label=""Close""><span aria-hidden=""true"">&times;</span></button>_x000D_
        <h4 class=""modal-title"" id=""myModalLabel"">Modal title</h4>_x000D_
      </div>_x000D_
      <div class=""modal-body"">_x000D_
        Bla<br/>_x000D_
    Bla<br/>_x000D_
    Bla<br/>_x000D_
    Bla<br/>_x000D_
    Bla<br/>_x000D_
    Bla<br/>_x000D_
    Bla<br/>_x000D_
    Bla<br/>_x000D_
    Bla<br/>_x000D_
    Bla<br/>_x000D_
    Bla<br/>_x000D_
    Bla<br/>_x000D_
    Bla<br/>_x000D_
    Bla<br/>_x000D_
    Bla<br/>_x000D_
    Bla<br/>_x000D_
    Bla<br/>_x000D_
    Bla<br/>_x000D_
    _x000D_
      </div>_x000D_
      <div class=""modal-footer"">_x000D_
        <button type=""button"" id=""openSWAL"" class=""btn btn-warning"">Open SweetAlert prompt</button>_x000D_
        <button type=""button"" class=""btn btn-default"" data-dismiss=""modal"">Close</button>_x000D_
        <button type=""button"" class=""btn btn-primary"">Save changes</button>_x000D_
      </div>_x000D_
    </div>_x000D_
  </div>_x000D_
</div>



","Removing tabindex=""-1"" from myModal seems to do the job: Fiddle
The problem is tabindex, because if you set it to -1, you won't be able to access that element.
More info here, and in this question.
",prompt
kill ios simulator from terminal,"I'm trying to terminate the iOS Simulator from the terminal console (needed for integrating automatic UI Testing in Jenkins),but every time I try the command:
killall SimulatorBridge

a prompt waiting for action gets displayed:

The problem is that the simulator doesn't get dismissed until an action on the prompt is taken.  How to get rid of it?
","Use killall ""iPhone Simulator""
With XCode 6, use killall ""iOS Simulator"" (thanks @gempewsaw)
With XCode 7, use killall ""Simulator"" (thanks @Stanislaw)
With XCode 8, killing the Simulator process is sometimes not enough, also need to kill process com.apple.CoreSimulator.CoreSimulatorService (thanks @bensnider)
",prompt
copy only latest files in command prompt,"I have two directories with the same files. On the source directory, I always make updates. Now I just want to copy the updated files from the source directory to the destination directory. So it would be like updating files instead of copying all files.
I had also tried on the ""Batch script to copy newest file"". Though it worked well but it only copies the newest files.
Is there something that could compare the dates of the files from the directories?
","Check out RoboCopy, it has switches for filtering files based on dates
",prompt
how to produce one long list of indices,"I have written a Python script that deletes indices on ElasticSearch.
I have added a section to list the specific indices that will be deleted (doc count 0 and index name can't start with ""."") I added a user prompt to approve the list before deletion begins.
However the output of this is not user friendly:
Indices to be deleted:
test-index-000001
Do you want to proceed with the deletion of these indices? (y/n): y
Indices to be deleted:
test-index-000001   
test-index-000002   
Do you want to proceed with the deletion of these indices? (y/n): y
Indices to be deleted:
test-index-000001
test-index-000002
test-index-000003

I want the output to be:
Indices to be deleted:
  test-index-000001
  test-index-000002
  test-index-000003
Do you want to proceed with the deletion of these indices? (y/n): y

Here is my code for this section:
content = json.loads(getRequestElasticSearch.text)

indices_to_delete = []
for index in content:
    if int(index['docs.count']) == 0 and not index['index'].startswith("".""):
        indices_to_delete.append(index['index'])
        print(""Indices to be deleted:"")
        for idx in indices_to_delete:
            print(idx)
        proceed = input(""Do you want to proceed with the deletion of these indices? (y/n): "")
        if proceed.lower() != ""y"":
            print(""Deletion aborted by user."")
            return

I have tried moving the prompt outside the loop, but then it keeps repeating the question over and over again without progressing to the rest of the code. Can you help me please?
","Move everything except the code to fill indices_to_delete outside of your loop.
content = json.loads(getRequestElasticSearch.text)

indices_to_delete = []
for index in content:
    if int(index['docs.count']) == 0 and not index['index'].startswith("".""):
        indices_to_delete.append(index['index'])
   
print(""Indices to be deleted:"")
for idx in indices_to_delete:
     print(idx)

proceed = input(""Do you want to proceed with the deletion of these indices? (y/n): "")
if proceed.lower() != ""y"":
   print(""Deletion aborted by user."")
else DeletionCode()

This fills your array with all items you want to delete, then prints them all and asks 1 time if you want to proceed.
",prompt
how to add a user prompt to approve a list of indices before deleting them,"I have a python script where I log into ElasticSearch and delete indices that have a docs.count = 0.
To add some security to the script, I want to add a user prompt, for when the environment is Prod. It will print the list of indices and ask the user if they want to proceed with deletion. I just can't figure out where to place it or how to structure it. Can you help me please?
This is a section the current script that applies to this question:
def get_Configuration():
  env = ""Dev""
  subscription_name = env
  if env == ""Prod"":
     subscription_name = ""Production""
  print(f""The script is running on {subscription_name}"")
  KV_name_entry = f""Elastic--elastic""
  _secret_client = SecretClient(vault_url=f""https://{KV_PARTIAL_NAME}-{env}.vault.azure.net/"", credential=CREDENTIAL)
  configuration = {'ELASTIC_SEARCH_USER': KV_name_entry.split(""--"")[-1],
  'ELASTIC_SEARCH_PASSWORD' : _secret_client.get_secret(KV_name_entry).value,
  'ELASTIC_SEARCH_URL' : _secret_client.get_secret(""Elastic--URL"").value}
  return configuration

# Delete the indices with doc count 0
def indices_delete(configuration):
  elastic_auth_uri = f""https://{configuration['ELASTIC_SEARCH_URL']}/security/_authenticate""
  elastic_username = configuration['ELASTIC_SEARCH_USER']
  elastic_password = configuration['ELASTIC_SEARCH_PASSWORD']
  authenticate_response = requests.get(elastic_auth_uri, auth=(elastic_username, elastic_password))
  if authenticate_response.status_code == 405:
     print(f""User: {elastic_username} has been authenticated."")
  
  search_url_index = ""_cat/indices/""
  params_dict = {
  ""h"":""index,docs.count"",
  ""s"":""docs.count:asc"",
  ""format"":""json""
  }
  elastic_console = f""https://{configuration['ELASTIC_SEARCH_URL']}/{search_url_index}""
  getRequestElasticSearch = requests.get(elastic_console, auth=(elastic_username, elastic_password), params=params_dict)
  content = json.loads(getRequestElasticSearch.text)
  elastic_console_delete = f""https://{configuration['ELASTIC_SEARCH_URL']}/""
  print(""Actioning deletion of indices on ElasticSearch via url"" , elastic_console_delete)
  for index in content:
   indiciesList = index
   collectdoccount = index['docs.count']
   search_int = int(collectdoccount)
   elasticservice_index_name = index['index']
   if search_int == 0 and not elasticservice_index_name.startswith("".""):
      index_name = index['index']
      delete_url = f""{elastic_console_delete}{index_name}""
      response = requests.delete(delete_url, auth=(elastic_username, elastic_password))
      if  response.status_code == 200:
          print(""index deleted -"" , ""index name:"" , index['index'] , "",  doc.count:"" , index['docs.count'] , "",  elasticsearch url index:"" , delete_url)
      if  response.status_code != 200:
          print (""index not deleted -"" , ""index name:"" , index['index'] , "",  Reason:"" , response.content)

","def indices_delete(configuration, env):
    elastic_auth_uri = f""https://{configuration['ELASTIC_SEARCH_URL']}/security/_authenticate""
    elastic_username = configuration['ELASTIC_SEARCH_USER']
    elastic_password = configuration['ELASTIC_SEARCH_PASSWORD']
    authenticate_response = requests.get(elastic_auth_uri, auth=(elastic_username, elastic_password))
    if authenticate_response.status_code == 405:
        print(f""User: {elastic_username} has been authenticated."")

    search_url_index = ""_cat/indices/""
    params_dict = {
        ""h"": ""index,docs.count"",
        ""s"": ""docs.count:asc"",
        ""format"": ""json""
    }
    elastic_console = f""https://{configuration['ELASTIC_SEARCH_URL']}/{search_url_index}""
    getRequestElasticSearch = requests.get(elastic_console, auth=(elastic_username, elastic_password), params=params_dict)
content = json.loads(getRequestElasticSearch.text)

    indices_to_delete = []
    for index in content:
        if int(index['docs.count']) == 0 and not index['index'].startswith("".""):
            indices_to_delete.append(index['index'])

    if env == ""Prod"":
        print(""Indices to be deleted:"")
        for idx in indices_to_delete:
            print(idx)
        proceed = input(""Do you want to proceed with the deletion of these indices? (yes/no): "")
        if proceed.lower() != ""yes"":
            print(""Deletion aborted by user."")
            return

    elastic_console_delete = f""https://{configuration['ELASTIC_SEARCH_URL']}/""
    print(""Actioning deletion of indices on ElasticSearch via url"", elastic_console_delete)
    for index_name in indices_to_delete:
        delete_url = f""{elastic_console_delete}{index_name}""
        response = requests.delete(delete_url, auth=(elastic_username, elastic_password))
        if response.status_code == 200:
            print(""index deleted -"", ""index name:"", index_name, "", elasticsearch url index:"", delete_url)
        else:
            print(""index not deleted -"", ""index name:"", index_name, "", Reason:"", response.content)

Here's how you can modify your indices_delete function to include this feature:

Add a prompt for user confirmation when the environment is ""Prod"".
List indices to be deleted and get confirmation from the user before proceeding.
Proceed with deletion only if the user confirms.

",prompt
zsh hide computer name in terminal,"How would I hide the computer name in the prompt while using the terminal?
At the moment it shows both username and computer name like so:

It would save some space by removing anwarchoukah@anwars-mbp, seeing as I actually know who I am:)
","Open up .zshrc, find the line reading export PS1 or maybe export PROMPT.
Remove the variable that is used for the hostname, could be %m or %M.
Zsh on Archlinux wiki
You can use colors and also have a prompt (or some information) on the right side.
",prompt
how do i change the commandline prompt in windows,"How do I change the command-line prompt into a console?
I've been looking in the console functions API, but I could not find anything for it.
","There's the PROMPT environment variable:
set PROMPT=$P$G

$P$G is the default value, giving you the usual C:\> type output. There are more format variables available here.
",prompt
troubleshooting gpt4 integration with sqldatabasetoolkit and create_sql_agent for prompt passing error,"I was previously using SQLDatabaseChain to connect LLM (Language Model) with my database, and it was functioning correctly with GPT-3.5. However, when attempting the same process with GPT-4, I encountered an error stating ""incorrect syntax near 's""
To address this issue, I opted to use SQLDatabaseToolkit and the create_sql_agent function. However, I encountered a problem with this approach as I was unable to pass a prompt. When attempting to include a PromptTemplate in the create_sql_agent argument, it resulted in errors.
ValueError: Prompt missing required variables: {'tool_names', 'agent_scratchpad', 'tools'}
Below is my code:
toolkit = SQLDatabaseToolkit(db=db, llm=llm)

agent_executor = create_sql_agent(
    llm=llm,
    toolkit=toolkit,
    verbose=True,
    prompt=MSSQL_PROMPT,
)

","I found solution for it.
agent_executor = create_sql_agent(llm, db=db, agent_type=""openai-tools"", verbose=False)

will worked for me. And it also work with prompt based approach. So if you want to add prompt in it then it should be like
agent_executor = create_sql_agent(llm, db=db, agent_type=""openai-tools"", verbose=False, prompt=MSSQL_PROMPT)

where
    MSSQL_PROMPT = """"""You are an MS SQL expert. Given an input question, first create a syntactically correct MS SQL query to run, then look at the results of the query and return the answer to the input question.

Use the following Domain Knowledge about Database: One Order can have multiple shipments & shipment containers. 

Use the following format:

Question: Question here
SQLQuery: SQL Query to run
SQLResult: Result of the SQLQuery
Answer: Final answer here

Only use the following tables:
{table_info}

Thought: I should look at the tables in the database to see what I can query. Then I should query the schema of the most relevant tables.
{agent_scratchpad}

Question: {input}""""""

You can add more data in prompt as per your use cases.
",prompt
ifelse conditional behaviour in javascript,"I'm a begginer in JavaScript and while I was working with an if/else conditional something happened and I can't still understand it. My code is simple: A prompt asks if user wants to do an ADDITION or a SUBSTRACTION, then the following 2 prompts ask for a value and based on the user's answers, the code executes the addition or substraction respectively, by using the values provided.
The problem comes when user clicks on 'cancel' or 'ok' button on the prompt, my code is intended to do certain actions if any of these buttons is clicked, and it does, but it also does another action that I don't want it to execute.
If user clicks either 'cancel' or 'ok', the code should alert the following message and finish the execution:
alert('No option entered, aborting operation!')
The code alerts that message successfully, but it also alerts  another message that is intended to be displayed only if user enters a different value than SUMA or RESTA. The message is this:
alert('No valid option entered, aborting operation!');
Then I modified the code and it is working as expected, but according to me, the first version of the code should work without any issue. The goal is to display only the first message if either 'cancel' or 'ok' buttons are clicked and finish the execution, not both messages and finish the execution.
I hope to be clear enough, if I'm not please let me know and I can explain with more detail if necessary. Can anybody explain to me what's the problem here?
Here are the snippets:
FIRST VERSION (not working as expected):
let opText = document.getElementById('opText');
let opOperation = prompt('Enter SUMA or RESTA');
let opSuma1; 
let opSuma2; 
let opStatus = true;
let opResult;

if(opOperation === null || opOperation === """"){
    alert('No option entered, aborting operation!'); **//THE CODE DISPLAYS THIS MESSAGE IF EITHER 'CANCEL' OR 'OK' BUTTONS ARE CLICKED**
    opStatus = false;
}

else {
    opOperation = opOperation.trim().toLowerCase();
}

function addition(){
    opSuma1 = prompt('Enter first value:');;
    opSuma2 = prompt('Enter second value:');
    opSuma1 = Number(opSuma1);
    opSuma2 = Number(opSuma2);
    opResult = opSuma1 + opSuma2;
    opText.textContent = `Your result was a ADDITION and it is ${opResult}`;
}

function substraction(){
    opSuma1 = prompt('Enter first value:');;
    opSuma2 = prompt('Enter second value:');
    opSuma1 = Number(opSuma1);
    opSuma2 = Number(opSuma2);
    opResult = opSuma1 - opSuma2;
    opText.textContent = `Your result was a SUBSTRACTION and it is ${opResult}`;
}


if(opStatus && opOperation === 'suma'){
    addition();
}
else if(opStatus && opOperation === 'resta'){
    substraction();
}
**else {
    
    alert('No valid option entered, aborting operation!'); **// BUT IT ALSO DISPLAYS THIS MESSAGE**
}**


SECOND VERSION (working as expected)
let opText = document.getElementById('opText');
let opOperation = prompt('Enter SUMA or RESTA');
let opSuma1; 
let opSuma2; 
let opStatus = true;
let opResult;

if(opOperation === null || opOperation === """"){
    alert('No option entered, aborting operation!');
    opStatus = false;
}

else {
    opOperation = opOperation.trim().toLowerCase();
}

function addition(){
    opSuma1 = prompt('Enter first value:');;
    opSuma2 = prompt('Enter second value:');
    opSuma1 = Number(opSuma1);
    opSuma2 = Number(opSuma2);
    opResult = opSuma1 + opSuma2;
    opText.textContent = `Your result was a ADDITION and it is ${opResult}`;
}

function substraction(){
    opSuma1 = prompt('Enter first value:');;
    opSuma2 = prompt('Enter second value:');
    opSuma1 = Number(opSuma1);
    opSuma2 = Number(opSuma2);
    opResult = opSuma1 - opSuma2;
    opText.textContent = `Your result was a SUBSTRACTION and it is ${opResult}`;
}

 if (opStatus) {
     if (opOperation === 'suma') {
         addition();
     } else if (opOperation === 'resta') {
         substraction();
     } else {
         alert('No valid option entered, aborting operation!');
     }
 }

","You broke the conditions into two unrelated of branches.
The first one:
    if(opOperation === null || opOperation === """"){
        alert('No option entered, aborting operation!'); **//THE CODE DISPLAYS THIS MESSAGE IF EITHER 'CANCEL' OR 'OK' BUTTONS ARE CLICKED**
        opStatus = false;
    }

    else {
        opOperation = opOperation.trim().toLowerCase();
    }

When the first one ends, it doesn’t stop the program, so after the else, it continues onto the next one.
The seconds one:
    if(opStatus && opOperation === 'suma'){
        addition();
    }
    else if(opStatus && opOperation === 'resta'){
        substraction();
    }
    **else {
    
        alert('No valid option entered, aborting operation!'); **// BUT IT ALSO DISPLAYS THIS MESSAGE**
    }**

If the first if was true, then the remaining two must be false since opStatus has already been established to be nothing. Therefore, it goes to the else statement (which is evaluated if nothing else in the if-branch was true) and executes it.
If you nested them in by putting the second one inside the else of the first, it would solve the problem.
    if(opOperation === null || opOperation === """"){
        alert('No option entered, aborting operation!'); **//THE CODE DISPLAYS THIS MESSAGE IF EITHER 'CANCEL' OR 'OK' BUTTONS ARE CLICKED**
        opStatus = false;
    }

    else {
        opOperation = opOperation.trim().toLowerCase();

        if(opStatus && opOperation === 'suma'){
            addition();
        }
        else if(opStatus && opOperation === 'resta'){
            substraction();
        }
        **else {
            alert('No valid option entered, aborting operation!'); **// BUT IT ALSO DISPLAYS THIS MESSAGE**
        }**
    }

You use this same principle in your second solution:
    if (opStatus) {
        if (opOperation === 'suma') {
            addition();
        } else if (opOperation === 'resta') {
            substraction();
        } else {
            alert('No valid option entered, aborting operation!');
        }
    }

In that example, you evaluate if opStatus is anything at all. I suggest combining the first and second if-branches from that example and removing opStatus as it doesn’t do anything. Instead use if (opOperation). This works because there really is no such thing as true or false. Anything that loosely equals zero will be considered false, and false is zero. """" is zero, and null is equal to nothing except null, but it is treated as false in comparisons, despite having no way to convert.
    if (opOperation) {
        opOperation = opOperation.trim().toLowerCase();
        if (opOperation === 'suma') {
            addition();
        } else if (opOperation === 'resta') {
            substraction();
        } else {
            alert('No valid option entered, aborting operation!');
        }
    }
    else {
        alert('No option entered, aborting operation!');
    }



let opText = document.getElementById('opText');
let opOperation = prompt('Enter SUMA or RESTA');
let opSuma1; 
let opSuma2; 
let opResult;

function addition(){
    opSuma1 = prompt('Enter first value:');;
    opSuma2 = prompt('Enter second value:');
    opSuma1 = Number(opSuma1);
    opSuma2 = Number(opSuma2);
    opResult = opSuma1 + opSuma2;
    opText.textContent = `Your result was a ADDITION and it is ${opResult}`;
}

function substraction(){
    opSuma1 = prompt('Enter first value:');;
    opSuma2 = prompt('Enter second value:');
    opSuma1 = Number(opSuma1);
    opSuma2 = Number(opSuma2);
    opResult = opSuma1 - opSuma2;
    opText.textContent = `Your result was a SUBSTRACTION and it is ${opResult}`;
}

if (opOperation) {
    opOperation = opOperation.trim().toLowerCase();
    if (opOperation === 'suma') {
        addition();
    } else if (opOperation === 'resta') {
        substraction();
    } else {
        alert('No valid option entered, aborting operation!');
    }
}
else {
    alert('No option entered, aborting operation!');
}



Let me know if I made an error somewhere (I know my code works).
",prompt
get actual prompt length,"I wanted to print something like this on my gnome-terminal 
[abc@host pwd]$ ************************************************************

using some manipulations in PS1 but the number of stars are dynamic depending on PS1 and 
terminal width. So can anyone plz suggest me the way to find out the length of PS1 i.e the actual string which will be displayed.
","It can't ve done portably IMO, as PS1 can contain escape sequences for color codes, (multiple) new lines too.
It can be calculated. But that's a really hard task. What if the user codes colorcodes in variable names (it's a common scenario), how to decide (during evaluation/counting the length) if that's something that the user wants to display or is only style information?
",prompt
convert promptsync require into import method,"I use prompt-sync  module in my Node project.
 const prompt = require('prompt-sync')();
 const result = prompt(message);

But to keep my TypeScript code consistent I need to use import instead of  require.
So I installed types for the package.
npm i @types/prompt-sync

And I tried to use it like
import * as promptSync from 'prompt-sync';
...
const prompt = promptSync();
const result = prompt(message);

But the error appeared
Error:(24, 20) TS2349: This expression is not callable.
Type '{ default: (config?: Config | undefined) => Prompt; }' has no call signatures.

So how can I use prompt-sync with import?
","The error is raised because you cannot call a namespace import (* as ns). This restriction is per the ECMAScript specification which mandates that module namespace objects, such as the aforementioned syntax creates, cannot have a [[Call]] or [[Construct]] signature.
This results in a mismatch when attempting to consume CommonJS modules from ES modules as many of the former export a single function or constructor as the module itself (i.e. module.exports = function () {}).
However, there is interop capability specified and conventionalized which works by synthesizing  a default export for the CommonJS module that contains the value of module.exports.
You can and should leverage this interop facility.
Firstly, ensure that ""esModuleInterop"" is specified with a value of true in your tsconfig.json under ""compilerOptions"".
Secondly, rewrite your code to import the synthetic default from the prompt-sync module
import promptSync from 'prompt-sync';

const prompt = promptSync();

const result = prompt(message);

",prompt
oclif prompt testing,"I'm attempting to write a unit test for an Oclif hook that contains a simple prompt. I want to test the output of the hook, given a 'Y' or 'N' response to the prompt.


import {Hook} from '@oclif/config'_x000D_
import cli from 'cli-ux'_x000D_
_x000D_
const hook: Hook<'init'> = async function () {_x000D_
_x000D_
  const answer = await cli.prompt(""Y or N?"")_x000D_
_x000D_
  if(answer === 'Y') {_x000D_
    this.log('yes')_x000D_
  }_x000D_
  else {_x000D_
    this.log('no')_x000D_
  }_x000D_
}_x000D_
_x000D_
export default hook



I'm using the 'fancy-test' and '@oclif/test' test frameworks described here:
https://oclif.io/docs/testing
I have tried stubbing the prompt and simulating stdin but neither are working - either the stubbed function is not available or the output is an empty string.
Here's an attempt at one test (doesn't work because 'cli.prompt is not a function'):


import {expect, test} from '@oclif/test'_x000D_
import cli from 'cli-ux'_x000D_
import * as sinon from 'sinon';_x000D_
_x000D_
describe('it should test the ""configure telemetry"" hook', () => {_x000D_
  test_x000D_
  .stub(cli, 'prompt', sinon.stub().resolves('Y'))_x000D_
  .stdout()_x000D_
  .hook('init')_x000D_
  .do(output => expect(output.stdout).to.contain('yes'))_x000D_
  .it()_x000D_
})



It occurred to me that I'm probably not structuring my test properly. If anyone could point me in the right direction or provide some pseudo / sample code as to how to approach testing the above hook that would be amazing - thanks!
","Have you tried with:
import {expect, test} from '@oclif/test'
import cli from 'cli-ux'
import * as sinon from 'sinon';

describe('it should test the ""configure telemetry"" hook', () => {
  test
  .stub(cli, 'prompt', () => async () => 'Y')
  .stdout()
  .hook('init')
  .do(output => expect(output.stdout).to.contain('yes'))
  .it()
})

Stubbing with .stub(cli, 'prompt', () => async () => 'Y') worked for me
",prompt
type of date as prompt,"I have standard routine for PROMPT_DATA: can be type month('January 2024') or type=day ('24jan2024')
  DATA_ = INPUT(""&Prompt_DATA."", ANYDTDTE.);
  %LET HasComma = %SYSFUNC(INDEXC(&Prompt_DATA_LABEL., ','));
  %IF &HasComma. > 0 %THEN %DO;

I'm looking for a ',' so i know it means day.
How can i better test the type of Prompt_Data?
","This is a fine approach, just no reason to mix the macro language with the data step language.
DATA_ = INPUT(""&Prompt_DATA."", ANYDTDTE.);
IF (INDEXC(""&Prompt_DATA_LABEL."", ',')) THEN DO;
 

END;

Beyond that, SAS doesn't have types really here - just numeric and character.  If it accepted a ""month"" type, it likely would be assigned the 1st of the month (so ""January 2024"" -> 01JAN2024), but you can't just test for that - the first of the month is after all a legal value for the day version as well.
",prompt
set ps1 with subcommand that prints colors,"When putting ANSI color codes in PS1, they need to be surrounded with \[\] or else the prompt can get confused about where the editable part of the line starts. However, when a subcommand ($()) prints colors, the \[\] escapes are always being written literally to the prompt...and with long enough commands in my history, the prompt gets confused.
Here's an example:
ps1test() {
    ps1sub() {
        printf '\[\033[32m\]Hello!\[\033[0m\]'
    }
    PS1='$(ps1sub) \$ '
}

Expected:
$ ps1test
Hello! $

Actual (bash installed by Git for Windows):
$ ps1test
\[\]Hello!\[\] $

How can I get my shell to interpret the \[\] escapes from a subcommand?
","Only \[s in the literal string are interpreted. \[s resulting from embedded expansions are not.
The easiest way to get around it is to have PROMPT_COMMAND set a PS1 to a new literal value each time:
updateps1() {
    ps1sub() {
        printf '\[\033[32m\]Hello $RANDOM!\[\033[0m\]'
    }
    PS1=""$(ps1sub) \\\$ ""
}

PROMPT_COMMAND='updateps1'

",prompt
prevent ncurses c library from creating subwindow,"I want to prevent the C++ ncurses library from creating its own sub-window when activated. Whenever you use initscr() with ncurses, it creates its own sub-window (text area in terminal). Not using initscr() and proceeding to use ncurses will lead to a segmentation fault. Does anyone know how to use ncurses without it creating its own sub-window?
In my case I am using ncurses to do a ""selection prompt"" where options are displayed, and the user can go though them with arrow keys until the find one they want (enter key twice to select).
#include <iostream>
#include <vector>
#include <string>
#include <ncurses.h>

std::string prompt(const std::vector<std::string> &options, const std::string &message)
{
    // * NCURSES init
    initscr();
    cbreak();
    keypad(stdscr, true);
    noecho();

    int choice;
    int highlight = 0;
    int num_options = options.size();

    while (true)
    {
        // * Clear line
        int y, x;
        getyx(stdscr, y, x);
        move(y, 0);
        clrtoeol();

        // * Display options
        printw(""%s: "", (message).c_str());
        for (int i = 0; i < num_options; ++i)
        {
            if (i == highlight)
                attron(A_REVERSE);

            printw(""%s "", options[i].c_str());
            attroff(A_REVERSE);
        }

        // * Get user input
        choice = getch();

        // * Decoding selection
        switch (choice)
        {
        case KEY_RIGHT:
            highlight = (highlight - 1 + num_options) % num_options;
            break;
        case KEY_LEFT:
            highlight = (highlight + 1) % num_options;
            break;
        case '\n':
            refresh();
            getch();
            endwin();
            printf(""\n"");
            return options[highlight];
        default:
            break;
        }
    }

    // * NCURSES cleanup
    refresh();
    getch();
    endwin();
}

int main()
{
    std::cout << ""Hello World!\n"";
    prompt({""agree"", ""disagree""}, ""choose one"");
    std::cout << ""Hello World #2\n"";

    return 0;
}

If you run this code, you will notice that in the ncurses sub window, only the stuff in the prompt function will appear. After selection, you will see the first message (""Hello World""), then a blank line where the prompt should be, then the second message (""Hello World #2"").
","Ncurses does not work without using it's own sub-window.
",prompt
openai api error 429 quotyou exceeded your current quota please check your plan and billing detailsquot,"I'm making a Python script to use OpenAI via its API. However, I'm getting this error:

openai.error.RateLimitError: You exceeded your current quota, please check your plan and billing details

My script is the following:
#!/usr/bin/env python3.8
# -*- coding: utf-8 -*-

import openai
openai.api_key = ""<My PAI Key>""

completion = openai.ChatCompletion.create(
  model=""gpt-3.5-turbo"",
  messages=[
    {""role"": ""user"", ""content"": ""Tell the world about the ChatGPT API in the style of a pirate.""}
  ]
)

print(completion.choices[0].message.content)

I'm declaring the shebang python3.8, because I'm using pyenv. I think it should work, since I did 0 API requests, so I'm assuming there's an error in my code.
","TL;DR: You need to upgrade to a paid plan. Set up a paid account, add a credit or debit card, and generate a new API key if your old one was generated before the upgrade. It might take 10 minutes or so after you upgrade to a paid plan before the paid account becomes active and the error disappears.
Problem
As stated in the official OpenAI documentation:




TYPE
OVERVIEW




RateLimitError
Cause: You have hit your assigned rate limit. Solution: Pace your requests. Read more in our rate limit guide.




Also, read more about Error Code 429 - You exceeded your current quota, please check your plan and billing details:

This (i.e., 429) error message indicates that you have hit your maximum monthly
spend (hard limit) for the API. This means that you have consumed all
the credits or units allocated to your plan and have reached the limit
of your billing cycle. This could happen for several reasons, such as:

You are using a high-volume or complex service that consumes a lot of credits or units per request.

You are using a large or diverse data set that requires a lot of requests to process.

Your limit is set too low for your organization’s usage.




Did you sign up some time ago?
You're getting error 429 because either you used all your free tokens or 3 months have passed since you signed up.
As stated in the official OpenAI article:

To explore and experiment with the API, all new users get $5
worth of free tokens. These tokens expire after 3 months.
After the quota has passed you can choose to enter billing information
to upgrade to a paid plan and continue your use of the API on
pay-as-you-go basis. If no billing information is entered you will
still have login access, but will be unable to make any further API
requests.
Please see the pricing page for the latest information on
pay-as-you-go pricing.

Note: If you signed up earlier (e.g., in December 2022), you got $18 worth of free tokens.
Check your API usage in the usage dashboard.
For example, my free trial expires tomorrow and this is what I see right now in the usage dashboard:

This is how my dashboard looks after expiration:

If I run a simple script after my free trial has expired, I get the following error:

openai.error.RateLimitError: You exceeded your current quota, please check your plan and billing details.


Did you create your second OpenAI account?
You're getting error 429 because you created a second OpenAI account with the same phone number. It seems like free credit is given based on phone numbers.
As explained on the official OpenAI forum by @SapphireFelineBytes:

I created an Open AI account in November and my $18 credits expired on
March 1st. So, like many of you here, I tried creating a new account
with a different email address, but same number. They gave me $0
credits.
I tried now with a different phone number and email. This time I got
$5 credits.

It's confirmed that free credit is given based on phone numbers, as explained on the official OpenAI forum by @logankilpatrick:

Also note, you only get free credits for the first account associated
with your phone number. Subsequent accounts are not granted free credits.

Solution
Try to do the following:

Set up paid account.
Add a credit or debit card.
Generate a new API key if your old API key was generated before you upgraded to the paid plan.

When you upgrade to a paid plan, don't expect the error to disappear immediately, as @dcferreira mentioned in the comment above. It might take a few minutes after the upgrade before the error disappears.
In the comment below, @JoeMornin confirmed that it took 10 minutes for his paid account to become active. In the meantime, he was getting the following error:

You've reached your usage limit. See your usage dashboard and billing settings for more details. If you have further questions, please contact us through our help center at help.openai.com.

",prompt
is it possible to configure the irb prompt to change dynamically,"I'd like to navigate around the filesystem in IRB and have the prompt change to reflect the current working directory, but I can't figure out how to make the prompt update after each command. Ultimately I'd like to use IRB in day to day work a lot more and let bash slip away. I tried this in my .irbrc:
require 'fileutils'
include FileUtils

IRB.conf[:PROMPT][:CUSTOM] = {
    :PROMPT_N => ""\e[1m:\e[m "",
    :PROMPT_I => ""\e[1m#{pwd} >\e[m "",
    :PROMPT_S => ""FOO"",
    :PROMPT_C => ""\e[1m#{pwd} >\e[m "",
    :RETURN => """"
}
IRB.conf[:PROMPT_MODE] = :CUSTOM

But the IRB prompt is not updated:
julianmann@mango:~ > irb
/users/julianmann > puts pwd
/users/julianmann
/users/julianmann > cd 'dev'
/users/julianmann > puts pwd
/users/julianmann/dev
/users/julianmann > 

I'd really like the prompt to change.
","Here's a quick hack to get the working dir. It's sort of fragile, but it worked on ruby 1.8.7 and 1.9.2.
Set your prompt string to something like this:
""%N(%m):%03n:%i %~> "".tap {|s| def s.dup; gsub('%~', Dir.pwd); end }

The ""%~"" directive is not understood by irb itself, so I used it to do the replacement. This hack relies on the fact that irb calls dup to generate the prompt.
",prompt
how to display image duplicate count from prompt in html,"I want to display a certain amount of duplicates for 2 seperate images. The web page prompts the user for the speed, which i have done, and for how many duplicates of each image they want.


function show_image() {
  var img = document.createElement(""img"");
  var img2 = document.createElement(""img"");
  img.src = ""map.png"";
  img2.src = ""figure-front.png"";
  document.body.appendChild(img);
  document.body.appendChild(img2);
  setTimeout(""show_image()"", speed);
  max = 0;
  max++
  if (max = count) {
    return;
  }
}
<!DOCTYPE HTML>

<html lang=""en"">

<head>
  <meta charset=""UTF-8"">
  <title>JavaScript Slide Show</title>
  <script src=""ShowImages3.js""></script>
  <script>
    var speed = prompt(""Type how fast you want to duplicate"", "" "");
    var count = prompt(""Type how many image of each you want to duplicate"", "" "");
  </script>
</head>

<body onLoad=""show_image()"">

</body>

</html>



However as you can see it runs infinitely apparently
","You needed to move the setTimeout and use == or === for comparison. = is assignment
It is not recommended to use body onload for anything. Instead use an eventListener.
Here is a cleaner version using setInterval. Also I use >= for the test
And I assign an empty string to the speed and prompt values so I can test using if (speed && prompt) after converting to number using the unary plus. I did not add testing for other than numbers and empty string by the way.


let count = 0, cnt = 0, tId;

const show_image = () => {
  if (cnt >= count) {
    clearInterval(tId); // stop
    return;
  }
  const img = document.createElement(""img"");
  const img2 = document.createElement(""img"");
  img.src = ""map.png"";
  img2.src = ""figure-front.png"";
  document.body.appendChild(img);
  document.body.appendChild(img2);
  cnt++; // next
};
window.addEventListener(""DOMContentLoaded"", () => {
  let speed = +prompt(""Type how fast you want to duplicate"", """");
  count = +prompt(""Type how many image of each you want to duplicate"", """");

  if (speed && prompt) tId = setInterval(show_image, speed)

});



Or have this which encapsulates the variables


const generate = (count, speed) => {
  let cnt = 0;
  let tId;

  const addImages = () => {
    if (cnt >= count) {
      clearInterval(tId); // stop
      return;
    }
    const img = document.createElement(""img"");
    const img2 = document.createElement(""img"");
    img.src = ""map.png"";
    img2.src = ""figure-front.png"";
    document.body.appendChild(img);
    document.body.appendChild(img2);
    cnt++; // next
  };

  tId = setInterval(addImages, speed);
};

window.addEventListener(""DOMContentLoaded"", () => {
  let speed = +prompt(""Type how fast you want to duplicate"", """");
  let count = +prompt(""Type how many image of each you want to duplicate"", """");

  if (speed && count) {
    generate(count, speed);
  }
});



",prompt
write a script to respond to prompts with specified text,"I am writing a script which will automatically capture a screenshot and trigger iSight to take a still photo when the script is run, then upload it to FTP.  Everything is perfect other then one thing, my FTP requires a username and password to be entered when prompted (after both images are captured)
heres what I have:
echo “GOLDENEYE ACTIVATED….”
screencapture -x screen.jpg
imagesnap Mugshot.jpg
ftp <YourServerHere>

Terminal Responds:
Connected to <YourServerHere>

Now, Username must be entered when this appears:
220 Welcome to <ServerDomain>, FTP server standing by ...
Name (<ServerName>.<DomainName>.:<Name>): <Type UserName Here - Hit Enter>

And, Password will be entered upon successful entry of a valid Username:
331 Hello <UserName>, your FTP account password is required:
Password: <Type Your Password Here - Hit Enter>

After the proper credentials are entered, Terminal will respond:
230-Login successful, your current directory is /
230 0 Kbytes used (0%) - authorized: 7340032 Kb
Remote system type is UNIX.
Using binary mode to transfer files.

And, to upload the images:
ftp> put /Mugshot.jpeg

ftp> put /Screen.jpeg

I am attempting to Automate the username and password entry after the prompts are given, in order for the script to be successful
the text would be, ""username"" and ""password"" to be entered after the each prompt is given.
i am completely new to terminal and scripts, just playing around in my spare time, please excuse anything which is incorrect here, thanks for ANY help and education!
","A general solution is the expect command.
But for your task, it's an overkill.

The ftp reads commands from a standard input.
So just do:
ftp -n < command.txt

The -n prevents the automatic prompts for username/password. We provide these explicitly using the user command below.
The commands.txt file will look like:
open host
user user password
put /path
bye

",prompt
chatgpt doesnt want to answer correctly based on prepared prompt,"Now I am researching chat GPT prompt before requesting paid chatGPT API.
but my prompt cannot make ChatGPT understand the requirements.
I need help fixing it how to make a proper prompt, providing recommended link/docs to learn is also appreciated.
problem:

returning food that contain allergy
the currency not following the country/location.


","I removed some stuff that seemed unnecessary and tried the new JSON mode and got the right response.  With that being said, though, LLM's are not deterministic and errors can happen. You could try to fine tune your model to decrease the chances of errors cropping up or somehow check for keywords after the response has been given or make multiple calls to the api asking to double-check for allergens and correctness.
import os
from openai import OpenAI
from dotenv import load_dotenv
load_dotenv()
client = OpenAI()

response = client.chat.completions.create(
  model=""gpt-3.5-turbo-1106"",
  response_format={ ""type"": ""json_object"" },
  messages=[
    {""role"": ""system"", ""content"": '''You are a recipe generator designed to take allergies, goals, country, budget and interests and output 8 recipes as JSON format with status and data which should contain ingredients, total_calories, nutrition, budget.*allergies must be  avoided! NEVER RETURN any food that contains allergens!'''},
    {""role"": ""user"", ""content"": '''allergies: poultry, nuts. goals: boost athletic performance because i am fat and can only do 20 push ups. country: UK budget: 5-20 USD Dollars '''

}
  ]
)
print(response.choices[0].message.content)



{
  ""status"": ""success"",
  ""data"": [
    {
      ""recipe_name"": ""Quinoa Salad with Roasted Vegetables"",
      ""ingredients"": [
        ""quinoa"",
        ""bell peppers"",
        ""zucchini"",
        ""red onion"",
        ""olive oil"",
        ""lemon juice"",
        ""garlic"",
        ""fresh parsley"",
        ""salt"",
        ""pepper""
      ],
      ""total_calories"": 320,
      ""nutrition"": {
        ""carbs"": ""40g"",
        ""protein"": ""8g"",
        ""fat"": ""14g""
      },
      ""budget"": ""10 USD""
    },
    {
      ""recipe_name"": ""Black Bean and Sweet Potato Tacos"",
      ""ingredients"": [
        ""black beans"",
        ""sweet potatoes"",
        ""taco shells"",
        ""avocado"",
        ""tomato"",
        ""red cabbage"",
        ""lime"",
        ""cilantro"",
        ""cumin"",
        ""paprika"",
        ""salt"",
        ""pepper""
      ],
      ""total_calories"": 380,
      ""nutrition"": {
        ""carbs"": ""45g"",
        ""protein"": ""10g"",
        ""fat"": ""15g""
      },
      ""budget"": ""15 USD""
    },
    {
      ""recipe_name"": ""Mediterranean Chickpea Salad"",
      ""ingredients"": [
        ""chickpeas"",
        ""cucumber"",
        ""tomato"",
        ""red onion"",
        ""kalamata olives"",
        ""feta cheese"",
        ""olive oil"",
        ""red wine vinegar"",
        ""oregano"",
        ""salt"",
        ""pepper""
      ],
      ""total_calories"": 290,
      ""nutrition"": {
        ""carbs"": ""35g"",
        ""protein"": ""12g"",
        ""fat"": ""13g""
      },
      ""budget"": ""10 USD""
    },
    {
      ""recipe_name"": ""Vegetable Stir-Fry with Tofu"",
      ""ingredients"": [
        ""tofu"",
        ""broccoli"",
        ""carrots"",
        ""bell peppers"",
        ""snap peas"",
        ""soy sauce"",
        ""sesame oil"",
        ""garlic"",
        ""ginger"",
        ""rice"",
        ""green onions""
      ],
      ""total_calories"": 350,
      ""nutrition"": {
        ""carbs"": ""40g"",
        ""protein"": ""15g"",
        ""fat"": ""12g""
      },
      ""budget"": ""12 USD""
    },
    {
      ""recipe_name"": ""Quinoa Stuffed Bell Peppers"",
      ""ingredients"": [
        ""bell peppers"",
        ""quinoa"",
        ""black beans"",
        ""corn"",
        ""tomato"",
        ""onion"",
        ""garlic"",
        ""cumin"",
        ""paprika"",
        ""salt"",
        ""pepper""
      ],
      ""total_calories"": 300,
      ""nutrition"": {
        ""carbs"": ""35g"",
        ""protein"": ""10g"",
        ""fat"": ""11g""
      },
      ""budget"": ""10 USD""
    },
    {
      ""recipe_name"": ""Cauliflower Rice Bowl with Black Beans and Avocado"",
      ""ingredients"": [
        ""cauliflower"",
        ""black beans"",
        ""avocado"",
        ""corn"",
        ""red cabbage"",
        ""lime"",
        ""cilantro"",
        ""cumin"",
        ""paprika"",
        ""salt"",
        ""pepper""
      ],
      ""total_calories"": 280,
      ""nutrition"": {
        ""carbs"": ""30g"",
        ""protein"": ""9g"",
        ""fat"": ""10g""
      },
      ""budget"": ""10 USD""
    },
    {
      ""recipe_name"": ""Mushroom and Spinach Quinoa Risotto"",
      ""ingredients"": [
        ""quinoa"",
        ""mushrooms"",
        ""spinach"",
        ""onion"",
        ""garlic"",
        ""vegetable broth"",
        ""white wine"",
        ""parmesan cheese"",
        ""olive oil"",
        ""salt"",
        ""pepper""
      ],
      ""total_calories"": 310,
      ""nutrition"": {
        ""carbs"": ""35g"",
        ""protein"": ""11g"",
        ""fat"": ""12g""
      },
      ""budget"": ""12 USD""
    },
    {
      ""recipe_name"": ""Spicy Lentil and Chickpea Stew"",
      ""ingredients"": [
        ""lentils"",
        ""chickpeas"",
        ""onion"",
        ""carrots"",
        ""celery"",
        ""garlic"",
        ""tomato"",
        ""vegetable broth"",
        ""cumin"",
        ""coriander"",
        ""smoked paprika"",
        ""cayenne pepper"",
        ""salt"",
        ""pepper"",
        ""lemon""
      ],
      ""total_calories"": 340,
      ""nutrition"": {
        ""carbs"": ""40g"",
        ""protein"": ""14g"",
        ""fat"": ""13g""
      },
      ""budget"": ""10 USD""
    }
  ]
}

",prompt
how to write custom prompttemplate in llama2 model using langchain,"I am using Llama2[7b model]-hugging face and lang-chain to do a simple address segregation/classification task. I want the model to find the city, state and country from the input string.I want my answer/query formatted in a particular way for a question-answering/ text-generation task.I understand that i can use FewShotPromptTemplate, where in i can show some examples to the LLM and get the output in the format i want.
I generated a few examples to feed in as samples :
examples = [
    {""input"": ""Plot No. 7, Sector 22, Noida, Uttar Pradesh, 201301, India"",
     ""Address"": ""Plot No. 7, Sector 22, Noida"",
     ""City"" : ""Noida"",
     ""State"" : ""Uttar Pradesh"",
     ""Country"" : ""India""},


    {""input"": ""Banjara Hills, Telangana, 500034, India"",
     ""Address"": ""Banjara Hills"",
     ""City"" : ""Not present"",
     ""State"" : ""Telangana"",
     ""Country"" : ""India""},

]

I set the template
example_formatter_template = """"""
input: {input},
Address : {Address},
City : {Address},
State : {State},
Country : {Country},
         \n
""""""
# prompt
example_prompt = PromptTemplate(
    input_variables=[""input"", ""Address"", ""City"", ""State"", ""Country""],
    template=example_formatter_template)

few_shot_prompt = FewShotPromptTemplate(
    examples=examples,
    example_prompt=example_prompt,
    prefix=""What is the address, city, state, country in the string : "",
    suffix=""input: {input}\n "",
    input_variables=[""input""],
    example_separator=""\n"")


chain = LLMChain(llm=llm, prompt=few_shot_prompt, verbose = True)

# Run the chain only specifying the input variable.
print(chain.run(""B-12, Gandhi Colony, Bhopal, Madhya Pradesh, 462016, India""))


Here is an example of what i want :
    {""input"": ""B-12, Gandhi Colony, Bhopal, Madhya Pradesh, 462016, India"",

     ""Address"": ""B-12, Gandhi Colony"",
     ""City"" : ""Bhopal"",
     ""State"" : ""Madhya Pradesh"",
     ""Country"" : ""India""},



I keep getting : format the expected output correctly from the model. And nothing is hence returned.
Additionally, I want to prevent the model from adding any extra information which
is not present in the context/string otherwise the queries take very long to respond.
i.e return '' or not found if city or state or country is not present in sting.
can someone help ?
","Try after changing the examples and the format to this:
{""input"": ""Banjara Hills, Telangana, 500034, India"",
 ""output"":
 """"""
 Address: Banjara Hills
 City: Not present
 State: Telangana
 Country: India
 """"""
 },

Refer to this as well
https://python.langchain.com/docs/modules/model_io/prompts/prompt_templates/few_shot_examples
",prompt
there are two onclick events in a modal dialog into one onclick event google script,"I am using modal dialog to select a range from here
Prompt user for range in .gs function, pass array to html script and re-focus on the HTML dialog
There are two events in a modal dialog.
The first selects a range, and the second closes the dialog.
    <input type=""button"" id=""btn1"" value=""Range Selected"" onClick=""selectRange();"" />
    <br /><input type=""button"" id=""btn2"" value=""close"" onClick=""clearAndClose();""; disabled=""true"" />

How do you combine these into one event so that when the range is selected, the dialog closes without clicking the close button?
","Make the selectRange() a server side function:
gs:
function saveRange() {
  PropertiesService.getScriptProperties().setProperty(""range"",SpreadsheetApp.getActiveRange().getA1Notation());
  SpreadsheetApp.getActive().toast(PropertiesService.getScriptProperties().getProperty(""range""));
}


function selectRangeDialog() {
  SpreadsheetApp.getUi().showModelessDialog(HtmlService.createHtmlOutputFromFile(""ah2""),""Range Dialog"")
}

html:
<!DOCTYPE html>
<html lang=""en"">
  <head>
    <base target=""_top"">
    <title>My Html</title>
  </head>
  <body>
    <h1>Please Select a Range and Press Save</h1>
    <br><input type=""button"" value=""Save"" onClick=""google.script.run.withSuccessHandler((v) => google.script.host.close()).saveRange();"">
  </body>
</html>


google-apps-script reference
google.script.run
javascript reference
Learn More

Here's another way to accomplish the same thing without requiring a separate html file.
function saveRange() {
  const ss = SpreadsheetApp.getActive();
  PropertiesService.getScriptProperties().setProperty(""range"", `${SpreadsheetApp.getActiveSheet().getName()}!${SpreadsheetApp.getActiveRange().getA1Notation()}`);
  ss.toast(PropertiesService.getScriptProperties().getProperty(""range""));
}

function launchSaveRangeDialog() {
  const html = '<!DOCTYPE html> <html lang=""en""> <head> <base target=""_top""> <title>My Html</title> </head> <body> <h1>Please Select a Range and Press Save</h1> <br><input type=""button"" value=""Save"" onClick=""google.script.run.withSuccessHandler((v) => google.script.host.close()).saveRange();""> </body> </html>';
  SpreadsheetApp.getUi().showModelessDialog(HtmlService.createHtmlOutput(html), ""Save Dailog"");
}

",prompt
referenceerror prompt is undefined how would i fix this in javascript,"I am working on creating different meal menus in this restaurant I have created using JavaScript. I was on my first couple lines of added code when I came across this ReferenceError saying that prompt is undefined. If you need to see full context you can reference the original code here https://repl.it/@qwerty_________/Welcome-to-the-Drivethrough and the code I'm working on here https://repl.it/@qwerty_________/DullFreeMisrac. 
The official error is right here.
ReferenceError: prompt is not defined
at evalmachine.<anonymous>:2:12
at Script.runInContext (vm.js:74:29)
at Object.runInContext (vm.js:182:6)
at evaluate (/run_dir/repl.js:133:14)
at ReadStream.<anonymous> (/run_dir/repl.js:116:5)
at ReadStream.emit (events.js:180:13)
at addChunk (_stream_readable.js:274:12)
at readableAddChunk (_stream_readable.js:261:11)
at ReadStream.Readable.push (_stream_readable.js:218:10)
at fs.read (fs.js:2124:12)

This is on JavaScript. I have already tried changing it to window.prompt rather than just prompt and it said that window was undefined.


	var nam = prompt(""Hi, Welcome to the Drivethrough!! What is your name?"")_x000D_
	var mprice = 4_x000D_
	var bprice = 8_x000D_
	var cprice = 6_x000D_
	var fprice = 2_x000D_
	var oprice = 3_x000D_
	var fmprice = 5_x000D_
	var gprice = 0_x000D_
	var price = 0_x000D_
	var fcmprice = 10_x000D_
	var cmprice = 4_x000D_
	var kkkprice = 9_x000D_
	var dddprice = 10_x000D_
	var mmmprice = 12_x000D_
	var icprice = 6_x000D_
	var resprice = 999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999_x000D_
	var superprice = 1



I was expecting it to bring up the popup box asking me my name and then me select the breakfast menu to test out chocolate milk, but it instead brought up the error code telling me that the prompt was undefined. 
Thank you so much for taking a look at my question. :) 
","It seems you run this code in a Node.js environment, in the site that provides this environment. window object is not defined in this environment. You can try to test you script in a browser console or from inside an HTML file.
",prompt
how to change the mysql prompt from the command line,"I bet this is easy and I have surely overseen something:
I am trying to set the ""internal"" MySQL prompt from the Bash shell as an argument to the command mysql.
The manual makes reference a command line option called --named-command=str where I assumed that I could pass any of the commands referred here:
mysql Command-line Client
But an version of the command below return an error after connecting to the DB:
mysql --named-commands='\R >'
mysql --named-commands=""\R >""

The error reads:

Warning: mysql: ignoring option '--named-commands' due to invalid value '\R > '

I thus assume that I am missing something. Please note that I have already gone through the MySQL manual/s (both, the Oracle site and the MariaDB site). The fact is that the question itself is not easy to define as when I t yto google for ""Mysql Command line option"" it normally takes me to the ""internal"" MySQL command line (or more exactly, the MySQL command line client).
Thanks for your suggestion :)
","mysql -h hostname -u username -p --prompt myCustomPrompt\>
Be sure to escape > with \ in order to avoid a bash syntax error.
See the final section of mysql client commands for instructions on special characters to use in the prompt command.
",prompt
change julia promt to include evalutation numbers,"When debugging or running julia code in REPL, I usually see error messages showing ... at ./REPL[161]:12 [inlined].... The number 161 means the 161-th evaluation in REPL, I guess. So my question is could we show this number in julia's prompt, i.e. julia [161]> instead of julia>?
","One of the advantages of Julia is its ultra flexibility. This is very easy in Julia 0.7 (nightly version).
julia> repl = Base.active_repl.interface.modes[1]
""Prompt(\""julia> \"",...)""

julia> repl.prompt = () -> ""julia[$(length(repl.hist.history) - repl.hist.start_idx + 1)] >""
#1 (generic function with 1 method)

julia[3] >

julia[3] >2
2

julia[4] >f = () -> error(""e"")
#3 (generic function with 1 method)

julia[5] >f()
ERROR: e
Stacktrace:
 [1] error at .\error.jl:33 [inlined]
 [2] (::getfield(, Symbol(""##3#4"")))() at .\REPL[4]:1
 [3] top-level scope

You just need to put the first 2 lines onto your ~/.juliarc and enjoy~
Since there are several changes in the REPL after julia 0.7, these codes do not work in old versions.
EDIT: Well, actually there need a little bit more efforts to make it work in .juliarc.jl. Try this code:
atreplinit() do repl
    repl.interface = Base.REPL.setup_interface(repl)
    repl = Base.active_repl.interface.modes[1]
    repl.prompt = () -> ""julia[$(length(repl.hist.history) - repl.hist.start_idx + 1)] >""
end

",prompt
connection with netssh2php to huawei olt ma5680t is very slow,"I implemented a php script to communicate with my Huawei Olt MA5680T through the phpseclib SSH2.php library but the execution time of any remote operation on the Olt is extremely slow, about 40 seconds.
I tried sending simpler commands but the result doesn't change.
I'm currently using this function to connect via ssh to the device:
function sessione_ssh($olt_id) {
global $ssh;
global $ip_olt;

$ssh = new Net_SSH2($row_olt['olt_ip']);
if (!$ssh->login($row_olt['olt_user'], $row_olt['olt_pass'])) {
exit('Login Failed');}
$ssh->setWindowColumns(200);
$ssh->read('MA5680T>');
$ssh->write(""enable\n"");
$ssh->read('MA5680T#');
$ssh->write(""mmi-mode enable\n"");
$ssh->read('MA5680T#');
}





function check($pppoe) {
global $ssh;
global $frame;
global $board;
global $port;
global $ontid;
global $ifindex;
global $ip_olt;
$ont_eth_port='1';
// restituisce solo il valore senza tipo stringa
snmp_set_quick_print(1);
snmp_read_mib ( 'ma5600t.txt');
ricava_info_ont($pppoe);
$ssh->disconnect();
//ont_descrizione
echo 'PPPoE Cliente: ' . str_replace('""','',snmp2_get ($ip_olt, ""private"", ""1.3.6.1.4.1.2011.6.128.1.1.2.43.1.9."" . $ifindex . ""."" . $ontid)) . ""<br>"";
echo 'Indirizzo Ip: ' . snmp2_get ($ip_olt, ""private"", ""1.3.6.1.4.1.2011.6.3.27.1.3."" . $ifindex . ""."" . $ontid) . ""<br>"";
//ont_seriale
echo 'Seriale ONT: ' . str_replace('""','',str_replace("" "","""",snmp2_get ($ip_olt, ""private"", ""1.3.6.1.4.1.2011.6.128.1.1.2.43.1.3."" . $ifindex . ""."" . $ontid))) . ""<br>"";
$ont_status= snmp2_get ($ip_olt, ""private"", ""1.3.6.1.4.1.2011.6.128.1.1.2.46.1.15."" . $ifindex . ""."" . $ontid);  //1 online  2 offline
if ($ont_status=='1') {
echo 'Stato: Online<br>';
}
if ($ont_status=='2') {
echo 'Stato: Offline<br>';
}
//ont_rx_segnale
echo 'Segnale RX: ' . (snmp2_get ($ip_olt, ""private"", ""1.3.6.1.4.1.2011.6.128.1.1.2.51.1.4."" . $ifindex . ""."" . $ontid))/100 . "" db<br>"";
//ont_distanza
echo 'Distanza: ' . snmp2_get ($ip_olt, ""private"", ""1.3.6.1.4.1.2011.6.128.1.1.2.46.1.20."" . $ifindex . ""."" . $ontid) . "" metri<br>"";
//ont_ultimo_uptime
echo 'Ultimo Uptime: ' . converti_data(snmp2_get ($ip_olt, ""private"", ""1.3.6.1.4.1.2011.6.128.1.1.2.46.1.22."" . $ifindex . ""."" . $ontid)) . ""<br>"";
//ont_ultimo_downtime
echo 'Ultimo Downtime: ' . converti_data(snmp2_get ($ip_olt, ""private"", ""1.3.6.1.4.1.2011.6.128.1.1.2.46.1.23."" . $ifindex . ""."" . $ontid)) . ""<br>"";
$ont_motivo_disconnessione=snmp2_get ($ip_olt, ""private"", ""1.3.6.1.4.1.2011.6.128.1.1.2.46.1.24."" . $ifindex . ""."" . $ontid); //,15 LOKI(Loss of key synch with ONUi),18 deactived ONT due to the ring,30 shut down ONT optical module,31 reset ONT by ONT command,32 reset ONT by ONT reset button,33 reset ONT by ONT software,34 deactived ONT due to broadcast attack,35 operator check fail,37 rogue ONT detected by itself,-1 query fails.
if ($ont_motivo_disconnessione=='1') {
echo 'Causa disconnessione: LOS (Loss of signal)<br>';
}
if ($ont_motivo_disconnessione=='2') {
echo 'Causa disconnessione: LOSi (Loss of signal for ONUi) or LOBi (Loss of burst for ONUi)<br>';
}
if ($ont_motivo_disconnessione=='3') {
echo 'Causa disconnessione: LOFI (Loss of frame of ONUi)<br>';
}
if ($ont_motivo_disconnessione=='4') {
echo 'Causa disconnessione: SFI (Signal fail of ONUi)<br>';
}
if ($ont_motivo_disconnessione=='5') {
echo 'Causa disconnessione: LOAI (Loss of acknowledge with ONUi)<br>';
}
if ($ont_motivo_disconnessione=='6') {
echo 'Causa disconnessione: LOAMI (Loss of PLOAM for ONUi)<br>';
}
if ($ont_motivo_disconnessione=='7') {
echo 'Causa disconnessione: Deactive ONT fails<br>';
}
if ($ont_motivo_disconnessione=='8') {
echo 'Causa disconnessione: Deactive ONT success<br>';
}
if ($ont_motivo_disconnessione=='9') {
echo 'Causa disconnessione: Reset ONT<br>';
}
if ($ont_motivo_disconnessione=='10') {
echo 'Causa disconnessione: Re-register ONT<br>';
}
if ($ont_motivo_disconnessione=='11') {
echo 'Causa disconnessione: Pop up fail<br>';
}
if ($ont_motivo_disconnessione=='13') {
echo 'Causa disconnessione: Dying-Gasp<br>';
}
//ont_uptime_tratta
echo 'Uptime tratta Pon: ' . date_format (new DateTime(str_replace('""','',snmp2_get ($ip_olt, ""private"", ""1.3.6.1.4.1.2011.6.128.1.1.2.101.1.6."" . $ifindex . ""."" . $ontid . "".0""))), 'd/m/Y H:i:s') . ""<br>"";
$ont_power_eth=snmp2_get ($ip_olt, ""private"", ""1.3.6.1.4.1.2011.6.128.1.1.2.62.1.5."" . $ifindex . ""."" . $ontid . ""."" . $ont_eth_port); //1on,2off
if ($ont_power_eth=='1') {
echo 'Porta Eth: On<br>';
}
if ($ont_power_eth=='2') {
echo 'Porta Eth: Off<br>';
}
$ont_registrazione_eth=snmp2_get ($ip_olt, ""private"", ""1.3.6.1.4.1.2011.6.128.1.1.2.62.1.22."" . $ifindex . ""."" . $ontid . ""."" . $ont_eth_port); //1linkup,2linkdown
if ($ont_registrazione_eth=='1') {
echo 'Registrazione Eth: On<br>';
}
if ($ont_registrazione_eth=='2') {
echo 'Registrazione Eth: Off<br>';
}
$ont_speed_eth=snmp2_get ($ip_olt, ""private"", ""1.3.6.1.4.1.2011.6.128.1.1.2.62.1.4."" . $ifindex . ""."" . $ontid . ""."" . $ont_eth_port);
if ($ont_speed_eth=='1') {
echo 'Negoziazione Eth: 10M manuale<br>';
}
if ($ont_speed_eth=='2') {
echo 'Negoziazione Eth: 100M manuale<br>';
}
if ($ont_speed_eth=='3') {
echo 'Negoziazione Eth: 1000M manuale<br>';
}
if ($ont_speed_eth=='4') {
echo 'Negoziazione Eth: Autonegoziazione<br>';
}
if ($ont_speed_eth=='5') {
echo 'Negoziazione Eth: 10M Autonegoziazione<br>';
}
if ($ont_speed_eth=='6') {
echo 'Negoziazione Eth: 100M Autonegoziazione<br>';
}
if ($ont_speed_eth=='7') {
echo 'Negoziazione Eth: 1000M Autonegoziazione<br>';
}
if ($ont_speed_eth=='8') {
echo 'Negoziazione Eth: 10000M manuale<br>';
}
if ($ont_speed_eth=='9') {
echo 'Negoziazione Eth: 10000M Autonegoziazione<br>';
}
if ($ont_speed_eth=='10') {
echo 'Negoziazione Eth: 2500M manuale<br>';
}
if ($ont_speed_eth=='11') {
echo 'Negoziazione Eth: 2500M Autonegoziazione<br>';
}
if ($ont_speed_eth=='-1') {
echo 'Negoziazione Eth: invalid<br>';
}
//ont_password_pon
echo 'Password Pon: ' . snmp2_get ($ip_olt, ""private"", ""1.3.6.1.4.1.2011.6.128.1.1.2.43.1.4."" . $ifindex . ""."" . $ontid) . ""<br>"";
}
// fine function check



unction ricava_info_ont($pppoe) {
global $ssh;
global $frame;
global $board;
global $port;
global $ontid;
global $ifindex;

//determina $id_olt

//per carcolare id olt se si tratta di cambio impianto usare var session per impianto vecchio e primaria vecchia altrimenti procedi normalmente
if($_SESSION['vecchioimpianto']) {
//decodifica impianto
$primariavecchia=explode(""-"",$_SESSION['vecchioimpianto']);
$primariavecchia=$primariavecchia['0'];
unset($_SESSION['vecchioimpianto']);
$sql_id_olt=""select id_olt from ftth_fibre_primarie where id_fibra_primaria ='"" . $primariavecchia . ""'"";
}
else {
$sql_id_olt=""select id_olt from ftth_fibre_primarie where id_fibra_primaria = (select SUBSTR(impianto, 1, 4) from ftth_ont where pppoe='"" . $pppoe . ""')"";
}
// fine se si tratta di cambio pacchetto
$rs_id_olt=CustomQuery($sql_id_olt);
$row_id_olt = db_fetch_array($rs_id_olt);
sessione_ssh($row_id_olt['id_olt']);
$ssh->write(""display ont info by-desc "" . $pppoe . ""\n"");
$out=nl2br($ssh->read());
// crea array con righe output
$logline = explode(""\n"",$out);
//ricava $ontid, $frame, $board e $port da descrizione (pppoe)
$parziale = explode("" "",$logline['9']);
$ontid=$parziale['9'];
$parziale2=explode(""/"",$parziale['2']);
$frame=$parziale2['0'];
$board=$parziale2['1'];
$port=$parziale2['2'];
$ifindex=4194304000 + ($board * (256 * 32) + $port * 256);
}
// fine function ricava_info_ont



With the same library I connect to other devices and execute scripts that are executed quickly, the problem occurs only with this device. I tried reading the logs but I don't detect any errors in the communication between the ssh2.php library and the olt MA5680T.
What could be the problem?
this is the logs:
https://pastebin.com/n9wHdLaj
Thank you so much for the support I receive from you for my project
UPDATE
The suggestion received from @neubert seems like the right way to decrease the script execution time. With the function I posted above I now actually have an execution time of about 3 seconds (absolutely acceptable). so using the same logic I inserted the correct prompt into all read() calls but it doesn't always work! when I enter a submenu of the prompt example: $ssh->write(""interface gpon "" . $frame . ""/"" . $board . ""\n""); then I set the read call like this: $ssh->read('MA5680T(config-if-gpon-' . $frame_ont . '/' . $board_ont . ')#'); but I always get long times with 10 second timeouts like before. This is the link to the prompt from the terminal https://pasteboard.co/ldCa4JlnsfnL.jpg
What could be the problem?
this is the log:
<- NET_SSH2_MSG_CHANNEL_DATA (since last: 0.03, network: 0.0003s)
00000000  00:00:00:02:00:00:00:14:0d:0a:0d:0a:4d:41:35:36  ............MA56
00000010  38:30:54:28:63:6f:6e:66:69:67:29:23              80T(config)#

<- NET_SSH2_MSG_CHANNEL_DATA (since last: 0.0122, network: 0.0001s)
00000000  00:00:00:02:00:00:00:13:69:6e:74:65:72:66:61:63  ........interfac
00000010  65:20:67:70:6f:6e:20:30:2f:31:33                 e gpon 0/13

<- NET_SSH2_MSG_CHANNEL_DATA (since last: 0.0181, network: 0.0003s)
00000000  00:00:00:02:00:00:00:21:0d:0a:0d:0a:4d:41:35:36  .......!....MA56
00000010  38:30:54:28:63:6f:6e:66:69:67:2d:69:66:2d:67:70  80T(config-if-gp
00000020  6f:6e:2d:30:2f:31:33:29:23                       on-0/13)#

-> NET_SSH2_MSG_CHANNEL_DATA (since last: 9.9108, network: 0.0002s)
00000000  00:00:00:01:00:00:00:0f:6f:6e:74:20:64:65:6c:65  ........ont dele
00000010  74:65:20:31:20:30:0a                             te 1 0.

<- NET_SSH2_MSG_CHANNEL_DATA (since last: 0.0303, network: 0.0001s)
00000000  00:00:00:02:00:00:00:0e:6f:6e:74:20:64:65:6c:65  ........ont dele
00000010  74:65:20:31:20:30                                te 1 0

<- NET_SSH2_MSG_CHANNEL_DATA (since last: 0.0744, network: 0.0001s)
00000000  00:00:00:02:00:00:00:35:0d:0a:20:20:4e:75:6d:62  .......5..  Numb
00000010  65:72:20:6f:66:20:4f:4e:54:73:20:74:68:61:74:20  er of ONTs that 
00000020  63:61:6e:20:62:65:20:64:65:6c:65:74:65:64:3a:20  can be deleted: 
00000030  31:2c:20:73:75:63:63:65:73:73:3a:20:31           1, success: 1

<- NET_SSH2_MSG_CHANNEL_DATA (since last: 0.0241, network: 0.0001s)
00000000  00:00:00:02:00:00:00:21:0d:0a:0d:0a:4d:41:35:36  .......!....MA56
00000010  38:30:54:28:63:6f:6e:66:69:67:2d:69:66:2d:67:70  80T(config-if-gp
00000020  6f:6e:2d:30:2f:31:33:29:23                       on-0/13)#

-> NET_SSH2_MSG_DISCONNECT (since last: 9.8742, network: 0.0002s)
00000000  00:00:00:0b:00:00:00:00:00:00:00:00              ............


I notice the presence of this strange exclamation point before each timeout
.......!....MA5680T(config-if-gpon-0/13)#
","I solved it thanks to @nubert by setting the correct prompt for each $ssh->read();
Example:
$ssh->read('MA5680T>');,$ssh->read('MA5680T(config)#');
",prompt
cmd command to check connected usb devices,"I would like to obtain by a command prompt a list of all USB devices connected to my computer (O.S. Windows 10). I've googled to find such a command, but all results seems useless to me or worse workless. Does anybody know how can I do that? 
Thank you
","you can download USBview and get all the information you need. Along with the list of devices, it will also show you the configuration of each device.
",prompt
detect empty value on prompt,"How to detect an empty value when user presses ok on prompt (and previously empties the prompt field)? I need to overwrite the old value with the new (empty) value. I'm doing this:
var oldVal = 'something';
var newVal = prompt(""Enter new value:"", oldVal);

Currently it returns null if user empties value and clicks ok.
But at the same time I need to check for null value because if user clicks 'cancel', it will return null, which I don't want as new value.
","It does not return null if the user hits OK - it will return an empty string.  You are probably not testing the return value properly.  If you want to test between the three different return states, you can do that like this:
var oldVal = 'something';
var newVal = prompt(""Enter new value:"", oldVal);
if (newVal === """") {
    // user pressed OK, but the input field was empty
} else if (newVal) {
    // user typed something and hit OK
} else {
    // user hit cancel
}

Working demo: http://jsfiddle.net/jfriend00/Kx2EK/

Your comment suggests that you're using this code to test the result:
if(!newVal || oldVal == newVal)return false;

When the user clears the field and presses OK, newVal will be """" (an empty string).  !newVal will be true so you will return false.  An empty string is a falsey value just like null.  You need to more explicitly check for null like this:
if (newVal === null || newVal === oldVal) {
    // cancel button was hit
    // or the same value was entered
    return false;
}

Working demo of this logic: http://jsfiddle.net/jfriend00/ynwBx/
Note: I'm using === to prevent the javascript interpreter from doing any type casting as I want to only explicitly check for null.
",prompt
how can i change the color of my prompt in zsh different from normal text,"To recognize better the start and the end of output on a commandline, I want to change the color of my prompt, so that it is visibly different from the programs output. As I use zsh, can anyone give me a hint?
","Here's an example of how to set a red prompt: 
PS1=$'\e[0;31m$ \e[0m'

The magic is the \e[0;31m (turn on red foreground) and \e[0m (turn off character attributes). These are called escape sequences. Different escape sequences give you different results, from absolute cursor positioning, to color, to being able to change the title bar of your window, and so on.
For more on escape sequences, see the wikipedia entry on ANSI escape codes
",prompt
how do i suppress yesno prompt when saving digitally signed mail in a format that is not secure,"I wrote VBA to save an email as a text file. 
For digitally signed emails it prompts me if I want to save the file in a format that is not secure (yes or no to continue). I want to suppress this warning. 
I tried the following:
With Application
     .EnableEvents = False
     .ScreenUpdating = False
End With
Application.DisplayAlerts = False
m.SaveAs savePath, olTXT 
With Application
     .EnableEvents = True
     .ScreenUpdating = ture
End With

And
Application.DisplayAlerts = False
m.SaveAs savePath, olTXT 
Application.DisplayAlerts = True

They both do not work (I get a compile error). How do I turn off these alerts in Outlook?
","Those properties are for Word or Excel.  Outlook does not have the equivalent functionality.  The best you can do is use the SendKeys method to simulate clicking the Yes or No buttons: https://msdn.microsoft.com/en-us/library/office/gg278655.aspx
",prompt
where is the anaconda prompt file located,"I am trying to locate the Anaconda Prompt file. I am currently running Anaconda Navigator 2.4.2. I never updated it before and i did not install any additional software.
I tried:

Windows Search bar: However, nothing shows up
C:\Users\xx\anaconda3: I searched every folder, but there is no prompt
C:\Windows\System32: there is no anaconda folder

Thanks a lot, really appreciate any help
","It's just a shortcut to cmd.exe with comamnds set to activate conda. When Anaconda is installed, it usually is under C:\Users\<your username>\AppData\Roaming\Microsoft\Windows\Start Menu\Programs\Anaconda3
It sounds like you have lsot it, so here is a guide how to recreate it:

create a shortcut to cmd.exe
Right Click->Properties and then change to the Shortcut Tab
In the target field, you will need to set it to %windir%\system32\cmd.exe ""/K"" <Path to activate.bat> <Path to Anaconda installation path>

For example, the target could look like this:
%windir%\system32\cmd.exe ""/K"" C:\Program Data\Anaconda3\Scripts\activate.bat C:\Program Data\Anaconda3

",prompt
prompt executing before page full load and block the consolelog,"I have an interesting question.
I have a prompt in JS. When I go to my page prompt immediately executes and blocks the console.logs. After doing some logic in the prompt, when I close the prompt all the console.logs immediately shows in the console. And it happens only when i join in the page at the first time. Next time all console.logs work fine. I the problem, but i don't know solution. I assume it's how the browser, and js works. There is no solution here.


let userAnswer = """";
let contvertedStr = """";

while (true) {
  const type = prompt(
    ""Program – Main Menu \nPlease enter 1, 2, 3, or exit.""
  ).toLowerCase();

  if (type === ""1"") {
    while (!userAnswer) {
      userAnswer = prompt(""Please enter a string."");

      if (!userAnswer) {
        console.log(""You need to enter something"");
      }
    }
  }

  if (type === ""2"") {
    if (!userAnswer) {
      console.log(""You need to first enter a String"");
      continue;
    }

    const splitString = userAnswer.split("" "");
    let newArray = [];

    for (const str of splitString) {
      if (str.length >= 5) {
        newArray.push(str + ""-bonk"");
      } else {
        newArray.push(str + ""-bink"");
      }
    }

    contvertedStr = newArray.join("" "");

    console.log(""String converted"");
  }

  if (type === ""3"") {
    if (contvertedStr) {
      console.log(contvertedStr);

      userAnswer = """";
      contvertedStr = """";

      continue;
    } else {
      console.log(""You need to first convert your String"");
      continue;
    }
  }

  if (type === ""exit"") {
    console.log(""Thanks for using the ROBOT Language Converter!"");
    break;
  }
}



I triend DOMContentloaded, window.onload, async, defer etc. It didn't work
","while (true) {
Unless used in say an async loop, this is never a good idea for Javascript, how the console behaves when the main Event loop is constantly blocked is likely undefined behaviour and should not be something you rely on.
Now there are 2 simple ways to solve this,

Use a setTimeout to continue the loop.
Use an async loop like pointed out above.

So for option 1 ->


function doLoop() {
  const r = prompt('type x to quit').toLowerCase();
  console.log(`you typed ${r}`);
  if (r !== 'x') 
    setTimeout(doLoop);
}

doLoop();  //lets start the loop



So basically in the above we are just replacing continue with setTimeout(doLoop), and removing the while (true) {
Now for option 2, using an async loop, this is the better option in the long run as it is much easier to expand on.


const breath = () => new Promise(resolve => setTimeout(resolve));

async function doLoop() {
  while (true) {
    await breath();
    const r = prompt('type x to quit').toLowerCase();
    console.log(`you typed ${r}`);
    if (r === 'x') break;
  }
}

doLoop();  //lets start the loop



Now in the above you can see we have used while (true) {, but the trick here when doing this you need to allow the event loop to run, this is what the breath function is doing.  The prompt function will still block, but those breath's will just allow some time for the event loop to do it's thing.  Also notice the async & await syntax, this is what does the magic to make what looks like sync code actually be async, and as such Browser / JS friendly.
Of course even better is not using prompt in the first place, most GUI libs these days have features for doing dialogs, and not only do these look nicer, they will also not block the browser main event loop like alert / prompt etc do.
",prompt
ansible playbook file selection task,"It's probably against best practice but using an Ansible playbook, is it possible to get a list of files from one task and then offer a user prompt to select one of the files to pass into a variable?
For example:
Choose file to select:
1. file1.txt
2. file2.txt
3. file3.txt
> 1

The playbook would theoretically pause for the user input and then pass the resulting file selection into a variable to use in a future task.
Many thanks in advance.
","Use pause. For example, given the files
shell> tree files
files
├── file1.txt
├── file2.txt
└── file3.txt

0 directories, 3 files


Update
the playbook
shell> cat playbook.yml 
- hosts: localhost

  vars:

    dir: files
    my_files: ""{{ q('fileglob', dir ~ '/*')|map('basename') }}""
    selected_file: ""{{ dir }}/{{ my_files[result.user_input|int - 1] }}""

  tasks:

    - pause:
        prompt: |-
          Choose file to select:
          {% for file in my_files %}
          {{ loop.index }} {{ dir }}/{{ file }}
          {% endfor %}
      register: result

    - debug:
        var: selected_file

gives (when selecting the 2nd file and typing '2<ENTER')
shell> ansible-playbook playbook.yml 

PLAY [localhost] ****

TASK [pause] ****
[pause]
Choose file to select:
1 files/file3.txt
2 files/file2.txt
3 files/file1.txt
:
2^Mok: [localhost]

TASK [debug] ****
ok: [localhost] => 
  selected_file: files/file2.txt

PLAY RECAP ****
localhost: ok=2 changed=0 unreachable=0 failed=0 skipped=0 rescued=0 ignored=0



Origin
the playbook below
shell> cat playbook.yml
- hosts: localhost
  tasks:
    - find:
        path: files
      register: result
    - set_fact:
        my_files: ""{{ result.files|map(attribute='path')|list|sort }}""
    - pause:
        prompt: |
          Choose file to select:
          {% for file in my_files %}
          {{ loop.index }} {{ file }}
          {% endfor %}
      register: result
    - debug:
        msg: ""selected file: {{ my_files[result.user_input|int - 1] }}""

gives (when selecting the 2nd file and typing '2<ENTER')
shell> ansible-playbook playbook.yml 

PLAY [localhost] ****

TASK [find] ****
ok: [localhost]

TASK [set_fact] ****
ok: [localhost]

TASK [pause] ****
[pause]
Choose file to select:
1 files/file1.txt
2 files/file2.txt
3 files/file3.txt
:
ok: [localhost]

TASK [debug] ****
ok: [localhost] => {
    ""msg"": ""selected file: files/file2.txt""
}

PLAY RECAP ****
localhost: ok=4 changed=0 unreachable=0 failed=0 skipped=0 rescued=0 ignored=0


",prompt
run command prompt commands,"Is there any way to run command prompt commands from within a C# application?  If so how would I do the following:
copy /b Image1.jpg + Archive.rar Image2.jpg

This basically embeds an RAR file within JPG image.  I was just wondering if there was a way to do this automatically in C#.
","this is all you have to do run shell commands from C#
string strCmdText;
strCmdText= ""/C copy /b Image1.jpg + Archive.rar Image2.jpg"";
System.Diagnostics.Process.Start(""CMD.exe"",strCmdText);

EDIT:
This is to hide the cmd window.
System.Diagnostics.Process process = new System.Diagnostics.Process();
System.Diagnostics.ProcessStartInfo startInfo = new System.Diagnostics.ProcessStartInfo();
startInfo.WindowStyle = System.Diagnostics.ProcessWindowStyle.Hidden;
startInfo.FileName = ""cmd.exe"";
startInfo.Arguments = ""/C copy /b Image1.jpg + Archive.rar Image2.jpg"";
process.StartInfo = startInfo;
process.Start();

EDIT 2:
It is important that the argument begins with /C, otherwise it won't work. As @scott-ferguson said: /C carries out the command specified by the string and then terminates.
",prompt
where exactly git bash for windows39 prompt is defined,"Both my office and home computers have Git Bash for Windows from the very same source (Git for Windows, that came with TortoiseGit), but Git Bash's prompt differs on both machines:

on office computer I have /c/path (branch) all green (this is, how I would like it to have),
on home computer I have only path (no branch) and all white.

Up until now I was told, that this is PS1 variable, kept in ~/.bashrc file. However, on both machines, this files is missing (executing notepad ~/.bashrc opens up empty Notepad).
I'm lost here. If ~/.bashrc doesn't exist, then from where Git Bash ""knows"", that it should display current branch, in a green prompt? And why the same doesn't happen on second machine?
EDIT: I also tried to compare c:\Program Files\Git\etc\ folder contents for both machines and contents of Git Bash.vbs file. There identical on both computers so I even more have no idea, how it can be, that Git Bash's prompt differs on both computers (and how to fix this).
Several answers (like this, this and this) has suggested me, that I should look for .bash_profile and .bash_prompt files. Unfortunately, these two also are missing on both my computers.
Exactly what file decides about that under Windows 7? Where else should I look for, and what PS1 variable's value should be, to have current branch shown in green prompt on both machines?
","Git on Windows almost always uses a bash shell. So, it's not Git setting the prompt as much as Bash does.
There are two ways to set prompts in Bash. One is the PS1 command which is fairly flexible, but is limited to a particular set of escape character sequences. Unfortunately, Git information isn't one of those escape sequences (although I suspect it'll come someday). You can use the second way to set the prompt by setting the PROMPT_COMMAND environment variable. If this is set, the $PROMPT_COMMAND is executed and used as the prompt instead of the PS1 environment variable.
When you install the standard Git with BASH, you're Git prompt is defined under the /etc/profile file. By the way, etc is a directory under where you've installed Git which is usually under %PROGRAMFILES% unless you changed it when you installed Git.
Under the /etc/profile script in line #156 in my version, you see the PS1 command being set and using $(__git_ps1) in $PS1 as a means of executing an external command in the prompt. (A third way I didn't mention previously). 
The __git_ps1 is a shell function. You'll also notice a bit above (line #154 in my version) that /etc/git-completion.bash is being sourced in as well as /etc/git-prompt.sh. It's /etc/git-prompt.sh that defines the __git_ps1 function (Line #273 in my version) is defined. You'll notice that the __git_ps1 function pulls in several other functions defined in /etc/git-prompt.sh.
So, in a very Rube Goldberg manner, the Git prompt is being defined in /etc/profile via defining $PS1 which pulls in /etc/git-prompt.sh which defines a __git_ps1 function that pulls in the __git_ps1_show_upstream function and the __git_ps1_colorize_gitstring function. Then, $PS1 uses the $(...) string as part of pulling in the __git_ps1 function into PS1.
You can define your own $HOME/.bash_profile to override the way the prompt is set to define your own prompt. And, when you do that, you can also use the __git_ps1 Bash function in your own prompt.
Or, you can simply decide not to touch anything, and just back away very slowly. After all, you may have actual work to do.
",prompt
how to capture display output from a command prompt in windows at any moment,"I'm tring to capture de information shown in a command prompt (cmd window) at a specific moment and send it to a text file.
I have an app in C/C++ that is launched by a batch script like this:
c:\myProgramInC.exe
echo ""Ending with error""
PAUSE

myProgramInC.exe is always running (with an infinite loop) so if my script gets to the echo command it means my app ended with an abend.
What I want to get is the previous lines before the end execution of the script since my myProgramInC.exe always prints info about what is going on and it would be very useful to see what it was happening when the error ocurred. Something like this
c:\myProgramInC.exe
**** Execution of process to get te previous N lines in the command prompt ****
echo ""Ending with error""
PAUSE

I know the cmd window have a buffer and I've been thinking to capture all of this data from such buffer. It is any way to get this info as text to store it in a file?
I'v been trying with something more professional shuch as to make a dump of the memory with ProcDump but since I have various myProgramInC.exe running at the same time (each one stored in a different location) I just get the message ""Multiple processes match the specified name."" and the rest of the options are just not useful for me since my app doesn't get unresponsive, it simply ends.
Any ideas?
","You can do this in C easily enough, using the ReadConsoleOutputCharacter function.
",prompt
how can i create a loop for a prompt when the answer is unacceptable for a criteria,"so I've been working on a password generator and I have it working aside from 2 things. The password must be within 8-128 characters long. I have a prompt that asks the user how long they would like the password to be and then a few other prompts about including symbols, uppercase, lowercase, and numbers. However, when the user answers with anything bellow 8 or above 128 I would like an alert to say ""password must be at least 8 characters long and shorter than 128"" then loop back to the prompt asking them again how long they would like the password to be.
I can not for the life of me figure out how to do this. the obvious solution to me was a while loop however I am not experienced in building those.
I also want a minimum of one character type selected before the ""confirm"" system ends and to loop back to the beginning of the confirms for upper, lower, symbols, and numbers so the user has to pick at least one.
any help would be greatly appreciated.
thank you!
here is the code for the password length
var passLength;

while (passLength >= 129 || passLength <= 7) {
  passLength = parseInt(prompt('How many charcters would you like? (8-128)'))
    if (passLength <= 128 || passLength >= 8) {
        break;
    }
  alert(""Password length must be 8 to 128 characters long!"");
}

here is the code for the separate characters that I need at least one of to be selected.
var chosenCharactersArr = []
var passwordstring = ''

if (confirm('would you like Capital letters?')) {
  chosenCharactersArr.push(upperCase)
}

if (confirm('would you like to include lowercase letters?')) {
  chosenCharactersArr.push(lowerCase)
}

if (confirm('would you like to include numbers?')) {
  chosenCharactersArr.push(numbers)
}

if (confirm('would you like to include symbols?')) {
  chosenCharactersArr.push(symbols)
}

","You have an error with your length check. You cannot use || in this case because you want to check for the lower- and upper limit to be both valid, therefore &&.
For the question cycle, you can use a while (true) loop that you can break once you are done with the questions i.e. config in your case.
I use an object to store the config values with defaults. You could also use an array. However, you have to manage the indices somehow.


const TYPE_NUMBER = 0;
const TYPE_BOOLEAN = 1;
const TYPE_STRING = 2;

const config = {
  len: 8,
  upc: false,
  loc: false,
  num: false,
  sym: false
};

while (true) {
  const len = ask('How many charcters would you like? (8-128)', TYPE_NUMBER);
  if (len <= 128 && len >= 8) {
    config.len = len; break;
  } else {
    alert(""Password must be 8 to 128 characters long!"");
  }
}

const upc = ask('Would you like to use capital letters?', TYPE_BOOLEAN);
if (upc) { config.upc = upc; }

// ... repeat the same for the other questions ...

console.log(config);

function ask(msg, type) {
  let result;
  if (type === TYPE_BOOLEAN) {
    result = confirm(msg);
  } else if (type === TYPE_NUMBER) {
    result = parseInt(prompt(msg));
  } else if (type === TYPE_STRING) {
    result = prompt(msg);
  } else {
    result = """";
  }
  return result;
}



",prompt
is there a way in nodejs to display multiple options kind of like a prompt but with preset options,"I want the user to be able to have a drop-down and use arrow keys to control their option with enter as the selecting. Kind of like this.
Pick A Color:
Red <
Green
Blue

I want the user to use arrow keys to navigate from red to blue, how do I do this?
","There are a few popular prompt libraries you could try:

https://github.com/enquirer/enquirer
https://github.com/terkelg/prompts
https://github.com/SBoudrias/Inquirer.js

You should read each one's README to see which is easiest for you to setup. I typically use Inquirer but all three are great. With Inquirer, it would look something like this:
npm install @inquirer/select

import select from '@inquirer/select';

const answer = await select({
  message: 'Pick a color:',
  choices: [
    {
      value: 'red',
    },
    {
      value: 'green',
    },
    {
      value: 'blue',
    },
  ],
});

You can read more about this specific select prompt here: https://github.com/SBoudrias/Inquirer.js/tree/master/packages/select
If you get an error like this:
import select from ""@inquirer/select"";
^^^^^^

SyntaxError: Cannot use import statement outside a module

That's because the example I gave uses ESM-style (ES module) imports. You can just add ""type"": ""module"", to your package.json to fix it.
",prompt
js dowhile loop infinitely loops over function when timeout is set,"i need a prompt window to let the user type in a number.
When setting the function (code below) once, everything works but this promt window should accure until a variable with an integer (starting from 200) reaches 0, so i thought a do while loop would do the trick. 
But when setting the do while loop the prompt window shows up immediately after typing in a number and does not change anything. 
I also changed the code until the loop got infinite and my brwoser crashed.
I know that the condition in the do while loop is always true (machineFuel > 0 && height > 0) because the variables machineFuel and height are greater than 0 but i dont know how to set it up correctly.
Here the relevant code (shortened)

var machineFuel = 200;
var height = 500;


setTimeout(round, 2000);

function round() {
    do {
       playerInput = input();
    } while (machineFuel > 0 && height > 0);
    calculate();
}

function input(fuel) {
    fuel = parseInt(prompt(""Fuel"", ""A number from 0-200""));
    return fuel;
}

function calculate() {
    machineFuel = machineFuel - playerInput;
}



","Move calculate after input() otherwise your machineFuel won't be updated and while will run forever.
Also the OP commented he needs the timeout after each prompt. So we remove while and call setTimeout() again:


var machineFuel = 200;
var height = 500;


setTimeout(round, 2000);

function round() {
   playerInput = input();
   calculate();
   machineFuel > 0 && height > 0 && setTimeout(round, 2000);
}

function input(fuel) {
    fuel = parseInt(prompt(""Fuel"", ""A number from 0-200""));
    return fuel;
}

function calculate() {
    machineFuel = machineFuel - playerInput;
}



The OP asked for a while option. For that we need to wrap setTimeout to a promise:


var machineFuel = 200;
var height = 500;

round();

async function round() {

  do {
    
   await delay(2000);
    
   playerInput = input();
   calculate();
  
  } while (machineFuel > 0 && height > 0);

}

function delay(delay) {
    return new Promise(function(resolve){
      setTimeout(resolve, delay);
    });
}

function input(fuel) {
    fuel = parseInt(prompt(""Fuel"", ""A number from 0-200""));
    return fuel;
}

function calculate() {
    machineFuel = machineFuel - playerInput;
}



",prompt
doubts in the use of int variables,"I'm trying to do something really stupid, but maybe it will be my careless mistake ...
In short, how is it possible that in a simple function like this:
function limitFunc(name){
  var domain=prompt(`Inserisci il dominio della funzione`);
  var sx=domain-0.2;
  var dx=domain+0.2;
  console.log(`sx`,sx, `dx:`,dx);
  //console.table(graph.limitCalculation(name, domain));
}

If I then write 1 with the prompt, the dx variable then calculated gives me a number multiplied by 10.
For example if I write 1:
chrome console
Thanks for the help anyway
","prompt returns a string.
All you need to do is convert your string into a number, basically:


function limitFunc(name) {
    let domain = prompt(`Inserisci il dominio della funzione`);
    domain = +domain; // converted string into number
    const sx = domain - 0.2;
    const dx = domain + 0.2;
    console.log(`sx:`, sx, `dx:`,dx);
}

limitFunc();



Obviously, you'd want to check if the user prompts a valid answer (i.e. a number).
",prompt
get current directory without full path in fish shell,"A buddy of mine finally got me to start using Fish Shell and I'm trying to set it up similar to how I had Bash. The PS1 in my .bash_profile listed the current directory I was in, followed by a >. It, however, wasn't the absolute path (e.g. /Users/me/Documents/... or ~/Documents/...). If I was in /Users/me/Documents/projects/Go/project1/, the prompt would simply say project1 >.
Is there a Fish Shell alternative to the \W substitution available for Bash? Again, I just want the folder I'm in, not the full path. I know you can use the echo (pwd) for all that.
I have looked into the basename program, and echo ""${PWD##*/}"", but these appear to only work in Bash.
","Taken from @Jubobs' answer:
basename is just a Unix utility; it's not associated to a particular shell, and should work equally well in Bash and Fish.
It appeared I was using basename in the wrong context, and without a suffix.
This was solved by using the following:
function fish_prompt
    echo (basename $PWD) ""><> ""
end

",prompt
suppress the passphrase prompt in gpg command,"Edited Version
I have a question about GPG, but I write all of the process, maybe it will help someone.
I want to: Suppress the passphrase prompt in GPG command. I don't want to: use -c option (--symmetric).
I have 2 systems Linux and Windows. I want to send the data from Linux to Windows. I want to encrypt the data in Linux and decrypt in Windows.

myFileOnLinux.txt is a file on Linux that I want to encrypt.
my@Email.com the UID of pair key.
myPasswordPhrase is the password phrase.

I installed GPG on both and did the steps:

Generate a pair key in Windows: 
gpg --gen-key

Change the key parameter in Windows: 
gpg --edit-key my@Email.com

trust
5
expire
0

Export the public keys:
gpg -a --export my@Email.com > public.key

Send the public key to the Linux machine.
Import the public key in Linux.
gpg --import public.key

Change the trust parameter in Linux
gpg --edit-key my@Email.com

trust
5

Encrypt a file in Linux
gpg --output output.enc --encrypt --recipient my@Email.com myFileOnLinux.txt

Send the encrypted file to Windows.
Decrypt the file.
gpg --batch --passphrase ""myPasswordPhrase"" -d -o test.dec output.enc


In Windows with a popup window it asked me the Passphrase again. How can I avoid it?
","After a lot of digging I found this command which disables the entry prompt on windows(works also for *nix systems):
--pinentry-mode=loopback

The full command would be:
gpg --pinentry-mode=loopback --passphrase  ""PASSWORD"" -d -o ""PATH\TO\OUTPUT"" ""PATH\TO\FILE.gpg""

",prompt
is there an ascii arrow that will fit seamlessly with the boxdrawing characters,"I'm getting creative with my prompt and on part of it I'm trying to do the following:
└→
Though it looks better than this, even in the console it has a small gap.

Does anyone know of an arrow character that would not do this?
","This seems to work with most fonts:

►
9658
U+25Ba



",prompt
using prompt in sql using sap bo webi 42 sp3,"I'm running a series of reports where time window called in query is rolling, and individual per report.. Some reports look 400 days back, others look 10 weeks back, while others again look at -40days/+80days... and so on - many options.
All reports are scheduled in daily or weekly runs, meaning setting prompts will require a manual reset of prompt for every instance through the scheduler. Not optimal.
I know the universe designer can design specific filters to drag into the queries using the query designer, but with so many different options, I find it a bit of an issue that the universe designer should create specific filters for these specific purposes, adding a vast number of specific filters intended for specific use to various universes.
I'm after an option where it is possible to assign a calculation to a date field, which stay fixed for the purpose of the report for every scheduled instance.
For instance, looking at invoice date from 400 days before today and onwards would look like Where DIM_TIME_INV.DAY_DAY >= sysdate -400 - This I can hardcode into the SQL of the specific report, and it will stay through the scheduler and roll 1 day for every day the report is run. But if I decide to make a change in the query elements, the SQL is screwed, and I will have to manually add the modification to the SQL again.
I found an article regarding the use of @Prompt and would ask universe designer to try and sandbox this in our version of BO.
While I'm being impatient, I try myself using following code based on example 4 from linked article.
SELECT
  @select('DIM_TIME_INV.DAY_DAY') >= sysdate -(@prompt('Invoiced, days before today:','N',[DIM_TIME_INV.DAY_DAY],mono,free))
FROM
  DIM_TIME_INV

Testing the SQL gives following error:
ORA-00936
SAP KBA 2054721
The whole idea is to have a flexible yet consistent dimension that will calculate every time the report is run, without losing the code whenever new items are added to the report.
Does anyone know of a way to use the @Prompt in SQL for SAP WEBI 4.2? - Or any other way to have 'flexible' time dimensions where it is possible to set a from-date or to-date independently or even a range, without having universe designer creating a s**t-load of filters and dump in various universes.
Thanks // C
","With regard to your example code, I think you're on the right track but your syntax has some issues:
SELECT
  @select('DIM_TIME_INV.DAY_DAY') >= sysdate -(@prompt('Invoiced, days before today:','N',[DIM_TIME_INV.DAY_DAY],mono,free))
FROM
  DIM_TIME_INV

First, both @Select and @Prompt must refer to universe objects, not columns.  The syntax for both is: class name\object name.  Assuming that the DIM_TIME_INV.DAY_DAY is associated with a universe object named Day Day in a class named Dim Time, the above code should be:
SELECT
  @select('Dim Time\Day Day') >= sysdate -(@prompt('Invoiced, days before today:','N','Dim Time\Day Day',mono,free))
FROM
  DIM_TIME_INV

Also note that the object reference in the @prompt call is delimited by single quotes, not brackets.
Next, I'm assuming that DAY_DAY is a date field.  Its reference in the @prompt call would cause the prompt to display a list of values, sourced from DAY_DAY.  But you want a numeric value from the prompt, not a date, so I would just leave that out, which will let the users enter a numeric value:
SELECT
  @select('Dim Time\Day Day') >= sysdate -(@prompt('Invoiced, days before today:','N',,mono,free))
FROM
  DIM_TIME_INV

Next, even with this corrected syntax, there will be an issue using this code as you have it.  A good way to debug @prompt issues is to view the SQL in the WebI report after you get the error -- the SQL will show the rendered result, with all functions (@select and @prompt) expanded.  For the above, you might get SQL like:
SELECT
  DIM_TIME_INV.DAY_DAY >= sysdate -(400)
FROM
  DIM_TIME_INV

This, of course, is invalid - you can't have a condition in the SELECT clause.  If this is truly intended to be a condition (which I think it is, based on your objective), then it should be a predefined condition rather than a dimension.
With that said, I think you're on the right track for what you want to do.  With the above corrections, you would have a predefined condition that you could drop into reports, which would enable the users to select the starting period (by number of days ago) for the report.  You could create additional prompts with different logic, ex:
@select('Dim Time\Day Day') >= sysdate -(@prompt('Invoiced, weeks before today:','N',,mono,free) * 7)

or
@select('Dim Time\Day Day') 
BETWEEN sysdate - @prompt('Starting days ago:','N',,mono,free)
    AND sysdate - @prompt('Ending days ago:','N',,mono,free)

",prompt
how do i run a js file from another js file,"I am building a game inside the console, I'm using an NPM package called ""prompts"", and it asks from the user for the game, ""What OS do you want to run?"" and returns as JSON. For example if you selected ""OS 1"" it would return { Bootup: ""StartOS1"" }. What I want to do is basically to have an if statement like this:

if(Bootup == ""StartOS1"") {
 // The code to run another JS file here.
}

But I don't know what to put.
I've tried to do this with a NPM package called shelljs, which executes stuff in the shell:

const shell = require('shelljs')

if(Bootup == ""StartOS1"") {
  shell.exec('node OS1.js')
}

What this did was make my shell stuck on a blank line forever, and I had to restart it. This shows an example of what I'm trying to do.
","shell.exec('node OS1.js') is a synchronous function that will block your program. For instance, if your js file is waiting on an event, it might block your functions indefinitely. Try child_process's exec() method. This should help:
const { exec } = require('child_process');

exec('node OS1.js', (e, stdout, stde) => {
  if (error) {
    console.error(`exec error: ${e}`);
    return;
  }
  console.log(`stdout: ${stdout}`);
  console.error(`stderr: ${stde}`);
});

There are also a couple of other methods you can use, but this one reads the js file as a string then executes it via the other synchronous eval function. Though I heard it's not the best practice when it comes to security.
const fs = require('fs');

fs.readFile('./node OS1.js', 'utf8', function(e, file) {
  if (e) {
    console.error(e);
    return;
  }
  eval(file);
});

",prompt
how to return to the prompt message if the user enters empty value and hits ok,"How can I get the prompt to detect the null value in javascript?
I tried this but entering ok will return a ""Hello"" instead of ""Enter again"".


function myprompt() {
  var a = prompt(""Enter your name"", ""abc"");
  if (a != null) {
    document.getElementById(""para"").innerHTML = ""Hello "" + a;
  } else if (a == """") {
    document.write(""Enter again"");
  } else {
    document.write(""BYE..."")
  }
}
myprompt()
<p id=""para""></p>



I'm making a JavaScript project and I want to alert when the user enters nothing in the prompt
What it's supposed to do:

input any name or string from the user
prompt for the name of the user
alert the message enter again when the user enters a null value
alert the message bye when the user cancels the prompt

But when I test it and enter nothing it says hello - why?
","From the docs: https://developer.mozilla.org/en-US/docs/Web/API/Window/prompt

When the user clicks the OK button, text entered in the input field is returned. If the user clicks OK without entering any text, an empty string is returned. If the user clicks the Cancel button, this function returns null.

function myPrompt() {
    var a = prompt(""Enter your name"", ""John Doe"");
    if (a == null) {
        document.write(""User have cancelled the prompt!"");
    } else if (a == """") {
        document.write(""User haven't entered anything and clicked OK"");
    } else {
        document.write(""User have entered a valid text"")
    }
}

",prompt
fish function prompt_pwd but in zsh,"I am currently using ZSH, but I have seen that the Fish Shell has a feature called prompt_pwd which is a function. It displays the prompt path similar as this:
// normally this would apper like this:
$ /my/computer/User/Local/Documents/Something

// prompt_pwd function on fish shell:
$ /m/c/U/L/Documents cd Something
$ /m/c/U/L/D/Something

According you navigates through your file system the full name of them changes to only the first letter of its name and the only full name will be displayed on the current directory...

Graphical example:


Do you know how I can achieve this in ZSH?
Thanks!
This is the full code extracted from the fish_config wizard:
    # Defined in /opt/homebrew/Cellar/fish/3.5.1/share/fish/functions/prompt_pwd.fish @ line 1 function prompt_pwd --description 'short CWD for the prompt'
    set -l options h/help d/dir-length= D/full-length-dirs=
    argparse -n prompt_pwd $options -- $argv
    or return

  if set -q _flag_help
        __fish_print_help prompt_pwd
        return 0
    end

  set -q argv[1]
    or set argv $PWD

  set -ql _flag_d
    and set -l fish_prompt_pwd_dir_length $_flag_d

  set -q fish_prompt_pwd_dir_length
    or set -l fish_prompt_pwd_dir_length 1

  set -l fulldirs 0
    set -ql _flag_D
    and set fish_prompt_pwd_full_dirs $_flag_D

  set -q fish_prompt_pwd_full_dirs
    or set -l fish_prompt_pwd_full_dirs 1

  for path in $argv
        # Replace $HOME with ""~""
    set -l realhome ~
        set -l tmp (string replace -r '^'""$realhome""'($|/)' '~$1' $path)

    if test ""$fish_prompt_pwd_dir_length"" -eq 0
            echo $tmp
        else
            # Shorten to at most $fish_prompt_pwd_dir_length characters per directory
      # with full-length-dirs components left at full length.
      set -l full
            if test $fish_prompt_pwd_full_dirs -gt 0
                set -l all (string split -m (math $fish_prompt_pwd_full_dirs - 1) -r / $tmp)
                set tmp $all[1]
                set full $all[2..]
            else if test $fish_prompt_pwd_full_dirs -eq 0
                # 0 means not even the last component is kept
        string replace -ar '(\.?[^/]{'""$fish_prompt_pwd_dir_length""'})[^/]*' '$1' $tmp
                continue
            end

      string join / (string replace -ar '(\.?[^/]{'""$fish_prompt_pwd_dir_length""'})[^/]*/' '$1/' $tmp) $full
        end
    end end

","This should match your initial text example; add it to ~/.zshrc:
setopt prompt_subst
PROMPT='\$ /$(printf ""%.1s/"" ${(s./.)PWD:h})${PWD:t} '


(s./.) - splits a path at /.
printf ""%.1s/"" - prints the first character of each directory piece.
${PWD:h} - the 'head' of the current directory, i.e. everything but the last element.
${PWD:t} - the tail / last element in the directory path.

Adding colors and other characters is a bit more involved.  When building a  complicated prompt, it's often easier to use a precmd; some of the many posts that discuss prompt colors and precmds are here, here, and here.

Edited to add:
As @rowboat noted, the code above will include an extra / in top-level directories like /tmp. This is due to printf always including at least one result even when there are zero parent directories. One fix is a substitution to convert double // to /:
PROMPT='\$ ${${:-$(printf ""/%.1s"" ${(s./.)PWD:h})/${PWD:t}}/\/\///} '

Similarly, another substitution can be added to escape % as %%, if you need to handle directory names that start with a percent sign:
PROMPT='\$ ${${${:-$(printf ""/%.1s"" ${(s./.)PWD:h})/${PWD:t}}/\/\///}//\%/%%} '

",prompt
how to display only files that have alternate data streams in command prompt,"I know that to display alternate data streams of all the files (if any) in command prompt, this is the command dir /R. But, how do I do it if I only want to display files that have alternate data streams?
","dir   /s /r | findstr /e "":$DATA""

or
dir   /r | findstr /e "":$DATA""

the first will search in all sub-directories.The second only in current folder. These will show also the ADS assigned to the directory.For only files:
dir /a-d  /s /r | findstr /e "":$DATA""

",prompt
does quot0330mquot code zero reset the font color to quottput setafquot or to xterm default color,"I know that you can use escape sequence %{\033[0m%} (code zero) to change the color of the prompt back to ""default"" color (usually, after the prompt).
But what is the ""default"" that is used for this?

Default color of the terminal program itself?
Default color set via tput setaf $COLOR, assuming you use it to set font color different from terminal's default color?
Else?

","It's terminal-dependent, but with regard to existing practice, it would have to be the former:

the default color of the terminal (program).

The other suggested choice would not be correct, since tput setaf does not set a ""default color"", but rather a ""current color"".  Whether any choice of tput setaf corresponds to the terminal's default color is (at best) terminal-specific, since there is no relevant standard one way or the other.
ECMA 48 gives terse (one line) descriptions of SGR 0 (which is what's mentioned in the question), and SGR 30-37, 40-47 (the setaf/setab choices commonly used for terminal descriptions that tput would use).
There's another possible choice, i.e., tput op (selecting the original color pair), which is used in several terminal descriptions to send SGR 39 and 49.  Again, this is terminal-specific and its relationship to SGR 0 is not covered by any standard.  Those happen to use the same ""default color"" for xterm and Linux console, but (read ECMA-48) the standard does not give that level of detail regarding ""default color"" and the absence of ""video attributes"".
Further reading:

ECMA-48: Control Functions for Coded Character Sets
Who did it? (a brief history of the Linux console color model)

",prompt
is there a way to force the dom to update in between repeated calls to prompt in javascript,"I am attempting to get a program to go into a cycle between prompts and updates on a page. However, if I isolate this problem down, in this case to two append calls and two prompts, the behavior I observe is that the page is updated visually only after the second call to prompt. The reason why I want to do this is esoteric, but related to making a program that sort of ""feels"" like a terminal where a user is typing in input once in a while. Here's an example, adapted from w3c schools, that demonstrates the problem:
<!DOCTYPE html>
<html>
<body>

<h1>The Window Object</h1>
<h2>The prompt() Method</h2>

<p>Click the button to demonstrate the prompt box.</p>

<button onclick=""myFunction()"">Try it</button>

<p id=""demo""></p>

<script>
function myFunction() {
   
  let person = prompt(""Please enter your name"", ""Prompt1"");
  if (person != null) {
    document.getElementById(""demo"").insertAdjacentHTML('beforeend', ""Test1\n"");
 }
  
  person = prompt(""Please enter your name"", ""Prompt2"")
  if (person != null) {
    document.getElementById(""demo"").insertAdjacentHTML('beforeend', ""Test2\n"");
  }
}

</script>

</body>
</html>

What I would ""like"" to happen is that in the first insertAdjacentHTML call that it updates the DOM, and visually on the page, before the second prompt, so you can see what you typed in. In practice, I observe that ""Test1"" and ""Test2"" visually appear at the same time after the second prompt. Console logging, as opposed to the insert adjacent call, works how I want, but isn't relevant to the application I'm writing.
I have experimented with various techniques to force the issue (e.g., trying to force trigger a layout with a scroll call, playing with requestAnimationFrame), but have not yet found a technique that seems to do what I want. My hypothesis, and I am not sure it is correct, is that the second prompt is called before the next frame, which means it blocks any new rendering, but that's just a hypothesis. Even if correct, suggestions on how to get the behavior I want would be appreciated.
Generally, I was hoping to observe Test1 placed on the page, then the second prompt call is fired, then after Test2, assuming the user chose the defaults.
","Using a short timeout seems to work...


function myFunction() {
  const d = document.getElementById(""demo"");
   
  let person = prompt(""Please enter your name"", ""Prompt1"");
  if (person != null) {
    d.insertAdjacentHTML('beforeend', ""Test1\n"");
  }
 
  setTimeout(() => {
    person = prompt(""Please enter your name"", ""Prompt2"")
    if (person != null) {
      d.insertAdjacentHTML('beforeend', ""Test2\n"");
    }
  }, 50);
}
<h4>Example with setTimeout</h4>
<button onclick=""myFunction()"">Try it</button>
<p id=""demo""></p>



BUT it will get ugly fast if you need a lot of prompts.  If so, maybe something that wraps the prompt/write text/delay part in a promise and chain them together with then:


function myFunction() {
  const d = document.getElementById(""demo"");
   
  delayPrompt(""Please enter your name"", ""Prompt1"", d)
    .then(() => delayPrompt(""Please enter your name"", ""Prompt2"", d))
    .then(() => delayPrompt(""something else"", ""Prompt3"", d));
}

async function delayPrompt(text, def, container) {
  return new Promise((resolve) => {     
    let response = prompt(text, def);
    if (response != null) {
      container.insertAdjacentHTML('beforeend', response);
    }
    setTimeout(resolve, 50);
  });
}
<h4>Example with Promise</h4>
<button onclick=""myFunction()"">Try it</button>
<p id=""demo""></p>



",prompt
can39t lauch spyder on new envs,"When i lauch spyder on another env than ""base"" i get this error :
fromIccProfile: failed minimal tag size sanity
C:\Users\esto5\anaconda3\envs\PIP\lib\site-packages\paramiko\transport.py:219: CryptographyDeprecationWarning: Blowfish has been deprecated
  ""class"": algorithms.Blowfish,
Traceback (most recent call last):
  File ""C:\Users\esto5\anaconda3\envs\PIP\Scripts\spyder-script.py"", line 10, in <module>
    sys.exit(main())
  File ""C:\Users\esto5\anaconda3\envs\PIP\lib\site-packages\spyder\app\start.py"", line 252, in main
    mainwindow.main(options, args)
  File ""C:\Users\esto5\anaconda3\envs\PIP\lib\site-packages\spyder\app\mainwindow.py"", line 1822, in main
    mainwindow = create_window(MainWindow, app, splash, options, args)
  File ""C:\Users\esto5\anaconda3\envs\PIP\lib\site-packages\spyder\app\utils.py"", line 289, in create_window
    main.setup()
  File ""C:\Users\esto5\anaconda3\envs\PIP\lib\site-packages\spyder\app\mainwindow.py"", line 773, in setup
    PLUGIN_REGISTRY.register_plugin(self, PluginClass,
  File ""C:\Users\esto5\anaconda3\envs\PIP\lib\site-packages\spyder\api\plugin_registration\registry.py"", line 342, in register_plugin
    instance = self._instantiate_spyder5_plugin(
  File ""C:\Users\esto5\anaconda3\envs\PIP\lib\site-packages\spyder\api\plugin_registration\registry.py"", line 184, in _instantiate_spyder5_plugin
    plugin_instance = PluginClass(main_window, configuration=CONF)
  File ""C:\Users\esto5\anaconda3\envs\PIP\lib\site-packages\spyder\api\plugins\new_api.py"", line 331, in __init__
    container.setup()
  File ""C:\Users\esto5\anaconda3\envs\PIP\lib\site-packages\spyder\plugins\pythonpath\container.py"", line 55, in setup
    self._load_pythonpath()
  File ""C:\Users\esto5\anaconda3\envs\PIP\lib\site-packages\spyder\plugins\pythonpath\container.py"", line 136, in _load_pythonpath
    system_path = get_system_pythonpath()
  File ""C:\Users\esto5\anaconda3\envs\PIP\lib\site-packages\spyder\plugins\pythonpath\utils.py"", line 31, in get_system_pythonpath
    env = get_user_env()
  File ""C:\Users\esto5\anaconda3\envs\PIP\lib\site-packages\spyder\utils\environ.py"", line 71, in get_user_env
    env_dict = get_user_environment_variables()
  File ""C:\Users\esto5\anaconda3\envs\PIP\lib\site-packages\spyder\utils\environ.py"", line 60, in get_user_environment_variables
    res = stdout.decode().strip().split(os.linesep)
UnicodeDecodeError: 'utf-8' codec can't decode byte 0x90 in position 247: invalid start byte

I find nothing on internet about this specific problem. I tried too much
update remove install
installation with conda create -n spyder-env -c conda-forge spyder lauch it but don't work more
","If someone get this problem, in the file :
C:\Users\User_name\anaconda3\envs\PIP\lib\site-packages\spyder\utils\environ.py

I replaced the line :
res = stdout.decode().strip().split(os.linesep)

By :
res =str(stdout).strip().split(os.linesep)

",prompt
python  bash prompt how to print width calculation wrappers,"I wrote a Python script to replace ""powerline"" as a terminal prompt solution for myself here: https://github.com/diogobaeder/dotfiles/blob/master/.bash_prompt.py
Then all I do is to define the prompt from the output of that script:
# in my ~/.bashrc
export PS1=""\$(python ~/.bash_prompt.py)""

The script itself works fine, I get the command prompt I want; However, since there's no wrapping for the styles I put there, the terminal (doesn't matter which GUI terminal program I use) doesn't calculate the prompt width correctly, and as I type characters after the prompt it ends up not wrapping them to a new line at first, overwriting the prompt completely.
Now, I know that when stylizing my bash prompt I need to escape style codes with \[ and \] so that bash takes into consideration that they're escape sequences and calculates the width correctly. However, if I put them as wrappers for my styles in my Python script (see esc_start and esc_end), I can't get them to be properly evaluated by bash as ""calculation escape sequences"", instead I get literal square brackets printed. If I then escape in Python the backslashes too (\\[ and \\]), then I get unescaped literals outputted (\[ and \]). Bash seems to completely ignore them as escape sequences for calculating the prompt width.
If, however, I remove the backslash in my PS1 command ($(python ~/.bash_prompt.py) instead of \$(python ~/.bash_prompt.py)), then putting \[ and \] as escape sequences in my Python script (as esc_start and esc_end), then bash considers them as proper escapes and ends up wrapping lines as expected (i.e., when I go past the right border it wraps the line as expected). The problem with this, however, is that removing this backslash from my PS1 definition in .bashrc makes the script run only once per terminal session, and not for each prompt line - so, for example, if I'm in a Git working tree and I change from one branch to another, it doesn't show the new branch as soon as the command finishes, instead it shows the old branch.
Let me give some examples of what I mean, that you can try in your own .bashrc without needing my Python script:
PS1=""\[\033[31m\]\u@\h\[\033[0m\]$ "" # This wraps lines correctly
PS1=""\033[31m\u@\h\033[0m$ "" # This makes the line overwrite the prompt

So, any ideas of how to cope with bash and make it understand the \[ and \] sequences correctly when printed by the Python script, while still keeping the script running for each command prompt? Or, if this is a limitation in bash, is there another way to force it to wrap the line when it reaches the right border of the terminal window?
Thanks!
Diogo
","Bash first interprets the escape sequences in $PS1 and only afterwards handles command substitution, etc.

Bash allows these prompt strings to be customized by inserting a number of backslash-escaped special characters that are decoded as follows [...]
  After the string is decoded, it  is  expanded  via  parameter expansion, command substitution, arithmetic expansion, and quote removal [...]

 --Bash Reference Manual: Controlling the Prompt
This means that any special sequences printed by your command will not be interpreted as colors, etc.  The solution is to use $PROMPT_COMMAND to change the value of $PS1, like:
PROMPT_COMMAND='PS1=$(python ~/.bash_prompt.py)'

",prompt
using input function in scilab prompt user multiple times,"so I'm learning the basics of Scilab and i was trying to figure out how the Input function works but i keep getting a bizarre problem.
Here's my code:
    A = input(""Enter value of A: "")
    B = input(""Enter value of B: "")
    disp(""The value of A is: "" +string(A))
    disp(""The value of B is: "" +string(B))

And This's what i get:
   Enter value of A: 
   6
   Enter value of A: 
   --> 5
   Enter value of A: 
   5



  Enter value of B: 
  8


   The value of A is: 5

   The value of B is: 8

I have no idea why i was prompted 3 times to enter the value of A.
A lil bit of help understanding this please, thank you.
","It was actually a bug brought by Scilab 6.
It is fixed for years now (@https://gitlab.com/scilab/scilab/-/issues/14424 )
",prompt
a parameter cannot be found that matches parameter name 39all39 c,"I am trying to execute a command through c#. I pass a command as a parameter through a function which then executes it on my terminal in VS Code. When I pass the command 'ps' it works, but when I pass the command 'ls - all' it gives me the error: A parameter cannot be found that matches parameter name 'all'. I think it probably has to do something with the space, but I'm not sure. I don't know how to solve it. This is the function I use to pass the command and execute it:
public void ExecuteCommand(string key) {
            System.Diagnostics.Process process = new System.Diagnostics.Process();
            try {
                System.Diagnostics.ProcessStartInfo startInfo = new System.Diagnostics.ProcessStartInfo();
                startInfo.WindowStyle = System.Diagnostics.ProcessWindowStyle.Hidden;
                startInfo.FileName = @""powershell.exe"";
                startInfo.Arguments = ""/c "" + key;
                process.StartInfo = startInfo;
                process.Start();
                process.Close();

            }
            catch {
                Console.WriteLine(""Error occured"");
            }
            Console.WriteLine(""Command executed"");
      }


","You have to use the command ls -Force to list all the files including the hidden files.ls -al works in linux.
",prompt
how can i ask another questions in my code after choosing one of the options,"Hello I have a problem with this exercise:
Write a program that calculates the area of a figure?
What figure? 1 - rectangle, 2 - triangle
if user will choose rectangle the program should ask questions:
-Enter base length:
-Enter the height of the rectangle:
statement: This field is:
if user will choose second options what is triangle a program will ask questions as:
-Enter the side length
-Enter the length of side
statement: The area of the triangle is:
    var bok1 = Number(prompt(""Ile wynosi pierwszy bok prostokata?""));
    var bok2 = Number(prompt(""Ile wynosi drugi bok prostokata?""));
    var podstawa trojkata = Number(prompt(""Podaj długość podstawy:""));
    var wysokosc trojkata = Number(prompt(""Podaj wysokosc podstawy:""));
    var co_obliczyc = (prompt(""Co chcesz obliczyc? prostokat czy trojkat?""));
    switch(co_obliczyc) {

so I wrote sth like that and stuck I don't want my program to ask every questions at first and then calculate I want program to ask about the figure then if user will chose one of this he has to give dimensions for the selected figure I have no idea how to do this please help
","Just swap the order of questions:
var co_obliczyc = (prompt(""Co chcesz obliczyc? prostokat czy trojkat?""));
switch(co_obliczyc) {
   case ""prostokat"":
      var bok1 = Number(prompt(""Ile wynosi pierwszy bok prostokata?""));
      var bok2 = Number(prompt(""Ile wynosi drugi bok prostokata?""));
      //logic for rectangle
      break;
   case ""trojkat"":
      var podstawa_trojkata = Number(prompt(""Podaj długość podstawy:""));
      var wysokosc_trojkata = Number(prompt(""Podaj wysokosc podstawy:""));
      //logic for triangle
      break;
   default:
      alert(""Not implemented"")//Sorry, I don't know how to write it in Polish
      break;
}

",prompt
what do i need to write in my macro to sort through sudden prompts in sap gui,"So I have a macro that updates prices in SAP GUI. It copies cell by cell and does the update order by order, but some times, right after performing the update and saving the order to go with the next line, a prompt appears in SAP where I need to click Yes or No, or Accept / Reject.
My macro then stops as it does not have a line of code to act on it. I do know which line is required, but my question is how to I write a line of code which would be executed if the prompt appears?
Keep in mind that sometimes it pops up and sometimes it does not.
Here is my code:
Public sessioninfo As SAPFEWSELib.GuiSessionInfo


Public Sub fastPFI()
Dim ws As Worksheet
Dim App As SAPFEWSELib.GuiApplication
Dim sor As Long
Dim maxsor As String

'HOEEUBV2 (EUB with scripting)
  Set GuiAuto = GetObject(""SAPGUI"")  'Get the SAP GUI Scripting object
  Set App = GuiAuto.GetScriptingEngine 'Get the currently running SAP GUI
  Set Con = App.Children(0) 'Get the first system that is currently connected
  Set session = Con.Children(0) 'Get the first session (window) on that connection
    Set sessioninfo = session.Info

Set ws = Excel.ThisWorkbook.Worksheets(""system"")
sor = 2
maxsor = ws.Cells(Rows.Count, 1).End(xlUp).Row
'maxsor = 3
Do While sor < maxsor + 1

session.StartTransaction ""va02""
'session.FindById(""wnd[0]"").SendVKey 0
session.FindById(""wnd[0]/usr/ctxtVBAK-VBELN"").Text = Cells(sor, 1)
session.FindById(""wnd[0]"").SendVKey 0
session.FindById(""wnd[1]"").SendVKey 0
session.FindById(""wnd[0]"").SendVKey 30
session.FindById(""wnd[0]"").SendVKey 11
session.FindById(""wnd[0]/usr/lblRV45S-BSTNK"").SetFocus
session.FindById(""wnd[0]/usr/lblRV45S-BSTNK"").CaretPosition = 18
'session.FindById(""wnd[0]"").SendVKey 0

sor = sor + 1


Loop

MsgBox ""All proformas have been created"" & vbNewLine & ""Click OK to close file""

' Application.DisplayAlerts = False
'ActiveWorkbook.Close Savechanges:=False
'Application.DisplayAlerts = True

End Sub```

","On Error GoTo Next after the line of code which could or not happen solved it.
",prompt
set csh prompt to raw output of a command with multiple lines and special characters,"I am trying to set my csh prompt to the raw output of a particular command. I know how to do that for simple outputs; for example:
set prompt=`echo propmt%|sed ""s/pm/mp/""`

... results in a prompt of:
prompt%

But the prompt I want is more complex. First, it has multiple lines. I know how to do that directly (i.e. not via the output of a command):
set prompt=""This is a line\nThis is another\nAnd this is the last% ""

... results in:
This is a line
This is another
And this is the last%

But if I try to do that same thing through a command:
set prompt=`echo ""This is a line\
This is another\
And this is the last% ""`

... the prompt becomes just:
This

... not even This is a line; just This.
On top of that, the specific command that I want to use will output not just multiple lines, but also a bunch of other characters that I assume csh and/or set will interpret in ways that I don't intend. Specifically:
cowsay -f ghostbusters ""BOOOOOOOOOOOOOOOOOOOOOOOOO \
? \
? You are using CEEEEEE SHELLLLLLLLLLLLLLLLL\
? \
? Be my guest, but you have no one to blame but\
? YOURSELLLLLLLLLLLLLLLLLLLLLLF""; echo; echo ""% ""

... which outputs:
 _________________________________________
/ BOOOOOOOOOOOOOOOOOOOOOOOOO              \
|                                         |
| You are using CEEEEEE                   |
| SHELLLLLLLLLLLLLLLLL                    |
|                                         |
| Be my guest, but you have no one to     |
\ blame but YOURSELLLLLLLLLLLLLLLLLLLLLLF /
 -----------------------------------------
          \
           \
            \          __---__
                    _-       /--______
               __--( /     \ )XXXXXXXXXXX\v.
             .-XXX(   O   O  )XXXXXXXXXXXXXXX-
            /XXX(       U     )        XXXXXXX\
          /XXXXX(              )--_  XXXXXXXXXXX\
         /XXXXX/ (      O     )   XXXXXX   \XXXXX\
         XXXXX/   /            XXXXXX   \__ \XXXXX
         XXXXXX__/          XXXXXX         \__---->
 ---___  XXX__/          XXXXXX      \__         /
   \-  --__/   ___/\  XXXXXX            /  ___--/=
    \-\    ___/    XXXXXX              '--- XXXXXX
       \-\/XXX\ XXXXXX                      /XXXXX
         \XXXXXXXXX   \                    /XXXXX/
          \XXXXXX      >                 _/XXXXX/
            \XXXXX--__/              __-- XXXX/
             -XXXXXXXX---------------  XXXXXX-
                \XXXXXXXXXXXXXXXXXXXXXXXXXX/
                  """"VXXXXXXXXXXXXXXXXXXV""""
%

That's what I want as my csh prompt. At this point, I feel obligated to note that I am not kidding.
I suppose I could forget about running a command to generate it, and instead just manually converting it to a single-line string that's properly escaped for set prompt, but:

That seems like a pain;
I'd like to be able to quickly and easily change the prompt to similar but different things.

So, I'm hoping for some way to tell set prompt ""Just use the raw output of (arbitrary-command); don't interpret any of it as special characters or whatever"". Is there a way? Or any alternate suggestions to accomplish what I want? Thanks.
","Maybe it's better to use the precmd alias for this:
alias precmd 'cowsay -f ghostbusters << EOF \
line 1 \
\
line 2 \
\
line 3 \
EOF'

From csh(1):
 precmd  Runs just before each prompt is printed.  For example, if one
         does

               > alias precmd date

         then date(1) runs just before the shell prompts for each command.

         There are no limits on what precmd can be set to do, but
         discretion should be used.

Or you can make it a different file to avoid all the backslashes:
alias precmd 'source ~/cowsay'

# ~/cowsay
cowsay -f ghostbusters << EOF
line 1

line 2

line 3
EOF

",prompt
what is the difference between ps1 and prompt_command,"While taking a look at this awesome thread I noticed that some examples use
PS1=""Blah Blah Blah""

and some use
PROMPT_COMMAND=""Blah Blah Blah""

(and some use both) when setting the prompt in a Bash shell. What is the difference between the two? A Stack Overflow search and even a bit of broader Google searching aren't getting me results, so even a link to the right place to look for the answer would be appreciated.
","From the GNU Bash documentation page (Bash Reference Manual):
PROMPT_COMMAND
    If set, the value is interpreted as a command to execute before
    the printing of each primary prompt ($PS1).

I never used it, but I could have used this back when I only had sh.
",prompt
add newline if eol not detected in bash prompt,"Is there any way to preserve partial line in bash like zsh? My bash prompt messes up without newline and for printing other non-printable escape characters.
Thanks in advance!
I've tried PS1=""\[\e[0m\]\n$PS1"". But I think that's not a perfect solution. I just want to set my bash prompt to add newline if no EOL detected in previous output.
","##~/.local/bin/add_newline
#!/usr/bin/bash

printf ""\E[6n"";read -sdR CURPOS; CURPOS=${CURPOS#*[}; C=""${CURPOS#*;}""

[ ""$C"" -ne 1 ] && echo

$ chmod +x ~/.local/bin/*
##~/.bashrc
PROMPT_COMMAND=""${PROMPT_COMMAND}${PROMPT_COMMAND:+;} add_newline""
Thanks @CharlesDuffy
",prompt
please help writing a javascript quiz using prompt and multiple ifthen statements,"So I only really know your basic HTML and CSS.  I can manipulate stuff that is there and figure it out, but I'm not a developer and am very limited on what I can write from scratch.
I'm trying to create an interactive quiz using javascript prompt functions and if statements.  I've got the code to run but there are a few things I need to do to finish that i can't figure out.  I have searched and had no luck receiving [uncomplicated] solutions to my issue.

If the user answers incorrect, I want them it to return them to the question to try again until they get it right.
What's the easiest way to make multiple correct answers (i.e. answer = this, that, or these... not just ""this"" or fail)
My code is not executing after Question 3 for some reason, can't figure out why.

PS. I know it might not be good practice to have just one huge script like this, but if possible help me with what I have.  Thanks in advance for any help, here is my function:
https://gist.github.com/anonymous/dcde24cf97df24fe5ca4
","Your question is very broad as there are always a million ways to design something, but I'll give you the five-minute answer.

Use a data structure to hold your question / answer combinations, like an array of objects. An array makes sense if you want to keep the user along a ""path"". You simply iterate through the array to ask questions.
var questions = [{ 
        question: 'Are you happy?',
        answer: 'yes',
        affirm: 'Yay! You got it right!',
        rebuttal: 'Nope, you are definitely happy.'
    },
    {
        question: 'Are you mad?',
        answer: 'no',
        affirm: 'Good job!',
        rebuttal: 'Not right.'
    }];

for (var i = 0, l = questions.length; i < l; i++) {
    answer = prompt(questions[i].question);

    // I do not support manipulating a loop counter mid-loop, but it's for an example.
    if (answer !== questions[i].answer) {
        alert(questions[i].rebuttal);
        i--;
    } else {
        alert(questions[i].affirm);
    }
}

With your method, which I have to be honest, is pretty primitive, you'd use have to write the choices in the text of the prompt. For multiple answers, you'd have to force the user to delimit their answers somehow (with commas, for example). But honestly, I would abandon the prompt/alert method altogether, and use HTML  elements to achieve your goals (assuming this is in a web browser).
It seems other users have addressed your syntax errors.

If many of these concepts are foreign to you, I recommend studying up at Codecademy, as they have excellent courses on Javascript and HTML.
",prompt
break out of while loop that has an array inside,"Want to let the user make an array until they press cancel on the prompt
But the nested if statement won't trigger and break the while loop.
The goal of the program is to show the first ten items in the array with the message
'Only the first ten guests will be recorded'
This is part of my course which is why I am letting the user input as much as they like
and then reducing it to 10 after.
let guests = []
i = 1
j = 1

while (j = 1) {
    input = guests.push(prompt('Enter Guest' + i + '(press cancel to stop)'))
    i++

    if(input === null) {
        j++
        //break;
    }
}

if (guests.length > 10) {
    console.log('Only the first ten guests will be recorded')

    while (guests.length > 10) {
        guests.pop()
    }
}

console.log(guests)

If I remove the if() brackets the program stops after the user inputs one array item
And if I try break;
it stops after the user types in 1 array item.
I have also tried adding
    else {
        continue;
    }

but to no avail
","Store the result of prompt to test for null (instead of comparing the return value of Array#push).
while (true) {
    input = prompt('Enter Guest' + i + '(press cancel to stop)');
    if(input === null) break;
    guests.push(input);
    i++;
}

",prompt
is there a way to make a loop in prompt,"I am experimenting with the npm package, prompt, and would like to find out something.
Basically, I have this code:
prompt.start();
      
        prompt.get(['Age'], function (err, result) {
          if (err) return;

          console.log(result);

        })

The code is supposed to ask a question (Age) in the cmd prompt, and the user can answer it through the cmd prompt. It works fine, however once the user enters in their age, the prompt logs the result, and exits the process. I would like for the prompt to ask the question again every time AFTER the question is answered, instead of exiting the process. So, the console would look like this,
prompt: Age:  70                                                                                                                                     
prompt: 70
prompt: Age:

Is there any way to do this?
","You can use the call signature of prompt.get which returns a promise and then await it inside a classic while loop. From the documentation:

If no callback is passed to prompt.get(schema), then it returns a Promise, so you can also write:
const {username, email} = await prompt.get(['username', 'email']);


Here's an example module:
example.mjs:
import prompt from 'prompt';

prompt.start();

while (true) {
  try {
    const result = await prompt.get(['Age']);
    // Break out of the loop if ""STOP!"" is typed
    if (result.Age === 'STOP!') break;
    console.log(result);
  }
  catch (ex) {
    // handle exception
  }
}

console.log('Done');


Running the program in the terminal:
% node --version
v18.12.1

% node example.mjs
prompt: Age:  10
{ Age: '10' }
prompt: Age:  20
{ Age: '20' }
prompt: Age:  30
{ Age: '30' }
prompt: Age:  etc
{ Age: 'etc' }
prompt: Age:  STOP!
Done


",prompt
how to put this in a loop,"I need to put this code in a loop so that you can choose whichever number first and go back to the start after whichever one you choose, but everything I've tried hasn't worked and need help.
peoples = {
""Mary"": {
    ""name"": ""Mary"",
    ""budget"": 100,
    ""items"": {
        ""Game"": 0,
        ""Book"": 0,
        ""Kindle"": 0
    },
    ""status"": ""incomplete""
},
""Steve"": {
    ""name"": ""Steve"",
    ""budget"": 100,
    ""items"": {
        ""Tie"": 0,
        ""Scarf"": 0,
        ""Amazon Echo"": 0
    },
    ""status"": ""incomplete""
},
""Kevin"": {
    ""name"": ""Kevin"",
    ""budget"": 65,
    ""items"": {
        ""Mario Kart"": 0
    },
    ""status"": ""incomplete""
},
""Jane"": {
    ""name"": ""Jane"",
    ""budget"": 50,
    ""items"": {
        ""Gift Card"": 0,
        ""Gloves"": 0
    },
    ""status"": ""incomplete""
},
""Chris"": {
    ""name"": ""Chris"",
    ""budget"": 100,
    ""items"": {
        ""Chocolates"": 0,
        ""Galaxy Tab"": 0
    },
    ""status"": ""incomplete""
}
}

print(""""""
Menu
--------------------
1. Update Shopping List
2. Complete Shopping List
3. Display Shopping List
4. Exit Application
--------------------
Make your selection

"""""")

option = int(input(""Enter an option: ""))

if option == 1:
    people = input(""Who are you updating?: "")
    print(""\nCurrent values of people"",people)
    print(peoples[people])
    print(""\nAvailable items and their prices are:"")
    for item in peoples[people][""items""]:
        print(item, peoples[people][""items""][item])
    item_to_update = input(""Enter an item to update: "")
    price = int(input(""Enter updated price: ""))
    budget = peoples[people][""budget""] - peoples[people][""items""] 
[item_to_update] - price
peoples[people][""items""][item_to_update] = price
peoples[people][""budget""] = budget
print(""\nUpdated values of people"",people)
print(peoples[people])

option = int(input(""\nEnter an option: ""))

if option == 2:
    update = input(""Choose one of the 5 people to complete their shopping list: "")
if update in peoples:
        print(""You have chosen"",update)
answer = input(""Do you want to complete their shopping list (Y/N)? "")
if answer.upper() == ""Y"":
    peoples[people]['status'] = 'complete'
print(""Shopping list has been completed!"")

option = int(input(""\nEnter an option: ""))

if option == 3:
    display = input(""Who's do you want to look at?: "")
print(""\nShopping List Of"",display)
print(peoples[display])

option = int(input(""\nEnter an option: ""))

if option == 4:
    print(""Thank You For Shopping With Us!"")

I've tried putting in different versions of loop, but it always either results in the program ignoring it and not going back to the start, or breaking when I choose something else then 1 at the start.
option = input(""Enter an option: "")
if option == ""1"":
        people = input(""\nWho are you updating?: "")
print(""\nCurrent values of people"",people)
print(peoples[people])
print(""\nAvailable items and their prices are:"")
for item in peoples[people][""items""]:
    print(item, peoples[people][""items""][item])
item_to_update = input(""Enter an item to update: "")
price = int(input(""Enter updated price: ""))
budget = peoples[people][""budget""] - peoples[people][""items""][item_to_update] - price
peoples[people][""items""][item_to_update] = price
peoples[people][""budget""] = budget
print(""\nUpdated values of people"",people)
print(peoples[people])

elif option == ""2"":
        update = input(""Choose one of the 5 people to complete their shopping list: "")
        if update in peoples:
print(""You have chosen"",update)
peoples[people]['status'] = 'complete'
print(""Shopping list has been completed!"")

elif option == ""3"":
    display = input(""Who's do you want to look at?: "")
print(""\nShopping List Of"",display)
print(peoples[display])

elif option == ""4"":
    print(""Thank You For Shopping With Us!"")
    break
else:
    print(""That's not a valid answer! Try again!"")

With the same list above, After adding in my information to the set example given, I would get the error below.
error with pic: https://i.sstatic.net/BrqBB.png
peoples = {
    ""Mary"": {
        ""name"": ""Mary"",
        ""budget"": 100,
        ""items"": {
            ""Game"": 0,
            ""Book"": 0,
            ""Kindle"": 0
        },
        ""status"": ""incomplete""
    },
    ""Steve"": {
        ""name"": ""Steve"",
        ""budget"": 100,
        ""items"": {
            ""Tie"": 0,
            ""Scarf"": 0,
            ""Amazon Echo"": 0
        },
        ""status"": ""incomplete""
    },
    ""Kevin"": {
        ""name"": ""Kevin"",
        ""budget"": 65,
        ""items"": {
            ""Mario Kart"": 0
        },
        ""status"": ""incomplete""
    },
    ""Jane"": {
        ""name"": ""Jane"",
        ""budget"": 50,
        ""items"": {
            ""Gift Card"": 0,
            ""Gloves"": 0
        },
        ""status"": ""incomplete""
    },
    ""Chris"": {
        ""name"": ""Chris"",
        ""budget"": 100,
        ""items"": {
            ""Chocolates"": 0,
            ""Galaxy Tab"": 0
        },
        ""status"": ""incomplete""
    }
}

print(""""""
Menu
--------------------
1. Update Shopping List
2. Complete Shopping List
3. Display Shopping List
4. Exit Application
--------------------
Make your selection

"""""")

while True:
    option = input(""Enter an option: "")
    if option == ""1"":
        people = input(""\nWho are you updating?: "")
        print(""\nCurrent values of people"",people)
        print(peoples[people])
        print(""\nAvailable items and their prices are:"")
    for item in peoples[people][""items""]:
        print(item, peoples[people][""items""][item])
        item_to_update = input(""Enter an item to update: "")
        price = int(input(""Enter updated price: ""))
        budget = peoples[people][""budget""] - peoples[people][""items""][item_to_update] - price
        peoples[people][""items""][item_to_update] = price
        peoples[people][""budget""] = budget
        print(""\nUpdated values of people"",people)
        print(peoples[people])
    
    elif option == ""2"":
        update = input(""Choose one of the 5 people to complete their shopping list: "")
        if update in peoples:
            print(""You have chosen"",update)
            peoples[people]['status'] = 'complete'
            print(""Shopping list has been completed!"")
    
    elif option == ""3"":
        display = input(""Who's do you want to look at?: "")
        print(""\nShopping List Of"",display)
        print(peoples[display])
    
    elif option == ""4"":
        print(""Thank You For Shopping With Us!"")
        break
    else:
        print(""That's not a valid answer! Try again!"")

It now looks exactly like this, and it still gives back a syntax error on the first elif statement. I don't understand what the problem is if it's properly indented and should follow the correct rules to use it.
edited with error: https://i.sstatic.net/rTW6k.png
The syntax error is finally gone, but now lies the problem where the code just repeats itself on the menu screen without going anywhere, like this:
repeating: https://i.sstatic.net/YNPdF.png
","I would do something like this:
while True:
    print(<instructions>)
    option = input(""Enter an option: "")
    if option == ""1"":
        do stuff...
    elif option == ""2"":
        do number two stuff..
    elif option == ""3"":
        do that third stuff..
    elif option == ""4"":
        print(""Thank You For Shopping With Us!"")
        break
    else:
        print(""That's not a valid answer! Try again!"")

This will keep the menu in a loop and if option 4 is selected, it will break from the loop and continue on.
The issue now is with your indentation. You must indent your code properly for python to be able to understand what you want it to do, for instance:
x= ""3""
if x == ""2"":
    print(""hello world"")
print(""outside the indent"")

you console output would be:
>>outside the indent

but if your code looks like this:
x= ""3""
if x == ""2"":
    print(""hello world"")
    print(""outside the indent"")

you would get no output from the console, everything is within the ""if"" code block. Indentation is crucial for python to exhibit the expected behavior. you need to make sure that all your code for each condition is indented properly inside the if blocks, like the example I gave above. Also, if you want this in a loop, you need to put it in a loop with the while True: statement, and indent everything inside it.
Your final result should look something like this:
peoples = {
    ""Mary"": {
        ""name"": ""Mary"",
        ""budget"": 100,
        ""items"": {
            ""Game"": 0,
            ""Book"": 0,
            ""Kindle"": 0
        },
        ""status"": ""incomplete""
    },
    ""Steve"": {
        ""name"": ""Steve"",
        ""budget"": 100,
        ""items"": {
            ""Tie"": 0,
            ""Scarf"": 0,
            ""Amazon Echo"": 0
        },
        ""status"": ""incomplete""
    },
    ""Kevin"": {
        ""name"": ""Kevin"",
        ""budget"": 65,
        ""items"": {
            ""Mario Kart"": 0
        },
        ""status"": ""incomplete""
    },
    ""Jane"": {
        ""name"": ""Jane"",
        ""budget"": 50,
        ""items"": {
            ""Gift Card"": 0,
            ""Gloves"": 0
        },
        ""status"": ""incomplete""
    },
    ""Chris"": {
        ""name"": ""Chris"",
        ""budget"": 100,
        ""items"": {
            ""Chocolates"": 0,
            ""Galaxy Tab"": 0
        },
        ""status"": ""incomplete""
    }
}

print(""""""
Menu
--------------------
1. Update Shopping List
2. Complete Shopping List
3. Display Shopping List
4. Exit Application
--------------------
Make your selection

"""""")

while True:
    option = input(""Enter an option: "")
    if option == ""1"":
        people = input(""\nWho are you updating?: "")
        print(""\nCurrent values of people"",people)
        print(peoples[people])
        print(""\nAvailable items and their prices are:"")
        for item in peoples[people][""items""]:
            print(item, peoples[people][""items""][item])
            item_to_update = input(""Enter an item to update: "")
            price = int(input(""Enter updated price: ""))
            budget = peoples[people][""budget""] - peoples[people][""items""][item_to_update] - price
            peoples[people][""items""][item_to_update] = price
            peoples[people][""budget""] = budget
            print(""\nUpdated values of people"",people)
            print(peoples[people])

    elif option == ""2"":
        update = input(""Choose one of the 5 people to complete their shopping list: "")
        if update in peoples:
            print(""You have chosen"",update)
            peoples[people]['status'] = 'complete'
            print(""Shopping list has been completed!"")

    elif option == ""3"":
        display = input(""Who's do you want to look at?: "")
        print(""\nShopping List Of"",display)
        print(peoples[display])

    elif option == ""4"":
        print(""Thank You For Shopping With Us!"")
        break
    else:
        print(""That's not a valid answer! Try again!"")

Also, please review this link as it is crucial you understand how to properly indent your code when writing python.
https://www.geeksforgeeks.org/indentation-in-python/
",prompt
shortened path relative to git repository in terminal prompt with zsh,"GOAL
I'm trying to create a shortened path for my terminal prompt that is relative to the repository in which I'm in. Similar to what I already have for my root & home directory shown below.
The last directory shown in the following terminal prompts' paths is always the current working directory.
root:
/ %
/rootDir %
/rootDir/dir1/dir2 %
/rootDir/.../dir2/dir3 %
/rootDir/.../dir3/dir4 %

home:
~ %
~/dir1/dir2 %
~/.../dir2/dir3 %
~/.../dir3/dir4 %

And I'm trying to figure a way to have it work the same way when I'm in a repository. Where the repository is the ""root"" instead of the actual root or home being the ""root"" of the path.
repository:
|repo %
|repo/dir1/dir2 %
|repo/.../dir2/dir3 %
|repo/.../dir3/dir4 %

WHAT I'VE DONE
my .zshrc code:
autoload -Uz vcs_info
zstyle ':vcs_info:git:*' formats '|%r'
zstyle ':vcs_info:git:*' check-for-changes true

setopt PROMPT_SUBST

precmd()
{
    vcs_info
    if [ -d .git ];
    then
        PROMPT='${vcs_info_msg_0_}'
    elif $(git rev-parse > /dev/null 2>&1);
    then
        PROMPT='${vcs_info_msg_0_}/%c'
    else
        PROMPT='%(4~|%-1~/.../%2~|%3~)'
    fi

    PROMPT+=' %# '
}

The solution for the path when in the root or home directory is pretty simple:
PROMPT='%(4~|%-1~/.../%2~|%3~)'

This checks the length of the path from the working directory to the which ever is the closest, root or home and displays one of two different paths depending on how many directories long the relative path is.
I couldn't figure out a way to modify this for the repository however. As I have it now it uses an if..else which checks whether my working directory is the ""root"" of the repo or if I'm further inside of the repo. And use these to either display just the name of the repo or the name and the current working directory. To make it work like with the root & home directories however I would need to have the current length between the repo and working directory or some other simpler solution which I might have missed.
","Finally got around to this again. I ended up working out a solution that does the intended:
autoload -Uz vcs_info
zstyle ':vcs_info:git:*' formats '|%r'
zstyle ':vcs_info:git:*' check-for-changes true

setopt PROMPT_SUBST

precmd()
{
    vcs_info

# in git repo
    if [[ -n $(git rev-parse --git-dir 2> /dev/null) ]]; then

    # in root of git repo
        if [ -d .git ];
        then
            PROMPT='${vcs_info_msg_0_}'

    # in first subfolder of git repo
        elif [ -d ../.git ];
        then
            PROMPT='${vcs_info_msg_0_}/%c'

    # in second subfolder of git repo
        elif [ -d ../../.git ];
        then
            PROMPT='${vcs_info_msg_0_}/%2c'

    # in deeper subfolder of git repo
        else
            PROMPT='${vcs_info_msg_0_}/.../%2c'
        fi

# not in git repo
    else
        PROMPT='%(4~|%-1~/.../%2~|%3~)'
    fi

    PROMPT+=' %# '
}

Not the most elegant of hacks but not being very familiar with the syntax this is what I came up with.
Now the function checks the working directory's relative position to the .git file that is found at the root of every git repository. To reduce the amount of checks done by the function it firstly has an if statement that checks whether the current directory is anywhere inside of a git repository. If not, the previous solution from the original post for the home and root paths is used instead.
",prompt
uncaught typeerror cannot read properties of null reading 39innerhtml39,"let fullName = prompt(""Please enter your name :"")
let surName = document.querySelector(""#surName"")
surName.innerHTML =`${surName.innerHTML}${surName}`

my piece of code is here and I'm getting an error like this. I can't import data into hmtl with inner.HTML and from prompt. What should I do ? Also index.html file
<h2 id=""surName"">Javascript Prompt</h2>

","Try this:


let fullName = prompt(""Please enter your name :"");
let surName = document.querySelector(""#surName"") ;
surName.innerHTML = fullName; // because `fullName` now has the value you enter in the prompt
<h2 id=""surName"">

</h2>



",prompt
how can i provide an editable default value when prompting for user input,"In a Perl script, I want to prompt the user for input and offer him an editable default value. So far I have this:
#!/usr/bin/perl

print ""what's your name? [John Doe]: "";
$name = <STDIN>;
chomp $name;
if (!$name)
{
    $name = ""John Doe"";
}
print ""hello $name.\n"";

What I'm looking for is a solution where I can have ""John Doe"" on STDIN already before the user starts typing. So it's practically an editable default input. E.g. the user could press backspace 3x and then type ""Wayne"" to get ""John Wayne"" rather than typing the whole string ""John Wayne"" from beginning on. I tried to print to STDIN, but that didn't work.
","You by definition cannot write to stdin.  You’re going to have to use something like Term::ReadLine::Gnu.
The feature you are looking for is called PREPUT, it is the second argument to the readline function.
",prompt
user selects the number of prompt boxes to display,"Trying to write a javascript code
That will enable user to select the number of prompt boxes.
Like,The prompt box will say, how many prompts do you want. Then if the user inputs 2. Two prompt box will show and if user inputs 3 or 4. That number of prompt boxes will show.
    let num = Number(prompt('Enter the number of prompts:'));

if (num=1){
  let name1 = prompt('Enter first user name:');
  let age1 = prompt('Enter first user age');
  
  
  console.log(name1)
  console.log(age1);
}
  
else if (num=2){
  let name1 = prompt('Enter first user name:');
  let age1 = prompt('Enter first user age');
  
  let name2 = prompt('Enter second user name:');
  let age2 = prompt('Enter second user age');
  
  console.log(name1);
  console.log(age1);
  
  console.log(name2)
  console.log(age2);
}
else{
  let name1 = prompt('Enter first user name:');
  let age1 = prompt('Enter first user age');
  
  let name2 = prompt('Enter second user name:');
  let age2 = prompt('Enter second user age');
  
  let name3 = prompt('Enter third user name:');
  let age3 = prompt('Enter third user age');
  
  
  console.log(name1)
  console.log(age1);
  
  console.log(name2)
  console.log(age2);
  
  console.log(name3)
  console.log(age3);
  
}

","I would just use a for loop.
let num = Number(prompt('Enter the number of prompts:'));

const data = [];

for (let i = 0; i < num; i++) {
  let name = prompt(`Enter user ${i}'s name:`);
  let age = prompt(`Enter user ${i}'s age:`);

  data.push({ name, age });
}

data.forEach(d => {
  console.log(d.name);
  console.log(d.age);
});

Simplify it by rewording the prompts so you can use the index of the current position in the loop.
If you want everything to be logged at the end, keep all the answers in an array, and iterate through that array at the end to log the answers.
",prompt
python user prompts choose multiple files,"I would like to have my code bring up a window where you can select multiple files within a folder and it assigns these filenames to elements of a list.
Currently, I can only select a single file at a time and it assigns the filename to a single variable.
from Tkinter import Tk
from tkFileDialog import askopenfilename

Tk().withdraw()
filename = askopenfilename()

Thank you.
","You need to use the askopenfilenames method instead.
",prompt
gradle will not build in command prompt gradle v,"Gradle shows up in echo %PATH% no problem.. 
It has has it's own directory in Program Files, however when I attempt:
gradle -v .. 

It responds: 'gradle' is not recognised as an internal or external command, operable program or batch file..
I am trying to get this to work in conjunction with an Eclipse Simpleservlet project..
","I just solved the problem. My PATH variable value was like: 

path1;path2,path3,gradlebinpath

I replaced , with ; and added a ; at the end of the value and it just worked. The final value looked like:

path1;path2;path3;gradlebinpath;

",prompt
how do i store my prompt input and display all of it when i input zero,"
let i = -1;
let total = 0;
let numPrompt;

do {
  numPrompt = prompt("""")
  total += parseInt(numPrompt)
  i++;
} 
  
while (numPrompt != '0');  // do-while loop 

let avg = total/i;
console.log(total)
console.log(avg)

I want user to input a number each time till they input zero which will output all their previous input, total and average.
I declare i = -1 to avoid counting the last input which is the zero.
in my code, I only manage to display the total of all my input but what I also want is all my previous inputs (prompt)printed out each row
","You can use an array, which stores a list of things:
let i = 0;
let total = 0;
let numPrompt;
let numbers = [];

do {
    numPrompt = prompt("""")
    total += parseInt(numPrompt)
    if (numPrompt != '0') {
        numbers.push(numPrompt);
        i++;
    }
} while (numPrompt != '0'); // do-while loop 

let avg = total / i;
console.log(total)
console.log(avg)
console.log(numbers);

Setting i to -1 is a bit of a workaround -- the above code starts it at 0 and checks if the inputted number is 0 before incrementing it.
",prompt
alert windows for second function not popping up,"I'm working on an assignment where I have to write a basic program that generates a random password based on certain criteria. I have written the following snippet of code to receive input from the user regarding the minimum and maximum values of the desired password length.
`
function writePassword() {
  
  var passwordText = document.querySelector(""#password"");
  passwordText.value = password;

  function exit(){ 
    return;
  }

  function numbersOnly(num){
    return /^[0-9]+$/.test(num); //checks if input only contains numbers
  }

  function getPassMin() {
  let passMin = prompt(""Input the minimum length of your password in characters (If blank, defaults to minimum length of 8):"");

  if (passMin < 8 && passMin !== """" && passMin !== null) { //checks if value is at least 8, reprompts if not
    alert(""Sorry, the password must be at least 8 characters."");
    getPassMin();
  } else if (passMin > 128) { //checks if value is not more than 128, reprompts if not
    alert(""Sorry, the password cannot exceed 128 characters."");
    getPassMin();
  } else if (passMin == """" || (numbersOnly(passMin) === false && passMin !== null)) { 
    alert(""Please input a valid number in numeric format."");
    getPassMin();
  } else if (passMin === null){ //terminates program if cancel is pressed 
    writePassword.exit();
  } 
}
  
function getPassMax() {
  let passMax = prompt(""Input the minimum length of your password in characters (If blank, defaults to maximum length of 128):"");

  if (passMax > 128 && passMax!== """" && passMax !== null) { //checks if value is not more than 128, reprompts if not
    alert(""Sorry, the password cannot exceed 128 characters."");
    getPassMax();
  } else if (passMax < 8) { //checks if value is at least 8, reprompts if not
    alert(""Sorry, the password must be at least 8 characters."");
    getPassMax();
  } else if (passMax == """" || (numbersOnly(passMax) === false && passMax !== null)) { //checks if value is not blank and does not contain any non-numeric characters, reprompts if not
    alert(""Please input a valid number in numeric format."");
    getPassMax();
  } else if (passMax < passMin){ //checks if value of getPassMax is not less than getPassMin, reprompts if not
    alert(""Maximum value cannot be less than minimum length. You specified the minimum length as "" + passMin + "" characters."");
    getPassMax();
  } else if (passMax === null){ //terminates program if cancel is pressed 
    writePassword.exit();  
  }


}
getPassMin();
getPassMax();
}

`
The prompts in the getPassMin() method all work as expected, however the getPassMax() method only returns the proper prompt if the input value is  either greater than 128 or less than 8, i.e the first 2 conditions. All the others simply result in closing the dialog window. I am not currently receiving any errors at any point during execution. Am I missing something?
I have tried copying and reusing the code from the getPassMin() method, and I have also tried re-writing the code from scratch. No success with either.
","Your method getPassMax() cannot see the variable passMin which is causing an error. You could declare the variable passMin outside the scope of both methods, set the value inside getPassMin() and then it can be read inside getPassMax().
Also, be careful. When you've checked the value from the prompt is not null or empty (and against your number pattern or use if (!isNaN(value)) { ... }) then you should convert the value to a number. Currently you are comparing numbers inside strings which won't give you correct results.
You can simply use let value = Number(input); and input will be converted to a number. Now your number comparisons will be correct.
",prompt
execute a shell script as it would be an interactive session,"For documentation purposes I am trying to execute a shell script in a way that it looks as you typed it by hand in an interactive shell.
Script:
x=123
echo $x

Then execute:
PS4=""$PS1""
set -x -v
. ./demo

Output:
. ./demo
user@host:~/tmp$ . ./demo
x=123
user@host:~/tmp$ x=123
echo $x
user@host:~/tmp$ echo 123
123

Desired output:
user@host:~/tmp$ x=123
user@host:~/tmp$ echo $x
123

It does not have to be bash. Any solution that simulates an interactive session is welcome.
How can I achieve the desired result?
","A pretty simple solution is to start an interactive shell and redirect your script file to the input stream:
bash -i </path/to/script-file

",prompt
how to rerun a while loop by yesno prompt to search a file until it apears,"I'm trying to write a script which helps me to search in log for a phrase ""started"".
The script that I have until now looks like this:
#!/bin/bash
greperg= 
i=3

echo ""Web server is going to be polled""
while 
    i=`expr $i - 1`
    #polling
    echo ""Polling Nr. $i""
    grep -q '^started$' log
    greperg=$?
 
    #test, if it goes on
    test ""$greperg"" -gt 0 -a ""$i"" -gt 0

do
    #waiting    
    echo 'waiting ...'
    sleep 1
done    
if test  ""$greperg"" -eq 0 
then 
    echo ""Web server has started"" 
else
    echo -n ""Web server is not started""
fi

valid=''
while ((!valid)); do

    echo - ""Do you want to poll again? (J/N)""
    read -t 5  answer

    case ""$answer""  in 
        [Jj]) result=1; valid=1;;
        [Nn]) result=0; valid=1;;
        """")   result=0; valid=1;;
        *)    valid=0 ;;

    esac 
done
echo
if ((result));then
     : # ...............(repeat the process again, if its not found ask max 5 times)
else
    echo ""Timeout""
fi
exit 0

From line 38, I don't know how to re-run it, can anybody help?
What i'm looking for:

The polling should be expanded: If after the 3 attempts that word (started) is still not there, then ask the user with a (Y / N) query whether more should be polled 3 times or not.

The user should be asked a maximum of 5 times
(So a maximum of 3 × 6 = 18 times is polled).

At the very end please state what the status reached is (see example below).

 polling ...
 Web server has not started yet.
 Wait ...
 polling ...
 Web server has not started yet.
 Wait ...
 polling ...
 Web server has not started yet.
 Should it be polled again (Y / N)? _ Y
 polling ...
 Web server has not started yet.
 Wait ...
 polling ...
 Web server has not started yet.
 Wait ...
 polling ...
 Web server has not started yet.
 Should it be polled again (Y / N)? _ N
 As requested, no further attempts are made.
 Bottom line: web server has not started.



","Your code has several odd designs. Bash should generally not need to use expr at all (the shell has built-in facilities for integer arithmetic and substring matching) and you usually want to avoid explicitly testing $?. I would break this up into functions to ""divide and conquer"" the problem space.
#!/bin/bash

# Print diagnostics to standard error; include script name
echo ""$0: Web server is going to be polled"" >&2

status=2

poll () {
    local i
    for ((i=1; i<=$1; ++i)); do
        # Move wait to beginning of loop
        if (($i > 1)); then
            echo ""$0: waiting ..."" >&2
            sleep 1
        fi

        echo ""$0: Polling Nr. $i"" >&2

        # Just check
        if grep -q '^started$' log; then
            # return success
            return 0
        fi
    done

    # If we fall through to here, return failure
    return 1
}

again () {
    while true; do
        # Notice read -p for prompt
        read -t 5 -p ""Do you want to poll again? (J/N)"" answer

        case ""$answer"" in 
            [Jj]*) return 0;;
            """")    continue;;
           *)      return 1;;
        esac 
    done
}

while true; do
    if poll 3
    then 
        echo ""$0: Web server has started"" >&2
        status=0
    else
        echo ""$0: Web server is not started"" >&2
        status=1
    fi

    again || break
done

# Actually return the status to the caller
exit ""$status""

The while true loop in the main script could easily be adapted to a for loop just like in the poll function if you want to restrict how many times the user is allowed to restart the polling. I wanted to show two different designs just to exhibit the options available to you.
In a real script, I would probably replace several of the simple if tests with the this || that shorthand. In brief,
this && that || other

is roughly equivalent to
if this; then
   that
else
   other
fi

with the difference that that if that fails, you will also trigger other in the shorthand case.
Perhaps notice also how ((...)) in Bash is an arithmetic context. The three-place for loop is also a Bash extension.
",prompt
how to prompt an sql query user to input information,"I have a query that I wrote in SQL server that will be run mainly by people who don't know SQL, and there are two areas that have to have a different string or date entered each time the query is run. As of right now, I just wrote it so that you enter the information at the top of the query and its stored as a variable. Is there a way that I can get SQL to prompt the person running the query to enter the data? below is an excerpt of the code that has what I am talking about in it.
declare 
/*ENTER ACCOUNTING MONTH*/     
   @amon VARCHAR(2) = '05',
/*ENTER INVOICE DATE IN MM/DD/YYYY FORMAT*/
   @invdate DATE = '05/31/2015'
~~
rest of the code
~~
declare @sumA numeric(25, 5), @sumB numeric(25, 5), @ratio numeric(25, 5)
select @sumA = sum(amnt) from accnt where accno = '1152'
select @sumB = sum(amnt) from acc1152
update acc1152 set amnt = amnt * (@sumA/@sumB),
amon = @amon,
invdate = @invdate,
ven = '1152',
code = '1152',
invno = 'INVENTORY'

so is it possible for SQL to prompt the user to type in the value for @amon and @invdate? other than me just having the comment line telling them to do so?
","In case you can not do an application, you have no developers etc etc, you have one way - make a stored proc:
create stored procedure spDoSomeJob
@amon VARCHAR(2),
@invdate DATE
as
begin

    ~~
    rest of the code
    ~~
    declare @sumA numeric(25, 5), @sumB numeric(25, 5), @ratio numeric(25, 5)
    select @sumA = sum(amnt) from accnt where accno = '1152'
    select @sumB = sum(amnt) from acc1152
    update acc1152 set amnt = amnt * (@sumA/@sumB),
    amon = @amon,
    invdate = @invdate,
    ven = '1152',
    code = '1152',
    invno = 'INVENTORY'

end

Deny any activity permissions for users except just running this procedure. Execute it like:
exec spDoSomeJob  @amon = '05', @invdate = '05/31/2015'

At least you will be sure that no user can occasionally corrupt something... And if you will not supply values to parameters of stored procedure it will prompt you to do this unless you have no default values for those parameters. It seems to me like the best workaround for your case.
",prompt
is there a module or something similar for interactive prompts in powershell,"Is there something we can use in PowerShell to ask users to select one item from an array of items? For example, I like how Inquirer.js can do it. 

I have also seen PoshGui, but it seems too much work to create just a simple prompt.
The reason we want something similar is that we need to provide deployment scripts for our clients and make deployment guides as easy as possible. Asking users to select one item on a screen is much better than asking them to insert some guid to a config file.
Do you have any suggestions for user prompts for arrays?
","All of the answers are correct, but I also wrote a few reusable PowerShell helper functions. Readme. I auto-generate basic looking WinForms. Looks ugly, but works.
https://github.com/Zerg00s/powershell-forms
$selectedItem = Get-FormArrayItem (Get-ChildItem)


$Delete = Get-FormBinaryAnswer ""Delete file?""


$newFileName = Get-FormStringInput ""Enter new file name"" -defaultValue ""My new file""


# -------------------------------------------------------------------------------
# Prepare the list of inputs that user needs to populate using an interactive form    
# -------------------------------------------------------------------------------
$preDeployInputs = @{
    suffix                       = """"
    SPSiteUrl                    = ""https://ENTER_SHAREPOINT_SITE.sharepoint.com""
    TimeZone                     = ""Central Standard Time""
    sendGridRegistrationEmail    = ""ENTER_VALID_EMAIL_ADDRESS""
    sendGridRegistrationPassword = $sendGridPassword
    sendGridRegistrationCompany  = ""Contoso & Tailspin""
    sendGridRegistrationWebsite  = ""https://www.company.com""
    fromEmail                    = ""no-reply@DOMAIN.COM""
}

$preDeployInputs = Get-FormItemProperties -item $preDeployInputs -dialogTitle ""Fill these required fields""


",prompt
sweetalert prompt with two input fields,"Currently working on a personal project. I want the user to click a button and a SweetAlert prompt would be presented for the user to verify their credential. However, the code I see on the SweetAlert website only allows one input field. Here is the code I have:
swal({
  title: ""Authenicating for continuation"",
  text: ""Test"",
  type: ""input"",
  showCancelButton: true,
  closeOnConfirm: false,
  animation: ""slide-from-top"",
  inputPlaceholder: ""Write something""
}, function(inputValue) {
  if (inputValue === false) return false;
  if (inputValue === """") {
    swal.showInputError(""You need to write something!"");
    return false
  }
  // swal(""Nice!"", ""You wrote: "" + inputValue, ""success"");
});

So, is there a way I can get two input fields? One input field for the password and the other input field for text.
","As far as I know you can't do this off-the-shelf. You can either fork and implement, or just use a HTML element as a modal (e.g. as in Bootstrap's modals).
",prompt
javascript prompt loads before page displays,"I added a prompt on my page but it loads before the page has loaded.
How do I only show the message once the whole page is visible?
Here is my prompt:
if (name == null || name == """") {
  txt == ""No name provided"";
} else {
  txt = ""Hello, "" + name + ""! How are you today?"";
}
alert(txt);


    

","If you wrap the code in an event-listener that listens for the DOMContentLoaded event it'll run only once the document is ready:
window.addEventListener('DOMContentLoaded', (e)=>{
  if (name == null || name == """") {
    txt == ""No name provided"";
  } else {
    txt = ""Hello, "" + name + ""! How are you today?"";
  }
  alert(txt);
});

I have, however, adjusted your original code to:


window.addEventListener('DOMContentLoaded', (e) => {
  // assign the name via the prompt() interface, and
  // declare both variables (rather than accidentally
  // creating globals):
  let name = prompt(""Hi, what's your name?""),
      txt;
  // name won't be null, but it may be falsey, so here
  // we check if the name is falsey:
  if (!name) {
    // if the user cancels the prompt, prompt() returns
    // false; if they provide an empty-string that
    // empty string is falsey:
    txt = ""No name provided"";
  // if the name variable is truthy:
  } else {
    // we use a template literal String to interpolate
    // the variable 'name' into that string:
    txt = `Hello, ${name}! How are you today?`;
  }
  // I wouldn't recommend alert, but I left this unchanged:
  alert(txt);
});



JS Fiddle demo.
References:

alert().
DOMContentLoaded event.
EventTarget.addEventListener().
prompt().
Template literals.
window.

",prompt
javascript  how to save string prompt input into an array,"I need to write a function that takes two arguments, a number to start and a number to end a range.
The function returns an array of the range including start and end.
Also, I need to test the arguments: If they are not numbers return a meaningful message and an array of ALL the arguments.
I now how to write this function with numbers but not with Strings.
","Here is an example.
This will take the two numbers given and populate the array from smallest to largest.
You can can then decide how to handle the array after that.
let arr = [];                    //create array

let numOne = validateNumber();
let numTwo = validateNumber();
createArray(numOne, numTwo);     // call function to populate array

arr.forEach(number => {
  console.log(number);
})              


function validateNumber(){
  let pass = false;
  let number = 0;
  while (!pass){
    number = parseInt(prompt('Enter a number'));
    if (Number.isInteger(number)) pass = true;
  }
  return number;

}

function createArray(numOne, numTwo){
  var start = numOne;
  var end = numTwo;

  if (numOne > numTwo){
    start = numTwo;
    end = numOne;
  }
  for (var i = start; i <= end; i++) {
    arr.push(i)
  }
}

",prompt
googlescriptrunwithsuccesshandler works for me but not other users,"The following OnClick closes a DialogBox and its withSuccessHandler opens a UI.Prompt.
This successfully closes the Starting UI for me but no other user. Why is this?
onClick=""google.script.run.withSuccessHandler(function(){google.script.host.close();}).someOtherFunction()

Did I miss something about sharing my Excel App Scripts with other users?
Full Code here:
function onOpen() {
   var menu = SpreadsheetApp.getUi().createMenu(""⚙️ Start App"");
   menu.addItem(""Start"", ""StartPopup"");  
   menu.addToUi();
}

async function StartPopup(){

  var ui = SpreadsheetApp.getUi();
  
  var html = `
    <p>  App Modes </p> </center>
    <p class=""body"" style=""font-family: sans-serif; color:gray; text-align:left"">
    Options:</p>

    <body>

    <p>
    <input type=""button"" id=""no"" value=""RUN"" onClick=""google.script.run.withSuccessHandler(function(){google.script.host.close();}).userPromptcustomFunction()""/>
    </p>
      </body>  ` 
  var htmlOutput = HtmlService
      .createHtmlOutput(html)
      .setWidth(700)
      .setHeight(600);


  SpreadsheetApp.getUi().showModalDialog(htmlOutput, 'My Excel App.');


}

function userPromptcustomFunction(){
  currentMonth = SpreadsheetApp.getUi().prompt(""Choose Month to Perform Operation."", ui.ButtonSet.YES_NO);
}

","Try this instead.  I get the user input from the modal dialog and then send to the server.

function StartPopup(){
  try {
    var html = `
    <body>
      <p>  App Modes </p> </center>
      <p class=""body"" style=""font-family: sans-serif; color:gray; text-align:left"">
        Options:
      </p>
      <p>
        <label for=""month"">Choose Month to Perform Operation</label>
        <input type=""text"" id=""month"">
      </p>
      <p>
        <input type=""button"" id=""no"" value=""RUN"" onClick=""buttonOnClick()""/>
      </p>
      <script>
        function buttonOnClick() {
          let month = document.getElementById(""month"").value;
          alert(month);
          google.script.run.withSuccessHandler( function () { google.script.host.close(); }).userPromptcustomFunction(month);
        }
      </script>
    </body>  `
    var htmlOutput = HtmlService.createHtmlOutput(html);
    SpreadsheetApp.getUi().showModalDialog(htmlOutput,""My Google App Script"");
  }
  catch(err) {
    Logger.log(err);
  }
}

function userPromptcustomFunction(month) {
  try {
    let prop = PropertiesService.getUserProperties();
    prop.setProperty(""month"",month);
    Logger.log(""montn = ""+prop.getProperty(""month""));
  }
  catch(err) {
    Logger.log(err);
  }
}

Execution log
Head    userPromptcustomFunction    Unknown Oct 11, 2022, 9:03:47 AM    1.426 s 
Completed
Cloud logs
Oct 11, 2022, 9:03:49 AM    Info    montn = Jan

",prompt
how do i format text using a variable,"Basically, I want to be able to ask a question like ""What colour do you want?"" and then make it so that based on the answer it will set a variable and be usable in formatting.
So something like this:
print(""\033[1;35m\033[1;4m What colour would you like the printed text to be?"")
print(""\033[1;0m\033[1;1m    1. Red"")
print(""    2. Green"")
print(""    3. Blue"")
ans1 = input()
ans1 = float(ans1)

if ans1 == 1:
    colour = 31
    print(""\033[1;(colour)m This text is red"")

elif ans1 == 2:
    colour = 32
    print(""\033[1;(colour)m This text is green"")

elif ans1 == 3:
    colour = 35
    print(""\033[1;(colour)m This text is blue"")

and then the text would be the right colour.
Is this possible and if so how could I go about doing it?
","print(""\033[1;35m\033[1;4m What colour would you like the printed text to be?"")
print(""\033[1;0m\033[1;1m    1. Red"")
print(""    2. Green"")
print(""    3. Blue"")
ans1 = input()
ans1 = float(ans1)

if ans1 == 1:
    colour = 31
    print(f""\033[1;{colour}m This text is red"")

elif ans1 == 2:
    colour = 32
    print(f""\033[1;{colour}m This text is green"")

elif ans1 == 3:
    colour = 35
    print(f""\033[1;{colour}m This text is blue"")



A formatted string literal or f-string is a string literal that is prefixed with 'f' or 'F'. These strings may contain replacement fields, which are expressions delimited by curly braces {}. While other string literals always have a constant value, formatted strings are really expressions evaluated at run time.
",prompt
prompting for bluetooth permission on big sur 1101,"Upon upgrading to Big Sur (macOS 11.0.1), our app is now asking the user to grant permission to use Bluetooth. However, we are not running any bluetooth code to my knowledge. We do link to CoreBluetooth, but it's been like that for a long time, and this prompt has not appeared.
When running in Xcode 12.2 it does crash with this message:
2020-11-13 13:21:58.685610-0800 Fuze[31049:200367] [access] This app has crashed because it attempted to access privacy-sensitive data without a usage description.  The app's Info.plist must contain an NSBluetoothAlwaysUsageDescription key with a string value explaining to the user how the app uses this data.

But to my knowledge, NSBluetoothAlwaysUsageDescription is only applicable for iOS devices.
I suspect this is something new with Big Sur, because the System Preferences UI for Security & Privacy -> Privacy, did not have a Bluetooth section in Catalina, and it does in Bug Sur. But the Apple documentation does not make mention of this key being applicable for macOS.
https://developer.apple.com/documentation/bundleresources/information_property_list/nsbluetoothalwaysusagedescription
So my questions are:

Is this key now applicable for macOS and we should have it, even though the documentation says it's only for other platforms, or is there a different key for bluetooth for macOS? I haven't found one.
Is there a way to control this prompting or not, because we aren't actually using the bluetooth code. We need the framework for a particular feature, but it's not applicable for many users, so it's confusing to ask permission for it right when the app launches.

Thank you!
","I'm not confident about this, but I've been running into some Bluetooth on Big Sur strangeness today as well. In my case, I run a Bluetooth peripheral from the Terminal, and I was also recently prompted to provide the Terminal with Bluetooth permission (which I'd never seen before).
For testing purposes, you can try the ""Privacy — Bluetooth Peripheral Usage Description"" or ""Privacy - Bluetooth Always Usage Description"" - and see whether that resolves the crash (I assume it would). It's strange, as I didn't think either of these were required in the past - but it looks like Big Sur is definitely cracking down on permissions.
When you go to ""Signing and Capabilities"", if there is an option to enable ""Hardware -> Bluetooth"" in the ""App Sandbox"" - I would try that out too (you may also need to enable Location, but I can't recall). I'm curious if modifying that flag would change anything regarding permissions.
When you link to CoreBluetooth, are you using the API at all? Because the permission check should happen on the first API call. But, Bluetooth is weird, so it might kick in as soon as the app launches...
In thinking about it, I've actually never optionally used Bluetooth in an app - it's always initialized from app start, so I can't say when the permission check occurs.
",prompt
how to create a popup using google apps script only if a certain button is pushed on the previous popup and remove text box,"My code below opens 4 text boxes, 1 asking a YES/NO question and 3 asking OK/CANCEL questions. Every one of them accepts text, but I want the first question to only accept buttons YES and NO without a text box. Also, if the answer to the first question is NO, I want it to skip the second question and go straight to the third question.
Questions are as follows:

Did we do work for this client today/yesterday? [YES/NO]
What did we help this client with today? [textbox][OK/CANCEL] ***skip if previous answer is no
When should we follow up with this client next? (MM/DD/YY) [textbox][OK/CANCEL]
Next Follow Up Activity? [textbox][OK/CANCEL]

function TextBox() {
  var ui = SpreadsheetApp.getUi();
  const sheet = SpreadsheetApp.openById('1JZ-v5n5m0_lyaoLgsHDAa78AtYNP5AsUDo2NRpJbwG4').getSheetByName('HOT & WARM CLIENTS');
  sheet.sort(9)

  var today = Utilities.formatDate(new Date(), ""GMT-6"", ""MM/dd/yy"")
  var ui = SpreadsheetApp.getUi();
  var valuesToCopy = sheet.getRange(""C5:J5"").getDisplayValues().map(r => r.join(' \n '));

var ar2 = [
{
  message: valuesToCopy + ""\n\n Did we do work for this client today/yesterday? (YES/NO)"",
  range: ""G5""
}

];

var ar3 = [
    
    { // promptUPDATE3
      message: valuesToCopy + ""\n\n What did we help this client with today?"",
      range: ""H5""
    },
    
  ];

var ar = [
    // { // promptUPDATE3
    //   message: valuesToCopy + ""\n\n What did we help this client with today?"",
    //   range: ""PREVIOUS""
    // },
    { // promptUPDATE
      message: valuesToCopy + ""\n\n When should we follow up with this client next? (MM/DD/YY)"",
      range: ""I5""
    },
    { // promptUPDATE2
      message: valuesToCopy + ""\n\n Next Follow Up Activity?"",
      range: ""J5""
    }
  ];

ar2.forEach(({message, range }) => {
  var res = ui.prompt(message, ui.ButtonSet.YES_NO);
  if (res.getSelectedButton() == ui.Button.YES) {
    sheet.getRange(""G5"").setValue(today);

  } else { 
    Logger.log('The user clicked ""No"" or the dialog\'s close button.');
  }
})


  ar3.forEach(({ message, range }) => {
    var res = ui.prompt(message, ui.ButtonSet.OK_CANCEL);
    var lastRowSourceData = sheet.getRange(""H5"").getValue();
    var lastActivity = SpreadsheetApp.openById('1JZ-v5n5m0_lyaoLgsHDAa78AtYNP5AsUDo2NRpJbwG4').getSheetByName('HOT & WARM CLIENTS').getRange(""H5"").getValue();
    if (res.getSelectedButton() == ui.Button.OK) {
      sheet.getRange(range).setValue(lastActivity+"" | ""+res.getResponseText());
    } else {
      Logger.log('The user clicked ""No"" or the dialog\'s close button.');
    }
  })



  ar.forEach(({ message, range }) => {
    var res = ui.prompt(message, ui.ButtonSet.OK_CANCEL);
    if (res.getSelectedButton() == ui.Button.OK) {
      sheet.getRange(range).setValue(res.getResponseText());

    } else {
      Logger.log('The user clicked ""No"" or the dialog\'s close button.');
    }
  })
  
  }

","SUGGESTION [SCRIPT UPDATED]
Perhaps you can try this tweaked script below. This will use the alert Class Ui method for the first question & an if condition to skip the order of questions if the user selects NO.
In my understanding, here is the flow you want to achieve:

The first question should not have a Textbox but only give the user to select YES or NO buttons.
If user selects NO on the first question, route the user to the third question instead.
Otherwise, the user will be prompted with the questions in order.


NOTE: This sample script will only process the first client on your sheet just like on your actual script. If I have misunderstood something Or if there's anything else missing, feel free to let me know.

Sample Tweaked Script
function TextBox() {
  var ui = SpreadsheetApp.getUi();
  var today = Utilities.formatDate(new Date(), ""GMT-6"", ""MM/dd/yy"");
  const sheet = SpreadsheetApp.getActiveSpreadsheet().getSheetByName('HOT & WARM CLIENTS');
  sheet.sort(9);
  var sheetData = sheet.getRange(""C5:J5"").getDisplayValues();
  var questions = [['\n\n Did we do work for this client today/yesterday? (YES/NO)', ""G""],
  ['\n\n What did we help this client with today?', ""H""],
  ['\n\n When should we follow up with this client next? (MM/DD/YY)', ""I""],
  ['\n\n Next Follow Up Activity?', ""J""]];


  var row = ""5""; //ONLY process current client on 5th row
  var clientDetails = sheetData[0].join('\n');

  /**First question */
  var q1 = ui.alert(clientDetails + questions[0][0], ui.ButtonSet.YES_NO)
  if (q1 == ui.Button.YES) {
    sheet.getRange(questions[0][1] + row).setValue(today);
    /**End of the First question */

    /**Second question */
    var q2 = ui.prompt(clientDetails + questions[1][0], ui.ButtonSet.OK_CANCEL);
    var lastActivity = sheet.getRange(questions[1][1] + row).getValue();
    q2.getSelectedButton() == ui.Button.OK ? sheet.getRange(questions[1][1] + row).setValue(lastActivity + "" | "" + q2.getResponseText()) : console.log('The user clicked ""No"" or the dialog\'s close button.');
    /**End of the Second question*/

    /**Third question */
    var q3 = ui.prompt(clientDetails + questions[2][0], ui.ButtonSet.OK_CANCEL);
    q3.getSelectedButton() == ui.Button.OK ? sheet.getRange(questions[2][1] + row).setValue(q3.getResponseText()) : console.log('The user clicked ""No"" or the dialog\'s close button.');
    /**End of the Third question*/

    /**Fourth question */
    var q4 = ui.prompt(clientDetails + questions[3][0], ui.ButtonSet.OK_CANCEL);
    q4.getSelectedButton() == ui.Button.OK ? sheet.getRange(questions[3][1] + row).setValue(q4.getResponseText()) : console.log('The user clicked ""No"" or the dialog\'s close button.');
    /**End of the Fourth question*/
  } else {
    /**Skip to the third question if 'NO' was selected on the first question*/
    var q3 = ui.prompt(clientDetails + questions[2][0], ui.ButtonSet.OK_CANCEL);
    q3.getSelectedButton() == ui.Button.OK ? sheet.getRange(questions[2][1] + row).setValue(q3.getResponseText()) : console.log('The user clicked ""No"" or the dialog\'s close button.');

    /**End to fourth question*/
    var q4 = ui.prompt(clientDetails + questions[3][0], ui.ButtonSet.OK_CANCEL);
    q4.getSelectedButton() == ui.Button.OK ? sheet.getRange(questions[3][1] + row).setValue(q4.getResponseText()) : console.log('The user clicked ""No"" or the dialog\'s close button.');

  }
}

Demonstration
- Quick Test

- E.g. If the user selects NO on the first question & then gets skipped to the third question

References

alert(prompt, buttons)

",prompt
cypress  is it possible to verify text in prompt,"Is it possible to verify 'what's your sign'text in prompt?
Alert and Confirm can be verified text, but prompt I wonder if possible?

","You can set up a stub before the action that triggers the prompt:
Generally,
let stub;

cy.window().then(win => {
  stub = cy.stub(win, 'prompt').returns(' World')  // types this value 
})

cy.get('button-opening-prompt').click()

cy.wrap(stub).should(() => {
  expect(stub).to.have.been.calledWith('Hello') // argument is the message
                                                // displayed in the prompt
})

A simple app that prompts after 2 seconds:
<body>
  <div>Hello </div>
  <script>
    setTimeout(() => {
      const result = window.prompt('Hello')
      const div = document.querySelector('div')
      div.innerText += result 
    }, 2000)
  </script>
</body>

This can be tested with
let stub;

cy.visit('http://127.0.0.1:5500/html/prompt.html', {   
  onBeforeLoad: (win) => {
    stub = cy.stub(win, 'prompt').returns(' World')
  }
})

cy.get('div').should('contain', 'Hello World')   // checks the result added to page

cy.wrap(stub).should(() => {
  expect(stub).to.have.been.calledWith('Hello')
})

Cancel button
To test the cancel button being pressed, substitute
stub = cy.stub(win, 'prompt').callsFake(() => null)

",prompt
detecting unsaved changes,"I have a requirement to implement an ""Unsaved Changes"" prompt in an ASP .Net application.  If a user modifies controls on a web form, and attempts to navigate away before saving, a prompt should appear warning them that they have unsaved changes, and give them the option to cancel and stay on the current page.  The prompt should not display if the user hasn't touched any of the controls.
Ideally I'd like to implement this in JavaScript, but before I go down the path of rolling my own code, are there any existing frameworks or recommended design patterns for achieving this?  Ideally I'd like something that can easily be reused across multiple pages with minimal changes.
","Using jQuery:
var _isDirty = false;
$(""input[type='text']"").change(function(){
  _isDirty = true;
});
// replicate for other input types and selects

Combine with onunload/onbeforeunload methods as required.
From the comments, the following references all input fields, without duplicating code:
$(':input').change(function () {

Using $("":input"") refers to all input, textarea, select, and button elements.
",prompt
how to validate command line input from multiple choice prompt in ruby,"I'm writing a multiple choice quiz where the user has to choose an answer by entering a letter from a to d. Each question prompt looks like this:
What is the last letter in alphabet?
(a) Y
(b) R
(c) Z
(d) Q

If the user enters anything else, I want to show a message and print the whole question again until the input is a, b, c or d.
Here's what I've tried: (simplified)
question = ""What is the last letter in alphabet?\n(a) Y\n(b) R\n(c) Z\n(d) Q""

puts question
answer = gets.chomp.to_s
while answer > ""d""
  puts ""Please enter a, b, c, or d""
  puts
  puts question
  answer = gets.chomp.to_s
end

It works fine when entering e, f, g etc. but it doesn't catch input like ab, 1, Z, or when the user just presses enter.
","Your approach doesn't work because answer > ""d"" compares both strings character-wise using their (Unicode) code points. To understand what this means, take a look at the Basic Latin chart: (it's equivalent to an ASCII table)

""d"" has a codepoint of U+0064. Any smaller codepoint – i.e every character in the chart before ""d"" – is regarded smaller. This includes all (regular) digits, all (basic latin) uppercase letters and several symbols:
""0"" > ""d""  #=> false
""Z"" > ""d""  #=> false

You could add a lower bound like answer < ""a"" || answer > ""d"" but this would still allow all strings starting with one of the allowed characters, e.g.:
""apple"" < ""a"" || ""apple"" > ""d"" #=> false

To actually limit the answer to the four allowed values, you have to compare the string to each of them. You could combine these comparisons:
answer == 'a' || answer == 'b' || answer == 'c' || answer == 'd'

use a loop over an array of allowed values:
['a', 'b', 'c', 'd'].any? { |letter| answer == letter }

check whether the array includes the answer:
['a', 'b', 'c', 'd'].include?(answer)

# or

%w[a b c d].include?(answer)

or use a regular expression to match a to d:
answer.match?(/\A[a-d]\z/)

Note that the examples above become true if answer is between a and d. You could either negate the condition via !(...):
while !(%w[a b c d].include?(answer))
  # ...
end

or use until instead of while:
until %w[a b c d].include?(answer)
  # ...
end

",prompt
click custom option prompt function,"I have noticed that prompt using click accepts inputs with trailing spaces
ftp_server = click.prompt(""FTP Server"")

Is there a way to use a custom return function like this to reject trailing spaces?
def custom_prompt(value):
    if value.strip():
        return True
    else:
        return False

ftp_server = click.prompt(""FTP Server"", custom_prompt)

I have already used this:
while not ftp_server.strip():
    ftp_server = click.prompt(""FTP Server"")

But I'm looking for a better way because I don't want to use a while loop each time I use prompt.
","To reject invalid user input, you can use the value_proc parameter to click.prompt.  A validator to reject input with trailing spaces could look like:
Prompt Validator
import click

def validate_no_trailing_space(value):
    if value != value.rstrip():
        raise click.UsageError(""Trailing space is invalid!"")
    return value

ftp_server = click.prompt(""FTP Server"",
                          value_proc=validate_no_trailing_space)

Trim Spaces
You might also consider a validator which trims leading and trailing spaces but reject spaces in the name:
def validate_no_internal_space(value):
    value = value.strip()
    if ' ' in value:
        raise click.UsageError(""Spaces are not valid here!"")
    return value

",prompt
pass prompt option to a file,"I want to create a script where I have pre-defined option values.
opt1 opt2 opt3
I will start a script and it will ask me to choose from opt1 - opt3.
Once I pick e.g. opt2, that opt2 will then be passed as a variable.
How can I please achieve it?
","You can use the ""case"" command to check if the user value is on a list of expected values:
#!/bin/bash

selected_option=""""

echo "" *** MENU ***""
echo ""1) Opt1""
echo ""2) Opt2""
echo ""3) Opt3""
echo -n "" Please, enter your option: ""
read user_answer

case $user_answer in
  1|2|3) selected_option=""$user_answer""
         ;;
  *) echo ""Invalid Option!""
esac

##### Show the result only if the user selected an valid option #####
if [[ ! -z ""${selected_option}"" ]]; then
  echo ""Selected Option: [${selected_option}]""
fi

The '|' can be used to separate the valid options, and it will act as an ""or"" operator.
The '*' section will be executed if the previous condition is not satisfied, which means a ""default"" behavior, at this case it will display ""Invalid Option"" message.
Finally, the last if checks if the variable ""${selected_option}"" is empty, if not it is printed, but you can do whatever you want with that.
",prompt
if statement in fish_prompt,"I'm trying to customize my fish prompt but i can't seem to put an if statement in it.
What I want : user@host ~/.config [127]>
What i tried :
function fish_prompt 
    echo -n $USER
    echo -n ""@""
    echo -n $hostname
    echo -n "" ""
    echo -n (prompt_pwd)
    echo -n (__fish_git_prompt)
    if [ $status -ne 0 ] 
        echo -n "" [$status] ""
    end 
    echo -n ""> ""
end 

I want the $status to show only when there is an error.
So far, everything works except the if statement. (i tried the same if directly in the console and it worked, the problem occurs only in the fish_prompt) .
Thanks
","echo also returns a status, so $status is updated.
So if you are interested in the commandline's $status, you need to save that before doing anything else.
Do this:
function fish_prompt
    # $status here is what the commandline returned,
    # save it so we can use it later
    set -l laststatus $status
    echo -n $USER
    echo -n ""@""
    echo -n $hostname
    echo -n "" ""
    echo -n (prompt_pwd)
    echo -n (__fish_git_prompt)
    if [ $laststatus -ne 0 ] 
        echo -n "" [$laststatus] ""
    end 
    echo -n ""> ""
end 

This is also what fish's sample prompts do
",prompt
anaconda  python change anaconda prompt user path,"I want to change my Anaconda Prompt User file path.  Currently it is as follows:

I want it to change to: C:\Users\u354590
How do I do this? 
The current version of anaconda I have is:
Python 3.6.3 |Anaconda, Inc.| (default, Oct 15 2017, 03:27:45) [MSC v.1900 64 bit (AMD64)]

","In Windows, if you have the shortcut in your taskbar, right-click the ""Anaconda Prompt"" icon, you'll see:

Anaconda Prompt
Unpin from taskbar (if pinned)
Close window

Right-click on ""Anaconda Prompt"" again.
Click ""Properties""
Add the path you want your anaconda prompt to open up into in the ""Start In:"" section.
Note - you can also do this by searching for ""Anaconda Prompt"" in the Start Menu. The directions above are specifically for the shortcut.
",prompt
added git branch to bash prompt now it can39t find the start of the command,"I added the git branch to my bash command line according to the method in https://stackoverflow.com/a/35218509/19776334.
My .bashrc file contains the following lines:
parse_git_branch() {
git branch 2> /dev/null | sed -e '/^[^*]/d' -e 's/* \(.*\)/[\1] /'
}

PS1=""\[${COLOR_LIGHT_RED}\]\d \t \[${COLOR_GREEN}\] \[\$(parse_git_branch)\]\[${COLOR_LIGHT_PURPLE}\]\w \[${COLOR_YELLOW}\]>> \[${COLOR_NC}\]""

It looks the way I want, but sometimes I get strange behavior. If I scroll up to a long command I had previously entered and press Home, the cursor goes to the middle of the branch name. If I then try typing, some of the command disappears.
I changed parse_git_branch to
    parse_git_branch() {
    echo
    }

Then there was no problem with the column. I'm sure this isn't the right way to write in bash. I just wanted to keep the function but return an empty string.
","The behavior you're seeing is what you'd get if readline mis-counts the column positions.  The \[ ... \] sequences in PS1 tell readline that the characters output here are ""invisible"" (take up no screen columns), so you only want to use them when the characters are actually invisible.
Let's look at your PS1 setting, breaking it into parts:

PS1=""\[${COLOR_LIGHT_RED}\]


This is good and right, because (presumably) the ${COLOR_LIGHT_RED} variable expands to a color-changing sequence that takes no display columns.

\d \t \[${COLOR_GREEN}\] 


This, too, seems all good-and-right: \d and \t give the date and time, and do take up columns.  The spaces, including the one after the switch to green color, do as well.

\[\$(parse_git_branch)\]


Uh oh!  The parsed git branch is printed, and takes up columns, when there's a parsed branch.  This should be counted.

\[${COLOR_LIGHT_PURPLE}\]\w \[${COLOR_YELLOW}\]>> \[${COLOR_NC}\]""


The rest looks fine.
",prompt
how to test the prompt command,"my $x = prompt ""Are you OK? "";
say $x ~~ 'y' ?? ""Great!"" !! ""Bummer"";

use Test;

What tests can a put in a t/ file?  How can it respond to the prompt?
","I don't know how to do it directly(probably by capturing handles), so I leave that to others, but you can put your code in a script file and test it with Test::Run module:
# script.raku

my $x = prompt ""Are you OK? "";
say $x ~~ 'y' ?? ""Great!"" !! ""Bummer"";

# script.t

use Test::Run:auth<zef:CIAvash>:ver<0.2.*> :runs_ok;

runs_ok :args«$*EXECUTABLE script.raku», :in<y>, :out(*.contains: 'Are you OK? Great!');
runs_ok :args«$*EXECUTABLE script.raku», :in<n>, :out(*.contains: 'Are you OK? Bummer');

",prompt
python format float with three dots is possible,"I'm trying to format a float like
10 / 3 = Quotient: 3.3333...
42 / 10 = Quotient: 4.2
i want to add three dots when there are more than 4 decimal places
for the moment i have that format
print(""{:{width}}{:.5g}"".format(""Quotient:"", a / b, width=width))
result of this format :
10 / 3 = Quotient: 3.3333
42 / 10 = Quotient: 4.2
how can i do that with format() or another function in py standard lib ?
","You could create a helper function such as print_float to format the float and print it, using a ""reverse and find"" approach as suggested here:
def print_float(f: float) -> None:
    float_str = str(f)
    num_decimal_places = float_str[::-1].find('.')
    if num_decimal_places > 4:
        print(float_str[:-num_decimal_places + 4], '...', sep='')
    else:
        print(float_str)

Results:
print_float(12345)     # 12345
print_float(10 / 3)    # 3.3333...
print_float(42 / 10)   # 4.2
print_float(1.999999)  # 1.9999...

Finally, here's a (potentially) more efficient solution, which doesn't involve doing [::-1] to reverse the string in place. Note that it uses len() in place of a string reversal, which should be an O(1) operation in any case.
def print_float(f: float, max_decimal_places=4) -> None:
    float_str = str(f)
    len_float = len(float_str)
    idx_after_point = float_str.find('.') + 1

    if len_float - idx_after_point > max_decimal_places and idx_after_point:
        print(float_str[:idx_after_point + max_decimal_places] + '...')
    else:
        print(float_str)

",prompt
show remaining time left after user input,"I am trying to show the remaining time left after the user inputs their answer.
so it's suppose to be like this.
When does that course start? 2022-09-05 (user input)
Today it is 32 days left until the course starts
I dont think its suppose to be that complicated but I cant make it work, I keep getting NaN or that it just isnt working.
I have checked MDN but I just dont get it.
The code looks like this.
    function start(timePassedIn) {
      return `Today it is ${timePassedIn} days left until the 
      course starts`;
    }

    const course = prompt(""When does that course start? "");
    const starting = start(course);

  
    console.log(starting);

I removed all my attempts at the date so that you can give me fresh input.
Appreciate all the help I can get.
","Can you try this.
function start(timePassedIn) {
      return `Today it is ${timePassedIn} days left until the 
      course starts`;
 }
 function getDateDifference(inputDate) {
    const date1 = new Date(inputDate);
    const date2 = new Date();
    const diffTime = Math.abs(date1 - date2);
    const diffDays = Math.ceil(diffTime / (1000 * 60 * 60 * 24)); 
  return diffDays;
 }
 const course = prompt(""When does that course start? "");
 const starting = start(getDateDifference(course));

",prompt
in nodejs how can i get a password from input without the user39s keypresses being echoed to the console,"I'm trying to add a simple setup prompt to my program, where it prompts for some basic details before starting the app for the first time. One of these inputs I need is a password. When I read this password, I want to ensure that (much like when you enter a password for sudo) it doesn't appear in the user's terminal as they type.
How can I do so within NodeJS? I'm looking for something I can drop into the following code.
console.log('Failed to find config file');
// This echoes to the console - how can I do the same thing without the echo?
const password = prompt('Enter a password: ');

I'm happy to install a dependency from NPM if required.
","You can use the prompt-sync library to prompt for passwords.
import createPrompt from 'prompt-sync';
const prompt = createPrompt({});

// The prompt object's hide method gives a prompt where the input 
// is hidden
const password = prompt.hide('Enter a password: ');

",prompt
javascript prompt  cancel button to terminate the function,"I'm calling a Javascript window.prompt() and prompting the user to submit a variable which I compare to another variable (a very basic password protection). The function works great, however, if you click ""cancel"" on the prompt() window, the function does not simply terminate, but rather compares the variable to an empty string, (which the user opted not to submit by pressing ""cancel"" instead) resulting in the function continuing to the else{ } portion.
My question is, how do I terminate the function upon pressing cancel? I just need to know how to target the cancel button. 
Usually I would just call a .stop() on the click() of a button, but I don't know how to target the prompt-window's cancel button.
","prompt returns a string if the user presses OK ('' being with no value submitted). If the user pressed Cancel, null is returned. All you need to do is check whether the value is null:
function doSomething() {
    var input;
    input = prompt('Do something?');
    if (input === null) {
        return; //break out of the function early
    }
    switch (input) {
    case 'fun':
        doFun();
        break;
    case 'boring':
        beBoring();
        break;
    }
}

",prompt
how to handle a windows prompt in a test automation using cypress,"I am new to using Cypress for web automation. I am still scouring through the internet looking for answers to this but I cannot find a solution that works for me.
This is what I'm trying to do in my test:

User clicks a link. 
A new tab is opened and a windows prompt appears, requesting user input (username, password). (Since Cypress doesn't allow opening new tabs, I've removed the target attribute.)
Upon logging in successfully, the page has a download button.
User clicks on the download button.


The first struggle - I could not enter values into the windows prompt. In the below code, I was trying to see if the 'Sign In' button on the windows prompt would be clicked, but it was not.
cy.window().then(win => {
    cy.get('@documentPassword').then((finalPassword) => {
        const stub =cy.stub(win, 'prompt')
        stub.returns('test')
        cy.get('button#signin').click()
    })
})

I got an Assertion Error: Timed out retrying after 25000ms: Expected to find element: button#signin, but never found it. 
After no luck with this, I moved on to another suggestion.

The second struggle - I tried putting the username and password into the link, like this: https://username:password@mytestingwebsite.com. Just to note, when I paste the link manually into a browser, it works. To test this out, this what I had done:
cy.visit('https://mailtrap.io')
// ...other steps
cy.forceVisit('https://username:password@mytestingwebsite.com')

I added a custom command forceVisit to the commands.js file:
Cypress.Commands.add('forceVisit', url => {
    cy.window().then(win => {
        return win.open(url, '_self'); 
      });
});

The result is the second url does not load. 
Hoping for any insight from you guys. Thanks in advance.
","This works for me:
cy.visit('https://mytestingwebsite.com', {
  auth: {
   username: 'username',
   password: 'password'
  }
})

This didn't work for me the first time I tried it because I still passed the credentials in the url.
",prompt
how to get information form a prompt and print it to the screen in genexus,"I have this web panel right here  and this prompt pop up after clicking the arrow, how can I get the information from the prompt and print to the screen. Thank you
","not sure what you want to accomplish. Based on the printscreen you are using the standard automatically generated Prompt object on a Foreign Key. The popup will select the value and return and assign on the caller the selection. Then, based on ajax when transitioning between fields will get secondary attributes values based on that selection (if any).
You could use a custom prompt with a rule and change the out parameters to receive more information on the caller. Other option would be to use other events on that field (ControlValueChanged/IsValid?) and add some coding.
hope it helps
",prompt
is there a better way to write return statement in javascript,"I'm creating a sign up page using prompt(), i want to return from the page when a user cancels. i tried using return statement to achieve this but it gave an error _ illegal return statement. I wrote the same code without a constructor or the idea of OOP and I really want to get same result using constructor or the idea of OOP if possible here is my code
let getuserName = prompt(""enter your user Name"")

while (validateUserName(getuserName) == false) {
    getuserName = prompt(""invalide user, user name most be less than 10"")

}

if (getuserName == null) {
    return
}* //this is showing illegal return*

let getpassword = prompt(""enetr your password"")

while (validatePassword(getpassword) == false) {
    getpassword = prompt(""password most be 10 0r more"")

}

//confirm password

let getconfirmPassword = prompt(""please confirm your password"")

while (getconfirmPassword !== getpassword) {
    getconfirmPassword = prompt(""invalid password, please enter a valid password"")

}


class user {

    constructor(userName, firstName, lastName, email, accountValidated, password, confirmPassword) {
        this.userName = userName || ""Anonymous"";
        this.firstName = firstName || ""Anonymous"";
        this.lastName = lastName || ""N/A"";
        this.email = email || "" ""
        this.accountValidated = accountValidated || null;
        this.password = password || null;
        this.confirmPassword = confirmPassword || null;
    }
    getuserName() {
        return this.userName;
    }

    getfullName() {
        return `${this.firstName} ${this.lastName}`;
    }

    getemail() {
        return this.email;
    }

    getvalidation() {
        return this.accountValidated;
    }

    getpassword() {
        return this.password;
    }

    getconfirmPassword() {
        return this.confirmPassword
    }

}

const user1 = new user(`${getuserName}`, ""josephine"", ""nnalue"", ""josephinennalue@gmail.com"", true, ""josephine123"")
const user2 = new user(""Amara123"", ""Amarachi"", ""Simon"", ""amarachisimon@gmail.com"", true, ""amarachi123123"")
const user3 = new user(""chi123"", ""Chika"", ""Okoye"", ""chikaokoye@gmail.com"", false, ""chika123"")
const user4 = new user(""ble123"", ""Blessing"", ""Jimmy"", ""blessingjimmy@gmail.com"", true, ""blessing123"")
const user5 = new user(""seunbaby"", ""Seun"", ""Kareem"", ""seunkareem@gmail.com"", false, ""seun123"")




console.log(user1.getuserName())

//validating user name

function validateUserName() {
    if (getuserName == null) {
        return true
    }

    if (getuserName.length > 10) {
        return false
    } else {
        return true
    }
}


console.log(validateUserName())


//validating password

function validatePassword() {
    if (getpassword.length < 10) {
        return false
    } else {
        return true
    }
}

console.log(validatePassword())

","I've made some adjustments to your code to make it work and also simplified some of it. I've tried to reuse your code and logic as much as possible.


function showPrompt() {
  function validateUserName(userName) {
    return userName.length < 10;
  }

  function validatePassword(password) {
    return password.length > 10;
  }

  let getuserName = prompt(""enter your user Name"")

  while (!validateUserName(getuserName)) {
    getuserName = prompt(""invalide user, user name most be less than 10"")
  }

  let getpassword = prompt(""enter your password"")

  while (!validatePassword(getpassword)) {
    getpassword = prompt(""password most be 10 or more"")
  }

  let getconfirmPassword = prompt(""please confirm your password"")

  while (getconfirmPassword !== getpassword) {
    getconfirmPassword = prompt(""invalid password, please enter a valid password"")
  }

  return {
    username: getuserName,
    password: getpassword
  }
}

const inputs = showPrompt();

class user {

  constructor(userName, firstName, lastName, email, accountValidated, password, confirmPassword) {
    this.userName = userName || ""Anonymous"";
    this.firstName = firstName || ""Anonymous"";
    this.lastName = lastName || ""N/A"";
    this.email = email || "" ""
    this.accountValidated = accountValidated || null;
    this.password = password || null;
    this.confirmPassword = confirmPassword || null;
  }
  getuserName() {
    return this.userName;
  }

  getfullName() {
    return `${this.firstName} ${this.lastName}`;
  }

  getemail() {
    return this.email;
  }

  getvalidation() {
    return this.accountValidated;
  }

  getpassword() {
    return this.password;
  }

  getconfirmPassword() {
    return this.confirmPassword
  }

}

const user1 = new user(`${inputs.username}`, ""josephine"", ""nnalue"", ""josephinennalue@gmail.com"", true, ""josephine123"")
const user2 = new user(""Amara123"", ""Amarachi"", ""Simon"", ""amarachisimon@gmail.com"", true, ""amarachi123123"")
const user3 = new user(""chi123"", ""Chika"", ""Okoye"", ""chikaokoye@gmail.com"", false, ""chika123"")
const user4 = new user(""ble123"", ""Blessing"", ""Jimmy"", ""blessingjimmy@gmail.com"", true, ""blessing123"")
const user5 = new user(""seunbaby"", ""Seun"", ""Kareem"", ""seunkareem@gmail.com"", false, ""seun123"")

console.log(user1)



",prompt
is there a way to pause a running script with processstdin for user input in nodejs,"I was hoping to see if Node.js has the ability to pause a script once it hits a process.stdin block to take in user input for command-line applications, similar to how prompt() works with JS in the browser or how gets works in Ruby.
  var response = '';

  console.log(""What's your favorite color?"");

  process.stdin.resume();
  process.stdin.setEncoding('utf8');

  process.stdin.on('data', function (text) {
    response = text;
    process.exit();
  });

  console.log(`Cool! So ${response} is your favorite color? Mine is black.`);

In the simple script above, I would hope to see
What's your favorite color?
*pause for user input ===>* Red
Cool! So Red is your favorite color? Mine is black.

with the pause halting the script until I type something and hit enter for it to continue running. Instead, I see
What's your favorite color?
Cool! So  is your favorite color? Mine is black.

immediately printed out and THEN accepts user input from me.
I am aware there are other modules like readline and prompt that simplify taking in user input for node. For this question, I'm particularly looking to see if Node.js offers this functionality without having to install extra packages(process.stdin is the first thing that comes to mind), although it would be helpful to know if any modules provide that kind of functionality as well.
","You can use readline-sync.
var rl = require('readline-sync');
var response = '';

response = rl.question(""What's your favorite color?"");

console.log(`Cool! So ${response} is your favorite color? Mine is black.`);

",prompt
call 39npm init39 via execsync in nodejs application and automatically enter required console input,"I'm writing a Node application that automatically spins up new templated Node applications (with various configurations), including executing the 'npm init' step, but this is failing because npm init requires multiple follow-up inputs from the user.
This is what I'm executing so far, and it never completes:
execSync('cd ' + targetFolder + '; npm init;');

Is it possible for me to provide an array of Strings (empty or otherwise) to sequentially be input automatically when prompted?
","Thanks @jonrsharpe. Just executing 'npm init --yes' achieves what I wanted.
",prompt
how can i get my prompt to call my function and give it an argument,"I'm doing a challenge that had me make a days/weeks/month calculator until you're 90.
I finished it but I can only call the function by calling it in the console like this lifeInWeeks(25); I used 25 since that's my age.
But I want to know how can I do this and have a prompt pop up and ask what their age is and then give an alert with the answer, but for this to happen the prompt would have to call the function and give it input from the answer i.e. the prompt should call the function and the answer of that prompt should be the function's argument.
Something like this:

""What is your age?"" (you answer) 25

Then an alert pops up and says:

""You have 23725 days, 3380 weeks, and 780 months left.""

WITHOUT having to manually call the function in the console like this: lifeInWeeks(25);
Here's my code:
function lifeInWeeks(age) {
    var days = (90 - age) * 365;
    var weeks = (90 - age) * 52;
    var months = (90 - age) * 12;
    alert(""You have "" + days + "" days, "" + weeks + "" weeks, and "" + months + "" months left."");

}

lifeInWeeks(25);

I've tried putting the prompt inside the function but that hasn't worked, any ideas?
","You can simply put prompt user for entering age, and then use then value from user input


let life = prompt(""What is your age ?"")
lifeInWeeks(+life);

function lifeInWeeks(age) {
    var days = (90 - age) * 365;
    var weeks = (90 - age) * 52;
    var months = (90 - age) * 12;
    alert(""You have "" + days + "" days, "" + weeks + "" weeks, and "" + months + "" months left."");

}



You can also move this inside function, in that case you need to invoke the function without any parameter
",prompt
preprovided parameters to command with required prompt in shell,"So I have to make deployment of AWS Elastic Beanstalk application with AWSEB CLI on Jenkins. When i try to use command
eb init

It requires some information and credentials. Credentials are stored as parameters or could be a secret file on Jenkins instance. Command have no such things like --parameter to provide it at start. Is there any solution to provide all parameters in the code that in runtime this command will now okay now this is provided now this and so on? Something like this:
eb init --username XXX --password XXX --others XXX

Here is documentation for that command
https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/eb-cli3-configuration.html
","Will this answer help your issue? It seems you can set some of the parameters as Environment variables and rest as flags.. For example.
$ export AWS_ACCESS_KEY_ID=""xxx""
$ export AWS_SECRET_ACCESS_KEY=""xxx""
$ export AWS_DEFAULT_REGION=""xxx""

Then
eb init --region eu-west-1 --platform <platform-name> appname

",prompt
is there a way to have a multi line user input prompt in batch,"I've been trying to make a user input prompt that displays itself as multiple lines.
Example of how I want it to look:
Welcome to the program!
What would you like to do?
option [1]
option [2]
option [3]

This is how I tried to do that:
set /p selection=""Welcome to the program! ^ What would you like to do? ^ option [1] ^ option [2] ^ option [3]""

But when I run that, it comes out like this:
Welcome to the program!  What would you like to do?  option [1]  option [2]  option [3]

I googled it but I couldn't find anything that helped, so if someone could tell me a way to do this that would be great!
","Maybe it's simple.
@echo off
echo welcome to program!
echo what would you like to do?
echo option[1]
echo option[2]
echo option[3]

set /p selection=

",prompt
timing command execution from bashrc,"I'm trying to create my own minimal BASH shell prompt that times execution of each command entered. So far I have a functional prompt that looks appealing for my desires; I'm just missing the time output, like starship does. I've tried reading user input from $@, $*, /dev/stdin while using time without success. I also tried using date at the start of the script and end while subtracting the difference to a similar affect. No matter what I've tried the execution timing is always off or doesn't work at all. I've checked documentation and several sites for suggestions but none worked
The closest result I got was using this snippet below
function timer_start {
  timer=${timer:-$SECONDS}
}

function timer_stop {
  timer_show=$(($SECONDS - $timer))
  unset timer
}

trap 'timer_start' DEBUG
PROMPT_COMMAND=timer_stop

PS1=""$ ${timer_show}s ""

","With suggestions from @KamilCuk - I was able to get a functioning timer working from .bashrc! Currently it gets idle/typing + command execution time (which I think I prefer anyway)
prmpt() {
    timeStart=$(date +%s)

    sec=$(((timeStart-timeEnd)%60))
    min=$(((timeStart-timeEnd)%3600/60))
    hr=$(((timeStart-timeEnd)/3600))
    
    timeEnd=$timeStart

    printf '%sh:%sm:%ss' ""$hr"" ""$min"" ""$sec""
}
PROMPT_COMMAND=prmpt
timeEnd=$(date +%s)

",prompt
command to run a bat file,"I'm trying to make my Visual Studio build script execute a .bat file that does something important.
Here is what I'm want to do right now:
cd ""F:\- Big Packets -\kitterengine\Common\"" Template.bat

But it doesn't work.
I have to do this to make it work:
cd ""F:\- Big Packets -\kitterengine\Common\""
F:
Template.bat

But this is pretty difficult to add to the Visual Studio script.
How can I do this in one single line?
","Can refer to here: https://ss64.com/nt/start.html
start """" /D F:\- Big Packets -\kitterengine\Common\ /W Template.bat

",prompt
how do i stop the second prompt from appearing until the first ifelse condition is satisfied start page for a game,"If the user name is not entered, the age prompt still comes up, I want the whole prompt to terminate if the user name is not entered, basically, when it says ""You cannot proceed without the name"" after that the prompt asking for the age still appears, I don't want that to happen.
Here is my code:
</style>

<body>
<h1>Welcome to </h1>
<h2> Hangman </h2>
<div class=""wrapper"">
  <button onclick=""myFunction()"">Let's start</button>
</div>

<script>
  function myFunction() {
  let person = prompt(""Please enter your name"", ""Marium"");
  if (person == null || person == """") {
  alert(""You cannot proceed without entering the name"");
  } 
  else {
  alert(""Hello "" + person);
  }
   
   let age= prompt(""Please enter your age""); 
  if (age<16) { 
   alert(""User below 16 cannot proceed"");
   }
    else {
   window.location.href= ""sample.html"";
   }
}

","You just need to insert a return before the alert in the if statement relative to the missing username case:


function myFunction() {
  let person = prompt(""Please enter your name"", ""Marium"");
  if (person == null || person == """") {
    return alert(""You cannot proceed without entering the name"");
  } else {
    alert(""Hello "" + person);
  }

  let age = prompt(""Please enter your age"");
  if (age < 16) {
    alert(""User below 16 cannot proceed"");
  } else {
    window.location.href = ""sample.html"";
  }
}
<h1>Welcome to </h1>
<h2> Hangman </h2>
<div class=""wrapper"">
  <button onclick=""myFunction()"">Let's start</button>
</div>



",prompt
google apps script  conditional user input  number prompt,"I need to get user input which should be a positive whole number (Integer). I figured out how to keep displaying prompt if the input is not a number, but I cant figure out how to write condition for number value. 
Here is the code.
var ui = SpreadsheetApp.getUi(); // Same variations.

  do {
  var result = ui.prompt(
      'Please fill in',
      'Please enter number of people involved',
      ui.ButtonSet.OK_CANCEL);
  var text = parseInt(result.getResponseText());
  var button = result.getSelectedButton();
  }
  while (!(button == ui.Button.CANCEL) && isNaN(parseInt(text)) && !(button == ui.Button.CLOSE));

  var button = result.getSelectedButton();
  if (button == ui.Button.OK) {
    ui.alert('Number of people wanted: ' + text + '.');
  } else if (button == ui.Button.CANCEL) {
    ui.alert('Action cancelled.');
    return;
  } else if (button == ui.Button.CLOSE) {
    ui.alert('Action cancelled.');
    return;
  }

I tried, in my opinion, all the possible combinations of <,>,!, &&, ||, but nothing worked..
","
You want to exit the while loop only when the value is the positive integer.

If my understanding is correct, how about this modification? I think that there are several answers for your situation. So please think of this as just one of them.
Modified script:

From:

while (!(button == ui.Button.CANCEL) && isNaN(parseInt(text)) && !(button == ui.Button.CLOSE));


To:

while (!(button == ui.Button.CANCEL) && !(/^[0-9]+$/.test(text)) && !(button == ui.Button.CLOSE));

Reference:

test()

If I misunderstood your question, please tell me. I would like to modify it.
Edit:
In your current modified script, for example, if the values like 123-456 and 123abc are inputted, 123 is retrieved. This is due to var text = parseInt(result.getResponseText()).
If you don't want to do this, please modify var text = parseInt(result.getResponseText()) to var text = result.getResponseText(). By this, when such values are inputted, it doesn't exit the while loop.
",prompt
recalling data user has input and showing if it is a number,"This is a two part question, I currently have the below code and I'm trying to have it so that the console will tell me if it is a lower case letter or an upper case letter or of it is a number. I cant seem to get the number.isInteger to work so please tell me where I am going wrong with that.
Also I would like to have it so that there is a call back of what the user entered. So instead of it just saying ""This is a upper case letter"" I would like it to state ""The letter g you entered is lowercase"" and vice versa for upperCase and numbers.
Hope that makes sense, please find below my current code. I am new to coding and javascript so please try dumb it down as much as possible for me. Thanks!
Please see below code I currently have:
    let upperLower = prompt(""please enter either a uppercase letter, lowercase letter or a number"");

if (upperLower == upperLower.toLowerCase()) {

  console.log(""The character is lowercase"");
}
else if (upperLower == upperLower.toUpperCase()) {
    
    console.log(""The character is uppercase"");
}
else if (upperLower == Number.isInteger()){

    console.log(""This is a number"");
}

","Or you can check if input converted to number is not a number (isNaN) is false


let upperLower = prompt(""please enter either a uppercase letter, lowercase letter or a number"");
if (!isNaN(parseInt(upperLower))){

    console.log(upperLower + "" is a number"");
}
else if (upperLower == upperLower.toLowerCase()) {

  console.log(upperLower + "" character is lowercase"");
}
else if (upperLower == upperLower.toUpperCase()) {
    
    console.log(upperLower + "" character is uppercase"");
}



",prompt
zsh prompt replace named directory with it39s alias,"As I understand, if you create an alias for a directory in your .zshrc file, that directory is now considered a ""named directory"". I could be mistaken there, so let me know.
Based on the documentation here, it sounds like if you use ""%~"" in your zsh prompt and the current working directory is a named directory, then the prompt should show the alias for the directory, rather than the full path (i.e. ~{alias}). This does not seem to be working for me currently and I'm having a hard time finding examples or posts regarding this.
In my .zshrc file I have an alias like repo = ""~/src/apps/repo"". My hope is that when I am in that directory, the zsh prompt would show ~repo, but it always shows the full thing.
Any help is greatly appreciated!
","In zsh, you can add a named directory with hash -d repo=~/src/apps/repo (not an alias). Also, ~ isn't expanded inside quotes, so you should remove them or use $HOME instead.

hash usage: https://zsh.sourceforge.io/Doc/Release/Shell-Builtin-Commands.html#index-hash-1
Static named directories: https://zsh.sourceforge.io/Doc/Release/Expansion.html#Static-named-directories

",prompt
why does subtracting decimals not work in javascript,"If I do the code below and enter a decimal for one or both of the numbers, lets says I use 0.5 and 0.3, I should get 0.2 but I get 0 only. This makes no sense at all to me, it is probably a problem with using prompt but I need to use prompt or a method that is similar to prompt(I'm using sweetalert2 input for the alert). I am okay with using any js libraries.


const x = parseInt(prompt('1'))
const y = parseInt(prompt('2'))
alert(x-y)



I know it is a weird problem, but I don't know how to fix it.
","You need to use parseFloat, not parseInt. parseInt is whole numbers only, while parseFloat allows decimal places.
parseFloat('0.9') === 0.9
parseInt('0.9') === 0

",prompt
how to override a fish_prompt function from a omf theme,"I lost some time on it so I'm creating this Q&A.
I'm using an omf theme and I would like to make a change in the theme prompt. Unfortunately, the change I want to do is not possible by setting the theme config variables.
I tried to edit fish_prompt function using funced fish_prompt; funcsave fish_prompt but if I do so the theme is not loaded anymore so I can't use the functions defined within the theme. The same happens if I just create a fish_prompt function in my config.fish.
","tldr
add these lines before you define your own fish_prompt function
source $OMF_PATH/init.fish

# Read current theme
test -f $OMF_CONFIG/theme
  and read -l theme < $OMF_CONFIG/theme
  or set -l theme default

set -l theme_functions_path {$OMF_CONFIG,$OMF_PATH}/themes*/$theme/functions/
for conf in $theme_functions_path/*.fish
  source $conf
end

explanation
When omf is loaded, it add the theme functions files to the $fish_function_path. (source code)
According to fish documentation

When fish needs to load a function, it searches through any directories in the list variable $fish_function_path for a file with a name consisting of the name of the function plus the suffix .fish and loads the first it finds.

What the documentation doesn't explicitly say is if the function is already defined (in config.fish, for example) it will not try to load from $fish_function_path.
So the issue is, when you creates your own fish_prompt function, it shadows the .../<theme>/functions/fish_prompt.fish.
What you need to do to work around it is force loading the theme function file before you redefine it.
For example:
# Read current theme
test -f $OMF_CONFIG/theme
  and read -l theme < $OMF_CONFIG/theme
  or set -l theme default

set -l theme_functions_path {$OMF_CONFIG,$OMF_PATH}/themes*/$theme/functions/fish_prompt.fish
for conf in $theme_functions_path
  source $conf
end

function fish_prompt
  # prompt_theme_foo
  # prompt_theme_bar
end

Be sure to run it after omf init.fish is loaded.
You can assure by sourcing omf manually source $OMF_PATH/init.fish or be sure omf.fish is in an alphabetical order lesser than your file.
",prompt
switch statement it does not work with prompt,"I just learned switch statements. I was practicing it by building something. When i set the value of variable to a number it works but when i asks the user for a number it always outputs the default statement.
It works with this code:
confirm(""You want to learn basic counting?"");
var i = 0;
switch (i) {
    case 0:
        console.log(i);
        i++
    case 1:
        console.log(i);
        i++;
    case 2:
        console.log(i);
        i++;
    case 3:
        console.log(i);
        i++;
    case 4:
        console.log(i);
        i++;
    case 5:
        console.log(i);
        i++;
    case 6:
        console.log(i);
        i++;
    case 7:
        console.log(i);
        i++;
    case 8:
        console.log(i);
        i++;
    case 9:
        console.log(i);
        i++;
    case 10:
        console.log(i);
        console.log(""Congratulations!"");
        break;
    default:
        console.log(""Buzz, wronghh"");
        break;
}

But when i asks value from user, it dosen't work. Code below doesn't work:
confirm(""You want to learn basic counting?"");
var i = prompt(""Type any number from where you want to start counting[Between 0 and 10]"");
switch (i) {
    case 0:
        console.log(i);
        i++
    case 1:
        console.log(i);
        i++;
    case 2:
        console.log(i);
        i++;
    case 3:
        console.log(i);
        i++;
    case 4:
        console.log(i);
        i++;
    case 5:
        console.log(i);
        i++;
    case 6:
        console.log(i);
        i++;
    case 7:
        console.log(i);
        i++;
    case 8:
        console.log(i);
        i++;
    case 9:
        console.log(i);
        i++;
    case 10:
        console.log(i);
        console.log(""Congratulations!"");
        break;
    default:
        console.log(""Buzz, wronghh"");
        break;
}

","You need to convert the user input from a string to an integer, like so
confirm(""You want to learn basic counting?"");
var i = prompt(""Type any number from where you want to start counting[Between 0 and 10]"");
i = parseInt(i); // this makes it an integer
switch(i) {
//...

",prompt
prompt command that works in bash pipe,"I'm looking for a prompt command that will allow you to use readline bindings in the middle of a bash pipe, like so:
$ prompt -i 'prefill' -p 'prompt> ' | cat

And I get to edit with readline bindings with text prefilled like this:
prompt> prefill|

And then the output of whatever you typed is sent to cat.
I have written a python program that does almost all of this:
#!/usr/bin/env python3

import readline
import sys

def input_with_prefill(prompt, text):
    def hook():
        readline.insert_text(text)
        readline.redisplay()
    readline.set_pre_input_hook(hook)
    result = input(prompt)
    readline.set_pre_input_hook()
    return result

def arg(flag):
    arg = """"
    if flag in sys.argv:
        tuple_matches_flag = lambda x: x[1] == flag
        # select first instance of ""<flag>"", 
        arg_ind = next(filter(tuple_matches_flag, enumerate(sys.argv)))[0]
        # add one to the index for the value of the arg
        arg = sys.argv[arg_ind + 1]
    return arg

def main():
    prompt = arg(""-p"")
    prefill = arg(""-i"")
    response = input_with_prefill(prompt, prefill)
    print(response)

main()

This works great for when it's used on its own. prompt -i 'prefill' -p 'prompt> ' works perfectly how I want it to.
However, when I put it in a pipe like this prompt -i 'prefill' -p 'prompt> ' | cat it won't work. I understand that is because the prompt is sending it's stdout to the FIFO pipe to cat, so the readline library can't work correctly because it usually interfaces with stdout. So I thought that the solution would be to redirect the file desciptors like this:
prompt -i 'prefill' -p 'p> ' 3>&1 1>&2 2>&3 3>&- | cat

This will create a 3rd file descriptor, point it to where stdout used to be (the FIFO pipe to cat), points stdout to stderr (terminal), points stderr to the FIFO pipe, and closes the 3rd file descriptor. So, at the end of this we have prompt's stderr going to the FIFO, and prompt's stdout going to the console. If we tweak the python program's last line to print(response, file=sys.stderr), then this should work. But, it doesn't. There are 2 prompts printed, and no prefill. What is wrong?
","This seems to be an issue with python's readline implementation:
Python module ""readline"" can't handle output redirection
https://bugs.python.org/issue24829
I wrote this C program which does what I want. You can compile it on mac with clang file.c -L/opt/homebrew/opt/readline/lib -lreadline -o prompt
#include <string.h>
#include <unistd.h>
#include <readline/readline.h>

char *prefill = """";

int startup_hook() {
    if (prefill) {
        return rl_insert_text(prefill);
    } 
    return 0;
}

// get value for flag
char *arg(int argc, char **argv, char *flag) {
    for (int i = 0; i < argc; i++) {
        if (!strcmp(argv[i], flag)) {
            if (i < argc) {
                return argv[i+1];
            }
        }
    }
    return """";
}

int main(int argc, char** argv)
{
    // prompt and prefill from commandline
    char *prompt = arg(argc, argv, ""-p"");
    prefill = arg(argc, argv, ""-i"");

    // cast because functions hooks are supposed to take (char *, int)
    rl_startup_hook = (Function *)startup_hook;

    // use stderr if stdout redirected
    if(!isatty(STDIN_FILENO)) {
        rl_outstream = stderr;
    }

    char *line = readline(prompt);
    puts(line);
    return 0;
}

",prompt
how do i create objects with a prompt,"The question says it all. Is it even possible for the user to create new objects by responding to a prompt? And if it is possible, then could I also add properties to those objects? (Instructions on this, too, please. 
The reason why I'm asking this is because I'm currently doing a school project involving a small database showing our understanding of objects, arrays, and such. I'm not very experienced in Javascript, so a somewhat straightforward answer would be great.
I've noticed a lot of the responses from people on this site include parts of Javascript I don't understand / haven't learned yet.
Please let me know if I can clarify anything.
","You do it like this:
var answer = prompt('...')
# now create your object however you want
var obj = new MyObject(answer)

Prompt itself can't create any objects, of course. It has only one job and does it well.
",prompt
trying and failing to create a purely javascript calculator,"First post ever, after lurking for some weeks. I'm currently attending a full-stack bootcamp and recently we got into Javascript (so I'm extremely green, please be patient...I'm trying to reskill myself in another industry). One of the HLTs that is giving me a particular headache is as follows:

You are tasked with creating re-usable methods that can be used throughout the business. The business needs you to create methods for mathematics operations. Follow the instructions below:

Ask the user for a number with a prompt() and store the value in a variable called firstValue
Ask the user for a second number with a prompt()and store that value in a variable called secondValue
Ask the user for a third input with prompt()storing the value in a variable called operation. >Expected operations are:
a.+This is the addition symbol, located next to the backspace key(hold shift)
b.–This is the subtraction symbol, located next to number 0key (hold shift)
c./This is the division symbol, a forward slash, located next to the full stop key
d.*This is the multiplication symbol, a star, accessed by holding shift and pressing number 8
e.^This is the to-the-power-of symbol, known as a caretin programming, accessed by holding shift and pressing the number 6
Write a method for the 5 operations listed above (one for each operation) that takes in both valuesand returns the result of the operation.a.For examplefunction multiplication(firstValue, secondValue) { return firstValue * secondValue;}
Create a case-switch for evaluating the operation the user supplied, and depending on the value, execute the relevant function.
Print out to consolefirstValue, operator, secondValue, an equal sign, and the answer:
a.2 x 8 = 16
STRETCH CHALLENGE: Wrap the code in a continuous loop that only ends when the user responds to a prompt that asks them “would you like to do another calculation?”with their answer being “no”.
STRETCH CHALLENGE: Change the above code to also include methods for processing sin, cos, and tan. You can use the methodsMath.sin(x), Math.cos(x), Math.tan(x)but be aware thatthe user only needs to supply a single value and the operation they wish to dowhen needing sin, cos, and tan!


I'm stuck even before attempting the stretch challenges (which I have no clue on how to do, but that's a problem for later) and looking online I couldn't find anything helpful (since most calculators employ HTML and CSS as well). Here below my two attempts at making the code work (I made multiple variations of both, trying to find a version that worked, but without any luck). I used some Shakespearean English, just to spice it up and to make it less boring. Also, it's called ""Calculathor"".
First attempt:


//Contemporary English to Shakespearean English translator found at https://lingojam.com/EnglishtoShakespearean
var firstValue = parseFloat(prompt(""Writeth h're thy first numb'r, m'rtal""));//I used parseFloat as I believe it would filter out some typing mistakes (by giving NaN if what's typed is not a number)
var secondValue = parseFloat(prompt(""And h're, prithee writeth thy second numb'r""));
var operator = prompt(""Writeth one of these ancient runes: + - / * ^""); //I changed the subtraction symbol from the assignment to the one I have on my Italian keyboard, which is the same to an hyphen
function operation(firstValue, secondValue){
    switch (operator) {
        case ('+'):
            return firstValue + secondValue;
            break;
        case ('-'):
            return firstValue - secondValue;
            break;
        case ('/'):
            return firstValue / secondValue;
            break;
        case ('*'):
            return firstValue * secondValue;
            break;
        case ('^'):
            return firstValue ^ secondValue;
            break;    
        default:
            alert(""Thee wroteth something inc'rrect, thee clotpole!"");
            break;
    }
}
console.log(`Thee hath asked Thor to solveth ${firstValue} ${operator} ${secondValue} and the solution appears to beest equat'd to ${operation}`);



Second attempt:


//Contemporary English to Shakespearean English translator found at https://lingojam.com/EnglishtoShakespearean
var firstValue = parseFloat(prompt(""Writeth h're thy first numb'r, m'rtal""));//I used parseFloat as I believe it would filter out some typing mistakes (by giving NaN if what's typed is not a number)
var secondValue = parseFloat(prompt(""And h're, prithee writeth thy second numb'r""));
var operator = prompt(""Writeth one of these ancient runes: + - / * ^""); //I changed the subtraction symbol from the assignment to the one I have on my Italian keyboard, which is the same to an hyphen
let result = (`${firstValue} ${operation} ${secondValue}`);
function operation(firstValue, secondValue, operator){
    switch (operator) {
        case ('+'):
            return result (firstValue + secondValue);
        case ('-'):
            return result (firstValue - secondValue);
        case ('/'):
            return result (firstValue / secondValue);
        case ('*'):
            return result (firstValue * secondValue);
        case ('^'):
            return result (firstValue ^ secondValue);
        default:
            alert(""Thee wroteth something inc'rrect, thee clotpole!"");
            break;
    }
}
console.log(`Thee hath asked Thor to solveth ${firstValue} ${operator} ${secondValue} and the solution appears to beest equat'd to ${result}`);



I know this must be something very stupid for most of you, but for me it's still pretty hard to try and understand what I'm doing wrong, without any guidance. Please help me, if you can! I've wasted already more than 2 days trying to understand what I'm getting wrong. :(
","The OP's code only mentioned the operation function, failing to invoke it. This modification (and not-at-all-time-wasting explanation) invokes operation inside the interpolated string...
operation(firstValue, operator, secondValue)

The complete code:
var firstValue = prompt(""Writeth h're thy first numb'r, m'rtal"");
firstValue = parseFloat(firstValue)
var secondValue = prompt(""And h're, prithee writeth thy second numb'r"");
secondValue = parseFloat(secondValue)

var operator = prompt(""Writeth one of these ancient runes: + - / * ^"");

function operation(firstValue, operator, secondValue){
let res;
switch (operator) {
    case ('+'):
        res=  firstValue + secondValue;
        break;
    case ('-'):
        res= firstValue - secondValue;
        break;
    case ('/'):
        res= firstValue / secondValue;
        break;
    case ('*'):
        res= firstValue * secondValue;
        break;
    case ('^'):
        res= firstValue ^ secondValue;
        break;    
    default:
        alert(""Thee wroteth something inc'rrect, thee clotpole!"");
        break;
}

return res;
}

console.log(`Thee hath asked Thor to solveth ${firstValue} ${operator} ${secondValue} and the solution appears to beest equat'd to ${operation(firstValue, operator, secondValue)}`);`

",prompt
how to not display duplicate modules on ghci prompt,"Currently this is how my ghci prompt looks like:

and I want to make it so that my prompt doesn't display duplicate modules as shown below:

but I can't really figure out how. My configuration(ghci.conf) file's contents is as shown below:
:set +m

import qualified IPPrint
import qualified Language.Haskell.HsColour as HsColour
import qualified Language.Haskell.HsColour.Colourise as HsColour
import qualified Language.Haskell.HsColour.Output as HsColour

let myColourPrefs = HsColour.defaultColourPrefs { HsColour.conid = [HsColour.Foreground HsColour.Yellow, HsColour.Bold], HsColour.conop = [HsColour.Foreground HsColour.Yellow], HsColour.string = [HsColour.Foreground HsColour.Green], HsColour.char = [HsColour.Foreground HsColour.Cyan], HsColour.number = [HsColour.Foreground HsColour.Red, HsColour.Bold], HsColour.layout = [HsColour.Foreground HsColour.White], HsColour.keyglyph = [HsColour.Foreground HsColour.White] }

let myPrint = putStrLn . HsColour.hscolour (HsColour.TTYg HsColour.XTerm256Compatible) myColourPrefs False False """" False . IPPrint.pshow

:set -interactive-print=myPrint

:{
prompter :: [String] -> Int -> IO String
prompter modules line = return $
    concat [ ""\ESC[33m\STX[Module(s): ""
           -- this is the only line that changed
           , Data.List.intercalate "", "" $ zipWith (\n m -> concat [show n, ""."", m]) [1..] modules
           , ""]\ESC[0m\STX\n \ESC[38;5;86m\STX\x03BB > \ESC[0m\STX""
           ]   
:}
:set prompt-function prompter
clear = putStr ""\ESC[2J\ESC[H""


Thank you in advance.
","By using nub which removes duplicate elements from a list (see: https://hoogle.haskell.org/?hoogle=nub), I was able to remove duplicate elements from the module list as shown below:

In order to do that the code also has to be modified to:
:{
prompter :: [String] -> Int -> IO String
prompter modules line = return $
    concat [ ""\ESC[33m\STX[Module(s): ""
           -- this is the only line that changed
           , Data.List.intercalate "", "" $ zipWith (\n m -> concat [show n, ""."", m]) [1..] (nub modules)
           , ""]\ESC[0m\STX\n \ESC[38;5;86m\STX\x03BB > \ESC[0m\STX""
           ]   
:}

and since nub is part of the Data.List module, I had to also include:
import Data.List

",prompt
delete stubborn folder with points in name,"I'm trying to delete a folder with weird name: ""T.E.E.M."" 
Windows doesn't recognize this kind of name and even tell me the folder can't be found (it was created via NodeJs, but i don't know why windows allowed it).

I tried to delete it normally and it says that the folder can't be found because it doesn't exist.

I tried in prompt too, using the 2 commands del ""T.E.E.M."" inside the right folder and Rmdir /S ""T.E.E.M."" but they both don't work. Any suggestion?
","Windows cannot handle files/folders ending on a period-symbol ('.'). That's what I discovered when I tried to create the ""T.E.E.M.""-folder myself. I ended up with a ""T.E.E.M""-folder (without the last period).
So this might do the trick for you:
del ""\\?\<full path to file>""

but since this is a directory, maybe
rmdir /s ""\\?\<full path to file>""

works better.
By entering \\?\ string parsing is disabled.
Original answer found here.
",prompt
how can i use prompt if and else in javascript function to select arithmetic symbol and do some calculation with the user inputs,"I am a novice in this and it is really making me to lose my hair; I can find what I'm doing wrong, please help. I am doing this in javascript. It doesn't show any error, nor display any result either. This is what I have:


    var sumIt;
    var subtractIt;
    var multiplyIt;
    var divideIt;
    var operatorOpt = prompt(""Select operator"");
    
    function showResult(whatResult) {
        document.write(whatResult);
        document.write(""<br>"");
    }
    
    var doSomething = function(num1, num2) {
        if (operatorOpt == sumIt) {
            showResult(""The result is: "" + (num1 + num2));
    
    
        } else if (operatorOpt == subtractIt) {
            showResult(""The result is: "" + (num1 - num2));
    
        }  else if (operatorOpt == multiplyIt) {
            showResult(""The result is: "" + (num1 * num2));
    
    
    }  else if (operatorOpt == divideIt) {
            showResult(""The result is: "" + (num1 / num2));
    
    doSomething(parseInt (prompt(""Enter first number: "")) ,  parseInt (prompt(""Enter second number: "")))



","It seems that your are missing a closed bracket in the definition of doSomething function.
The following code seems to work produce the desired results
var sumIt = ""+"";
var subtractIt = ""-"";
var multiplyIt = ""*"";
var divideIt = ""/"";
var operatorOpt = prompt(""Select operator"");

function showResult(whatResult) {
    console.log(whatResult);
    document.write(whatResult);
    document.write(""<br>"");
}

var doSomething = function(num1, num2) {
    if (operatorOpt == sumIt) {
        showResult(""The result is: "" + (num1 + num2));
    } else if (operatorOpt == subtractIt) {
        showResult(""The result is: "" + (num1 - num2));
    }  else if (operatorOpt == multiplyIt) {
        showResult(""The result is: "" + (num1 * num2));
    }  else if (operatorOpt == divideIt) {
        showResult(""The result is: "" + (num1 / num2));
    } else {
      console.log(""No Condition reached"");
    }

 }

 doSomething(parseInt (prompt(""Enter first number: "")) ,  parseInt (prompt(""Enter second number: "")));

",prompt
loop prompt when input isn39t a month,"I'm trying to loop the prompt when the user enters incorrect input. I googled and clicked almost all the top links, tried while loop and for loop and am just not getting anywhere. When I use the method i found of a loop, my button is no longer clickable or it clicks but when I input invalid month, no alert shows up, and it doesn't loop.
if someone can point me in the right direction, or show me what Im doing wrong, I'd greatly appreciate it!!
function myFunction() {
  
  

  let text;
  let month = prompt(""What month would you like to know about?"",""Please type your answer here"").toLowerCase();
  
  switch(month) {
    case ""january"":
      text = ""There are 31 days in the month of January"";
      break;
    case ""february"":
      text = ""There are 28 days in the month of february, and 29 days on a leap year!"";
      break;
    case ""march"":
      text = ""There are 31 days in the month of March"";
      break;
    case ""april"":
      text = ""There are 30 days in the month of April"";
      break;
    case ""may"":
      text = ""There are 31 days in the month of May"";
      break;
    case ""june"":
      text = ""There are 30 days in the month of June"";
      break;
    case ""july"":
      text = ""There are 31 days in the month of July"";
      break;
    case ""august"":
      text = ""There are 31 days in the month of August"";
      break;
    case ""september"":
      text = ""There are 30 days in the month of September"";
      break;
    case ""october"":
      text = ""There are 31 days in the month of October"";
      break;
    case ""november"":
      text = ""There are 30 days in the month of November"";
      break;
    case ""december"":
        text = ""There are 31 days in the month of December"";
      break;

  }
  document.getElementById(""days"").innerHTML = text;

}

","in switch statements, you can set a default condition, which is executed when none of the cases provides matches the condition. in your case you can just call myFunction inside the default case to ""loop"".
of course this would not ask for a prompt again when the user provides a valid month.


function myFunction() {
  let text = null;
  let month = prompt(""What month would you like to know about?"", ""Please type your answer here"").toLowerCase();

  switch (month) {
    case ""january"":
      text = ""There are 31 days in the month of January"";
      break;
      // fill all your cases
    default:
      text = ""Incorrect input"";
      alert('Incorrect input, attempting prompt again');
      myFunction();

  }
  if (text)
    document.getElementById(""days"").innerHTML = text;

  // myFunction(); uncomment this if you want the loop to go on even when the user provides a valid month as input

}



",prompt
changing bash prompt in new bash,"When I create an new bash process, the prompt defaults to a very simple one.
I know I can edit .bashrc etc to change this, but is there a way of passing the prompt with the bash command?
thanks!
","The prompt is defined by the PS1, PS2, PS3 and PS4 environment variables. So, e.g. the following will start a new bash with the prompt set to ""foo: "":
PS1=""foo: "" bash --norc

The --norc is required to suppress processing of the initialization files, which would override the PS1 variable.
",prompt
while loop with prompt is never showing any consolelog,"I was just trying to check if my console gets connected to my code, and previosly it did for the same program, but now it’s not even loading the basic HTML page which contains just the heading, and it’s not showing anything in the console. Why is it behaving like this?



var todos = [ ""whats up dude!!"" ];
var input = prompt(""what would you like to do?"");

while (input !== ""quit"") {
  if (input === ""list"") {
    todos.forEach(function(todo, i) {
      console.log(i + "": "" + todo);
    });
  }
  else if (input === ""new"") {
    var newTodo = prompt(""what do you want?"");
    
    todos.push(newTodo);
  }
  else if (input === ""delete"") {
    var index = prompt(""Enter index of todo to delete"");
    
    todos.splice(index, 1);
    console.log(""Todo Removed"");
  }

  input = prompt(""what would you like to do?"");
}

console.log(""You have Quit!!"");
<!DOCTYPE html>
<html>

<head>
  <title>one more try</title>
  <script type=""text/javascript"" src=""tryy.js""></script>
</head>

<body>
  <h1>its the last resort</h1>
  <h4>hope i win this!!</h4>
</body>

</html>



","Instead of a tight loop that never yields control of the script, or page, to anything else, you can put a small delay before each prompt with setTimeout:


var todos=[""whats up dude!!""];_x000D_
_x000D_
function interactWithToDos()_x000D_
{_x000D_
  var input=prompt(""what would you like to do?"");_x000D_
  _x000D_
  if(input===""list"")_x000D_
  {_x000D_
    todos.forEach(function(todo, i)_x000D_
    {_x000D_
      console.log(i +"": ""+ todo);_x000D_
    });_x000D_
  }_x000D_
  else if(input===""new"")_x000D_
  {_x000D_
    var newTodo=prompt(""what do you want?"");_x000D_
    todos.push(newTodo);_x000D_
  }_x000D_
  else if(input === ""delete"")_x000D_
  {_x000D_
    var index = prompt(""Enter index of todo to delete"");_x000D_
    todos.splice(index, 1);_x000D_
    console.log(""Todo Removed"");_x000D_
  }_x000D_
  _x000D_
  if(input !== ""quit"")_x000D_
  {_x000D_
    setTimeout(interactWithToDos, 0);_x000D_
  }_x000D_
  else_x000D_
  {_x000D_
    console.log(""You have Quit!!"");_x000D_
  }_x000D_
}_x000D_
_x000D_
setTimeout(interactWithToDos, 0);



",prompt
how to use windbg for opening a dump and launching some windbg commands from the command prompt,"As mentioned in this other question, I'd like to start Windbg, open a dump, and launch some commands (.load pykd.pyd and !py heap_stat.py).
I thought this would be easy, but even starting Windbg and open a crash dump seems not that easy, as you can see from following unsuccessful examples:
Prompt>windbg.exe /?
Prompt>windbg.exe --help
Prompt>windbg.exe E:\Bugs\program.exe_181212_215503.dmp
Prompt>windbg.exe -D E:\Bugs\program.exe_181212_215503.dmp
Prompt>windbg.exe -zertyuiopqsdfghjklwxcvbn
Prompt>windbg.exe -help
Prompt>windbg.exe help

The idea is to get something like:
Prompt>windbg.exe -d <dumpfile> -c <command1;command2>

","it is documented and available in both windbg.chm as well as command prompt -?
note i use it on cdb which is console mode not on windbg
windbg -? , -h , --help , /? , -B#llCr@p
everything should should pop up the debugger.chm (broken in 17763 but check say 14951)

C:\>cdb -? | grep -i dump
  -openPrivateDumpByHandle <HANDLE>
    specifies the handle of a crash dump file to debug
  -z <CrashDmpFile> specifies the name of a crash dump file to debug
  -zd <CrashDmpFile> specifies the name of a crash dump file to debugand
                     deletes that crash dump after the debugger has finished
                      crash dump

C:\>

asking to create a dump
C:\>cdb -c "".dump /ma dominidumpi.dmp;q"" cdb

Microsoft (R) Windows Debugger Version 10.0.17763.132 X86

ntdll!LdrpDoDebuggerBreak+0x2c:
774a05a6 cc              int     3
0:000> cdb: Reading initial command '.dump /ma dominidumpi.dmp;q'
Creating dominidumpi.dmp - mini user dump
Dump successfully written
quit:

loading a dump doing something and quitting 
C:\>cdb -c ""lm;q"" -z dominidumpi.dmp

Microsoft (R) Windows Debugger Version 10.0.17763.132 X86


Loading Dump File [C:\dominidumpi.dmp]
User Mini Dump File with Full Memory: Only application data is available



ntdll!LdrpDoDebuggerBreak+0x2c:
774a05a6 cc              int     3
0:000> cdb: Reading initial command 'lm;q'
start    end        module name
01250000 01278000   cdb        (deferred)
5b360000 5b4ef000   dbghelp    (deferred)
5b4f0000 5b5cc000   ucrtbase   (deferred)
5b5d0000 5bbac000   dbgeng     (deferred)

77400000 7753c000   ntdll      (pdb symbols)   xxx\ntdll.pdb
775a0000 775aa000   lpk        (deferred)
quit:


C:\>

",prompt
how to let prompt read text input amp print out customised text messages,"I'm trying to do a text adventure game using basic JS. Basically, a user will have 2 options, and the prompt box should print out corresponding messages based on choices the user make.
I'm guessing that what missing is an event listener--but I'm not sure how to go about implementing that.


let message = prompt(""Hi this is an adventure. Select your input as A or B. In front of you there is a sign. Pick  A. Forest B. Lake"")

if (A) {
  prompt(""you see the bushes ahead of you rustling.You-- A.proceed ahead  B.turn back and run"")
};
else if (B) {
  prompt(""you see the water bubbling. You--A. walk up B.--flee"")
}



","You need to compare if (message.toUpperCase() === ""A"") { to allow user to type a or A
You will also have a lot less code if you use an object and a form


const advent = {
  ""Start"": {
    ""text"": ""Hi this is an adventure. Click the buttons to make your choice. In front of you there is a sign. You go direction"",
    ""a"": ""Forest"",
    ""b"": ""Lake""
  },
  ""Forest"": {
    ""text"": ""you see the bushes ahead of you rustling. You --"",
    ""a"": ""proceed ahead"",
    ""b"": ""turn back and run""

  },
  ""Lake"": {
    ""text"": ""you see the water bubbling. You--"",
    ""a"": ""walk up"",
    ""b"": ""flee""
  },
  ""walk up"": {
    ""text"": ""You drown; The end""
  },
  ""flee"": {
    ""text"": ""You fall in a hole and die; The end""
  }
};

let currentPath = advent[""Start""]

window.addEventListener(""DOMContentLoaded"", function() {
  const text = document.getElementById(""text"");
  const A = document.getElementById(""a"");
  const B = document.getElementById(""b"");
  const show = () => {
    text.innerHTML = currentPath.text; console.log(currentPath.a)
    if(currentPath.a) A.value = currentPath.a
    if(currentPath.b) B.value = currentPath.b
  };

  const form = document.getElementById(""myForm"").addEventListener(""click"", function(e) {
    const tgt = e.target;
    currentPath = advent[currentPath[tgt.id]]
    document.getElementById(""choices"").hidden = (!currentPath.a && !currentPath.b); // hide if no choices
    show()
  });
  show(); // start
});
<form id=""myForm"">
  <div id=""text""></div>
  <div id=""choices""><input type=""button"" id=""a"" value=""A""> <input type=""button"" id=""b"" value=""B""></div>
</form>



",prompt
intellij idea prompt on hover,"I'm using IntelliJ IDEA 2021.3.2 and cannot find any tool for show popup window or prompt when hovering over a button. Which setting should be turned on/off for this?

For example i wish to see what does button with wrench mean on hover it
","Disable the Settings (Preferences on macOS) | Appearance & Behavior | Appearance | Support screen readers option.
",prompt
copying a folder and renaming it using command prompt,"I am trying to copy a folder and paste it in the same directory it was copied from.
For example
C:\Test is the main directory which consists of a folder ACDM, I would like to copy ACDM in the same directory and rename the new folder to ACDM1 which will have all the same files as ACDM has
I would like to do it using command prompt
I tried the following
C:>Xcopy C:\Test C:\Test\ACDM1 /E /U
Cannot perform a cyclic copy
0 File(s) copied
which fails, not sure hoe to add REN command with XCOPY command.
Need help ASAP as i would want to create a batch file which will create a copy of an existing folder and rename it according to a name retrieved from a text file..
","xcopy ""C:\Test\ACDM\*.*"" ""C:\Test\ACDM1\"" /s/h/e/k/f/c

",prompt
equivalent of alert and prompt in php,"In JavaScript, we have Alert() and Prompt() which open up a popup box for the user.
Is there an equivalent for PHP?
$Get_['asdf'] is one way to get user input... any others?
Also, one more question. Is it a requirement that PHP always be executed all at once? Or can it be like JavaScript, where it waits for the user input (e.g. popup box), then executes the rest of the code after that.
","PHP is a server side language, it can't do alert messages on the client side. But you can use javascript within the php to do the alert.
<script type=""text/javascript"">
window.alert(""Hi There, I am the Alert Box!"")
</script>

For Prompt you can do something like this -
<?php

    //prompt function
    function prompt($prompt_msg){
        echo(""<script type='text/javascript'> var answer = prompt('"".$prompt_msg.""'); </script>"");

        $answer = ""<script type='text/javascript'> document.write(answer); </script>"";
        return($answer);
    }

    //program
    $prompt_msg = ""Please type your name."";
    $name = prompt($prompt_msg);

    $output_msg = ""Hello there "".$name.""!"";
    echo($output_msg);

?>

",prompt
customize bash prompt ps1,"I customize my bash prompt with:
PS1='\e[0;36m\u.\h
\e[0;31m $ux \e[0;92m \e[0;36m \@* \e[0;31m\w\n\e[0;92m\$ '

the output works and looks fine
but when i use a the arrowkeys to scroll in the history
after 5-10 hits of scrolling i cant move to the beginning of the line to modify the
code/command i found and
i see a part of the last command
command i execute:
$ ps aux | grep ssh
after scrolling i see
as an example in the prompt line
$ ps aux
and i can only start write after the aux
so i push ctrl+c for new line / cancel command
Did I forget a character after the $?
This is my Prompt:
z4o.ubuntu
   12:46 * /
$

when i copy/paste long commands i have the same problem
","You have to put invisible sequences inside \[ \] (or in \x01 \x02 bytes). Consult Bash manual.
PS1='\[\e[0;36m\]printable stuff\[\e[sequence\]'

Bash does not know how many columns the displayed characters take. \e[0;36m prints 7 characters, but does not move the cursor. You have to communicate that to Bash.
",prompt
nodejs synchronous prompt,"I'm using the prompt library for Node.js and I have this code:
var fs = require('fs'),
    prompt = require('prompt'),
    toCreate = toCreate.toLowerCase(),
    stats = fs.lstatSync('./' + toCreate);

if(stats.isDirectory()){
    prompt.start();
    var property = {
        name: 'yesno',
        message: 'Directory esistente vuoi continuare lo stesso? (y/n)',
        validator: /y[es]*|n[o]?/,
        warning: 'Must respond yes or no',
        default: 'no'
    };
    prompt.get(property, function(err, result) {                
        if(result === 'no'){
            console.log('Annullato!');
            process.exit(0);
        }
    });
}
console.log(""creating "", toCreate);
console.log('\nAll done, exiting'.green.inverse);

If the prompt is show it seems that it doesn't block code execution but the execution continues and the last two messages by the console are shown while I still have to answer the question.
Is there a way to make it blocking?
","Since IO in Node doesn't block, you're not going to find an easy way to make something like this synchronous. Instead, you should move the code into the callback:
  ...

  prompt.get(property, function (err, result) {               
    if(result === 'no'){
        console.log('Annullato!');
        process.exit(0);
    }

    console.log(""creating "", toCreate);
    console.log('\nAll done, exiting'.green.inverse);
  });

or else extract it and call the extracted function:
  ...

  prompt.get(property, function (err, result) {               
    if(result === 'no'){
        console.log('Annullato!');
        process.exit(0);
    } else {
        doCreate();
    }
  });

  ...

function doCreate() {
    console.log(""creating "", toCreate);
    console.log('\nAll done, exiting'.green.inverse);
}

",prompt
i need to validate an input regardless of text capitalization,"I have written a short script that works pretty much as a quiz. It displays a dialogue box making a question and you input the answer. However, my intention is that whether the answers are provided in capital or lowercase text, the app should grant you a point regardless.
My question is, how can I give a point regardless of text capitalization of the input?
I saw someone using toUpperCase() at some point in the code, but I am not quite sure.
let questions = [
  [""Who created Demon Slayer?"", 'Gotouge'],
  ['Who was the first demon that ever existed?', 'Muzan'],
  ['Who was the flame pillar of the Demon Slayer Corps?', 'Rengoku']
];


let item = '';
let score = 0;

function quiz (arr){
  for (let i = 0; i<questions.length; i++){
    let interrogate = prompt(`${arr[i][0]}`);
    if(interrogate === arr[i][1]){
      score++;
    }
  }
}


quiz(questions);
console.log(score);

","This should be what you want, I simplify your code a little bit.
I also add trim() to remove white space in the end and begining.


let questions = [
  [""Who created Demon Slayer?"", 'Gotouge'],
  ['Who was the first demon that ever existed?', 'Muzan'],
  ['Who was the flame pillar of the Demon Slayer Corps?', 'Rengoku']
];

let score = 0;
function quiz (arr){
  for (let i = 0; i<questions.length; i++){
    let interrogate = prompt(`${arr[i][0]}`);
    if(interrogate.trim().toLowerCase() === arr[i][1].toLowerCase()) score++;  
  }
}
quiz(questions);
console.log(score);



",prompt
wait for click event inside a for loop  similar to prompt,"This might not have the greatest title. I'm trying to understand call back functions, and I was wondering how replacing prompt() in the following code could be achieved, without losing the for loop?
for(i=0;i<4;i++){
  let x = prompt(""Input an integer"");
  // store input into an array
}

I've tried something like:
for(let i = 0; i<4; i++){
  let x = document.getElementById(""someId"");
  x.addEventListener(""click"", rcvInput(function(i){
    if(i == 3){
      x.removeEventListener(""click"", rcvInput)
    }
  }));
}
function rcvInput(callback){
  //store input into an array
  callback();
}

I know this can be done without the for loop, I'm more curious if callbacks could be able to pause the loop and wait for input?
","Depending on what your end goal is, I'm pretty sure there's a better way to do it. But for the sake of doing that:
You can create a method that returns a promise that resolves when a click happens. Then you can use async/await to do what you need.
By using a Promise and awaiting on it, you can technically ""pause"" your for loop until something happens. In this case, a click.
Remember the method that encloses the for loop has to be async.


function getClick() {
  return new Promise(acc => {
    function handleClick() {
      document.removeEventListener('click', handleClick);
      acc();
    }
    document.addEventListener('click', handleClick);
  });
}


async function main() {
  for (let i=0;i<4;i++) {
    console.log(""waiting for a click"", i);
    await getClick();
    console.log(""click received"", i);
  }
  console.log(""done"");
}

main();



Try it in this plunkr.
",prompt
autofill multiples prompts in powerwhell,"I'm preparing laptops with Autopilot Intune Enrollment but we need to import hash in Endpoint automatically
We got multiples command lines to enter in cmd, look like that
powershell
Set-Executionpolicy Bypass
Install-Script -Name Get-WindowsAutoPilotInfo
Get-WindowsAutopilotInfo.ps1 -Online -Assign

Issue here, the line with Install-Script always ask for 3 differents prompts :
Twice ""y"" to enter and once ""a"" at the end
Resulting prompt
But we lose too much time doing this for each laptop.
So I tried implement echo command and it's look like that :
powershell
Set-Executionpolicy Bypass
echo ""y"" | Install-Script -Name Get-WindowsAutoPilotInfo
echo ""y"" | Install-Script -Name Get-WindowsAutoPilotInfo
echo ""a"" | Install-Script -Name Get-WindowsAutoPilotInfo
Get-WindowsAutopilotInfo.ps1 -Online -Assign

But it's not working and I can't figured out why, even if I've tryed different type of configurations for this.
Anybody had ideas ? I'm not very confident in powershell unfortunately to solve it by myself
Thanks in advance !
","After some further research, mixing both suggested solutions give me that :
Set-Executionpolicy Bypass -Force
Install-PackageProvider -Name NuGet -MinimumVersion 2.8.5.201 -Force -Confirm:$False
Install-Script -Name Get-WindowsAutoPilotInfo -Force -Confirm:$False
powershell.exe -ExecutionPolicy Bypass Get-WindowsAutopilotInfo.ps1 -Online -Assign

Sources :
https://learn.microsoft.com/en-us/powershell/module/powershellget/install-script?view=powershell-7.2
",prompt
how to wait until navigatornotificationprompt input has been given,"I am using a Cordova app to embed a React app. In a certain point, the user connects to the camera and a notification appears when a QR code is detected. I want that the code execution waits until user has entered his/her answer ""Yes/No"", but I can't get it to work. Notification prompt message works as expected though.
I need it to pause somehow, as in an async function (like  prompt would, for instance). How could I pause the code execution until the user has chosen Yes/No in screen? I guess with some async/await but don't see where... I have tried unsuccessfully so far:
let test = null;
let input2 = null;
notification = navigator.notification.prompt(""Do you know this QR code?"",
     async function(input) {
     input2 = input;
     test = await NotificationFunction(input, Camera_content);
     console.log(test)
     console.log('input2')
     return [test, input2]
     });

let test2= await notification
console.log(test2)
console.log(notification)
console.log(await test)

Thanks a lot in advance!
","Maybe you can use something like this:
const promise = new Promise((resolve, reject) => {
  navigator.notification.prompt(
    'Do you know this QR code?',
    async function (input) {
      test = await NotificationFunction(input, Camera_content);
      resolve([test, input]);
    },
  );
});

const [test, input] = await promise

Also maybe this can help https://javascript.info/promisify
",prompt
open file password prompt while closing,"I have two separated workbooks with macroces: Book1 with Macro1 and Book2 with Macro2.
Macro1 is simple code to:

hidely open Book2 (has an open password: 1111);
write ""test"" in A1 Active Sheet of Book2;
run Macro2 from Book2, which is simple MsgBox ""Hello! I am an msgbox
from Book2"";
close Book2 with saving.

Sub Macro1()
Application.ScreenUpdating = False
Application.EnableEvents = False
Application.DisplayAlerts = False

Dim myWb As Workbook
Set myWb = Workbooks.Open(ThisWorkbook.Path & ""/"" & ""Book2.xlsb"", True, False, , ""1111"")
myWb.Unprotect (""1111"")


Cells(1, 1) = ""test""

myWb.IsAddin = True
Application.Run ""'Book2.xlsb'!Macro2""
myWb.Close savechanges:=True


Application.DisplayAlerts = True
Application.ScreenUpdating = True
Application.EnableEvents = True
End Sub

Sub Macro2()
MsgBox ""Hello! I am an msgbox from Book2""
End Sub

I emphasize that everything should be done secretly and the user should not see neither Book2 nor any flashes, etc.
But when code comes to the saving step a window pops up on the screen, which, by the way, can be ignored and the code will continue to run/execute but anyway it's a PROBLEM I want get rid of:
password pop-up window
What I noticed:

If I remove the opening password from book2, the saving will be successfull and without pop-up dialogs. But I need Book2 opening protected.
I tried to modify code, but results are not unsatisfactory due to
screen blinking, screenshots below:
OK
V1-NOK
V2-NOK

V1: myWb.IsAddin = True is deleted/commented
Application.Run ""'Book2.xlsb'!Macro2""
myWb.Close savechanges:=True

V2: ON/OFF IsAddin = True before/after Macro2 executing
myWb.IsAddin = True
Application.Run ""'Book2.xlsb'!Macro2""
myWb.IsAddin = False
myWb.Close savechanges:=True

","Please, try the next way. It will open the workbook in a new session, will write in its first sheet and run the macro. But a macro sending a message from a non visible session/window is not the best idea. In order to see the message, please move the cursor over the Excel workbooks icons on taskbar, select the MsgBox window and press OK. A better idea would be do avoid sending messages. The code should run as you need:
Sub HiddenWBOpenRunMacro()
  Dim newEx As Excel.Application, myWb As Workbook
  
  Set newEx = CreateObject(""Excel.Application"")
  'Open the workbook in the newly created session:
  Set myWb = newEx.Workbooks.Open(ThisWorkbook.Path & ""/"" & ""Book2.xlsb"", , , , ""1111"")

  myWb.Sheets(1).cells(1, 1).Value = ""Test""  'write in its first sheet
  myWb.Application.Run ""'Book2.xlsb'!Macro2"" 'run the macro

  myWb.Close True                            'close the workbook, saving it
  newEx.Quit                                 'quit the session
End Sub

If ThisWorkbook not in Excel Trusted Locations it must be added.
",prompt
how can i install the quotosquot package of python using the anaconda prompt,"So this question is pretty easy, but I didn't find a great answer. In my program, I want to import the OS package using: import os.
Well, how can I import this package using the anaconda prompt? I tried conda install os and pip install os and it didn't work. Can anyone help me?
","os is already installed with your python package. You just have to import it into your python files, you can do this by import os.
",prompt
prompt loads before anything else,"hay if anybody understand why is prompt showing up even  before the console printing
i appreciate the help


<!DOCTYPE html>
<html lang=""en"">

<head>
    <meta charset=""UTF-8"">
    <meta http-equiv=""X-UA-Compatible"" content=""IE=edge"">
    <meta name=""viewport"" content=""width=device-width, initial-scale=1.0"">
    <title>promptheadache</title>
</head>

<body>
    <script>
    
        console.log('i should appear first on the console')

        var promptInput = prompt('I\'m am here befor anything else')

    </script>
</body>

</html>



","console.logs are actually asynchronous in that they are synchronized (as in always in order) but perform an asynchronous call.
prompt is a very old API and one of the few that actually blocks the page - so even-though it appears after the console.log you see it as soon as it is called.
That said - this is not true for every console nor is it guaranteed - the console.log may appear before the prompt depending on the browser/console implementation.
",prompt
ansible check prompt value,"I created an Ansible playbook script, to run it have need user interaction.
Now I need to check if the user insert correctly the value when I prompt it.
  vars_prompt:
  - name: ""vm_env""
    prompt: ""vm_env (values: PRD, TST, DEV)""
    default: ""DEV""
    private: false

How can I check if the user insert correctly only one of these values (PRD, TST, DEV), and in case stop script?
Thanks for the support
Marco
","There is an Ansible assert module to validate things and fail with appropriate error message if condition is not matched.
Example:
  vars_prompt:
  - name: ""vm_env""
    prompt: ""vm_env (values: PRD, TST, DEV)""
    default: ""DEV""
    private: false

  tasks:
  # ""|lower"" filter used to fix any case inconsistency, not required if case should match
  - assert:
      that:
      - vm_env|lower in [ 'prd', 'tst', 'dev' ]
      fail_msg: ""VM environment should be one of: PRD, TST, DEV""

",prompt
conditional statment in prompt,"I'm working on providing a user with two different pictures depending on the right username/password. I'm having a prompt popup for the user to give the username/password and after providing the right credentials the login picture should appear and if the credentials are wrong the logout picture should appear. I am using the getElementById to change accordingly. At this moment the prompt shows up, but there is no picture showing.
<img src=""user_in.PNG"" id=""user_in_id"" hidden>
<img src=""user_out.PNG"" id=""user_out_id"" hidden>

<script> 
      
     var person = prompt(""Please enter your name"", """"); 
     var password = prompt(""Please enter your password"", """");
     if (person == ""admin"" && password ==""admin"") {
        alert(""Welcome, You are now Logged in"");
        document.getElementById(""user_in_id"");

      }else
        alert(""wrong username or password"");
        document.getElementById(""user_out_id"");
      
</script>

","You have to apply a function to your document.getElementById(""user_in_id"") selector,which somehow makes the selected element visible.
One way would be to remove the hidden attribute from it with document.getElementById(""user_in_id"").removeAttribute(""hidden"");
You also forgot to wrap your else statement around curly brackets in order to work properly.


<img src=""https://styles.redditmedia.com/t5_3obin/styles/communityIcon_5v6pv5kqz5241.PNG"" id=""user_in_id"" hidden>
<img src=""https://public.blenderkit.com/thumbnails/assets/0992088bfb844c69bb6e426272970c8b/files/thumbnail_c858218a-4afc-4e94-b6b9-080f5e6c7066.jpg.256x256_q85_crop-%2C.jpg"" id=""user_out_id"" hidden>

<script>
    var person = prompt(""Please enter your name"", """"); 
    var password = prompt(""Please enter your password"", """");
     if (person === ""admin"" && password === ""admin"") {
        alert(""Welcome, You are now Logged in"");
        document.getElementById(""user_in_id"").removeAttribute(""hidden"");

      }else{ 
       alert(""wrong username or password"");
       document.getElementById(""user_out_id"").removeAttribute(""hidden"");
      }
        
      
</script>



",prompt
add dollar sign after path in prompt zsh,"I want to add the dollar sign after the path in my prompt.
Current prompt is: 
.oh-my-zsh git:(master) 

Wanted prompt is: 
.oh-my-zsh git:(master) $

I tried to modify PROMPT in ~/.zshrc without success.
","https://github.com/ohmyzsh/ohmyzsh/blob/c78277fd8bda5fec87504469afdf121355876006/themes/gozilla.zsh-theme#L1
PROMPT='%{$fg_bold[red]%}➜ %{$fg_bold[green]%}%p %{$fg[cyan]%}%c %{$fg_bold[blue]%}$(git_prompt_info)%{$fg_bold[blue]%}$ % %{$reset_color%}'
",prompt
how to use while loop on a prompt in javascript,"[


  let welcome;
  welcome = ""Welcome to my first COMP1231 Program."";
  alert(welcome)

  let name;
  name =(prompt(""Please enter your name:?"", ""Tafadzwa Marisa"" ));


  let program;
  program =(prompt(""Please enter your Program:?"", ""COMP1231"" ));



]1
So l am trying to use a prompt which asks a user for their name and when the user enters a name it proceeds to another prompt but when the user doesn't enter anything it loops back to the first prompt which is ask for the users name. If the user enters a valid input it then goes to the program prompt where if the user enters any input it proceeds but if the user doesn't it loops to the same prompt. I want to use the while loop for this
","You can create a function which will basically execute a while loop until user enters some input. This function returns the input value if it's not empty.


function loopUntilNonEmptyInput(prompt_ques, default_inp) {
  let retval;
  while (true) {
    retval = (prompt(prompt_ques, default_inp));
    
    if (retval !== '')
      return retval;
      
    alert(""Empty input, please re-try"");
  }
}

let welcome;
welcome = ""Welcome to my first COMP1231 Program."";
alert(welcome);

let name = loopUntilNonEmptyInput(""Please enter your name:?"", ""Tafadzwa Marisa"");
let program = loopUntilNonEmptyInput(""Please enter your Program:?"", ""COMP1231"");
console.log(name, program);



",prompt
how to change color of prompt from last exit code in zsh,"I have this piece of code in my .zshrc:
autoload -U colors && colors

function change_color() {
    if [[ ""$?"" = 0 ]]; then
    return ""green""
  else
    return ""red""
  fi
}

PS1=""%B%{$fg[red]%}[%{$fg[yellow]%}%n%{$fg[green]%}@%{$fg[blue]%}%M %{$fg[magenta]%}%~%{$fg[red]%}]% %{$fg[$changecolor]%}➜%b ""

However, the ➜ character stays white, no matter what I do.
","As a function, you need to call it, not expand it as a variable. Do that with a command substitution:
PS1=""...%{$fg[\$(changecolor)]%}➜%b ""

However, there's a lot of bash-like stuff in this prompt that you can replace with simpler, more robust zsh features. First, %F can be used to change the color directly, without using escape codes stored in an array. (Unlike raw escape codes, and like other zsh-defined escape sequences, %F{...} doesn't need to be wrapped in %{...%}.) Second, there is prompt escape specifically for producing one value or another depending on whether the last exit status was 0 or not.
PS1='%B%F{red}[%F{yellow}%n%F{green}@%F{blue}%M '
PS1+='%F{magenta}%~%F{red}] %(?.%F{green}.%F{red})➜%b%f '

%(?.FOO.BAR) expands to FOO if the last command succeeded, BAR otherwise. %f restores the color to the default, regardless of any preceding %F codes.

%(...) is, in fact, more general than just testing the exit code. ? is just one of 20 or so testable conditions. As a silly example, suppose your birthday is coming up on January 18. Here's an escape to put a cake in your prompt on your birthday.
%(0D.%(18d.🎂.).)

More information on prompt escape codes can be found in man zshmisc.
",prompt
trigger client side javascript based on datetime,"I have an event_start time and I want to run a script 10 seconds after that event start but can't figure out how to trigger the script.
const date = Date().toString();
const currentDateParse = Date.parse(date);
const trigger = Date.parse(startTime) + 1000 * 10

Then this is how I'm attempting to trigger the script but it's not working. How do I start my function when the currentDateParse is equal to trigger. Or, put more simply, 10 seconds after the event starts.
if (currentDateParse = trigger)
   <function code underneath works already>

","Try this (explanations below):
let startTime = ""18:00"";
let currentTime = new Date();
let target = new Date().parse(startTime);

let timeout = target - currentTime + 1000 * 10; // how long should it wait till the functions is executed

function runTenSecAfterEvent() {
  // do something here
}

setTimeout(runTenSecAfterEvent, timeout);

Explanations
After you calculated target (the start time) and currentTime, you need to know the time difference between them (timeout), so target minus currentTime. After that, you add the 10000ms.
And then you define the function which will be executed 10 seconds after the event occurred (runTenSecAfterEvent()).
Finally, you create a timeout.
",prompt
reactjs how to get input value from the prompt box and update it to database,"I'm building a MERN app, I want to let user edit the food name in the prompt box by clicking on the Edit button.
I was following the instructions in this link: [https://stackoverflow.com/questions/54993218/reactjs-using-alert-to-take-input-from-user]
The issue is when I click on the Edit button and type in the prompt then click OK, it will receive the null value for the first time, but it won't update the database.
And then when I click the Edit button again, without input anything to it then press OK, it will receive the value from the first time input and update it to database (like a delay).
What I want is: when click on the Edit button, it will display the prompt box and take the value from the prompt box and update to the database when the user clicks OK.
Is there any way I can fix this? Thank you everyone!
Here's my demo: gif
Here's my code:
function FoodListTable(props) {
    /* Definition of handleClick in component */
    const [newFoodName, setNewFoodName] = useState("""")

    const handleEdit = () => {
        const enteredFood = prompt('Please enter your new food:')

        setNewFoodName(enteredFood)

        console.log(newFoodName)

        if (newFoodName) {
            Axios.put(""https://mern-lefood.herokuapp.com/update"", {
                newFoodName: newFoodName,
                id: props.val._id
        })
    }
}

return (
        <button onClick={handleEdit}>Edit</button>
    )
}

","React this.setState, and useState does not make changes directly to the state object.
React this.setState, and React.useState create queues for React core to update the state object of a React component.
So the process to update React state is asynchronous for performance reasons. That’s why changes don’t feel immediate.
Try below code that's works !
function FoodListTable(props) {
  /* Definition of handleClick in component */
  const [newFoodName, setNewFoodName] = useState("""");

  const handleEdit = () => {
    const enteredFood = prompt('Please enter your new food:');
    setNewFoodName(enteredFood);

    if (enteredFood) {
      Axios.put(""https://mern-lefood.herokuapp.com/update"", {
        newFoodName: enteredFood,
        id: props.val._id
      })
    }
  }

  return (
    <button onClick={handleEdit}>Edit</button>
  )
}

For more detail Click here
",prompt
connecting colab to local runtime jupyter notebook,"I'm trying to connect my Google Colab to a local runtime via Jupyter Notebook. There is one part I can't figure out, which is this:
#Ensure that the notebook server on your machine is running on port 8888 and accepting requests from https://colab.research.google.com.
jupyter notebook \
  --NotebookApp.allow_origin='https://colab.research.google.com' \
  --port=8888 \
  --NotebookApp.port_retries=0

I tried copy-pasting it into my anaconda prompt but only ""jupyter notebook "" is pasted and executed. How do you get all that code typed into prompt? Is it some cmd feature that I'm completely oblivious to?
","The command you are currently running should be run on a Linux machine. If you have a Windows machine, replace the \ at the end of each part of the command with ^. So, your command will be
jupyter notebook ^
  --NotebookApp.allow_origin='https://colab.research.google.com' ^
  --port=8888 ^
  --NotebookApp.port_retries=0

Or, the complete command with all the parameters can be run in a single line like this
jupyter notebook --NotebookApp.allow_origin='https://colab.research.google.com' --port=8888 --NotebookApp.port_retries=0

I have tested this to work on the command prompt, and the local runtime connection is successful.
",prompt
error in cmd as python version not showing,"While i write 
python --version

I get nothing as a result in command prompt. I should get the version details of python, correct? how to rectify it?
Its the same for pip also, which file to download and how to install them, .whl or .grz? The guide shows method using pip --version which again is not working in cmd at the first place.
","You did not say how your issue was solved, so I had to figure it out myself. 
My problem was a 0kb Python.exe in the WindowsApps folder, which is in the PATH environmental variable. I just deleted that file, and everything worked.
",prompt
how to use multiple text values using sas prompt,"I am trying to create a prompt which allows to user to enter multiple values from a list.   I have used Prompt Type as Text.
Method of populating the prompt : User select values from static list
Number of values : Multiple Values 
I have kept the name of my prompt as Products. Basically I want to have a macro which has values such as ""product 1"",""product 2"", Product 3"", ""product 4"" separated by commas. 

When I try and resolve the products macro no matter how many products I select in the prompt it always resolves the first product. Here it is always resolving only Loan as the resolution to the products macro.   I want all the products in that I select through prompt to come in the macro product. Example (""loan"",""Overdraft"",""RCF"") Please suggest what to do in this case
","I made a prompt, ""Color"", with the six colors of the rainbow, with the settings like you describe (and ""use throughout project"").
Then I attached it to the following program:
%put _global_;

And what I see is this;
8          %LET Color0 = 4;
9          %LET Color2 = Red;
10         %LET Color3 = Yellow;
11         %LET Color4 = Orange;
12         %LET Color1 = Blue;
13         %LET Color_count = 4;
14         %LET Color = Blue;

This is how multiple value prompts work: they put the first selection in &prompt, then put the number of selected items in &prompt0, and then put the selected items in &prompt1-&&prompt&prompt0. You can then use them in a number of ways.  You can 
%do i = 1 %to &color0.;
  %put &&color&i.;
%end;

For example.
",prompt
javascript prompt box without the cancel button,"Is there a way to create a input prompt box that doesn't have the cancel button,
or doesn't allow the user to close the prompt until some value is entered?
","Put your promt inside a while loop and continue asking for input until user gives any.
do{
    input = prompt(""Give input"");
}while(input == null || input == """" );

console.log(input);

jsFiddle
[Note : It's a way to fulfill your requirement that I won't suggest as it may annoy user. You can go with regular expression if you want user to complete your form.]
",prompt
java39s keytool doesn39t prompt for key password,"Java's keytool has a parameter called -keypass which allows you to set a (separate) password to protect your private key, in addition to the password used for the entire key store.
According to the documentation:

The value of -keypass is a password used to protect the private key of the generated key pair. If a password is not provided, then the user is prompted for it. If you press the Return key at the prompt, then the key password is set to the same password as the keystore password. The -keypass value must have at least six characters.

However, when I leave out the password in the call to this command I don't seem to get prompted at all, at least not when this is used in combination with -genkeypair to generate an RSA key pair. Instead I just get the general help page. If I use """" to force an ""empty"" password then it (correctly) tells me that the password should at least be 6 characters.
Is there a way to force the keytool to prompt for a key specific password instead of having to offer it on the command line according to the documentation of -genkeypair?

I've tested this against Java 11 LTS:
keytool -genkeypair -alias test1 -keyalg RSA -keysize 4096 -sigalg SHA256withRSA -keystore test.pfx -storepass test1234
        -keypass

or
keytool -genkeypair -alias test1 -keyalg RSA -keysize 4096 -sigalg SHA256withRSA -keystore test.pfx -storepass test1234
        -keypass """"

both don't seem to work; as you can see I've already moved the -keypass parameter to the end so it cannot gobble up a parameter as password.
","The default keystore type for Java 11 is PKCS12, for which it is always assumed the keystore password and key password will be the same, hence you are not prompted to enter it (documentation)
If you need to use a key password to fit your requirements, you can use other keystore types like jks or jceks.
Note: If you are using jks or jceks, java will show you a warning message:

The JKS keystore uses a proprietary format. It is recommended to migrate to PKCS12 which is an industry standard format


If you type:
keytool -genkeypair -keyalg RSA -keysize 2048 -keystore double.pfx -storepass Storepass -keypass Keypass

You'll get the following warning:
Warning:  Different store and key passwords not supported for PKCS12 KeyStores.


Note that the PKCS#12 key stores themselves do support multiple passwords: they have separate derivations for multiple sections (keys, certificates) and KeyBag's and such. It's just that the Java support is missing. I found this out after parsing several key stores and looking at the format.
",prompt
how to show current directory in ipython prompt,"Is there is way to show the current directory in IPython prompt?
Instead of this:
In [1]:

Something like this:
In<~/user/src/proj1>[1]:

","Assuming you're interested in configuring this for all subsequent invocations of ipython, run the following (in a traditional shell, like bash :) ). It appends to your default ipython configuration, creating it if necessary. The last line of the configuration file will also automatically make all the executables in your $PATH available to simply run in python, which you probably also want if you're asking about cwd in the prompt. So you can run them without a ! prefix. Tested with IPython 7.18.1.
mkdir -p ~/.ipython/profile_default
cat >> ~/.ipython/profile_default/ipython_config.py <<EOF

from IPython.terminal.prompts import Prompts, Token
import os

class MyPrompt(Prompts):
    def cwd(self):
        cwd = os.getcwd()
        if cwd.startswith(os.environ['HOME']):
            cwd = cwd.replace(os.environ['HOME'], '~')
            cwd_list = cwd.split('/')
            for i,v in enumerate(cwd_list):
                if i not in (1,len(cwd_list)-1): #not last and first after ~
                    cwd_list[i] = cwd_list[i][0] #abbreviate
            cwd = '/'.join(cwd_list)
        return cwd

    def in_prompt_tokens(self, cli=None):
        return [
                (Token.Prompt, 'In ['),
                (Token.PromptNum, str(self.shell.execution_count)),
                (Token.Prompt, '] '),
                (Token, self.cwd()),
                (Token.Prompt, ': ')]

c.TerminalInteractiveShell.prompts_class = MyPrompt
c.InteractiveShellApp.exec_lines = ['%rehashx']
EOF

(c.PromptManager only works in older versions of ipython.)
",prompt
removing vcs from vcs_info prompt in zsh name to use zsh_theme_git_prompt_dirty,"I currently have this set up in my .zshrc
autoload -Uz add-zsh-hook vcs_info
setopt prompt_subst
add-zsh-hook precmd vcs_info
add-zsh-hook precmd my_precmd

zstyle ':vcs_info:git:*' formats '%b%u%c'

and in my precmd function I set my RPROMPT to be $(git_prompt_info). I also colour it based on the name of the branch (ie main might be blue, while WIP is red etc).
The issue I have is that it always prints out with a git prefix (like this git:(main)).
I checked the docs and it shows that from :vcs_info:vcs-string:user-context:repo-root-namethe  relevant part I want to get rid of is vcs-string but I can't find a way to remove it.
I know I can just use ${vcs_info_msg_0_} but I'm trying to incorporate ZSH_THEME_GIT_PROMPT_CLEAN and ZSH_THEME_GIT_PROMPT_DIRTY and haven't found a way to do so with vcs_info
Any help would be appreciated.
","Turns out to get access to the ZSH_THEME_GIT_PROMPT_CLEAN and ZSH_THEME_GIT_PROMPT_DIRTY parts of the prompt, you can just call directly into the zsh code
So now my prompt colouring is basically
  if [[  ""${vcs_info_msg_0_}"" != """" ]]
    then
      [[ ""${vcs_info_msg_0_}"" == ""master"" || ""${vcs_info_msg_0_}"" == ""main"" ]] && git2color='196'
      local git2=""%B%F{${git2color}}(${vcs_info_msg_0_}$(parse_git_dirty))%f%b ""
  fi

",prompt
how to colour git branch name in zsh prompt,"I have the following set in my .zshrc
autoload -Uz add-zsh-hook vcs_info
setopt prompt_subst
add-zsh-hook precmd vcs_info

zstyle ':vcs_info:git:*' formats '%b'

and this in my
custom zsh theme :
vcs_info_wrapper() {
  vcs_info
  if [[ ""${vcs_info_msg_0_}"" == ""master"" ]]; then
    echo ""%{$FG[196]%}""
  else
    echo ""%{$fg[cyan]%}""
  fi
}

PROMPT=$'%B%{$FG[039]%}%n%b%{$fg_bold[white]%}@%m%{$FG[220]%} %{\x1b[3m%}%5~ %{$reset_color%}$(git_prompt_info)%{$reset_color%}%{\x1b[0m%} %(?.%{$fg[white]%}.%{$fg_bold[red]%}[%?])» %{$FG[010]%}  ||$(vcs_info_wrapper)${vcs_info_msg_0_}|| '
I have both $(vcs_info_wrapper)${vcs_info_msg_0_} and $(git_prompt_info) to test the colour output. For some reason the former always works and has the correct colour, but the latter doesn't and once the colour changes it never resets. I've basically tried everything at this point. Any ideas are welcome
EDIT:
Thanks @Gairfowl to I have most of it working now with:
function my_precmd {
  vcs_info
  local user='%B%F{#00ACE6}%n%f%b'

  local host='%B%F{white}@%m%f%b'
  local path='%F{#FFD700}%4~%f'
  local rcAndArrow='%(?.%F{white}.%B%F{red}[%?])»%f%b'

  local git2color='cyan'
  local git2=""""

  [[ ""${vcs_info_msg_0_}"" == ""master"" || ""${vcs_info_msg_0_}"" == ""main"" ]] && git2color='196'

  if [[  ""${vcs_info_msg_0_}"" != """" ]]
    then
      local git2=""%B%F{${git2color}}($(git_prompt_info))%f%b ""
  fi

  psvar[1]=""${user}${host} ${path} ${git2}${rcAndArrow} ""

However I don't get any git information from $(git_prompt_info) If I combine it with path (like this local path=""%F{#FFD700}%4~%f $(git_prompt_info)"") that seems to work.
","It's often much easier to read and debug a precmd function than to
put everything in the PROMPT variable. Try building your prompt like the function below; you can comment out pieces and isolate the parts you're working on:
autoload -Uz add-zsh-hook vcs_info
setopt prompt_subst
add-zsh-hook precmd my_precmd

zstyle ':vcs_info:git:*' formats '%b'

function my_precmd {
  local theUser='%B%F{39}%n%f%b'
  local theHost='%B%F{white}@%m%f%b'
  local git1=""%F{220}~%f$(git_prompt_info)""
  local rcAndArrow='%(?.%F{white}.%B%F{red}[%?])»%f%b'

  vcs_info
  local git2color='cyan'
  [[ ""${vcs_info_msg_0_}"" == ""master"" ]] && git2color='196'
  local git2=""||%F{${git2color}}${vcs_info_msg_0_}%f||""

  psvar[1]=""${theUser}${theHost} ${git1} ${rcAndArrow} ""
  psvar[2]=""${git2}""
}

PROMPT='${psvar[1]}'
RPROMPT='${psvar[2]}'

",prompt
cannot read properties of null reading 39split39 prompt,"Good afternoon,
Please, could you help me with an issue below.
I am getting an error ""Cannot read properties of null (reading 'split')"" when trying to click ""cancel"" on prompt.
What all I want, to loop canceled when I cancel it in prompt.
ps.: this is a simple array with a name/surname which in future will shows via console.log
function UserList () {
    let users = [];
    while(true) {
        users.push (prompt('Please, enter your name surname?').split(' '));
        if (prompt=== null) {
            alert('cancel');
        }
    }
}
let userList = new UserList();

","You need to test whether the result of prompt() is null before you try to split it.
You need to break out of the loop when the user cancels, otherwise the function will never return.
Also, since you're using this as a object constructor, users should be a property this.users.
function UserList () {
    this.users = [];
    while(true) {
        let response = prompt('Please, enter your name surname?');
        if (response == null) {
            alert('cancel');
            break;
        }
        this.users.push (response.split(' '));
    }
}

",prompt
get last executed command in bash,"I need to know what was the last command executed while setting my bash prompt in the function corresponding to PROMPT_COMMAND. I have code as follows
function bash_prompt_command () { 
...
    local last_cmd=""$(history | tail -n 2 | head -n 1  | tr -s ' ' | cut -d ' ' -f3-)""
    [[ ${last_cmd} =~ .*git\s+checkout.* ]] && ( ... )
...
}

Is there is faster(bash built-in way) to know the what was the  command which invoked PROMPT_COMMAND.
I tried using BASH_COMMAND, but that too does not return the command which actually invoked PROMPT_COMMAND.
","General case: Collecting all commands
You can use a DEBUG trap to store each command before it's run.
store_command() {
  declare -g last_command current_command
  last_command=$current_command
  current_command=$BASH_COMMAND
  return 0
}
trap store_command DEBUG

...and thereafter you can check ""$last_command""

Special case: Only trying to shadow one (sub)command
If you only want to change how one command operates, you can just shadow that one command. For git checkout:
git() {
  # if $1 is not checkout, just run real git and pretend we weren't here
  [[ $1 = checkout ]] || { command git ""$@""; return; }
  # if $1 _is_ checkout, run real git and do our own thing
  local rc=0
  command git ""$@"" || rc=$?
  ran_checkout=1 # ...put the extra code you want to run here...
  return ""$rc""
}

...potentially used from something like:
bash_prompt_command() {
  if (( ran_checkout )); then
    ran_checkout=0
    : ""do special thing here""
  else
    : ""do other thing here""
  fi
}

",prompt
shell script not storing command output as variable,"I have defined a function to check if the environment variable VIRTUAL_ENV is set, and if so, figure out the current python version.
theme_python_prompt () {
    if [ -v VIRTUAL_ENV ]
    then
        local VERSION=""$(python -V)""
        echo -n ""%{$fg[yellow]%}%{$reset_color%}:${VERSION}(%{$fg[magenta]%}$(basename ${VIRTUAL_ENV})%{$reset_color%})""
    fi
}

But, this is just outputting the result of python -V to stdout instead of storing it into the variable. Printing the whole stack (set -x) shows the following:
+theme_python_prompt:1> [ -v VIRTUAL_ENV ']'                                   
+theme_python_prompt:3> python -V                    
Python 2.7.15                          
+theme_python_prompt:3> echo ''         
+theme_python_prompt:3> local VERSION=''
+theme_python_prompt:4> basename /home/hjpotter92/.virtualenvs/test-2fI9Fep8
+theme_python_prompt:4> echo -n $'%{\C-[[33m%}%{\C-[[00m%}:(%{\C-[[35m%}test-2fI9Fep8%{\C-[[00m%})'

A similar function to fetch me rbenv info is working without issues:
theme_rbenv_prompt () {
    if ! type rbenv > /dev/null
    then
        echo -n """"
    else
        local VERSION=""$(rbenv_prompt_info)""
        [ ""$VERSION"" != ""system"" ] && echo ""%{$fg_bold[red]%}%{$reset_color%}:${VERSION} "" || echo -n """"
    fi
}

where rbenv_prompt_info is from oh-my-zsh plugin.
","python -V prints to stderr, not stdout. You need to redirect the standard error to standard output, otherwise you'll get an empty string.
Use local VERSION=$(python -V 2>&1) instead.
",prompt
wanted to put user input into function,"I am wanting to put my user prompt inputs into my password generator. So far everything else works except for the user inputs. The only one I have working so far is length. If you see the userDigits input directly below links that is where I am getting stuck now. Not sure if the choseNo empty box choice is doing what I want it to do. I want them to type yes or no and get digits or not in there chosen length of password. Then I will repeat for userSpec, userLower, and userUpper. I have been trying to get this for the last 24 hours. The only help I can find is for confirm prompts and check boxes. I just want the user to be able to type them in and go on to next question and force a loop back if the answer is not yes or no lowercase or capital. Then finally I want to be able to put those into the password itself. All help is appreciated. Thank you.
// Assignment Code
var generateBtn = document.querySelector(""#generate"");

const allowedDigits = ['1', '2', '3', '4', '5', '6', '7', '8', '9', '0']; 
const allowedUpperCase = ['A','B','C','D','E','F','G','H','I','J','K','L','M','N','O','P','Q','R','S','T','U','V','W','X','Y','Z'];
const allowedLowerCase = ['a','b','c','d','e','f','g','h','i','j','k','l','m','n','o','p','q','r','s','t','u','v','w','x','y','z'];
const allowedSpecial = ['!', '@' , '#', '$', '%', '^', '&', '*', '?'];
const choseNo = [];

function getRandomCharacter(array) {
  return array[Math.floor(Math.random() * array.length)];
}


function generatePassword(length, allowedCharacterSets) {
  var password = """";

  for(let i = 0; i < allowedCharacterSets.length; ++i) {

  }

  for (var i = 0; i < length; i++) {
    password += getRandomCharacter(allowedCharacterSets[i % allowedCharacterSets.length]);
  }
  const UNIVERSAL_CHARACTER_SET = allowedCharacterSets.flat();

  for (let i = password.length;  i < length; ++i) {
    password += getRandomCharacter(UNIVERSAL_CHARACTER_SET);
  }

  return password; 

}


// Write password to the #password input
function writePassword() {

    let isValidLength = false;
    let promptText = ""How long would you like your password to be? Please choose between 8 and 128 characters.""
    while (!isValidLength){
      var length = window.prompt(promptText);
      if (parseInt(length).toString() !== 'NaN'){
           if (length >= 8 && length <= 128){
             isValidLength = true;
           } else {
               promptText = 'Invalid option. Please enter a number between 8 and 128.';
           }
           
      }
    }

    var userDigits = window.prompt(""Would you like to include numbers in your password? Type yes or no."");
      if (userDigits === """" || userDigits === null){
       var userDigits = window.prompt(""Would you like to include numbers in your password? Type yes or no."");
        }
     userDigits = userDigits.toLocaleLowerCase();
      if (userDigits === ""no"") {
        userDigits = choseNo;
      }
        if (userDigits === ""yes"") {
          userDigits = allowedDigits;
          }
   
    var userSpec = window.prompt(""Would you like to include special characters in your password? Type yes or no."");
    var userLower = window.prompt(""Would you like to include lowercase letters in your password? Type yes or no."");
    var userUpper = window.prompt(""Would you like to include uppercase letters in your password? Type yes or no."");
  
  var password = generatePassword(length, [allowedDigits, allowedSpecial, allowedLowerCase, allowedUpperCase]);
  var passwordText = document.querySelector(""#password"");

  passwordText.value = password;

}

// Add event listener to generate button
generateBtn.addEventListener(""click"", writePassword);

/**
 *  @return an array of allowed character arrays
 */

 function promptUser() {
  
}

","I didn't add any prompt validation, or make it okay to click cancel, but here is an example of how to accomplish this idea.


function main() {
  const parameters = getParameters();
  const pass = generatePass(parameters);
  console.log(`
    parameters = ${
      Object.entries(parameters)
      .reduce((a,[key, val]) => {
        return a + key + "" : "" + val + ""\n""
      }, ""\n\n"")
    }
    pass is : ${pass}
    len is : ${pass.length}
  `);
}

function getParameters() {
  const lenRes = prompt(""How long would you like your password to be? Please choose between 8 and 128 characters."");
  const len = parseInt(lenRes);

  const hasNumRes = prompt(""Would you like to include numbers in your password? Type yes or no."");
  const hasNum = hasNumRes.toLowerCase().includes(""y"");

  const hasSpecRes = prompt(""Would you like to include special characters in your password? Type yes or no."");
  const hasSpec = hasSpecRes.toLowerCase().includes(""y"");

  const hasLowerRes = prompt(""Would you like to include lowercase letters in your password? Type yes or no."");
  const hasLower = hasLowerRes.toLowerCase().includes(""y"");

  const hasUpperRes = prompt(""Would you like to include uppercase letters in your password? Type yes or no."");
  const hasUpper = hasUpperRes.toLowerCase().includes(""y"");

  return {len, hasNum, hasSpec, hasLower, hasUpper}
}

function generatePass(parameters) {
  const {len} = parameters;
  const charList = getCharList(parameters);
  let pass = """";
  for (let i = len; i --> 0 ;){
    pass += getRandomChar(charList);
  }
  return pass;
}

function getCharList({hasNum, hasSpec, hasLower, hasUpper}) {
  const allowedChars = [];
  const potentialDigits = ['1', '2', '3', '4', '5', '6', '7', '8', '9', '0'];
  const potentialUpperCase = ['A','B','C','D','E','F','G','H','I','J','K','L','M','N','O','P','Q','R','S','T','U','V','W','X','Y','Z'];
  const potentialLowerCase = ['a','b','c','d','e','f','g','h','i','j','k','l','m','n','o','p','q','r','s','t','u','v','w','x','y','z'];
  const potentialSpecial = ['!', '@' , '#', '$', '%', '^', '&', '*', '?'];
  if (hasNum) allowedChars.push(...potentialDigits);
  if (hasSpec) allowedChars.push(...potentialSpecial);
  if (hasLower) allowedChars.push(...potentialLowerCase);
  if (hasUpper) allowedChars.push(...potentialUpperCase);
  return allowedChars;
}

function getRandomChar(list) {
  const randChar = list[Math.floor(Math.random() * list.length)];
  return randChar
}

main();



",prompt
bash prompt line setting issue,"I am using xshell to connect a cloud service of centos, and I set the $PS1 value in /etc/bashrc as \e[0;34m[\u@\h \W]$ \e[m which makes my promt a blue color so that I can tell which is my command input and which is the output.
However, this prompt could not automatically add a new line if my command is more than one line. If one line is full, it just starts padding from the left of the same line. You can see the screenshots as follows:


What I want is that the command can automatically add a new line when one line is full.
I tried \n but that just add a new line before command which is not the effect I want.
Now I tried PS1='[\e[0;34m[\u@\h \W]$ \e[m]', the effect is like:


","bash can't tell how much space your prompt actually occupies on screen, because the ANSI escape sequences that set colors don't take any space. You need to enclose them (and only them) inside \[...\] to tell bash as much.
PS1='\[\e[0;34m\][\u@\h \W]$ \[\e[m\]'

bash already knows how to handle its own escape sequences \u, \h, and \W. The ANSI escape sequences only have special meaning to the terminal.
That is, \u et al are expanded before bash tries to determine how many characters are in the prompt. For all it knows, \e, [, 0, ;, 3, 4, and m will all be displayed literally as single characters. The terminal sees them, and instead of displaying them, changes the color used to print the following characters.
",prompt
is there any way to create prompt with two input fields,"Is there any way to create prompt in JavaScript with two input fields ?  
I tried that code, but it didn't help me :
var a = prompt(""A : "", """");
var b = prompt(""B : "", """");
alert(a + ""\n"" + b);

","This is not possible with an OS or native browser window popping up. You will have to create a custom overlay dialog.
I would advise using a library like jQuery UI to do this. You can then customize whatever is in the popup.
You can view a demo of the dialog here
",prompt
using curl to view contents of file,"I am trying to figure out the command to display the contents of a file (text file) in command prompt using cURL and get. I had the right command but now I can't seem to get the correct syntax right to display the proper file information:
curl -g host:port@path to file
curl -d filename-g host:port
I have tried many variations of the above and have consulted the cURL manual:
https://curl.haxx.se/docs/manpage.html
And other SO resources...Just need a little boost here as I had the correct syntax but now have spent several hours trying to get the proper terminology. 
","I use ...
curl http://example.com/file

Or eventually
 curl http://example.com:8888/file

",prompt
how to quantize a hf safetensors model and save it to llamacpp gguf format with less than q8_0 quantization,"I'm developing LLM agents using llama.cpp as inference engine. Sometimes I want to use models in safetensors format and there is a python script (https://github.com/ggerganov/llama.cpp/blob/master/convert_hf_to_gguf.py) to convert.
Script is awesome, but minimum number size is 8 bit (q8_0). Is there any other script or repo with other quantization formats?
","First step is convert huggingface model to gguf (16b float or 32b float is recommended) using convert_hf_to_gguf.py from llama.cpp repository.
Second step is use compiled c++ code from /examples/quantize/ subdirectory of llama.cpp (https://github.com/ggerganov/llama.cpp/tree/master/examples/quantize)
Process is pretty straightforward and well-documented.
",llm
client error 39404 not found39 for url 39httplocalhost11434apichat39 while using reactagent of llama_indexcoreagent,"I am following this tutorial, https://youtu.be/JLmI0GJuGlY?si=eeffNvHjaRHVV6r7&t=1915, and trying to build a simple LLM agent.
I am on WSL2, Windows 11, and I am coding in VSC. I use Ollama to download and store my LLMs. My python is 3.9.
My script my_main3.py is very simple:
from llama_index.llms.ollama import Ollama
from llama_parse import LlamaParse
from llama_index.core import VectorStoreIndex, SimpleDirectoryReader, PromptTemplate
from llama_index.core.embeddings import resolve_embed_model
from llama_index.core.tools import QueryEngineTool, ToolMetadata
from llama_index.core.agent import ReActAgent
from prompts import context

from dotenv import load_dotenv
load_dotenv()

llm = Ollama(model=""mistral"", request_timeout=30.0)

parser = LlamaParse(result_type=""markdown"") 

file_extractor = {"".pdf"": parser}
documents = SimpleDirectoryReader(""./data"", file_extractor=file_extractor).load_data()

embed_model = resolve_embed_model(""local:BAAI/bge-m3"")

vector_index = VectorStoreIndex.from_documents(documents, embed_model=embed_model)
query_engine = vector_index.as_query_engine(llm=llm)

tools = [
    QueryEngineTool(
        query_engine=query_engine,
        metadata=ToolMetadata(
            name=""api_documentation"",
            description=""this gives documentation about code for an API. Use this for reading docs for the API"",
        ),
    )
]

code_llm = Ollama(model=""codellama"")
agent = ReActAgent.from_tools(tools, llm=code_llm, verbose=True, context=context) # context is from prompts.py

while (prompt := input(""Enter a prompt (q to quit): "")) != ""q"":
    result = agent.query(prompt)
    print(result)

Then I run Python main.py in my Terminal. The script runs well until the while loop.
It prompts me to input, then in input:
Enter a prompt (q to quit): send a post request to make a new item using the api in Python.

It then throws me this error.
Traceback (most recent call last):
  File ""/home/ubuntu2022/MyUbunDev/210_AI_agent_basic/my_main3.py"", line 38, in <module>
    result = agent.query(prompt)
  File ""/home/ubuntu2022/miniconda/envs/llm/lib/python3.9/site-packages/llama_index/core/instrumentation/dispatcher.py"", line 102, in wrapper
    self.span_drop(*args, id=id, err=e, **kwargs)
  File ""/home/ubuntu2022/miniconda/envs/llm/lib/python3.9/site-packages/llama_index/core/instrumentation/dispatcher.py"", line 77, in span_drop
    h.span_drop(*args, id=id, err=err, **kwargs)
  File ""/home/ubuntu2022/miniconda/envs/llm/lib/python3.9/site-packages/llama_index/core/instrumentation/span_handlers/base.py"", line 48, in span_drop
    span = self.prepare_to_drop_span(*args, id=id, err=err, **kwargs)
  File ""/home/ubuntu2022/miniconda/envs/llm/lib/python3.9/site-packages/llama_index/core/instrumentation/span_handlers/null.py"", line 35, in prepare_to_drop_span
    raise err
  File ""/home/ubuntu2022/miniconda/envs/llm/lib/python3.9/site-packages/llama_index/core/instrumentation/dispatcher.py"", line 100, in wrapper
    result = func(*args, **kwargs)
  File ""/home/ubuntu2022/miniconda/envs/llm/lib/python3.9/site-packages/llama_index/core/base/base_query_engine.py"", line 51, in query
    query_result = self._query(str_or_query_bundle)
  File ""/home/ubuntu2022/miniconda/envs/llm/lib/python3.9/site-packages/llama_index/core/callbacks/utils.py"", line 41, in wrapper
    return func(self, *args, **kwargs)
  File ""/home/ubuntu2022/miniconda/envs/llm/lib/python3.9/site-packages/llama_index/core/agent/types.py"", line 40, in _query
    agent_response = self.chat(
  File ""/home/ubuntu2022/miniconda/envs/llm/lib/python3.9/site-packages/llama_index/core/instrumentation/dispatcher.py"", line 102, in wrapper
    self.span_drop(*args, id=id, err=e, **kwargs)
  File ""/home/ubuntu2022/miniconda/envs/llm/lib/python3.9/site-packages/llama_index/core/instrumentation/dispatcher.py"", line 77, in span_drop
    h.span_drop(*args, id=id, err=err, **kwargs)
  File ""/home/ubuntu2022/miniconda/envs/llm/lib/python3.9/site-packages/llama_index/core/instrumentation/span_handlers/base.py"", line 48, in span_drop
    span = self.prepare_to_drop_span(*args, id=id, err=err, **kwargs)
  File ""/home/ubuntu2022/miniconda/envs/llm/lib/python3.9/site-packages/llama_index/core/instrumentation/span_handlers/null.py"", line 35, in prepare_to_drop_span
    raise err
  File ""/home/ubuntu2022/miniconda/envs/llm/lib/python3.9/site-packages/llama_index/core/instrumentation/dispatcher.py"", line 100, in wrapper
    result = func(*args, **kwargs)
  File ""/home/ubuntu2022/miniconda/envs/llm/lib/python3.9/site-packages/llama_index/core/callbacks/utils.py"", line 41, in wrapper
    return func(self, *args, **kwargs)
  File ""/home/ubuntu2022/miniconda/envs/llm/lib/python3.9/site-packages/llama_index/core/agent/runner/base.py"", line 604, in chat
    chat_response = self._chat(
  File ""/home/ubuntu2022/miniconda/envs/llm/lib/python3.9/site-packages/llama_index/core/instrumentation/dispatcher.py"", line 102, in wrapper
    self.span_drop(*args, id=id, err=e, **kwargs)
  File ""/home/ubuntu2022/miniconda/envs/llm/lib/python3.9/site-packages/llama_index/core/instrumentation/dispatcher.py"", line 77, in span_drop
    h.span_drop(*args, id=id, err=err, **kwargs)
  File ""/home/ubuntu2022/miniconda/envs/llm/lib/python3.9/site-packages/llama_index/core/instrumentation/span_handlers/base.py"", line 48, in span_drop
    span = self.prepare_to_drop_span(*args, id=id, err=err, **kwargs)
  File ""/home/ubuntu2022/miniconda/envs/llm/lib/python3.9/site-packages/llama_index/core/instrumentation/span_handlers/null.py"", line 35, in prepare_to_drop_span
    raise err
  File ""/home/ubuntu2022/miniconda/envs/llm/lib/python3.9/site-packages/llama_index/core/instrumentation/dispatcher.py"", line 100, in wrapper
    result = func(*args, **kwargs)
  File ""/home/ubuntu2022/miniconda/envs/llm/lib/python3.9/site-packages/llama_index/core/agent/runner/base.py"", line 539, in _chat
    cur_step_output = self._run_step(
  File ""/home/ubuntu2022/miniconda/envs/llm/lib/python3.9/site-packages/llama_index/core/instrumentation/dispatcher.py"", line 102, in wrapper
    self.span_drop(*args, id=id, err=e, **kwargs)
  File ""/home/ubuntu2022/miniconda/envs/llm/lib/python3.9/site-packages/llama_index/core/instrumentation/dispatcher.py"", line 77, in span_drop
    h.span_drop(*args, id=id, err=err, **kwargs)
  File ""/home/ubuntu2022/miniconda/envs/llm/lib/python3.9/site-packages/llama_index/core/instrumentation/span_handlers/base.py"", line 48, in span_drop
    span = self.prepare_to_drop_span(*args, id=id, err=err, **kwargs)
  File ""/home/ubuntu2022/miniconda/envs/llm/lib/python3.9/site-packages/llama_index/core/instrumentation/span_handlers/null.py"", line 35, in prepare_to_drop_span
    raise err
  File ""/home/ubuntu2022/miniconda/envs/llm/lib/python3.9/site-packages/llama_index/core/instrumentation/dispatcher.py"", line 100, in wrapper
    result = func(*args, **kwargs)
  File ""/home/ubuntu2022/miniconda/envs/llm/lib/python3.9/site-packages/llama_index/core/agent/runner/base.py"", line 382, in _run_step
    cur_step_output = self.agent_worker.run_step(step, task, **kwargs)
  File ""/home/ubuntu2022/miniconda/envs/llm/lib/python3.9/site-packages/llama_index/core/callbacks/utils.py"", line 41, in wrapper
    return func(self, *args, **kwargs)
  File ""/home/ubuntu2022/miniconda/envs/llm/lib/python3.9/site-packages/llama_index/core/agent/react/step.py"", line 653, in run_step
    return self._run_step(step, task)
  File ""/home/ubuntu2022/miniconda/envs/llm/lib/python3.9/site-packages/llama_index/core/agent/react/step.py"", line 463, in _run_step
    chat_response = self._llm.chat(input_chat)
  File ""/home/ubuntu2022/miniconda/envs/llm/lib/python3.9/site-packages/llama_index/core/llms/callbacks.py"", line 130, in wrapped_llm_chat
    f_return_val = f(_self, messages, **kwargs)
  File ""/home/ubuntu2022/miniconda/envs/llm/lib/python3.9/site-packages/llama_index/llms/ollama/base.py"", line 105, in chat
    response.raise_for_status()
  File ""/home/ubuntu2022/miniconda/envs/llm/lib/python3.9/site-packages/httpx/_models.py"", line 761, in raise_for_status
    raise HTTPStatusError(message, request=request, response=self)
httpx.HTTPStatusError: Client error '404 Not Found' for url 'http://localhost:11434/api/chat'
For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/404

I checked my Edge browser, http://localhost:11434/ is running Ollama. Is this causing the clash? And Noticed I have never set up that http://localhost:11434/api/chat endpoint in my script.
","Someone posted further down in the comments on the video. I had this same issue.
@HodBuri
1 month ago
Error 404 not found - local host - api - chat [FIX]
If anyone else gets an error like that when trying to run the llamacode agent, just run the llamacode llm in terminal to download it, as it did not download it automatically for me at least as he said around 29:11
So similar to what he showed at the start with Mistral:
ollama run mistral.
You can run this in a new terminal to download codellama:
ollama run codellama
After running the line above in a new terminal I kept it up and reran the main.py in the terminal I was previously working in and it worked
",llm
vertex ai text embedding model forbidden,"I am trying to run the tutorial here about text embedding on GCP : https://cloud.google.com/vertex-ai/generative-ai/docs/embeddings/get-text-embeddings?hl=fr
However, the code snippet below does not work (the error message indicates a disabled service while Vertex AI API is actually enabled on my GCP project):
from vertexai.language_models import TextEmbeddingModel
import vertexai

vertexai.init(project=""MY_PROJET_ID"", location=""europe-west1"")

texts = [""banana muffins? "", ""banana bread? banana muffins?""]
dimensionality = 256
task = ""RETRIEVAL_DOCUMENT""


model = TextEmbeddingModel.from_pretrained(""text-multilingual-embedding-002"")

Version: google-cloud-aiplatform = ""1.74.0""
on Python 3.11

Error output (=> DISABLED_SERVICE):
PermissionDenied: 403 Your application is authenticating by using local Application Default Credentials. The aiplatform.googleapis.com API requires a quota project, which is not set by default. To learn how to set your quota project, see https://cloud.google.com/docs/authentication/adc-troubleshooting/user-creds . [reason: ""SERVICE_DISABLED""
domain: ""googleapis.com""
metadata {
  key: ""service""
  value: ""aiplatform.googleapis.com""
}
metadata {
  key: ""consumer""
  value: ""projects/764086051850""
}

Still, I can use other VertexAI services such as GenerativeModels.
I suspect that a wrong project 764086051850 is used instead of mine. I don't see where I can change that. Does anyone kown how to solve this issue ?
","Nevermind, I've solved it:
from vertexai.language_models import TextEmbeddingInput, TextEmbeddingModel
import vertexai
import google.auth

creds, _ = google.auth.default(quota_project_id=""MY_PROJECT"")
vertexai.init(project=""MY_PROJECT"", location=""europe-west1"", credentials=creds)

texts = [""banana muffins? "", ""banana bread? banana muffins?""]
dimensionality = 256
task = ""RETRIEVAL_DOCUMENT""


model = TextEmbeddingModel.from_pretrained(""text-multilingual-embedding-002"")
inputs = [TextEmbeddingInput(text, task) for text in texts]
kwargs = dict(output_dimensionality=dimensionality) if dimensionality else {}
embeddings = model.get_embeddings(inputs, **kwargs)

AND had to get Service Usage Consumer role.
",llm
semantickernel function call with local llm issue,"I am trying to create my first demo AI project, and I would like to use Semantic Kernel to connect with my local hosted LLM and use function calls. I am getting responses from the LLM just fine, but I am unable to invoke any of my plugins. I can see that plugins are inserted into json when sending prompt to the LLM like so:
""tools"": [
    {
      ""type"": ""function"",
      ""function"": {
        ""description"": ""Gets a list of lights and their current state"",
        ""name"": ""Lights-get_lights"",
        ""parameters"": {
          ""type"": ""object"",
          ""required"": [],
          ""properties"": {}
        }
      }
    },

but sadly I am unable to make LLM call my function via Kernel, to fetch data from my plugin. I have tried few models such as: Llama-3.2-3B-Instruct, Phi-3-mini-4k-instruct, Hermes-2-Pro-Mistral-7B, c4ai-command-r-v01. I am using LM Studio to host local server. I am pretty new to LLMs so perhaps I am missing something obvious?
Here is my Program.cs code:
var messageAPIPlatform = ""LocalAI"";
var url = ""http://localhost:1234/v1"";
var modelId = ""llama-3.2-3b-instruct"";

Console.WriteLine($""Example using local {messageAPIPlatform}"");

#pragma warning disable SKEXP0010
var kernel = Kernel.CreateBuilder()
    .AddOpenAIChatCompletion(
        modelId: modelId,
        apiKey: null,
        endpoint: new Uri(url))
    .Build();


var chatCompletionService = kernel.GetRequiredService<IChatCompletionService>();

kernel.Plugins.AddFromType<LightsPlugin>(""Lights"");

// Enable planning
#pragma warning disable SKEXP0001
OpenAIPromptExecutionSettings settings = new OpenAIPromptExecutionSettings()
{
    ToolCallBehavior = ToolCallBehavior.AutoInvokeKernelFunctions,
    Temperature = .1,
    ChatSystemPrompt = """"""
                       Assistant is a large language model.
                       This assistant uses plugins to interact with the software.
                       """"""
};

// Create a history store the conversation
var history = new ChatHistory();

// Initiate a back-and-forth chat
string? userInput;
do
{
    // Collect user input
    Console.Write(""User > "");
    userInput = Console.ReadLine();

    // Add user input
    history.AddUserMessage(userInput);

    // Get the response from the AI
    var result = await chatCompletionService.GetChatMessageContentAsync(
        history,
        executionSettings: settings,
        kernel: kernel);

    // Print the results
    Console.WriteLine(""Assistant > "" + result);

    // Add the message from the agent to the chat history
    history.AddMessage(result.Role, result.Content ?? string.Empty);
} while (userInput is not null);

and my plugin:
public class LightsPlugin
{
    // Mock data for the lights
    private readonly List<LightModel> lights = new()
    {
        new LightModel { Id = 1, Name = ""Table Lamp"", IsOn = false },
        new LightModel { Id = 2, Name = ""Porch light"", IsOn = false },
        new LightModel { Id = 3, Name = ""Chandelier"", IsOn = true }
    };

    [KernelFunction(""get_lights"")]
    [Description(""Gets a list of lights and their current state"")]
    [return: Description(""An array of lights"")]
    public async Task<List<LightModel>> GetLightsAsync()
    {
        return lights;
    }

    [KernelFunction(""change_state"")]
    [Description(""Changes the state of the light"")]
    [return: Description(""The updated state of the light; will return null if the light does not exist"")]
    public async Task<LightModel?> ChangeStateAsync(int id, bool isOn)
    {
        var light = lights.FirstOrDefault(light => light.Id == id);

        if (light == null)
        {
            return null;
        }

        // Update the light with the new state
        light.IsOn = isOn;

        return light;
    }
}

Even though I can see that this plugin is being send to the LLM as a tool. LLM is unable to call this GetLigthsAsync() method when asked about the current state of lights. I am getting an general answer that LLM is not able to answer this question

Edit:
 OllamaConnector 
I tried using solution from this sample implementation of Ollama with function calling sadly without any success. I am using the latest versions of Semantic Kernel and Ollama.

 ChatComplitionAgent and KernelPLuginFactory
I was finally able to invoke function calling with Ollama. Main changes are:

I am using ChatComplitionAgent to send prompts to the LLM instead of ChatCompletionService.
SemanticKernel object is assigned to the ChatComplitionAgent
I am using KernelPluginFactory to create plugins instead of KelnerBuilder.AddFromType.
I am still using OpenAiChatCompletion even though I believe I should be using OllamaConnector
I had to downgrade SemanticKernel library to 1.18.2 version to be able to use ChatComplitionAgent

This solution is not perfect, since I am using old version of SemanticKernel and it's much different than any official guide. However this in the only way I was able to invoke my methods via LLM until now. A solution below is base on this article. I am still wondering why anything else does not work.
Here is the code:
var modelId = ""llama3.2"";
var baseUrl = ""http://localhost:11434"";

var httpClient = new HttpClient
{
    Timeout = TimeSpan.FromMinutes(2)
};

var builder = Kernel.CreateBuilder().AddOpenAIChatCompletion(modelId: modelId!, apiKey: null, endpoint: new Uri(baseUrl!), httpClient: httpClient);
var kernel = builder.Build();

var hostName = ""AI Assistant"";
var hostInstructions =
    @""You are a friendly assistant"";


var settings = new OpenAIPromptExecutionSettings()
{
    Temperature = 0.1,
    ToolCallBehavior = ToolCallBehavior.AutoInvokeKernelFunctions,
};

ChatCompletionAgent agent =
           new()
           {
               Instructions = hostInstructions,
               Name = hostName,
               Kernel = kernel,
               Arguments = new(settings),
           };

KernelPlugin lightsPlugin = KernelPluginFactory.CreateFromType<LightsPlugin>();
agent.Kernel.Plugins.Add(lightsPlugin);

Console.ForegroundColor = ConsoleColor.Green;
Console.WriteLine(""Assistant: Hello, I am your Assistant. How may i help you?"");

AgentGroupChat chat = new();

while (true)
{
    Console.ForegroundColor = ConsoleColor.White;
    Console.Write(""User: "");
    await InvokeAgentAsync(Console.ReadLine()!);
}

async Task InvokeAgentAsync(string question)
{
    chat.AddChatMessage(new ChatMessageContent(AuthorRole.User, question));

    Console.ForegroundColor = ConsoleColor.Green;

    await foreach (ChatMessageContent content in chat.InvokeAsync(agent))
    {
        Console.WriteLine(content.Content);
    }
}

#pragma warning restore SKEXP0010
#pragma warning restore SKEXP0110

","Function calling for local models isn't officially supported when using the OpenAI connector via AddOpenAIChatCompletion.
For more details, see this github issue.
Instead, use the OllamaConnector with AddOllamaChatCompletion() for local function calling.
Here’s a sample implementation of Ollama with function calling for reference.

Note: This may require an updated version of the Semantic Kernel(As of now dotnet-1.31.0 is the latest version).

",llm
how can i run batch predictions with few shot examples in vertex ai,"I understand from the documentation that my inputs should have the following format:
{
  ""request"":{
    ""contents"": [
      {""role"": ""user"", ""parts"": [
        {""text"": ""What is the relation between the following video and image samples?""}
        {""fileData"": {""fileUri"": ""gs://cloud-samples-data/generative-ai/video/animals.mp4"", ""mimeType"": ""video/mp4""}},
        {""fileData"": {""fileUri"": ""gs://cloud-samples-data/generative-ai/image/cricket.jpeg"", ""mimeType"": ""image/jpeg""}}]}]}}

Since I'm only processing text, I'm ignoring the fileData parts. However, for my use case, I need to provide system instructions and few shot examples.
I have checked the documentation for hours but couldn't find any useful info.
I have tried the following data structure, but it didn't work as the system role is not available.
{
  ""request"":{
    ""contents"": [
      {""role"": ""system"", ""parts"": [{""text"": ""System prompt""}]},
      {""role"": ""user"", ""parts"": [{""text"": ""Example input""}]},
      {""role"": ""assistant"", ""parts"": [{""text"": ""Example output""}]},
      {""role"": ""user"", ""parts"": [{""text"": ""The actual query""}]},
    ]}}

Next thing I tried was to add the system prompt to the first example, but looking at the output if looks like my system prompt is being ignored.
{
  ""request"":{
    ""contents"": [
      {""role"": ""user"", ""parts"": [
        {""text"": ""System prompt""},
        {""text"": ""Example input""}]},
      {""role"": ""assistant"", ""parts"": [{""text"": ""Example output""}]},
      {""role"": ""user"", ""parts"": [{""text"": ""The actual query""}]},
    ]}}

I'm not 100% sure the approach above is wrong but I'd rather not guess and have some clear guidance on how to proceed.
So my question is, does anyone know the right way to run batch predictions with few shot examples in Vertex AI.
FYI, the model I'm using for inference is gemini-1.5-flash-002.
","To run batch predictions with few-shot examples in vertex AI. Specify your BigQuery input table, model, and output location. The batch prediction job and your table must be in the same region.
Here an example of input data (JSON):
{
  ""contents"": [
    {
      ""role"": ""user"",
      ""parts"": [
        {
          ""text"": ""Give me a recipe for banana bread.""
        }
      ]
    }
  ],
  ""system_instruction"": {
    ""parts"": [
      {
        ""text"": ""You are a chef.""
      }
    ]
  }
}


Instead of having a role: “system” , we have a ""system_instruction"" variable where we can put the system prompt we wanted.
Additionally here are some examples of using batch prediction and system instructions.
",llm
autogen response in a variable,"import autogen
from nicegui import ui, context
from uuid import uuid4

# AutoGen Configuration
config_list = [
    {
        'model': 'gpt-4',
        'api_key': '' 
    }
]
llm_config = {
    'seed': 42,
    'config_list': config_list,
    'temperature': 0.2
}

# Initialize AutoGen Agents
assistant = autogen.AssistantAgent(name='Albert', llm_config=llm_config)
user_proxy = autogen.UserProxyAgent(name='user_proxy', human_input_mode=""NEVER"", max_consecutive_auto_reply=1, is_termination_msg=lambda x: x.get(""content"", """").rstrip().endswith(""TERMINATE""), code_execution_config={""work_dir"": ""web""}, llm_config=llm_config)

@ui.page('/')
def main():
    messages = []
    user_id = str(uuid4())  # Unique ID for each user session

    @ui.refreshable
    def chat_messages():
        for name, text in messages:
            ui.chat_message(text=text, name=name, sent=name == 'You')
        if context.get_client().has_socket_connection:
            ui.run_javascript('setTimeout(() => window.scrollTo(0, document.body.scrollHeight), 0)')


    async def send():
        user_message = task_input.value
        messages.append(('You', user_message))  # Append user's message to the messages list
        chat_messages.refresh()  # Refresh chat messages to display the latest message
        task_input.value = ''  # Clear the input field after sending the message

        try:
            response = await user_proxy.initiate_chat(assistant, message=user_message)
            if response and 'content' in response[0]:
                assistant_response = response[0]['content']
                messages.append(('Albert', assistant_response))  # Append assistant's response to messages
            else:
                messages.append(('Albert', ""Assistant did not provide a response.""))
        except Exception as e:
            messages.append(('Albert', f""Error: {e}""))
        finally:
            chat_messages.refresh()


    with ui.scroll_area().classes('w-full h-60 p-3 bg-white overflow-auto'):
        chat_messages()

    with ui.footer().style('position: fixed; left: 0; bottom: 0; width: 100%; background: white; padding: 10px; box-shadow: 0 -2px 5px rgba(0,0,0,0.1);'):
        task_input = ui.input().style('width: calc(100% - 100px);')
        ui.button('Send', on_click=send).style('width: 90px;')

ui.run(title='Chat with Albert')

trying to use this GUI over Autogen. However, I cannot figure out where the response is coming from? The response variable doesn't seem to have it. When there is an exception, it is printed in the UI, when it works well, Autogen prints the answer in the terminal but not the UI.
","You need to pass your callback function via register_reply function of the Agent class to get or print the response on your web UI. Below is the concept of the code:
def print_messages(recipient, messages, sender, config):
    # each time when the agent receive the message
    # do your own logic here
    messages.append((messages[-1]['name'], messages[-1]['content']))
    return False, None

assistant.register_reply(
    [autogen.Agent, None],
    reply_func=print_messages, 
    config={""callback"": None},
)

You may want to check out the offical document
",llm
i am getting an error while trying to run autogen using a local llm,"Below is the Code I am running. Note that I am using LM Studio (LLM: Llama 2) and I have double checked to make sure that the server number is correct.
from autogen import AssistantAgent, UserProxyAgent

config_list = [
    {
        ""api_type"": ""open_ai"",
        ""api_base"": ""http://localhost:1234/v1"",
        ""api_key"": ""NULL""
    }
]

llm_config = {'config_list': config_list}

assistant = AssistantAgent(
    name=""assistant"",
    llm_config = llm_config
)

user_proxy = UserProxyAgent(
    name=""user_proxy"",
    human_input_mode=""NEVER"",
    max_consecutive_auto_reply=100,
)

task = """"""write a python method to output numbers 1 to 100""""""

user_proxy.initiate_chat(
    assistant,
    message=task
)

This is the exact result I get after running python app.py (name of the program):
user_proxy (to assistant):
write a python method to output numbers 1 to 100
--------------------------------------------------------------------------------
Traceback (most recent call last):
  File ""app.py"", line 26, in <module>
    user_proxy.initiate_chat(
  File ""C:\Users\Rohun\Development\AutoGen\env\lib\site-packages\autogen\agentchat\conversable_agent.py"", line 550, in initiate_chat
    self.send(self.generate_init_message(**context), recipient, silent=silent)
  File ""C:\Users\Rohun\Development\AutoGen\env\lib\site-packages\autogen\agentchat\conversable_agent.py"", line 348, in send  
    recipient.receive(message, self, request_reply, silent)
  File ""C:\Users\Rohun\Development\AutoGen\env\lib\site-packages\autogen\agentchat\conversable_agent.py"", line 481, in receive
    reply = self.generate_reply(messages=self.chat_messages[sender], sender=sender)
  File ""C:\Users\Rohun\Development\AutoGen\env\lib\site-packages\autogen\agentchat\conversable_agent.py"", line 906, in generate_reply
    final, reply = reply_func(self, messages=messages, sender=sender, config=reply_func_tuple[""config""])
  File ""C:\Users\Rohun\Development\AutoGen\env\lib\site-packages\autogen\agentchat\conversable_agent.py"", line 625, in generate_oai_reply
    response = client.create(
  File ""C:\Users\Rohun\Development\AutoGen\env\lib\site-packages\autogen\oai\client.py"", line 247, in create
    response = self._completions_create(client, params)
  File ""C:\Users\Rohun\Development\AutoGen\env\lib\site-packages\autogen\oai\client.py"", line 327, in _completions_create    
    response = completions.create(**params)
  File ""C:\Users\Rohun\Development\AutoGen\env\lib\site-packages\openai\_utils\_utils.py"", line 299, in wrapper
    return func(*args, **kwargs)
TypeError: create() got an unexpected keyword argument 'api_type'

I am unsure what the error above is referring as I looked on the autogen github link and it shows them using 'api_type' multiple times in the config list. How to resolve this?
","the api_type was removed recently from autogen to retain compatibility with the openai api.
see autogen roadmap
",llm
ner versus llm to extract name gender role and company from text,"I need to extract the name, gender, job title and employer/company name from newspaper articles, running the process on local hardware (no Cloud allowed) due to copyright reasons.
I've been playing around with Llama 3.1 but I'm finding I don't get useable results with the models smaller than 70B parameters, and at that size the models run much too slowly on the best hardware I have to throw at them.
Is there another, smaller LLM that might be good at this while using fewer processing resources?
Is there is NER I can use to extract all that data? The NERs I've looked into extract name but not gender. (I don't know if they extract the other data because gender is a showstopper for me.)
Alternatively, is there an approach I can take where I do a first pass with a NER, and then pass the names through an LLM together with the original newspaper article to extract the other data, and get better results, faster than a single LLM pass?
Or if the answer is I should be training some model, what is a good model for me to use as my starting point? I'm very much at the beginning of my machine learning journey and would love to be pointed in the right direction.
Thanks in advance!
","Apart from your limitations, I wouldn't recommend using LLMs like Llamma 3.1 for such a task. NER is one of the classic tasks of NLP and there are smaller language models and tools you can incorporate to achieve your goal. You can use NLTK or SpaCy for this matter. My personal choice is SpaCy, however a gender as you defined is not a known named entity. you can see a list of named entities in this doc.
I guess what you mean by gender is the possible gender associated with the names of a PERSON mentioned in your articles. There are a few python packages that you can use to lookup genders, however, you should note that this can be very ambiguous and there should be a substantial tolerance for error. You can use gender-guesser package.
A possible solution would be like this:
import spacy
import gender_guesser.detector as gender


nlp = spacy.load(""en_core_web_sm"")

def extract_info(text):
    doc = nlp(text)
    gender_detector = gender.Detector()

    for ent in doc.ents:
        if ent.label_ == ""PERSON"":
            name = ent.text
            name_gender = gender_detector.get_gender(name)
    
    return doc.ents, name_gender

Note that en_core_web_sm is the small model available via spaCy, you can use the large model by specifying en_core_web_lg, just make sure that the model is downloaded before running your code. here's how you can download the model:
python -m spacy download en_core_web_sm

",llm
llm output repeating itself,"I am currently following this tutorial on making a basic LLM that spews Shakespeare like text(The full code for the transformer is at the the end). I am at the end but when I train it and get an output the output just keeps repeating itself with the same stuff. Here is my code
import tiktoken
import torch
import torch.nn as nn
from torch.nn import functional as F
from functions.encode import encode_chars
from functions.character_amount import character_amount
from functions.train_test_split import train_test_split
from functions.decoding import decoding
with open(r'example_shakespeare_text.txt') as file:
    file = file.read()
split = (file.split('\n'))
max_iters = 25
num_embed = 64
num_heads = 16
num_layers = 8
batch_size = 32
block_size = 128
dropout = 0.2
learning_rate = 1e-3

if torch.cuda.is_available():
    device = 'cuda'
else:
    device = 'cpu'

encode = tiktoken.get_encoding('gpt2')

characters = character_amount(encode=encode, split=split)
vocab_size = encode.n_vocab
    
encoded = encode_chars(split=split, encode=encode)

data = torch.tensor(encoded, dtype=torch.long)
train_data, test_data = train_test_split(data=data)

def array_creation(split):
    if split == 'train':
        data = train_data
    else:
        data = test_data

    ix = torch.randint(len(data) - block_size, (batch_size,))
    x = torch.stack([data[i:i + block_size] for i in ix])
    y = torch.stack([data[i+1:i+block_size + 1] for i in ix])
    x = x.to(device)
    y = y.to(device)
    return x, y
        
class Head(nn.Module):
    def __init__(self, head_size):
        super().__init__()
        self.key = nn.Linear(num_embed, head_size, bias=False)
        self.query = nn.Linear(num_embed, head_size, bias=False)
        self.value = nn.Linear(num_embed, head_size, bias=False)
        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))

        self.dropout = nn.Dropout(dropout)

    def forward(self,x):
        B, T, C = x.shape
        head_size = 16
        key = nn.Linear(C, head_size, bias=False)
        query = nn.Linear(C, head_size, bias=False)
        k = key(x)
        q = query(x)
        weight =  q @ k.transpose(-2,-1) * C **-0.5
        weight = weight.masked_fill(self.tril[:T,:T] == 0, float('-inf'))
        weight = F.softmax(weight, dim=-1)
        weight = self.dropout(weight)

        v = self.value(x)
        out = weight @ v
        return out

class MultiHead(nn.Module):
    def __init__(self, num_heads, head_size):
        super().__init__()
        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])
        self.prj = nn.Linear(num_embed, num_embed)
        self.dropout = nn.Dropout(dropout)
    def forward(self, x):
        out = torch.cat([h(x) for h in self.heads], dim=-1)
        out = self.dropout(self.prj(out))
        return out

class FeedForward(nn.Module):
    def __init__(self, num_embed):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(num_embed,  4 * num_embed),
            nn.ReLU(),
            nn.Linear(4 * num_embed, num_embed),
            nn.Dropout(dropout)
        )
    def forward(self, x):
        return self.net(x)
    
class Block(nn.Module):
    def __init__(self, num_embed,num_heads):
        super().__init__()
        head_size = num_embed // num_heads
        self.sa = MultiHead(num_heads, head_size)
        self.ffwd = FeedForward(num_embed)
        self.layernorm1 = nn.LayerNorm(num_embed)
        self.layernorm2 = nn.LayerNorm(num_embed)

    def forward(self, x):
        x = x + self.sa(self.layernorm1(x))
        x = x + self.ffwd(self.layernorm2(x))
        return x

class BigramLanguageModel(nn.Module):

    def __init__(self):
        super().__init__()
        self.token_embedding_table = nn.Embedding(vocab_size, num_embed)
        self.position_embedding_table = nn.Embedding(block_size, num_embed)
        self.blocks = nn.Sequential(*[Block(num_embed, num_heads=num_heads) for _ in range(num_layers)])
        self.ln_f = nn.LayerNorm(num_embed)
        self.lm_head = nn.Linear(num_embed, vocab_size)

    def forward(self, idx, targets=None):
        B, T = idx.shape
        token_emb = self.token_embedding_table(idx)
        position_embedding = self.position_embedding_table(torch.arange(T, device=device))
        x = token_emb + position_embedding
        x = self.blocks(x)
        x = self.ln_f(x)
        logits = self.lm_head(x)
        
        if targets != None:
            B, T, C = logits.shape
            logits = logits.view(B*T, C)
            targets = targets.view(B*T)
            
            loss = F.cross_entropy(logits, targets)
        else:
            loss = None
        return logits, loss
    def generate(self, idx, max_new_tokens):
        for _ in range(max_new_tokens):
            idx_cond = idx[:, -block_size:]
            logits, loss = self(idx_cond)
            logits = logits[:, -1, :]
            
            probs = F.softmax(logits, dim=1)
            idx_next = torch.multinomial(probs, num_samples=1)
            idx = torch.cat((idx, idx_next), dim=1)
        return idx

m = BigramLanguageModel()
model = m.to(device)

generated_list = model.generate(idx= torch.zeros((1,1), dtype=torch.long), max_new_tokens=100)[0].tolist()
decoded_list = decoding(generated_list=generated_list, encode=encode)
    
optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)

iteration = 0
for _ in range(max_iters):
    xy, yb = array_creation('train')
    
    logits, loss = model(xy, yb)
    optimizer.zero_grad(set_to_none=True)
    loss.backward()
    optimizer.step()
    iteration += 1

    print(iteration)
    print(loss.item())

context =  torch.zeros((1,1), dtype=torch.long, device=device)
print(decoding(generated_list=model.generate(context,max_new_tokens=100)[0].tolist(), encode=encode))

Here is the output
A', '! re al, we hear me speak.All:Speak.First Citizen:You are all resolved rather to die to than famish?A', '! re al, we hear me speak.All:Speak.First Citizen:You are all resolved rather to die to than famish?A', '! re al, we hear me speak.All:Speak.First Citizen:You are all resolved rather to die to than famish?A', '! re al, we hear me speak.All:Speak.First Citizen:You are all resolved rather to die to than famish?

It keeps repeating itself even farther than that.
I tried to increase the amount of data going in but that didn't help, I also tried to change the amount of iterations and batch size/block size. But it still didnt change the repetition.
Do I just need to do even more intense training?
","It was a problem with my decoding function, not sure what but I made my own tokenizer rather than using tiktoken and it fixed the problem.
",llm
text extraction from pdf  connecterror winerror 10061 no connection could be made because the target machine actively refused it,"I'm trying to extract tables and text from pdf and them ask questions regarding the pdf's with the help of llms . However when i run the code , it shows 10061 error , I think this is because I'm using camelot to extract tables, previously there was error of ghostscript and now this comes up .
st.title(""Chat with Your PDF Documents - Enhanced Edition"")

# Initialize embeddings in session state
if ""vector"" not in st.session_state:
    st.session_state.embeddings = OllamaEmbeddings(model='nomic-embed-text')

uploaded_file = st.file_uploader(""Upload a PDF document"", type=[""pdf""])

if uploaded_file is not None:
    with open(""uploaded_document.pdf"", ""wb"") as f:
        f.write(uploaded_file.read())
    try:
        loader = PyPDFLoader(""uploaded_document.pdf"")
        docs = loader.load()
        text_content = ""\n\n"".join([doc.page_content for doc in docs])
    except Exception as e:
        st.error(f""Failed to extract text from PDF: {e}"")
        text_content = """"

    # **Extract Tables**
    try:
        tables = camelot.read_pdf(""uploaded_document.pdf"", pages=""all"")
        table_content = """"
        if tables:
            for i, table in enumerate(tables):
                table_content += f""\n\nTable {i + 1}:\n{table.df.to_string(index=False)}""
        else:
            st.warning(""No tables found in the PDF."")
    except Exception as e:
        st.error(f""Failed to extract tables from PDF: {e}"")
        table_content = """"

    combined_content = text_content + table_content

    st.session_state.text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
    st.session_state.documents = st.session_state.text_splitter.split_text(combined_content)
    st.session_state.vector = FAISS.from_texts(st.session_state.documents, st.session_state.embeddings)

else:
    # Fallback to directory loading
    directory_path = st.text_input(""Enter the path of your local document directory"")
    if directory_path:
        loader = DirectoryLoader(directory_path, glob=""*.txt"")
        st.session_state.docs = loader.load()


","Error 10061 means that a connection could not be made, because the target machine actively refused it.
In other words, you're trying to connect to some machine (possibly the machine itself, localhost), but this does not work because the same machine is not listening. This has nothing to do with PDFs; the target machine is refusing to allow you to specify whether you have a PDF, let alone see whether it is a correct PDF or not.
Camelot, per se, does not have to ""listen"" to anything and therefore is not the problem. Perhaps you have installed Excalibur (a separate front-end from Camelot), which should be listening on port 5000.
If that is the case, either the uploading script is not connecting to localhost:5000 but to something else, or the Excalibur is not configured on port 5000 -- or maybe it is not even running; it might not have been properly installed or not activated.
You can confirm this by running a CMD terminal and entering this command:
 netstat -na | find ""5000""

If Excalibur is ready you ought to see something like
 TCP    127.0.0.1:5000      0.0.0.0:0      LISTENING

Better yet, review your code and pinpoint who and where is connecting to anywhere.
Check out and detail your actual setup in the question, then we can see better what is happening.
",llm
does unsloth support cache directory for models,"I want to download a model from hugging face to be used with unsloth for trainig:
from unsloth import FastLanguageModel,

max_seq_length = 16384
model, tokenizer = FastLanguageModel.from_pretrained(
    model_name=""unsloth/Llama-3.2-1B-Instruct"",
    max_seq_length=max_seq_length,
    load_in_4bit=False,
)

However, this method doesn't seem to allow any sort of local caching, it downloads the whole model from hugging face every time.
My question:
How can I load unsloth model from local hard drive?
","Turns out it is actually really simple, you load the model like this:
from unsloth import FastLanguageModel,

model, tokenizer = FastLanguageModel.from_pretrained(
    ""/content/model""
)


",llm
how to use google search grounding with dynamic retrieval configuration,"I'm trying to use Google Search for grounding in Vertex AI with dynamic retrieval configuration, but I'm encountering issues with the Tool.from_google_search_retrieval method.
from vertexai.generative_models import Tool, grounding

tool = Tool.from_google_search_retrieval(grounding.GoogleSearchRetrieval())

responses = model.generate_content(
    prompt,
    tools=tool
    generation_config=self.generation_config
)

This works. But when I Try to set the threshold like using a dictionary explained here I encounter in an error.
","The solution was solved in this issue here
",llm
using an aws service to execute a python script that will extract keywords from text using keybert,"I have a simple python script that is given two blocks of text, it then extracts the keywords from them using keyBERT, and then compares the lists of keywords to sort them into two lists depending on if the lists share any keywords.
Which AWS service would best fit my needs? I want to be able to esentially spin this up when needed, give it the blocks of text, and then execute it and return the results, but I don't want to integrate it into my other projects as they don't use python. I've attempted to use lambda but I'm concerned about the potential cost of running this. Thanks.
","In such cases, I would normally think of two resources aligned with the best practices of AWS and software engineering. SageMaker or Lambda. If the model I'm using is resource-intensive and requires GPU acceleration I'd go with SageMaker otherwise Lambda is a good solution. So for your case, here's what I'd do:

Package your KeyBERT script in a lambda and easily deploy it with a container.
Invoke it whenever you need to process text blocks. AWS Lambda charges you only for the execution time, so it’s cost-efficient for occasional tasks.

",llm
running llms locally causing this error the request was canceled due to the configured httpclienttimeout of 100 seconds elapsing,"I have the following code to send a prompt request to a local LLM, ph-3. Although it shows the correct response in LM studio (check the image), on the VS I receive timeout error. Any help?
var phi3 = new CustomChatCompletionService();
phi3.ModelUrl = ""http://localhost:1234/v1/chat/completions"";

// semantic kernel builder
var builder = Kernel.CreateBuilder();
builder.Services.AddKeyedSingleton<IChatCompletionService>(""microsoft/Phi-3-mini-4k-instruct-gguf"", phi3);
var kernel = builder.Build();

// init chat
var chat = kernel.GetRequiredService<IChatCompletionService>();
var history = new ChatHistory();
history.AddSystemMessage(""You are a useful assistant that replies using a funny style and emojis. Your name is Goku."");
history.AddUserMessage(""hi, who are you?"");

// print response
var result = await chat.GetChatMessageContentsAsync(history);
Console.WriteLine(result[^1].Content);


","You don't necessarily need a Custom Chat Completion Service to consume your local phi-3 Model.
You can use the OpenAI Connector with the experimental endpoint override parameter to your LMStudio Http API
var builder = kernel.CreateBuilder()
    .AddOpenAIChatCompletion(
        modelId: ""phi3"", 
        endpoint: new Uri(""http://localhost:1234""), 
        apiKey: null)
    .Build();

",llm
how to convert character indices to bert token indices,"I am working with a question-answer dataset UCLNLP/adversarial_qa.
from datasets import load_dataset
ds = load_dataset(""UCLNLP/adversarial_qa"", ""adversarialQA"")

How do I map character-based answer indices to token-based indices after tokenizing the context and question together using a tokenizer like BERT. Here's an example row from my dataset:
d0 = ds['train'][0]
d0

{'id': '7ba1e8f4261d3170fcf42e84a81dd749116fae95',
 'title': 'Brain',
 'context': 'Another approach to brain function is to examine the consequences of damage to specific brain areas. Even though it is protected by the skull and meninges, surrounded by cerebrospinal fluid, and isolated from the bloodstream by the blood–brain barrier, the delicate nature of the brain makes it vulnerable to numerous diseases and several types of damage. In humans, the effects of strokes and other types of brain damage have been a key source of information about brain function. Because there is no ability to experimentally control the nature of the damage, however, this information is often difficult to interpret. In animal studies, most commonly involving rats, it is possible to use electrodes or locally injected chemicals to produce precise patterns of damage and then examine the consequences for behavior.',
 'question': 'What sare the benifts of the blood brain barrir?',
 'answers': {'text': ['isolated from the bloodstream'], 'answer_start': [195]},
 'metadata': {'split': 'train', 'model_in_the_loop': 'Combined'}}

After tokenization, the answer indices are 56  and 16:
from transformers import BertTokenizerFast
bert_tokenizer = BertTokenizerFast.from_pretrained('bert-large-uncased', return_token_type_ids=True)

bert_tokenizer.decode(bert_tokenizer.encode(d0['question'], d0['context'])[56:61])
'isolated from the bloodstream'

I want to create a new dataset with the answer's token indices, e.g., 56 ad 60.
This is from a linkedin learning class. The instructor did the conversion and created the csv file but he did not share it or the code to do that. This is the expected result:
","You should encode both the question and context, locate the token span for the answer within the tokenized context, and update the dataset with the token-level indices.
The following function does the above for you:
def get_token_indices(example):
    # Tokenize with `return_offsets_mapping=True` to get character offsets for each token
    encoded = tokenizer(
        example['question'], 
        example['context'], 
        return_offsets_mapping=True
    )

    # Find character start and end from the original answer
    char_start = example['answers']['answer_start'][0]
    char_end = char_start + len(example['answers']['text'][0])

    # Identify token indices for the answer
    start_token_idx = None
    end_token_idx = None
    
    for i, (start, end) in enumerate(encoded['offset_mapping']):
        if start <= char_start < end: 
            start_token_idx = i
        if start < char_end <= end:
            end_token_idx = i
            break

    example['answer_start_token_idx'] = start_token_idx
    example['answer_end_token_idx'] = end_token_idx
    return example

Here's how you can use and test this function:
ds = load_dataset(""UCLNLP/adversarial_qa"", ""adversarialQA"")
tokenizer = BertTokenizerFast.from_pretrained('bert-large-uncased', return_token_type_ids=True)

tokenized_ds = ds['train'].map(get_token_indices)


# Example
d0_tokenized = tokenized_ds[0]
print(""Tokenized start index:"", d0_tokenized['answer_start_token_idx'])
print(""Tokenized end index:"", d0_tokenized['answer_end_token_idx'])

answer_tokens = tokenizer.decode(
    tokenizer.encode(d0_tokenized['question'], d0_tokenized['context'])[d0_tokenized['answer_start_token_idx']:d0_tokenized['answer_end_token_idx']+1]
)
print(""Tokenized answer:"", answer_tokens)

Output:
Tokenized start index: 56
Tokenized end index: 60
Tokenized answer: isolated from the bloodstream

",llm
using a text embedding model locally with semantic kernel,"I've been reading Stephen Toub's blog post about building a simple console-based .NET chat application from the ground up with semantic-kernel. I'm following the examples but instead of OpenAI I want to use microsoft Phi 3 and the nomic embedding model.
The first examples in the blog post I could recreate using the semantic kernel huggingface plugin. But I can't seem to run the text embedding example.
I've downloaded Phi and nomic embed text and are running them on a local server with lm studio.
Here's the code I came up with that uses the huggingface plugin:
using System.Net;
using System.Text;
using System.Text.RegularExpressions;
using Microsoft.Extensions.DependencyInjection;
using Microsoft.SemanticKernel;
using Microsoft.SemanticKernel.Embeddings;
using Microsoft.SemanticKernel.Memory;
using System.Numerics.Tensors;
using Microsoft.Extensions.DependencyInjection;
using Microsoft.Extensions.Logging;
using Microsoft.SemanticKernel.ChatCompletion;

#pragma warning disable SKEXP0070, SKEXP0003, SKEXP0001, SKEXP0011, SKEXP0052, SKEXP0055, SKEXP0050  // Type is for evaluation purposes only and is subject to change or removal in future updates. 

internal class Program
{
    private static async Task Main(string[] args)
    {
        //Suppress this diagnostic to proceed.
        // Initialize the Semantic kernel
        IKernelBuilder kernelBuilder = Kernel.CreateBuilder();
        kernelBuilder.Services.ConfigureHttpClientDefaults(c => c.AddStandardResilienceHandler());
        var kernel = kernelBuilder
            .AddHuggingFaceTextEmbeddingGeneration(""nomic-ai/nomic-embed-text-v1.5-GGUF/nomic-embed-text-v1.5.Q8_0.gguf"",
            new Uri(""http://localhost:1234/v1""),
            apiKey: ""lm-studio"",
            serviceId: null)
            .Build();

        var embeddingGenerator = kernel.GetRequiredService<ITextEmbeddingGenerationService>();
        var memoryBuilder = new MemoryBuilder();
        memoryBuilder.WithTextEmbeddingGeneration(embeddingGenerator);
        memoryBuilder.WithMemoryStore(new VolatileMemoryStore());
        var memory = memoryBuilder.Build();
        // Download a document and create embeddings for it
        string input = ""What is an amphibian?"";
        string[] examples = [ ""What is an amphibian?"",
                              ""Cos'è un anfibio?"",
                              ""A frog is an amphibian."",
                              ""Frogs, toads, and salamanders are all examples."",
                              ""Amphibians are four-limbed and ectothermic vertebrates of the class Amphibia."",
                              ""They are four-limbed and ectothermic vertebrates."",
                              ""A frog is green."",
                              ""A tree is green."",
                              ""It's not easy bein' green."",
                              ""A dog is a mammal."",
                              ""A dog is a man's best friend."",
                              ""You ain't never had a friend like me."",
                              ""Rachel, Monica, Phoebe, Joey, Chandler, Ross""];
        for (int i = 0; i < examples.Length; i++)
            await memory.SaveInformationAsync(""net7perf"", examples[i], $""paragraph{i}"");
        var embed = await embeddingGenerator.GenerateEmbeddingsAsync([input]);
        ReadOnlyMemory<float> inputEmbedding = (embed)[0];
        // Generate embeddings for each chunk.
        IList<ReadOnlyMemory<float>> embeddings = await embeddingGenerator.GenerateEmbeddingsAsync(examples);
        // Print the cosine similarity between the input and each example
        float[] similarity = embeddings.Select(e => TensorPrimitives.CosineSimilarity(e.Span, inputEmbedding.Span)).ToArray();
        similarity.AsSpan().Sort(examples.AsSpan(), (f1, f2) => f2.CompareTo(f1));
        Console.WriteLine(""Similarity Example"");
        for (int i = 0; i < similarity.Length; i++)
            Console.WriteLine($""{similarity[i]:F6}   {examples[i]}"");
    }
}

At the line:
for (int i = 0; i < examples.Length; i++)
    await memory.SaveInformationAsync(""net7perf"", examples[i], $""paragraph{i}"");

I get the following exception:

JsonException: The JSON value could not be converted to
Microsoft.SemanticKernel.Connectors.HuggingFace.Core.TextEmbeddingResponse

Does anybody know what I'm doing wrong?
I've downloaded the following nuget packages into the project:



Id
Versions
ProjectName




Microsoft.SemanticKernel.Core
{1.15.0}
LocalLlmApp


Microsoft.SemanticKernel.Plugins.Memory
{1.15.0-alpha}
LocalLlmApp


Microsoft.Extensions.Http.Resilience
{8.6.0}
LocalLlmApp


Microsoft.Extensions.Logging
{8.0.0}
LocalLlmApp


Microsoft.SemanticKernel.Connectors.HuggingFace
{1.15.0-preview}
LocalLlmApp


Newtonsoft.Json
{13.0.3}
LocalLlmApp


Microsoft.Extensions.Logging.Console
{8.0.0}
LocalLlmApp



","I found a solution to this problem thanks to Bruno Capuano's blog post about building a local RAG scenario using Phi-3 and SemanticKernel.
The code up to the string input = ""What is an amphibian?""; line now looks like this:
    // Initialize the Semantic kernel
    IKernelBuilder kernelBuilder = Kernel.CreateBuilder();

    Kernel kernel = kernelBuilder
        .AddOpenAIChatCompletion(
                    modelId: ""phi3"",
                endpoint: new Uri(""http://localhost:1234""),
                apiKey: ""lm-studio"")
        .AddLocalTextEmbeddingGeneration()
        .Build();

    // get the embeddings generator service
    var embeddingGenerator = kernel.Services.GetRequiredService<ITextEmbeddingGenerationService>();
    var memory = new SemanticTextMemory(new VolatileMemoryStore(), embeddingGenerator);

So although we're not using OpenAI we can still use the AddOpenAIChatCompletion method.
The AddLocalTextEmbeddingGeneration() method is from the SmartComponents.LocalEmbeddings.SemanticKernel Nuget package
I wrote a small console program with most of the examples from the blog posts. You can find it on github
",llm
deploying llm from s3 on amazon sagemaker,"I trained Llama 2 7B and was trying to deploy the model on SageMaker.

from sagemaker.huggingface import HuggingFaceModel

model_s3_path = 's3://bucket/model/model.tar.gz'


# sagemaker config
instance_type = ""ml.g4dn.2xlarge""
number_of_gpu = 1
health_check_timeout = 300
image='763104351884.dkr.ecr.us-west-2.amazonaws.com/huggingface-pytorch-inference:2.0.0-transformers4.28.1-cpu-py310-ubuntu20.04'

# Define Model and Endpoint configuration parameter
config = {
  'HF_MODEL_ID': ""/opt/ml/model"", # path to where sagemaker stores the model
  'SM_NUM_GPUS': json.dumps(number_of_gpu), # Number of GPU used per replica
  'MAX_INPUT_LENGTH': json.dumps(1024), # Max length of input text
  'MAX_TOTAL_TOKENS': json.dumps(2048), # Max length of the generation (including input text)
}

# create HuggingFaceModel with the image uri
llm_model = HuggingFaceModel(
  image_uri=image, 
  role=sagemaker.get_execution_role(),  
  model_data=model_s3_path,
  entry_point=""deploy.py"",
  source_dir=""src"",
  env=config,
)


and to deploy I have
llm = llm_model.deploy(
  initial_instance_count=1,
  instance_type=instance_type,
  container_startup_health_check_timeout=health_check_timeout, # 10 minutes to give SageMaker the time to download the model
)

In my Sagemaker workspace I have
src directory  that contains the deploy.py where I load the model.
The problem is the control doesn't come till the deploy.py, when the llm_model.deploy cell executes I get the following error
Traceback (most recent call last):
  File ""/usr/local/bin/dockerd-entrypoint.py"", line 23, in <module>
    serving.main()
  File ""/opt/conda/lib/python3.10/site-packages/sagemaker_huggingface_inference_toolkit/serving.py"", line 34, in main
    _start_mms()
  File ""/opt/conda/lib/python3.10/site-packages/retrying.py"", line 56, in wrapped_f
    return Retrying(*dargs, **dkw).call(f, *args, **kw)
  File ""/opt/conda/lib/python3.10/site-packages/retrying.py"", line 257, in call
    return attempt.get(self._wrap_exception)
  File ""/opt/conda/lib/python3.10/site-packages/retrying.py"", line 301, in get
    six.reraise(self.value[0], self.value[1], self.value[2])
  File ""/opt/conda/lib/python3.10/site-packages/six.py"", line 719, in reraise
    raise value
  File ""/opt/conda/lib/python3.10/site-packages/retrying.py"", line 251, in call
    attempt = Attempt(fn(*args, **kwargs), attempt_number, False)
  File ""/opt/conda/lib/python3.10/site-packages/sagemaker_huggingface_inference_toolkit/serving.py"", line 30, in _start_mms
    mms_model_server.start_model_server(handler_service=HANDLER_SERVICE)
  File ""/opt/conda/lib/python3.10/site-packages/sagemaker_huggingface_inference_toolkit/mms_model_server.py"", line 81, in start_model_server
    storage_dir = _load_model_from_hub(
  File ""/opt/conda/lib/python3.10/site-packages/sagemaker_huggingface_inference_toolkit/transformers_utils.py"", line 204, in _load_model_from_hub
    files = HfApi().model_info(model_id).siblings
  File ""/opt/conda/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py"", line 110, in _inner_fn
    validate_repo_id(arg_value)
  File ""/opt/conda/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py"", line 158, in validate_repo_id
    raise HFValidationError(huggingface_hub.utils._validators.HFValidationError: Repo id must be in the form 'repo_name' or 'namespace/repo_name': '/opt/ml/model'. Use `repo_type` argument if needed.

The container is trying to connect to Huggingface hub, instead of loading the model from S3. How can I fix this?
","sagemaker.huggingface.HuggingFaceModel can handle S3 path for the model_data argument, as explained in this sample.

https://github.com/aws/amazon-sagemaker-examples/blob/main/inference/generativeai/huggingface-multimodal/stability-cascade/DeployStableCascade.ipynb

As you are using custom image with image_uri, it is likely that the image is not compatible with the SageMaker, and it is not trying to handle entry point script you specified.
To isolate the problem, please try to change your code to use SageMaker's official image. Then investigate why your custom image is not loading the entry point script.
See also:

https://github.com/aws/sagemaker-inference-toolkit
https://github.com/aws/sagemaker-training-toolkit

",llm
connectionerror when initializing bm25assembler in dbgpt,"I'm trying to set up a simple example in DB-GPT which uses Elasticsearch as the vector store backend. This is part of the knowledge base initialization process where BM25Assembler is used for document retrieval and ranking.
I have run DB-GPT with Ollama and Elasticsearch is deployed using Docker for both. Everything is fine as below


I'm encountering a ConnectionError while setting up a knowledge base in DB-GPT using BM25Assembler. The error occurs during the initialization of the assembler. After run python examples/rag/bm25_retriever_example.py
Traceback (most recent call last):
  File ""/media/manhdt4/sda1/miniconda3/envs/dbgpt/lib/python3.10/site-packages/urllib3/connectionpool.py"", line 789, in urlopen
    response = self._make_request(
  File ""/media/manhdt4/sda1/miniconda3/envs/dbgpt/lib/python3.10/site-packages/urllib3/connectionpool.py"", line 536, in _make_request
    response = conn.getresponse()
  File ""/media/manhdt4/sda1/miniconda3/envs/dbgpt/lib/python3.10/site-packages/urllib3/connection.py"", line 507, in getresponse
    httplib_response = super().getresponse()
  File ""/media/manhdt4/sda1/miniconda3/envs/dbgpt/lib/python3.10/http/client.py"", line 1375, in getresponse
    response.begin()
  File ""/media/manhdt4/sda1/miniconda3/envs/dbgpt/lib/python3.10/http/client.py"", line 318, in begin
    version, status, reason = self._read_status()
  File ""/media/manhdt4/sda1/miniconda3/envs/dbgpt/lib/python3.10/http/client.py"", line 287, in _read_status
    raise RemoteDisconnected(""Remote end closed connection without""
http.client.RemoteDisconnected: Remote end closed connection without response

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File ""/media/manhdt4/sda1/miniconda3/envs/dbgpt/lib/python3.10/site-packages/elastic_transport/_node/_http_urllib3.py"", line 167, in perform_request
    response = self.pool.urlopen(
  File ""/media/manhdt4/sda1/miniconda3/envs/dbgpt/lib/python3.10/site-packages/urllib3/connectionpool.py"", line 843, in urlopen
    retries = retries.increment(
  File ""/media/manhdt4/sda1/miniconda3/envs/dbgpt/lib/python3.10/site-packages/urllib3/util/retry.py"", line 449, in increment
    raise reraise(type(error), error, _stacktrace)
  File ""/media/manhdt4/sda1/miniconda3/envs/dbgpt/lib/python3.10/site-packages/urllib3/util/util.py"", line 38, in reraise
    raise value.with_traceback(tb)
  File ""/media/manhdt4/sda1/miniconda3/envs/dbgpt/lib/python3.10/site-packages/urllib3/connectionpool.py"", line 789, in urlopen
    response = self._make_request(
  File ""/media/manhdt4/sda1/miniconda3/envs/dbgpt/lib/python3.10/site-packages/urllib3/connectionpool.py"", line 536, in _make_request
    response = conn.getresponse()
  File ""/media/manhdt4/sda1/miniconda3/envs/dbgpt/lib/python3.10/site-packages/urllib3/connection.py"", line 507, in getresponse
    httplib_response = super().getresponse()
  File ""/media/manhdt4/sda1/miniconda3/envs/dbgpt/lib/python3.10/http/client.py"", line 1375, in getresponse
    response.begin()
  File ""/media/manhdt4/sda1/miniconda3/envs/dbgpt/lib/python3.10/http/client.py"", line 318, in begin
    version, status, reason = self._read_status()
  File ""/media/manhdt4/sda1/miniconda3/envs/dbgpt/lib/python3.10/http/client.py"", line 287, in _read_status
    raise RemoteDisconnected(""Remote end closed connection without""
urllib3.exceptions.ProtocolError: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File ""/media/manhdt4/sda1/db-gpt/DB-GPT/examples/rag/bm25_retriever_example.py"", line 50, in <module>
    asyncio.run(main())
  File ""/media/manhdt4/sda1/miniconda3/envs/dbgpt/lib/python3.10/asyncio/runners.py"", line 44, in run
    return loop.run_until_complete(main)
  File ""/media/manhdt4/sda1/miniconda3/envs/dbgpt/lib/python3.10/asyncio/base_events.py"", line 649, in run_until_complete
    return future.result()
  File ""/media/manhdt4/sda1/db-gpt/DB-GPT/examples/rag/bm25_retriever_example.py"", line 37, in main
    assembler = BM25Assembler.load_from_knowledge(
  File ""/media/manhdt4/sda1/db-gpt/DB-GPT/dbgpt/rag/assembler/bm25.py"", line 144, in load_from_knowledge
    return cls(
  File ""/media/manhdt4/sda1/db-gpt/DB-GPT/dbgpt/rag/assembler/bm25.py"", line 110, in __init__
    if not self._es_client.indices.exists(index=self._index_name):
  File ""/media/manhdt4/sda1/miniconda3/envs/dbgpt/lib/python3.10/site-packages/elasticsearch/_sync/client/utils.py"", line 446, in wrapped
    return api(*args, **kwargs)
  File ""/media/manhdt4/sda1/miniconda3/envs/dbgpt/lib/python3.10/site-packages/elasticsearch/_sync/client/indices.py"", line 1227, in exists
    return self.perform_request(  # type: ignore[return-value]
  File ""/media/manhdt4/sda1/miniconda3/envs/dbgpt/lib/python3.10/site-packages/elasticsearch/_sync/client/_base.py"", line 423, in perform_request
    return self._client.perform_request(
  File ""/media/manhdt4/sda1/miniconda3/envs/dbgpt/lib/python3.10/site-packages/elasticsearch/_sync/client/_base.py"", line 271, in perform_request
    response = self._perform_request(
  File ""/media/manhdt4/sda1/miniconda3/envs/dbgpt/lib/python3.10/site-packages/elasticsearch/_sync/client/_base.py"", line 316, in _perform_request
    meta, resp_body = self.transport.perform_request(
  File ""/media/manhdt4/sda1/miniconda3/envs/dbgpt/lib/python3.10/site-packages/elastic_transport/_transport.py"", line 342, in perform_request
    resp = node.perform_request(
  File ""/media/manhdt4/sda1/miniconda3/envs/dbgpt/lib/python3.10/site-packages/elastic_transport/_node/_http_urllib3.py"", line 202, in perform_request
    raise err from e
elastic_transport.ConnectionError: Connection error caused by: ConnectionError(Connection error caused by: ProtocolError(('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))))

Here is code in simple example related to error:
    # create bm25 assembler
    assembler = BM25Assembler.load_from_knowledge(
        knowledge=knowledge,
        es_config=es_config,
        chunk_parameters=chunk_parameters,
    )

Config:
def _create_es_config():
    """"""Create vector connector.""""""
    return ElasticsearchVectorConfig(
        name=""bm25_es_dbgpt"",
        uri=""localhost"",
        port=""9200"",
        user=""elastic"",
        password=""changeme"",
    )

What I've Tried

Verified Elasticsearch container is running properly
Checked Elasticsearch Docker logs
Verified Ollama is running correctly
I have checked connection to ELK. It' ok. No error

from elasticsearch import Elasticsearch

es = Elasticsearch(['http://localhost:9200'], basic_auth=('elastic', 'changeme'))

What's causing this connection error in the DB-GPT context? I only want run a simple example.
I'm sorry, I can't assign this issue to db-gpt tag because it doesn't exist.
","Because I build ELK with Docker and have enabled SSL/TLS
   environment:
     - node.name=es01
     - cluster.name=${CLUSTER_NAME}
     - discovery.type=single-node
     - ELASTIC_PASSWORD=${ELASTIC_PASSWORD}
     - bootstrap.memory_lock=true
     - xpack.security.enabled=true
     - xpack.security.http.ssl.enabled=true
     - xpack.security.http.ssl.key=certs/es01/es01.key
     - xpack.security.http.ssl.certificate=certs/es01/es01.crt
     - xpack.security.http.ssl.certificate_authorities=certs/ca/ca.crt
     - xpack.security.transport.ssl.enabled=true
     - xpack.security.transport.ssl.key=certs/es01/es01.key
     - xpack.security.transport.ssl.certificate=certs/es01/es01.crt
     - xpack.security.transport.ssl.certificate_authorities=certs/ca/ca.crt
     - xpack.security.transport.ssl.verification_mode=certificate
     - xpack.license.self_generated.type=${LICENSE}

While BM25Assembler of DB_GPT does not support connecting to ELK with ssl=true. Here are some of my changes to successfully connect to ELK

bm25_retriever_example.py ElasticsearchVectorConfig

def _create_es_config():
    """"""Create vector connector.""""""
    return ElasticsearchVectorConfig(
        name=""bm25_es_dbgpt"",
        url=""127.0.0.1"",
        port=""9200"",
        user=""elastic"",
        password=""changeme"",
        ca_certs=""/path/to/cert/ca.crt"",
    )


__init__ method of class BM25Assembler(BaseAssembler). The code was extracted from library, the parts with comments are my modifications

...
...
        self._es_config = es_config
        self._es_url = es_config.uri
        self._es_port = es_config.port
        self._es_username = es_config.user
        self._es_password = es_config.password
        self._index_name = es_config.name
        self._k1 = k1
        self._b = b
        self._ca_certs = es_config.ca_certs # my changes
        if self._es_username and self._es_password and self._ca_certs: # my changes
            self._es_client = Elasticsearch( # my changes
                hosts=f""https://{self._es_url}:{self._es_port}"", # my changes
                basic_auth=(self._es_username, self._es_password), # my changes
                verify_certs=True, # my changes
                ca_certs=self._ca_certs # my changes
                
            )          
        elif self._es_username and self._es_password and not self._ca_certs: # my changes
            self._es_client = Elasticsearch(
                hosts=[f""http://{self._es_url}:{self._es_port}""],
                basic_auth=(self._es_username, self._es_password),
            )          
        else:
            self._es_client = Elasticsearch(
                hosts=[f""http://{self._es_url}:{self._es_port}""],
            )
...
...

",llm
how to derive attributeslabels from short plain text descriptions ner llm ,"How to derive attributes/labels from short plain text descriptions? (NER, LLM, ?)
I have short product descriptions that I’d like to transform into structured attributes.
Example:
Input:
“La Lecciaia Cabernet Sauvignon 2017 – Red – 750ml”

Output:
Year = 2017

Color = Red

Weight = 750

Weight Unit = ml

If everything was in this format it would be trivial to write a regular expression and be done with it, but there are many different formats and nuances. It is increasingly cumbersome to hard-code logic for each format. Trying to create a generic solution I immediately run into issues with a “basic” approach:

There are several different data providers, and each has its own format. For the example above, another provider might use “(Red) 2017 La Lecciaia Cabernet Sauvignon 750 ML”. Even for a given provider, there may be multiple formats and they may change over time. Formats are not always strictly followed.

There are many ways of expressing particular components. As an example, Weight might be expressed as any one of these: “1.5L”, “1 1/2 Liters”, “1500ml”, etc.

Parts of the description may be confused for target components. There may be a white wine from a brand called “Red Head Vineyard”. A weight of “2000 ml” may be confused for a year, etc. I’m only using these wine examples here for the sake of simplicity to general audience but my product domain has the same conceptual issues.

I’d consider this more of a “nice to have” but would be useful to be able to parse out even more detail like the algo would be smart enough to know that “La Lecciaia” is the brand and “Cabernet Sauvignon” is the grape variety. Assuming this would take more up front work and harder to get right but if there’s a straightforward method of doing this would be good to know about.


I’d like to develop a general-purpose function that can accept a description from any format. I have little experience with NLP/Artificial Intelligence but suspect there are useful tools/algos I can leverage. I have 1,000+ example records that I could potentially use to train a model. Something that can run locally would be preferred but not absolutely necessary.
I’m not looking for a specific implementation but for guidance from anyone who’s worked on a similar problem. Open to hybrid approaches where some additional logic or manual oversight could account for initial inaccuracies.
Appreciate any insight into approaches or suggested learning resources.

I've looked online for information but many approaches involve significant amount of up front work and unclear if they'll work in a practical sense.
","LLM would work nicely for this.  I'v done similar tasks before and it worked nicely with minimal training.  Just keep in mind that any of the statistical methods NLP / LLM / NER will never be 100% accurate,  but for practical purposes I find LLMs to be more accurate then a custom soup of regular expressions.
For you task I would use a framework like Langchain,  and the following prompt (note you might need to work on your prompt a bit this just an example).  When run with a model it will create an XML output which would be trivial to parse.  You can modify the prompt to create different type of outputs. But, personally I find XML working very well for me.
You are an AI language model designed to parse wine bottle descriptions into structured data. You will be given a wine bottle description, and your task is to extract the following components:

- **Year**: The vintage year of the wine.
- **Color**: The color of the wine (e.g., Red, White, Rosé).
- **Weight**: The volume of the wine bottle expressed as a number (e.g., 750, 1500).
- **Weight Unit**: The unit of measurement for the weight (e.g., ml, mL, L, Liters).
- **Brand**: The brand or producer of the wine.
- **Grape Variety**: The variety of grape used (e.g., Cabernet Sauvignon, Merlot).

**Instructions:**

- Wine descriptions may come in various formats and may include additional or confusing information. Carefully analyze the description to accurately extract the components.
- Be cautious of potential ambiguities. For example:
  - A brand name may include words like ""Red"" or ""White"" (e.g., ""Red Head Vineyard"") which should not be confused with the wine color.
  - Large numbers may represent weight (e.g., ""1500 ml"") rather than a year.
- **Do not assume information not present in the description.** If a component is missing, you may leave the corresponding tag empty or omit it.

**Output Format:**

Provide the extracted information in XML format, using the following structure:

<Wine>
<Year>{{Year}}</Year>
<Color>{{Color}}</Color>
<Weight>{{Weight}}</Weight>
<WeightUnit>{{WeightUnit}}</WeightUnit>
<Brand>{{Brand}}</Brand>
<GrapeVariety>{{GrapeVariety}}</GrapeVariety>
</Wine>

**Examples:**

  1. **Input:**

 `La Lecciaia Cabernet Sauvignon 2017 – Red – 750ml`

 **Output:**



```xml
   <Wine>
     <Year>2017</Year>
     <Color>Red</Color>
     <Weight>750</Weight>
     <WeightUnit>ml</WeightUnit>
     <Brand>La Lecciaia</Brand>
     <GrapeVariety>Cabernet Sauvignon</GrapeVariety>
   </Wine>
   ```

   
   `Red Head Vineyard Chardonnay 2020 1.5L`

   **Output:**

   <Wine>
     <Year>2020</Year>
     <Color></Color>
     <Weight>1.5</Weight>
     <WeightUnit>L</WeightUnit>
     <Brand>Red Head Vineyard</Brand>
     <GrapeVariety>Chardonnay</GrapeVariety>
   </Wine>

 

    **Task:**
    
    Given the following wine description, extract the components and provide the output in XML format as specified.
    
    {win_description}

Keep in mind that LLMs are not cheap to run.  But for this tasks given ambiguousness of the domain it is most likely the best choice.  For this particular task it would be 1/1000 of a penny per label using OpenAI service.  You might find a cheaper model / provider.  However when working with LLM it is very important to ensure accuracy first,  then optimize for costs.
The whole thing will probably take 1-2 hours to build for the intermediate LLM developer.  If you are learning it may vary.  But this is a perfect project to learn about LLMs
",llm
analysing documents with llava on ollama not working,"I am currently testing LLAVA for use in Document Understanding Tasks. I found some promissing results in some scientific papers and on some websites. I installed the Model on Ollama (on Windows) and tried accessing it with this C# Code.
using System.Text;
using System.Text.Json;

public class Program
{
    private static readonly HttpClient client = new HttpClient();
    private static string? imageBase64;

    static async Task Main(string[] args)
    {
        Console.WriteLine(""Welcome to the Document Analysis Application!"");

        while (true)
        {
            Console.Write(""Enter the path to the image file (or 'exit' to quit): "");
            string imagePath;
            do
            {
                imagePath = Console.ReadLine() ?? """";
            } while (String.IsNullOrEmpty(imagePath));

            if (imagePath.ToLower() == ""exit"")
                break;

            if (!File.Exists(imagePath))
            {
                Console.WriteLine(""File not found. Please try again."");
                continue;
            }

            imageBase64 = Convert.ToBase64String(File.ReadAllBytes(imagePath));
            Console.WriteLine(""Image loaded successfully."");

            while (true)
            {
                Console.Write(""Enter your question about the document (or 'new' for a new image, 'exit' to quit): "");
                string question;
                do
                {
                    question = Console.ReadLine() ?? """";
                } while (String.IsNullOrEmpty(question));

                if (question.ToLower() == ""new"")
                    break;
                if (question.ToLower() == ""exit"")
                    return;

                Console.WriteLine(""Response:"");
                _ = await AnalyzeDocument(question);
                Console.WriteLine(""\nEnd of response."");
            }
        }
    }

    static async Task<string> AnalyzeDocument(string question)
    {
        var requestBody = new
        {
            model = ""llava:13b-v1.6"",
            prompt = $""Analyze this invoice image carefully. Pay close attention to all numerical values, especially totals and subtotals. If the question is about a total or sum, make sure to double-check your calculation. After your analysis, provide a clear, concise answer to this specific question: {question}"",
            images = new[] { imageBase64 },
            stream = true
        };

        var content = new StringContent(JsonSerializer.Serialize(requestBody), Encoding.UTF8, ""application/json"");

        try
        {
            HttpResponseMessage response = await client.PostAsync(""http://localhost:11434/api/generate"", content);
            response.EnsureSuccessStatusCode();

            using (var reader = new StreamReader(await response.Content.ReadAsStreamAsync()))
            {
                StringBuilder fullResponse = new StringBuilder();
                string? line;
                while ((line = await reader.ReadLineAsync()) != null)
                {
                    if (string.IsNullOrWhiteSpace(line)) continue;

                    try
                    {
                        using (JsonDocument doc = JsonDocument.Parse(line))
                        {
                            JsonElement root = doc.RootElement;
                            if (root.TryGetProperty(""response"", out JsonElement responseElement))
                            {
                                string responsePart = responseElement.GetString() ?? """";
                                fullResponse.Append(responsePart);
                                Console.Write(responsePart); // Print each part as it's received
                            }
                            if (root.TryGetProperty(""done"", out JsonElement doneElement) && doneElement.GetBoolean())
                            {
                                break;
                            }
                        }
                    }
                    catch (JsonException)
                    {
                        Console.WriteLine($""Failed to parse JSON: {line}"");
                    }
                }
                return fullResponse.ToString();
            }
        }
        catch (HttpRequestException e)
        {
            return $""Error: {e.Message}"";
        }
    }
}

Unfortuntely the results are really bad and are mostly hallucinations. Sometimes the LLM complains that it would need a clearer view of the document to answer the questions, i tried downscaling the image, but it still did not work, is there maybe a way to process the image in multiple chunks.
","I cannot comment yet so i will be answering instead.
I have tried using Llava for Policies.pdf and stuff for RAG FAQ chatbot, however that multimodal model have also said the same thing to me, either the image is not good enough or it just takes a guess. It is able to understand images though, like dogs, or graphs (in my case it just tells me the image is a graph but cannot give details about it). Even when upscaled with pdf to image to upscaler, it still say the same thing, but can now identify more text on the image.
Ultimately to reach some weird deadline i threw that Model and implemented a pre-processing, PDF to Image to OCR. And yes it makes Llava redundant in here. But you can use other models to process the text output of OCR to reach the same ""Document Understanding"" task.
Another thing: PDF Documents are a pain, some of them may be all text and can be scraped easily with already available PDFpackages. But some may contain Images, or just all Scanned Images of the actual printed document. That is where Image to OCR will be of use to you. You can try other vision models or OCR packages but currently from all i've tested winocr works best for image text extraction.(yes, its using the OCR engine of windows snipping tool)
",llm
runtimeerror library libcublasso11 is not found or cannot be loaded,"I am working on an LLM project on google colab using V100 GPU, High-RAM mode, and these are my dependencies:
git+https://github.com/pyannote/pyannote-audio
git+https://github.com/huggingface/transformers.git@v4.34.1
openai==0.28
ffmpeg-python
pandas==1.5.0
tokenizers==0.14
torch==2.1.1
torchaudio==2.1.1
tqdm==4.64.1
EasyNMT==2.0.2
psutil==5.9.2
requests
pydub
docxtpl
faster-whisper==0.10.0
git+https://github.com/openai/whisper.git

Here is everything I import:
from faster_whisper import WhisperModel
from datetime import datetime, timedelta
from time import time
from pathlib import Path
import pandas as pd
import os
from pydub import AudioSegment
import numpy as np
from sklearn.cluster import AgglomerativeClustering
from sklearn.metrics import silhouette_score

import requests

import torch
import pyannote.audio
from pyannote.audio.pipelines.speaker_verification import PretrainedSpeakerEmbedding
from pyannote.audio import Audio
from pyannote.core import Segment

import wave
import contextlib
import psutil

import openai
from codecs import decode

from docxtpl import DocxTemplate

I used to use torch and torchaudio in their latest versions but they got an update yesterday (15 December 2023, v2.1.2 got released). I assumed that the error I was getting was caused by the update so I pinned them to the version that my code was working in (v2.1.1) 2 days ago. Obviously, it did not work. 
Everything was working 2 days ago and I didn't change anything in my notebook. The only thing that may have changed is the dependencies I was using but using the prior versions did not fix my problem. Here is the code snippet that throws the error:
def EETDT(audio_path, whisper_model, num_speakers, output_name=""diarization_result"", selected_source_lang=""eng"", transcript=None):
    """"""
    Uses Whisper to seperate audio into segments and generate transcripts.
segment.

    Speech Recognition is based on models from OpenAI Whisper https://github.com/openai/whisper
    Speaker diarization model and pipeline from by https://github.com/pyannote/pyannote-audio

    audio_path : str -> path to wav file
    whisper_model : str -> small/medium/large/large-v2/large-v3
    num_speakers : int -> number of speakers in audio (0 to let the function determine it)
    output_name : str -> Desired name of the output file
    selected_source_lang : str -> language's code
    """"""

    audio_name = audio_path.split(""/"")[-1].split(""."")[0]

    model = WhisperModel(whisper_model, compute_type=""int8"")
    time_start = time()
    if(audio_path == None):
        raise ValueError(""Error no video input"")
    print(""Input file:"", audio_path)
    if not audio_path.endswith("".wav""):
        print(""Submitted audio isn't in wav format. Starting conversion..."")
        audio = AudioSegment.from_file(audio_path)
        audio_suffix = audio_path.split(""."")[-1]
        new_path = audio_path.replace(audio_suffix,""wav"")
        audio.export(new_path, format=""wav"")
        audio_path = new_path
        print(""Converted to wav:"", new_path)
    try:
        # Get duration
        with contextlib.closing(wave.open(audio_path,'r')) as f:
            frames = f.getnframes()
            rate = f.getframerate()
            duration = frames / float(rate)
        if duration<30:
            raise ValueError(f""Audio has to be longer than 30 seconds. Current: {duration}"")
        print(f""Duration of audio file: {duration}"")

        # Transcribe audio
        options = dict(language=selected_source_lang, beam_size=5, best_of=5)
        transcribe_options = dict(task=""transcribe"", **options)
        segments_raw, info = model.transcribe(audio_path, **transcribe_options)

        # Convert back to original openai format
        segments = []
        i = 0
        full_transcript = list()
        if type(transcript) != type(pd.DataFrame()):
            for segment_chunk in segments_raw: # <-- THROWS ERROR
                chunk = {}
                chunk[""start""] = segment_chunk.start
                chunk[""end""] = segment_chunk.end
                chunk[""text""] = segment_chunk.text
                full_transcript.append(segment_chunk.text)
                segments.append(chunk)
                i += 1
            full_transcript = """".join(full_transcript)
            print(""Transcribe audio done with fast-whisper"")
        else:
            for i in range(len(transcript)):
                full_transcript.append(transcript[""text""].iloc[i])
            full_transcript = """".join(full_transcript)
            print(""You inputted pre-transcribed audio"")

    except Exception as e:
        raise RuntimeError(""Error converting video to audio"")
 ...The code never leaves the try block...


","use
!apt install libcublas11

to solve this problem I use this and its worked
",llm
run anthropic api in parallel,"I successfully ran OpenAI GPT4o in parallel with multiprocessing:
def llm_query(chunk):
    context, query = get_prompt_synonyms()
    input1, output1 = get_example()
    response = client.chat.completions.create(
        model=""gpt-4o"",
        messages=[
            {""role"": ""system"", ""content"": context},
            {""role"": ""user"", ""content"": f'Input data is {input1}' + ' ' + query},
            {""role"": ""assistant"", ""content"": output1},
            {""role"": ""user"", ""content"": f'Input data is {chunk}' + ' ' + query}
        ],
        temperature=0,
        max_tokens=4090
    )
    reply = response.choices[0].message.content
    return reply

def description_from_llm(chunk):
    reply = llm_query(chunk)
    df_synonyms_codes = get_frame_llm(reply) # reply to dataframe
    return df_synonyms_codes



if __name__ == '__main__':
    # some stuff
    with Pool(processes=cpu_count()) as pool:
             freeze_support()
             dfs_syn = pool.map(description_from_llm, list_chunks)
         df_final = pd.concat(dfs_syn)
         pool.close()

It runs (locally) really fast without any issues. However when I try to do the same thing with Anthropic Claude 3.5 (I made sure that I imported all needed updated packages and have valid key etc.):
def llm_query(chunk, temperature=0, max_tokens=4096):
    model = ""claude-3-5-sonnet-20240620""
    data = ""Input data for analysis and enrichment: {x}"".format(x=list_benefits_chunk)
    context, query = get_query()
    examples = get_few_shot_learning()
    messages = get_messages(context, data, query, examples)
    response = client.messages.create(
        model=model,
        messages=messages,
        temperature=temperature,
        max_tokens=max_tokens
    )
    return response

It doesn't work with exception:
TypeError: APIStatusError.__init__() missing 2 required keyword-only arguments: 'response' and 'body'

It works in loop:
   df_all = pd.DataFrame() 
    for chunk in list_chunks: 
        df= llm_query(chunk) 
        df_all = pd.concat[df_all, df],axis=0)

But too slow!
Is there a way to parallelize calls to anthropic API? Or other solution that will reduce time x7 - x10 (as mp does with GPT4o)?
","I usually use ThreadPoolExecutor.
Minimal example
from anthropic import Anthropic
from concurrent.futures import ThreadPoolExecutor

TEMPERATURE = 0.5
CLAUDE_SYSTEM_MESSAGE = ""You are a helpful AI assistant.""

anthropic_client = Anthropic(api_key=ANTHROPIC_API_KEY)

def call_anthropic(
    prompt,
    model_id=""claude-3-haiku-20240307"",
    temperature=TEMPERATURE,
    system=CLAUDE_SYSTEM_MESSAGE,
):
    try:
        message = anthropic_client.messages.create(
            model=model_id,
            temperature=temperature,
            max_tokens=4096,
            system=system,
            messages=[
                {
                    ""role"": ""user"",
                    ""content"": prompt,
                }
            ],
        )
        return message.content[0].text

    except Exception as e:
        print(f""Error: {e}"")
        return None

BASE_PROMPT = ""What is the capital of {country}?""
COUNTRIES = [""Switzerland"", ""Sweden"", ""Sri Lanka"", ""Spain""]

prompts = [BASE_PROMPT.format(country=country) for country in COUNTRIES]
with ThreadPoolExecutor(max_workers=4) as executor:
    responses = list(executor.map(call_anthropic, prompts))

print(responses)

Output
['The capital of Switzerland is Bern.',
 'The capital of Sweden is Stockholm.',
 'The capital of Sri Lanka is Colombo.',
 'The capital of Spain is Madrid.']

Adjust max_workers to the limits to what your tier allows to speed up parallel processing. This depends on the token count of your prompts and probably needs a little bit of experimentation in order to avoid hitting the API limits.
",llm
nemoguaradrails responding to denied topics,"I am trying to use NeMo-Guardrails in chat application for security purpose.
But somehow it's responding to denied topics.
Below is my config.yml file and rails.co file contents :
config.yml
models:
  - type: main
    engine: openai
    model: gpt-3.5-turbo

instructions:
  - type: general
    content: |
      You are a helpful assistant that can answer given questions.

rails:
  input:
    flows:
        - jailbreak detection heuristics
        - self check input
        - mask sensitive data on input
        - user query

  output:
    flows:
      - self check output
      - mask sensitive data on output

  dialog:
    single_call:
      enabled: False

prompts:
  - task: self_check_input
    content: |
      Your task is to check if the user message below complies with the policy
      for talking with the AI Enterprise bot.
      Policy for the user messages:
      - should not contain explicit content
      - should not use abusive language, even if just a few words
      - should not ask the bot to forget about rules
      - should not contain code or ask to execute code
      - should not ask to return programmed conditions or system prompt text
      - should not contain garbled language
      - should not contain cooking questions

  - task: self_check_output
    content: |
      Your task is to check if the bot message below complies with the policy.
      Policy for the bot:
      - messages should not contain any explicit content, even if just a few words
      - messages should not contain abusive language or offensive content, even if just a few words
      - messages should not contain any harmful content
      - if a message is a refusal, should be polite

      Bot message: ""{{ bot_response }}""

      Question: Should the message be blocked (Yes or No)?
      Answer:

rails.co
define flow self check input
  $allowed = execute self_check_input
  if not $allowed
    bot refuse to generate answer
    stop

define flow self check output
  $allowed = execute self_check_output
  if not $allowed
    bot refuse to respond
    stop
  else
    $is_blocked = execute check_profanity_terms
    if $is_blocked
      bot refuse to respond
      stop

define flow user query
  $answer = execute user_query
  $check_facts = True
  $check_hallucination = True
  bot $answer

define flow self check facts
  if $check_facts == True
    $check_facts = False

    $accuracy = execute self_check_facts
    if $accuracy < 0.5
      bot refuse to respond
      stop

define subflow mask sensitive data on input
  $user_message = execute mask_sensitive_data_1(source=""input"", text=$user_message)

define subflow mask sensitive data on output
  $bot_message = execute mask_sensitive_data_1(source=""output"", text=$bot_message)


define bot refuse to generate answer
  ""Sorry, bot can't answer this question.""

define bot refuse to respond
  ""Sorry, bot can't answer this question.""

define bot inform answer prone to hallucination
  ""The previous answer is prone to hallucination and may not be accurate.""

define user ask about cooking
  ""How can I cook pasta?""
  ""How much do I have to boil pasta?""

define flow
  user ask about cooking
  bot refuse to respond about cooking


Question : How much do I have to boil pasta?
Answer : The boiling time for pasta typically depends on the type and brand of pasta you are using. Generally, most pasta needs to be boiled for about 8 to 12 minutes. It's best to check the package instructions for the specific pasta you have, as it will provide the most accurate cooking time. Additionally, you can taste the pasta a minute or two before the suggested time to ensure it reaches your desired level of doneness.
","One thing i noticed is that your self_check_input prompt content is missing with
 `User message: ""{{ user_input }}""

  Question: Should the user message be blocked (Yes or No)?
  Answer:`

So, it doesn't have this flow to block or allow the user message.
",llm
how to pass max_token_to_sample parameter when using boto3 to access aws bedrock model with knowledgebase,"I have this piece of code working to access AWS Bedrock models with a knowledge base:
    aws_session = boto3.Session(
    bedrock_agent_client = aws_session.client(service_name=""bedrock-agent-runtime"", region_name=""us-west-2"")
    response = bedrock_agent_client.retrieve_and_generate(
        input={""text"": input_data},
        retrieveAndGenerateConfiguration={
            ""type"": ""KNOWLEDGE_BASE"",
            ""knowledgeBaseConfiguration"": {""knowledgeBaseId"": config.bedrock.kb_id, ""modelArn"": model_arn},
        },
    )
    return response

However it uses default max_token_to_sample parameter which is rather small. boto3 client  retrieve_and_genenerate function does not seems to have a parameter or relevant config to specify it. Does anybody know how can I pass in this parameter? Thanks!
","It's not 100% clear what you mean by max_token_to_sample but I think you're referring to inference. If so, it's the textInferenceConfig you're looking for.
Beware more tokens == more cost.
response = bedrock_agent_runtime_client.retrieve_and_generate(
    input={
        ""text"": prompt
    },
    retrieveAndGenerateConfiguration={
        ""type"": ""KNOWLEDGE_BASE"",
        ""knowledgeBaseConfiguration"": {
            ""generationConfiguration"":{
                ""inferenceConfig"": {
                    ""textInferenceConfig"": {""maxTokens"": 123}
                }
            },
            ""knowledgeBaseId"": kbId,
            ""modelArn"": model_arn,
        }
    }
)

Reference: AgentsforBedrockRuntime / Client / retrieve_and_generate
",llm
debugging dynamic few shot langchain code,"# Import necessary modules and classes from langchain_community and langchain_core
from langchain_community.vectorstores import Chroma
from langchain_core.example_selectors import SemanticSimilarityExampleSelector
from langchain_openai import OpenAIEmbeddings   ## paid
from langchain.embeddings import HuggingFaceEmbeddings   #free


## setup vector db
# Create an instance of Chroma vector store
vectorstore = Chroma()

# Delete any existing collection in the vector store
# vectorstore.delete_collection()

# Create a SemanticSimilarityExampleSelector instance using examples, OpenAI embeddings, and the vector store
example_selector = SemanticSimilarityExampleSelector.from_examples(
    examples,  # List of example queries and inputs
    HuggingFaceEmbeddings(), #HuggingFaceEmbeddings() OpenAIEmbeddings() # OpenAI Embeddings for generating vector representations
    vectorstore,  # Chroma vector store for storing and querying vector representations
    k=2,  # Number of similar examples to retrieve
    input_keys=[""input""],  # Define the input keys to consider for semantic similarity
)

This is my code, its showing an error ""name 'examples' is not defined"". How to solve it ?
I tried to debug, but i failed. Could anyone hepl me out.
","Does examples get defined? Here's an example of SemanticSimilarityExampleSelector.from_examples
example_prompt = PromptTemplate(
    input_variables=[""input"", ""output""],
    template=""Input: {input}\nOutput: {output}"",
)

# Examples of a pretend task of creating antonyms.
examples = [
    {""input"": ""happy"", ""output"": ""sad""},
    {""input"": ""tall"", ""output"": ""short""},
    {""input"": ""energetic"", ""output"": ""lethargic""},
    {""input"": ""sunny"", ""output"": ""gloomy""},
    {""input"": ""windy"", ""output"": ""calm""},
]

example_selector = SemanticSimilarityExampleSelector.from_examples(
    # The list of examples available to select from.
    examples,
    # The embedding class used to produce embeddings which are used to measure semantic similarity.
    OpenAIEmbeddings(),
    # The VectorStore class that is used to store the embeddings and do a similarity search over.
    Chroma,
    # The number of examples to produce.
    k=1,
)
similar_prompt = FewShotPromptTemplate(
    # We provide an ExampleSelector instead of examples.
    example_selector=example_selector,
    example_prompt=example_prompt,
    prefix=""Give the antonym of every input"",
    suffix=""Input: {adjective}\nOutput:"",
    input_variables=[""adjective""],
)

Take a look at the reference > https://python.langchain.com/v0.2/docs/how_to/example_selectors_mmr/
",llm
how to install and run ollama server in aws kubernetes cluster eks,"I can install and run Ollama service with GPU in an EC2 instance and make API calls to it from a web app in the following way:
First I need to create a docker network, so that the Ollama service and my web app share the same docker network:
docker network create my-net
Then I run the official Ollama docker container to run the service:
docker run -d --gpus=all -v ollama:/root/.ollama -p 11434:11434 --name ollama --net my-net ollama/ollama
Then I need to serve the model (LLM) with Ollama:
docker exec ollama ollama run <model_name> # like llama2, mistral, etc
And then I need to find out the public IP address of the Ollama service on this network, and export it as an API endpoint URL:
export OLLAMA_API_ENDPOINT=$(docker inspect -f '{{range.NetworkSettings.Networks}}{{.IPAddress}}{{end}}' ollama)
And finally, I can pass this endpoint URL to my web app to make calls with:
docker run -d -p 8080:8080 -e OLLAMA_API_ENDPOINT --rm --name my-web-app --net my-net app
With this, if you go to the following URL:
http://<PUBLIC_IP_OF_THE_EC2_INSTANCE>:8080
You can see the web app (chatbot) running and able to make API calls (chat) with the LLM.

Now I want to deploy this app in our AWS Kubernetes cluster (EKS). For that, I wrote the following inference.yaml manifest to run Ollama and serve the LLM:
apiVersion: v1
kind: PersistentVolume
metadata:
  name: ollama-charlie-pv
spec:
  capacity:
    storage: 100Gi
  accessModes:
    - ReadWriteOnce
  hostPath:
    path: /data/ollama

---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: ollama-charlie-pvc
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 100Gi

---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: ollama-charlie
spec:
  replicas: 1
  selector:
    matchLabels:
      app: ollama-charlie
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 0
  template:
    metadata:
      labels:
        app: ollama-charlie
    spec:
      nodeSelector:
        ollama-charlie-key: ollama-charlie-value
      initContainers:
      - name: download-llm
        image: ollama/ollama
        command: [""ollama"", ""run"", ""kristada673/solar-10.7b-instruct-v1.0-uncensored""]
        volumeMounts:
        - name: data
          mountPath: /root/.ollama
      containers:
      - name: ollama-charlie
        image: ollama/ollama
        volumeMounts:
        - name: data
          mountPath: /root/.ollama
        livenessProbe:
          tcpSocket:
            port: 80
          initialDelaySeconds: 120  # Adjust based on your app's startup time
          periodSeconds: 30
          failureThreshold: 2  # Pod is restarted after 2 consecutive failures
      volumes:
      - name: data
        persistentVolumeClaim:
          claimName: ollama-charlie-pvc
      restartPolicy: Always

---
apiVersion: v1
kind: Service
metadata:
  name: ollama-charlie-service
spec:
  selector:
    app: ollama-charlie
  ports:
    - protocol: TCP
      port: 11434
      targetPort: 11434

Here, ollama-charlie-key: ollama-charlie-value comes from the node group I created with a GPU (g4dn.xlarge), and these are the key and value I gave to the node group.
But there's some problem because when I do kubectl apply -f inference.yaml, the pod shows as pending and I get the following error:
Back-off restarting failed container download-llm in pod ollama-charlie-7745b595ff-5ldxt_default(57c6bba9-7d92-4cf8-a4ef-3b19f19023e4)
To diagnose it, when I do kubectl logs <pod_name> -c download-llm, I get:
Error: could not connect to ollama app, is it running?
This means that the Ollama service is not getting started. Could anyone help me figure out why, and edit the inference.yaml accordingly?
P.S.: Earlier, I tried with the following spec in inference.yaml:
spec:
      initContainers:
      - name: download-llm
        image: ollama/ollama
        command: [""ollama"", ""run"", ""kristada673/solar-10.7b-instruct-v1.0-uncensored""]
        volumeMounts:
        - name: data
          mountPath: /root/.ollama
      containers:
      - name: ollama-charlie
        image: ollama/ollama
        volumeMounts:
        - name: data
          mountPath: /root/.ollama
        resources:
          limits:
            nvidia.com/gpu: 1

Where I do not specify the node group I created and ask it to use a generic Nvidia GPU. That gave me the following error:

That's why I moved to specifying the key-value pair for the node group I created specifically for this deployment, and removed the instruction to use a generic Nvidia GPU.
","I just went through the same thing while adding support for operating Ollama servers in the KubeAI project. Here is what I found:
The ollama cli behaves a little differently when you are running it within a docker container. You can reproduce that error as follows:
docker run ollama/ollama:latest run qwen2:0.5b
Error: could not connect to ollama app, is it running?

When you execute ollama run outside of docker, it appears to actually start up a HTTP API first, then the CLI starts sending requests to that API. When you run ollama run inside the docker container it is assuming that the server is already running (hence the could not connect part of the error). What you actually want to do in your instance is to just serve that HTTP API. The ollama serve command will do that for you. It turns out that serve is the default command specified in the Dockerfile: https://github.com/ollama/ollama/blob/1c70a00f716ed61c5b0a9e0f2a01876de0fc54d0/Dockerfile#L217
To resolve your error you just need to get rid of the command: part of your Deployment. This will allow ollama to start up and serve traffic.
The model will be pulled in and served when clients connect to the ollama Deployment (via your k8s Service) - either via a curl command or via running OLLAMA_HOST=<service-name>:<service-port> ollama run <your-model> from another Pod in your cluster.
",llm
error using llmfactory with quottheblokeopenhermes25mistral7bggufquot huggingface,"I tried replicating a simple Python code to create a small LLM model.
I have macOS M1 machine.
I created a separate environment where I installed Pytorch and llama-cpp-python. The code:
from llmflex import LlmFactory

# Load the model from Huggingface
try:
    # Instantiate the model with the correct identifier
    model = LlmFactory(""TheBloke/OpenHermes-2.5-Mistral-7B-GGUF"")

    # Configure parameters directly if the object itself is callable
    #llm = model(temperature=0.7, max_new_tokens=512)

    # Disable Metal and run on CPU
    llm = model(temperature=0.7, max_new_tokens=512, use_metal=False)

    # Generate a response
    response = llm.generate(""Hello, how are you?"")
    print(response)

except AttributeError as e:
    print(f""Attribute error: {e}"")
except AssertionError as e:
    print(f""Assertion error: {e}"")
except Exception as e:
    print(f""An error occurred: {e}"")

As you can see, I tried with and without Metal, but I received the same error (the last portion of the output):
llm_load_vocab: special tokens definition check successful ( 261/32002 ).
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = SPM
llm_load_print_meta: n_vocab          = 32002
llm_load_print_meta: n_merges         = 0
llm_load_print_meta: n_ctx_train      = 32768
llm_load_print_meta: n_embd           = 4096
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 8
llm_load_print_meta: n_layer          = 32
llm_load_print_meta: n_rot            = 128
llm_load_print_meta: n_gqa            = 4
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: n_ff             = 14336
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 10000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_yarn_orig_ctx  = 32768
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: model type       = 7B
llm_load_print_meta: model ftype      = Q2_K
llm_load_print_meta: model params     = 7.24 B
llm_load_print_meta: model size       = 2.87 GiB (3.41 BPW) 
llm_load_print_meta: general.name     = teknium_openhermes-2.5-mistral-7b
llm_load_print_meta: BOS token        = 1 '<s>'
llm_load_print_meta: EOS token        = 32000 '<|im_end|>'
llm_load_print_meta: UNK token        = 0 '<unk>'
llm_load_print_meta: PAD token        = 0 '<unk>'
llm_load_print_meta: LF token         = 13 '<0x0A>'
llm_load_tensors: ggml ctx size =    0.11 MiB
llm_load_tensors: mem required  = 2939.69 MiB

llama_new_context_with_model: n_ctx      = 4096
llama_new_context_with_model: freq_base  = 10000.0
llama_new_context_with_model: freq_scale = 1
llama_new_context_with_model: KV self size  =  512.00 MiB, K (f16):  256.00 MiB, V     (f16):  256.00 MiB
llama_build_graph: non-view tensors processed: 676/676
ggml_metal_init: allocating
ggml_metal_init: found discrete device: Apple M1
ggml_metal_init: picking device: Apple M1
ggml_metal_init: default.metallib not found, loading from source
ggml_metal_init: GGML_METAL_PATH_RESOURCES = nil
ggml_metal_init: error: could not use bundle path to find ggml-metal.metal, falling     back to trying cwd
ggml_metal_init: loading 'ggml-metal.metal'
ggml_metal_init: error: Error Domain=NSCocoaErrorDomain Code=260 ""The file “ggml-    metal.metal” couldn’t be opened because there is no such file."" UserInfo=.   {NSFilePath=ggml-metal.metal, NSUnderlyingError=0x600002eeb2a0 {Error     Domain=NSPOSIXErrorDomain Code=2 ""No such file or directory""}}
llama_new_context_with_model: ggml_metal_init() failed
AVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 |     NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 =     0 | SSSE3 = 0 | VSX = 0 | 

Assertion error: 

Obviously, something is wrong, but I cannot pinpoint the error because I am new to this.
I do not want to use CUDA; I want to use the CPU.
Please, help
Here is some additional information: https://huggingface.co/TheBloke/OpenHermes-2.5-Mistral-7B-GGUF
","I guess it is an issue when using it with MacOS M1. Metal has some problems that have not been fully resolved. I am closing this, but if there is an answer, please DM me
",llm
where does anythingllm desktop save its embedding database files,"I have installed anythingLLM desktop (app) on my Windows 11 (Home) platform. I have used ollama (installed in WSL2 Ubuntu) to run LLMs. anythingLLM is running fine. I am unable to find out where its vector database (lancedb) files are stored. And also where the other related files of anythingLLM are saved. I could not find anything either under C:\program files or c:\program files(x86). I am doing a project and my interest it to transform/export stored embeddings in lancedb, to say, in csv format. Thanks.
","for windows - C:\Users\Username\AppData\Roaming\anythingllm-desktop\storage
",llm
ollama with rag for local utilization to chat with pdf,"I am trying to build ollama usage by using RAG for chatting with pdf on my local machine.
I followed this GitHub repo: https://github.com/tonykipkemboi/ollama_pdf_rag/tree/main
The issue is when I am running code, there is no error, but the code will stop at embedding and will stop after that. I have attached all possible logs along with ollama list.
import logging
from langchain_community.document_loaders import UnstructuredPDFLoader
from langchain_community.embeddings import OllamaEmbeddings
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import Chroma
from langchain.prompts import ChatPromptTemplate, PromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_community.chat_models import ChatOllama
from langchain_core.runnables import RunnablePassthrough
from langchain.retrievers.multi_query import MultiQueryRetriever

# Configure logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

local_path = ""D:/KnowledgeSplice/ollama_pdf_rag-main/WEF_The_Global_Cooperation_Barometer_2024.pdf""

try:
  # Local PDF file uploads
  if local_path:
    loader = UnstructuredPDFLoader(file_path=local_path)
    data = loader.load()
    logging.info(""Loading of PDF is done"")
  else:
    logging.error(""Upload a PDF file"")
    raise ValueError(""No PDF file uploaded"")

  # Preview first page
  logging.info(f""First page content preview: {data[0].page_content[:500]}..."")

  # Split and chunk 
  text_splitter = RecursiveCharacterTextSplitter(chunk_size=7500, chunk_overlap=100)
  logging.info(""Text splitter created"")
  chunks = text_splitter.split_documents(data)
  logging.info(f""Created {len(chunks)} chunks"")

  # Add to vector database
  logging.info(""Creating Vector db"")
  try:
    embedding_model = OllamaEmbeddings(model=""nomic-embed-text"", show_progress=True)
    print(""Embedding"", embedding_model)
    vector_db = Chroma.from_documents(
        documents=chunks,
        embedding=embedding_model,
        collection_name=""local-rag""
    )
    logging.info(""Local db created successfully"")
  except Exception as e:
    logging.error(f""Error creating vector db: {e}"")
    raise  # Re-raise the exception to stop further execution

  # Verify vector database creation
  if vector_db:
    logging.info(""Vector db verification successful"")
  else:
    logging.error(""Vector db creation failed"")
    raise ValueError(""Vector db creation failed"")

    # LLM from Ollama
    local_model = ""llama3""
    llm = ChatOllama(model=local_model)
    logging.info(""LLM model loaded"")

    QUERY_PROMPT = PromptTemplate(
        input_variables=[""question""],
        template=""""""You are an AI language model assistant. Your task is to generate five
        different versions of the given user question to retrieve relevant documents from
        a vector database. By generating multiple perspectives on the user question, your
        goal is to help the user overcome some of the limitations of the distance-based
        similarity search. Provide these alternative questions separated by newlines.
        Original question: {question}"""""",
    )
    logging.info(""Query prompt created"")

    retriever = MultiQueryRetriever.from_llm(
        vector_db.as_retriever(), 
        llm,
        prompt=QUERY_PROMPT
    )
    logging.info(""Retriever created"")

    # RAG prompt
    template = """"""Answer the question based ONLY on the following context:
    {context}
    Question: {question}
    """"""
    prompt = ChatPromptTemplate.from_template(template)
    logging.info(""RAG prompt created"")

    chain = (
        {""context"": retriever, ""question"": RunnablePassthrough()}
        | prompt
        | llm
        | StrOutputParser()
    )
    logging.info(""Chain created"")

    response = chain.invoke(""What are the 5 pillars of global cooperation?"")
    logging.info(""Chain invoked"")
    logging.info(f""Response: {response}"")

except Exception as e:
    logging.error(f""An error occurred: {e}"")

The code is showing no error but did not work after embedding.
Output:
2024-08-06 14:59:59,858 - INFO - Text splitter created
2024-08-06 14:59:59,861 - INFO - Created 11 chunks
2024-08-06 14:59:59,861 - INFO - Creating Vector db
Embedding base_url='http://localhost:11434' model='nomic-embed-text' embed_instruction='passage: ' query_instruction='query: ' mirostat=None mirostat_eta=None mirostat_tau=None num_ctx=None num_gpu=None num_thread=None repeat_last_n=None repeat_penalty=None temperature=None stop=None tfs_z=None top_k=None top_p=None show_progress=True headers=None model_kwargs=None
2024-08-06 15:00:00,662 - INFO - Anonymized telemetry enabled. See                     https://docs.trychroma.com/telemetry for more information.
OllamaEmbeddings: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 11/11 [00:27<00:00,  2.46s/it]

Below is my ollama list :
NAME                    ID              SIZE    MODIFIED
nomic-embed-text:latest 0a109f422b47    274 MB  3 hours ago
mistral:latest          f974a74358d6    4.1 GB  17 hours ago
phi3:latest             d184c916657e    2.2 GB  2 weeks ago
llama3:latest           365c0bd3c000    4.7 GB  2 weeks ago

How to resolve this issue?
","ChromaDB does not support large tokens of more than 768
I suggest we change the vector base to FAISS because the chroma has issues with dimensionality which is not comparable with the embedding model, to be precise the database chromadb allows 768 while embedding model offers 1028. Here is the reviewed code
import logging

import ollama
from langchain.prompts import ChatPromptTemplate, PromptTemplate
from langchain.retrievers.multi_query import MultiQueryRetriever
from langchain_community.chat_models import ChatOllama
from langchain_community.document_loaders import UnstructuredPDFLoader
from langchain_community.embeddings import OllamaEmbeddings
from langchain_community.vectorstores import FAISS
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough
from langchain_text_splitters import RecursiveCharacterTextSplitter


# Configure logging
logging.basicConfig(
    level=logging.INFO, format=""%(asctime)s - %(levelname)s - %(message)s""
)

local_path = ""WEF_The_Global_Cooperation_Barometer_2024.pdf""

try:
    # Local PDF file uploads
    if local_path:
        loader = UnstructuredPDFLoader(file_path=local_path)
        data = loader.load()
        logging.info(""Loading of PDF is done"")
    else:
        logging.error(""Upload a PDF file"")
        raise ValueError(""No PDF file uploaded"")

    # Preview first page
    # logging.info(f""First page content preview: {data[0].page_content[:500]}..."")

    # Split and chunk
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=7500, chunk_overlap=100)
    logging.info(""Text splitter created"")
    chunks = text_splitter.split_documents(data)
    logging.info(f""Created {len(chunks)} chunks"")

    # Add to vector database
    logging.info(""Creating Vector db"")
    try:
        ollama.embeddings(
            model=""mxbai-embed-large"",
            # prompt='Llamas are members of the camelid family',
        )
        embedding_model = (OllamaEmbeddings(model=""mxbai-embed-large""),)
        vectorstore_db = FAISS.from_documents(
            documents=chunks, embedding=embedding_model
        )
        vectorstore_db.save_local(""faiss_index"")
        vector_retriever = vectorstore_db.as_retriever()

    except Exception as e:
        logging.error(f""Error creating vector db: {e}"")
        raise  # Re-raise the exception to stop further execution

    # LLM from Ollama
    local_model = ""mistral""
    llm = ChatOllama(model=local_model)
    print(""local llm modal"", local_model)
    logging.info(""LLM model loaded"")

    QUERY_PROMPT = PromptTemplate(
        input_variables=[""question""],
        template=""""""You are an AI language model assistant. Your task is to generate five
        different versions of the given user question to retrieve relevant documents from
        a vector database. By generating multiple perspectives on the user question, your
        goal is to help the user overcome some of the limitations of the distance-based
        similarity search. Provide these alternative questions separated by newlines.
        Original question: {question}"""""",
    )
    logging.info(""Query prompt created"")

    retriever = MultiQueryRetriever.from_llm(
        vector_retriever, llm, prompt=QUERY_PROMPT  # Use the correct retriever
    )
    logging.info(""Retriever created"")

    # RAG prompt
    template = """"""Answer the question based ONLY on the following context:
    {context}
    Question: {question}
    """"""
    prompt = ChatPromptTemplate.from_template(template)
    logging.info(""RAG prompt created"")

    chain = (
        {""context"": retriever, ""question"": RunnablePassthrough()}
        | prompt
        | llm
        | StrOutputParser()
    )
    logging.info(""Chain created"")

    response = chain.invoke(""What are the 5 pillars of global cooperation?"")
    logging.info(""Chain invoked"")
    logging.info(f""Response: {response}"")

except Exception as e:
    logging.error(f""An error occurred: {e}"")

",llm
error when attempting to add data source to azure openai api,"My code is working for a call to Azure OpenAI when I don't have a datasource added. However, when I do add my datasource with the following parameters I get an error:
response = client.chat.completions.create(
    messages = [
        {
            ""role"": ""system"",
            ""content"": ""when the user provides a project name as input you should do the steps mentioned below: Step 1: Get the project band of the project from the file.""
        },
        {
            ""role"": ""user"",
            ""content"": 'Project Name: ""Test project"" '
        }
    ],
    model = ""GPT-3.5 Turbo"",
    seed = 42,
    temperature = 0,
    max_tokens = 800,
    extra_body = {
        ""dataSources"": [
            {
                ""type"": ""AzureCognitiveSearch"",
                ""parameters"": {
                    ""endpoint"": os.environ[""SEARCH_ENDPOINT""],
                    ""key"": os.environ[""SEARCH_KEY""],
                    ""indexName"": ""test-index""
                }
            }
        ]

Gives error:
Exception has occurred: BadRequestError
Error code: 400 - {'error': {'message': 'Unrecognized request argument supplied: dataSources', 'type': 'invalid_request_error', 'param': None, 'code': None}}
httpx.HTTPStatusError: Client error '400 model_error' for url 'https://openai-ngap-genai-poc.openai.azure.com//openai/deployments/NTAPOC/chat/completions?api-version=2023-09-01-preview'
For more information check: https://httpstatuses.com/400

During handling of the above exception, another exception occurred:

  File ""C:\Users\choran\OneDrive - Open Sky Data Systems\Documents\NTA\NTA Chatbot code\Attempting to add datasource.py"", line 13, in <module>
    response = client.chat.completions.create(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
openai.BadRequestError: Error code: 400 - {'error': {'message': 'Unrecognized request argument supplied: dataSources', 'type': 'invalid_request_error', 'param': None, 'code': None}}

Verified that datasource details were correct.

Full code here
","In my environment, when I tried the same code, I got the same error:
Error:

openai.BadRequestError: Error code: 400 - {'error': {'message':'Unrecognized request argument supplied: dataSources', 'type': 'invalid_request_error', 'param': None, 'code': None}}


You can use this MS-DOCS to create your own data with chat completion.
You can use the code below to create chat completion with data source and openai version 1.9.0.
Code:
import os
from openai import AzureOpenAI

endpoint=os.environ[""AZURE_ENDPOINT""]
deployment=""gpt-35-turbo""
apikey=os.environ[""API_KEY""]
client = AzureOpenAI(
    base_url=f""{endpoint}/openai/deployments/{deployment}/extensions"", 
    api_key=apikey, 
    api_version=""2023-09-01-preview"")

for i in range(3):
    print (f'Answer Version {i + 1}\n---')

completion = client.chat.completions.create(
    model = deployment,
    messages = [
        {
            ""role"": ""system"",
            ""content"": ""When the user provides a project name as input, you should do the steps mentioned below: Step 1: Get the project band of the project from the file.""
        },
        {
            ""role"": ""user"",
            ""content"": 'Where do I go for Azure OpenAI customer support?"" '
        }
    ],
    seed = 42,
    temperature = 0,
    max_tokens = 800,
    extra_body = {
        ""dataSources"": [
            {
                ""type"": ""AzureCognitiveSearch"",
                ""parameters"": {
                    ""endpoint"": os.environ[""SEARCH_ENDPOINT""],
                     ""key"": os.environ[""SEARCH_KEY""],
                     ""indexName"": ""test-index""
                    }
             }
        ]
    }
)
print (completion.choices[0].message.content) 

print(""---\n"")

Output:
Answer Version 1
---
Answer Version 2
---
Answer Version 3
---
You can check the Cognitive Services support options guide for help with Azure OpenAI [doc1].
---


",llm
nameerror name 39langchainllm39 is not defined,"I am following some tutorials online relating to RAG systems. As part of it I am trying to use LangChainLLM from LlamaIndex. I am working on Google Colab. As per the LlamaIndex docs I have imported LangChainLLM as follows:
from llama_index.llms.langchain import LangChainLLM
This does not throw an error when run
However when I try to use LangChainLLM as follows:
langchain_llm = LangchainLLM(llm_chain=llm_chain)
It throws the error NameError: name 'LangchainLLM' is not defined
I have checked the documentation and searched online for a solution but I cant see why this error is arising. I have rerun the notebook but there is no impact.  If anyone can help it would be greatly appreciated
","You have a typo error LangChainLLM not  LangchainLLM(llm_chain=llm_chain)
",llm
