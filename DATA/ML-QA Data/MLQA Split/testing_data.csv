instruction,input,output,Categories
implementing gridsearchcv and pipelines to perform hyperparameters tuning for knn algorithm,"I have been reading about perfroming Hyperparameters Tuning for KNN Algorthim, and understood that the best practice of implementing it is to make sure that for each fold, my dataset should be normalized and oversamplmed using a pipeline (To avoid data leakage and overfitting).
What I'm trying to do is that I'm trying to identify the best number of neighbors (n_neighbors) possible that gives me the best accuracy in training. In the code I have set the number of neighbors to be a list range (1,50), and the number of iterations cv=10.
My code below:
# dataset reading & preprocessing libraries
import pandas as pd
import numpy as np
from sklearn.preprocessing import MinMaxScaler

#oversmapling
from imblearn.over_sampling import SMOTE

#KNN Model related Libraries
import cuml 
from imblearn.pipeline import Pipeline
from sklearn.model_selection import train_test_split, GridSearchCV
from cuml.neighbors import KNeighborsClassifier

#loading the dataset
df = pd.read_csv(""/content/drive/MyDrive/Colab Notebooks/dataset/IanDataset.csv"")

#filling missing values with zeros
df = df.fillna(0)

#replace the data in from being objects to integers
df[""command response""].replace({""b'0'"": ""0"", ""b'1'"": ""1""}, inplace=True)
df[""binary result""].replace({""b'0'"": ""0"", ""b'1'"": ""1""}, inplace=True)

#change the datatype of some features to be able to be used later 
df[""command response""] = pd.to_numeric(df[""command response""]).astype(float)
df[""binary result""] = pd.to_numeric(df[""binary result""]).astype(int)

# dataset splitting
X = df.iloc[:, 0:17]
y_bin = df.iloc[:, 17]

# spliting the dataset into train and test for binary classification
X_train, X_test, y_bin_train, y_bin_test = train_test_split(X, y_bin, random_state=0, test_size=0.2)

#making pipleline that normalize, oversample and use classifier before GridSearchCV
pipe = Pipeline([
        ('normalization', MinMaxScaler()),
        ('oversampling', SMOTE()),
        ('classifier', KNeighborsClassifier(metric='eculidean', output='input'))
])

#Using GridSearchCV
neighbors = list(range(1,50))
parameters = {
    'classifier__n_neighbors': neighbors 
}

grid_search = GridSearchCV(pipe, parameters, cv=10)
grid_search.fit(X_train, y_bin_train)

print(""Best Accuracy: {}"" .format(grid_search.best_score_))
print(""Best num of neighbors: {}"" .format(grid_search.best_estimator_.get_params()['n_neighbors']))

At step grid_search.fit(X_train, y_bin_train), the program is repeating the error that i'm getting is :
/usr/local/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:619: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: 
Traceback (most recent call last):
  File ""/usr/local/lib/python3.7/site-packages/sklearn/model_selection/_validation.py"", line 598, in _fit_and_score
    estimator.fit(X_train, y_train, **fit_params)
  File ""/usr/local/lib/python3.7/dist-packages/imblearn/pipeline.py"", line 266, in fit
    self._final_estimator.fit(Xt, yt, **fit_params_last_step)
  File ""/usr/local/lib/python3.7/site-packages/cuml/internals/api_decorators.py"", line 409, in inner_with_setters
    return func(*args, **kwargs)
  File ""cuml/neighbors/kneighbors_classifier.pyx"", line 176, in cuml.neighbors.kneighbors_classifier.KNeighborsClassifier.fit
  File ""/usr/local/lib/python3.7/site-packages/cuml/internals/api_decorators.py"", line 409, in inner_with_setters
    return func(*args, **kwargs)
  File ""cuml/neighbors/nearest_neighbors.pyx"", line 397, in cuml.neighbors.nearest_neighbors.NearestNeighbors.fit
ValueError: Metric  is not valid. Use sorted(cuml.neighbors.VALID_METRICSeculidean[brute]) to get valid options.

I'm not sure from which side is this error coming from, is it because I'm importing KNN Algorthim from cuML Library instead of sklearn ? Or is there something wrong wtih my Pipeline and GridSearchCV implementation?
","This error indicates you've passed an invalid value for the metric parameter (in both scikit-learn and cuML). You've misspelled ""euclidean"".
import cuml
from sklearn import datasets
​
from sklearn.preprocessing import MinMaxScaler
​
from imblearn.over_sampling import SMOTE
​
from imblearn.pipeline import Pipeline
from sklearn.model_selection import train_test_split, GridSearchCV
from cuml.neighbors import KNeighborsClassifier
​
X, y = datasets.make_classification(
    n_samples=100
)
​
pipe = Pipeline([
        ('normalization', MinMaxScaler()),
        ('oversampling', SMOTE()),
        ('classifier', KNeighborsClassifier(metric='euclidean', output='input'))
])
​
parameters = {
    'classifier__n_neighbors': [1,3,6] 
}
​
grid_search = GridSearchCV(pipe, parameters, cv=2)
grid_search.fit(X, y)
GridSearchCV(cv=2,
             estimator=Pipeline(steps=[('normalization', MinMaxScaler()),
                                       ('oversampling', SMOTE()),
                                       ('classifier', KNeighborsClassifier())]),
             param_grid={'classifier__n_neighbors': [1, 3, 6]})

",scikit-learn
finding most similar sentences among all in python,"Suggestions / refer links /codes are appreciated.
I have a data which is having more than 1500 rows. Each row has a sentence. I am trying to find out the best method to find the most similar sentences among all.
What I have tried

I have tried K-mean algorithm which groups similar sentences in a cluster. But I found a drawback in which I have to pass K to create a cluster. It is hard to guess K. I tried elbo method to guess the clusters but grouping all together isn't sufficient. In this approach I am getting all the data grouped. I am looking for data which is similar above 0.90% data should be returned with ID.

I tried cosine similarity in which I used TfidfVectorizer to create matrix and then passed in cosine similarity. Even this approach didn't worked properly.


What I am looking for
I want an approach where I can pass a threshold example 0.90 data in all rows which are similar to each other above 0.90% should be returned as a result.
Data Sample
ID    |   DESCRIPTION
-----------------------------
10    | Cancel ASN WMS Cancel ASN   
11    | MAXPREDO Validation is corect
12    | Move to QC  
13    | Cancel ASN WMS Cancel ASN   
14    | MAXPREDO Validation is right
15    | Verify files are sent every hours for this interface from Optima
16    | MAXPREDO Validation are correct
17    | Move to QC  
18    | Verify files are not sent

Expected result
Above data which are similar upto 0.90% should get as a result with ID
ID    |   DESCRIPTION
-----------------------------
10    | Cancel ASN WMS Cancel ASN
13    | Cancel ASN WMS Cancel ASN
11    | MAXPREDO Validation is corect  # even spelling is not correct
14    | MAXPREDO Validation is right
16    | MAXPREDO Validation are correct
12    | Move to QC  
17    | Move to QC  

","Why did it not work for you with cosine similarity and the TFIDF-vectorizer?
I tried it and it works with this code:
import pandas as pd
import numpy as np

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity

df = pd.DataFrame(columns=[""ID"",""DESCRIPTION""], data=np.matrix([[10,""Cancel ASN WMS Cancel ASN""],
                                                                [11,""MAXPREDO Validation is corect""],
                                                                [12,""Move to QC""],
                                                                [13,""Cancel ASN WMS Cancel ASN""],
                                                                [14,""MAXPREDO Validation is right""],
                                                                [15,""Verify files are sent every hours for this interface from Optima""],
                                                                [16,""MAXPREDO Validation are correct""],
                                                                [17,""Move to QC""],
                                                                [18,""Verify files are not sent""]
                                                                ]))

corpus = list(df[""DESCRIPTION""].values)

vectorizer = TfidfVectorizer()
X = vectorizer.fit_transform(corpus)

threshold = 0.4

for x in range(0,X.shape[0]):
  for y in range(x,X.shape[0]):
    if(x!=y):
      if(cosine_similarity(X[x],X[y])>threshold):
        print(df[""ID""][x],"":"",corpus[x])
        print(df[""ID""][y],"":"",corpus[y])
        print(""Cosine similarity:"",cosine_similarity(X[x],X[y]))
        print()

The threshold can be adjusted as well, but will not yield the results you want with a threshold of 0.9.
The output for a threshold of 0.4 is:
10 : Cancel ASN WMS Cancel ASN
13 : Cancel ASN WMS Cancel ASN
Cosine similarity: [[1.]]

11 : MAXPREDO Validation is corect
14 : MAXPREDO Validation is right
Cosine similarity: [[0.64183024]]

12 : Move to QC
17 : Move to QC
Cosine similarity: [[1.]]

15 : Verify files are sent every hours for this interface from Optima
18 : Verify files are not sent
Cosine similarity: [[0.44897995]]

With a threshold of 0.39 all your expected sentences are features in the output, but an additional pair with the indices [15,18] can be found as well:
10 : Cancel ASN WMS Cancel ASN
13 : Cancel ASN WMS Cancel ASN
Cosine similarity: [[1.]]

11 : MAXPREDO Validation is corect
14 : MAXPREDO Validation is right
Cosine similarity: [[0.64183024]]

11 : MAXPREDO Validation is corect
16 : MAXPREDO Validation are correct
Cosine similarity: [[0.39895808]]

12 : Move to QC
17 : Move to QC
Cosine similarity: [[1.]]

14 : MAXPREDO Validation is right
16 : MAXPREDO Validation are correct
Cosine similarity: [[0.39895808]]

15 : Verify files are sent every hours for this interface from Optima
18 : Verify files are not sent
Cosine similarity: [[0.44897995]]

",tensorflow
generate array of positive integers that sum of to k,"My task is simple: I want to generate an (ideally numpy) array containing all combinations of m positive (>=0), but bounded (<= e) integers that sum exactly to k. Note that k and m might be relatively high, so generating all combinations and filtering will not work.
I have implemented it in plain, recursive python but this small functions takes most of my time and I need to replace it to perform better. I have tried to come up with numpy/pytorch code to generate this array but I didn't manage to do it so far.
I currently use numpy and pytorch in my project, but I am open to other libraries as long as I write python code and I get something I can convert to numpy arrays in the end.
Here's some code:
import timeit


def get_summing_up_to(max_degree, sum, length, current=0):
    assert sum >= 0
    assert length >= 1
    if length == 1:
        residual = sum - current
        if residual <= max_degree:
            return [(residual,)]
        else:
            return []
    max_element = min(max_degree, sum - current)
    return [
        (i,) + t
        for i in range(max_element + 1)
        for t in get_summing_up_to(
            max_degree, sum, length - 1,
            current=current + i
        )
    ]


if __name__ == '__main__':
    result = timeit.timeit('get_summing_up_to(60, 60, 6)', globals=globals(), number=1)
    print(f""Execution time: {result} for max_degree=60, sum=60, length=6"")

    result = timeit.timeit('get_summing_up_to(30, 30, 8)', globals=globals(), number=1)
    print(f""Execution time: {result} for max_degree=30, sum=30, length=8"")

","One thing to notice is your function generates redundant outputs. ie get_summing_up_to(30, 30, 8) would contain (30, 0, 0, 0, 0, 0, 0, 0), (0, 30, 0, 0, 0, 0, 0, 0), ....
One way to make this more efficient is to generate unique integer combinations excluding 0s from 1 to max_length. We can also add caching to the sub-problem of generating partitions for added efficiency.
from functools import lru_cache

def get_summing_up_to_minimal(max_value, target_sum, max_length):
    # optional caching - setting maxsize recommended 
    @lru_cache(maxsize=None)
    def generate_partitions(remaining_sum, max_val, length):
        # Early pruning conditions
        if remaining_sum < 0:
            return []
        if length == 0:
            return [()] if remaining_sum == 0 else []
        
        # Minimum possible sum with given length (using all 1's)
        if remaining_sum < length:
            return []
        
        # Maximum possible sum with given length (using max_val)
        if remaining_sum > max_val * length:
            return []
            
        # Base case for length 1
        if length == 1:
            return [(remaining_sum,)] if remaining_sum <= max_val else []

        results = []
        # Optimize the start value
        start = min(max_val, remaining_sum)
        # Calculate minimum value needed to achieve remaining_sum with remaining length
        min_required = (remaining_sum - 1) // length + 1
        
        # Iterate only through viable values
        for i in range(start, min_required - 1, -1):
            # Early pruning: check if remaining values can sum to target
            remaining_length = length - 1
            remaining_target = remaining_sum - i
            
            # If maximum possible sum with remaining length is too small, break
            if i * remaining_length < remaining_target:
                break
                
            # If minimum possible sum with remaining length is too large, continue
            if remaining_target < remaining_length:
                continue
                
            sub_partitions = generate_partitions(
                remaining_target,
                min(i, max_val),
                remaining_length
            )
            
            for sub_partition in sub_partitions:
                results.append((i,) + sub_partition)
        
        return results

    all_partitions = []
    # Only try lengths that could possibly work
    min_length = (target_sum - 1) // max_value + 1
    max_possible_length = min(max_length, target_sum)
    
    for length in range(min_length, max_possible_length + 1):
        partitions = generate_partitions(target_sum, max_value, length)
        all_partitions.extend(partitions)
    
    return all_partitions

If the full output with redundant results is required, we can generate them after the fact using the minimal set of outputs from get_summing_up_to_minimal:
from itertools import permutations

def expand_partitions(compact_partitions, max_length):
    result = []
    
    for partition in compact_partitions:
        # Calculate how many zeros we need to add
        zeros_needed = max_length - len(partition)
        if zeros_needed < 0:
            continue
            
        # Create the full partition with zeros
        full_partition = partition + (0,) * zeros_needed
        
        # Generate all unique permutations
        # Using a set to handle cases where partition contains duplicate numbers
        result.extend(set(permutations(full_partition)))
    
    return result

Note that expanding partitions would be the bulk of compute time.
I profiled the following:
# run 1, your original code
out = get_summing_up_to(30, 60, 6)

# run 2, generating just minimal outputs
out = get_summing_up_to_minimal(30, 60, 6)

# run 3, generate minimal outputs and expand to full outputs
out = get_summing_up_to_minimal(30, 60, 6)
out = expand_partitions(out, 8)

On my machine, your original code takes ~4.6 seconds. Generating the minimal outputs takes ~17.9 milliseconds. Generating the minimal outputs and expanding takes ~1.1 seconds.
If your downstream use case doesn't require the redundant combinations, you can save a lot of time just generating the minimal set. If you need to pack the outputs into a numpy array/torch tensor (requiring everything be the same length), you can pad the minimal outputs to max_length with zeros without generating all the combinations. ie:
out = get_summing_up_to_minimal(30, 60, 6)
out_array = np.zeros((len(out), 6))

for i, o in enumerate(out):
    out_array[i][:len(o)] = sorted(o)

",pytorch
understanding scikit learn import variants,"Scikit learn import statements in their tutorials are on the form
from sklearn.decomposition import PCA

Another versions that works is
import sklearn.decomposition
pca = sklearn.decomposition.PCA(n_components = 2)

However
import sklearn
pca = sklearn.decomposition.PCA(n_components = 2)

does not, and complains
AttributeError: module 'sklearn' has no attribute 'decomposition'

Why is this, and how can I predict which ones will work and not so i don't have to test around? If the understanding and predictiveness extends to python packages in general that would be the best.
","sklearn doesn't automatically import its submodules. If you want to use sklearn.<SUBMODULE>, then you will need to import it explicitly e.g. import sklearn.<SUBMODULE>. Then you can use it without any further imports like result = sklearn.<SUBMODULE>.function(...).
Large packages often behave this way where they don't automatically import all the submodules.
Memory and load-time efficiency become worse if the submodules are automatically loaded; by specifying the submodule explicitly it saves on memory consumption and minimises the start-up time. I think namespace cluttering is another consideration, where explicit imports reduce the chance of naming conflicts and help maintain clarity about the specific functionality being used.
",scikit-learn
why does calling the kfold generator with shuffle give the same indices,"With sklearn, when you create a new KFold object and shuffle is true, it'll produce a different, newly randomized fold indices. However, every generator from a given KFold object gives the same indices for each fold even when shuffle is true. Why does it work like this?
Example: 
from sklearn.cross_validation import KFold
X = np.array([[1, 2], [3, 4], [1, 2], [3, 4]])
y = np.array([1, 2, 3, 4])
kf = KFold(4, n_folds=2, shuffle = True)
​
for fold in kf:
    print fold
​
print '---second round----'
​
for fold in kf:
    print fold

Output:
(array([2, 3]), array([0, 1]))
(array([0, 1]), array([2, 3]))
---second round----#same indices for the folds
(array([2, 3]), array([0, 1]))
(array([0, 1]), array([2, 3]))

This question was motivated by a comment on this answer. I decided to split it into a new question to prevent that answer from becoming too long.
","A new iteration with the same KFold object will not reshuffle the indices, that only happens during instantiation of the object. KFold() never sees the data but knows number of samples so it uses that to shuffle the indices. From the code during instantiation of KFold:
if shuffle:
    rng = check_random_state(self.random_state)
    rng.shuffle(self.idxs)

Each time a generator is called to iterate through the indices of each fold, it will use same shuffled indices and divide them the same way.
Take a look at the code for the base class of KFold _PartitionIterator(with_metaclass(ABCMeta)) where __iter__ is defined. The __iter__ method in the base class calls _iter_test_indices in KFold to divide and yield the train and test indices for each fold.
",cross-validation
custom crossvalidation and pycaret,"I've been working with hierarchical time series, and, as a result, I needed to create my own CV to make sure that all timestamps and products are included evenly in the test (validation) set. It worked just fine for sklearn, but I can't make it work in pycaret: best = compare_models() yields nothing at all. Here is the custom CV I used:
class custom_cv:
    def __init__(self, train_end, test_size, n_splits): # val_size
        self.train_end = train_end
        # self.val_size = val_size
        self.test_size = test_size
        self.n_splits = n_splits
        
    def split(self, X):
        self.X = X
        
        for i in range(self.n_splits, 0, -1): # range(start, stop, step)
            tr_threshol = self.train_end - self.test_size*i
            te_threshol = tr_threshol + self.test_size
    
            tr_idx = np.array(self.X.reset_index(drop = True).index[self.X['N_month'] <= tr_threshol])
            te_idx = np.array(self.X.index[(self.X['N_month'] > tr_threshol) & (self.X['N_month'] <= te_threshol)])
        
            yield(tr_idx, te_idx)

    custom_CV = custom_cv(train_end = 365, test_size = 28, n_splits = 5)
    # custom_CV = custom_CV.split(X = df)

My Data looks like this: 1
For sklearn I used the following loop:
def custom_cv(df, train_end = 36, test_size = 4, n_splits = 4):
    cv_idx = []

    for i in range(n_splits, 0, -1): # range(start, stop, step)
        tr_threshol = train_end - test_size*i
        te_threshol = tr_threshol + test_size
    
        tr_idx = list(df.reset_index(drop = True).index[df['N_month'] <= tr_threshol])
        te_idx = list(df.index[(df['N_month'] > tr_threshol) & (df['N_month'] <= te_threshol)])
    
        cv_idx.append((tr_idx, te_idx))
    
    return cv_idx

custom_CV = custom_cv(df = df, train_end = 365, test_size = 28, n_splits = 5)

However, pycaret requires a custom CV generator object compatible with scikit-learn (something I've never dealt with before). I can't figure out what's wrong exactly, and I hope you can kindly help me out.
","Your class for Pycaret is probably missing the get_n_splits method.I had similar problem and solved with the class structure like here:
How to generate a custom cross-validation generator in scikit-learn?
",cross-validation
unable to install torch using pip,"I using Windows
I installed Python 3.13
I am trying to pip install torch
However, when attempting to do so, I encounter this error.
F:\Kit>pip install torch
ERROR: Could not find a version that satisfies the requirement torch (from versions: none)
ERROR: No matching distribution found for torch

What is the issue and how can it be resolved?
","It is stated in the docs that:

PyTorch on Windows only supports Python 3.8-3.11.

However on PyPi, The stable release of torch (i.e. version 2.4.1)  supports Python 3.8 - 3.12. There are wheels for Python 3.12 Windows as can be seen here.
However, there is no support yet for Python 3.13. To use this package, downgrade to a supported version perhaps Python 3.12.
",pytorch
tensorflow docker not using gpu,"I'm trying to get Tensorflow working on my Ubuntu 24.04.1 with a GPU.
According to this page:

Docker is the easiest way to run TensorFlow on a GPU since the host machine only requires the NVIDIA® driver

So I'm trying to use Docker.
I'm checking to ensure my GPU is working with Docker by running docker run --gpus all --rm nvidia/cuda:12.6.2-cudnn-runtime-ubuntu24.04 nvidia-smi. The output of that is:
==========
== CUDA ==
==========

CUDA Version 12.6.2

Container image Copyright (c) 2016-2023, NVIDIA CORPORATION & AFFILIATES. All rights reserved.

This container image and its contents are governed by the NVIDIA Deep Learning Container License.
By pulling and using the container, you accept the terms and conditions of this license:
https://developer.nvidia.com/ngc/nvidia-deep-learning-container-license

A copy of this license is made available in this container at /NGC-DL-CONTAINER-LICENSE for your convenience.

Sat Oct 26 01:16:50 2024
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 560.35.03              Driver Version: 560.35.03      CUDA Version: 12.6     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA TITAN RTX               Off |   00000000:01:00.0 Off |                  N/A |
| 41%   40C    P8             24W /  280W |       1MiB /  24576MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+

+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|  No running processes found                                                             |
+-----------------------------------------------------------------------------------------+

(Side note, I'm not using the command they suggest because docker run --gpus all --rm nvidia/cuda nvidia-smi doesn't work due to nvidia/cuda not having a latest tag anymore)
So it looks to be working. However when I run:
docker run --gpus all -it --rm tensorflow/tensorflow:latest-gpu \
   python -c ""import tensorflow as tf; print(tf.config.list_physical_devices('GPU'))""

The output is:
2024-10-26 01:20:51.021242: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1729905651.033544       1 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1729905651.037491       1 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2024-10-26 01:20:51.050486: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
W0000 00:00:1729905652.350499       1 gpu_device.cc:2344] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.
Skipping registering GPU devices...
[]

Which indicates that there is no GPU detected by Tensorflow.
What am I doing wrong here?
","I don't think you're doing anything wrong, but I'm concerned that the image may be a ""pip install"" short of a complete image.
I'm running a different flavor of linux, but to start off with I had to make sure I had my gpu available to docker (see here Add nvidia runtime to docker runtimes ) and I upgraded my cuda version to the latest.
Even after doing all this I had the same error as you.
So I logged into the container as follows:
docker run -it --rm --runtime=nvidia --gpus all tensorflow/tensorflow:latest-gpu /bin/bash
and ran
pip install tensorflow[and-cuda]
Some of the dependencies were there and some or the dependencies had to be installed because they were missing.  This is undesireable because you'd expect everything necessary to be there to run (maybe they'll fix the image in the future)
After it finished I ran python -c ""import tensorflow as tf; print(tf.config.list_physical_devices('GPU'))"" and it finally found my GPU
You're going to want to create your own docker image using their docker image as a base. So your dockerfile may look like something like:
# Use the official TensorFlow GPU base image
FROM tensorflow/tensorflow:latest-gpu

# Install TensorFlow with CUDA support
RUN pip install tensorflow[and-cuda]

# Shell
CMD [""bash""]

",tensorflow
valueerror in user code while using keras model in r,"I am trying to run 1 dimensional CNN in R using keras package. I am using the following code
library(MASS)
library(keras)

##Create some data
data(""Boston"")
data <- Boston
# create a list of 70% of the rows in the original dataset we can use for training
set.seed(123)
training <- sample(nrow(data), 0.7 * nrow(data))

dataTrain <- data[training,]
dataTest <- data[-training,]

dataTrain_y <- as.matrix(dataTrain$medv)
dataTrain_x <- as.matrix(subset(dataTrain, select = -c(medv)))

dataTest_y <- as.matrix(dataTest$medv)
dataTest_x <- as.matrix(subset(dataTest, select = -c(medv)))

#Reshaping the data for CNN
dataTrain_x <- array_reshape(dataTrain_x, c(ncol(dataTrain_x), nrow(dataTrain_x), 1))
dataTest_x <- array_reshape(dataTest_x, c(ncol(dataTest_x), nrow(dataTest_x), 1))

#CNN model
model <- keras_model_sequential() %>%
  layer_conv_1d(filters=32, kernel_size=4, activation=""relu"", 
                input_shape=c(ncol(dataTrain_x), nrow(dataTrain_x))) %>%
  layer_max_pooling_1d(pool_size=2) %>%
  layer_conv_1d(filters=64, kernel_size=2, activation=""relu"") %>%
  layer_max_pooling_1d(pool_size=2) %>%
  layer_dropout(rate=0.4) %>%
  layer_flatten() %>%
  layer_dense(units=100, activation=""relu"") %>%
  layer_dropout(rate=0.2) %>%
  layer_dense(units=1, activation=""linear"")

model %>% compile(
  loss = ""mse"",
  optimizer =  ""adam"", #'sgd' can also be used
  metrics = list(""mean_absolute_error"")
)

model %>% summary()

history <- model %>% fit(dataTrain_x, dataTrain_y, 
                         epochs = 100, batch_size = 50, 
                         #callbacks = callback_tensorboard(""logs/run_a""),
                         validation_split = 0.2)

But it returns me the following error
Error in py_call_impl(callable, dots$args, dots$keywords) : 
  ValueError: in user code:
    C:\Python37\lib\site-packages\tensorflow\python\keras\engine\training.py:805 train_function  *
        return step_function(self, iterator)
    C:\Python37\lib\site-packages\tensorflow\python\keras\engine\training.py:795 step_function  **
        outputs = model.distribute_strategy.run(run_step, args=(data,))
    C:\Python37\lib\site-packages\tensorflow\python\distribute\distribute_lib.py:1259 run
        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)
    C:\Python37\lib\site-packages\tensorflow\python\distribute\distribute_lib.py:2730 call_for_each_replica
        return self._call_for_each_replica(fn, args, kwargs)
    C:\Python37\lib\site-packages\tensorflow\python\distribute\distribute_lib.py:3417 _call_for_each_replica
        return fn(*args, **kwargs)
    C:\Python37\lib\site-packages\tensorflow\python\keras\engine\training.py:788 run_step  **
        outputs = model.train_step(data)
    C:\Python37\lib\site-pac 

Now how can I solve this error?
","The error message is truncated, so it's not much help, but it looks to me like you've got some minor typos. If you make the suggested changes (the ""###"" comments) the model appears to compile and train as expected, e.g.
library(MASS)
library(keras)

##Create some data
data(""Boston"")
data <- Boston
# create a list of 70% of the rows in the original dataset we can use for training
set.seed(123)
training <- sample(nrow(data), 0.7 * nrow(data))

dataTrain <- data[training,]
dataTest <- data[-training,]

dataTrain_y <- as.matrix(dataTrain$medv)
dataTrain_x <- as.matrix(subset(dataTrain, select = -c(medv)))

dataTest_y <- as.matrix(dataTest$medv)
dataTest_x <- as.matrix(subset(dataTest, select = -c(medv)))

#Reshaping the data for CNN
### These dimensions don't look correct; switch ncol() with nrow()
dataTrain_x <- array_reshape(dataTrain_x, c(nrow(dataTrain_x), ncol(dataTrain_x), 1))
dataTest_x <- array_reshape(dataTest_x, c(nrow(dataTest_x), ncol(dataTest_x), 1))

#CNN model
model <- keras_model_sequential() %>%
  layer_conv_1d(filters=32, kernel_size=4, activation=""relu"", 
                ### The input shape doesn't look correct; instead of 
                ### `c(ncol(dataTrain_x), nrow(dataTrain_x))` (354, 13)
                ### I believe you want `dim(dataTest_x)` (13, 1)
                input_shape=c(ncol(dataTrain_x), 1)) %>%
  layer_max_pooling_1d(pool_size=2) %>%
  layer_conv_1d(filters=64, kernel_size=2, activation=""relu"") %>%
  layer_max_pooling_1d(pool_size=2) %>%
  layer_dropout(rate=0.4) %>%
  layer_flatten() %>%
  layer_dense(units=100, activation=""relu"") %>%
  layer_dropout(rate=0.2) %>%
  layer_dense(units=1, activation=""linear"")

model %>% compile(
  loss = ""mse"",
  optimizer =  ""adam"", #'sgd' can also be used
  metrics = list(""mean_absolute_error"")
)

model %>% summary()
#> Model: ""sequential""
#> ________________________________________________________________________________
#> Layer (type)                        Output Shape                    Param #     
#> ================================================================================
#> conv1d_1 (Conv1D)                   (None, 10, 32)                  160         
#> ________________________________________________________________________________
#> max_pooling1d_1 (MaxPooling1D)      (None, 5, 32)                   0           
#> ________________________________________________________________________________
#> conv1d (Conv1D)                     (None, 4, 64)                   4160        
#> ________________________________________________________________________________
#> max_pooling1d (MaxPooling1D)        (None, 2, 64)                   0           
#> ________________________________________________________________________________
#> dropout_1 (Dropout)                 (None, 2, 64)                   0           
#> ________________________________________________________________________________
#> flatten (Flatten)                   (None, 128)                     0           
#> ________________________________________________________________________________
#> dense_1 (Dense)                     (None, 100)                     12900       
#> ________________________________________________________________________________
#> dropout (Dropout)                   (None, 100)                     0           
#> ________________________________________________________________________________
#> dense (Dense)                       (None, 1)                       101         
#> ================================================================================
#> Total params: 17,321
#> Trainable params: 17,321
#> Non-trainable params: 0
#> ________________________________________________________________________________

history <- model %>% fit(dataTrain_x, dataTrain_y, 
                         epochs = 100, batch_size = 50, 
                         #callbacks = callback_tensorboard(""logs/run_a""),
                         validation_split = 0.2)


Created on 2021-07-20 by the reprex package (v2.0.0)
",tensorflow
how to calculate r2 in tensorflow,"I am trying to do regression in Tensorflow. I'm not positive I am calculating R^2 correctly as Tensorflow gives me a different answer than sklearn.metrics.r2_score Can someone please look at my below code and let me know if I implemented the pictured equation correctly. Thanks

total_error = tf.square(tf.sub(y, tf.reduce_mean(y)))
unexplained_error = tf.square(tf.sub(y, prediction))
R_squared = tf.reduce_mean(tf.sub(tf.div(unexplained_error, total_error), 1.0))
R = tf.mul(tf.sign(R_squared),tf.sqrt(tf.abs(R_squared)))

","What you are computing the ""R^2"" is

compared to the given expression, you are computing the mean at the wrong place. You should take the mean when computing the errors, before doing the division.
unexplained_error = tf.reduce_sum(tf.square(tf.sub(y, prediction)))
total_error = tf.reduce_sum(tf.square(tf.sub(y, tf.reduce_mean(y))))
R_squared = tf.sub(1, tf.div(unexplained_error, total_error))

",tensorflow
sklearn pipelines  gridsearchcv  xgboost  learning curve,"I am new to sklearn & XGBoost.
I would like to use GridSearchCV to tune a XGBoost classifier. One of the checks that I would like to do is the graphical analysis of the loss from train and test. So far I have created the following code:
# Create a new instance of the classifier
xgbr =  xgb.XGBClassifier()
# Create a new pipeline with preprocessing steps and model (imballanced learn)
pipeline  = imb_pipeline([
                          ('preprocess', preprocess), # Encode and transform categorical variables
                          ('re-sample', samplers[0]), # re-samples data to ballanced state
                          ('scale', scalers[0]), # scales the data
                          ('model', xgbr), # models
                          ])

# Create parameter values for gridsearch - carefull, ""model__"" prepended defined in pipeline
params = { 
    'model__max_depth': [3, 4, 5, 6, 8, 10, 12, 15],
    'model__learning_rate': [0.001, 0.01, 0.1, 0.20, 0.25, 0.30],
    ""model__gamma"":[0, 0.25, 0.5, 0.75,1],
    'model__n_estimators': [100, 500, 1000],
    ""model__subsample"":[0.9],
    ""model__colsample_bytree"":[0.5],
    ""model__early_stopping_rounds"": [10], 
    ""model__random_state"": [random_state], 
    ""model__eval_metric"" : [""error""], 
    ""model__eval_set"" : [[(X_train, Y_train), (X_test,Y_test)]]
}

# Use GridSearchCV for all combinations
grid = GridSearchCV(
    estimator = pipeline,
    param_grid = params,
    scoring = 'roc_auc',
    n_jobs = -1,
    cv = 5,
    verbose = 3,
)

# Model fitting
grid.fit(X_train, Y_train)

I have create in params a key-value pair for eval_metric and eval_set:
My question is now, how to access those values and plot a curve of train and test loss (sorry I cannot post a figure here).
Another question: Are the values hand-over by eval_set also piped by the pipeline or do I have to create a separate pipeline for those?
I am using xgb.__version == 0.90, sklearn.__version__ == 1.0.2, python == 3.7.13 @ (google colab)
","I think you are misunderstanding how the grid search is coupled to the cross-validation. Here your training set will be partitioned in 5 (cv = 5) almost even chunks, for each value of the hyperparameter grid it will train on 4 and predict (+ eval) on on the last one, iterating on all possible splits. This will give a cv estimate of the error. But this is all happening inside your training data. After picking the hyperparameters giving the best (=smallest) error, you want to evaluate this model (trained on the entire training set) on the evaluation data (held out and totally new data to the model and hyperparameters. This will give you the a reliable estimate of the generalization error of your model.
Also if you use
    ""model__eval_set"" : [[(X_train, Y_train), (X_test,Y_test)]]

the grid will assume that this is a binary parameter to optimize on, and you do not want that.
All in all you are looking for something of the like:

params = { 
    'model__max_depth': [3, 4, 5, 6, 8, 10, 12, 15],
    'model__learning_rate': [0.001, 0.01, 0.1, 0.20, 0.25, 0.30],
    ""model__gamma"":[0, 0.25, 0.5, 0.75,1],
    'model__n_estimators': [100, 500, 1000],
    ""model__subsample"":[0.9],
    ""model__colsample_bytree"":[0.5],
    ""model__early_stopping_rounds"": [10], 
    ""model__random_state"": [random_state], 
}

grid = GridSearchCV(
    estimator = pipeline,
    param_grid = params,
    scoring = 'roc_auc',
    n_jobs = -1,
    cv = 5,
    verbose = 3,
)

# Model fitting
grid = grid.fit(X_train, Y_train, eval_set=[(X_test, Y_test)])

eval_auc = sklearn.metrics.roc_auc_score( Y_test, grid.best_estimator_.predict_proba(X_test)[:,1])

",scikit-learn
how to clean images to use with a mnist trained model,"I am creating a machine learning model for classifying images of numbers. I have trained the model using Tensorflow and Keras using the inbuilt tf.keras.datasets.mnist dataset. The model works quite well with the test images from the mnist dataset itself but I would like to feed it images of my own. The images that I am feeding this model is extracted from a Captcha so they will follow a similar pattern. I have included some examples of the images in this public google drive folder. When I feed these images, I noticed that the model is not very accurate and I have some guesses as to why.

The background of the image creates too much noise in the picture.
The number is not centered.
The image is not striclty in the color format of MNIST training set (Black background white text).

I wanted to ask how can I remove the background and centre it so that the noise in the image is reduced allowing for better classifications.
Here is the model I am using:
import tensorflow as tf
from tensorflow import keras

mnist = keras.datasets.mnist
(x_train, y_train), (x_test, y_test) = mnist.load_data()

class Stopper(keras.callbacks.Callback):
    def on_epoch_end(self, epoch, log={}):
        if log.get('acc') >= 0.99:
            self.model.stop_training = True
            print('\nReached 99% Accuracy. Stopping Training...')

model = keras.Sequential([
    keras.layers.Flatten(),
    keras.layers.Dense(1024, activation=tf.nn.relu),
    keras.layers.Dense(10, activation=tf.nn.softmax)])

model.compile(
    optimizer=tf.train.AdamOptimizer(),
    loss='sparse_categorical_crossentropy',
    metrics=['accuracy'])

x_train, x_test = x_train / 255, x_test / 255

model.fit(x_train, y_train, epochs=10, callbacks=[Stopper()])

And here is my method of importing the image into tensorflow:
from PIL import Image
img = Image.open(""image_file_path"").convert('L').resize((28, 28), Image.ANTIALIAS)
img = np.array(img)
model.predict(img[None,:,:])

I have also included some examples from the MNIST dataset here. I would like a script to convert my images as closely to the MNIST dataset format as possible. Also, since I would have to do this for an indefinite number of images, I would appreciate if you could provide a fully automated method for this conversion.
","You need to train with a dataset similar to the images you're testing. The MNIST data is hand-written numbers, which is not going to be similar to the computer generated fonts for Captcha data. 
What you need to do is gain a catalog of Captcha data similar to what you're predicting on (preferably from the same source you will be inputting to the final model). It's a painstaking task to capture the data, and you'll probably need around 300-400 images for each label before you start to get something useful.
A key note: your model will only ever be as good as the training data you supplied to the model. Trying to make a good model with bad training data is an effort in pure frustration
To address some of your thoughts:

[the model is not very accurate because] the background of the image creates too much noise in the picture.

This is true. If the image data has noise and the neural net was not trained using any noise in the images, then it will not recognize a strong pattern when it encounters this type of distortion. One possible way to combat this is to take clean images and progamatically add noise to the image (noise similar to what you see in the real Captcha) before sending it to be trained.

[the model is not very accurate because] The number is not centered.

Also true for the same reasons. If all the training data is centered, the model will be overtuned for this property and make incorrect guesses. Follow a similar pattern to the one above if you don't have the capacity to manually capture and catalog a good sampling of data.

[the model is not very accurate because] The image is not striclty in the color format of MNIST training set (Black background white text).

You can get around this by applying a binary threshold to the data before processing/ normalize the color input before training. Depending on the amount of noise in the captcha you may have better results allowing the number and noise to retain some of it's color information (still put in greyscale and normalize, just don't apply the threshold).

Additionally I'd recommend using a convolution net rather than the linear network as it is better at distinguishing 2D features like edges and corners. i.e. use keras.layers.Conv2D layers before flattening with keras.layers.Flatten
See the great example found here: Trains a simple convnet on the MNIST dataset.
model = tf.keras.models.Sequential(
    [
        tf.keras.layers.Conv2D(
            32,
            kernel_size=(3, 3),
            activation=tf.nn.relu,
            input_shape=input_shape,
        ),
        tf.keras.layers.Conv2D(64, (3, 3), activation=tf.nn.relu),
        tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),
        tf.keras.layers.Dropout(0.25),
        tf.keras.layers.Flatten(),
        tf.keras.layers.Dense(128, activation=tf.nn.relu),
        tf.keras.layers.Dropout(0.5),
        tf.keras.layers.Dense(
            num_classes, activation=tf.nn.softmax
        ),
    ]
)

I've used this setup for reading fonts in video gameplay footage, and with a test set of 10,000 images I'm achieving 99.98% accuracy, using a random sampling of half the dataset in training, and calculating accuracy using the total set.
",tensorflow
keras h5 model to tensorflowlite tflite model conversion,"I am trying to convert a .h5 keras model to a .tflite model. But the conversion results in core dumped error. Here's the script that I am running,
import tensorflow as tf
from keras.models import Sequential
from keras.layers import Dense

# Create a simple Keras model
model = Sequential([
    Dense(64, activation='relu', input_shape=(784,)),
    Dense(64, activation='relu'),
    Dense(10, activation='softmax')
])

# Compile the model
model.compile(optimizer='adam',
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])

# Convert the Keras model to TensorFlow Lite model
converter = tf.lite.TFLiteConverter.from_keras_model(model)
tflite_model = converter.convert()

# Save the TensorFlow Lite model to a file
with open('model.tflite', 'wb') as f:
    f.write(tflite_model)

print(""TensorFlow Lite model saved successfully!"")

I am getting this error, if I run the script
2024-04-01 12:27:04.793910: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
loc(fused[""ReadVariableOp:"", callsite(""sequential_1/dense_1/Add/ReadVariableOp@__inference_serving_default_98""
callsite(""/home/spoon/Documents/GTSRB/lib/python3.9/site-packages/keras/src/ops/numpy.py"":311:1 at callsite(""/home/spoon/Documents/GTSRB/lib/python3.9/site-packages/keras/src/backend/tensorflow/sparse.py"":491:1 at callsite(""/home/spoon/Documents/GTSRB/lib/python3.9/site-packages/keras/src/backend/tensorflow/numpy.py"":35:1 at ""/home/spoon/Documents/GTSRB/lib/python3.9/site-packages/keras/src/backend/tensorflow/core.py"":64:1)))))))))))))))))))))))))))]): error: missing attribute 'value'
LLVM ERROR: Failed to infer result type(s).
Aborted (core dumped)

OS: Ubuntu 20.04.6 LTS
Python version: 3.9.18
pip freeze info:
keras==3.1.1
keras-core==0.1.7
keras-cv==0.8.2
tensorboard==2.16.2
tensorboard-data-server==0.7.2
tensorflow==2.16.1
tensorflow-datasets==4.9.3
tensorflow-io-gcs-filesystem==0.36.0
tensorflow-metadata==1.14.0

","This is a bug caused by inconsistent Keras versions.
TensorFlow versions ≥ 2.16 switched from Keras 2 to Keras 3, that is good since Keras 3 introduced many improvements with a significant performance uplift, but unfortunately they haven't properly ported all the TensorFlow modules to make them work as well with the latest Keras 3 module (and not even properly documented what this change has broken), so converter still expects a model generated by old Keras 2 module and crashes when a Keras 3 model is provided instead.
Hoping for a future update of the converter module, in meanwhile as workaround, you need to:
1) install tf_keras package (so that Keras 2 legacy mode is available) with:
pip install tf_keras

2) enable legacy mode adding in your code:
import os
os.environ[""TF_USE_LEGACY_KERAS""] = ""1""

placing it before
import tensorflow as tf

so that TensorFlow will be initialized with Keras 2 module.
Alternatively you can downgrade your TensorFlow to the latest Keras 2-based version with:
pip install tensorflow==2.15.0

",tensorflow
how to solve this error could not find a version that satisfies the requirement torch160,"I'm using a demo on Colab notebook called Deep Exemplar Based Video Colorization. https://colab.research.google.com/drive/1Y1XTlTdUG-2LzrH1Vnr_osg9BQavfYsz?usp=sharing
Worked swimmingly for four years until this monday. In the Run Environment cell - I get the error below. I'm not sure what the protocol is here, but I thought I'd just post the Run environment cell, after I run it, which displays the errors. As it was working fine till last sunday, does anyone know if there was a recent update which could have caused the issue?
Run Environment Command cell:
# seems to be a colab bug, need to install previous version for pytorch
!pip install torch==1.6.0 torchvision==0.7.0
!pip install -q moviepy
!apt install imagemagick
!pip install imageio==2.4.1

%cd Deep-Exemplar-based-Video-Colorization/
! pip install -r requirements.txt

ERROR: Could not find a version that satisfies the requirement torch==1.6.0 (from versions: 1.11.0, 1.12.0, 1.12.1, 1.13.0, 1.13.1, 2.0.0, 2.0.1, 2.1.0, 2.1.1, 2.1.2, 2.2.0, 2.2.1, 2.2.2, 2.3.0, 2.3.1, 2.4.0, 2.4.1, 2.5.0)
ERROR: No matching distribution found for torch==1.6.0

--------------------------------
ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.
moviepy 1.0.3 requires imageio<3.0,>=2.5; python_version >= ""3.4"", but you have imageio 2.4.1 which is incompatible.
scikit-image 0.24.0 requires imageio>=2.33, but you have imageio 2.4.1 which is incompatible.

------------------------------------------------
ERROR: Failed building wheel for scipy
  Running setup.py clean for scipy
  error: subprocess-exited-with-error
  
  × python setup.py clean did not run successfully.
  │ exit code: 1
  ╰─> See above for output.
  
  note: This error originates from a subprocess, and is likely not a problem with pip.
  ERROR: Failed cleaning build dir for scipy
---------------------------------------------

","I don't know of a specific update since alst sunday, but the versions specified in the requirements.txt (scikit==1.2) and also directly in the cell (torch==1.6) are not compatible with python 3.10, which is in colab for quite some time.
The best approach is to use a python version that supports all versions required in this demo. You could try to update the package versions required in requirements.txt to a higher version compatible with python 3.10, but then you have no guarantee that the module you are trying to use still works.
To achieve the downgrade of python, add a new cell to the very top containing
!sudo apt-get install python3.7
!sudo update-alternatives --install /usr/bin/python3 python3 /usr/bin/python3.7 1
!sudo apt install python3.7-distutils
!curl https://bootstrap.pypa.io/pip/3.7/get-pip.py | sudo python3.7

",pytorch
how to get scikitlearn to ensure that all prediction outputs should sum to 100,"I have a 'MultiOutputRegressor' which is based on a 'LinearRegression' regressor.
I am using it to predict three outputs per row of X_data (like a classifier) which represent the percentage likelihood of three outcomes.
The regressor is fitted against y_data where the three labels sum correctly to 100%.
Obviously the regressor doesn't really know that it's three prediction outputs should sum, it just knows roughly what values they should be.
Is there a way that I can tell the regressor explicitly that one of the rules is that all three prediction outputs should together sum to 100%?
","Shortly, no. Regressors cannot know this. Your problem is a multi-class classification problem. You need to use classifier for your problem. Classifier model predicts probabilites of three labels. And sum of them will be 1 (100%).
https://scikit-learn.org/stable/modules/generated/sklearn.multioutput.MultiOutputClassifier.html
",scikit-learn
how made crossvalidation with python,"Hi i made a neural network and i need to do a cross validation.
I don't know how made that, specifically how train or made that.
if someone knows made that please write or give me some indications.
here is my code:
###Division Train / Test
X = df.drop('Peso secado',axis=1)  #Variables de entrada, menos la variable de salida
y = df['Peso secado']              #Variable de salida

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.3,random_state=101)

###

from sklearn.preprocessing import MinMaxScaler
scaler = MinMaxScaler()
X_train= scaler.fit_transform(X_train)
X_train
X_test = scaler.transform(X_test)
X_test



###Creacion del modelo###
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Activation
from tensorflow.keras.optimizers import Adam
import tensorflow as tf

model = Sequential()
num_neuronas = 50
model.add(tf.keras.layers.Dense(units=6, activation='sigmoid', input_shape=(6, )))
model.add(Dense(num_neuronas,activation='relu'))
model.add(tf.keras.layers.Dense(units=1, activation='linear')) 

#Buscar mejor funcion de activacion para capa de salida sigmoid? o linear?
model.summary()
model.compile(optimizer='adam',loss='mse')

###Entrenamiento###
model.fit(x = X_train, y = y_train.values,
          validation_data=(X_test,y_test.values), batch_size=10, epochs=1000) 

losses = pd.DataFrame(model.history.history)  
losses
losses.plot()
   
###Evaluacion###
from sklearn.metrics import mean_squared_error,mean_absolute_error,explained_variance_score,mean_absolute_percentage_error
X_test
predictions = model.predict(X_test)
mean_absolute_error(y_test,predictions)
mean_absolute_percentage_error(y_test,predictions)

mean_squared_error(y_test,predictions)
explained_variance_score(y_test,predictions)  

mean_absolute_error(y_test,predictions)/df['Peso secado'].mean() 
mean_absolute_error(y_test,predictions)/df['Peso secado'].median()

Some recomendation for training or validation would be helpful
","My first observation is that the code is pretty ugly and unstructured. You should import the modules on the top part of your code
For performing cross validation first import the module from sklearn (and all other modules that you need)
from sklearn.model_selection import StratifiedKFold

I'd put the model definition in a separate function as such:
def get_model():
  model = Sequential()
  model.add(Dense(4, input_dim=8, activation='relu'))
  model.add(Dense(1, activation='sigmoid'))
  model.compile(loss='binary_crossentropy', optimizer='adam')
  return model

Define your variables and if you are working with tensorflow / Keras, do something like this:
BATCH_SIZE = 64  #  128
EPOCHS = 100

k = 10
# Use stratified k-fold if the data is imbalanced
kf = StratifiedKFold(n_splits=k, shuffle=False, random_state=None)

# here comes the Cross validation
fold_index = 1
for train_index, test_index in kf.split(X, y):
            X_train = X[train_index]
            y_train = y[train_index]

            X_test = X[test_index]
            y_test = y[test_index]

            # fit the model on the training set
            model = get_model()

            model.fit(
                X_train,
                y_train,
                batch_size=BATCH_SIZE,
                epochs=EPOCHS,
                verbose=0,
                validation_data=(X_test, y_test),
            )

            # predict values
            # pred_values = model.predict(X_test)
            pred_values_prob = np.array(model(X_test))

Note: when working with tensorflow you need to define a new model every time in the loop. This is not the case with sklearn as sklearn starts with fresh initialized weights when called. Here you need to do that separately.
",cross-validation
sklearn get distance from point to nearest cluster,"I'm using clustering algorithms like DBSCAN.
It returns a 'cluster' called -1 which are points that are not part of any cluster. For these points I want to determine the distance from it to the nearest cluster to get something like a metric for how abnormal this point is. Is this possible? Or are there any alternatives for this kind of metric?
","The answer will depend on the linkage strategy you choose. I'll give the example of single linkage.
First, you can construct the distance matrix of your data.
from sklearn.metrics.pairwise import pairwise_distances
dist_matrix = pairwise_distances(X)

Then, you'll extract the nearest cluster:
for point in unclustered_points:
    distances = []
    for cluster in clusters:
        distance = dist_matrix[point, cluster].min()  # Single linkage
        distances.append(distance)
    print(""The cluster for {} is {}"".format(point, cluster)

EDIT: This works, but it's O(n^2) as noted by Anony-Mousse. Considering core points is a better idea because it cuts down on your work. In addition, it is somewhat similar to centroid linkage.
",scikit-learn
difference between numpylinalglstsq and sklearnlinear_modellinearregression,"As I understand, numpy.linalg.lstsq and sklearn.linear_model.LinearRegression both look for solutions x of the linear system Ax = y, that minimise the resdidual sum ||Ax - y||.
But they don't give the same result:
from sklearn import linear_model
import numpy as np

A = np.array([[1, 0], [0, 1]])
b = np.array([1, 0])
x , _, _, _ = np.linalg.lstsq(A,b)
x

Out[1]: array([ 1.,  0.])

clf = linear_model.LinearRegression()
clf.fit(A, b)                              
coef = clf.coef_
coef

Out[2]: array([ 0.5, -0.5])

What am I overlooking?
","Both of them are implemented by LPACK gelsd.
The difference is that linear_model.LinearRegression will do data pre-process (default) as below for input X (your A). But np.linalg.lstsq don't. You can refer to the source code of LinearRegression for more details about the data pre-process.
X = (X - X_offset) / X_scale

If you don't want the data pre-process, you should set fit_intercept=False.
Briefly speaking, if you normalize your input before linear regression, you will get the same result by both linear_model.LinearRegression and np.linalg.lstsq as below.
# Normalization/Scaling
from sklearn.preprocessing import StandardScaler
A = np.array([[1, 0], [0, 1]])
X_scaler = StandardScaler()
A = X_scaler.fit_transform(A)

Now A is array([[ 1., -1.],[-1.,  1.]])
from sklearn import linear_model
import numpy as np

b = np.array([1, 0])
x , _, _, _ = np.linalg.lstsq(A,b)
x
Out[1]: array([ 0.25, -0.25])

clf = linear_model.LinearRegression()
clf.fit(A, b)                              
coef = clf.coef_
coef

Out[2]: array([ 0.25, -0.25])

",scikit-learn
generate negative predictive value using cross_val_score in sklearn for model performance evaluation,"As part of evaluating a model's metrics, I would like to use cross_val_score in sklearn to generate negative predictive value for a binary classification model.
In example below, I set the 'scoring' parameter within cross_val_score to calculate and print 'precision' (mean and standard deviations from 10-fold cross-validation) for positive predictive value of the model:
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import cross_val_score
log=LogisticRegression()    
log_prec = cross_val_score(log, x, y, cv=10, scoring='precision')
print(""PPV(mean, std): "", np.round(log_prec.mean(), 2), np.round(log_prec.std(), 2))

How can I use something like the above line of code to generate negative predictive value/NPV (likelihood of a predicted negative to be a true negative) from within the cross_val_score method?
sklearn provides many scoring options (eg: roc_auc, recall, accuracy, F1, etc) but unforunately not one for NPV...
","You can use make_scorer to pass in pos_label=0 to the precision score function (metrics.precision_score) to get NPV. Like this:
from sklearn.metrics import make_scorer, precision_score
npv = cross_val_score(log, x, y, cv=10, scoring=make_scorer(precision_score, pos_label=0))

For more details see this sklearn example.
",cross-validation
using groupshufflesplit with gridsearchcv and cross_val_score for nested crossvalidation,"I am trying to perform nested cross-validation while also incorporating group-based splitting using the GroupShuffleSplit class. However, I'm encountering a ""TypeError: cannot pickle 'generator' object"" when trying to use a custom cross-validation object with GridSearchCV. As fas as i know this Error occurs because group_split.split(...) returns an generator which cant be used in the cross_val_score function. Therefore i want to ask if there is a way to easily use GroupShuffleSplit for nested cross-validation.
Regarding my simplified sample code:
I have a dataset with features X, labels y, and group labels groups. The goal is to perform nested cross-validation, where both the inner and outer loops split the data based on the group labels. I would like to use GridSearchCV for hyperparameter tuning and cross_val_score for evaluating the performance.
import numpy as np
from sklearn.model_selection import GroupShuffleSplit, GridSearchCV, cross_val_score
from sklearn.ensemble import RandomForestClassifier

X = np.random.rand(100, 10)
y = np.random.randint(2, size=100)
groups = np.random.randint(4, size=100)  # Example group labels

rf_classifier = RandomForestClassifier()
param_grid = {'n_estimators': [50, 100, 200]}

inner_cv = GroupShuffleSplit(n_splits=5, test_size=0.2)
outer_cv = GroupShuffleSplit(n_splits=5, test_size=0.2)

grid_search = GridSearchCV(estimator=rf_classifier, param_grid=param_grid, cv=inner_cv.split(X, y, groups=groups))
nested_scores = cross_val_score(estimator=grid_search, X=X, y=y, cv=outer_cv.split(X, y, groups=groups))

Resulting in the following Stacktrace Error:
---------------------------------------------------------------------------
Empty                                     Traceback (most recent call last)
File c:\Anaconda3_x64\lib\site-packages\joblib\parallel.py:825, in Parallel.dispatch_one_batch(self, iterator)
    824 try:
--> 825     tasks = self._ready_batches.get(block=False)
    826 except queue.Empty:
    827     # slice the iterator n_jobs * batchsize items at a time. If the
    828     # slice returns less than that, then the current batchsize puts
   (...)
    831     # accordingly to distribute evenly the last items between all
    832     # workers.

File c:\Anaconda3_x64\lib\queue.py:168, in Queue.get(self, block, timeout)
    167     if not self._qsize():
--> 168         raise Empty
    169 elif timeout is None:

Empty: 

During handling of the above exception, another exception occurred:

TypeError                                 Traceback (most recent call last)
Cell In[29], line 16
     13 outer_cv = GroupShuffleSplit(n_splits=5, test_size=0.2)
     15 grid_search = GridSearchCV(estimator=rf_classifier, param_grid=param_grid, cv=inner_cv.split(X, y, groups=groups))
---> 16 nested_scores = cross_val_score(estimator=grid_search, X=X, y=y, cv=outer_cv.split(X, y, groups=groups))
     18 print(nested_scores)

File c:\Anaconda3_x64\lib\site-packages\sklearn\model_selection\_validation.py:515, in cross_val_score(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, error_score)
    512 # To ensure multimetric format is not supported
    513 scorer = check_scoring(estimator, scoring=scoring)
--> 515 cv_results = cross_validate(
    516     estimator=estimator,
    517     X=X,
    518     y=y,
    519     groups=groups,
    520     scoring={""score"": scorer},
    521     cv=cv,
    522     n_jobs=n_jobs,
    523     verbose=verbose,
    524     fit_params=fit_params,
    525     pre_dispatch=pre_dispatch,
    526     error_score=error_score,
    527 )
    528 return cv_results[""test_score""]

File c:\Anaconda3_x64\lib\site-packages\sklearn\model_selection\_validation.py:266, in cross_validate(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, return_train_score, return_estimator, error_score)
    263 # We clone the estimator to make sure that all the folds are
    264 # independent, and that it is pickle-able.
    265 parallel = Parallel(n_jobs=n_jobs, verbose=verbose, pre_dispatch=pre_dispatch)
--> 266 results = parallel(
    267     delayed(_fit_and_score)(
    268         clone(estimator),
    269         X,
    270         y,
    271         scorers,
    272         train,
    273         test,
    274         verbose,
    275         None,
    276         fit_params,
    277         return_train_score=return_train_score,
    278         return_times=True,
    279         return_estimator=return_estimator,
    280         error_score=error_score,
    281     )
    282     for train, test in cv.split(X, y, groups)
    283 )
    285 _warn_or_raise_about_fit_failures(results, error_score)
    287 # For callabe scoring, the return type is only know after calling. If the
    288 # return type is a dictionary, the error scores can now be inserted with
    289 # the correct key.

File c:\Anaconda3_x64\lib\site-packages\sklearn\utils\parallel.py:63, in Parallel.__call__(self, iterable)
     58 config = get_config()
     59 iterable_with_config = (
     60     (_with_config(delayed_func, config), args, kwargs)
     61     for delayed_func, args, kwargs in iterable
     62 )
---> 63 return super().__call__(iterable_with_config)

File c:\Anaconda3_x64\lib\site-packages\joblib\parallel.py:1048, in Parallel.__call__(self, iterable)
   1039 try:
   1040     # Only set self._iterating to True if at least a batch
   1041     # was dispatched. In particular this covers the edge
   (...)
   1045     # was very quick and its callback already dispatched all the
   1046     # remaining jobs.
   1047     self._iterating = False
-> 1048     if self.dispatch_one_batch(iterator):
   1049         self._iterating = self._original_iterator is not None
   1051     while self.dispatch_one_batch(iterator):

File c:\Anaconda3_x64\lib\site-packages\joblib\parallel.py:836, in Parallel.dispatch_one_batch(self, iterator)
    833 n_jobs = self._cached_effective_n_jobs
    834 big_batch_size = batch_size * n_jobs
--> 836 islice = list(itertools.islice(iterator, big_batch_size))
    837 if len(islice) == 0:
    838     return False

File c:\Anaconda3_x64\lib\site-packages\sklearn\utils\parallel.py:59, in <genexpr>(.0)
     54 # Capture the thread-local scikit-learn configuration at the time
     55 # Parallel.__call__ is issued since the tasks can be dispatched
     56 # in a different thread depending on the backend and on the value of
     57 # pre_dispatch and n_jobs.
     58 config = get_config()
---> 59 iterable_with_config = (
     60     (_with_config(delayed_func, config), args, kwargs)
     61     for delayed_func, args, kwargs in iterable
     62 )
     63 return super().__call__(iterable_with_config)

File c:\Anaconda3_x64\lib\site-packages\sklearn\model_selection\_validation.py:268, in <genexpr>(.0)
    263 # We clone the estimator to make sure that all the folds are
    264 # independent, and that it is pickle-able.
    265 parallel = Parallel(n_jobs=n_jobs, verbose=verbose, pre_dispatch=pre_dispatch)
    266 results = parallel(
    267     delayed(_fit_and_score)(
--> 268         clone(estimator),
    269         X,
    270         y,
    271         scorers,
    272         train,
    273         test,
    274         verbose,
    275         None,
    276         fit_params,
    277         return_train_score=return_train_score,
    278         return_times=True,
    279         return_estimator=return_estimator,
    280         error_score=error_score,
    281     )
    282     for train, test in cv.split(X, y, groups)
    283 )
    285 _warn_or_raise_about_fit_failures(results, error_score)
    287 # For callabe scoring, the return type is only know after calling. If the
    288 # return type is a dictionary, the error scores can now be inserted with
    289 # the correct key.

File c:\Anaconda3_x64\lib\site-packages\sklearn\base.py:89, in clone(estimator, safe)
     87 new_object_params = estimator.get_params(deep=False)
     88 for name, param in new_object_params.items():
---> 89     new_object_params[name] = clone(param, safe=False)
     90 new_object = klass(**new_object_params)
     91 params_set = new_object.get_params(deep=False)

File c:\Anaconda3_x64\lib\site-packages\sklearn\base.py:70, in clone(estimator, safe)
     68 elif not hasattr(estimator, ""get_params"") or isinstance(estimator, type):
     69     if not safe:
---> 70         return copy.deepcopy(estimator)
     71     else:
     72         if isinstance(estimator, type):

File c:\Anaconda3_x64\lib\copy.py:161, in deepcopy(x, memo, _nil)
    159 reductor = getattr(x, ""__reduce_ex__"", None)
    160 if reductor is not None:
--> 161     rv = reductor(4)
    162 else:
    163     reductor = getattr(x, ""__reduce__"", None)

TypeError: cannot pickle 'generator' object

","I'm not sure that this is possible before version 1.3 without writing a manual loop to replace cross_val_score.  Besides the generator issue, you're trying to tell the grid search object that it should split all of X, but it won't see all of X (it having already been split by the outer splitter).
In 1.3, we get metadata routing which automatically routes groups to group splitters.  Then we can do e.g.
from sklearn import set_config
set_config(enable_metadata_routing=True)

grid_search = GridSearchCV(estimator=rf_classifier, param_grid=param_grid, cv=inner_cv)
nested_scores = cross_val_score(estimator=grid_search, X=X, y=y, cv=outer_cv, params={'groups': groups})

Just to check that this really routes to both splitters, here's a modified version of your script:
import numpy as np
import pandas as pd
from sklearn.model_selection import GroupShuffleSplit, GridSearchCV, cross_val_score
from sklearn.base import BaseEstimator, ClassifierMixin
from sklearn import set_config

set_config(enable_metadata_routing=True)

X = np.random.rand(100, 10)
y = np.random.randint(2, size=100)
groups = np.random.randint(4, size=100)  # Example group labels

X = pd.DataFrame(X)


class MyClassifier(ClassifierMixin, BaseEstimator):
    def __init__(self, n_estimators=1):
        self.n_estimators = n_estimators

    def fit(self, X, y):
        print(""train: "", groups[X.index])
        return self
    
    def predict(self, X):
        print(""test: "", groups[X.index])
        return np.random.randint(2, size=len(X))


rf_classifier = MyClassifier()
param_grid = {'n_estimators': [50, 100]}

inner_cv = GroupShuffleSplit(n_splits=2, test_size=0.33)
outer_cv = GroupShuffleSplit(n_splits=2, test_size=0.25)

grid_search = GridSearchCV(estimator=rf_classifier, param_grid=param_grid, cv=inner_cv, verbose=10)
nested_scores = cross_val_score(estimator=grid_search, X=X, y=y, cv=outer_cv, params={'groups': groups}, verbose=10)

print(nested_score)

The outer splits put a single group in the test set, then the inner splits pick one of the remaining three as test and the last two in train.  Here's my output:
[CV] START .....................................................................
Fitting 2 folds for each of 2 candidates, totalling 4 fits
[CV 1/2; 1/2] START n_estimators=50.............................................
train:  [1 3 3 3 1 3 3 1 1 1 1 1 3 1 1 1 3 3 1 3 3 3 3 1 1 1 3 3 3 3 3 3 3 3 3 1 3
 3 3 3 1 3 1 1 1 3 3 1 1 3 1 1 1 1 1 1]
test:  [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]
[CV 1/2; 1/2] END ..............n_estimators=50;, score=0.353 total time=   0.0s
[CV 2/2; 1/2] START n_estimators=50.............................................
train:  [3 3 3 3 3 0 0 3 3 3 3 3 3 0 3 0 0 0 3 3 0 0 0 3 3 0 0 3 3 3 3 3 3 3 3 3 0
 0 0 0 3 3 3 0 0 3]
test:  [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]
[CV 2/2; 1/2] END ..............n_estimators=50;, score=0.407 total time=   0.0s
[CV 1/2; 2/2] START n_estimators=100............................................
train:  [1 3 3 3 1 3 3 1 1 1 1 1 3 1 1 1 3 3 1 3 3 3 3 1 1 1 3 3 3 3 3 3 3 3 3 1 3
 3 3 3 1 3 1 1 1 3 3 1 1 3 1 1 1 1 1 1]
test:  [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]
[CV 1/2; 2/2] END .............n_estimators=100;, score=0.412 total time=   0.0s
[CV 2/2; 2/2] START n_estimators=100............................................
train:  [3 3 3 3 3 0 0 3 3 3 3 3 3 0 3 0 0 0 3 3 0 0 0 3 3 0 0 3 3 3 3 3 3 3 3 3 0
 0 0 0 3 3 3 0 0 3]
test:  [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]
[CV 2/2; 2/2] END .............n_estimators=100;, score=0.333 total time=   0.0s
train:  [1 3 3 3 1 3 3 1 1 1 0 1 1 0 3 1 1 1 3 3 1 3 3 3 0 3 1 1 0 1 0 0 3 3 0 0 0
 3 3 0 0 3 3 3 3 3 1 3 3 3 3 0 0 1 0 0 3 1 1 1 3 3 1 1 0 0 3 1 1 1 1 1 1]
test:  [2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2]
[CV] END ................................ score: (test=0.481) total time=   0.0s
[CV] START .....................................................................
Fitting 2 folds for each of 2 candidates, totalling 4 fits
[CV 1/2; 1/2] START n_estimators=50.............................................
train:  [3 3 3 3 2 3 2 2 2 3 2 3 2 3 2 2 2 3 3 2 3 3 2 2 2 3 3 2 2 3 3 2 2 2 3 3 3
 3 3 3 3 2 3 3 2 2 3 2 2 2 2 3 3 2 3 2]
test:  [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]
[CV 1/2; 1/2] END ..............n_estimators=50;, score=0.588 total time=   0.0s
[CV 2/2; 1/2] START n_estimators=50.............................................
train:  [3 3 3 3 2 3 2 2 2 3 2 3 2 3 2 2 2 3 3 2 3 3 2 2 2 3 3 2 2 3 3 2 2 2 3 3 3
 3 3 3 3 2 3 3 2 2 3 2 2 2 2 3 3 2 3 2]
test:  [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]
[CV 2/2; 1/2] END ..............n_estimators=50;, score=0.588 total time=   0.0s
[CV 1/2; 2/2] START n_estimators=100............................................
train:  [3 3 3 3 2 3 2 2 2 3 2 3 2 3 2 2 2 3 3 2 3 3 2 2 2 3 3 2 2 3 3 2 2 2 3 3 3
 3 3 3 3 2 3 3 2 2 3 2 2 2 2 3 3 2 3 2]
test:  [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]
[CV 1/2; 2/2] END .............n_estimators=100;, score=0.647 total time=   0.0s
[CV 2/2; 2/2] START n_estimators=100............................................
train:  [3 3 3 3 2 3 2 2 2 3 2 3 2 3 2 2 2 3 3 2 3 3 2 2 2 3 3 2 2 3 3 2 2 2 3 3 3
 3 3 3 3 2 3 3 2 2 3 2 2 2 2 3 3 2 3 2]
test:  [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]
[CV 2/2; 2/2] END .............n_estimators=100;, score=0.471 total time=   0.0s
train:  [3 3 3 3 2 3 2 2 0 2 0 3 2 3 2 3 2 2 2 3 3 2 3 0 3 2 2 2 0 0 0 3 3 2 0 0 2
 0 3 3 0 0 2 2 2 3 3 3 3 3 3 3 2 3 3 2 2 0 0 0 0 3 2 2 2 2 3 3 2 0 0 3 2]
test:  [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]
[CV] END ................................ score: (test=0.593) total time=   0.0s
[0.48148148 0.59259259]
[Parallel(n_jobs=1)]: Done   1 tasks      | elapsed:    0.0s

",cross-validation
openai api what is the proper format to finetune the openai model,"I am creating an OpenAI model for one of the Catalog Book through which users can ask anything from the book, and the model is able to answer it.
I created a Q&A list from the book (70 QA). ChatGPT gives me the following format to submit it to the fine-tune API endpoint:
""{
    ""context"": ""Introducing BAUTER - an innovative thermal insulation product which has been developed by engineers for many years for the production of paints and other thin-coating materials that results in a unique group of thermo coating solutions. As part of the market demand for a product that will work in conditions where insulation has not yet been possible or difficult, our product resolves many problems in construction and industry. BAUTER is a family of various products with many applications."",
    ""question"": ""What is BAUTER?"",
    ""answer"": ""BAUTER is an innovative thermal insulation product developed by engineers for the production of paints and other thin-coating materials. It offers unique thermo coating solutions and addresses insulation challenges in construction and industry.""
},""

What is the proper format and the proper way to fine-tune the OpenAI model?
","The proper format to fine-tune the OpenAI model depends on the model you want to fine-tune.
As of today, you can fine-tune the following OpenAI models, as stated in the official OpenAI documentation:

gpt-3.5-turbo-0613 (recommended)
babbage-002
davinci-002

Let's say you want to create a chatbot that occasionally gives sarcastic responses, as shown in the official OpenAI documentation.
If you want to fine-tune the gpt-3.5-turbo-0613 model, use the following format:
{""messages"": [{""role"": ""system"", ""content"": ""Marv is a factual chatbot that is also sarcastic.""}, {""role"": ""user"", ""content"": ""What's the capital of France?""}, {""role"": ""assistant"", ""content"": ""Paris, as if everyone doesn't know that already.""}]}
{""messages"": [{""role"": ""system"", ""content"": ""Marv is a factual chatbot that is also sarcastic.""}, {""role"": ""user"", ""content"": ""Who wrote 'Romeo and Juliet'?""}, {""role"": ""assistant"", ""content"": ""Oh, just some guy named William Shakespeare. Ever heard of him?""}]}
{""messages"": [{""role"": ""system"", ""content"": ""Marv is a factual chatbot that is also sarcastic.""}, {""role"": ""user"", ""content"": ""How far is the Moon from Earth?""}, {""role"": ""assistant"", ""content"": ""Around 384,400 kilometers. Give or take a few, like that really matters.""}]}

If you want to fine-tune the babbage-002 or davinci-002 model, use the following format:
{""prompt"": ""What's the capital of France?"", ""completion"": ""Paris, as if everyone doesn't know that already.""}
{""prompt"": ""Who wrote 'Romeo and Juliet'?"", ""completion"": ""Oh, just some guy named William Shakespeare. Ever heard of him?""}
{""prompt"": ""How far is the Moon from Earth?"", ""completion"": ""Around 384,400 kilometers. Give or take a few, like that really matters.""}

",fine-tune
gridsearchcv on logisticregression in scikitlearn,"I am trying to optimize a logistic regression function in scikit-learn by using a cross-validated grid parameter search, but I can't seem to implement it. 
It says that Logistic Regression does not implement a get_params() but on the documentation it says it does. How can I go about optimizing this function on my ground truth? 
>>> param_grid = {'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000] }
>>> clf = GridSearchCV(LogisticRegression(penalty='l2'), param_grid)
>>> clf
GridSearchCV(cv=None,
       estimator=LogisticRegression(C=1.0, intercept_scaling=1, dual=False, fit_intercept=True,
          penalty='l2', tol=0.0001),
       fit_params={}, iid=True, loss_func=None, n_jobs=1,
       param_grid={'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000]},
       pre_dispatch='2*n_jobs', refit=True, score_func=None, verbose=0)
>>> clf = clf.fit(gt_features, labels)
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/Library/Python/2.7/site-packages/scikit_learn-0.14_git-py2.7-macosx-10.8-x86_64.egg/sklearn/grid_search.py"", line 351, in fit
    base_clf = clone(self.estimator)
  File ""/Library/Python/2.7/site-packages/scikit_learn-0.14_git-py2.7-macosx-10.8-x86_64.egg/sklearn/base.py"", line 42, in clone
    % (repr(estimator), type(estimator)))
TypeError: Cannot clone object 'LogisticRegression(C=1.0, intercept_scaling=1, dual=False, fit_intercept=True,
          penalty='l2', tol=0.0001)' (type <class 'scikits.learn.linear_model.logistic.LogisticRegression'>): it does not seem to be a scikit-learn estimator a it does not implement a 'get_params' methods.
>>> 

","The class name scikits.learn.linear_model.logistic.LogisticRegression refers to a very old version of scikit-learn. The top level package name is now sklearn since at least 2 or 3 releases. It's very likely that you have old versions of scikit-learn installed concurrently in your python path. Uninstall them all, then reinstall 0.14 or later and try again.
",cross-validation
understanding sklearn39s knnimputer,"I was going through its documentation and it says

Each sample’s missing values are imputed using the mean value from
n_neighbors nearest neighbors found in the training set. Two samples
are close if the features that neither are missing are close.

Now, playing around with a toy dataset, i.e.
>>>X = [[1, 2, nan], [3, 4, 3], [nan, 6, 5], [8, 8, 7]]
>>>X

   [[ 1.,  2., nan],
    [ 3.,  4.,  3.],
    [nan,  6.,  5.],
    [ 8.,  8.,  7.]]

And we make a KNNImputer as follows:
imputer = KNNImputer(n_neighbors=2)

The question is, how does it fill the nans while having nans in 2 of the columns? For example, if it is to fill the nan in the 3rd column of the 1st row, how will it choose which features are the closest since one of the rows has nan in the first column as well? When I do imputer.fit_transform(X) it gives me
array([[1. , 2. , 4. ],
       [3. , 4. , 3. ],
       [5.5, 6. , 5. ],
       [8. , 8. , 7. ]])

which means for filling out the nan in row 1, the nearest neighbors were the second and the third row. How did it calculate the euclidean distance between the first and the third row?
","
How does it fill the NaNs using rows that also have NaNs?

This doesn't seem to be mentioned in the docs. But by digging a bit into the source code, it appears that for each column being imputed, all donors at a smaller distance are considered, even if they have missing values. The way this is handled is by setting to 0 the missing values in a weight matrix, which is obtained according to the used distance, see _get_weights.
The relevant code is in _calc_impute, where after finding a distance matrix for all potential donors, and then the above mentioned matrix of weights, it is imputed as:
# fill nans with zeros
if weight_matrix is not None:
    weight_matrix[np.isnan(weight_matrix)] = 0.0

Where all potential donors are considered if they have at least one non-nan distance with the reciever
dist_pot_donors : ndarray of shape (n_receivers, n_potential_donors)
    Distance matrix between the receivers and potential donors from
    training set. There must be at least one non-nan distance between
    a receiver and a potential donor.

We could check this with a toy example; in the following matrix, when inputting the missing value in [nan,  7.,  4.,  5.], the last row (which also contains two NaNs) is chosen (note that I've set n_neighbors=1). This is because the distance wrt the last row is 0, as the distance corresponding to the NaN values has been set to 0. So by just having a minimal difference with rows 2 and 3, the last row is chosen since it is seen as being equal:
X = np.array([[np.nan,7,4,5],[2,8,4,5],[3,7,4,6],[1,np.nan,np.nan,5]])

print(X)
array([[nan,  7.,  4.,  5.],
       [ 2.,  8.,  4.,  5.],
       [ 3.,  7.,  4.,  6.],
       [ 1., nan, nan,  5.]])

from sklearn.impute import KNNImputer
imputer = KNNImputer(n_neighbors=1)

imputer.fit_transform(X)
array([[1., 7., 4., 5.],
       [2., 8., 4., 5.],
       [3., 7., 4., 6.],
       [1., 7., 4., 5.]])

",scikit-learn
pytorch model object has no attribute 39predict39 bert,"I had train a BertClassifier model using pytorch. After creating my best.pt I would like to make in production my model and using it to predict and classifier starting from a sample, so I resume them from the checkpoint. Otherwise after put it in evaluation and freeze model, I use .predict to make in work on my sample but I'm encountering this Attribute Error. I had also inizialize it before calling the checkpoint. When I am wrong? Thank you for your help!
def save_ckp(state, is_best, checkpoint_path, best_model_path):
    """"""
    function created to save checkpoint, the latest one and the best one. 
    This creates flexibility: either you are interested in the state of the latest checkpoint or the best checkpoint.
    state: checkpoint we want to save
    is_best: is this the best checkpoint; min validation loss
    checkpoint_path: path to save checkpoint
    best_model_path: path to save best model
    """"""
    f_path = checkpoint_path
    # save checkpoint data to the path given, checkpoint_path
    torch.save(state, f_path)
    # if it is a best model, min validation loss
    if is_best:
        best_fpath = best_model_path
        # copy that checkpoint file to best path given, best_model_path
        shutil.copyfile(f_path, best_fpath)

def load_ckp(checkpoint_fpath, model, optimizer):
    """"""
    checkpoint_path: path to save checkpoint
    model: model that we want to load checkpoint parameters into       
    optimizer: optimizer we defined in previous training
    """"""
    # load check point
    checkpoint = torch.load(checkpoint_fpath)
    # initialize state_dict from checkpoint to model
    model.load_state_dict(checkpoint['state_dict'])
    # initialize optimizer from checkpoint to optimizer
    optimizer.load_state_dict(checkpoint['optimizer'])
    # initialize valid_loss_min from checkpoint to valid_loss_min
    valid_loss_min = checkpoint['valid_loss_min']
    # return model, optimizer, epoch value, min validation loss 
    return model, optimizer, checkpoint['epoch'], valid_loss_min.item()

#Create the BertClassfier class
class BertClassifier(nn.Module):
    """"""Bert Model for Classification Tasks.""""""
    def __init__(self, freeze_bert=True):
        """"""
         @param    bert: a BertModel object
         @param    classifier: a torch.nn.Module classifier
         @param    freeze_bert (bool): Set `False` to fine-tune the BERT model
        """"""
        super(BertClassifier, self).__init__()
        
        .......
        
    def forward(self, input_ids, attention_mask):
        ''' Feed input to BERT and the classifier to compute logits.
         @param    input_ids (torch.Tensor): an input tensor with shape (batch_size,
                       max_length)
         @param    attention_mask (torch.Tensor): a tensor that hold attention mask
                       information with shape (batch_size, max_length)
         @return   logits (torch.Tensor): an output tensor with shape (batch_size,
                       num_labels) '''
         # Feed input to BERT
        outputs = self.bert(input_ids=input_ids,
                             attention_mask=attention_mask)
         
         # Extract the last hidden state of the token `[CLS]` for classification task
        last_hidden_state_cls = outputs[0][:, 0, :]
 
         # Feed input to classifier to compute logits
        logits = self.classifier(last_hidden_state_cls)
 
        return logits

def initialize_model(epochs):
    """""" Initialize the Bert Classifier, the optimizer and the learning rate scheduler.""""""
    # Instantiate Bert Classifier
    bert_classifier = BertClassifier(freeze_bert=False)

    # Tell PyTorch to run the model on GPU
    bert_classifier = bert_classifier.to(device)

    # Create the optimizer
    optimizer = AdamW(bert_classifier.parameters(),
                      lr=lr,    # Default learning rate
                      eps=1e-8    # Default epsilon value
                      )

    # Total number of training steps
    total_steps = len(train_dataloader) * epochs

    # Set up the learning rate scheduler
    scheduler = get_linear_schedule_with_warmup(optimizer,
                                                num_warmup_steps=0, # Default value
                                                num_training_steps=total_steps)
    return bert_classifier, optimizer, scheduler
    

def train(model, train_dataloader, val_dataloader, valid_loss_min_input, checkpoint_path, best_model_path, start_epochs, epochs, evaluation=True):

    """"""Train the BertClassifier model.""""""
    # Start training loop
    logging.info(""--Start training...\n"")

    # Initialize tracker for minimum validation loss
    valid_loss_min = valid_loss_min_input 


    for epoch_i in range(start_epochs, epochs):
        # =======================================
        #               Training
        # =======================================
        # Print the header of the result table
        logging.info((f""{'Epoch':^7} | {'Batch':^7} | {'Train Loss':^12} | {'Val Loss':^10} | {'Val Acc':^9} | {'Elapsed':^9}""))

        # Measure the elapsed time of each epoch
        t0_epoch, t0_batch = time.time(), time.time()

        # Reset tracking variables at the beginning of each epoch
        total_loss, batch_loss, batch_counts = 0, 0, 0

        # Put the model into the training mode
        model.train()

        # For each batch of training data...
        for step, batch in enumerate(train_dataloader):
            batch_counts +=1
            # Load batch to GPU
            b_input_ids, b_attn_mask, b_labels = tuple(t.to(device) for t in batch)

            # Zero out any previously calculated gradients
            model.zero_grad()

            # Perform a forward pass. This will return logits.
            logits = model(b_input_ids, b_attn_mask)

            # Compute loss and accumulate the loss values
            loss = loss_fn(logits, b_labels)
            batch_loss += loss.item()
            total_loss += loss.item()

            # Perform a backward pass to calculate gradients
            loss.backward()

            # Clip the norm of the gradients to 1.0 to prevent ""exploding gradients""
            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)

            # Update parameters and the learning rate
            optimizer.step()
            scheduler.step()

            # Print the loss values and time elapsed for every 20 batches
            if (step % 500 == 0 and step != 0) or (step == len(train_dataloader) - 1):
                # Calculate time elapsed for 20 batches
                time_elapsed = time.time() - t0_batch

                # Print training results
                logging.info(f""{epoch_i + 1:^7} | {step:^7} | {batch_loss / batch_counts:^12.6f} | {'-':^10} | {'-':^9} | {time_elapsed:^9.2f}"")

                # Reset batch tracking variables
                batch_loss, batch_counts = 0, 0
                t0_batch = time.time()

        # Calculate the average loss over the entire training data
        avg_train_loss = total_loss / len(train_dataloader)

        logging.info(""-""*70)
        # =======================================
        #               Evaluation
        # =======================================
        if evaluation == True:
            # After the completion of each training epoch, measure the model's performance
            # on our validation set.
            val_loss, val_accuracy = evaluate(model, val_dataloader)

            # Print performance over the entire training data
            time_elapsed = time.time() - t0_epoch
            
            logging.info(f""{epoch_i + 1:^7} | {'-':^7} | {avg_train_loss:^12.6f} | {val_loss:^10.6f} | {val_accuracy:^10.6f} | {time_elapsed:^9.2f}"")

            logging.info(""-""*70)
        logging.info(""\n"")


         # create checkpoint variable and add important data
        checkpoint = {
            'epoch': epoch_i + 1,
            'valid_loss_min': val_loss,
            'state_dict': model.state_dict(),
            'optimizer': optimizer.state_dict(),
        }
        
        # save checkpoint
        save_ckp(checkpoint, False, checkpoint_path, best_model_path)
        
        ## TODO: save the model if validation loss has decreased
        if val_loss <= valid_loss_min:
            print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(valid_loss_min,val_loss))
            # save checkpoint as best model
            save_ckp(checkpoint, True, checkpoint_path, best_model_path)
            valid_loss_min = val_loss
    
    logging.info(""-----------------Training complete--------------------------"")

def evaluate(model, val_dataloader):
    """"""After the completion of each training epoch, measure the model's performance on our validation set.""""""
    # Put the model into the evaluation mode. The dropout layers are disabled during the test time.
    model.eval()

    # Tracking variables
    val_accuracy = []
    val_loss = []

    # For each batch in our validation set...
    for batch in val_dataloader:
        # Load batch to GPU
        b_input_ids, b_attn_mask, b_labels = tuple(t.to(device) for t in batch)

        # Compute logits
        with torch.no_grad():
            logits = model(b_input_ids, b_attn_mask)

        # Compute loss
        loss = loss_fn(logits, b_labels)
        val_loss.append(loss.item())

        # Get the predictions
        preds = torch.argmax(logits, dim=1).flatten()

        # Calculate the accuracy rate
        accuracy = (preds == b_labels).cpu().numpy().mean() * 100
        val_accuracy.append(accuracy)

    # Compute the average accuracy and loss over the validation set.
    val_loss = np.mean(val_loss)
    val_accuracy = np.mean(val_accuracy)

    return val_loss, val_accuracy

bert_classifier, optimizer, scheduler = initialize_model(epochs=n_epochs)
train(model = bert_classifier ......)



bert_classifier, optimizer, scheduler = initialize_model(epochs=n_epochs)
model, optimizer, start_epoch, valid_loss_min = load_ckp(r""./best_model/best_model.pt"", bert_classifier, optimizer)

model.eval()
model.freeze()

sample = {
  ""seq"": ""ABCDE"",}

predictions = model.predict(sample)

AttributeError: 'BertClassifier' object has no attribute 'predict'

","Generally, people wrote the prediction function for you.
If not, you need to handle the low level stuff.
After this line, you loaded the trained parameters.
model, optimizer, start_epoch, valid_loss_min = load_ckp(r""./best_model/best_model.pt"", bert_classifier, optimizer)

After that, you need to do the model.forward(intput_seq,this_attention_mask_maybe_null).
You can see the forward method here is the: def forward(self, input_ids, attention_mask) in the model.
",pytorch
sklearnmodel_selectioncross_val_score has different results from a manual calculation done on a confusion matrix,"TL;DR When I calculate precision, recall, and f1 through CV cross_val_score(), it gives me different results than when I calculate through the confusion matrix. Why does it give different precision, recall, and f1 scores?
I'm learning SVM in machine learning and I wanted to compare the result returned by cross_val_score and the result I get from manually calculating the metrics from the confusion matrix. However, I have different result.
To start, I have written the code below using cross_val_score.
clf = svm.SVC()
kfold = KFold(n_splits = 10)

accuracy = metrics.make_scorer(metrics.accuracy_score)
precision = metrics.make_scorer(metrics.precision_score, average = 'macro')
recall = metrics.make_scorer(metrics.recall_score, average = 'macro')
f1 = metrics.make_scorer(metrics.f1_score, average = 'macro')

accuracy_score = cross_val_score(clf, X, y, scoring = accuracy, cv = kfold)
precision_score = cross_val_score(clf, X, y, scoring = precision, cv = kfold)
recall_score = cross_val_score(clf, X, y, scoring = recall, cv = kfold)
f1_score = cross_val_score(clf, X, y, scoring = f1, cv = kfold)

print(""accuracy score:"", accuracy_score.mean())
print(""precision score:"", precision_score.mean())
print(""recall score:"",recall_score.mean())
print(""f1 score:"", f1_score.mean())

The result for each metric is shown below:
accuracy score: 0.97
precision score: 0.96
recall score: 0.97
f1 score: 0.96

In addition, I created a Confusion Matrix so that I can manually calculate the accuracy, precision, recall, and f1 score based on the values on the matrix. I manually created the Confusion Matrix because I am using K-Fold Cross Validation. To do that, I have to get the actual classes and predicted classes for each iteration of the Cross Validation and so I have this code:
def cross_val_predict(model, kfold : KFold, X : np.array, y : np.array) -> Tuple[np.array, np.array]:
    
    model_ = cp.deepcopy(model)
    
    # gets the number of classes in the column/attribute
    no_of_classes = len(np.unique(y))
    
    # initializing empty numpy arrays to be returned
    actual_classes = np.empty([0], dtype = int)
    predicted_classes = np.empty([0], dtype = int)

    for train_index, test_index in kfold.split(X):

        X_train, X_test = X[train_index], X[test_index]
        y_train, y_test = y[train_index], y[test_index]
        
        # append the actual classes for this iteration
        actual_classes = np.append(actual_classes, y_test)
        
        # fit the model
        model_.fit(X_train, y_train)
        
        # predict
        predicted_classes = np.append(predicted_classes, model_.predict(X_test))
        
    return actual_classes, predicted_classes


Afterwards, I created my confusion matrix after calling the above function.
actual_classes, predicted_classes = cross_val_predict(clf, kfold, X, y)
cm = metrics.confusion_matrix(y_true = actual_classes, y_pred = predicted_classes)
cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = cm, display_labels = [2,4])
cm_display.plot()

Now, my confusion matrix looks like the below:
where: col is the predicted label, and row is the true label.

    |------|------|
 2  | 431  |  13  |
    |------|------|
 4  |  9   | 230  |
    |------|------|
       2      4

If I manually calcuate the accuracy, precision, recall, and f1 score from that matrix, I have the ff:
confusion matrix accuracy: 0.97
confusion matrix precision: 0.95
confusion matrix recall: 0.96
confusion matrix f1 score: 0.95

My question is that why did I get different result from manually calculating the metrics from the confusion matrix and the result from calling cross_val_score while specifying which scorer to use, i.e., [accuracy, precision, recall, fscore].
I hope you guys can help me understand why. Thank you very much for your responses!
","With cross_val_score you take the mean of the metrics calculated over each fold, but when you do it manually you concatenate the predictions before calculating the scores. Because of that, the F1 score, precision are modified, while the accuracy and recall are not affected.
Accuracy
If n is the number of samples you have, and k the number of folds, then you can write:

From this equation, you can see that in the case where each fold is the same size, averaging the accuracies is equivalent to calculating the global average. However that is not true if some folds have a different size. But because the difference is only of one participant maximum, and one participant is often small compared to the dataset size, the difference between mean over folds and computed over the whole prediction accuracies is different.
Precision
Now let's consider precision (but this is also true for f1 score or ROC AUC which are calculated from both precision); which is the ratio of true positive/positive predictions
Let's say you have 3 folds:

Fold 1: 4 positive predictions, 1 true positive: precision=1/4

Fold 2: 2 positive predictions, 2 true positive: precision=1

Fold 3: 3 positive predictions, 1 true positive: precision=1/3


Now if you take the average you will get a precision of 19/36=0.527. However if you some the number of positive predictions and true positives you get 4/9=0.44 which is quite different.
The difference comes from the fact that the denominator, i.e. your number of positive predictions, is not constant over folds. Extreme values have more influence when averaging.
Recall
Recall is the rate of True positive/Positive samples. In the case where you stratified your k-fold, the averaged value should be the same as your concatenated metric.
Which to use?
This is a question I have not yet find any clear answer to. Most frameworks or definition use the averaging of the score, but I did not find any comparison with computing the metrics over all the predictions, so here are some personal observations:

computed the metric over each fold seems to be the most used method. The advantage is that you can get the mean, but also the std/quartiles of the metrics. With this you can really assess the performances of the learning process, on the condition that you have enough data.
for a low sample size, the evaluation folds can be really short (few samples). In that case the metrics are less stable (more prone to reach extreme values) and I find the average to still be sensible to extreme values. There I would recommend that you concatenate the predictions/targets to compute the score, but again this is just a personal observation and opinion

",cross-validation
how to create a scaler applying log transformation and minmaxscaler in sklearn,"I want to apply log() to my DataFrame and MinMaxScaler() together.
I want the output to be a pandas DataFrame() with indexes and columns from the original data.
I want to use the parameters used to fit_transform() to inverse_transform() resulting in a new data frame. So, it needs to be constructed inside the FunctionTransformer.
What I tried:
from sklearn.preprocessing import FunctionTransformer
from sklearn.preprocessing import MinMaxScaler
import pandas as pd
import numpy as np

# Initialize MinMaxScaler with range (0, 1)
scaler_logMinMax = MinMaxScaler(feature_range=(0, 1))

# Log transformation function
def log_and_scale(X, scaler=scaler_logMinMax, shift=1e-9):
    X_log = np.log(X + shift)  # Apply log transformation with a small shift
    return pd.DataFrame(scaler.fit_transform(X_log))  # Scale the log-transformed data


# Inverse transformation: first unscale, then inverse log transform
def inv_log_and_scale(X, scaler=scaler_logMinMax, shift=1e-9):
    X_unscaled = scaler.inverse_transform(X)  # Inverse scaling
    return np.exp(X_unscaled) - shift  # Inverse of log transformation

# Create FunctionTransformer for the log and scale transformation
log_and_scale_transformer = FunctionTransformer(func=log_and_scale, inverse_func=inv_log_and_scale, validate=True)
df_subset = pd.DataFrame(
    {
        1: [135.2342984, 83.17136704, 23.41329775, 3.574450787],
        2: [59.31328422, 18.15285711, 11.1736562, 4.788951527],
        3: [45.0087282, 4.094515245, 106.536704, 527.0962651],
    }
)
df_subset.columns = [1, 2, 3]
df_subset.index = [""201001"", ""201002"", ""201003"", ""201004""]
df_subset.index.name = ""Date""
df_subset.columns.name = ""id""
cols_to_apply_scaler = [1, 2]
df_subset

id  1           2           3
Date            
201001  135.234298  59.313284   45.008728
201002  83.171367   18.152857   4.094515
201003  23.413298   11.173656   106.536704
201004  3.574451    4.788952    527.096265

# Transforming
df_subset[cols_to_apply_scaler] = pd.DataFrame(log_and_scale_transformer.fit_transform(df_subset[cols_to_apply_scaler]))
df_subset

id  1   2   3
Date            
201001  NaN NaN 45.008728
201002  NaN NaN 4.094515
201003  NaN NaN 106.536704
201004  NaN NaN 527.096265
# The way that I expect to apply the inverse transformer.
# df_subset[cols_to_apply_scaler] = log_and_scale_transformer.inverse_transform(df_subset[cols_to_apply_scaler])

Questions:

The pd.DataFrame(log_and_scale_transformer.fit_transform(df_subset[cols_to_apply_scaler])) works, but it can't assign to the original DataFrame because the name of columns change. How to fix it?
How the values of scaler_logMinMax from fit_transform() were carried through the inverse_transform?

I also tried log_and_scale_transformer = log_and_scale_transformer.set_output(transform=""pandas"") after creating the dataframe, but it did not work.
I need to filter the columns before applying the function.
I also want to stick with FunctionTransformer because I use other transformers with the same structure. For ex:
# Define the inverse transformation function with a shift
def inv_y(X, shift=0.5):
    return 1 / (X + shift)

# Define the inverse inverse transformation to revert to original values
def inv_inv_y(X, shift=0.5):
    return (1 - X * shift) / X

# Create the FunctionTransformer
inverse_transformer = FunctionTransformer(func=inv_y, inverse_func=inv_inv_y, validate=False, check_inverse=True)

In summary, I cannot apply a function and a scaler together.

With a different simple example, it works:
# DataFrame Example
X = np.array([[0, 1, 2], [2, 3, 4], [5, 7, 9]])
cols = [""A"", ""B"", ""C""]
cols_to_apply_scaler = cols[:-1]
X = pd.DataFrame(X, columns=cols, index=[0,1,2])
X

    A   B   C
0   0   1   2
1   2   3   4
2   5   7   9

# Transforming
X[cols_to_apply_scaler] = pd.DataFrame(log_and_scale_transformer.fit_transform(X[cols_to_apply_scaler]))

    A           B           C
0   0.000000    0.000000    2
1   0.958971    0.564575    4
2   1.000000    1.000000    9

/home/guilherme/anaconda3/envs/time_series/lib/python3.11/site-packages/sklearn/base.py:493: UserWarning: X does not have valid feature names, but FunctionTransformer was fitted with feature names
  warnings.warn(

# Inverse
X[cols_to_apply_scaler] = log_and_scale_transformer.inverse_transform(X[cols_to_apply_scaler])
X

    A               B   C
0   6.203855e-25    1.0 2
1   2.000000e+00    3.0 4
2   5.000000e+00    7.0 9

But I did not understand the warning. Can I fix it?
","Concerning your first question, to preserve the index and columns work with Dataframes.
Concerning your second question, the values used in fit_transform() are carried through to inverse_transform() because the state of the scaler is stored internally within the object instance.
Full example based on you OP:
from sklearn.preprocessing import FunctionTransformer, MinMaxScaler
import pandas as pd, numpy as np


scaler_logMinMax = MinMaxScaler(feature_range=(0, 1))

# Log transformation function
def log_and_scale(X, scaler=scaler_logMinMax, shift=1e-9):
    X_log = np.log(X + shift)
    scaled = scaler.fit_transform(X_log)
    return pd.DataFrame(scaled, index=X.index, columns=X.columns)


def inv_log_and_scale(X, scaler=scaler_logMinMax, shift=1e-9):
    unscaled = scaler.inverse_transform(X)
    return pd.DataFrame(np.exp(unscaled) - shift, index=X.index, columns=X.columns)


log_and_scale_transformer = FunctionTransformer(
    func=log_and_scale, 
    inverse_func=inv_log_and_scale, 
    validate=False  # Allow pandas
)


df_subset = pd.DataFrame(
    {
        1: [135.2342984, 83.17136704, 23.41329775, 3.574450787],
        2: [59.31328422, 18.15285711, 11.1736562, 4.788951527],
        3: [45.0087282, 4.094515245, 106.536704, 527.0962651],
    },
    index=[""201001"", ""201002"", ""201003"", ""201004""]
)
df_subset.columns = [1, 2, 3]
df_subset.index.name = ""Date""
df_subset.columns.name = ""id""
cols_to_apply_scaler = [1, 2]

# fit and transform
df_subset[cols_to_apply_scaler] = log_and_scale_transformer.fit_transform(df_subset[cols_to_apply_scaler])


print(""Transformed DataFrame:"")
print(df_subset)

# inverse transform the same columns
df_subset[cols_to_apply_scaler] = log_and_scale_transformer.inverse_transform(df_subset[cols_to_apply_scaler])


print(""\nInverse Transformed DataFrame:"")
print(df_subset)

This prints
id           1         2           3
Date                                
201001  1.000000  1.000000   45.008728
201002  0.894048  0.789684    4.094515
201003  0.574869  0.649927  106.536704
201004  0.000000  0.000000  527.096265

and
id            1          2           3
Date                                 
201001  135.234298  59.313284   45.008728
201002   83.171367  18.152857    4.094515
201003   23.413298  11.173656  106.536704
201004    3.574451   4.788952  527.096265

",scikit-learn
how to find the indexes of the first n maximum values of a tensor,"I know that torch.argmax(x, dim = 0) returns the index of the first maximum value in x along dimension 0. But is there an efficient way to return the indexes of the first n maximum values? If there are duplicate values I also want the index of those among the n indexes.
As a concrete example, say x=torch.tensor([2, 1, 4, 1, 4, 2, 1, 1]). I would like a function
generalized_argmax(xI torch.tensor, n: int)

such that
generalized_argmax(x, 4)
returns [0, 2, 4, 5] in this example.
","To acquire all you need you have to go over the whole tensor. The most efficient should therefore be to use argsort afterwards limited to n entries.
>>> x=torch.tensor([2, 1, 4, 1, 4, 2, 1, 1])
>>> x.argsort(dim=0, descending=True)[:n]
[2, 4, 0, 5]

Sort it again to get [0, 2, 4, 5] if you need the ascending order of indices.
",pytorch
grid search parameter and crossvalidated data set in knn classifier in scikitlearn,"I'm trying to perform my first KNN Classifier using SciKit-Learn. I've been following the User Guide and other online examples but there are a few things I am unsure about. For this post lets use the following
X = data
Y = target

In most introduction to machine learning pages that I've read it seems to say you want a training set, a validation set, and a test set. From what I understand, cross validation allows you to combine the training and validations sets to train the model, and then you should test it on the test set to get a score. However, I have seen in papers that in a lot of cases you can just cross validate on the entire data set and then report the CV score as the accuracy. I understand in an ideal world you would want to test on separate data but if this is legitimate I would like to cross-validate on my entire dataset and report those scores

So starting the process


I define my KNN Classifier as follows
knn = KNeighborsClassifier(algorithm = 'brute')

I search for best n_neighbors using
clf = GridSearchCV(knn, parameters, cv=5)

Now if I say
clf.fit(X,Y)

I can check the best parameter using
clf.best_params_

and then I can get a score
clf.score(X,Y)

But - as I understand it, this hasn't cross validated the model, as it only gives 1 score?
If I have seen clf.best_params_ = 14 now could I go on
knn2 = KNeighborsClassifier(n_neighbors = 14, algorithm='brute')
cross_val_score(knn2, X, Y, cv=5)

Now I know the data has been cross validated but I don't know if it is legitimate to use clf.fit to find the best parameter and then use cross_val_score with a new knn model?

I understand that the 'proper' way to do it would be as follows

Split to X_train, X_test, Y_train, Y_test,
Scale train sets -> apply transform to test sets
knn = KNeighborsClassifier(algorithm = 'brute')
clf = GridSearchCV(knn, parameters, cv=5)
clf.fit(X_train,Y_train)
clf.best_params_

and then I can get a score
clf.score(X_test,Y_test)

In this case, is the score calculated using the best parameter?

I hope that this makes sense. I've been trying to find as much as I can without posting but I have come to the point where I think it would be easier to get some direct answers.
In my head I am trying to get some cross-validated scores using the whole dataset but also use a gridsearch (or something similar) to fine tune the parameters.
","
Yes you can CV on your entire dataset it is viable, but I still suggest you to at least split your data into 2 sets one for CV and one for testing.
The .score function is supposed to return a single float value according to the documentation which is the score of the best estimator(which is the best scored estimator you get from fitting your GridSearchCV) on the given X,Y
If you saw that the best parameter is 14 than yes you can go on whith using it in your model, but if you gave it more parameters you should set all of them. 
(- I say that because you haven't given your parameters list) And yes it is legitimate to check your CV once again just in case if this model is as good as it should.

Hope that makes the things clearer :)
",cross-validation
stratified groupshufflesplit in scikitlearn,"I would like to ask if it is possible to do ""Stratified GroupShuffleSplit"" in scikit-learn which is in other words a combination of GroupShuffleSplit and StratifiedShuffleSplit
Here is a sample of the code I am using:
cv=GroupShuffleSplit(n_splits=n_splits,test_size=test_size,\
    train_size=train_size,random_state=random_state).split(\
    allr_sets_nor[:,:2],allr_labels,groups=allr_groups)
opt=GridSearchCV(SVC(decision_function_shape=dfs,tol=tol),\
    param_grid=param_grid,scoring=scoring,n_jobs=n_jobs,cv=cv,verbose=verbose)
opt.fit(allr_sets_nor[:,:2],allr_labels)

Here I applied the GroupShuffleSplit but I still want to add the startification according to allr_labels
","I solved the problem by applying StratifiedShuffleSplit on the groups and then finding training and testing sets indices manually because they are linked to the groups indices (in my case each group contains 6 successive sets from 6*index to 6*index+5)
as in the following:
sss=StratifiedShuffleSplit(n_splits=n_splits,test_size=test_size,
    train_size=train_size,random_state=random_state).split(all_groups,all_labels) 
        # startified splitting for groups only

i=0
train_is = [np.array([],dtype=int)]*n_splits
test_is = [np.array([],dtype=int)]*n_splits
for train_index,test_index in sss :
        # finding the corresponding indices of reflected training and testing sets
    train_is[i]=np.hstack((train_is[i],np.concatenate([train_index*6+i for i in range(6)])))
    test_is[i]=np.hstack((test_is[i],np.concatenate([test_index*6+i for i in range(6)])))
    i=i+1

cv=[(train_is[i],test_is[i]) for i in range(n_splits)]
        # constructing the final cross-validation iterable: list of 'n_splits' tuples;
        # each tuple contains two numpy arrays for training and testing indices respectively

opt=GridSearchCV(SVC(decision_function_shape=dfs,tol=tol),param_grid=param_grid,
                 scoring=scoring,n_jobs=n_jobs,cv=cv,verbose=verbose)
opt.fit(allr_sets_nor[:,:2],allr_labels)

",cross-validation
how to show loss values during training in scikitlearn,"I want check my loss values during the training time so I can observe the loss at each iteration. So far I haven't found an easy way for scikit learn to give me a history of loss values, nor did I find a functionality already within scikit to plot the loss for me.
If there was no way to plot this, it'd be great if I could simply fetch the final loss values at the end of classifier.fit.
Note: I am aware of the fact that some solutions are closed form. I'm using several classifiers which do not have analytical solutions, such as logistic regression and svm.
Does anyone have any suggestions? 
","So I couldn't find very good documentation on directly fetching the loss values per iteration, but I hope this will help someone in the future:
old_stdout = sys.stdout
sys.stdout = mystdout = StringIO()
clf = SGDClassifier(**kwargs, verbose=1)
clf.fit(X_tr, y_tr)
sys.stdout = old_stdout
loss_history = mystdout.getvalue()
loss_list = []
for line in loss_history.split('\n'):
    if(len(line.split(""loss: "")) == 1):
        continue
    loss_list.append(float(line.split(""loss: "")[-1]))
plt.figure()
plt.plot(np.arange(len(loss_list)), loss_list)
plt.savefig(""warmstart_plots/pure_SGD:""+str(kwargs)+"".png"")
plt.xlabel(""Time in epochs"")
plt.ylabel(""Loss"")
plt.close()

This code will take a normal SGDClassifier(just about any linear classifier), and intercept the verbose=1 flag, and will then split to get the loss from the verbose printing. Obviously this is slower but will give us the loss and print it.
",scikit-learn
leave one out encoding on test set with transform,"Context: When preprocessing a data set using sklearn, you use fit_transform on the training set and transform on the test set, to avoid data leakage. Using leave one out (LOO) encoding, you need the target variable value to calculate the encoded value of a feature value. When using the LOO encoder in a pipeline, you can apply it to the training set using the fit_transform function, which accepts the features (X) and the target values (y).
How do I calculate the LOO encodings for the test set with the same pipeline, knowing that transform does not accept the target variable values as an argument? I'm quite confused about this. The transform function indeed transforms the columns but without considering the value of the target, since it doesn't have that information.
","You shouldn't need the target variable of the test set while applying leave-one-out (or any other) encoding. Even if you somehow managed to pass it when you do your offline evaluations on the test set, how will you actually apply
it during inference time? During inference time when your model is serving traffic from real users, obviously the true label wouldn't be available. And you should always compute your test metrics such that they are representative of what happens in the real world. So conceptually, it seems wrong to use the test labels to do feature encoding.
I looked up the source code of leave-one-out encoding in the category_encoders package and it's apparent that they find the mean target without leaving the current example out when the target variable is not supplied
# Replace level with its mean target; if level occurs only once, use global mean
level_means = (colmap['sum'] / colmap['count']).where(level_notunique, self._mean)

So if I would just use the encoder like this
import category_encoders as ce
from sklearn.model_selection import train_test_split
import pandas as pd

dataframe = pd.DataFrame({
    'f1': ['P', 'Q', 'P', 'Q', 'P', 'P', 'Q', 'Q'],
    'f2': ['M', 'N', 'M', 'N', 'M', 'N', 'M', 'N'],
    'f3': ['A', 'B', 'C', 'C', 'C', 'C', 'A', 'C'],
    'y': [1, 0, 1, 0, 1, 1, 0, 0]
})

train_data, test_data = train_test_split(dataframe, test_size=0.2)

encoder = ce.LeaveOneOutEncoder(cols=['f1', 'f2', 'f3'])

encoded_train = encoder.fit_transform(train_data, train_data['y'])
encoded_test = encoder.transform(test_data)

",scikit-learn
where do i get a cpuonly version of pytorch,"I'm trying to get a basic app running with Flask + PyTorch, and host it on Heroku. However, I run into the issue that the maximum slug size is 500mb on the free version, and PyTorch itself is ~500mb.
After some google searching, someone wrote about finding a cpu-only version of PyTorch, and using that, which is much smaller how-and-why-i-built-an-ml-based-python-api-hosted-on-heroku-j74qbfwn1.
However, I'm pretty lost as to how this is done, and the person didn't document this at all. Any advice is appreciated, thanks.
EDIT:
To be more specific about my problem, I tried installing torch by (as far as I understand), including a requirements.txt which listed torch as a dependency. Current I have: torch==0.4.1. However this doesn't work bc of size.
My question is, do you know what I could write in the requirements file to get the cpu-only version of torch that is smaller, or alternatively, if the requirements.txt doesn't work for this, what I would do instead, to get the cpu version.
","Per the Pytorch website, you can install pytorch-cpu with
conda install pytorch-cpu torchvision-cpu -c pytorch

You can see from the files on Anaconda cloud, that the size varies between 26 and 56MB depending on the OS where you want to install it.
You can get the wheel from http://download.pytorch.org/whl/cpu/.
The wheel is 87MB.
You can setup the installation by putting the link to the wheel in the requirements.txt file. If you use Python 3.6 on Heroku:
http://download.pytorch.org/whl/cpu/torch-0.4.1-cp36-cp36m-linux_x86_64.whl

otherwise, for Python 2.7:
http://download.pytorch.org/whl/cpu/torch-0.4.1-cp27-cp27mu-linux_x86_64.whl

For example if your requirements are pytorch-cpu, numpy and scipy and you're using Python 3.6, the requirements.txt would look like:
http://download.pytorch.org/whl/cpu/torch-0.4.1-cp36-cp36m-linux_x86_64.whl
numpy
scipy

",pytorch
pytorch tensor to numpy array,"I have a pytorch Tensor of shape [4, 3, 966, 1296]. I want to convert it to numpy array using the following code:
imgs = imgs.numpy()[:, ::-1, :, :]

How does that code work?
","There are 4 dimensions of the tensor you want to convert.
[:, ::-1, :, :] 

: means that the first dimension should be copied as it is and converted, same goes for the third and fourth dimension.
::-1 means that for the second axes it reverses the the axes
",pytorch
unable to download tensorflow 2150,"I'm working on this at paperspace not jupyterlab or kaggle
I can't seem to download newest version of tensorflow. For some reason giving that there is no such version of it. Any ideas
ERROR: Could not find a version that satisfies the requirement tensorflow==2.15.0 (from versions: 2.2.0, 2.2.1, 2.2.2, 2.2.3, 2.3.0, 2.3.1, 2.3.2, 2.3.3, 2.3.4, 2.4.0, 2.4.1, 2.4.2, 2.4.3, 2.4.4, 2.5.0, 2.5.1, 2.5.2, 2.5.3, 2.6.0rc0, 2.6.0rc1, 2.6.0rc2, 2.6.0, 2.6.1, 2.6.2, 2.6.3, 2.6.4, 2.6.5, 2.7.0rc0, 2.7.0rc1, 2.7.0, 2.7.1, 2.7.2, 2.7.3, 2.7.4, 2.8.0rc0, 2.8.0rc1, 2.8.0, 2.8.1, 2.8.2, 2.8.3, 2.8.4, 2.9.0rc0, 2.9.0rc1, 2.9.0rc2, 2.9.0, 2.9.1, 2.9.2, 2.9.3, 2.10.0rc0, 2.10.0rc1, 2.10.0rc2, 2.10.0rc3, 2.10.0, 2.10.1, 2.11.0rc0, 2.11.0rc1, 2.11.0rc2, 2.11.0, 2.11.1, 2.12.0rc0, 2.12.0rc1, 2.12.0, 2.12.1, 2.13.0rc0, 2.13.0rc1, 2.13.0rc2, 2.13.0, 2.13.1)
ERROR: No matching distribution found for tensorflow==2.15.0


code pic
It should have had it automatically installed like it did with jupyter notebook or kaggle but it seems to not work here on paperspace
","You are using python version 3.8 which does not have tensorflow==2.15.0
Update your python version to python 3.11.^ or just simply use pip install tensorflow to download the module which is compatible with old python version.
",tensorflow
how do i fine tune bert39s self attention mechanism,"My goal is to fine tune BERT's self attention so that I can see to what extent two random sentences in a document (with positional encoding) rely on each other contextually.
Many explanations and article that I see talk about the implementation of self-attention but do not mention how to further train self attention.
Here is what I'm thinking of doing to train BERT's self attention:

Use some kind of word to vector algorithm and vectorize all the words in the article.

Add positional encoding to each sentence [where the sentence is an array of vectors (words)] using a sinusoidal function.

Make matrix of each sentence concatenated with every other sentence

For each sentence-sentence pair, iterate through each words masking them.  The model must guess the word based on context; back prop is based on accuracy of the guess.

The finished model should be able to take in an arbitrary sentence-sentence pair and output an attention matrix.


I'm not sure if such a method is the right one for fine tuning (or if this even counts as continuing pre-training) a self attention mechanism, or if BERT is even the best model to train a self attention function on.
I'm obviously very new to fine tuning LLMs, so any guidance would be greatly appreciated!
","Hugginface provides a model class, that you can use for your task:
https://huggingface.co/docs/transformers/model_doc/bert#transformers.BertForNextSentencePrediction.
See also the small example provided in the link. You can use the logits in the output to create your attention matrix.
This is just an inference task. Fine-tuning means to do further training on custom data.
",fine-tune
why sklearn39s kfold can only be enumerated once also on using it in xgboostcv,"Trying to create a KFold object for my xgboost.cv, and I have
import pandas as pd
from sklearn.model_selection import KFold

df = pd.DataFrame([[1,2,3,4,5],[6,7,8,9,10]])

KF = KFold(n_splits=2)
kf = KF.split(df)

But it seems I can only enumerate once:
for i, (train_index, test_index) in enumerate(kf):
    print(f""Fold {i}"")

for i, (train_index, test_index) in enumerate(kf):
    print(f""Again_Fold {i}"")

gives output of
Fold 0
Fold 1

The second enumerate seems to be on an empty object.
I am probably fundamentally understanding something wrong, or completed messed up somewhere, but could someone explain this behavior?
[Edit, adding follow up question] This behavior seems to cause passing KFold object to xgboost.cv setting xgboost.cv(..., folds = KF.split(df)) to have index out of range error. My fix is to recreate the list of tuples with
kf = []
for i, (train_index, test_index) in enumerate(KF.split(df)):
    this_split = (list(train_index), list(test_index))
    kf.append(this_split)

xgboost.cv(..., folds = kf)

looking for smarter solutions.
","Using an example:
from sklearn.model_selection import KFold
import xgboost as xgb
import numpy as np

data = np.random.rand(5, 10)  # 5 entities, each contains 10 features
label = np.random.randint(2, size=5)  # binary target
dtrain = xgb.DMatrix(data, label=label)

param = {'max_depth': 2, 'eta': 1, 'objective': 'binary:logistic'}

If we run your code :
KF = KFold(n_splits=2)
xgboost.cv(params= param,dtrain=dtrain, folds = KF.split(df))

I get the error :
IndexError                                Traceback (most recent call last)
Cell In[51], line 2
      1 KF = KFold(n_splits=2)
----> 2 xgboost.cv(params= param,dtrain=dtrain, folds = KF.split(df))
[..]

IndexError: list index out of range

In the documentation, it ask for a KFold instance, so you just need to do:
KF = KFold(n_splits=2)
xgb.cv(params= param,dtrain=dtrain, folds = KF)

You can check out the source code and see that it will call the split method, so you don't need to provide KF.split(..) .
",cross-validation
fine tuning freezing weights nnunet,"Good morning,
I've followed the instructions in this github issue:
https://github.com/MIC-DKFZ/nnUNet/issues/1108
to fine-tune an nnUNet model (pyTorch) on a pre-trained one, but this method retrain all weights, and i would like to freeze all weigths and retrain only the last layer's weights, changing the number of segmentation classes from 3 to 1.
Do you know a way to do that?
Thank you in advance
","To freeze the weights you need to set parameter.requires_grad = False.
Example:
from nnunet.network_architecture.generic_UNet import Generic_UNet

model = Generic_UNet(input_channels=3, base_num_features=64, num_classes=4, num_pool=3)

for name, parameter in model.named_parameters():
    if 'seg_outputs' in name:
        print(f""parameter '{name}' will not be freezed"")
        parameter.requires_grad = True
    else:
        parameter.requires_grad = False

To check parameter names you can use print:
print(model)

which produces:
Generic_UNet(
  (conv_blocks_localization): ModuleList(
    (0): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (instnorm): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (instnorm): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
  )
  (conv_blocks_context): ModuleList(
    (0): StackedConvLayers(
      (blocks): Sequential(
        (0): ConvDropoutNormNonlin(
          (conv): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (dropout): Dropout2d(p=0.5, inplace=True)
          (instnorm): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
        )
        (1): ConvDropoutNormNonlin(
          (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (dropout): Dropout2d(p=0.5, inplace=True)
          (instnorm): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
        )
      )
    )
    (1): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (dropout): Dropout2d(p=0.5, inplace=True)
            (instnorm): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (dropout): Dropout2d(p=0.5, inplace=True)
            (instnorm): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
  )
  (td): ModuleList(
    (0): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)
  )
  (tu): ModuleList(
    (0): Upsample()
  )
  (seg_outputs): ModuleList(
    (0): Conv2d(64, 4, kernel_size=(1, 1), stride=(1, 1), bias=False)
  )
)


Or you can visualize your network with netron:
https://github.com/lutzroeder/netron
",fine-tune
standardize data with kfold cross validation,"I'm using StratifiedKFold so my code looks like this 
def train_model(X,y,X_test,folds,model):
    scores=[]
    for fold_n, (train_index, valid_index) in enumerate(folds.split(X, y)):
        X_train,X_valid = X[train_index],X[valid_index]
        y_train,y_valid = y[train_index],y[valid_index]        
        model.fit(X_train,y_train)
        y_pred_valid = model.predict(X_valid).reshape(-1,)
        scores.append(roc_auc_score(y_valid, y_pred_valid))
    print('CV mean score: {0:.4f}, std: {1:.4f}.'.format(np.mean(scores), np.std(scores)))
folds = StratifiedKFold(10,shuffle=True,random_state=0)
lr = LogisticRegression(class_weight='balanced',penalty='l1',C=0.1,solver='liblinear')
train_model(X_train,y_train,X_test,repeted_folds,lr)

now before train the model I want to standardize the data so which is the correct way?
1)
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

doing this before calling train_model function
2)
doing standardization inside function like this
def train_model(X,y,X_test,folds,model):
    scores=[]
    for fold_n, (train_index, valid_index) in enumerate(folds.split(X, y)):
        X_train,X_valid = X[train_index],X[valid_index]
        y_train,y_valid = y[train_index],y[valid_index]
        scaler = StandardScaler()
        X_train = scaler.fit_transform(X_train)
        X_vaid = scaler.transform(X_valid)
        X_test = scaler.transform(X_test)
        model.fit(X_train,y_train)
        y_pred_valid = model.predict(X_valid).reshape(-1,)

        scores.append(roc_auc_score(y_valid, y_pred_valid))

    print('CV mean score: {0:.4f}, std: {1:.4f}.'.format(np.mean(scores), np.std(scores)))

As per my knowlwdge in 2nd option I'm not leaking the data.so which way is correct if I'm not using pipeline and also how to use pipeline if i want to use cross validation?
","Indeed the second option is better because the scaler does not see the values of X_valid to scale X_train.
Now if you were to use a pipeline, you can do:
from sklearn.pipeline import make_pipeline

def train_model(X,y,X_test,folds,model):
    pipeline = make_pipeline(StandardScaler(), model)
    ...

And then use pipeline instead of model. At every fit or predict call, it will automatically standardize the data at hand.
Note that you can also use the cross_val_score function from scikit-learn, with the parameter scoring='roc_auc'.
",cross-validation
get outoffold predictions from xgboostcv in python,"In the R xgboost package, I can specify predictions=TRUE to save the out-of-fold predictions during cross-validation, e.g.:
library(xgboost)
data(mtcars)
xgb_params = list(
  max_depth = 1,
  eta = 0.01
)
x = model.matrix(mpg~0+., mtcars)
train = xgb.DMatrix(x, label=mtcars$mpg)
res = xgb.cv(xgb_params, train, 100, prediction=TRUE, nfold=5)
print(head(res$pred))

How would I do the equivalent in the python package?  I can't find a prediction argument for xgboost.cvin python.
","I'm not sure if this is what you want, but you can accomplish this by using the sklearn wrapper for xgboost: (I know I'm using iris dataset as regression problem -- which it isn't but this is for illustration).
import xgboost as xgb
from sklearn.cross_validation import cross_val_predict as cvp
from sklearn import datasets
X = datasets.load_iris().data[:, :2]
y = datasets.load_iris().target
xgb_model = xgb.XGBRegressor()
y_pred = cvp(xgb_model, X, y, cv=3, n_jobs = 1)
y_pred


array([  9.07209516e-01,   1.84738374e+00,   1.78878939e+00,
         1.83672094e+00,   9.07209516e-01,   9.07209516e-01,
         1.77482617e+00,   9.07209516e-01,   1.75681138e+00,
         1.83672094e+00,   9.07209516e-01,   1.77482617e+00,
         1.84738374e+00,   1.84738374e+00,   1.12216723e+00,
         9.96944368e-01,   9.07209516e-01,   9.07209516e-01,
         9.96944368e-01,   9.07209516e-01,   9.07209516e-01,
         9.07209516e-01,   1.77482617e+00,   8.35850239e-01,
         1.77482617e+00,   9.87186074e-01,   9.07209516e-01,
         9.07209516e-01,   9.07209516e-01,   1.78878939e+00,
         1.83672094e+00,   9.07209516e-01,   9.07209516e-01,
         8.91427517e-01,   1.83672094e+00,   9.09049034e-01,
         8.91427517e-01,   1.83672094e+00,   1.84738374e+00,
         9.07209516e-01,   9.07209516e-01,   1.01038718e+00,
         1.78878939e+00,   9.07209516e-01,   9.07209516e-01,
         1.84738374e+00,   9.07209516e-01,   1.78878939e+00,
         9.07209516e-01,   8.35850239e-01,   1.99947178e+00,
         1.99947178e+00,   1.99947178e+00,   1.94922602e+00,
         1.99975276e+00,   1.91500926e+00,   1.99947178e+00,
         1.97454870e+00,   1.99947178e+00,   1.56287444e+00,
         1.96453893e+00,   1.99947178e+00,   1.99715066e+00,
         1.99947178e+00,   2.84575284e-01,   1.99947178e+00,
         2.84575284e-01,   2.00303388e+00,   1.99715066e+00,
         2.04597521e+00,   1.99947178e+00,   1.99975276e+00,
         2.00527954e+00,   1.99975276e+00,   1.99947178e+00,
         1.99947178e+00,   1.99975276e+00,   1.99947178e+00,
         1.99947178e+00,   1.91500926e+00,   1.95735490e+00,
         1.95735490e+00,   2.00303388e+00,   1.99975276e+00,
         5.92201948e-04,   1.99947178e+00,   1.99947178e+00,
         1.99715066e+00,   2.84575284e-01,   1.95735490e+00,
         1.89267385e+00,   1.99947178e+00,   2.00303388e+00,
         1.96453893e+00,   1.98232651e+00,   2.39597082e-01,
         2.39597082e-01,   1.99947178e+00,   1.97454870e+00,
         1.91500926e+00,   9.99531507e-01,   1.00023842e+00,
         1.00023842e+00,   1.00023842e+00,   1.00023842e+00,
         1.00023842e+00,   9.22234297e-01,   1.00023842e+00,
         1.00100708e+00,   1.16144836e-01,   1.00077248e+00,
         1.00023842e+00,   1.00023842e+00,   1.00100708e+00,
         1.00023842e+00,   1.00077248e+00,   1.00023842e+00,
         1.13711983e-01,   1.00023842e+00,   1.00135887e+00,
         1.00077248e+00,   1.00023842e+00,   1.00023842e+00,
         1.00023842e+00,   9.99531507e-01,   1.00077248e+00,
         1.00023842e+00,   1.00023842e+00,   1.00023842e+00,
         1.00023842e+00,   1.00023842e+00,   1.13711983e-01,
         1.00023842e+00,   1.00023842e+00,   1.00023842e+00,
         1.00023842e+00,   9.78098869e-01,   1.00023842e+00,
         1.00023842e+00,   1.00023842e+00,   1.00023842e+00,
         1.00023842e+00,   1.00023842e+00,   1.00077248e+00,
         9.99531507e-01,   1.00023842e+00,   1.00100708e+00,
         1.00023842e+00,   9.78098869e-01,   1.00023842e+00], dtype=float32)

",cross-validation
shortest syntax to use numpy 1darray as sklearn x,"I often have two numpy 1d arrays, x and y, and would like to perform some quick sklearn fitting + prediction using them.
 import numpy as np
 from sklearn import linear_model

 # This is an example for the 1d aspect - it's obtained from something else.
 x = np.array([1, 3, 2, ...]) 
 y = np.array([12, 32, 4, ...])

Now I'd like to do something like
 linear_model.LinearRegression().fit(x, y)...

The problem is that it expects an X which is a 2d column array. For this reason, I usually feed it 
 x.reshape((len(x), 1))

which I find cumbersome and hard to read. 
Is there some shorter way to transform a 1d array to a 2d column array (or, alternatively, get sklearn to accept 1d arrays)?
","You can slice your array, creating a newaxis:
x[:, None]

This:
>>> x = np.arange(5)
>>> x[:, None]
array([[0],
       [1],
       [2],
       [3],
       [4]])

Is equivalent to:
>>> x.reshape(len(x), 1)
array([[0],
       [1],
       [2],
       [3],
       [4]])

If you find it more readable, you can use a transposed matrix:
np.matrix(x).T

If you want an array:
np.matrix(x).T.A

",scikit-learn
is it necessary to use crossvalidation after data is split using stratifiedshufflesplit,"I used StratifiedShuffleSplit to split the data and now I am wondering if I need to use cross-validation again as I go for building the classification model(Logistic Regression,KNN,Random Forest etc.) I am confused about it because reading the documentation in Sklearn I get the impression that StratifiedShuffleSplit is a mix of splitting the data and cross-validating it at the same time.
","StratifiedShuffleSplit provides you just a list with train/test indices. How it will be used depends on you.

You can fit the model with train set and predict on the test and calculate the score manually - so implementing cross validation by yourself
Or you can use cross_val_score and pass StratifiedShuffleSplit() to it and cross_val_score will do the same thing.

Example:
import numpy as np
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score
from sklearn.model_selection import StratifiedShuffleSplit, cross_val_score

X = np.array([[1, 2], [3, 4], [1, 2], [3, 4], [1, 2], [3, 4]])
y = np.array([0, 0, 0, 1, 1, 1])

sss = StratifiedShuffleSplit(n_splits=5, test_size=0.5, random_state=0)
model = RandomForestClassifier(n_estimators=1, random_state=1)

# Calculate scores automatically
accuracy_per_split = cross_val_score(model, X, y, scoring=""accuracy"", cv=sss, n_jobs=1)
print(f""Accuracies per splits: {accuracy_per_split}"")

# Calculate scores manually
accuracy_per_split = []
for train_index, test_index in sss.split(X, y):
    X_train, X_test = X[train_index], X[test_index]
    y_train, y_test = y[train_index], y[test_index]

    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)

    acc = accuracy_score(y_test, y_pred)
    accuracy_per_split.append(acc)

print(f""Accuracies per splits: {accuracy_per_split}"")


",cross-validation
r caret train failed for repeatedcv with factor predictors,"The following function shall be used with Caret's train() function. Without any factor variables or without cross-validation it works fine.
The problems appear when using factors as predictors and repeatedcv, because in the folds not all the factors are present but still appear within the factor levels:
Consider the following adapted cforest model (from the package partykit):
cforest_partykit <- list(label = ""Conditional Inference Random Forest with partykit"",
          library = c(""partykit"", ""party""),
          loop = NULL,
          type = c(""Classification"", ""Regression""),
          parameters = data.frame(parameter = 'mtry',
                                  class = 'numeric',
                                  label = ""#Randomly Selected Predictors""),
          grid = function(x, y, len = NULL, search = ""grid""){
            if(search == ""grid"") {
              out <- data.frame(mtry = caret::var_seq(p = ncol(x), 
                                                      classification = is.factor(y), 
                                                      len = len))
            } else {
              out <- data.frame(mtry = unique(sample(1:ncol(x), replace = TRUE, size = len)))
            }
            out
          },
          fit = function(x, y, wts, param, lev, last, classProbs, ...) {
            
             # make consistent factor levels
                if(any(sapply(x, is.factor))){                      
                  fac_col_names <- names(grep(""factor"", sapply(x, class), value=TRUE))
                  # assign present levels to each subset
                  for (i in 1:length(fac_col_names)) {                        
                    x[, which(names(x) == fac_col_names[i])] <- factor(x[, which(names(x) == fac_col_names[i])], 
                                                                       levels = as.character(unique(x[, which(names(x) == fac_col_names[i])])))                       
                  }              
                }
                 

            dat <- if(is.data.frame(x)) x else as.data.frame(x, stringsAsFactors = TRUE)
            dat$.outcome <- y
            theDots <- list(...)
            
            if(any(names(theDots) == ""mtry"")) # # change controls to mtry?
            {
              theDots$mtry <- as.integer(param$mtry) # remove gtcrl 
              theDots$mtry
              theDots$mtry <- NULL
              
            } else mtry <- min(param$mtry, ncol(x))
            
            ## pass in any model weights
            if(!is.null(wts)) theDots$weights <- wts
            
            modelArgs <- c(list(formula = as.formula(.outcome ~ .),
                                data = dat,
                                mtry = mtry), # change controls to mtry?
                           theDots)
            
            out <- do.call(partykit::cforest, modelArgs)
            out
          },
          predict = function(modelFit, newdata = NULL, submodels = NULL) {
            if(!is.null(newdata) && !is.data.frame(newdata)) newdata <- as.data.frame(newdata, stringsAsFactors = TRUE)

            # make consistent factor levels
                if(any(sapply(newdata, is.factor))){                      
                  fac_col_names <- names(grep(""factor"", sapply(newdata, class), value=TRUE))
                  # assign present levels to each subset
                  for (i in 1:length(fac_col_names)) {                       
                    newdata[, which(names(newdata) == fac_col_names[i])] <- factor(newdata[, which(names(newdata) == fac_col_names[i])], 
                                                                       levels = as.character(unique(newdata[, which(names(newdata) == fac_col_names[i])])))                      
                  }                     
                }
                

            ## party builds the levels into the model object, so I'm
            ## going to assume that all the levels will be passed to
            ## the output
            out <- partykit:::predict.cforest(modelFit, newdata = newdata, OOB = TRUE) # predict_party, id?
            if(is.matrix(out)) out <- out[,1]
            if(!is.null(modelFit$'(response)')) out <- as.character(out) #  if(!is.null(modelFit@responses@levels$.outcome)) out <- as.character(out)
            
            out
          },
          prob = function(modelFit, newdata = NULL, submodels = NULL) { # submodels ?
            if(!is.null(newdata) && !is.data.frame(newdata)) newdata <- as.data.frame(newdata, stringsAsFactors = TRUE)
            obsLevels <- levels(modelFit$'(response)')
            rawProbs <- partykit::predict.cforest(modelFit, newdata = newdata, OOB = TRUE) # predict(, type=""prob) ? id?
            probMatrix <- matrix(unlist(rawProbs), ncol = length(obsLevels), byrow = TRUE)
            out <- data.frame(probMatrix)
            colnames(out) <- obsLevels
            rownames(out) <- NULL
            out
          },
          predictors = function(x, ...) {
            vi <- partykit::varimp(x, ...)
            names(vi)[vi != 0]
          },
          varImp = function(object, ...) {
            variableImp <- partykit::varimp(object, ...)
            out <- data.frame(Overall = variableImp)
            out
          },
          tags = c(""Random Forest"", ""Ensemble Model"", ""Bagging"", ""Implicit Feature Selection"", ""Accepts Case Weights""),
          levels = function(x) levels(x@data@get(""response"")[,1]),
          sort = function(x) x[order(x[,1]),],
          oob = function(x) {
            obs <- x@data@get(""response"")[,1]
            pred <- partykit:::predict.cforest(x, OOB = TRUE, newdata = NULL)
            postResample(pred, obs)
          })

When applying it within train and repeatedcv using a data frame with a factor predictor variable, an error occurs:
library(caret)
library(party)
library(partykit)

dat <- as.data.frame(ChickWeight)[1:20,]
dat$class <- as.factor(rep(letters[seq( from = 1, to = 20)], each=1))

# specifiy folds with CreateMultiFolds
set.seed(43, kind = ""Mersenne-Twister"", normal.kind = ""Inversion"")
folds_train <- caret::createMultiFolds(y = dat$weight,
                                   k = 3,   
                                   times = 2)

# specifiy trainControl for tuning mtry and with specified folds
finalcontrol <- caret::trainControl(search = ""grid"", method = ""repeatedcv"", number = 3, repeats = 2, 
                                    index = folds_train, 
                                    savePred = T)

preds <- dat[,2:5]
response <- dat[,1]

# tune hyperparameter mtry and build final model
tunegrid <- expand.grid(mtry=c(1,2,3,4)) 
#set.seed(42, kind = ""Mersenne-Twister"", normal.kind = ""Inversion"")
model <- caret::train(x = preds, # predictors
                      y = response, # response
                      method = cforest_partykit,
                      metric = ""RMSE"", 
                      tuneGrid = tunegrid, 
                      trControl = finalcontrol,
                      ntree = 150)

warnings()
1: predictions failed for Fold1.Rep1: mtry=1 Error in model.frame.default(object$predictf, data = newdata, na.action = na.pass, : factor class has new levels a, c, g, k, m, p, s, t
The aim is to identify the levels of each fold.rep and assign only those, which are present in the respective fold:
for (i in 1:length(folds_train)) {

  preds_temp <- preds[folds_train[[i]],]
  # check levels 
  levels(preds_temp$class)
  # which are actually present
  unique(preds_temp$class)
  # assign present levels to each subset
  preds_temp$class <- factor(preds_temp$class, levels = as.character(unique(preds_temp$class)))

}

I tried to include the assignment of the right factor levels within the cforest_partykit function (# make consistent factor levels), but it seems to have no effect.
How could I implement this in the caret train() or trainControl() or createDataPartition() function?
","To make sure cforest_partykit treats categorical variables appropriately, it is best to create the design matrix explicitly through the model.matrix command.
For example
# Create a formula for the model
model_formula <- as.formula(""y_column ~ . -1"")

# Then create the design matrix
model_train.design.matrix <- model.matrix(model_formula, data = dat)

# Add in the y-variable
model_train.design.data <- cbind(y_column = data$y_column, model_train.design.matrix)

",cross-validation
how to cluster parts of a mask in an image using python,"I need to split a mask in such a way that if there is an inconsistency within the mask, it gets separated. For example, if I draw a mask on a cat, I want the wide part (the body) to be one mask and the narrow part (the tail) to be another.
Currently, I have a continuous mask that includes both the cat's body and its tail. I want to separate this into two distinct masks. How can I achieve this using Python?
original mask

desired mask

I looked into using methods described in this, which focuses on polygon partitioning and separating contours into multiple triangles. However, this approach does not suit my needs, as I want to split the mask based on size and shape rather than creating multiple triangular partitions.
","You can use Convexity defects to identify the points to ""cut"" between.
It is done like this here for example.
import cv2
import matplotlib.pyplot as plt
import numpy as np

def split_mask(mask):
    _, thresh = cv2.threshold(mask , 120,255, cv2.THRESH_BINARY)
    contours, _ = cv2.findContours(thresh, 2, 1)

    for contour in contours:
        if  cv2.contourArea(contour) > 20:
            hull = cv2.convexHull(contour, returnPoints = False)
            defects = cv2.convexityDefects(contour, hull)
            if defects is None:
                continue
            
            # Gather all defect points to filter them.
            dd = [e[0][3]/256 for e in defects]
            points = []
            for i in range(len(dd)):
                _,_,f,_ = defects[i,0]
                if dd[i] > 1.0 and dd[i]/np.max(dd) > 0.2:
                    points.append(f)

            # If there is defect points, erase the mask closest points.
            if len(points) >= 2:
                for i, f1 in enumerate(points):
                    p1 = tuple(contour[f1][0])
                    nearest = min((tuple(contour[f2][0]) for j, f2 in enumerate(points) if i != j),
                                  key=lambda p2: (p1[0] - p2[0])**2 + (p1[1] - p2[1])**2)
                    cv2.line(thresh, p1, nearest, 0, 20)
    return thresh    

if __name__==""__main__"":
    mask = cv2.imread(""<path-to-your-image>"", cv2.IMREAD_GRAYSCALE)
    mask_splitted = split_mask(mask)
    plt.imshow(mask_splitted)
    plt.show()

This yield the following on your image:

",scikit-learn
warningtensorflowlayers in a sequential model should only have a single input tensor,"I have copy past code from tensorflow website's introduction to autoencoder first examplefollowing code works with mnist fashion dataset but not mine.This gives me a very long warning.Please tell me what is worng with my dataset
the warning
screen short of same error
here x_train is my dataset:
tf.shape(x_train)

output <tf.Tensor: shape=(3,), dtype=int32, numpy=array([169,**  **28,  28])>

here x_train is the mnist dataset:
tf.shape(x_train)

output<tf.Tensor: shape=(3,), dtype=int32, numpy=array([60000,    28,    28])>

My whole code to make dataset:
dir_path='auto/ttt/'
data=[]
x_train=[]
for i in os.listdir(dir_path):
    img=image.load_img(dir_path+'//'+i,color_mode='grayscale',target_size=(28,28))  
    data=np.array(img)
    data=data/255.0
    x_train.append(data)

this is the warning:
WARNING:tensorflow:Layers in a Sequential model should only have a single input tensor. Received: inputs=(<tf.Tensor 'IteratorGetNext:0' shape=(None, 28) dtype=float32>, <tf.Tensor 'IteratorGetNext:1' shape=(None, 28) dtype=float32>, <tf.Tensor 'IteratorGetNext:2' shape=(None, 28) 
dtype=float32>, <tf.Tensor 'IteratorGetNext:3' shape=(None, 28) 
dtype=float32>, <tf.Tensor 'IteratorGetNext:4' shape=(None, 28) dtype=float32>, <tf.Tensor 'IteratorGetNext:5' shape=(None, 28) dtype=float32>, <tf.Tensor 'IteratorGetNext:6' shape=(None, 28) dtype=float32>, <tf.Tensor 'IteratorGetNext:7' shape=(None, 28) dtype=flo...

also this value error (same warning):
ValueError: Exception encountered when calling layer ""sequential_4"" (type Sequential).
        
Layer ""flatten_2"" expects 1 input(s), but it received 169 input tensors. Inputs received: [<tf.Tensor 'IteratorGetNext:0' shape=(None, 28) dtype=float32>, <tf.Tensor 'IteratorGetNext:1' shape=(None, 28) dtype=float32>, <tf.Tensor 'IteratorGetNext:2' shape=(None, 28) dtype=float32>, <tf.Tensor 'IteratorGetNext:3' shape=(None, 28) dtype=float32>, <tf.Tensor 'IteratorGetNext:4' shape=(None, 28) dtype=float32>, <tf.Tensor 'IteratorGetNext:5' shape=(None, 28) dtype=float32>, <tf.Tensor 'IteratorGetNext:6' shape=(None, 28) dtype=float32>, <tf.Tensor 'IteratorGetNext:7' shape=(None, 28) dtype=float32>, <tf.Tensor 'IteratorGetNext:8' shape=(None, 28) dtype=float3...

","The model.fit() is given a list of arrays as input. A list of arrays is generally passed to fit() when a model has multiple inputs. In this case, the fit() method is treating each array as an input, resulting in the error.
Please convert the data to a tensor as follows and try again.
x_train=tf.convert_to_tensor(x_train) 

Refer to the gist for complete code.
",tensorflow
setting hidden layers and neurons in neuralnet and caret r,"I would like to cross-validate a neural network using the package neuralnet and caret.
The data df can be copied from this post. 
When running the neuralnet() function, there is an argument called hidden where you can set the hidden layers and neurons in each. Let's say I want 2 hidden layers with 3 and 2 neurons respectively. It would be written as hidden = c(3, 2).
However, as I want to cross-validate it, I decided to use the fantastic caret package. But when using the function train(), I do not know how to set the number of layers and neurons.
Does anyone know where can I add these numbers?
This is the code I ran:
nn <- caret::train(DC1 ~ ., data=df, 
                   method = ""neuralnet"", 
                   #tuneGrid = tune.grid.neuralnet,
                   metric = ""RMSE"",
                   trControl = trainControl (
                     method = ""cv"", number = 10,
                     verboseIter = TRUE
))

By the way, I am getting some warnings with the previous code:
predictions failed for Fold01: layer1=3, layer2=0, layer3=0 Error in cbind(1, pred) %*% weights[[num_hidden_layers + 1]] : 
  requires numeric/complex matrix/vector arguments

Ideas on how to solve it?
","When using neural net model in caret in order to specify the number of hidden units in each of the three supported layers you can use the parameters layer1, layer2 and layer3. I found out by checking the source.
library(caret)

grid <-  expand.grid(layer1 = c(32, 16),
                     layer2 = c(32, 16),
                     layer3 = 8)

Use case with BostonHousing data:
library(mlbench)

data(BostonHousing)

lets just select numerical columns for the example to make it simple:
BostonHousing[,sapply(BostonHousing, is.numeric)] -> df

nn <- train(medv ~ ., 
            data = df, 
            method = ""neuralnet"", 
            tuneGrid = grid,
            metric = ""RMSE"",
            preProc = c(""center"", ""scale"", ""nzv""), #good idea to do this with neural nets - your error is due to non scaled data
            trControl = trainControl(
              method = ""cv"",
              number = 5,
              verboseIter = TRUE)
            )

The part
preProc = c(""center"", ""scale"", ""nzv"")

is essential for the algorithm to converge, neural nets don't like unscaled features
Its super slow though.
nn
#output
Neural Network 

506 samples
 12 predictor

Pre-processing: centered (12), scaled (12) 
Resampling: Cross-Validated (5 fold) 
Summary of sample sizes: 405, 404, 404, 405, 406 
Resampling results across tuning parameters:

  layer1  layer2  RMSE      Rsquared   MAE     
  16      16           NaN        NaN       NaN
  16      32      4.177368  0.8113711  2.978918
  32      16      3.978955  0.8275479  2.822114
  32      32      3.923646  0.8266605  2.783526

Tuning parameter 'layer3' was held constant at a value of 8
RMSE was used to select the optimal model using the smallest value.
The final values used for the model were layer1 = 32, layer2 = 32 and layer3 = 8.

",cross-validation
tensorflow create protobuf for a tensor,"I want to have a Python script that converts Numpy arrays to TensorFlow Tensors in Protobuf Binary so later in C++ I can reload them. This can be done with a compute graph like this.
I found the following functions and features in the TensorFlow Python API.

Python Tensor Object
convert_to_tensor
Registration system for to_proto functions

C++ has a corresponding load operation
Can you give me an example of serializing a TF tensor to Protobuf binary and back?
","I'll post the answer as I figure it out, so perhaps someone can pitch in with the rest of the solution.
Python
Tensor -> Protobuf Binary
>>> import tensorflow as tf
>>> with tf.Graph().as_default():
...     s = tf.constant([1.2, 3.4, 5.6, 7.8])._op.node_def.attr['value'].SerializeToString()
...
>>> s
'B\x1a\x08\x01\x12\x04\x12\x02\x08\x04""\x10\x9a\x99\x99?\x9a\x99Y@33\xb3@\x9a\x99\xf9@'

Protobuf Binary -> Tensor
>>> import tensorflow as tf
>>> s = 'B\x1a\x08\x01\x12\x04\x12\x02\x08\x04""\x10\x9a\x99\x99?\x9a\x99Y@33\xb3@\x9a\x99\xf9@'
>>> with tf.Graph().as_default():
...     c = tf.constant(1)
...     c._op.node_def.attr['value'].ParseFromString(s)
...     c._op.node_def.attr['dtype'].type = c._op.node_def.attr['value'].tensor.dtype
...     print c.eval(session=tf.Session())
... 
28
[ 1.20000005  3.4000001   5.5999999   7.80000019]

Benchmarks
   Array Elements  from_string [us]  to_string [us]
0              10         10.273593        2.308139
1             100         10.450414        2.291126
2            1000         10.540897        2.359392
3           10000         12.175265        2.734819
4          100000         31.460438        7.349958


Benchmark script
import tensorflow as tf
import pandas as pd
import numpy as np
import timeit
import matplotlib.pyplot as plt

def to_string(shape):
    with tf.Graph().as_default():
        s = tf.constant(np.empty(shape))._op.node_def.attr['value'].SerializeToString()
    return s


def from_string(s):
    with tf.Graph().as_default():
        c = tf.constant(1)
        c._op.node_def.attr['value'].ParseFromString(s)
        c._op.node_def.attr['dtype'].type = c._op.node_def.attr['value'].tensor.dtype
        c.eval(session=tf.Session())

NUM_RUNS = 10000
MAX_POW = 6

print ""Collecting to_string stats""
to_string_results = np.array([[N, timeit.timeit('to_string((%d,))' % N,
                                                setup=""from __main__ import to_string"",
                                                number=NUM_RUNS)]
                              for N in 10**np.arange(1, MAX_POW)]).T

print ""Collecting from_string stats""
strings = {N:to_string((N,)) for N in 10**np.arange(1, MAX_POW)}
from_string_results = np.array([[N, timeit.timeit('from_string(strings[%d])' % N,
                                                  setup=""from __main__ import from_string, strings"",
                                                  number=NUM_RUNS)]
                                for N in 10**np.arange(1, MAX_POW)]).T

df = pd.DataFrame.from_dict({""Array Elements"": to_string_results[0],
                             ""to_string [us]"": to_string_results[1],
                             ""from_string [us]"": from_string_results[1]})

print df
df.to_csv('benchmark.csv')

plt.subplot(2, 1, 1)
plt.loglog(to_string_results[0], to_string_results[1])
plt.title('to_string')
plt.ylabel('microseconds')
plt.xlabel('Array size')
plt.subplot(2, 1, 2)
plt.loglog(from_string_results[0], from_string_results[1])
plt.title('from_string')
plt.ylabel('microseconds')
plt.xlabel('Array size')
plt.show()

C++
Working on it
",tensorflow
compute hessian matrix only diagonal part with respect to a high rank tensor,"I would like to compute the first and the second derivatives (diagonal part of Hessian) of my specified Loss with respect to each feature map of a vgg16 conv4_3 layer's kernel which is a 3x3x512x512 dimensional matrix. I know how to compute derivatives if it is respected to a low-rank one according to How to compute all second derivatives (only the diagonal of the Hessian matrix) in Tensorflow?
However, when it turns to higher-rank, I got completed lost.
# Inspecting variables under Ipython notebook
In  : Loss 
Out : <tf.Tensor 'local/total_losses:0' shape=() dtype=float32>

In  : conv4_3_kernel.get_shape() 
Out : TensorShape([Dimension(3), Dimension(3), Dimension(512), Dimension(512)])

## Compute derivatives
Grad = tf.compute_gradients(Loss, conv4_3_kernel)
Hessian = tf.compute_gradients(Grad, conv4_3_kernel)

In  : Grad 
Out : [<tf.Tensor 'gradients/vgg/conv4_3/Conv2D_grad/Conv2DBackpropFilter:0' shape=(3, 3, 512, 512) dtype=float32>]

In  : Hessian 
Out : [<tf.Tensor 'gradients_2/vgg/conv4_3/Conv2D_grad/Conv2DBackpropFilter:0' shape=(3, 3, 512, 512) dtype=float32>]

Please help me to check my understandings. So, for conv4_3_kernel, each dim stand for [Kx, Ky, in_channels, out_channels], so Grad should be partial derivatives of Loss with respect to each element(pixel) in the each feature maps. And Hessian is the second derivatives.
But, Hessian computes all the derivatives, how can I only compute only the diagonal part? should I use tf.diag_part()?
","tf.compute_gradients computes derivative of a scalar quantity. If the quantity provided isn't scalar, it turns it into scalar by summing up the components which is what's happening in your example
To compute full Hessian you need n calls to tf.gradients, The example is here. If you want just the diagonal part, then modify arguments to ith call to tf.gradients to differentiate with respect to ith variable, rather than all variables.
",tensorflow
python  lightgbm with gridsearchcv is running forever,"Recently, I am doing multiple experiments to compare Python XgBoost and LightGBM. It seems that this LightGBM is a new algorithm that people say it works better than XGBoost in both speed and accuracy.
This is LightGBM GitHub.
This is LightGBM python API documents, here you will find python functions you can call. It can be directly called from LightGBM model and also can be called by LightGBM scikit-learn.
This is the XGBoost Python API I use. As you can see, it has very similar data structure as LightGBM python API above.
Here are what I tried:

If you use train() method in both XGBoost and LightGBM, yes lightGBM works faster and has higher accuracy. But this method, doesn't have cross validation.
If you try cv() method in both algorithms, it is for cross validation. However, I didn't find a way to use it return a set of optimum parameters.
if you try scikit-learn GridSearchCV() with LGBMClassifier and XGBClassifer. It works for XGBClassifer, but for LGBClassifier, it is running forever.

Here are my code examples when using GridSearchCV() with both classifiers:
XGBClassifier with GridSearchCV
param_set = {
 'n_estimators':[50, 100, 500, 1000]
}
gsearch = GridSearchCV(estimator = XGBClassifier( learning_rate =0.1, 
n_estimators=100, max_depth=5,
min_child_weight=1, gamma=0, subsample=0.8, colsample_bytree=0.8, 
nthread=7,
objective= 'binary:logistic', scale_pos_weight=1, seed=410), 
param_grid = param_set, scoring='roc_auc',n_jobs=7,iid=False, cv=10)

xgb_model2 = gsearch.fit(features_train, label_train)
xgb_model2.grid_scores_, xgb_model2.best_params_, xgb_model2.best_score_

This works very well for XGBoost, and only tool a few seconds.
LightGBM with GridSearchCV
param_set = {
 'n_estimators':[20, 50]
}

gsearch = GridSearchCV(estimator = LGBMClassifier( boosting_type='gbdt', num_leaves=30, max_depth=5, learning_rate=0.1, n_estimators=50, max_bin=225, 
 subsample_for_bin=0.8, objective=None, min_split_gain=0, 
 min_child_weight=5, 
 min_child_samples=10, subsample=1, subsample_freq=1, 
colsample_bytree=1, 
reg_alpha=1, reg_lambda=0, seed=410, nthread=7, silent=True), 
param_grid = param_set, scoring='roc_auc',n_jobs=7,iid=False, cv=10)

lgb_model2 = gsearch.fit(features_train, label_train)
lgb_model2.grid_scores_, lgb_model2.best_params_, lgb_model2.best_score_

However, by using this method for LightGBM, it has been running the whole morning today still nothing generated.
I am using the same dataset, a dataset contains 30000 records.
I have 2 questions:

If we just use cv() method, is there anyway to tune optimum set of parameters?
Do you know why GridSearchCV() does not work well with LightGBM? I'm wondering whether this only happens on me all it happened on others to?

","Try to use n_jobs = 1 and see if it works.
In general, if you use n_jobs = -1 or n_jobs > 1 then you should protect your script by using if __name__=='__main__': :
Simple Example:
import ...

if __name__=='__main__':

    data= pd.read_csv('Prior Decompo2.csv', header=None)
    X, y = data.iloc[0:, 0:26].values, data.iloc[0:,26].values
    param_grid = {'C' : [0.01, 0.1, 1, 10], 'kernel': ('rbf', 'linear')}
    classifier = SVC()
    grid_search = GridSearchCV(estimator=classifier, param_grid=param_grid, scoring='accuracy', n_jobs=-1, verbose=42)
    grid_search.fit(X,y)

Finally, can you try to run your code using  n_jobs = -1 and including if __name__=='__main__': as I explained and see if it works?
",cross-validation
calculate the number of points in a given radius by x and y coordinates,"I have a python code where, when given a list of points (X and Y coordinate pairs) in a dataframe and a specified circle radius I would like the code to go through each point, treat it as the the center of the circle, and find how many other points are inside the circle selection.
Looking around Ive seen people suggest a similar problem using longitudes and latitudes can be solved with BallTree from SciKit-Learn so I've tried that but I'm not getting the answers I'm expecting. My code is below:
import numpy as np
import pandas as pd
from sklearn.neighbors import BallTree

df = pd.DataFrame({'id':list('abcde'),'X': [10, 1000, 1010, 5000, 5100],'Y': [10, 1000, 1010, 5000, 5100]})

radius = int(input('Enter the selection radius of the circle:'))

coords = df[[""X"",""Y""]]

tree = BallTree(coords, metric='haversine')
answers = tree.query_radius(coords, r=radius, count_only=True)

print(answers)

For example when i do radius = 100 i get an answer of [1 1 1 1 1] which is not correct. Any ideas how to get this to work in the simplest way in python?
","The issue you're encountering is because the BallTree is being used with the haversine metric, which is designed for computing distances on a spherical surface, typically used with longitude and latitude coordinates. However, in your case, you're working with Cartesian coordinates (X, Y), where the Euclidean distance should be used instead.
You should replace this line:
tree = BallTree(coords, metric='haversine')

With:
tree = BallTree(coords, metric='euclidean')

",scikit-learn
how to disable neptune callback in transformers trainer runs,"After installing Neptune.ai for occasional ML experiments logging, it became included by default into the list of callbacks in all transformers.trainer runs. As a result, it requires proper initialisation with token or else throws NeptuneMissingConfiguration error, demanding token and project name.
This is really annoying, I'd prefer Neptune callback to limit itself to warning or just have it disabled if no token is provided.
Unfortunately there is no obvious way to disable this callback, short of uninstalling Neptune.ai altogether. The doc page at https://huggingface.co/docs/transformers/main_classes/callback states that this callback is enabled by default and gives no way to disable it (unlike some other callbacks that can be disabled by environment variable).
Question: how to disable Neptune callback on per run basis?
","The reason Neptune is included is because the default value of report_to in TrainingArguments is ""all"", which implicitly includes all installed loggers from the officially supported list of loggers. You should either uninstall Neptune from the environment you use for the project, or pass report_to=""none"" to the TrainingArguments instance you use to initialize the Trainer (n.b.: that's the string literal ""none"", NOT a Python None).
The other answers here, including the accepted answer, are either poor workarounds for this problem, or simply do not work at all. The proper way to handle this issue is as above.
",pytorch
running modelfit multiple times without reinstantiating the model,"Background
I am watching a popular YouTube crash course on machine learning.
At 3:35:50, he mentions that the model is likely overfit, so fits it again with less epochs.
Since he didn't reinstantiate the model, isn't this equivalent to fitting the model with that same data, thereby continuing to overtrain it?
My Question
Assume you have a model created and data ready to go.
You run:
model.fit(train_images, train_labels, epochs=10)
model.fit(train_images, train_labels, epochs=8)

Is this equivalent to running:
model.fit(train_images, train_labels, epochs=18)

Or:
model.fit(train_images, train_labels, epochs=8)

If previously fitted data is overwritten, why does running model.fit a second time begin with the accuracy of the previous model?
In multiple other questions regarding saving and training models, the accepted solutions are to load the previously trained model, and run model.fit again. 
If this will overwrite the pre-existing weights, doesn't that defeat the purpose of saving the model in the first place? Wouldn't training the model for the first time on the new data be equivalent?
What is the appropriate way to train a model across multiple, similar datasets while retaining accuracy across all of the data?
","
Since he didn't reinstantiate the model, isn't this equivalent to
  fitting the model with that same data, thereby continuing to overtrain
  it?

You are correct! In order to check which number of epochs would do better in his example, he should have compiled the network again (that is, execute the above cell again).
Just remember that in general, whenever you instantiate a model again it most likely will start with completely new weights, totally different from past weights (unless you change this manually). So even though you keep the same amount of epochs, your final accuracy can change depending on the initial weights.
Are these two commands equivalent?
model.fit(train_images, train_labels, epochs=10)
model.fit(train_images, train_labels, epochs=8)

and
model.fit(train_images, train_labels, epochs=18)

No.
In the first case, you are training your network with some weights X going through all your training set 10 times, then you update your weights for some value y.
Then you will train your network again though all your training set 8 times but now you are using a network with weights X+y. 
For the second case, you will train your network through all your training data 18 times with the weights X. 
This is different!
",tensorflow
understanding states of a bidirectional lstm in a seq2seq model tf keras,"I am creating a language model: A seq2seq model with 2 Bidirectional LSTM layers. I have got the model to train and the accuracy seems good, but whilst stuck on figuring out the inference model, I've found myself a bit confused by the states that are returned by each LSTM layer.
I am using this tutorial as a guide, though the example in this link is not using bidriectional layers: https://blog.keras.io/a-ten-minute-introduction-to-sequence-to-sequence-learning-in-keras.html
Note: I am using a pretrained word embedding.
lstm_units = 100

# Set up embedding layer using pretrained weights
embedding_layer = Embedding(total_words+1, emb_dimension, input_length=max_input_len, weights=[embedding_matrix], name=""Embedding"")

# Encoder
encoder_input_x = Input(shape=(None,), name=""Enc_Input"")
encoder_embedding_x = embedding_layer(encoder_input_x)
encoder_lstm_x, enc_state_h_fwd, enc_state_c_fwd, enc_state_h_bwd, enc_state_c_bwd = Bidirectional(LSTM(lstm_units, dropout=0.5, return_state=True, name=""Enc_LSTM1""), name=""Enc_Bi1"")(encoder_embedding_x)
encoder_states = [enc_state_h_fwd, enc_state_c_fwd, enc_state_h_bwd, enc_state_c_bwd]

# Decoder
decoder_input_x = Input(shape=(None,), name=""Dec_Input"")
decoder_embedding_x = embedding_layer(decoder_input_x)
decoder_lstm_layer = Bidirectional(LSTM(lstm_units, return_state=True, return_sequences=True, dropout=0.5, name=""Dec_LSTM1""))
decoder_lstm_x, _, _, _, _= decoder_lstm_layer(decoder_embedding_x, initial_state=encoder_states) 
decoder_dense_layer = TimeDistributed(Dense(total_words+1, activation=""softmax"", name=""Dec_Softmax""))
decoder_output_x = decoder_dense_layer(decoder_lstm_x)

model = Model(inputs=[encoder_input_x, decoder_input_x], outputs=decoder_output_x)

model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])

I believe  diagram of the model looks like this, with 60 time steps.:

I want the encoder to pass the enc_state_h_fwd and enc_state_c_fwd forward to the decoder. This connection is highlighted by the orange arrow.
But since the model is bidirectional, I have some questions:

Do I need to pass the decoder states backwards to the encoder? And how would one possibly do this, it seems like a chicken and egg scenario.
The encoder_states that come from the encoder lstm layer output 4 states. h and c states going forward and backward. I feel like the ""backward"" states are denoted in my diagram by the pink arrow going left out of the encoder. I am passing these to the decoder, but why does it need them? Am I incorrectly connecting the pink arrow on the left to the purple arrow going into the decoder from the right?

","This model is not valid. It is set up as a translation model, which during inference would predict one word at a time, starting with the start of sequence token, to predict y1, then looping and feeding in the start of sequence token, y1 to get y2 etc.
A bidirectional LSTM cannot be used for real time predictions in a many to many prediction unless the entire decoder input is available. In this case, the decoder input is only available after predicting one step at a time, so the first prediction (of y1) is invalid without the rest of the sequence (y2-yt).
The decoder should therefore not be a bidirectional LSTM.
As for the states, the encoder Bidirectional LSTM does indeed output h and c states going forward (orange arrow), and h and c states going backward (pink arrow).
By concatenating these states and feeding them to the decoder, we can give the decoder more information. This is possible as we do have the entire encoder input at time of inference.
Also to be noted is that the bidirectional encoder with lstm_units (eg. 100) effectively has 200 lstm units, half going forward, half going backward. To feed these into the decoder, the decoder must have 200 units too.
",tensorflow
what is the current 171official187 waybest practice to train an object detection model using the tensorflow framework,"My goal is to use transfer learning to train an object detection model with my own data (bounding boxes) and convert the models to tflite-format.
I did it before, using the Tensorflow Object Detection API which was great because it provided a wide range of models to use for transfer learning. But the API is no longer maintained and I want to work with something “up to date”.
So I checked on the TensorFlow page and found this tutorial but it again uses the Object Detection API, which is deprecated, as stated before.
I also found this tutorial which uses the TensorFlow Lite Model Maker library so I gave the linked Collab a try but I don’t even get past the pip-install because there are some errors regarding the required versions of different packages (we are talking about the official tutorial collab here! And I ran into the same errors when using the code on my PC).
Then there is MediaPipe with this tutorial When I run the Collab I get a warning.

“TensorFlow Addons (TFA) has ended development and introduction of new features. TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.”

So again, some kind of dead end. And when I tried to run it on my PC I had some issues with importing parts of mediapipe_model_maker.
At this point, I am running out of ideas to be honest. I’m “just” looking for a way to use transfer learning with object detection models (especially smaller ones like Mobilenet, EfficientDet). Is there really no worthy successor of Tensorflow Object Detection API? Am I missing something? Some new API I didn't find, some package I am not aware of?
","Best thing I could do at this point was to use “tflite_model_maker” which was a challenge in itself because of dependency issues.
It still says “TensorFlow Addons (TFA) has ended development and introduction of new features.TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.”
But at least I can train “efficientdet_lite0” now until a better solution comes up (I’ve seen seen many comments on GitHub saying that MediaPipe should be used but I ran into even more dependency issues there).
So in case someone else wants to train an efficientdet_lite network for object detection, that’s how I set up my Conda-environment on Windows, maybe it helps (for the code itself, see tutorial here).

Start with a clean environment and install Python 3.9

Next (to prevent this issue from happening):
pip install ""cython<3.0.0"" wheel

pip install ""pyyaml==5.4.1"" --no-build-isolation


pip install tflite-model-maker

pip install pycocotools (now you would have everything installed that you need but there will be errors, which I fixed with the following steps)

pip install numpy==1.23.5 (to fix AttributeError: module 'numpy' has no attribute 'object', see here)

pip install --upgrade tensorflow-datasets==4.9.1 (to fix ImportError: cannot import name 'array_record_module' from 'array_record.python', see here)

And finally pip uninstall -y tensorflow && pip install -q tensorflow==2.8.0 (to fix  AttributeError: module 'keras.api._v2.keras.mixed_precision' has no attribute 'experimental', again see here)


After that, I had a working environment to train the models. It’s ugly, but it works for the moment. ;)
I’m still open for better solutions though...
",tensorflow
not able to use stratifiedkfold on multi label classifier,"The following code is used to do KFold Validation but I am to train the model as it is throwing the error
ValueError: Error when checking target: expected dense_14 to have shape (7,) but got array with shape (1,)

My target Variable has 7 classes. I am using LabelEncoder to encode the classes into numbers. 
By seeing this error, If I am changing the into MultiLabelBinarizer to encode the classes. I am getting the following error
ValueError: Supported target types are: ('binary', 'multiclass'). Got 'multilabel-indicator' instead.

The following is the code for KFold validation
skf = StratifiedKFold(n_splits=10, shuffle=True)
scores = np.zeros(10)
idx = 0
for index, (train_indices, val_indices) in enumerate(skf.split(X, y)):
    print(""Training on fold "" + str(index+1) + ""/10..."")
    # Generate batches from indices
    xtrain, xval = X[train_indices], X[val_indices]
    ytrain, yval = y[train_indices], y[val_indices]
    model = None
    model = load_model() //defined above

    scores[idx] = train_model(model, xtrain, ytrain, xval, yval)
    idx+=1
print(scores)
print(scores.mean())

I don't know what to do. I want to use Stratified K Fold on my model. Please help me.
","MultiLabelBinarizer returns a vector which is of the length of your number of classes.
If you look at how StratifiedKFold splits your dataset, you will see that it only accepts a one-dimensional target variable, whereas you are trying to pass a target variable with dimensions [n_samples, n_classes]
Stratefied split basically preserves your class distribution. And if you think about it, it does not make a lot of sense if you have a multi-label classification problem.
If you want to preserve the distribution in terms of the different combinations of classes in your target variable, then the answer here explains two ways in which you can define your own stratefied split function.
UPDATE:
The logic is something like this:
Assuming you have n classes and your target variable is a combination of these n classes. You will have (2^n) - 1 combinations (Not including all 0s). You can now create a new target variable considering each combination as a new label.
For example, if n=3, you will have 7 unique combinations:
 1. [1, 0, 0]
 2. [0, 1, 0]
 3. [0, 0, 1]
 4. [1, 1, 0]
 5. [1, 0, 1]
 6. [0, 1, 1]
 7. [1, 1, 1]

Map all your labels to this new target variable. You can now look at your problem as simple multi-class classification, instead of multi-label classification.
Now you can directly use StartefiedKFold using y_new as your target. Once the splits are done, you can map your labels back.
Code sample:
import numpy as np

np.random.seed(1)
y = np.random.randint(0, 2, (10, 7))
y = y[np.where(y.sum(axis=1) != 0)[0]]

OUTPUT:
array([[1, 1, 0, 0, 1, 1, 1],
       [1, 1, 0, 0, 1, 0, 1],
       [1, 0, 0, 1, 0, 0, 0],
       [1, 0, 0, 1, 0, 0, 0],
       [1, 0, 0, 0, 1, 1, 1],
       [1, 1, 0, 0, 0, 1, 1],
       [1, 1, 1, 1, 0, 1, 1],
       [0, 0, 1, 0, 0, 1, 1],
       [1, 0, 1, 0, 0, 1, 1],
       [0, 1, 1, 1, 1, 0, 0]])

Label encode your class vectors:
from sklearn.preprocessing import LabelEncoder

def get_new_labels(y):
    y_new = LabelEncoder().fit_transform([''.join(str(l)) for l in y])
    return y_new

y_new = get_new_labels(y)

OUTPUT:
array([7, 6, 3, 3, 2, 5, 8, 0, 4, 1])

",cross-validation
custom scoring function in sklearn cross validate,"I would like to use a custom function for cross_validate which uses a specific y_test to compute precision, this is a different y_test than the actual target y_test.
I have tried a few approaches with make_scorer but I don't know how to actually pass my alternative y_test:
scoring = {'prec1': 'precision',
     'custom_prec1': make_scorer(precision_score()}

scores = cross_validate(pipeline, X, y, cv=5,scoring= scoring)

Can any suggest an approach?
","Found this way. Maybe the code is not optimal, sorry for this.
Okay, let we start:
import numpy as np
import pandas as pd

from sklearn.linear_model import LogisticRegression

from sklearn.model_selection import GridSearchCV
from sklearn.metrics import make_scorer

xTrain = np.random.rand(100, 100)
yTrain = np.random.randint(1, 4, (100, 1))

yTrainCV = np.random.randint(1, 4, (100, 1))

model = LogisticRegression()

yTrainCV will be used here as the custom scorer.
def customLoss(xArray, yArray):
    indices = xArray.index.values
    tempArray = [1 if value1 != value2 else 0 for value1, value2 in zip(xArray.values, yTrainCV[[indices]])]
    
    return sum(tempArray)

scorer = {'main': 'accuracy',
          'custom': make_scorer(customLoss, greater_is_better=True)}

Few tricks here:

you need to pass to customLoss 2 values (predictions from the model + real values; we do not use the second parameter though)
there is some game with greater_is_better: True/False will return either positive or negative number
indices we get from CV in GridSearchCV

And...
grid = GridSearchCV(model,
                    scoring=scorer,
                    cv=5,
                    param_grid={'C': [1e0, 1e1, 1e2, 1e3],
                                'class_weight': ['balanced', None]},
                    refit='custom')
    
 grid.fit(xTrain, pd.DataFrame(yTrain))
 print(grid.score(xTrain, pd.DataFrame(yTrain)))


do not forget refit parameter in GridSearchCV
we pass target array as DataFrame here - it will help us to detect indices in the custom loss function

",cross-validation
how to implement contractive autoencoder in pytorch,"I'm trying to create a contractive autoencoder in Pytorch. I found this thread and tried according to that. This is the snippet I wrote based on the mentioned thread:
import datetime
import numpy as np 
import torch
import torchvision
from torchvision import datasets, transforms
from torchvision.utils import save_image, make_grid
import torch.nn as nn 
import torch.nn.functional as F 
import torch.optim as optim
import matplotlib.pyplot as plt 
%matplotlib inline

dataset_train = datasets.MNIST(root='MNIST',
                               train=True,
                               transform = transforms.ToTensor(),
                               download=True)
dataset_test  = datasets.MNIST(root='MNIST', 
                               train=False, 
                               transform = transforms.ToTensor(),
                               download=True)
batch_size = 128
num_workers = 2
dataloader_train = torch.utils.data.DataLoader(dataset_train,
                                               batch_size = batch_size,
                                               shuffle=True,
                                               num_workers = num_workers, 
                                               pin_memory=True)

dataloader_test = torch.utils.data.DataLoader(dataset_test,
                                               batch_size = batch_size,
                                               num_workers = num_workers,
                                               pin_memory=True)

def view_images(imgs, labels, rows = 4, cols =11):
    imgs = imgs.detach().cpu().numpy().transpose(0,2,3,1)
    fig = plt.figure(figsize=(8,4))
    for i in range(imgs.shape[0]):
        ax = fig.add_subplot(rows, cols, i+1, xticks=[], yticks=[])
        ax.imshow(imgs[i].squeeze(), cmap='Greys_r')
        ax.set_title(labels[i].item())


# now let's view some 
imgs, labels = next(iter(dataloader_train))
view_images(imgs, labels,13,10)

class Contractive_AutoEncoder(nn.Module):
    def __init__(self):
        super().__init__()
        self.encoder = nn.Linear(784, 512)
        self.decoder = nn.Linear(512, 784)

    def forward(self, input):
        # flatten the input
        shape = input.shape
        input = input.view(input.size(0), -1)
        output_e = F.relu(self.encoder(input))
        output = F.sigmoid(self.decoder(output_e))
        output = output.view(*shape)
        return output_e, output

def loss_function(output_e, outputs, imgs, device):
    output_e.backward(torch.ones(output_e.size()).to(device), retain_graph=True)
    criterion = nn.MSELoss()
    assert outputs.shape == imgs.shape ,f'outputs.shape : {outputs.shape} != imgs.shape : {imgs.shape}'
    
    imgs.grad.requires_grad = True 
    loss1 = criterion(outputs, imgs)
    print(imgs.grad)
    loss2 = torch.mean(pow(imgs.grad,2))
    loss = loss1 + loss2 
    return loss 

epochs = 50 
interval = 2000
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model = Contractive_AutoEncoder().to(device)
optimizer = optim.Adam(model.parameters(), lr =0.001)

for e in range(epochs):
    for i, (imgs, labels) in enumerate(dataloader_train):
        imgs = imgs.to(device)
        labels = labels.to(device)

        outputs_e, outputs = model(imgs)
        loss = loss_function(outputs_e, outputs, imgs,device)
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        if i%interval: 
            print('')

    print(f'epoch/epoechs: {e}/{epochs} loss : {loss.item():.4f} ')

For the sake of brevity I just used one layer for the encoder and the decoder. It should work regardless of number of layers in either of them obviously!
But the catch here is, aside from the fact that I don't know if this is the correct way of doing this, (calculating gradients with respect to the input), I get an error which makes the former solution wrong/not applicable.
That is:
    imgs.grad.requires_grad = True
produces the error :

AttributeError : 'NoneType' object has no attribute 'requires_grad'

I also tried the second method suggested in that thread which is as follows:
class Contractive_Encoder(nn.Module):
    def __init__(self):
        super().__init__()
        self.encoder = nn.Linear(784, 512)
        
    def forward(self, input):
        # flatten the input
        input = input.view(input.size(0), -1)
        output_e = F.relu(self.encoder(input))
        return output_e

class Contractive_Decoder(nn.Module):
    def __init__(self):
        super().__init__()
        self.decoder = nn.Linear(512, 784)

    def forward(self, input):
        # flatten the input
        output = F.sigmoid(self.decoder(input))
        output = output.view(-1,1,28,28)
        return output


epochs = 50 
interval = 2000
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

model_enc = Contractive_Encoder().to(device)
model_dec = Contractive_Decoder().to(device)

optimizer = optim.Adam([{""params"":model_enc.parameters()},
                        {""params"":model_dec.parameters()}], lr =0.001)

optimizer_cond = optim.Adam(model_enc.parameters(), lr = 0.001)

criterion = nn.MSELoss()

for e in range(epochs):
    for i, (imgs, labels) in enumerate(dataloader_train):
        imgs = imgs.to(device)
        labels = labels.to(device)

        outputs_e = model_enc(imgs)
        outputs = model_dec(outputs_e)
        loss_rec = criterion(outputs, imgs)
        optimizer.zero_grad()
        loss_rec.backward()
        optimizer.step()

        imgs.requires_grad_(True)
        y = model_enc(imgs)
        optimizer_cond.zero_grad()
        y.backward(torch.ones(imgs.view(-1,28*28).size()))

        imgs.grad.requires_grad = True
        loss = torch.mean([pow(imgs.grad,2)])
        optimizer_cond.zero_grad()
        loss.backward()
        optimizer_cond.step()
        
        if i%interval: 
            print('')

    print(f'epoch/epoechs: {e}/{epochs} loss : {loss.item():.4f} ')

but I face the error :
RuntimeError: invalid gradient at index 0 - got [128, 784] but expected shape compatible with [128, 512]

How should I go about this in Pytorch?
","Summary 
The final implementation for contractive loss that I wrote is as follows: 
def loss_function(output_e, outputs, imgs, lamda = 1e-4, device=torch.device('cuda')):

    criterion = nn.MSELoss()
    assert outputs.shape == imgs.shape ,f'outputs.shape : {outputs.shape} != imgs.shape : {imgs.shape}'
    loss1 = criterion(outputs, imgs)

    output_e.backward(torch.ones(outputs_e.size()).to(device), retain_graph=True)    
    # Frobenious norm, the square root of sum of all elements (square value)
    # in a jacobian matrix 
    loss2 = torch.sqrt(torch.sum(torch.pow(imgs.grad,2)))
    imgs.grad.data.zero_()
    loss = loss1 + (lamda*loss2) 
    return loss 

and inside training loop you need to do:
for e in range(epochs):
    for i, (imgs, labels) in enumerate(dataloader_train):
        imgs = imgs.to(device)
        labels = labels.to(device)

        imgs.retain_grad()
        imgs.requires_grad_(True)

        outputs_e, outputs = model(imgs)
        loss = loss_function(outputs_e, outputs, imgs, lam,device)

        imgs.requires_grad_(False)

        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
    print(f'epoch/epochs: {e}/{epochs} loss: {loss.item():.4f}')

Full explanation
As it turns out and rightfully @akshayk07 pointed out in the comments, the implementation found in Pytorch forum was wrong in multiple places. The notable thing, being it wasn't implementing the actual contractive loss that was introduced in Contractive Auto-Encoders:Explicit Invariance During Feature Extraction paper! and also aside from that, the implementation wouldn't work at all for obvious reasons that will be explained in a moment. 
The changes are obvious so I try to explain what's going on here. First of all note that imgs is not a leaf node, so the gradients would not be retained in the image .grad attribute.
In order to retain gradients for non leaf nodes, you should use retain_graph(). grad is only populated for leaf Tensors. Also imgs.retain_grad() should be called before doing forward() as it will       instruct the autograd to store grads into non-leaf nodes.
Update
Thanks to @Michael for pointing out that the correct calculation of Frobenius Norm is actually (from ScienceDirect):  

the square root of the sum of the squares of all the matrix entries

and not 

the the square root of the sum of the absolute values of all the
  matrix entries as explained here

",pytorch
difference between normalizer and minmaxscaler,"I'm trying to understand the effects of applying the Normalizer or applying MinMaxScaler or applying both in my data. I've read the docs in SKlearn, and saw some examples of use. I understand that MinMaxScaler is important (is important to scale the features), but what about Normalizer?
It keeps unclear to me the practical result of using the Normamlizer in my data.
MinMaxScaler is applied column-wise, Normalizer is apllied row-wise. What does it implies? Should I use the Normalizer or just use the MinMaxScale or should use then both?
","As you have said,

MinMaxScaler is applied column-wise, Normalizer is applied row-wise.

Do not confuse Normalizer with MinMaxScaler. The Normalizer class from Sklearn normalizes samples individually to unit norm. It is not column based but a row-based normalization technique. In other words, the range will be determined either by rows or columns.
So, remember that we scale features not records, because we want features to have the same scale, so the model to be trained will not give different weights to different features based on their range. If we scale the records, this will give each record its own scale, which is not what we need.
So, if features are represented by rows, then you should use the Normalizer. But in most cases, features are represented by columns, so you should use one of the scalers from Sklearn depending on the case:

MinMaxScaler transforms features by scaling each feature to a given range. It scales and translates each feature individually such that it is in the given range on the training set, e.g. between zero and one.
The transformation is given by:
X_std = (X - X.min(axis=0)) / (X.max(axis=0) - X.min(axis=0))

X_scaled = X_std * (max - min) + min

where min, max = feature_range.
This transformation is often used as an alternative to zero mean, unit variance scaling.

StandardScaler standardizes features by removing the mean and scaling to unit variance. The standard score of a sample x is calculated as:
z = (x - u) / s. Use this if the data distribution is normal.

RobustScaler is robust to outliers. It removes the median and scales the data according to IQR (Interquartile Range). The IQR is the range between the 25th quantile and the 75th quantile.


",scikit-learn
valueerror unrecognized model in mrpc should have a model_type key in its configjson or contain one of the following strings in its name,"Goal: Amend this Notebook to work with Albert and Distilbert models
Kernel: conda_pytorch_p36. I did Restart & Run All, and refreshed file view in working directory.
Error occurs in Section 1.2, only for these 2 new models.
For filenames etc., I've created a variable used everywhere:
MODEL_NAME = 'albert-base-v2'  # 'distilbert-base-uncased', 'bert-base-uncased'

I replaced imports with:
from transformers import (AutoConfig, AutoModel, AutoTokenizer)
#from transformers import (BertConfig, BertForSequenceClassification, BertTokenizer,)

As suggested in Transformers Documentation - Auto Classes.

Instantiating one of AutoConfig, AutoModel, and AutoTokenizer will directly create a class of the relevant architecture.


Section 1.2:
# load model
model = AutoModel.from_pretrained(configs.output_dir)  # BertForSequenceClassification
model.to(configs.device)


# quantize model
quantized_model = torch.quantization.quantize_dynamic(
    model, {torch.nn.Linear}, dtype=torch.qint8
)

#print(quantized_model)

def print_size_of_model(model):
    torch.save(model.state_dict(), ""temp.p"")
    print('Size (MB):', os.path.getsize(""temp.p"")/(1024*1024))
    os.remove('temp.p')

print_size_of_model(model)
print_size_of_model(quantized_model)

Traceback:
ValueError: Unrecognized model in ./MRPC/. Should have a `model_type` key in its config.json, or contain one of the following strings in its name: imagegpt, qdqbert, vision-encoder-decoder, trocr, fnet, segformer, vision-text-dual-encoder, perceiver, gptj, layoutlmv2, beit, rembert, visual_bert, canine, roformer, clip, bigbird_pegasus, deit, luke, detr, gpt_neo, big_bird, speech_to_text_2, speech_to_text, vit, wav2vec2, m2m_100, convbert, led, blenderbot-small, retribert, ibert, mt5, t5, mobilebert, distilbert, albert, bert-generation, camembert, xlm-roberta, pegasus, marian, mbart, megatron-bert, mpnet, bart, blenderbot, reformer, longformer, roberta, deberta-v2, deberta, flaubert, fsmt, squeezebert, hubert, bert, openai-gpt, gpt2, transfo-xl, xlnet, xlm-prophetnet, prophetnet, xlm, ctrl, electra, speech-encoder-decoder, encoder-decoder, funnel, lxmert, dpr, layoutlm, rag, tapas, splinter, sew-d, sew, unispeech-sat, unispeech, wavlm

Please let me know if there's anything else I can add to post.
","Explanation:
When instantiating AutoModel, you must specify a model_type parameter in ./MRPC/config.json file (downloaded during Notebook runtime).
List of model_types can be found here.

Solution:
Code that appends model_type to config.json, in the same format:
import json

json_filename = './MRPC/config.json'

with open(json_filename) as json_file:
    json_decoded = json.load(json_file)

json_decoded['model_type'] = # !!

with open(json_filename, 'w') as json_file:
    json.dump(json_decoded, json_file, indent=2, separators=(',', ': '))

config.json:
{
  ""attention_probs_dropout_prob"": 0.1,
  ""finetuning_task"": ""mrpc"",
  ""hidden_act"": ""gelu"",
  ""hidden_dropout_prob"": 0.1,
  ""hidden_size"": 768,
  ""initializer_range"": 0.02,
  ""intermediate_size"": 3072,
  ""layer_norm_eps"": 1e-12,
  ""max_position_embeddings"": 512,
  ""num_attention_heads"": 12,
  ""num_hidden_layers"": 12,
  ""num_labels"": 2,
  ""output_attentions"": false,
  ""output_hidden_states"": false,
  ""pruned_heads"": {},
  ""torchscript"": false,
  ""type_vocab_size"": 2,
  ""vocab_size"": 30522,
  ""model_type"": ""albert""
}


",tensorflow
optimizing a gam for smoothness,"I am currently trying to generate a general additive model in R using a response variable and three predictor variables.
One of the predictors is linear, and the dataset consists of 298 observations.
I have run the following code to generate a basic GAM:
GAM <- gam(response~ linearpredictor+ s(predictor2) + s(predictor3), data = data[2:5])


This produces a model with 18 degrees of freedom and seems to substantially overfit the data.
I'm wondering how I might generate a GAM that maximizes smoothness and predictive error.
I realize that each of these features is going to come at the expense of the other, but is there good a way to find the optimal model that doesn't overfit?
Additionally, I need to perform leave one out cross validation LOOCV, and I am not sure how to make sure that gam() does this in the MGCV package.
Any help on either of these problems would be greatly appreciated. Thank you.
I've run this to generate a GAM, but it overfits the data.
I have also generated 1,000,000 GAMs with varying combinations of smoothing parameters and ranged the maximum degrees of freedom allowed from 10 (as shown in the code below) to 19.
The variable combinations2 is a list of all 1,000,000 combinations of smoothers I selected.
This code is designed to try and balance degrees of freedom and AIC score. It does function, but I'm not sure that I'm actually going to be able to find the optimal model from this. I also cannot tell how to make sure that it uses LOOCV.
BestGAM <- gam(response~ linearpredictor+ predictor2+ predictor3, data = data[2:5])

for(i in 1:100000){
    PotentialGAM <- gam(response~ linearpredictor+ s(predictor2) + s(predictor3), data = data[2:5], sp=c(combinations2[i,]$Var1,combinations2[i,]$Var2))
    
    if (AIC(PotentialGAM,BestGAM)$df[1] <= 10 & AIC(PotentialGAM,BestGAM)$AIC[1] < AIC(PotentialGAM,BestGAM)$AIC[2]){
        
        BestGAM <<- PotentialGAM
        listNumber <- i
    }
}

","You are fitting your GAM using generalised cross validation (GCV) smoothness selection. GCV is a way to get around the invariance problem of ordinary cross validation (OCV; what you also call LOOCV) when estimating GAMs. Note that GCV is the same as OCV on a rotated version of the fitting problem (rotating y - Xβ by Q, any orthogonal matrix), and while when fitting with GCV {mgcv} doesn't actually need to do the rotation and the expected GCV score isn't affected by the rotation, GCV is just OCV (wood 2017, p. 260)
It has been shown that GCV can undersmooth (resulting in more wiggly models) as the objective function (GCV profile) can become flat around the optimum. Instead it is preferred to estimate GAMs (with penalized smooths) using REML or ML smoothness selection; add method = ""REML"" (or ""ML"") to your gam() call.
If the REML or ML fit is as wiggly as the GCV one with your data, then I'd be likely to presume gam() is not overfitting, but that there is something about your response data that hasn't been explained here (are the data ordered in time, for example?)
As to your question

how I might generate a GAM that maximizes smoothness and [minimize?] predictive error,

you are already doing that using GCV smoothness selection and for a particular definition of ""smoothness"" (in this case it is squared second derivatives of the estimated smooths, integrated over the range of the covariates, and summed over smooths).
If you want GCV but smoother models, you can increase the gamma argument above 1; gamma 1.4 is often used for example, which means that each EDF costs 40% more in the GCV criterion.
FWIW, you can get the LOOCV (OCV) score for your model without actually fitting 288 GAMs through the use of the influence matrix A. Here's a reproducible example using my {gratia} package:
library(""gratia"")
library(""mgcv"")

df <- data_sim(""eg1"", seed = 1)
m <- gam(y ~ s(x0) + s(x1) + s(x2) + s(x3), data = df, method = ""REML"")
A <- influence(m)
r <- residuals(m, type = ""response"")

ocv_score <- mean(r^2 / (1 - A))

",cross-validation
how to add type hints to scikitlearn functions,"I have the following simple function:
def f1(y_true, y_pred):
    return {""f1"": 100 * sklearn.metrics.f1_score(y_true, y_pred)}

According to the scikit-learn documentation, the arguments to f1_score can have the following types:

y_true: 1d array-like, or label indicator array / sparse matrix
y_pred: 1d array-like, or label indicator array / sparse matrix

and the output is of type:

float or array of float, shape = [n_unique_labels]

How do I add type hints to this function so that mypy doesn't complain?
I tried variations of the following:
Array1D = NewType('Array1D', Union[np.ndarray, List[np.float64]])

def f1(y_true: Union[List[float], Array1D], y_pred: Union[List[float], Array1D]) -> Dict[str, Union[List[float], Array1D]]:
    return {""f1"": 100 * sklearn.metrics.f1_score(y_true, y_pred)}

but that gave errors.
","This is the approach I use to avoid similar mypy issues. It takes advantage of numpy typing introduced in 1.20. The ArrayLike type covers List[float], so no need to worry about covering it explicitly.
Running mypy v0.971 with numpy v1.23.1 on this shows no issues.
from typing import List, Dict
import numpy as np
import numpy.typing as npt
import sklearn.metrics


def f1(y_true: npt.ArrayLike, y_pred: npt.ArrayLike) -> Dict[str, npt.ArrayLike]:
    return {""f1"": 100 * sklearn.metrics.f1_score(y_true, y_pred)}

y_true_list: List[float] = [1, 0, 1, 0]
y_pred_list: List[float] = [1, 0, 1, 1]
y_true_np: npt.ArrayLike = np.array(y_true_list)
y_pred_np: npt.ArrayLike = np.array(y_pred_list)

assert f1(y_true_list, y_pred_list) == f1(y_true_np, y_pred_np)

",scikit-learn
why do i have a label problem when using crossvalidator,"I'm new to spark :) I try to use CrossValidator. My model is as follows :
training
#training data - several repartition have been tested, 50/50 seems the best
(trainData, testData) = modelData.randomSplit([0.5, 0.5])

#counting data used
print(""Training dataset count : "" +str(trainData.count()))
print(""Test dataset count : "" +str(testData.count()))
trainData.cache()
testData.cache()

Model
from pyspark.ml.classification import LogisticRegression
lr = LogisticRegression(featuresCol = 'features', labelCol = 'v4_Indexer', maxIter = 5)
lrModel = lr.fit(trainData)
predictions = lrModel.transform(testData)
predictions.select('v4_Indexer','features','rawPrediction', 'prediction', 'probability').toPandas().head(2500)

I try this code for crossvalidation :
from pyspark.ml.tuning import ParamGridBuilder, CrossValidator
from pyspark.ml import Pipeline
pipeline = Pipeline(stages=[lr])
paramGrid = (ParamGridBuilder() .addGrid(lr.regParam, [0,0.5,1]).addGrid(lr.elasticNetParam, [0,0.5,1]).addGrid(lr.maxIter,[1,10]).build())
cv = CrossValidator(estimator=lr, estimatorParamMaps=paramGrid, evaluator=evaluator, numFolds=5)
cvModel = cv.fit(trainData)
trainingSummary = cvModel.bestModel

I have a warning /databricks/spark/python/pyspark/ml/util.py:92: UserWarning: CrossValidator_7ba8c8c903af fit call failed but some spark jobs may still running for unfinished trials. To address this issue, you should enable pyspark pinned thread mode.
warnings.warn(""{} fit call failed but some spark jobs ""
And an error : IllegalArgumentException: label does not exist. Available: v4_Indexer, features, CrossValidator_7ba8c8c903af_rand
this model worked for a while. I do not understand why it doesn't now.
Thx in advance for any help you could bring me =)
","I've solved the issue by changing entirely my code. This is what it looks like now (prerequisite %pip install mlflow):
from pyspark.ml.classification import DecisionTreeClassifier, DecisionTreeClassificationModel
from pyspark.ml.feature import StringIndexer
from pyspark.ml import Pipeline

# StringIndexer: Convert the input column ""label"" (digits) to categorical values
indexer = StringIndexer(inputCol=""v4_Indexer"", outputCol=""indexedLabel"")

# Create an evaluator.  In this case, use ""weightedPrecision"".
from pyspark.ml.evaluation import MulticlassClassificationEvaluator
evaluator = MulticlassClassificationEvaluator(labelCol=""v4_Indexer"", metricName=""weightedPrecision"")

from pyspark.ml.tuning import CrossValidator, ParamGridBuilder

# DecisionTreeClassifier: Learn to predict column ""indexedLabel"" using the ""features"" column

dtc = DecisionTreeClassifier(labelCol=""indexedLabel"")

# Chain indexer + dtc together into a single ML Pipeline
pipeline = Pipeline(stages=[indexer, dtc])
# Define the parameter grid to examine.
grid = ParamGridBuilder().addGrid(dtc.maxDepth, [2, 3, 4, 5, 6, 7, 8]).addGrid(dtc.maxBins, [2, 4, 8]).build()

# Create a cross validator, using the pipeline, evaluator, and parameter grid you created in previous steps.
cv = CrossValidator(estimator=pipeline, evaluator=evaluator, estimatorParamMaps=grid, numFolds=3)

# Explicitly create a new run.
# This allows this cell to be run multiple times.
# If you omit mlflow.start_run(), then this cell could run once, but a second run would hit conflicts when attempting to overwrite the first run.
import mlflow
import mlflow.spark

with mlflow.start_run():
    # Run the cross validation on the training dataset. The cv.fit() call returns the best model it found.
    cvModel = cv.fit(train)
    
    # Evaluate the best model's performance on the test dataset and log the result.
    test_metric = evaluator.evaluate(cvModel.transform(test))
    mlflow.log_metric('test_' + evaluator.getMetricName(), test_metric) 
  
    # Log the best model.
    mlflow.spark.log_model(spark_model=cvModel.bestModel, artifact_path='best-model')

",cross-validation
trying to find optimal price point in a data set,"I have a data set that looks like this.
Year    Quarter Quantity    Price
2000    1   23  142
2000    2   23  144
2000    3   23  147
2000    4   23  151
2001    1   22  160
2001    2   22  183
2001    3   22  186
2001    4   22  186
2002    1   21  212
2002    2   19  232
2002    3   19  223
2002    4   19  224
2003    1   19  231
2003    2   19  228
2003    3   19  238
2003    4   19  238
2004    1   19  234
2004    2   19  231
2004    3   20  239
2004    4   19  235
2005    1   19  233
2005    2   19  243
2005    3   20  244
2005    4   19  233
2006    1   19  234
2006    2   19  241

I am trying to figure out a pricing strategy to maximize revenue (optimal price) using Python.  I found a few example online, but didn't know how to adapt them to my specific scenario.  
This one looks good, but I'm not sure how to modify it to fit my data set.
https://www.datascience.com/resources/notebooks/python-dynamic-pricing
This one looks goo too, but again, I'm not sure how to make it work for me.
https://cscherrer.github.io/post/max-profit/
If someone here knows how to modify the sample code at one of those sites to fit the data I'm working with, I'd really like to see it.  Or, if you have a link to another site that answers my question, please do share it.  Thanks!
","Fundamentally I don't think there's a enough data here to be able to implement a pricing strategy based on pure statistics. The differences in quantity are barely outside of the standard deviation (std 1.6, mean 20.2). However theoretically what you want to do is:

convert the data to a more easily importable formate such as csv

Year,Quarter,Quantity,Price
2000,1,23,142
2000,2,23,144
2000,3,23,147
2000,4,23,151
2001,1,22,160
2001,2,22,183
2001,3,22,186
2001,4,22,186
2002,1,21,212
2002,2,19,232
2002,3,19,223
2002,4,19,224
2003,1,19,231
2003,2,19,228
2003,3,19,238
2003,4,19,238
2004,1,19,234
2004,2,19,231
2004,3,20,239
2004,4,19,235
2005,1,19,233
2005,2,19,243
2005,3,20,244
2005,4,19,233
2006,1,19,234
2006,2,19,241


load in the data prices = pd.read_csv(""price_data.csv"")
make a graph to visually show change in price with respect to quantity sns.scatterplot(x=prices[""Price""], y=prices[""Quantity""])

add columns for change in quantity and change in price

prices[""% Change in Quantity""] = prices[""Quantity""].pct_change()
prices[""% Change in Price""] = prices[""Price""].pct_change()


calculate the price elasticity prices[""Price Elasticity""] = prices[""% Change in Quantity""] / prices[""% Change in Price""]
Graph price elasticity vs the price


from this data you could then fit a model (depending on the complexity some order of polynomial makes sense), and use this to figure out at what point the price elasticity becomes too high and you wouldn't be making enough sales. But that's highly subjective and based more on business needs than anything else.
",scikit-learn
error in newdata objectmodellistvariables  subscript out of boundsquot,"When I am running this code, I am getting this error
""Error in newdata[, object$model.list$variables] : subscript out of bounds""
I am not getting how to solve it.
install.packages(""boot"")
install.packages(""plyr"")

library(boot)
library(plyr)

set.seed(50)
k=10
RMSE.NN=NULL

List=list( )

for(j in 10:65){
  for(i in 1:k){
    index=sample(1:nrow(data2),j)
    
    trainNN=d2[index,]
    testNN=d1[-index,]
    dataset=data2[-index,]
    
    NN=neuralnet(quality~.,d2,hidden=10,linear.output = T)
    predict_testNN=compute(NN,d1[,c(1:5)])
    predict_testNN=(predict_testNN$net.result*(max(data2$quality)-min(data2$quality)))+min(data2$quality)
    
    RMSE.NN[i]<-(sum((data2$quality-predict_testNN)^2)/nrow(dataset))^0.5
  }
  List[[j]]=RMSE.NN
}

(matrixRMSE=do.call(cbind, List))
matrixRMSE

","The wine data you shared has variable names with spaces in them.
R can get confused when variables have spaces in them; such names require quoting with backticks etc to refer to
# i.e. my_num would be simple but `my num` would be needed to maintain the space.

Easiest thing for you to do is use library(janitor) and clean_names on the dataset you want to work with.
",cross-validation
