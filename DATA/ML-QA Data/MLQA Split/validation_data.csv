instruction,input,output,Categories
finetuning a huggingface llm on two books using lora,"I have been trying to get into finetuning LLMs on my own hardware (Ryzen 3960x and RTX 3090 64 GB Ram) as efficiently as possible and running into some problems while doing so. As a test, I wanted to train GPT-2 on DavidCopperfield by Charles Dickens to test the result one could expect, so I tokenized the books using pdfReader and autoTokenize from my model. This seemed to work. Then, I wanted to finetune the model on this tokenized dataset, but I ran into some issues with CUDA installation. Every time I run my code, I get this error:
    bin C:\Users\salom\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\LocalCache\local-packages\Python311\site-packages\bitsandbytes\libbitsandbytes_cpu.so
    False
    C:\Users\salom\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\LocalCache\local-packages\Python311\site-packages\bitsandbytes\cextension.py:34: UserWarning: The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.
      warn(""The installed version of bitsandbytes was compiled without GPU support. ""
    'NoneType' object has no attribute 'cadam32bit_grad_fp32'
    CUDA SETUP: Required library version not found: libbitsandbytes_cpu.so. Maybe you need to compile it from source?
    CUDA SETUP: Defaulting to libbitsandbytes_cpu.so...
    
    ================================================ERROR=====================================
    CUDA SETUP: CUDA detection failed! Possible reasons:
    1. CUDA driver not installed
    2. CUDA not installed
    3. You have multiple conflicting CUDA libraries
    4. Required library not pre-compiled for this bitsandbytes release!
    CUDA SETUP: If you compiled from source, try again with `make CUDA_VERSION=DETECTED_CUDA_VERSION` for example, `make CUDA_VERSION=113`.
    CUDA SETUP: The CUDA version for the compile might depend on your conda install. Inspect CUDA version via `conda list | grep cuda`.
    ================================================================================
    
    CUDA SETUP: Problem: The main issue seems to be that the main CUDA library was not detected.
    CUDA SETUP: Solution 1): Your paths are probably not up-to-date. You can update them via: sudo ldconfig.
    CUDA SETUP: Solution 2): If you do not have sudo rights, you can do the following:
    CUDA SETUP: Solution 2a): Find the cuda library via: find / -name libcuda.so 2>/dev/null
    CUDA SETUP: Solution 2b): Once the library is found add it to the LD_LIBRARY_PATH: export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:FOUND_PATH_FROM_2a
    CUDA SETUP: Solution 2c): For a permanent solution add the export from 2b into your .bashrc file, located at ~/.bashrc
    CUDA SETUP: Setup Failed!


This is my code:

import PyPDF2

# Function to extract text from a PDF file
def extract_text_from_pdf(file_path):
    with open(file_path, 'rb') as file:
        pdf_reader = PyPDF2.PdfReader(file)
        text = """"
        for page in pdf_reader.pages:
            text += page.extract_text()
        return text

# Load the PDF file and extract text
pdf_file_path = ""DavidCopperfield.pdf""
book_text = extract_text_from_pdf(pdf_file_path)

import re

# Function to filter and clean the text
def filter_text(text):
    # Remove chapter titles and page numbers
    text = re.sub(r'CHAPTER \d+', '', text)
    text = re.sub(r'\d+', '', text)

    # Remove unwanted characters and extra whitespaces
    text = re.sub(r'[^\w\s\'.-]', '', text)
    text = re.sub(r'\s+', ' ', text)

    # Remove lines with all uppercase letters (potential noise)
    text = '\n'.join(line for line in text.split('\n') if not line.isupper())

    return text

# Apply text filtering to the book text
filtered_text = filter_text(book_text)

# Partition the filtered text into training texts with a maximum size
max_text_size = 150
train_texts = []
current_text = """"
for paragraph in filtered_text.split(""\n\n""):
    if len(current_text) + len(paragraph) < max_text_size:
        current_text += paragraph + ""\n\n""
    else:
        train_texts.append(current_text)
        current_text = paragraph + ""\n\n""
if current_text:
    train_texts.append(current_text)


from transformers import GPT2LMHeadModel, GPT2Tokenizer, GPT2Config
from transformers import AdamW
from torch.utils.data import Dataset, DataLoader
import torch
# Define your dataset class
class TextDataset(Dataset):
    def __init__(self, texts, tokenizer, max_length):
        self.texts = [text for text in texts if len(text) >= max_length]  # Filter out texts shorter than max_length
        self.tokenizer = tokenizer
        self.max_length = max_length

    def __len__(self):
        return len(self.texts)

    def __getitem__(self, idx):
        text = self.texts[idx]
        encoded_input = self.tokenizer.encode_plus(text, max_length=self.max_length, padding='max_length', truncation=True, return_tensors='pt')
        input_ids = encoded_input['input_ids'].squeeze()
        attention_mask = encoded_input['attention_mask'].squeeze()
        return input_ids, attention_mask

# Load pre-trained LM and tokenizer
lm_model = GPT2LMHeadModel.from_pretrained('gpt2')
tokenizer = GPT2Tokenizer.from_pretrained('gpt2')
tokenizer.add_special_tokens({'pad_token': '[PAD]'})  # Add padding token

# Prepare your training data
train_dataset = TextDataset(train_texts, tokenizer, max_length=128)
train_dataloader = DataLoader(train_dataset, batch_size=8, shuffle=True)

# Configure LM training
lm_model.train()
# Replace the optimizer initialization line
optimizer = torch.optim.AdamW(lm_model.parameters(), lr=1e-5)
num_epochs = 10

# Training loop
for epoch in range(num_epochs):
    for batch in train_dataloader:
        input_ids, attention_mask = batch
        outputs = lm_model(input_ids=input_ids, attention_mask=attention_mask, labels=input_ids)
        loss = outputs.loss

        # Backpropagation and optimization
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        # Print loss or other metrics for monitoring

# Save the fine-tuned LM
lm_model.save_pretrained('fine_tuned_lm')
tokenizer.save_pretrained('fine_tuned_lm')


","If anyone else is running into the issue, the fix for me was to remove all versions I had installed and use pytorch installation which aso installs CUDA. Then, I installed bitsandbytes again and ran it. The results from he finetune were mediocre though, as I had expected.
",fine-tune
how do i apply one hot encoding on a pandas dataframe with both categorical and numerical features,"Some features are numerical such as ""graduation rate from school"", while other features are categorical like the name of the school. I used a label encoder on the features that are categorical to transform them into integers.
I now have a dataframe with both floats and integers, representing numerical features and categorical features(transformed with label encoder) respectively.
I am unsure how to proceed with a learner, do I need to use one hot encoding? And if so, how can I do so? I cannot simply pass the dataframe to the sklearn OneHotEncoder since there are floats, according to my current understanding. Do I just apply the label encoder to all features to solve the issue?
Sample data from my dataframe. OPEID and opeid6 were transformed using a label encoder
","Just use the OneHotEncoder categorical_features argument to select with features are categorical:

categorical_features: “all” or array of indices or mask :
Specify what features are treated as categorical.

‘all’ (default): All features are treated as categorical.
array of indices: Array of categorical feature indices.
mask: Array of length n_features and with dtype=bool.
Non-categorical features are always stacked to the right of the matrix.


",scikit-learn
pandas create crossvalidation based on specific columns,"I have a dataframe of few hundreds rows , that can be grouped to ids as follows:
df = Val1 Val2 Val3 Id
      2     2   8    b
      1     2   3    a
      5     7   8    z
      5     1   4    a
      0     9   0    c
      3     1   3    b
      2     7   5    z
      7     2   8    c
      6     5   5    d
...
      5     1   8    a
      4     9   0    z
      1     8   2    z

I want to use GridSearchCV , but with a custom CV that will assure that all the rows from the same ID will always be on the same set. 
So either all the rows if a are in the test set , or all of them are in the train set - and so for all the different IDs.
I want to have 5 folds - so 80% of the ids will go to the train and 20% to the test.
I understand that it can't guarentee that all folds will have the exact same amount of rows - since one ID might have more rows than the other.
What is the best way to do so?
","As stated, you can provide cv with an iterator. You can use GroupShuffleSplit(). For example, once you use it to split your dataset, you can put the result within GridSearchCV() for the cv parameter.
",cross-validation
sklearn pipeline  how to apply different transformations on different columns,"I have a dataset that has a mixture of text and numbers i.e. certain columns have text only and rest have integers (or floating point numbers).
I was wondering if it was possible to build a pipeline where I can for example call LabelEncoder() on the text features and MinMaxScaler() on the numbers columns. The examples I have seen on the web mostly point towards using LabelEncoder() on the entire dataset and not on select columns. Is this possible? If so any pointers would be greatly appreciated.
","The way I usually do it is with a FeatureUnion, using a FunctionTransformer to pull out the relevant columns. 
Important notes:

You have to define your functions with def since annoyingly you can't use lambda or partial in FunctionTransformer if you want to pickle your model
You need to initialize FunctionTransformer with validate=False

Something like this:
from sklearn.pipeline import make_union, make_pipeline
from sklearn.preprocessing import FunctionTransformer

def get_text_cols(df):
    return df[['name', 'fruit']]

def get_num_cols(df):
    return df[['height','age']]

vec = make_union(*[
    make_pipeline(FunctionTransformer(get_text_cols, validate=False), LabelEncoder()))),
    make_pipeline(FunctionTransformer(get_num_cols, validate=False), MinMaxScaler())))
])

",scikit-learn
huggingface transformers for text generation with ctrl with google colab39s free gpu,"I wanted to test TextGeneration with CTRL using PyTorch-Transformers, before using it for fine-tuning. But it doesn't prompt anything like it does with GPT-2 and other similar language generation models. I'm very new for this and am stuck and can't figure out what's going on.
This is the procedure I followed in my Colab notebook,
!pip install transformers

!git clone https://github.com/huggingface/pytorch-transformers.git

!python pytorch-transformers/examples/run_generation.py \
    --model_type=ctrl \
    --length=100 \
    --model_name_or_path=ctrl \
    --temperature=0.2 \
    --repetition_penalty=1.2 \

And this is what I get after running the script
02/10/2020 01:02:31 - INFO - transformers.tokenization_utils -   loading file https://raw.githubusercontent.com/salesforce/ctrl/master/ctrl-vocab.json from cache at /root/.cache/torch/transformers/a858ad854d3847b02da3aac63555142de6a05f2a26d928bb49e881970514e186.285c96a541cf6719677cfb634929022b56b76a0c9a540186ba3d8bbdf02bca42
02/10/2020 01:02:31 - INFO - transformers.tokenization_utils -   loading file https://raw.githubusercontent.com/salesforce/ctrl/master/ctrl-merges.txt from cache at /root/.cache/torch/transformers/aa2c569e6648690484ade28535a8157aa415f15202e84a62e82cc36ea0c20fa9.26153bf569b71aaf15ae54be4c1b9254dbeff58ca6fc3e29468c4eed078ac142
02/10/2020 01:02:31 - INFO - transformers.configuration_utils -   loading configuration file https://storage.googleapis.com/sf-ctrl/pytorch/ctrl-config.json from cache at /root/.cache/torch/transformers/d6492ca334c2a4e079f43df30956acf935134081b2b3844dc97457be69b623d0.1ebc47eb44e70492e0c20494a084f108332d20fea7fe5ad408ef5e7a8f2baef4
02/10/2020 01:02:31 - INFO - transformers.configuration_utils -   Model config CTRLConfig {
  ""architectures"": null,
  ""attn_pdrop"": 0.1,
  ""bos_token_id"": 0,
  ""dff"": 8192,
  ""do_sample"": false,
  ""embd_pdrop"": 0.1,
  ""eos_token_ids"": 0,
  ""finetuning_task"": null,
  ""from_tf"": false,
  ""id2label"": {
    ""0"": ""LABEL_0""
  },
  ""initializer_range"": 0.02,
  ""is_decoder"": false,
  ""label2id"": {
    ""LABEL_0"": 0
  },
  ""layer_norm_epsilon"": 1e-06,
  ""length_penalty"": 1.0,
  ""max_length"": 20,
  ""model_type"": ""ctrl"",
  ""n_ctx"": 512,
  ""n_embd"": 1280,
  ""n_head"": 16,
  ""n_layer"": 48,
  ""n_positions"": 50000,
  ""num_beams"": 1,
  ""num_labels"": 1,
  ""num_return_sequences"": 1,
  ""output_attentions"": false,
  ""output_hidden_states"": false,
  ""output_past"": true,
  ""pad_token_id"": 0,
  ""pruned_heads"": {},
  ""repetition_penalty"": 1.0,
  ""resid_pdrop"": 0.1,
  ""summary_activation"": null,
  ""summary_first_dropout"": 0.1,
  ""summary_proj_to_labels"": true,
  ""summary_type"": ""cls_index"",
  ""summary_use_proj"": true,
  ""temperature"": 1.0,
  ""top_k"": 50,
  ""top_p"": 1.0,
  ""torchscript"": false,
  ""use_bfloat16"": false,
  ""vocab_size"": 246534
}

02/10/2020 01:02:31 - INFO - transformers.modeling_utils -   loading weights file https://storage.googleapis.com/sf-ctrl/pytorch/seqlen256_v1.bin from cache at /root/.cache/torch/transformers/c146cc96724f27295a0c3ada1fbb3632074adf87e9aef8269e44c9208787f8c8.b986347cbab65fa276683efbb9c2f7ee22552277bcf6e1f1166557ed0852fdf0
tcmalloc: large alloc 1262256128 bytes == 0x38b92000 @  0x7fe1900bdb6b 0x7fe1900dd379 0x7fe139843b4a 0x7fe1398455fa 0x7fe13bb7578a 0x7fe13bdbe30b 0x7fe13be05b37 0x7fe184c8cad5 0x7fe184c8d17b 0x7fe184c91160 0x7fe184ade496 0x551b15 0x5aa6ec 0x50abb3 0x50c5b9 0x508245 0x5096b7 0x595311 0x54a6ff 0x551b81 0x5aa6ec 0x50abb3 0x50c5b9 0x508245 0x509642 0x595311 0x54a6ff 0x551b81 0x5aa6ec 0x50abb3 0x50c5b9
tcmalloc: large alloc 1262256128 bytes == 0x19fdda000 @  0x7fe1900bdb6b 0x7fe1900dd379 0x7fe139843b4a 0x7fe1398455fa 0x7fe13bb7578a 0x7fe13bdbe30b 0x7fe13be05b37 0x7fe184c8cad5 0x7fe184c8d17b 0x7fe184c91160 0x7fe184ade496 0x551b15 0x5aa6ec 0x50abb3 0x50c5b9 0x508245 0x509642 0x595311 0x54a6ff 0x551b81 0x5aa6ec 0x50abb3 0x50d390 0x508245 0x509642 0x595311 0x54a6ff 0x551b81 0x5a067e 0x50d966 0x508245
^C

and then terminates. Could this be because of a GPU problem?
","The solution was to increase the RAM. Since I was using the Google Colab's free GPU, I was going through this: GitHub issue
and found this useful: Solution
The following piece of code will crash the session in Colab and select 'Get more RAM', which will increase the RAM up to 25.51GB
d=[]
while(1):
  d.append('1')

",pytorch
why there is 39unpickling error39 when using polars to read data for pytorch,"I have changed my data tool from xarray to polars in recent, and use pl.DataFrame.to_torch() to generate tensor for training my Pytorch model. Data source's format is parquet file.
For avoiding fork child processes, I use torch.multiprocessing.spawn to start my training process, however the process crashed with this:
/home/username/.conda/envs/torchhydro1/bin/python3.11 -X pycache_prefix=/home/username/.cache/JetBrains/IntelliJIdea2024.3/cpython-cache /home/username/.local/share/JetBrains/IntelliJIdea2024.3/python-ce/helpers/pydev/pydevd.py --multiprocess --qt-support=auto --port 29781 --file /home/username/torchhydro/experiments/train_with_era5land_gnn_ddp.py 
Console output is saving to: /home/username/torchhydro/experiments/results/train_gnn_ddp.txt
[20:38:51] DEBUG    No module named 'forge'                     signatures.py:43
           DEBUG    No module named 'forge'                     signatures.py:43
[20:38:52] DEBUG    Using selector: EpollSelector          selector_events.py:54
           ……
           DEBUG    Using fontManager instance from         font_manager.py:1580
                    /home/username/.cache/matplotlib/fontl                     
                    ist-v390.json                                               
update config file
!!!!!!NOTE!!!!!!!!
-------Please make sure the PRECIPITATION variable is in the 1st location in var_t setting!!---------
If you have POTENTIAL_EVAPOTRANSPIRATION, please set it the 2nd!!!-
!!!!!!NOTE!!!!!!!!
-------Please make sure the STREAMFLOW variable is in the 1st location in var_out setting!!---------
[20:39:04] DEBUG    No module named 'forge'                     signatures.py:43
           DEBUG    No module named 'forge'                     signatures.py:43
[20:39:06] DEBUG    Using selector: EpollSelector          selector_events.py:54
           ……
           DEBUG    Using fontManager instance from         font_manager.py:1580
                    /home/username/.cache/matplotlib/fontl                     
                    ist-v390.json                                               
……
Torch is using cuda:0
[2024-12-12 20:48:08,931] torch.distributed.distributed_c10d: [INFO] Using backend config: {'cuda': 'nccl'}
[W CUDAAllocatorConfig.h:30] Warning: expandable_segments not supported on this platform (function operator())
using 8 workers
Pin memory set to True
  0%|          | 0/22986 [00:00<?, ?it/s]
[20:48:40] DEBUG    No module named 'forge'                     signatures.py:43
           DEBUG    No module named 'forge'                     signatures.py:43
[20:48:41] DEBUG    Using selector: EpollSelector          selector_events.py:54
           ……
           DEBUG    Using fontManager instance from         font_manager.py:1580
                    /home/username/.cache/matplotlib/fontl                     
                    ist-v390.json                                               
[20:49:28] DEBUG    No module named 'forge'                     signatures.py:43
           DEBUG    No module named 'forge'                     signatures.py:43
[20:49:29] DEBUG    Using selector: EpollSelector          selector_events.py:54
           ……
           DEBUG    Using fontManager instance from         font_manager.py:1580
                    /home/username/.cache/matplotlib/fontl                     
                    ist-v390.json                                               
[20:50:19] DEBUG    No module named 'forge'                     signatures.py:43
           DEBUG    No module named 'forge'                     signatures.py:43
[20:50:20] DEBUG    Using selector: EpollSelector          selector_events.py:54
           ……
           DEBUG    Using fontManager instance from         font_manager.py:1580
                    /home/username/.cache/matplotlib/fontl                     
                    ist-v390.json                                               
[20:51:12] DEBUG    No module named 'forge'                     signatures.py:43
           DEBUG    No module named 'forge'                     signatures.py:43
[20:51:13] DEBUG    Using selector: EpollSelector          selector_events.py:54
           ……
           DEBUG    Using fontManager instance from         font_manager.py:1580
                    /home/username/.cache/matplotlib/fontl                     
                    ist-v390.json                                               
[20:52:07] DEBUG    No module named 'forge'                     signatures.py:43
           DEBUG    No module named 'forge'                     signatures.py:43
[20:52:09] DEBUG    Using selector: EpollSelector          selector_events.py:54
           ……
[20:52:13] DEBUG    CACHEDIR=/home/username/.cache/matplotlib   __init__.py:341
           DEBUG    Using fontManager instance from         font_manager.py:1580
                    /home/username/.cache/matplotlib/fontl                     
                    ist-v390.json                                               
[20:53:11] DEBUG    No module named 'forge'                     signatures.py:43
           DEBUG    No module named 'forge'                     signatures.py:43
[20:53:12] DEBUG    Using selector: EpollSelector          selector_events.py:54
           ……
           DEBUG    Using fontManager instance from         font_manager.py:1580
                    /home/username/.cache/matplotlib/fontl                     
                    ist-v390.json                                               
[20:55:12] DEBUG    No module named 'forge'                     signatures.py:43
           DEBUG    No module named 'forge'                     signatures.py:43
[20:55:14] DEBUG    Using selector: EpollSelector          selector_events.py:54
           ……
[20:55:19] DEBUG    CACHEDIR=/home/username/.cache/matplotlib   __init__.py:341
           DEBUG    Using fontManager instance from         font_manager.py:1580
                    /home/username/.cache/matplotlib/fontl                     
                    ist-v390.json                                               
Traceback (most recent call last):
  File ""/home/username/.local/share/JetBrains/IntelliJIdea2024.3/python-ce/helpers/pydev/pydevd.py"", line 1570, in _exec
    pydev_imports.execfile(file, globals, locals)  # execute the script
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/username/.local/share/JetBrains/IntelliJIdea2024.3/python-ce/helpers/pydev/_pydev_imps/_pydev_execfile.py"", line 18, in execfile
    exec(compile(contents+""\n"", file, 'exec'), glob, loc)
  File ""/home/username/torchhydro/experiments/train_with_era5land_gnn_ddp.py"", line 171, in <module>
    test_run_model()
  File ""/home/username/torchhydro/experiments/train_with_era5land_gnn_ddp.py"", line 56, in test_run_model
    mp.spawn(gnn_train_worker, args=(world_size, config_data, None), nprocs=world_size, join=True)
  File ""/home/username/.conda/envs/torchhydro1/lib/python3.11/site-packages/torch/multiprocessing/spawn.py"", line 241, in spawn
    return start_processes(fn, args, nprocs, join, daemon, start_method=""spawn"")
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/username/.conda/envs/torchhydro1/lib/python3.11/site-packages/torch/multiprocessing/spawn.py"", line 197, in start_processes
    while not context.join():
              ^^^^^^^^^^^^^^
  File ""/home/username/.conda/envs/torchhydro1/lib/python3.11/site-packages/torch/multiprocessing/spawn.py"", line 140, in join
    raise ProcessExitedException(
torch.multiprocessing.spawn.ProcessExitedException: process 0 terminated with signal SIGKILL
python-BaseException
Traceback (most recent call last):
  File ""/home/username/.conda/envs/torchhydro1/lib/python3.11/multiprocessing/spawn.py"", line 132, in _main
    self = reduction.pickle.load(from_parent)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
_pickle.UnpicklingError: pickle data was truncated
python-BaseException
/home/username/.conda/envs/torchhydro1/lib/python3.11/multiprocessing/resource_tracker.py:254: UserWarning: resource_tracker: There appear to be 30 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '

Now I have 2 problems:
First, why it will appears _pickle.UnpicklingError?
Second, after executing 0%|          | 0/22986 [00:00<?, ?it/s], there is 7 ……s in my process log, means that this DEBUG process has been repeated for 8 or 9 times! I have set num_worker of pytorch DataLoader to 8, does this problem have connection with num_worker?
This problem occurs after I'm using polars, so I think problem comes from polars, or threads in polars and pytorch have some mistakes.
But how to know why there is UnpicklingError and solve it? Hope for your reply.
","It's mistake to filter polars.Dataframe and convert result to torch.Tensor in __get_item__ of torch.Dataset.
Convert the whole dataframe to tensor solved the problem.
",pytorch
kfolds crossvalidator show keyerror none of int64index,"I try to use K-Folds cross-validator with dicision tree. I use for loop to train and test data from KFOLD like this code.
df = pd.read_csv(r'C:\\Users\data.csv')
    
# split data into X and y
X = df.iloc[:,:200]
Y = df.iloc[:,200]

X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2)

clf = DecisionTreeClassifier()

kf =KFold(n_splits=5, shuffle=True, random_state=3)

cnt = 1

# Cross-Validate
for train, test in kf.split(X, Y):
    print(f'Fold:{cnt}, Train set: {len(train)}, Test set:{len(test)}')
    cnt += 1
    
    X_train = X[train]
    y_train = Y[train]
    X_test = X[test]
    y_test = Y[test]

    clf = clf.fit(X_train,y_train)

    predictions = clf.predict(X_test)
    accuracy = accuracy_score(y_test, predictions)

    print(""test"")
    print(y_test)
    print(""predict"")
    print(predictions)
    print(""Accuracy: %.2f%%"" % (accuracy * 100.0))

when I run it show error like this.
KeyError: ""None of [Int64Index([  0,   1,   2,   5,   7,   8,   9,  10,  11,  12,\n            ...\n            161, 164, 165, 166, 167, 168, 169, 170, 171, 173],\n           dtype='int64', length=120)]

How to fix it?
","The issue is here:
X_train = X[train]
y_train = Y[train]
X_test = X[test]
y_test = Y[test]

To access some parts/slices of your dataframe, you should use the iloc property. This should solve your problem:
X_train = X.iloc[train]
y_train = Y.iloc[train]
X_test = X.iloc[test]
y_test = Y.iloc[test]

",cross-validation
openai api returns null when i retrieve the finetuned model,"I'm fine-tuning a model and generating actions from text. I create a train.jsonl file, upload it, and fine-tune the model. However, when I try to get the name of the model I just created, it returns null.
I fine-tune a model like this:
const model = await openai.fineTuning.jobs.create({
  training_file: process.env.FILE_ID,
  model: 'babbage-002',
})

Then I try to retrieve the fine-tuned model like this:
const response = await openai.fineTuning.jobs.retrieve(
    process.env.FINE_TUNE_ID,
)

But this is the response I get from the OpenAI API:
data:  {
  object: 'fine_tuning.job',
  id: 'ftjob-NSFvxzJtTSfR5jcqQTfeDTCo',
  model: 'babbage-002',
  created_at: 1722546992,
  fine_tuned_model: null,
  organization_id: 'org-GLjhkXwkbQrLOvHk0762UcmL',
  result_files: [],
  status: 'running',
  validation_file: null,
  training_file: 'file-Lh1C4Vv1HDIv7LxXUGh9mIL9',
  hyperparameters: { n_epochs: 9, batch_size: 1, learning_rate_multiplier: 16 },
  trained_tokens: null,
  error: {},
  user_provided_suffix: null,
  seed: 1564492262,
  estimated_finish: null,
  integrations: []
}

","The fine-tuning job hasn't finished yet.
The fine-tuning flow is the following:

Create a fine-tuning job.
Fine-tuning is in progress.
Retrieve the fine-tuning job.

Try to run the following code to see if the fine-tuning is still in progress.
import OpenAI from ""openai"";

const client = new OpenAI();

async function main() {
  const list = await client.fineTuning.jobs.list();

  for await (const fineTune of list) {
    console.log(fineTune);
  }
}

main();

",fine-tune
implement dropout to pretrained resnet model in pytorch,"I am trying to implement Dropout to pretrained Resnet Model in Pytorch, and here is my code
    feats_list = []
    for key, value in model._modules.items():
        feats_list.append(value)

    for feat in feats_list:
        if isinstance(feat, nn.Conv2d) or isinstance(feat, nn.Conv1d):
            feats_list.append(nn.Dropout(p=0.5, inplace=True))

    model.features = nn.Sequential(*feats_list)
    print(model.features)


I think it should apply dropout to all conv2 and conv1 layers, but in reality, only the last AdaptiveAvgPool2d has attached a drop out rate.
This is what I got
...
  (8): AdaptiveAvgPool2d(output_size=(1, 1))
  (9): Linear(in_features=2048, out_features=1000, bias=True)
  (10): Dropout(p=0.5, inplace=True)

Could someone help me? Thank you
Here is the partial code block FYI
def generic_classifier(model, criterion, optimizer, num_epochs=25):
    # try to select specific layers to freeze or unfreeze from the pretrained model
    # true:trainable;  false: freeze, untraibale
    '''
    n = 0
    for param in model.parameters():
        if n  < 7:
            param.requires_grad = True
        else:
            param.requires_grad = False
        n +=1
    '''
    feats_list = []
    for key, value in model._modules.items():
        feats_list.append(value)

    for feat in feats_list:
        if isinstance(feat, nn.Conv2d) or isinstance(feat, nn.Conv1d):
            feats_list.append(nn.Dropout(p=0.5, inplace=True))
            #print(feat)
    #print(feats_list)

    # modify convolution layers
    model.features = nn.Sequential(*feats_list)
    print(model.features)
    #for name, param in model.named_parameters():
    #    print(name, param.requires_grad)

    # remove all the fully connected layers
    model.fc = nn.Sequential()

    # add a number of fully connected layers of our choice right after the convolutional layers
    model.fc = nn.Sequential(
        # need to know the last layer of selected model architecture
        # resnet50:2048, resnet18: 512, resnet34:512
        # did not find a way to automate this part yet.
        nn.Linear(512, 256),
        nn.ReLU(),
        nn.Linear(256, 256),
        nn.ReLU(),
        nn.Linear(256, 3)
    )

    model = model.to(device)

    # train the model
    model_with_pretrained_train_acc = []
    model_with_pretrained_test_acc = []
    start = time.time()

...
...
        return model

","The issue in your code is that you are appending the nn.Dropout layers to the feats_list within the loop that iterates over the model's modules. However, when you append a nn.Dropout layer to feats_list, it immediately gets added to the end of the list, causing the loop to continue iterating over it. So you are appending lot of dropout layers at the end of your architecture.
The following code contains a loop that runs through all the layers of the pretrained network and if it encounters a convolutional layer it creates an exactly equal one and appends it to the list followed by a dropout layer, otherwise it appends the layer as is without adding dropout layers.
model = models.resnet18(pretrained=True)

feats_list = []
for key, value in model.named_children():  
    if isinstance(value, nn.Conv2d) or isinstance(value, nn.Conv1d):
        feats_list.append(nn.Conv2d(
            in_channels=value.in_channels,
            out_channels=value.out_channels,
            kernel_size=value.kernel_size,
            stride=value.stride,
            padding=value.padding,
            bias=value.bias,
        ))
        feats_list.append(nn.Dropout(p=0.5, inplace=True))
    else:
        feats_list.append(value)

# Create a new model with the modified layers
model = nn.Sequential(*feats_list)

",fine-tune
how to shift a tensor like pandasshift in tensorflow  keras without shift the last row to first row like tfroll,"I want to shift a tensor in a given axis. It's easy to do this in pandas or numpy. Like this:
import numpy as np
import pandas as pd

data = np.arange(0, 6).reshape(-1, 2)
pd.DataFrame(data).shift(1).fillna(0).values

Output is:

array([[0., 0.],
[0., 1.],
[2., 3.]])

But in tensorflow, the closest solution I found is tf.roll. But it shift the last row to the first row. (I don't want that). So I have to use something like

tf.roll + tf.slice(remove the last row) + tf.concat(add tf.zeros to the first row).

It's really ugly.
Is there a better way to handle shift in tensorflow or keras?
Thanks.
","I think I find a better way for this problem.
We could use tf.roll, then apply tf.math.multiply to set the first row to zeros.
Sample code is as follows:
Original tensor:
A = tf.cast(tf.reshape(tf.range(27), (-1, 3, 3)), dtype=tf.float32)
A

Output:
<tf.Tensor: id=117, shape=(3, 3, 3), dtype=float32, numpy=
array([[[ 0.,  1.,  2.],
        [ 3.,  4.,  5.],
        [ 6.,  7.,  8.]],

       [[ 9., 10., 11.],
        [12., 13., 14.],
        [15., 16., 17.]],

       [[18., 19., 20.],
        [21., 22., 23.],
        [24., 25., 26.]]], dtype=float32)>

Shift (like pd.shift):
B = tf.concat((tf.zeros((1, 3)), tf.ones((2, 3))), axis=0)
C = tf.expand_dims(B, axis=0)
tf.math.multiply(tf.roll(A, 1, axis=1), C)

Output:
<tf.Tensor: id=128, shape=(3, 3, 3), dtype=float32, numpy=
array([[[ 0.,  0.,  0.],
        [ 0.,  1.,  2.],
        [ 3.,  4.,  5.]],

       [[ 0.,  0.,  0.],
        [ 9., 10., 11.],
        [12., 13., 14.]],

       [[ 0.,  0.,  0.],
        [18., 19., 20.],
        [21., 22., 23.]]], dtype=float32)>

",tensorflow
error in prediction step after imported a keras model to tensorflow java,"my goal is to use a Keras model in a java program.
I export the keras model with model.export() and not model.save() so I get well a folder with the model in .pb format.
Then I used py .\saved_model_cli.py show -- dir '.' -all  to see the inputs and outputs to fill in the java code.
I get that :
MetaGraphDef with tag-set: 'serve' contains the following SignatureDefs:

signature_def['__saved_model_init_op']:
  The given SavedModel SignatureDef contains the following input(s):
  The given SavedModel SignatureDef contains the following output(s):
    outputs['__saved_model_init_op'] tensor_info:
        dtype: DT_INVALID
        shape: unknown_rank
        name: NoOp
  Method name is:

signature_def['serve']:
  The given SavedModel SignatureDef contains the following input(s):
    inputs['keras_tensor'] tensor_info:
        dtype: DT_FLOAT
        shape: (-1, 6)
        name: serve_keras_tensor:0
  The given SavedModel SignatureDef contains the following output(s):
    outputs['output_0'] tensor_info:
        dtype: DT_FLOAT
        shape: (-1, 1)
        name: StatefulPartitionedCall:0
  Method name is: tensorflow/serving/predict

signature_def['serving_default']:
  The given SavedModel SignatureDef contains the following input(s):
    inputs['keras_tensor'] tensor_info:
        dtype: DT_FLOAT
        shape: (-1, 6)
        name: serving_default_keras_tensor:0
  The given SavedModel SignatureDef contains the following output(s):
    outputs['output_0'] tensor_info:
        dtype: DT_FLOAT
        shape: (-1, 1)
        name: StatefulPartitionedCall_1:0
  Method name is: tensorflow/serving/predict
The MetaGraph with tag set ['serve'] contains the following ops: {'ReadVariableOp', 'Select', 'StatefulPartitionedCall', 'RestoreV2', 'NoOp', 'Identity', 'StaticRegexFullMatch', 'StringJoin', 'AssignVariableOp', 'SaveV2', 'MergeV2Checkpoints', 'VarIsInitializedOp', 'AddV2', 'VarHandleOp', 'DisableCopyOnRead', 'Pack', 'Placeholder', 'MatMul', 'Const', 'Relu', 'ShardedFilename'}

Concrete Functions:2024-11-12 16:47:24.597134: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.

  Function Name: 'serve'
    Option #1
      Callable with:
        Argument #1
          keras_tensor: TensorSpec(shape=(None, 6), dtype=tf.float32, name='keras_tensor')

Finally, the java code to import and make a prediction is :
public static void importKerasModel() {
        try (SavedModelBundle model = SavedModelBundle.load(""PATH\kerasModel"", ""serve"")) {
            float[] x = {0.48f, 0.48f, 0.48f, 0.48f, 0.48f, 0.48f};
            try (Tensor input = TFloat32.vectorOf(x);
                 Tensor output = model.session()
                         .runner()
                         .feed(""serve_keras_tensor"", input)
                         .fetch(""StatefulPartitionedCall"")
                         .run()
                         .get(0)) {

                float prediction = output.dataType().getNumber();
                System.out.println(""prediction = "" + prediction);
            }
        }
    }


But I get this error message :
2024-11-12 17:26:01.089591: I tensorflow/cc/saved_model/loader.cc:317] SavedModel load for tags { serve }; Status: success: OK. Took 61548 microseconds.
2024-11-12 17:26:01.317247: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: INVALID_ARGUMENT: In[0] is not a matrix
     [[{{node StatefulPartitionedCall/StatefulPartitionedCall/sequential_1/dense_1/Relu}}]]
Exception in thread ""main"" org.tensorflow.exceptions.TFInvalidArgumentException: In[0] is not a matrix
     [[{{node StatefulPartitionedCall/StatefulPartitionedCall/sequential_1/dense_1/Relu}}]]
    at org.tensorflow.internal.c_api.AbstractTF_Status.throwExceptionIfNotOK(AbstractTF_Status.java:76)
    at org.tensorflow.Session.run(Session.java:826)
    at org.tensorflow.Session$Runner.runHelper(Session.java:549)
    at org.tensorflow.Session$Runner.run(Session.java:476)
    at com.ptvgroup.platform.truckslogs.converter.HelloTensorFlow.importKerasModel(HelloTensorFlow.java:471)
    at com.ptvgroup.platform.truckslogs.converter.Main.main(Main.java:25)

Someone can help me ? What does it means ""In[0] is not a matrix"" ? It's because my dimensions/shapes of the inputs and outputs are (-1,6) and (-1,1) ?
","The error 'In[0] is not a matrix' comes from a wrong data type when I would like make a prediction. I create a tensor of vector instead a tensor of matrix. The exception message told well the value was not a matrix. The value concerned is the tensor of inputs.
public static void importKerasModel() {
        try (SavedModelBundle model = SavedModelBundle.load(""PATH"", ""serve"")) {
            float[] x = {200f,0f,1.5f,2f,2.5f,0f};
            FloatNdArray matrix = NdArrays.ofFloats(Shape.of(1, 6)); // my model have 6 features per observations
            matrix.set(NdArrays.vectorOf(x), 0);
            try (Tensor input = TFloat32.tensorOf(matrix);
                 TFloat32 output = (TFloat32) model.session()
                         .runner()
                         .feed(""serve_keras_tensor_272"", input)  ///## to know inputs and outputs  py .\saved_model_cli.py show --dir '.' --all
                         .fetch(""StatefulPartitionedCall"")
                         .run()
                         .get(0)) {
                float prediction =  output.getFloat();;
                System.out.println(""prediction = "" + prediction);
            }
        }
    }

This code fix the failed execution.
",tensorflow
retrieve cross validation performance auc on h2o automl for holdout dataset,"I am training a binary classification model with h2o AutoML using the default cross-validation (nfolds=5). I need to obtain the AUC score for each holdout fold in order to compute the variability.
This is the code I am using:
h2o.init()

prostate = h2o.import_file(""https://h2o-public-test-data.s3.amazonaws.com/smalldata/prostate/prostate.csv"")
# convert columns to factors
prostate['CAPSULE'] = prostate['CAPSULE'].asfactor()
prostate['RACE'] = prostate['RACE'].asfactor()
prostate['DCAPS'] = prostate['DCAPS'].asfactor()
prostate['DPROS'] = prostate['DPROS'].asfactor()

# set the predictor and response columns
predictors = [""AGE"", ""RACE"", ""VOL"", ""GLEASON""]
response_col = ""CAPSULE""

# split into train and testing sets
train, test = prostate.split_frame(ratios = [0.8], seed = 1234)


aml = H2OAutoML(seed=1, max_runtime_secs=100, exclude_algos=[""DeepLearning"", ""GLM""],
                    nfolds=5, keep_cross_validation_predictions=True)

aml.train(predictors, response_col, training_frame=prostate)

leader = aml.leader

I check that leader is not a StackedEnsamble model (for which the validation metrics are not available). Anyway, I am not able to retrieve the five AUC scores.
Any idea on how to do so?
","Here's how it's done:
import h2o
from h2o.automl import H2OAutoML

h2o.init()

# import prostate dataset
prostate = h2o.import_file(""https://h2o-public-test-data.s3.amazonaws.com/smalldata/prostate/prostate.csv"")
# convert columns to factors
prostate['CAPSULE'] = prostate['CAPSULE'].asfactor()
prostate['RACE'] = prostate['RACE'].asfactor()
prostate['DCAPS'] = prostate['DCAPS'].asfactor()
prostate['DPROS'] = prostate['DPROS'].asfactor()

# set the predictor and response columns
predictors = [""AGE"", ""RACE"", ""VOL"", ""GLEASON""]
response_col = ""CAPSULE""

# split into train and testing sets
train, test = prostate.split_frame(ratios = [0.8], seed = 1234)

# run AutoML for 100 seconds
aml = H2OAutoML(seed=1, max_runtime_secs=100, exclude_algos=[""DeepLearning"", ""GLM""],
                    nfolds=5, keep_cross_validation_predictions=True)
aml.train(x=predictors, y=response_col, training_frame=prostate)

# Get the leader model
leader = aml.leader

There is a caveat to mention here about cross-validated AUC -- H2O currently stores two computations of CV AUC.  One is an aggregated version (take the AUC of aggregated CV predictions), and the other is the ""true"" definition of cross-validated AUC (an average of the k AUCs from k-fold cross-validation).  The latter is stored in an object which also contains the individual fold AUCs, as well as the standard deviation across the folds.
If you're wondering why we do this, there's some historical & technical reasons why we have two versions, as well as a ticket open to only every report the latter.
The first one is what you get when you do this (and also what appears on the AutoML Leaderboard).
# print CV AUC for leader model
print(leader.model_performance(xval=True).auc())

If you want the fold-wise AUCs so you can compute or view their mean and variability (standard deviation), you can do that by looking here:
# print CV metrics summary
leader.cross_validation_metrics_summary()

Output:
Cross-Validation Metrics Summary:
             mean        sd           cv_1_valid    cv_2_valid    cv_3_valid    cv_4_valid    cv_5_valid
-----------  ----------  -----------  ------------  ------------  ------------  ------------  ------------
accuracy     0.71842104  0.06419111   0.7631579     0.6447368     0.7368421     0.7894737     0.65789473
auc          0.7767409   0.053587236  0.8206676     0.70905924    0.7982079     0.82538515    0.7303846
aucpr        0.6907578   0.0834025    0.78737605    0.7141305     0.7147677     0.67790955    0.55960524
err          0.28157896  0.06419111   0.23684211    0.35526314    0.2631579     0.21052632    0.34210527
err_count    21.4        4.8785243    18.0          27.0          20.0          16.0          26.0
---          ---         ---          ---           ---           ---           ---           ---
precision    0.61751753  0.08747421   0.675         0.5714286     0.61702126    0.7241379     0.5
r2           0.20118153  0.10781976   0.3014902     0.09386432    0.25050205    0.28393403    0.07611712
recall       0.84506994  0.08513061   0.84375       0.9142857     0.9354839     0.7241379     0.8076923
rmse         0.435928    0.028099842  0.41264254    0.47447023    0.42546       0.41106534    0.4560018
specificity  0.62579334  0.15424488   0.70454544    0.41463414    0.6           0.82978725    0.58

See the whole table with table.as_data_frame()

Here's what the leaderboard looks like (storing aggregated CV AUCs).  In this case, because the data is so small (300 rows), there's a noticeable difference between the two reported between the two reported CV AUC values, however for larger datasets, they should be much closer estimates.
# print the whole Leaderboard (all CV metrics for all models)
lb = aml.leaderboard
print(lb)

That will print the top of the leaderboard:
model_id                                                  auc    logloss     aucpr    mean_per_class_error      rmse       mse
---------------------------------------------------  --------  ---------  --------  ----------------------  --------  --------
XGBoost_grid__1_AutoML_20200924_200634_model_2       0.769716   0.565326  0.668827                0.290806  0.436652  0.190665
GBM_grid__1_AutoML_20200924_200634_model_4           0.762993   0.56685   0.666984                0.279145  0.437634  0.191524
XGBoost_grid__1_AutoML_20200924_200634_model_9       0.762417   0.570041  0.645664                0.300121  0.440255  0.193824
GBM_grid__1_AutoML_20200924_200634_model_6           0.759912   0.572651  0.636713                0.30097   0.440755  0.194265
StackedEnsemble_BestOfFamily_AutoML_20200924_200634  0.756486   0.574461  0.646087                0.294002  0.441413  0.194845
GBM_grid__1_AutoML_20200924_200634_model_7           0.754153   0.576821  0.641462                0.286041  0.442533  0.195836
XGBoost_1_AutoML_20200924_200634                     0.75411    0.584216  0.626074                0.289237  0.443911  0.197057
XGBoost_grid__1_AutoML_20200924_200634_model_3       0.753347   0.57999   0.629876                0.312056  0.4428    0.196072
GBM_grid__1_AutoML_20200924_200634_model_1           0.751706   0.577175  0.628564                0.273603  0.442751  0.196029
XGBoost_grid__1_AutoML_20200924_200634_model_8       0.749446   0.576686  0.610544                0.27844   0.442314  0.195642

[28 rows x 7 columns]

",cross-validation
llm output repeating itself,"I am currently following this tutorial on making a basic LLM that spews Shakespeare like text(The full code for the transformer is at the the end). I am at the end but when I train it and get an output the output just keeps repeating itself with the same stuff. Here is my code
import tiktoken
import torch
import torch.nn as nn
from torch.nn import functional as F
from functions.encode import encode_chars
from functions.character_amount import character_amount
from functions.train_test_split import train_test_split
from functions.decoding import decoding
with open(r'example_shakespeare_text.txt') as file:
    file = file.read()
split = (file.split('\n'))
max_iters = 25
num_embed = 64
num_heads = 16
num_layers = 8
batch_size = 32
block_size = 128
dropout = 0.2
learning_rate = 1e-3

if torch.cuda.is_available():
    device = 'cuda'
else:
    device = 'cpu'

encode = tiktoken.get_encoding('gpt2')

characters = character_amount(encode=encode, split=split)
vocab_size = encode.n_vocab
    
encoded = encode_chars(split=split, encode=encode)

data = torch.tensor(encoded, dtype=torch.long)
train_data, test_data = train_test_split(data=data)

def array_creation(split):
    if split == 'train':
        data = train_data
    else:
        data = test_data

    ix = torch.randint(len(data) - block_size, (batch_size,))
    x = torch.stack([data[i:i + block_size] for i in ix])
    y = torch.stack([data[i+1:i+block_size + 1] for i in ix])
    x = x.to(device)
    y = y.to(device)
    return x, y
        
class Head(nn.Module):
    def __init__(self, head_size):
        super().__init__()
        self.key = nn.Linear(num_embed, head_size, bias=False)
        self.query = nn.Linear(num_embed, head_size, bias=False)
        self.value = nn.Linear(num_embed, head_size, bias=False)
        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))

        self.dropout = nn.Dropout(dropout)

    def forward(self,x):
        B, T, C = x.shape
        head_size = 16
        key = nn.Linear(C, head_size, bias=False)
        query = nn.Linear(C, head_size, bias=False)
        k = key(x)
        q = query(x)
        weight =  q @ k.transpose(-2,-1) * C **-0.5
        weight = weight.masked_fill(self.tril[:T,:T] == 0, float('-inf'))
        weight = F.softmax(weight, dim=-1)
        weight = self.dropout(weight)

        v = self.value(x)
        out = weight @ v
        return out

class MultiHead(nn.Module):
    def __init__(self, num_heads, head_size):
        super().__init__()
        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])
        self.prj = nn.Linear(num_embed, num_embed)
        self.dropout = nn.Dropout(dropout)
    def forward(self, x):
        out = torch.cat([h(x) for h in self.heads], dim=-1)
        out = self.dropout(self.prj(out))
        return out

class FeedForward(nn.Module):
    def __init__(self, num_embed):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(num_embed,  4 * num_embed),
            nn.ReLU(),
            nn.Linear(4 * num_embed, num_embed),
            nn.Dropout(dropout)
        )
    def forward(self, x):
        return self.net(x)
    
class Block(nn.Module):
    def __init__(self, num_embed,num_heads):
        super().__init__()
        head_size = num_embed // num_heads
        self.sa = MultiHead(num_heads, head_size)
        self.ffwd = FeedForward(num_embed)
        self.layernorm1 = nn.LayerNorm(num_embed)
        self.layernorm2 = nn.LayerNorm(num_embed)

    def forward(self, x):
        x = x + self.sa(self.layernorm1(x))
        x = x + self.ffwd(self.layernorm2(x))
        return x

class BigramLanguageModel(nn.Module):

    def __init__(self):
        super().__init__()
        self.token_embedding_table = nn.Embedding(vocab_size, num_embed)
        self.position_embedding_table = nn.Embedding(block_size, num_embed)
        self.blocks = nn.Sequential(*[Block(num_embed, num_heads=num_heads) for _ in range(num_layers)])
        self.ln_f = nn.LayerNorm(num_embed)
        self.lm_head = nn.Linear(num_embed, vocab_size)

    def forward(self, idx, targets=None):
        B, T = idx.shape
        token_emb = self.token_embedding_table(idx)
        position_embedding = self.position_embedding_table(torch.arange(T, device=device))
        x = token_emb + position_embedding
        x = self.blocks(x)
        x = self.ln_f(x)
        logits = self.lm_head(x)
        
        if targets != None:
            B, T, C = logits.shape
            logits = logits.view(B*T, C)
            targets = targets.view(B*T)
            
            loss = F.cross_entropy(logits, targets)
        else:
            loss = None
        return logits, loss
    def generate(self, idx, max_new_tokens):
        for _ in range(max_new_tokens):
            idx_cond = idx[:, -block_size:]
            logits, loss = self(idx_cond)
            logits = logits[:, -1, :]
            
            probs = F.softmax(logits, dim=1)
            idx_next = torch.multinomial(probs, num_samples=1)
            idx = torch.cat((idx, idx_next), dim=1)
        return idx

m = BigramLanguageModel()
model = m.to(device)

generated_list = model.generate(idx= torch.zeros((1,1), dtype=torch.long), max_new_tokens=100)[0].tolist()
decoded_list = decoding(generated_list=generated_list, encode=encode)
    
optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)

iteration = 0
for _ in range(max_iters):
    xy, yb = array_creation('train')
    
    logits, loss = model(xy, yb)
    optimizer.zero_grad(set_to_none=True)
    loss.backward()
    optimizer.step()
    iteration += 1

    print(iteration)
    print(loss.item())

context =  torch.zeros((1,1), dtype=torch.long, device=device)
print(decoding(generated_list=model.generate(context,max_new_tokens=100)[0].tolist(), encode=encode))

Here is the output
A', '! re al, we hear me speak.All:Speak.First Citizen:You are all resolved rather to die to than famish?A', '! re al, we hear me speak.All:Speak.First Citizen:You are all resolved rather to die to than famish?A', '! re al, we hear me speak.All:Speak.First Citizen:You are all resolved rather to die to than famish?A', '! re al, we hear me speak.All:Speak.First Citizen:You are all resolved rather to die to than famish?

It keeps repeating itself even farther than that.
I tried to increase the amount of data going in but that didn't help, I also tried to change the amount of iterations and batch size/block size. But it still didnt change the repetition.
Do I just need to do even more intense training?
","It was a problem with my decoding function, not sure what but I made my own tokenizer rather than using tiktoken and it fixed the problem.
",pytorch
sagemaker failed to extract model data archive targz for container when deploying,"I am trying in Amazon Sagemaker to deploy an existing Scikit-Learn model. So a model that wasn't trained on SageMaker, but locally on my machine.
On my local (windows) machine I've saved my model as model.joblib and tarred the model to model.tar.gz.
Next, I've uploaded this model to my S3 bucket ('my_bucket') in the following path s3://my_bucket/models/model.tar.gz. I can see the tar file in S3.
But when I'm trying to deploy the model, it keeps giving the error message ""Failed to extract model data archive"".
The .tar.gz is generated on my local machine by running 'tar -czf model.tar.gz model.joblib' in a powershell command window.
The code for uploading to S3
import boto3
s3 = boto3.client(""s3"", 
              region_name='eu-central-1', 
              aws_access_key_id=AWS_KEY_ID, 
              aws_secret_access_key=AWS_SECRET)
s3.upload_file(Filename='model.tar.gz', Bucket=my_bucket, Key='models/model.tar.gz')

The code for creating the estimator and deploying:
import boto3
from sagemaker.sklearn.estimator import SKLearnModel

...

model_data = 's3://my_bucket/models/model.tar.gz'
sklearn_model = SKLearnModel(model_data=model_data,
                             role=role,
                             entry_point=""my-script.py"",
                             framework_version=""0.23-1"")
predictor = sklearn_model.deploy(instance_type=""ml.t2.medium"", initial_instance_count=1)                             

The error message:

error message: UnexpectedStatusException: Error hosting endpoint
sagemaker-scikit-learn-2021-01-24-17-24-42-204: Failed. Reason: Failed
to extract model data archive for container ""container_1"" from URL
""s3://my_bucket/models/model.tar.gz"". Please ensure that the object
located at the URL is a valid tar.gz archive

Is there a way to see why the archive is invalid?
","I had a similar issue as well, along with a similar fix to Bas (per comment above).
I was finding I wasn't necessarily having issues with the .tar.gz step, this command does work fine:
tar -czf <filename> ./<directory-with-files>
but rather with the uploading step.
Manually uploading to S3 should take care of this, however, if you're doing this step programmatically, you might need to double check the steps taken. Bas appears to have had filename issues, mine were around using boto properly. Here's some code that works (Python only here, but watch for similar issues with other libraries):
bucket = 'bucket-name'
key = 'directory-inside-bucket'
file = 'the file name of the .tar.gz'

s3_client = boto3.client('s3')
s3_client.upload_file(file, bucket, key)

Docs: https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/s3.html#S3.Client.upload_file
",scikit-learn
cross validation and model selection,"I am using sklearn for SVM training. I am using the cross-validation to evaluate the estimator and avoid the overfitting model.
I split the data into two parts. Train data and test data. Here is the code:
import numpy as np
from sklearn import cross_validation
from sklearn import datasets
from sklearn import svm

X_train, X_test, y_train, y_test = cross_validation.train_test_split(
    iris.data, iris.target, test_size=0.4, random_state=0
)
clf = svm.SVC(kernel='linear', C=1)
scores = cross_validation.cross_val_score(clf, X_train, y_train, cv=5)
print scores

Now I need to evaluate the estimator clf on X_test.
clf.score(X_test, y_test)

here,  I get an error saying that
the model is not fitted using fit()

but normally, in cross_val_score function the model is fitted? What is the problem?
","cross_val_score is basically a convenience wrapper for the sklearn cross-validation iterators. You give it a classifier and your whole (training + validation) dataset and it automatically performs one or more rounds of cross-validation by splitting your data into random training/validation sets, fitting the training set, and computing the score on the validation set. See the documentation here for an example and more explanation.
The reason why clf.score(X_test, y_test) raises an exception is because cross_val_score performs the fitting on a copy of the estimator rather than the original (see the use of clone(estimator) in the source code here). Because of this, clf remains unchanged outside of the function call, and is therefore not properly initialized when you call clf.fit.
",cross-validation
how to have untracked weights in custom keras layer,"I would like to create a custom keras layer (a codebook for a VQVAE model.)  While training I would like to have a tf.Variable which tracks the usage of each code so I can restart unused codes.  So I created my Codebook layer as follows...
class Codebook(layers.Layer): 
     def __init__(self, num_codes, code_reset_limit = None, **kwargs): 
         super().__init__(**kwargs) 
         self.num_codes = num_codes 
         self.code_reset_limit = code_reset_limit 
         if self.code_reset_limit: 
             self.code_counter = tf.Variable(tf.zeros(num_codes, dtype = tf.int32), trainable = False) 
     def build(self, input_shape): 
         self.codes = self.add_weight(name = 'codes',  
                                      shape = (self.num_codes, input_shape[-1]), 
                                      initializer = 'random_uniform',  
                                      trainable = True) 
         super().build(input_shape) 
                                                                                                             

The issue I have is that the Layer class finds the member variable self.code_counter and adds it to the list of weights which are saved with the layer.  It also expects the self.code_counter to be present when weights are loaded which is not the case when I run in inference mode.  How can I make it so keras does not track a variable in my layer.  I do not want it persisted or to be part of the layers.weights.
","I am a bit late with the answer, but I had the same problem and came across the question without an answer. Now, I have found an answer that works for Keras 2 and Keras 3, so I am sharing it here for others encountering the same question.
To prevent TensorFlow and Keras from tracking variables one needs to encapsulate the variable in a class that TensorFlow and Keras do not handle in the tracking module. The list of classes that are automatically tracked for Keras 3 are: keras.Variable, list, dict, tuple, and NamedTuple
(see here). For Keras 2 the list of objects is not so easy to find but appears to include tf.Variable (see the present question), dict, and list.
The solution that did work in my context for keras.Variable and tf.Variable is to create dataclass encapsulating the Variable. Here the setup for tensorflow and keras 2.
import tensorflow as tf
from dataclasses import dataclass

@dataclass
class DoNotTrackContainer:
    data: tf.Variable


In the code of the present question, this would then be used like this
 if self.code_reset_limit: 
     self.code_counter = DoNotTrackContainer(data=tf.Variable(tf.zeros(num_codes, dtype = tf.int32), trainable = False) )

When accessing the counter the data attribute needs to be included in the path
  # for accessing the counter
  self.code_counter.data.assign_add(1) 

For Keras 3 the Container becomes
import keras
from dataclasses import dataclass

@dataclass
class DoNotTrackContainer:
    data: keras.Variable


",tensorflow
unable to save generated data to jsonl file  always resulting in quotwrote 0 examples to finetuning_eventsjsonlquot message,"Issue Description
When attempting to generate JSONL data using Llama Index, the process works well until the final step where the results are saved to a JSONL file. However, every time I try to save the data, it seems to be unsuccessful as I always receive the message ""Wrote 0 examples to finetuning_events.jsonl"". I am unsure of the reason behind this issue.
Steps to Reproduce

Successfully generated JSONL data using Llama Index.
Attempted to save the results to a JSONL file.
Received the message ""Wrote 0 examples to finetuning_events.jsonl"".

Additional Information

Llama Index version used: 0.10.22
Operating System: Windows

Log
Wrote 0 examples to ./dataset_data/finetuning_events.jsonl
My code:
     def jsonl_generation(self):
        """"""
        Generate JSONL file for fine-tuning events and perform model refinement.
        """"""
        # Initialize OpenAI FineTuningHandler and CallbackManager
        finetuning_handler = OpenAIFineTuningHandler()
        callback_manager = CallbackManager([finetuning_handler])

        self.llm.callback_manager = callback_manager

        # Load questions for fine-tuning from a file
        questions = []
        with open(f'{self.dataset_path}/train_questions.txt', ""r"", encoding='utf-8') as f:
            for line in f:
                questions.append(line.strip())

        try:
            # Generate responses to the questions using GPT-4 and save the fine-tuning events to a JSONL file
            index = VectorStoreIndex.from_documents(
                self.documents
            )
            query_engine = index.as_query_engine(similarity_top_k=2, llm=self.llm)
            for question in questions:
                response = query_engine.query(question)
        except Exception as e:
            # Handle the exception here, you might want to log the error or take appropriate action
            print(f""An error occurred: {e}"")
        finally:
            # Save the fine-tuning events to a JSONL file
            finetuning_handler.save_finetuning_events(f'{self.dataset_path}/finetuning_events.jsonl')

","I just solved the problem.
It's my solution. Currently, It's storing the dataset to jsonl data.
    def jsonl_generation(self):
        """"""
        Generate JSONL file for fine-tuning events and perform model refinement.
        """"""
        # Initialize OpenAI FineTuningHandler and CallbackManager
        finetuning_handler = OpenAIFineTuningHandler()
        callback_manager = CallbackManager([finetuning_handler])

        llm = OpenAI(model=""gpt-4"", temperature=0.3)
        Settings.callback_manager, = (callback_manager,)

        # Load questions for fine-tuning from a file
        questions = []
        with open(f'{self.dataset_path}/train_questions.txt', ""r"", encoding='utf-8') as f:
            for line in f:
                questions.append(line.strip())

        try:
            from llama_index.core import VectorStoreIndex
            # Generate responses to the questions using GPT-4 and save the fine-tuning events to a JSONL file
            index = VectorStoreIndex.from_documents(
                self.documents
            )
            query_engine = index.as_query_engine(similarity_top_k=2, llm=llm)
            for question in questions:
                response = query_engine.query(question)
        except Exception as e:
            # Handle the exception here, you might want to log the error or take appropriate action
            print(f""An error occurred: {e}"")
        finally:
            # Save the fine-tuning events to a JSONL file
            finetuning_handler.save_finetuning_events(f'{self.dataset_path}/finetuning_events.jsonl')

",fine-tune
why does garchsvm output identical predictions for conditional volatility,"I'm using the SVR-GARCH model to predict conditional volatility, as described in the book Machine Learning for Financial Risk Management with Python: Algorithms for Modeling Risk by Abdullah Karasan.
I’ve encountered an issue where my code sometimes produces the same repeated value for the conditional volatility across the entire forecasting horizon. I understand that the initial parameter values are randomized, but I am confused about why, in most cases, the prediction results in a constant value throughout the forecast period.
import yfinance as yf
from datetime import datetime, timedelta
import pandas as pd
import numpy as np
from sklearn.svm import SVR
from sklearn.model_selection import RandomizedSearchCV
from scipy.stats import uniform as sp_rand
from sklearn.preprocessing import StandardScaler

# Select assets
stock_name = ['AAPL']
end_date = datetime.today()
start_date = end_date - timedelta(days = 365 * 25)

# Download the prices
prices = yf.download(
    stock_name,
    start = start_date,
    end = end_date,
    interval = '1d',
)['Adj Close']
prices = prices.dropna()
stock_name = ['Apple']
prices = prices.rename(stock_name[0], inplace = True)

# Log returns
returns = np.log(np.array(prices)[1:] / np.array(prices)[:-1])

# Forecasting horizon
H = 146

returns_series = pd.Series(returns)
realized_vol = returns_series.rolling(5).std()
realized_vol = pd.DataFrame(realized_vol)
realized_vol.reset_index(drop=True, inplace=True)

returns_svm = pd.DataFrame(returns ** 2)

X = pd.concat([realized_vol, returns_svm], axis=1, ignore_index=True)
X = X[4:].copy()
X = X.reset_index()
X.drop('index', axis=1, inplace=True)

realized_vol = realized_vol.dropna().reset_index()
realized_vol.drop('index', axis=1, inplace=True)

conditional_volatility = pd.DataFrame(index=prices.index[-H:], columns=['SVM Linear','SVM RBF','SVM Poly'])

para_grid = {'gamma': sp_rand(0.1, 1), 'C': sp_rand(0.1, 10), 'epsilon': sp_rand(0.1, 1)}

svr_lin = SVR(kernel='linear')
clf = RandomizedSearchCV(svr_lin, para_grid)
clf.fit(X[:-H], realized_vol.iloc[1:-(H-1)].values.reshape(-1,))
predict_svr_lin = clf.predict(X[-H:])
conditional_volatility['SVM Linear'] = predict_svr_lin

svr_rbf = SVR(kernel='rbf')
clf = RandomizedSearchCV(svr_rbf, para_grid)
clf.fit(X[:-H], realized_vol.iloc[1:-(H-1)].values.reshape(-1,))
predict_svr_rbf = clf.predict(X[-H:])
conditional_volatility['SVM RBF'] = predict_svr_rbf

svr_poly = SVR(kernel='poly')
clf = RandomizedSearchCV(svr_poly, para_grid)
clf.fit(X[:-H], realized_vol.iloc[1:-(H-1)].values.reshape(-1,))
predict_svr_poly = clf.predict(X[-H:])
conditional_volatility['SVM Poly'] = predict_svr_poly

print(conditional_volatility)

Output:
[*********************100%%**********************]  1 of 1 completed
            SVM Linear   SVM RBF  SVM Poly
Date                                      
2024-01-09    0.168156  0.168156  0.138204
2024-01-10    0.168156  0.168156  0.138204
2024-01-11    0.168156  0.168156  0.138204
2024-01-12    0.168156  0.168156  0.138204
2024-01-16    0.168156  0.168156  0.138204
...                ...       ...       ...
2024-08-01    0.168156  0.168156  0.138204
2024-08-02    0.168156  0.168156  0.138204
2024-08-05    0.168156  0.168156  0.138204
2024-08-06    0.168156  0.168156  0.138204
2024-08-07    0.168156  0.168156  0.138204

[146 rows x 3 columns]

Why this might be happening and how to address it?
","the issue is with your epsilon values. According to the SVR manual:

epsilon - Epsilon in the epsilon-SVR model. It specifies the
epsilon-tube within which no penalty is associated in the training
loss function with points predicted within a distance epsilon from the
actual value. Must be non-negative.

Thus with high epsilon (epsilon much greater than volatility) you won't give any penalties for bad predictions.
Let's check how your data is distributed:
print(np.quantile(realized_vol, q = [0,0.01,0.05,0.5,0.95, 0.99,1]))
# [0.00075101 0.00375092 0.0058641  0.04780242 0.06916338 0.33556116]

Basically in the beginning of the dataset volatility is very low and it is much greater in later periods.
Anyway, if you allow epsilon to be close to 1 (as in your code) you don't give penalty to any of the volatility points, because they all far below 1.
The fix is to change parameter grid to smth like this:
para_grid = {'gamma': sp_rand(0.1, 1), 
             'C': sp_rand(0.1, 10), 
             'epsilon': sp_rand(0.001, 0.01)
            }

or to multiply volatility by a high constant (after all volatility is a percentage):
para_grid = {'gamma': sp_rand(0.1, 1), 
             'C': sp_rand(0.1, 10), 
             'epsilon': sp_rand(0.1, 1)
             }

realized_vol = realized_vol * 100

",scikit-learn
why is scikitlearn svm classifier cross validation so slow,"I am trying to compare multiple classifiers on a dataset that I have. To get accurate accuracy scores for the classifiers I am now performing 10 fold cross validation for each classifier. This goes well for all of them except SVM (both linear and rbf kernels). The data is loaded like this:
dataset = pd.read_csv(""data/distance_annotated_indels.txt"", delimiter=""\t"", header=None)

X = dataset.iloc[:, [5,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26]].values
y = dataset.iloc[:, 4].values

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2)

from sklearn.preprocessing import StandardScaler
sc = StandardScaler()
X_train = sc.fit_transform(X_train)
X_test = sc.transform(X_test)

Cross validation for for example a Random Forest works fine:
start = time.time()
classifier = RandomForestClassifier(n_estimators = 100, criterion = 'entropy')
classifier.fit(X_train, y_train)
y_pred = classifier.predict(X_test)
cv = ShuffleSplit(n_splits=10, test_size=0.2)
scores = cross_val_score(classifier, X, y, cv=10)
print(classification_report(y_test, y_pred))
print(""Random Forest accuracy after 10 fold CV: %0.2f (+/- %0.2f)"" % (scores.mean(), scores.std() * 2) + "", "" + str(round(time.time() - start, 3)) + ""s"")

Output:
             precision    recall  f1-score   support

          0       0.97      0.95      0.96      3427
          1       0.95      0.97      0.96      3417

avg / total       0.96      0.96      0.96      6844

Random Forest accuracy after 10 fold CV: 0.92 (+/- 0.06), 90.842s

However for SVM this process takes ages (waited for 2 hours, still nothing). The sklearn website does not make me any wiser. Is there something I should be doing different for SVM classifiers? The SVM code is as follows:
start = time.time()
classifier = SVC(kernel = 'linear')
classifier.fit(X_train, y_train)
y_pred = classifier.predict(X_test)
scores = cross_val_score(classifier, X, y, cv=10)
print(classification_report(y_test, y_pred))
print(""Linear SVM accuracy after 10 fold CV: %0.2f (+/- %0.2f)"" % (scores.mean(), scores.std() * 2) + "", "" + str(round(time.time() - start, 3)) + ""s"")

","If you have a lot of samples the computational complexity of the problem gets in the way, see Training complexity of Linear SVM.
Consider playing with the verbose flag of cross_val_score to see more logs about progress. Also, with n_jobs set to a value  > 1 (or even using all CPUs with  n_jobs set to -1, if memory allows) you could speed up computation via parallelization. http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_score.html can be useful to evaluate these options.
If performance is poor I'd consider reducing the value of cv (see https://stats.stackexchange.com/questions/27730/choice-of-k-in-k-fold-cross-validation for a discussion on this)
",cross-validation
openai api error quotunrecognized request argument suppliedquot,"I'm receiving an error when calling the OpenAI API. It's not recognizing file argument, which I submitted to the API.
Here is my PHP code:
<?php

// Define your OpenAI API key and the endpoint
$apiKey = 'sk-TOh**********************************';
$endpoint = 'https://api.openai.com/v1/engines/davinci/completions';

// File ID of the uploaded data
$fileId = 'file-FlW6jPfNuuq1lTak91AjMj2j';

// Product name
$productName = '6 pack fresh grannies apples';

// Prompt to use the file ID as a reference
$prompt = ""Given the following data from the uploaded file $fileId, categorize the product '$productName':"";

// Prepare the cURL request data
$data = [
    'prompt' => $prompt,
    'max_tokens' => 1, // Adjust the token limit as needed
    'file' => $fileId // Reference the file by ID
];

// Prepare the cURL request
$ch = curl_init($endpoint);
curl_setopt($ch, CURLOPT_RETURNTRANSFER, true);
curl_setopt($ch, CURLOPT_HTTPHEADER, [
    'Authorization: Bearer ' . $apiKey,
    'Content-Type: application/json',
]);
curl_setopt($ch, CURLOPT_POST, 1);
curl_setopt($ch, CURLOPT_POSTFIELDS, json_encode($data));

// Execute the cURL request
$response = curl_exec($ch);

// Check for cURL errors
if (curl_errno($ch)) {
    echo 'cURL error: ' . curl_error($ch);
} else {
    // Parse the API response as JSON
    $responseData = json_decode($response, true);
echo ""<pre>"",print_r($responseData),""</pre>"";
    // Extract and display the category
    $category = $responseData['choices'][0]['text'];
    echo ""Product '$productName' belongs to the category: $category"";
}

// Close the cURL session
curl_close($ch);

?>

Here is the data of the file I uploaded:
{""prompt"": ""fruits"", ""completion"": ""apples, bananas, oranges, grapes, strawberries""}
{""prompt"": ""vegetables"", ""completion"": ""carrots, broccoli, spinach, lettuce, tomatoes""}
{""prompt"": ""dairy"", ""completion"": ""milk, cheese, yogurt, butter, cream""}
{""prompt"": ""meat"", ""completion"": ""chicken, beef, pork, lamb, turkey""}
{""prompt"": ""bakery"", ""completion"": ""bread, muffins, cookies, cakes, pies""}

Here is the error I'm receiving:
[error] => Array
(
  [message] => Unrecognized request argument supplied: file
  [type] => invalid_request_error
  [param] => 
  [code] => 
)

What am I doing wrong? I've tried searching for the answer and also looking at OpenAI documentation.
","Problem
You're trying to pass file as a parameter to the Completions API endpoint, which is not a valid parameter. You can't pass any parameter you make up to the Completions API endpoint.
Solution
See the complete list of parameters you can pass to the Completions API endpoint:

model
prompt
suffix
max_tokens
temperature
top_p
n
stream
logprobs
echo
stop
presence_penalty
frequency_penalty
best_of
logit_bias
user



Also, all Engines API endpoints are deprecated.

Use the Completions API endpoint.
Change the URL from this...
https://api.openai.com/v1/engines/davinci/completions

...to this.
https://api.openai.com/v1/completions

",fine-tune
how to setup tf and torch on one virtual environment with same cuda,"I want to setup tensorflow and pytorch on one virtual environment with same CUDA. However, I cannot find a CUDA version that can support both tensorflow and pytorch: For tensorflow 2.10, I selected CUDA 11.2. But I didn't find this CUDA version in the list for supporting pyTorch. I can only find the CUDA 11.1 in the list for pyTorch. Detailed information is listed below.

To find CUDA version for Tensorflow
https://www.tensorflow.org/install/source_windows#tested_build_configurations


To find CUDA version for PyTorch
https://elenacliu-pytorch-cuda-driver.streamlit.app/



Will there be any problems if I install 2 different CUDA versions if I want to run the codes with GPU card? For example, after I create a virtual environemt by ""conda create --name myenv python=3.10"", I want to run codes with tensorflow for project 1, and codes with pyTorch for project 2.
Do I need to modify the ""CUDA_PATH"" in system variable every time before I ran the codes. i.e., set CUDA_PATH for CUDA 11.1 when I need to use PyTorch, and set CUDA_PATH for CUDA 11.2 when I need to use Tensorflow?
I find there is an option of installing CUDA 11.0, which is compatible with TF-2.4 and PyTorch-1.7.
But there is a problem that it does not support CUDA capability SM_86. Will it be a problem of losing access to new features?

","There are no pre-built binaries of pytorch with cuda-11.2 indeed. If you necessarily want to go with this version of cuda, you have two choices I think:

Use pytorch binaries compiled with cuda-11.1, which should work just fine
Build pytorch from source, as described here

I'm basically repeating what is said on this pytorch thread, you can read it for more details
I would not try to have multiple versions of cuda and manually ""hotswap"" them by tinkering with the cuda paths. (Opinion here) From experience, it can work but is also very error prone and will lead to problems eventually
",tensorflow
inference error after training an ipadapter plus model,"I downloaded packages from https://github.com/tencent-ailab/IP-Adapter
run the commands to train an IP-Adapter plus model (input: text + image, output: image):
accelerate launch --num_processes 2 --multi_gpu --mixed_precision ""fp16"" \
  tutorial_train_plus.py \
  --pretrained_model_name_or_path=""stable-diffusion-v1-5/"" \
  --image_encoder_path=""models/image_encoder/"" \
  --data_json_file=""assets/prompt_image.json"" \
  --data_root_path=""assets/train/"" \
  --mixed_precision=""fp16"" \
  --resolution=512 \
  --train_batch_size=2 \
  --dataloader_num_workers=4 \
  --learning_rate=1e-04 \
  --weight_decay=0.01 \
  --output_dir=""out_model/"" \
  --save_steps=3

During training, there is the message but the training can be continued:
Removed shared tensor {'adapter_modules.27.to_k_ip.weight', 'adapter_modules.1.to_v_ip.weight', 'adapter_modules.31.to_k_ip.weight', 'adapter_modules.15.to_k_ip.weight', 'adapter_modules.31.to_v_ip.weight', 'adapter_modules.11.to_k_ip.weight', 'adapter_modules.23.to_k_ip.weight', 'adapter_modules.3.to_k_ip.weight', 'adapter_modules.25.to_v_ip.weight', 'adapter_modules.21.to_k_ip.weight', 'adapter_modules.17.to_v_ip.weight', 'adapter_modules.13.to_k_ip.weight', 'adapter_modules.17.to_k_ip.weight', 'adapter_modules.19.to_v_ip.weight', 'adapter_modules.13.to_v_ip.weight', 'adapter_modules.7.to_v_ip.weight', 'adapter_modules.7.to_k_ip.weight', 'adapter_modules.29.to_k_ip.weight', 'adapter_modules.3.to_v_ip.weight', 'adapter_modules.5.to_v_ip.weight', 'adapter_modules.21.to_v_ip.weight', 'adapter_modules.5.to_k_ip.weight', 'adapter_modules.23.to_v_ip.weight', 'adapter_modules.25.to_k_ip.weight', 'adapter_modules.1.to_k_ip.weight', 'adapter_modules.9.to_v_ip.weight', 'adapter_modules.9.to_k_ip.weight', 'adapter_modules.15.to_v_ip.weight', 'adapter_modules.27.to_v_ip.weight', 'adapter_modules.29.to_v_ip.weight', 'adapter_modules.19.to_k_ip.weight', 'adapter_modules.11.to_v_ip.weight'} while saving. This should be OK, but check by verifying that you don't receive anywarning while reloading

After training is finished and convert the weight to generate ip_adapter.bin, then run the inference code ip_adapter-plus_demo.py with the following model paths in this file:
base_model_path = ""SG161222/Realistic_Vision_V4.0_noVAE""
vae_model_path = ""stabilityai/sd-vae-ft-mse""
image_encoder_path = ""models/image_encoder""
ip_ckpt = ""out_model/demo_plus_checkpoint/ip_adapter.bin""

It shows the error:
raise RuntimeError('Error(s) in loading state_dict for {}:\n\t{}'.format(
RuntimeError: Error(s) in loading state_dict for ModuleList:
        Missing key(s) in state_dict: ""1.to_k_ip.weight"", ""1.to_v_ip.weight"", ""3.to_k_ip.weight"", ""3.to_v_ip.weight"", ""5.to_k_ip.weight"", ""5.to_v_ip.weight"", ""7.to_k_ip.weight"", ""7.to_v_ip.weight"", ""9.to_k_ip.weight"", ""9.to_v_ip.weight"", ""11.to_k_ip.weight"", ""11.to_v_ip.weight"", ""13.to_k_ip.weight"", ""13.to_v_ip.weight"", ""15.to_k_ip.weight"", ""15.to_v_ip.weight"", ""17.to_k_ip.weight"", ""17.to_v_ip.weight"", ""19.to_k_ip.weight"", ""19.to_v_ip.weight"", ""21.to_k_ip.weight"", ""21.to_v_ip.weight"", ""23.to_k_ip.weight"", ""23.to_v_ip.weight"", ""25.to_k_ip.weight"", ""25.to_v_ip.weight"", ""27.to_k_ip.weight"", ""27.to_v_ip.weight"", ""29.to_k_ip.weight"", ""29.to_v_ip.weight"", ""31.to_k_ip.weight"", ""31.to_v_ip.weight"".

Any step wrong to cause this error?
","The model can be trained and inferenced successfully now:
Set safe_serialization to False in model training file tutorial_train_plus.py:
accelerator.save_state(save_path, safe_serialization=False)

It will generate pytorch_model.bin instead of model.safetensors during training.
Once training is complete, modify the model conversion code as below based on the original instructions in readme:
ckpt = ""pytorch_model.bin"" # set correct path
sd = torch.load(ckpt)

Model file ip_adapter.bin will be generated for inference.
",pytorch
gridsearchcv not choosing the best hyperparameters for xgboost,"I am developing a regression model with xgboost. Since xgboost has multiple hyperparameters, I have added the cross validation logic with GridSearchCV(). As a trial, I set max_depth: [2,3]. My python code is as below.
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import make_scorer
from sklearn.metrics import mean_squared_error
​
xgb_reg = xgb.XGBRegressor()
​
# Obtain the best hyper parameter
scorer=make_scorer(mean_squared_error, False)
params = {'max_depth': [2,3], 
          'eta': [0.1], 
          'colsample_bytree': [1.0],
          'colsample_bylevel': [0.3],
          'subsample': [0.9],
          'gamma': [0],
          'lambda': [1],
          'alpha':[0],
          'min_child_weight':[1]
         }
grid_xgb_reg=GridSearchCV(xgb_reg,
                          param_grid=params,
                          scoring=scorer,
                          cv=5,
                          n_jobs=-1)
​
grid_xgb_reg.fit(X_train, y_train)
y_pred = grid_xgb_reg.predict(X_test)
y_train_pred = grid_xgb_reg.predict(X_train)

## Evaluate model
from sklearn.metrics import mean_squared_error
from sklearn.metrics import r2_score
​
print('RMSE  train: %.3f,  test: %.3f' %(np.sqrt(mean_squared_error(y_train, y_train_pred)),np.sqrt(mean_squared_error(y_test, y_pred))))
print('R^2   train: %.3f,  test: %.3f' %(r2_score(y_train, y_train_pred),r2_score(y_test, y_pred)))

The problem is the GridSearchCV does not seem to choose the best hyperparameters. In my case, when I set max_depth as [2,3], The result is as follows. In the following case, GridSearchCV chose max_depth:2 as the best hyper params.
#  The result when max_depth is 2
RMSE  train: 11.861,  test: 15.113
R^2   train: 0.817,  test: 0.601

However, if I updated max_depth to [3](by getting rid of 2), the test score is better than the previous value as follows.
#  The result when max_depth is 3
RMSE  train: 9.951,  test: 14.752
R^2   train: 0.871,  test: 0.620

Question
My understanding is that even if I set max_depth as [2,3], the GridSearchCV method SHOULD choose the max_depth:3 as the best hyperparameters since max_depth:3 can return the better score in terms of RSME or R^2 than max_depth:2. Could anyone tell me why my code cannot choose the best hyperparameters when I set max_depth as [2,3]?
","If you run a second experiment with max_depth:2, then the results are not comparable to the first experiment with max_depth:[2,3] even for the run with max_depth:2, since there are sources of randomness in your code which you do not explicitly control, i.e. your code is not reproducible.
The first source of randomness is the CV folds; in order to ensure that the experiments will be run on identical splits of the data, you should define your GridSearchCV as follows:
from sklearn.model_selection import KFold

seed_cv = 123 # any random value here

kf = KFold(n_splits=5, random_state=seed_cv)

grid_xgb_reg=GridSearchCV(xgb_reg,
                          param_grid=params,
                          scoring=scorer,
                          cv=kf,   # <- change here
                          n_jobs=-1)

The second source of randomness is the XGBRegressor itself, which also includes a random_state argument (see the docs); you should change it to:
seed_xgb = 456 # any random value here (can even be the same with seed_cv)
xgb_reg = xgb.XGBRegressor(random_state=seed_xgb)

But even with these arrangements, while your data splits will now be identical, the regression models built will not be necessarily so in the general case; here, if you keep the experiments like that, i.e. first with  max_depth:[2,3] and then with max_depth:2, the results will be identical indeed; but if you change it to, say,  first with  max_depth:[2,3] and then with max_depth:3, they will not, since in the first experiment, the run with max_depth:3 will start with a different state of the random number generator (i.e. the one after the run with max_depth:2 has finished).
There are limits to how identical you can make different runs under such conditions; for an example of a very subtle difference that nevertheless destroys the exact reproducibility between two experiments, see my answer in Why does the importance parameter influence performance of Random Forest in R?
",scikit-learn
why do i run out of memory when training with a large dataset but have no problems with a small dataset,"I'm trying to build a keypoint detection system using Keras. I've got a UNet like model, with a series of convolutions, batch normalization, and max pooling, followed by a symmetric series of up sampling, convolution, and batch normalization layers (and skip connections). When given 100 instances, I'm able to call model.fit() without a problem. However, if I leave the model the same but use 500 instances, Keras crashes with an OOM exception. Why does this happen, and is there anything I can do to fix it?
Here's (what I think is) the relevant part of the code where I call model.fit:
model = build_model(
    filters=50,
    filter_step=1,
    stages=5,
    stage_steps=1,
    initial_convolutions=0,
    stacks=1,
)

print(model.summary()) 

dataset = tf.data.Dataset.from_tensor_slices((X, y))
dataset = dataset.batch(1)

model.fit(
    dataset,
    epochs=2**7,
    callbacks=[
        EarlyStopping(monitor=""loss"", patience=5, min_delta=1e-7, start_from_epoch=10),
        LearningRateScheduler(step_decay)
    ],
)

X and y are Numpy arrays with the following shapes:

X: (100, 1024, 1024, 3)
y: (100, 1024, 1024)

100 here is the data set size. If I increase this to 500 (or more), I get the out-of-memory exception. It appears to me that Keras is perhaps trying to load the entire data set into memory, despite using from_tensor_slices and batch(1), so I'm clearly misunderstanding something.
","
When you use tf.data.Dataset.from_tensor_slices((X, y)), TensorFlow
attempts to create a dataset where each element is a pair (X[i], y[i]). If the dataset is too large, this can consume a significant
amount of memory, especially if X and y are large.

To address this memory issue, we can modify the data loading process using a generator to load the data in batches, during runtime.
You'll have to define a generator that yields batches of data (X_batch, y_batch) and then to create the dataset use:
tf.data.Dataset.from_generator
Full documentation here.
And an example could look like this:
import numpy as np
import tensorflow as tf

# Assume X and y are your data
X = np.random.rand(500, 1024, 1024, 3)
y = np.random.rand(500, 1024, 1024)

# Define a generator to yield batches of data
def data_generator(X, y, batch_size):
    num_samples = X.shape[0]
    for i in range(0, num_samples, batch_size):
        yield X[i:i+batch_size], y[i:i+batch_size]

# Parameters
batch_size = 16

# Create a generator
generator = data_generator(X, y, batch_size)

# Create a tf.data.Dataset using the generator
dataset = tf.data.Dataset.from_generator(
    lambda: generator,
    output_signature=(
        tf.TensorSpec(shape=(None, 1024, 1024, 3), dtype=tf.float32),
        tf.TensorSpec(shape=(None, 1024, 1024), dtype=tf.float32)
    )
)

# Model and training code would go here...

",tensorflow
how to apply labelencoder to a polars dataframe column,"I'm trying to use scikit-learn's LabelEncoder with a Polars DataFrame to encode a categorical column. I am using the following code.
import polars as pl

from sklearn.preprocessing import LabelEncoder

df = pl.DataFrame({
    ""Color"" : [""red"",""white"",""blue""]
})

enc = LabelEncoder()

However, an error is raised.

ValueError: y should be a 1d array, got an array of shape () instead.

Next, I tried converting the column to a NumPy.
df.with_columns(
    enc.fit_transform(pl.col(""Color"").to_numpy()) 
)

Now, a different error is raised.

AttributeError: 'Expr' object has no attribute 'to_numpy'

Note. I found that .cast(pl.Categorical).to_physical() could be used to obtain the desired result. Still, I'd prefer using something like transform() on my test dataset.
df.with_columns(
    pl.col(""Color"").cast(pl.Categorical).to_physical().alias(""Color_encoded"")
)

","For such a call to an external API taking an entire sequence of values, such as enc.fit_transform, pl.Expr.map_batches could be used.
df.with_columns(
    pl.col(""Color"").map_batches(enc.fit_transform)
)

shape: (3, 1)
┌───────┐
│ Color │
│ ---   │
│ i64   │
╞═══════╡
│ 1     │
│ 2     │
│ 0     │
└───────┘

Note. It would be nice if enc.set_output(""polars"") (as outlined in this answer) was available for the LabelEncoder. However, this is not implemented.

You already shared an approach to label encoding a column using polars' native expression API. A cleaner way could rely on dense ranking as follows.
df.with_columns(
    pl.col(""Color"").rank(""dense"") - 1
)

Subtraction is used only to obtain an output with lowest label being 0.
",scikit-learn
how to efficiently implement forward fill in pytorch,"How can I efficiently implement the fill forward logic (inspired for pandas ffill) for a vector shaped NxLxC (batch, sequence dimension, channel). Because each channel sequence is independent this can be equivalent to working with a tensor shaped (N*C)xL.
The computation should keep the torch variable so that the actual output is differentiable.
I managed to make something with advanced indexing, but it is L**2 in the memory and number of operations, so not very great and gpu friendly.

Example:
Assuming you have the sequence [0,1,2,0,0,3,0,4,0,0,0,5,6,0] in a tensor shaped 1x14 the fill forward will give you the sequence [0,1,2,2,2,3,3,4,4,4,4,5,6,6].
An other example shaped 2x4 is [[0, 1, 0, 3], [1, 2, 0, 3]] which should be forward filled into [[0, 1, 1, 3], [1, 2, 2, 3]].

Method used today:
We use the following code that is highly unoptimized but still faster than non vectorized loops:
def last_zero_sequence_start_indices(t: torch.Tensor) -> torch.Tensor:
    """"""
    Given a 3D tensor `t`, this function returns a two-dimensional tensor where each entry represents
    the starting index of the last contiguous sequence of zeros up to and including the current index.
    If there's no zero at the current position, the value is the tensor's length.

    In essence, for each position in `t`, the function pinpoints the beginning of the last contiguous
    sequence of zeros up to that position.

    Args:
    - t (torch.Tensor): Input tensor with shape [Batch, Channel, Time].

    Returns:
    - torch.Tensor: Three-dimensional tensor with shape [Batch, Channel, Time] indicating the starting position of
        the last sequence of zeros up to each index in `t`.
    """"""

    # Create a mask indicating the start of each zero sequence
    start_of_zero_sequence = (t == 0) & torch.cat([
        torch.full(t.shape[:-1] + (1,), True, device=t.device),
        t[..., :-1] != 0,
    ], dim=2)

    # Duplicate this mask into a TxT matrix
    duplicated_mask = start_of_zero_sequence.unsqueeze(2).repeat(1, 1, t.size(-1), 1)

    # Extract the lower triangular part of this matrix (including the diagonal)
    lower_triangular = torch.tril(duplicated_mask)

    # For each row, identify the index of the rightmost '1' (start of the last zero sequence up to that row)
    indices = t.size(-1) - 1 - lower_triangular.int().flip(dims=[3]).argmax(dim=3)

    return indices

","Here is an approach to this problem, without creating TxT matrix:
import torch
def forward_fill(t: torch.Tensor) -> torch.Tensor:
    n_dim, t_dim = t.shape
    # Generate indices range
    rng = torch.arange(t_dim)
    
    rng_2d = rng.unsqueeze(0).repeat(n_dim, 1)
    # Replace indices to zero for elements that equal zero
    rng_2d[t == 0] = 0
    
    # Forward fill of indices range so all zero elements will be replaced with previous non-zero index.
    idx = rng_2d.cummax(1).values
    t = t[torch.arange(n_dim)[:, None], idx]
    return t

Note that this is a solution for 2D input but can be easily modified for more dimensions.
",pytorch
should i first train_test_split and then use cross validation,"If I plan to use cross validation (KFold), should I still split the dataset into training and test data and perform my training (including cross valid) only on the training set? Or will CV do everything for me? E.g.
Option 1
X_train, X_test, y_train, y_test = train_test_split(X,y)
clf = GridSearchCV(... cv=5) 
clf.fit(X_train, y_train)

Option 2
clf = GridSearchCV(... cv=5) 
clf.fit(X y)

","CV is good, but it's better to have train/test split to provide independent score estimation on the untouched data.
If your CV and test data shows about the same score, then you can drop train/test split phase and CV on whole data to achieve slightly better model score. But don't do it before you sure your split and CV score is consistent.
",cross-validation
openai api error when using a finetuned model quotthe model xxxxx does not existquot,"I'm trying to do a chatbot with a fine tuned model.
I'm doing my request like this:
    const API_KEY = ""/"";
    const ORG_ID = ""/"";
    const headers = {
      ""Content-Type"": ""application/json"",
      Authorization: ""Bearer "" + API_KEY,
      ""OpenAI-Organization"": ORG_ID,
    };
    const res = await axios.post(
      ""https://api.openai.com/v1/chat/completions"",
      {
        model: ""ft-modelname"",
        messages: [
          {
            role: ""system"",
            content: ""your name is Name."",
          },
          {
            role: ""user"",
            content: message,
          },
        ],
      },
      { headers }
    );

And after listing my models with the API I can see it, it does exist.
But I have this error in the console:
{
    ""error"": {
        ""message"": ""The model `ft-modelname` does not exist"",
        ""type"": ""invalid_request_error"",
        ""param"": null,
        ""code"": null
    }
}

After looking up, i can't find a way to do this, some people use open ai engine api instead of completion, but it don't work too.
Do you have any ideas ?
Thank you.
Edit:
It works when I'm not using a fine tune model
","UPDATE: 22 August 2023
Fine-tuning for GPT-3.5 is now available, as stated in the official OpenAI blog:

Fine-tuning for GPT-3.5 Turbo is now available, with fine-tuning for
GPT-4 coming this fall.

Consequently, the Chat Completions API (i.e., the GPT-3.5 API) endpoint can be used for a fine-tuned model, but only for a GPT-3.5 fine-tuned model. For a GPT-3 fine-tuned model, use the Completions API (i.e., the GPT-3 API) endpoint.
As stated in the official OpenAI documentation:

When a job has succeeded, you will see the fine_tuned_model field
populated with the name of the model when you retrieve the job
details. You may now specify this model as a parameter to in the Chat
Completions API (for gpt-3.5-turbo) or legacy Completions API (for
babbage-002 and davinci-002), and make requests to it using the
Playground.


Problem
You're using the wrong API endpoint.
Solution
Use the Completions API (i.e., the GPT-3 API) endpoint instead of the Chat Completions API (i.e., the GPT-3.5 API) endpoint.
As stated in the official OpenAI documentation:

When a job has succeeded, the fine_tuned_model field will be populated
with the name of the model. You may now specify this model as a
parameter to our Completions API, and make requests to it using the
Playground.

In general
Python
import os
import openai

openai.api_key = os.getenv(""OPENAI_API_KEY"")

openai.Completion.create(
    model = FINE_TUNED_MODEL,
    prompt = YOUR_PROMPT)

NodeJS
const { Configuration, OpenAIApi } = require(""openai"");

const configuration = new Configuration({
  apiKey: process.env.OPENAI_API_KEY,
});

const openai = new OpenAIApi(configuration);

const response = await openai.createCompletion({
  model: FINE_TUNED_MODEL
  prompt: YOUR_PROMPT,
});

cURL
curl https://api.openai.com/v1/completions \
  -H ""Authorization: Bearer $OPENAI_API_KEY"" \
  -H ""Content-Type: application/json"" \
  -d '{""prompt"": YOUR_PROMPT, ""model"": FINE_TUNED_MODEL}'

OpenAI CLI
openai api completions.create -m <FINE_TUNED_MODEL> -p <YOUR_PROMPT>

Your case
Try this:
const res = await axios.post(
  ""https://api.openai.com/v1/completions"", {
    model: ""ft-modelname"",
    prompt: ""Say this is a test"",
  }, {
    headers
  }
);

",fine-tune
image not segmenting properly using dbscan,"I am trying to use DBSCAN from scikitlearn to segment an image based on color.  The results I'm getting are . As you can see there are 3 clusters.  My goal is to separate the buoys in the picture into different clusters. But obviously they are showing up as the same cluster. I've tried a wide range of eps values and min_samples but those two things always cluster together. My code is:
img= cv2.imread(""buoy1.jpg) 
labimg = cv2.cvtColor(img, cv2.COLOR_BGR2LAB)

n = 0
while(n<4):
    labimg = cv2.pyrDown(labimg)
    n = n+1

feature_image=np.reshape(labimg, [-1, 3])
rows, cols, chs = labimg.shape

db = DBSCAN(eps=5, min_samples=50, metric = 'euclidean',algorithm ='auto')
db.fit(feature_image)
labels = db.labels_

plt.figure(2)
plt.subplot(2, 1, 1)
plt.imshow(img)
plt.axis('off')
plt.subplot(2, 1, 2)
plt.imshow(np.reshape(labels, [rows, cols]))
plt.axis('off')
plt.show()

I assume this is taking the euclidean distance and since its in lab space euclidean distance would be different between different colors. If anyone can give me guidance on this I'd really appreciate it.
Update: 
The below answer works. Since DBSCAN requires an array with no more then 2 dimensions I concatenated the columns to the original image and reshaped to produce a n x 5 matrix where n is the x dimension times the y dimension.  This seems to work for me.
indices = np.dstack(np.indices(img.shape[:2]))
xycolors = np.concatenate((img, indices), axis=-1) 
np.reshape(xycolors, [-1,5])

","You need to use both color and position.
Right now, you are using colors only.
",scikit-learn
is there any downside in using multiple quotn_jobs1quot statements,"In the context of model selection for a classification problem, while running cross validation, is it ok to specify n_jobs=-1 both in model specification and cross validation function in order to take full advantage of the power of the machine?
For example, comparing sklearn RandomForestClassifier and xgboost XGBClassifier:
RF_model = RandomForestClassifier( ..., n_jobs=-1)
XGB_model = XGBClassifier( ..., n_jobs=-1)

RF_cv = cross_validate(RF_model, ..., n_jobs=-1)
XGB_cv = cross_validate(XGB_model, ..., n_jobs=-1)

is it ok to specify the parameters in both? Or should I specify it only once? And in which of them, model or cross validation statement?
I used for the example models from two different libraries (sklearn and xgboost) because maybe there is a difference in how it works, also cross_validate function is from sklearn.
","Specifying n_jobs twice does have an effect, though whether it has a positive or negative effect is complicated.
When you specify n_jobs twice, you get two levels of parallelism. Imagine you have N cores. The cross-validation function creates N copies of your model. Each model creates N threads to run fitting and predictions. You then have N*N threads.
This can blow up pretty spectacularly. I once worked on a program which needed to apply ARIMA to tens of thousands of time-series. Since each ARIMA is independent, I parallelized it and ran one ARIMA on each core of a 12-core CPU. I ran this, and it performed very poorly. I opened up htop, and was surprised to find 144 threads running. It turned out that this library, pmdarima, internally parallelized ARIMA operations. (It doesn't parallelize them well, but it does try.) I got a massive speedup just by turning off this inner layer of parallelism. Having two levels of parallelism is not necessarily better than having one.
In your specific case, I benchmarked a random forest with cross validation, and I benchmarked four configurations:

No parallelism
Parallelize across different CV folds, but no model parallelism
Parallelize within the model, but not on CV folds
Do both


(Error bars represent 95% confidence interval. All tests used RandomForestClassifier. Test was performed using cv=5, 100K samples, and 100 trees. Test system had 4 cores with SMT disabled. Scores are mean duration of 7 runs.)
This graph shows that no parallelism is the slowest, CV parallelism is third fastest, and model parallelism and combined parallelism are tied for first place.
However, this is closely tied to what classifiers I'm using - a benchmark for pmdarima, for example, would find that cross-val parallelism is faster than model parallelism or combined parallelism. If you don't know which one is faster, then test it.
",cross-validation
pytorch 040 there are three ways to create tensors on cuda device is there some difference between them,"I failed in the third way. t3 is still on CPU. No idea why.
a = np.random.randn(1, 1, 2, 3)

t1 = torch.tensor(a)
t1 = t3.to(torch.device('cuda'))

t2 = torch.tensor(a)
t2 = t2.cuda() 

t3 = torch.tensor(a, device=torch.device('cuda'))

","All three methods worked for me. 
In 1 and 2, you create a tensor on CPU and then move it to GPU when you use .to(device) or .cuda(). They are the same here.
However, when you use .to(device) method you can explicitly tell torch to move to specific GPU by setting device=torch.device(""cuda:<id>""). with .cuda() you have to do .cuda(<id>) to move to some particular GPU. 

Why do these two methods exist then?
.to(device) was introduced in 0.4 because it is easier to declare device variable at top of the code as 
device = torch.device(""cuda"") if torch.cuda.is_available() else torch.device(""cpu"")
and use .to(device) everywhere. This makes it quite easy to switch from CPU to GPU and vice-versa
Before this, we had to use .cuda() and your code will have if check for cuda.is_available() everywhere which made it cumbersome to switch between GPU/CPU.

The third method doesn't create a tensor on the CPU and directly copies data to GPU, which is more efficient. 
",pytorch
how to perform stratifiedgroupkfold based on id that should not be part of training,"I am trying to perform logistic regression using StratifiedGroupKFold as shown in the following code.
grid={'C':np.logspace(-3,3,7)}
grkf_cv = StratifiedGroupKFold(n_splits=10)
id_ls = X_train_df['ID'].to_list()  

log_reg = LogisticRegression(max_iter=100, random_state=42)
logreg_cv = GridSearchCV(log_reg, grid, cv=grkf_cv, scoring='roc_auc')
logreg_cv.fit(X_train_df, y_train_df, groups=id_ls)

This causes a conflict as the model is training with the group ID which is incorrect and it appears as a feature. My issue is I need to pass id_ls with X_train_df (which contains the ID). I am not sure how splits would be performed if X_train_df did not contain the ID.
I can drop the ID from X_train_df and then train but I do not think the splits would be performed based on groups.
Is there a way around this problem.
","In the example in sklearn documentation (found here), you can see that they define the groups parameter separately, without it ever being a part of the training dataset.
I am assuming this is because the groups parameter does not have to be checked against a column, as it already contains the group label for each sample in order.
It makes sense, the function knows that the first row of X_train has group id the first element of id_ls (which you are passing at the groups parameter), the second row is matched to the second element of the list etc.
",cross-validation
sklearnmetricsaccuracy_score is very slow,"I need to measure accuracy of my model's prediction for binary classification (0 and 1 outputs). I am testing my model with many different values of threshold, and my testing dataset is quite big (50-100 million of examples), so I need a fast way to compute model's accuracy.
I was optimizing my code and noticed that the standard function for computing accuracy is ~50 times slower than the direct computation.
Minimal example:
from sklearn.metrics import accuracy_score
import numpy as np
import timeit
a=np.random.randint(0,2,1000000)
b=np.random.randint(0,2,1000000)
%timeit accuracy_score(a,b)
# 46.7 ms ± 390 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)

%timeit (a==b).sum()/a.size
# 713 µs ± 7.22 µs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)

Am I missing something? It looks like accuracy_score is a standard way to measure accuracy. Why is it so slow? No C optimization under the hood?
","There are two reasons why this happens. Part of the answer is that accuracy_score() is doing more validation to assure that the answer it's computing makes sense. Another part, however, is that some of this validation doesn't seem to have been implemented as efficiently as it could have been.
To explain why I think this, I'm going to show some profiling data. I obtained this data using line profiler, and the following Jupyter command:
from sklearn.metrics import accuracy_score
from sklearn.metrics._classification import _check_targets
from sklearn.utils.multiclass import type_of_target
%load_ext line_profiler
%lprun -f accuracy_score.__wrapped__ -f _check_targets -f type_of_target [accuracy_score(a,b) for i in range(10)]

Some notes about this:

The -f option controls which functions are getting traced. I tracing accuracy_score.__wrapped__ because accuracy_score() has a decorator. The other functions are things that accuracy_score() calls; we'll get into that.
[accuracy_score(a,b) for i in range(10)] is the code we're running.

Here's the result of this for accuracy_score:
Timer unit: 1e-09 s

Total time: 0.813048 s
File: /home/jupyter-njodell/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py
Function: accuracy_score at line 137

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
   146                                           def accuracy_score(y_true, y_pred, *, normalize=True, sample_weight=None):
   ...
   211                                           
   212                                               # Compute accuracy for each possible representation
   213        10  791633889.0    8e+07     97.4      y_type, y_true, y_pred = _check_targets(y_true, y_pred)
   214        10     856305.0  85630.5      0.1      check_consistent_length(y_true, y_pred, sample_weight)
   215        10      11260.0   1126.0      0.0      if y_type.startswith(""multilabel""):
   216                                                   differing_labels = count_nonzero(y_true - y_pred, axis=1)
   217                                                   score = differing_labels == 0
   218                                               else:
   219        10   14279630.0    1e+06      1.8          score = y_true == y_pred
   220                                           
   221        10    6267160.0 626716.0      0.8      return _weighted_sum(score, sample_weight, normalize)

Notice that 97% of its time is spent inside _check_targets(). Let's trace that function:
Timer unit: 1e-09 s

Total time: 0.791422 s
File: /home/jupyter-njodell/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py
Function: _check_targets at line 58

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
    58                                           def _check_targets(y_true, y_pred):
    59                                               """"""Check that y_true and y_pred belong to the same classification task.
    60                                           
    61                                               This converts multiclass or binary types to a common shape, and raises a
    62                                               ValueError for a mix of multilabel and multiclass targets, a mix of
    63                                               multilabel formats, for the presence of continuous-valued or multioutput
    64                                               targets, or for targets of different lengths.
    65                                           
    66                                               Column vectors are squeezed to 1d, while multilabel formats are returned
    67                                               as CSR sparse label indicators.
    ...
    84                                               """"""
    85        10     586514.0  58651.4      0.1      check_consistent_length(y_true, y_pred)
    86        10  192463822.0    2e+07     24.3      type_true = type_of_target(y_true, input_name=""y_true"")
    87        10  172566835.0    2e+07     21.8      type_pred = type_of_target(y_pred, input_name=""y_pred"")
    88                                           
    89        10      11888.0   1188.8      0.0      y_type = {type_true, type_pred}
    90        10      15326.0   1532.6      0.0      if y_type == {""binary"", ""multiclass""}:
    91                                                   y_type = {""multiclass""}
    92                                           
    93        10      11207.0   1120.7      0.0      if len(y_type) > 1:
    94                                                   raise ValueError(
    95                                                       ""Classification metrics can't handle a mix of {0} and {1} targets"".format(
    96                                                           type_true, type_pred
    97                                                       )
    98                                                   )
    99                                           
   100                                               # We can't have more than one value on y_type => The set is no more needed
   101        10      18352.0   1835.2      0.0      y_type = y_type.pop()
   102                                           
   103                                               # No metrics support ""multiclass-multioutput"" format
   104        10       7047.0    704.7      0.0      if y_type not in [""binary"", ""multiclass"", ""multilabel-indicator""]:
   105                                                   raise ValueError(""{0} is not supported"".format(y_type))
   106                                           
   107        10       4171.0    417.1      0.0      if y_type in [""binary"", ""multiclass""]:
   108        10      91096.0   9109.6      0.0          xp, _ = get_namespace(y_true, y_pred)
   109        10     930291.0  93029.1      0.1          y_true = column_or_1d(y_true)
   110        10     498877.0  49887.7      0.1          y_pred = column_or_1d(y_pred)
   111        10       4883.0    488.3      0.0          if y_type == ""binary"":
   112        10       2488.0    248.8      0.0              try:
   113        10  424147403.0    4e+07     53.6                  unique_values = _union1d(y_true, y_pred, xp)
   114                                                       except TypeError as e:
   115                                                           # We expect y_true and y_pred to be of the same data type.
   116                                                           # If `y_true` was provided to the classifier as strings,
   117                                                           # `y_pred` given by the classifier will also be encoded with
   118                                                           # strings. So we raise a meaningful error
   119                                                           raise TypeError(
   120                                                               ""Labels in y_true and y_pred should be of the same type. ""
   121                                                               f""Got y_true={xp.unique(y_true)} and ""
   122                                                               f""y_pred={xp.unique(y_pred)}. Make sure that the ""
   123                                                               ""predictions provided by the classifier coincides with ""
   124                                                               ""the true labels.""
   125                                                           ) from e
   126        10      24858.0   2485.8      0.0              if unique_values.shape[0] > 2:
   127                                                           y_type = ""multiclass""
   128                                           
   129        10      31701.0   3170.1      0.0      if y_type.startswith(""multilabel""):
   130                                                   y_true = csr_matrix(y_true)
   131                                                   y_pred = csr_matrix(y_pred)
   132                                                   y_type = ""multilabel-indicator""
   133                                           
   134        10       4790.0    479.0      0.0      return y_type, y_true, y_pred

There are two major uses of time in this function:

type_of_target() is called for both y_true and y_pred. This is 24.3% + 21.8% = 46.1% of time used.
_union1d() is called to get the number of distinct classes across both y_true and y_pred. This is 53.6% of time used.

What are both of these functions doing?
In type_of_target(), most time is spent on this line:
Total time: 0.364502 s
File: /home/jupyter-njodell/.local/lib/python3.10/site-packages/sklearn/utils/multiclass.py
Function: type_of_target at line 228

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
   228                                           def type_of_target(y, input_name=""""):
   ...
   [... snip many lines of code ...]
   ...
   395        20  361088394.0    2e+07     99.1      if xp.unique_values(y).shape[0] > 2 or (y.ndim == 2 and len(first_row) > 1):
   396                                                   # [1, 2, 3] or [[1., 2., 3]] or [[1, 2]]
   397                                                   return ""multiclass"" + suffix

_union1d is taking the union of two NumPy arrays. It calls np.union1d, which is implemented like so:
def union1d(ar1, ar2):
    return unique(np.concatenate((ar1, ar2), axis=None))

In summary, for binary classification, accuracy_score spends 96% of its compute time computing one of three things:

np.unique(y_true)
np.unique(y_pred)
np.unique(np.concatenate([y_true, y_pred]))

However, the third thing can be computed from the first two.
Specifically, these three operations can be re-written as:
y_true_unique = np.unique(y_true)
y_pred_unique = np.unique(y_pred)
combined_unqiue = np.unique(np.concatenate([y_true_unique, y_pred_unique]))

In the case where the number of distinct classes is much smaller than the number of training examples (which I would think would apply to most machine learning problems) the second form would be about 50% faster, as np.concatenate([y_true_unique, y_pred_unique]) would be much smaller than np.concatenate([y_true, y_pred]).
Actually making this optimization and submitting a scikit-learn PR is left as an exercise to the reader. :)
",scikit-learn
sklearn importing standard scaler,"I imported standardscaler for a code:
from sklearn.preprocessing import standardscaler

It shows like
cannot import name 'standardscaler' from 'sklearn.preprocessing'

How to solve the error?
","Names in Python are case-sensitive.
Try:
from sklearn.preprocessing import StandardScaler

",scikit-learn
behavior of model saving when earlystopping is not triggered in keras,"I'm currently fine-tuning a network for a few epochs using the following code:
es= keras.callbacks.EarlyStopping(monitor='loss', min_delta=1e-4,patience=10, verbose=True, restore_best_weights=True)
history = model.fit(ft, steps_per_epoch=len(ft), callbacks=[es], epochs=150, verbose=verbose)
model.save('model_best.h5')

I understand that if there is no improvement in the loss (monitored by monitor='loss') for 10 epochs (determined by patience=10), the EarlyStopping callback is activated, and the model weights restored to the best encountered weights are saved.
My question is about the scenario where EarlyStopping does not trigger—say, if there isn't a sequence of 10 epochs without improvement, but the best performing epoch is not the last one. In this case, which weights are saved? Are they the weights from the last epoch or from the best epoch observed during training? Notably, when verbose=True, the EarlyStopping callback does not produce any output if it doesn't trigger, leaving me unsure about which model version is preserved. Can someone clarify what happens in this situation?
","By default, the model does not automatically save the weights of the best-
performing epoch. You need to explicitly configure this behavior using a
callback like modelcheckpoint in libraries like keras or TensorFlow.
I am attaching gist file for your reference
",tensorflow
check tpu workloadutilization,"I am training a model, and when I open the TPU in the Google Cloud Platform console, it shows me the CPU utilization (on the TPU, I suppose). It is really, really, low (like 0.07%), so maybe it is the VM CPU?  I am wondering whether the training is really proper or if the TPUs are just that strong.
Is there any other way to check the TPU usage? Maybe with a ctpu command?
","I would recommend using the TPU profiling tools that plug into TensorBoard. A good tutorial for install and use of these tools can be found here.
You'll run the profiler while your TPU is training. It will add an extra tab to your TensorBoard with TPU-specific profiling information. Among the most useful:

Average step time
Host idle time (how much time the CPU spends idling)
TPU idle time
Utilization of TPU Matrix units

Based on these metrics, the profiler will suggest ways to start optimizing your model to train well on a TPU. You can also dig into the more sophisticated profiling tools like a trace viewer, or a list of the most expensive graph operations.
For some guidelines on performance tuning (in addition to those ch_mike already linked) you can look at the TPU performance guide.
",tensorflow
multiple metrics for neural network model with cross validation,"I am trying to get F1, precision and recall of cross validation for an LSTM model.
I know how to show the accuracies, but when I try to show the other metrics using cross_validate I get many different errors.
My code is the following:
def nn_model():
    model_lstm1 = Sequential()
    model_lstm1.add(Embedding(20000, 100, input_length=49))
    model_lstm1.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))
    model_lstm1.add(Dense(2, activation='sigmoid'))
    model_lstm1.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
    return model_lstm1

classifier = KerasClassifier(build_fn=nn_model, batch_size=10,nb_epoch=10)

scoring = {'precision' : make_scorer(precision_score),
           'recall' : make_scorer(recall_score), 
           'f1_score' : make_scorer(f1_score)}

results = cross_validate(classifier, X_train, y_train, cv=skf, scoring = scoring)

print(""F1 score SVM: %0.2f (+/- %0.2f)"" % (np.mean(results[f1_score]), np.std(results[f1_score])))

print(""precision score SVM: %0.2f (+/- %0.2f)"" % (np.mean(results[precision]), np.std(results[precision])))
print(""recall macro SVM: %0.2f (+/- %0.2f)"" % (np.mean(results[recall]), np.std(results[recall])))


The error I get is the following:
Epoch 1/1 1086/1086 [==============================] - 18s 17ms/step - loss: 0.6014 - acc: 0.7035
--------------------------------------------------------------------------- ValueError                                Traceback (most recent call last) <ipython-input-40-5afe62c11676> in <module>
      6            'f1_score' : make_scorer(f1_score)}
      7 
----> 8 results = cross_validate(classifier, X_train, y_train, cv=skf, scoring = scoring)
      9 
     10 print(""F1 score SVM: %0.2f (+/- %0.2f)"" % (np.mean(results[f1_score]), np.std(results[f1_score])))

/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/model_selection/_validation.py in cross_validate(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, return_train_score, return_estimator, error_score)
    229             return_times=True, return_estimator=return_estimator,
    230             error_score=error_score)
--> 231         for train, test in cv.split(X, y, groups))
    232 
    233     zipped_scores = list(zip(*scores))

/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/joblib/parallel.py in __call__(self, iterable)
    919             # remaining jobs.
    920             self._iterating = False
--> 921             if self.dispatch_one_batch(iterator):
    922                 self._iterating = self._original_iterator is not None
    923 

/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/joblib/parallel.py in dispatch_one_batch(self, iterator)
    757                 return False
    758             else:
--> 759                 self._dispatch(tasks)
    760                 return True
    761 

/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/joblib/parallel.py in _dispatch(self, batch)
    714         with self._lock:
    715             job_idx = len(self._jobs)
--> 716             job = self._backend.apply_async(batch, callback=cb)
    717             # A job can complete so quickly than its callback is
    718             # called before we get here, causing self._jobs to

/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/joblib/_parallel_backends.py in apply_async(self, func, callback)
    180     def apply_async(self, func, callback=None):
    181         """"""Schedule a func to be run""""""
--> 182         result = ImmediateResult(func)
    183         if callback:
    184             callback(result)

/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/joblib/_parallel_backends.py in __init__(self, batch)
    547         # Don't delay the application, to avoid keeping the input
    548         # arguments in memory
--> 549         self.results = batch()
    550 
    551     def get(self):

/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/joblib/parallel.py in __call__(self)
    223         with parallel_backend(self._backend, n_jobs=self._n_jobs):
    224             return [func(*args, **kwargs)
--> 225                     for func, args, kwargs in self.items]
    226 
    227     def __len__(self):

/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/joblib/parallel.py in <listcomp>(.0)
    223         with parallel_backend(self._backend, n_jobs=self._n_jobs):
    224             return [func(*args, **kwargs)
--> 225                     for func, args, kwargs in self.items]
    226 
    227     def __len__(self):

/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/model_selection/_validation.py in _fit_and_score(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, error_score)
    552         fit_time = time.time() - start_time
    553         # _score will return dict if is_multimetric is True
--> 554         test_scores = _score(estimator, X_test, y_test, scorer, is_multimetric)
    555         score_time = time.time() - start_time - fit_time
    556         if return_train_score:

/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/model_selection/_validation.py in _score(estimator, X_test, y_test, scorer, is_multimetric)
    595     """"""
    596     if is_multimetric:
--> 597         return _multimetric_score(estimator, X_test, y_test, scorer)
    598     else:
    599         if y_test is None:

/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/model_selection/_validation.py in _multimetric_score(estimator, X_test, y_test, scorers)
    625             score = scorer(estimator, X_test)
    626         else:
--> 627             score = scorer(estimator, X_test, y_test)
    628 
    629         if hasattr(score, 'item'):

/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/metrics/scorer.py in __call__(self, estimator, X, y_true, sample_weight)
     95         else:
     96             return self._sign * self._score_func(y_true, y_pred,
---> 97                                                  **self._kwargs)
     98 
     99 

/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/metrics/classification.py in precision_score(y_true, y_pred, labels, pos_label, average, sample_weight)    1567                                                 average=average,    1568                                               warn_for=('precision',),
-> 1569                                                  sample_weight=sample_weight)    1570     return p    1571 

/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/metrics/classification.py in precision_recall_fscore_support(y_true, y_pred, beta, labels, pos_label, average, warn_for, sample_weight)    1413         raise ValueError(""beta should be >0 in the F-beta score"")    1414     labels
= _check_set_wise_labels(y_true, y_pred, average, labels,
-> 1415                                     pos_label)    1416     1417     # Calculate tp_sum, pred_sum, true_sum ###

/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/metrics/classification.py in _check_set_wise_labels(y_true, y_pred, average, labels, pos_label)  1237                          str(average_options))    1238 
-> 1239     y_type, y_true, y_pred = _check_targets(y_true, y_pred)    1240     present_labels = unique_labels(y_true, y_pred)    1241     if average == 'binary':

/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/metrics/classification.py in _check_targets(y_true, y_pred)
     79     if len(y_type) > 1:
     80         raise ValueError(""Classification metrics can't handle a mix of {0} ""
---> 81                          ""and {1} targets"".format(type_true, type_pred))
     82 
     83     # We can't have more than one value on y_type => The set is no more needed

ValueError: Classification metrics can't handle a mix of multilabel-indicator and binary targets

What am I doing wrong?
","Issue in your code

You cant use hot-one-encoded labels link. Use raw labels. You can use sparse_categorical_crossentropy loss with raw labels.
cross_validate returns scores as test_scores. For train scores set return_train_score

Corrected code
def nn_model():
    model_lstm1 = Sequential()
    model_lstm1.add(Embedding(200, 100, input_length=10))
    model_lstm1.add(LSTM(10, dropout=0.2, recurrent_dropout=0.2))
    model_lstm1.add(Dense(2, activation='sigmoid'))
    model_lstm1.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
    return model_lstm1

classifier = KerasClassifier(build_fn=nn_model, batch_size=10,nb_epoch=10)

scoring = {'precision' : make_scorer(precision_score),
           'recall' : make_scorer(recall_score), 
           'f1_score' : make_scorer(f1_score)}

results = cross_validate(classifier, np.random.randint(0,100,(1000,10)), 
                         np.random.np.random.randint(0,2,1000), scoring = scoring, cv=3, return_train_score=True)

print(""F1 score SVM: %0.2f (+/- %0.2f)"" % (np.mean(results['test_f1_score']), np.std(results['test_f1_score'])))
print(""precision score SVM: %0.2f (+/- %0.2f)"" % (np.mean(results['test_precision']), np.std(results['test_precision'])))
print(""recall macro SVM: %0.2f (+/- %0.2f)"" % (np.mean(results['test_recall']), np.std(results['test_recall'])))

Output
Epoch 1/1
666/666 [==============================] - 5s 7ms/step - loss: 0.6932 - acc: 0.5075
Epoch 1/1
667/667 [==============================] - 5s 7ms/step - loss: 0.6929 - acc: 0.5127
Epoch 1/1
667/667 [==============================] - 5s 7ms/step - loss: 0.6934 - acc: 0.5007
F1 score SVM: 0.10 (+/- 0.09)
precision score SVM: 0.43 (+/- 0.07)
recall macro SVM: 0.06 (+/- 0.06)

You might get
UndefinedMetricWarning: ....

warnings in initials epochs (if data is low), which you can ignore. This is because the classifier is classifying all the data to one class and no data into the  another class.
",cross-validation
how to standardize data with sklearn39s cross_val_score,"Let's say I want to use a LinearSVC to perform k-fold-cross-validation on a dataset. How would I perform standardization on the data?
The best practice I have read is to build your standardization model on your training data then apply this model to the testing data.
When one uses a simple train_test_split(), this is easy as we can just do:
X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y)

clf = svm.LinearSVC()

scalar = StandardScaler()
X_train = scalar.fit_transform(X_train)
X_test = scalar.transform(X_test)

clf.fit(X_train, y_train)
predicted = clf.predict(X_test)

How would one go about standardizing data while doing k-fold-cross-validation? The problem comes from the fact that every data point will be for training/testing so you cannot standardize everything before cross_val_score(). Wouldn't you need a different standardization for each cross validation? 
The docs do not mention standardization happening internally within the function. Am I SOL? 
EDIT: This post is super helpful: Python - What is exactly sklearn.pipeline.Pipeline?
","You can use a Pipeline to combine both of the processes and then send it into the cross_val_score().
When the fit() is called on the pipeline, it will fit all the transforms one after the other and transform the data, then fit the transformed data using the final estimator. And during predict() (Only available if last object in pipeline is an estimator, otherwise transform()) it will apply transforms to the data, and predict with the final estimator.
Like this:
scalar = StandardScaler()
clf = svm.LinearSVC()

pipeline = Pipeline([('transformer', scalar), ('estimator', clf)])

cv = KFold(n_splits=4)
scores = cross_val_score(pipeline, X, y, cv = cv)

Check out various examples of pipeline to understand it better:

http://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html#examples-using-sklearn-pipeline-pipeline

",cross-validation
cross validation metrics for h2o,"I'm having a hard time understanding why the output for various metrics on my models differs when I use h2o.  
For example,  if I use 'h2o.grid' then the logloss measure is different when I look at the mean model$cross_validation_metrics_summary.  It is the same as   model$cross_validation_metrics_summary.   What is the reasoning behind this difference?  What one should I report on?
library(mlbench) 
  library(h2o)
 data(Sonar)

h2o.init() Sonarhex <- as.h2o(Sonar) h2o.grid(""gbm"", grid_id = ""gbm_grid_id0"", x = c(1:50), y = 'Class',
         training_frame = Sonarhex, hyper_params = list(ntrees = 50, learn_rate = c(.1, .2, .3)), nfolds = 5, seed=1234)

grid <- h2o.getGrid(""gbm_grid_id0"", sort_by = 'logloss')

first_model = h2o.getModel(grid@model_ids[[1]]) first_model@model$cross_validation_metrics_summary first_model@model$cross_validation_metrics

","This inconsistency is an issue that has been documented and explained here and will be resolved in a future release.  The model$cross_validation_metrics_summary metrics are the correct CV metrics.  The metrics that appear in the Grid table or by using the utility functions like h2o.logloss(model, xval = TRUE) are slightly different because they aggregate the CV predictions and then compute the loss (instead of computing the loss separately across K folds and then taking the average).  This can lead to slight numerical differences.
",cross-validation
usage of retain graph in pytorch,"I get error if I don't supply retain_graph=True in y1.backward()
   import torch
   x = torch.tensor([2.0], requires_grad=True)
   y = torch.tensor([3.0], requires_grad=True)
   f = x+y
   z = 2*f
   y1 = z**2
   y2 = z**3
   y1.backward()
   y2.backward()

Traceback (most recent call last):
  File ""/Users/a0m08er/pytorch/pytorch_tutorial/tensor.py"", line 58, in <module>
    y2.backward()
  File ""/Users/a0m08er/pytorch/lib/python3.11/site-packages/torch/_tensor.py"", line 521, in backward
    torch.autograd.backward(
  File ""/Users/a0m08er/pytorch/lib/python3.11/site-packages/torch/autograd/__init__.py"", line 289, in backward
    _engine_run_backward(
  File ""/Users/a0m08er/pytorch/lib/python3.11/site-packages/torch/autograd/graph.py"", line 769, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
RuntimeError: Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward.

But I don't get error when I do this:
   import torch

   x = torch.tensor([2.0], requires_grad=True)
   y = torch.tensor([3.0], requires_grad=True)
   z = x+y
   y1 = z**2
   y2 = z**3
   y1.backward()
   y2.backward()

Since z is a common node for y1 and y2 why it is not showing me error when I do y2.backward()
","basically the error

Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward.

Error comes when the backwards pass tries to access tensors that were saved for the backwards pass (using ctx.save_for_backward), and those are not present (usually because they were freed after doing the first backward pass witoutretain_graph=True).
So the computation graph is still there after the first backwards pass, only the tensors saved in context were freed.
But the thing is, addition operations do not need to save tensors for backwards pass (the gradient along each of the inputs is the same as the gradient over the sum — so the gradient is just passed along the graph without doing any operation, no need to save anything for backward). Thus the error doesn't happen if the only shared node is an addition node.
In comparison, multiplication needs to save the input values for the backward pass (since the gradient for a * b along b is a * grad(a * b)). Thus the exception gets raised when it tries to access them
",pytorch
how can i load an image from url into a tensorflow pipeline in r,"I am playing around with image classification using the tensorflow and keras packages for R. I have build and trained a model that does well on the testing validation dataset. I now want to use that model to predict image classes for a lot of images stored online (i have all the URLs in a dataframe in R).
I can write a for loop to do this where i download each image, classify it, record the model prediction, and then delete the downloaded image, but this takes a long time and it would be faster to just read the image into memory instead of downloading each image. I cannot for the life of me figure out how to load an imagine into memory in R and convert it to a datatype that works with the rest of my tensorflow image standardization.
Here is my for loop:
data$score<-NA
for (i in 1:nrow(data)){
  
  img_tensor = 
    get_file(""t"",data$image_url[i]) %>% #download temp file
    tf$io$read_file() %>%
    tf$io$decode_image() %>%
    tf$image$resize(as.integer(image_size)) %>%
    tf$expand_dims(0L) 
  
  #delete temp file
  file.remove(""/Users/me/.keras/datasets/t"")
  
  data$score[i]=model %>% predict(img_tensor, verbose=0)
  
}

Here is an example image URL: https://inaturalist-open-data.s3.amazonaws.com/photos/451526093/medium.jpeg
All i want to do is be able to load that image into R directly from the URL (no writing the file to disk) and then use the tensorflow workflow (decode_image, resize, expand_dims). Any help is appreciated!
To replicate the code just replace data$image_url[i] with the URL i provided. No need to worry about predicting my model, that part is working fine. I just need the image to successfully feed into the rest of the pipe.
","A few notes:

Writing to a temporary directory on macOS and Linux usually has identical performance to keeping everything in memory, since /tmp is usually mounted as a RAM filesystem and never actually touches the disk. (If you're on Windows, or are swapping, the story is different)

As far as I know, TensorFlow doesn't have any graph ops that will fetch content from an http url, so you'll need to do that step using R or Python. If the op needs to live in a tf.data, you'll need to wrap it in tf.py_function.

To fetch a url directly into memory in R, without writing to the filesystem, you can do:
url <- ""https://inaturalist-open- data.s3.amazonaws.com/photos/451526093/medium.jpeg""
bytes <- readBin(url, raw(), 200000)
as_py_bytes <- reticulate::import_builtins(convert = FALSE)$bytes
bytes_tensor <- tf$constant(as_py_bytes(bytes), tf$string)


The bottleneck is most likely the download step, not the ""write to a file"" step. You'll probably see the most significant speedups from rewriting your loop to process batches of images instead of a single image at a time (e.g., using curl::multi_download(), and passing a batch of images to predict())


",tensorflow
scikitlearn cross validation function not allowing custom folds when indices are not sequential,"Attempting to pass in custom cross validation folds to sklearn's cross validate function.
The cross validate function seems to be triggering an error because it's insisting on using position-based indexing, rather than label-based indexing. The indices I'm passing in my cv_folds argument are consistent with the original dataframe's indices. The reason this is relevant is because I want to use a hash function value to select subsets for my train-test split, as well as my cv folds. I get the following error: IndexError: indices are out-of-bounds
df2 = pd.DataFrame(np.random.rand(8, 3), columns=['feature_1', 'feature_2', 'feature_3'])

train_index_list = [0,1,2,5,6,7]
test_index_list = [3,4]
X_train = df2.loc[train_index_list].drop(columns='feature_3').copy()
y_train = df2.loc[train_index_list]['feature_3'].copy()

# 2-fold cross validation
cv_folds = [ ([0,1,2,],[5,6,7]), ([5,6,7], [0,1,2])]

cv_output = cross_validate(model, X_train, y_train,  scoring=['neg_mean_squared_error'], cv=cv_folds) 

This triggers an error. But what puzzles me is that the following lines run just fine
X_train.loc[train_index_list]
y_train.loc[train_index_list]

How do I resolve this so I can pass in my custom-defined cv folds into Scikit-Learn?
","You can use a workaround by using a Index.get_indexer to convert labels to index positions:
def cv_folds(df, labels):
    for i, j in labels:
        i = df.index.get_indexer(i)
        j = df.index.get_indexer(j)
        yield (i.tolist(), j.tolist())

labels = [([0, 1, 2], [5, 6, 7]), ([5, 6, 7], [0, 1, 2])]
cv = cv_folds(X_train, labels)
cv_output = cross_validate(model, X_train, y_train, cv=cv,
                           scoring=['neg_mean_squared_error'])

Test:
>>> list(cv_folds(X_train, labels))
    [([0, 1, 2], [3, 4, 5]), ([3, 4, 5], [0, 1, 2])]  # <- positions
#   [([0, 1, 2], [5, 6, 7]), ([5, 6, 7], [0, 1, 2])]  # <- labels

",cross-validation
python package bug on installing via conda,"I attempted conda install on a remote pc from my university.
Attempted
conda install pytorch==2.2.0 torchvision==0.17.0 pytorch-cuda=12.1 -c pytorch -c nvidia -y
It is very slow and I am unable to install from it. It gives the following error.
Can anybody help on looking at the error? I installed miniconda from the official website by command line.
Here's the bug:
CondaVerificationError: The package for pytorch located at /home/wang.ziyue7/miniconda3/pkgs/pytorch-2.2.0-py3.10_cuda12.1_cudnn8.9.2_0
appears to be corrupted. The path 'lib/python3.10/site-packages/torchgen/packaged/ATen/templates/RegisterDispatchKey.cpp'
specified in the package manifest cannot be found.

CondaVerificationError: The package for pytorch located at /home/wang.ziyue7/miniconda3/pkgs/pytorch-2.2.0-py3.10_cuda12.1_cudnn8.9.2_0
appears to be corrupted. The path 'lib/python3.10/site-packages/torchgen/packaged/ATen/templates/RegisterFunctionalization.cpp'
specified in the package manifest cannot be found.

CondaVerificationError: The package for pytorch located at /home/wang.ziyue7/miniconda3/pkgs/pytorch-2.2.0-py3.10_cuda12.1_cudnn8.9.2_0
appears to be corrupted. The path 'lib/python3.10/site-packages/torchgen/packaged/ATen/templates/RegisterSchema.cpp'
specified in the package manifest cannot be found.

CondaVerificationError: The package for pytorch located at /home/wang.ziyue7/miniconda3/pkgs/pytorch-2.2.0-py3.10_cuda12.1_cudnn8.9.2_0
appears to be corrupted. The path 'lib/python3.10/site-packages/torchgen/packaged/ATen/templates/TensorMethods.cpp'
specified in the package manifest cannot be found.

CondaVerificationError: The package for pytorch located at /home/wang.ziyue7/miniconda3/pkgs/pytorch-2.2.0-py3.10_cuda12.1_cudnn8.9.2_0
appears to be corrupted. The path 'lib/python3.10/site-packages/torchgen/packaged/ATen/templates/UfuncCPU.cpp'
specified in the package manifest cannot be found.

CondaVerificationError: The package for pytorch located at /home/wang.ziyue7/miniconda3/pkgs/pytorch-2.2.0-py3.10_cuda12.1_cudnn8.9.2_0
appears to be corrupted. The path 'lib/python3.10/site-packages/torchgen/packaged/ATen/templates/UfuncCPUKernel.cpp'
specified in the package manifest cannot be found.

CondaVerificationError: The package for pytorch located at /home/wang.ziyue7/miniconda3/pkgs/pytorch-2.2.0-py3.10_cuda12.1_cudnn8.9.2_0
appears to be corrupted. The path 'lib/python3.10/site-packages/torchgen/packaged/ATen/templates/UfuncCUDA.cu'
specified in the package manifest cannot be found.

CondaVerificationError: The package for pytorch located at /home/wang.ziyue7/miniconda3/pkgs/pytorch-2.2.0-py3.10_cuda12.1_cudnn8.9.2_0
appears to be corrupted. The path 'lib/python3.10/site-packages/torchgen/packaged/ATen/templates/UnboxingFunctions.cpp'
specified in the package manifest cannot be found.

CondaVerificationError: The package for pytorch located at /home/wang.ziyue7/miniconda3/pkgs/pytorch-2.2.0-py3.10_cuda12.1_cudnn8.9.2_0
appears to be corrupted. The path 'lib/python3.10/site-packages/torchgen/packaged/autograd/README.md'
specified in the package manifest cannot be found.

CondaVerificationError: The package for pytorch located at /home/wang.ziyue7/miniconda3/pkgs/pytorch-2.2.0-py3.10_cuda12.1_cudnn8.9.2_0
appears to be corrupted. The path 'lib/python3.10/site-packages/torchgen/packaged/autograd/build.bzl'
specified in the package manifest cannot be found.

CondaVerificationError: The package for pytorch located at /home/wang.ziyue7/miniconda3/pkgs/pytorch-2.2.0-py3.10_cuda12.1_cudnn8.9.2_0
appears to be corrupted. The path 'lib/python3.10/site-packages/torchgen/packaged/autograd/templates/ADInplaceOrViewType.cpp'
specified in the package manifest cannot be found.

CondaVerificationError: The package for pytorch located at /home/wang.ziyue7/miniconda3/pkgs/pytorch-2.2.0-py3.10_cuda12.1_cudnn8.9.2_0
appears to be corrupted. The path 'lib/python3.10/site-packages/torchgen/packaged/autograd/templates/Functions.cpp'
specified in the package manifest cannot be found.

CondaVerificationError: The package for pytorch located at /home/wang.ziyue7/miniconda3/pkgs/pytorch-2.2.0-py3.10_cuda12.1_cudnn8.9.2_0
appears to be corrupted. The path 'lib/python3.10/site-packages/torchgen/packaged/autograd/templates/TraceType.cpp'
specified in the package manifest cannot be found.

CondaVerificationError: The package for pytorch located at /home/wang.ziyue7/miniconda3/pkgs/pytorch-2.2.0-py3.10_cuda12.1_cudnn8.9.2_0
appears to be corrupted. The path 'lib/python3.10/site-packages/torchgen/packaged/autograd/templates/VariableType.cpp'
specified in the package manifest cannot be found.

CondaVerificationError: The package for pytorch located at /home/wang.ziyue7/miniconda3/pkgs/pytorch-2.2.0-py3.10_cuda12.1_cudnn8.9.2_0
appears to be corrupted. The path 'lib/python3.10/site-packages/torchgen/packaged/autograd/templates/annotated_fn_args.py.in'
specified in the package manifest cannot be found.

CondaVerificationError: The package for pytorch located at /home/wang.ziyue7/miniconda3/pkgs/pytorch-2.2.0-py3.10_cuda12.1_cudnn8.9.2_0
appears to be corrupted. The path 'lib/python3.10/site-packages/torchgen/packaged/autograd/templates/python_enum_tag.cpp'
specified in the package manifest cannot be found.

CondaVerificationError: The package for pytorch located at /home/wang.ziyue7/miniconda3/pkgs/pytorch-2.2.0-py3.10_cuda12.1_cudnn8.9.2_0
appears to be corrupted. The path 'lib/python3.10/site-packages/torchgen/packaged/autograd/templates/python_fft_functions.cpp'
specified in the package manifest cannot be found.

CondaVerificationError: The package for pytorch located at /home/wang.ziyue7/miniconda3/pkgs/pytorch-2.2.0-py3.10_cuda12.1_cudnn8.9.2_0
appears to be corrupted. The path 'lib/python3.10/site-packages/torchgen/packaged/autograd/templates/python_functions.cpp'
specified in the package manifest cannot be found.

CondaVerificationError: The package for pytorch located at /home/wang.ziyue7/miniconda3/pkgs/pytorch-2.2.0-py3.10_cuda12.1_cudnn8.9.2_0
appears to be corrupted. The path 'lib/python3.10/site-packages/torchgen/packaged/autograd/templates/python_linalg_functions.cpp'
specified in the package manifest cannot be found.

CondaVerificationError: The package for pytorch located at /home/wang.ziyue7/miniconda3/pkgs/pytorch-2.2.0-py3.10_cuda12.1_cudnn8.9.2_0
appears to be corrupted. The path 'lib/python3.10/site-packages/torchgen/packaged/autograd/templates/python_nested_functions.cpp'
specified in the package manifest cannot be found.

CondaVerificationError: The package for pytorch located at /home/wang.ziyue7/miniconda3/pkgs/pytorch-2.2.0-py3.10_cuda12.1_cudnn8.9.2_0
appears to be corrupted. The path 'lib/python3.10/site-packages/torchgen/packaged/autograd/templates/python_nn_functions.cpp'
specified in the package manifest cannot be found.

CondaVerificationError: The package for pytorch located at /home/wang.ziyue7/miniconda3/pkgs/pytorch-2.2.0-py3.10_cuda12.1_cudnn8.9.2_0
appears to be corrupted. The path 'lib/python3.10/site-packages/torchgen/packaged/autograd/templates/python_return_types.cpp'
specified in the package manifest cannot be found.

CondaVerificationError: The package for pytorch located at /home/wang.ziyue7/miniconda3/pkgs/pytorch-2.2.0-py3.10_cuda12.1_cudnn8.9.2_0
appears to be corrupted. The path 'lib/python3.10/site-packages/torchgen/packaged/autograd/templates/python_sparse_functions.cpp'
specified in the package manifest cannot be found.

CondaVerificationError: The package for pytorch located at /home/wang.ziyue7/miniconda3/pkgs/pytorch-2.2.0-py3.10_cuda12.1_cudnn8.9.2_0
appears to be corrupted. The path 'lib/python3.10/site-packages/torchgen/packaged/autograd/templates/python_special_functions.cpp'
specified in the package manifest cannot be found.

CondaVerificationError: The package for pytorch located at /home/wang.ziyue7/miniconda3/pkgs/pytorch-2.2.0-py3.10_cuda12.1_cudnn8.9.2_0
appears to be corrupted. The path 'lib/python3.10/site-packages/torchgen/packaged/autograd/templates/python_torch_functions.cpp'
specified in the package manifest cannot be found.

CondaVerificationError: The package for pytorch located at /home/wang.ziyue7/miniconda3/pkgs/pytorch-2.2.0-py3.10_cuda12.1_cudnn8.9.2_0
appears to be corrupted. The path 'lib/python3.10/site-packages/torchgen/packaged/autograd/templates/python_variable_methods.cpp'
specified in the package manifest cannot be found.

ClobberError: This transaction has incompatible packages due to a shared path.
  packages: defaults/linux-64::jpeg-9e-h5eee18b_3, pytorch/linux-64::libjpeg-turbo-2.0.0-h9bf148f_0
  path: 'bin/cjpeg'


ClobberError: This transaction has incompatible packages due to a shared path.
  packages: defaults/linux-64::jpeg-9e-h5eee18b_3, pytorch/linux-64::libjpeg-turbo-2.0.0-h9bf148f_0
  path: 'bin/djpeg'


ClobberError: This transaction has incompatible packages due to a shared path.
  packages: defaults/linux-64::jpeg-9e-h5eee18b_3, pytorch/linux-64::libjpeg-turbo-2.0.0-h9bf148f_0
  path: 'bin/jpegtran'


ClobberError: This transaction has incompatible packages due to a shared path.
  packages: defaults/linux-64::jpeg-9e-h5eee18b_3, pytorch/linux-64::libjpeg-turbo-2.0.0-h9bf148f_0
  path: 'bin/rdjpgcom'


ClobberError: This transaction has incompatible packages due to a shared path.
  packages: defaults/linux-64::jpeg-9e-h5eee18b_3, pytorch/linux-64::libjpeg-turbo-2.0.0-h9bf148f_0
  path: 'bin/wrjpgcom'


ClobberError: This transaction has incompatible packages due to a shared path.
  packages: defaults/linux-64::jpeg-9e-h5eee18b_3, pytorch/linux-64::libjpeg-turbo-2.0.0-h9bf148f_0
  path: 'include/jconfig.h'


ClobberError: This transaction has incompatible packages due to a shared path.
  packages: defaults/linux-64::jpeg-9e-h5eee18b_3, pytorch/linux-64::libjpeg-turbo-2.0.0-h9bf148f_0
  path: 'include/jerror.h'


ClobberError: This transaction has incompatible packages due to a shared path.
  packages: defaults/linux-64::jpeg-9e-h5eee18b_3, pytorch/linux-64::libjpeg-turbo-2.0.0-h9bf148f_0
  path: 'include/jmorecfg.h'


ClobberError: This transaction has incompatible packages due to a shared path.
  packages: defaults/linux-64::jpeg-9e-h5eee18b_3, pytorch/linux-64::libjpeg-turbo-2.0.0-h9bf148f_0
  path: 'include/jpeglib.h'


ClobberError: This transaction has incompatible packages due to a shared path.
  packages: defaults/linux-64::jpeg-9e-h5eee18b_3, pytorch/linux-64::libjpeg-turbo-2.0.0-h9bf148f_0
  path: 'lib/libjpeg.a'


ClobberError: This transaction has incompatible packages due to a shared path.
  packages: defaults/linux-64::jpeg-9e-h5eee18b_3, pytorch/linux-64::libjpeg-turbo-2.0.0-h9bf148f_0
  path: 'lib/libjpeg.so'

","There are two issues associated with your post:

Corrupted Package Files: The CondaVerificationError suggests some files expected within Pytorch are either missing or corrupted. It is either the package installation process was interrupted or incorrectly downloaded.
Path Conflict: The error ClobberError suggest that there is an issue with the file path used. Such error occurs when two try to install files to the same location. In this case the conflict is between jpeg and lipjpeg-turbo, both of which want to use the bin and include folders or directories.

Please take the following steps to resolve the issues:

Clear conda package cache to remove potentially corrupted files.
Use the code (bash):
conda clean --all

To ensure that you using the latest version of conda, use the update Conda using this code (bash):
conda update conda
This is likely to fix the compatibility issues.

Attempt the installation again by reinstalling with the original command. This is the code: conda install pytorch==2.2.0 torchvision==0.17.0 pytorch-cuda=12.1 -c pytorch -c nvidia


",pytorch
scikitlearn loocv vs doing it manually give different results why,"So i have built a model for a small dataset and since it was a small dataset, i made a Leave-One-out Cross-Validation (LOOCV) check for its accuracy. so in short, i would remove one sample manually, train the model, predict the left out sample and save the prediction and repeat the process for all the samples. then i would use the list of predictions and the actual values to get a RMSE and R2.
and today i found out that there was a Scikit-Learn implementation sklearn.model_selection.LeaveOneOut, however, when i tried it, it gave me different results for the RMSE, and refused to use R-squared as accuracy in the LOOCV method (it seems to calculate the accuracy per sample which does not work with R2).
here is a brief example of the code:
from numpy import mean
from numpy import std
from sklearn.datasets import make_blobs
from sklearn.model_selection import LeaveOneOut
from sklearn.model_selection import cross_val_score
from sklearn.ensemble import RandomForestClassifier

cv = LeaveOneOut()
model = RandomForestRegressor(n_estimators=200, max_depth=6,n_jobs=40, random_state=0)

scores = cross_val_score(model, data2SN, labelCL, scoring='neg_root_mean_squared_error', cv=cv, n_jobs=-1)
# report performance
print('Accuracy: %.3f (%.3f)' % (mean(scores), std(scores)))

my guess is that I'm calculating the RMSE for the whole dataset, while the LOOCV is doing it per sample and eventually i would take the mean and this is what causes the discrepancy between the two codes output, however, when i tried to calculate the RMSE per sample it failed (citing this TypeError: Singleton array 3021.0 cannot be considered a valid collection).   so I'm not sure how the RMSE is calculated inside the LOOCV. and I'm not sure to trust my code or just blindly use scikit-learn implementation.
I'm lost at what to do and chatGPT was just confusing as hell, so my human brethren please help
","cross_val_score average the scores across folds, so with cv=LeaveOneOut() yes, it's computing the score per row (by a model trained on all other rows).  With RMSE, that's equivalent to MAE; and R2 will just fail.
You could use cross_val_predict to get the individual predictions, then score that collection all at once, to reproduce your manual work.
",cross-validation
sklearn pipeline  trying to count the number of times an estimator is called,"I'm trying to count the number of times LogisticRegression is called in this pipeline, so I extended the class and overrode .fit(). It was supposed to be simple but it generates this weird error:
TypeError: float() argument must be a string or a number, not 'MyLogistic'
where MyLogistic is the new class. You should be able to reproduce the whole thing if you copy and paste the code.
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import (GridSearchCV, StratifiedKFold)
from sklearn.feature_selection import SelectFromModel
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.datasets import load_iris
import numpy as np

class MyLogistic(LogisticRegression):
    __call_counter = 0
    def fit(X, y, sample_weight=None):
        print(""MyLogistic fit is called."")
        MyLogistic._MyLogistic__call_counter += 1
        # fit() returns self.
        return super().fit(X, y, sample_weight)

# If I use this ""extension"", everything works fine.
#class MyLogistic(LogisticRegression):
#    pass
    
initial_logistic = MyLogistic(solver=""liblinear"", random_state = np.random.RandomState(18))
final_logistic = LogisticRegression(solver=""liblinear"", random_state = np.random.RandomState(20))
# prefit = False by default
select_best = SelectFromModel(estimator = initial_logistic, threshold = -np.inf)

select_k_best_pipeline = Pipeline(steps=[
    ('first_scaler', StandardScaler(with_mean = False)),
    # initial_logistic will be called from select_best, prefit = false by default.
    ('select_k_best', select_best),
    ('final_logit', final_logistic)
])

select_best_grid = {'select_k_best__estimator__C' : [0.02, 0.03],
                    'select_k_best__max_features': [1, 2],
                    'final_logit__C' : [0.01, 0.5, 1.0]}

skf = StratifiedKFold(n_splits = 3, shuffle = True, random_state = 17)

logit_best_searcher = GridSearchCV(estimator = select_k_best_pipeline, param_grid = select_best_grid, cv = skf, 
                               scoring = ""roc_auc"", n_jobs = 6, verbose = 4)

X, y = load_iris(return_X_y=True)
logit_best_searcher.fit(X, y > 0)
print(""Best hyperparams: "", logit_best_searcher.best_params_)

","You just forgot to put self as the first parameter of the fit signature.  So the call is getting X=self, and when trying to check the input X it at some point tries to convert to float, hence the error message.
There's still some weirdness around the parallelization; I get the counter equal to 1. Setting n_jobs=1 instead, I get the correct 37 for the counter (2x2x3 hyperparameter candidates on x3 folds, +1 for the final refit).
",cross-validation
autograd error caused by relu in pytorch,"I am using a residual neural network for a classification task. Somehow adding or omitting a ReLU activation causes the autograd to fail. I would be grateful for any insights on the reason for this? It cannot make any sense of it. ReLU is not an inplace operation, is it? Error message:
RuntimeError: one of the variables needed for gradient computation has been modified by an inplace operation
Here is the network architecture. The 3rd to last line is what causes the issue when not commented out.
class ResidualBlock(nn.Module):
    def __init__(self, num_filters, kernel_size):
        super(ResidualBlock, self).__init__()
        self.conv1 = nn.Conv1d(num_filters, num_filters, kernel_size=kernel_size, padding='same')
        self.bn1 = nn.BatchNorm1d(num_filters)
        self.conv2 = nn.Conv1d(num_filters, num_filters, kernel_size=kernel_size, padding='same')
        self.bn2 = nn.BatchNorm1d(num_filters)

    def forward(self, x):
        shortcut = x
        out = F.relu(self.bn1(self.conv1(x)))
        out = self.bn2(self.conv2(out))
        out = F.relu(out) # causes the issue when not commented out
        out += shortcut
        return out

Below is a minimal working example. I am using Python 3.12 and torch 2.5.1.
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import DataLoader, TensorDataset

# Define the ResidualBlock
class ResidualBlock(nn.Module):
    def __init__(self, num_filters, kernel_size):
        super(ResidualBlock, self).__init__()
        self.conv1 = nn.Conv1d(num_filters, num_filters, kernel_size=kernel_size, padding='same')
        self.bn1 = nn.BatchNorm1d(num_filters)
        self.conv2 = nn.Conv1d(num_filters, num_filters, kernel_size=kernel_size, padding='same')
        self.bn2 = nn.BatchNorm1d(num_filters)

    def forward(self, x):
        shortcut = x
        out = F.relu(self.bn1(self.conv1(x)))
        out = self.bn2(self.conv2(out))
        out = F.relu(out) # causes the issue
        out += shortcut
        return out


class SimpleModel(nn.Module):
    def __init__(self, num_filters, kernel_size):
        super(SimpleModel, self).__init__()
        self.res_block = ResidualBlock(num_filters, kernel_size)
        self.fc = nn.Linear(num_filters, 1)

    def forward(self, x):
        x = self.res_block(x)
        x = x.mean(dim=2)
        x = self.fc(x)
        return x

torch.manual_seed(42)
num_samples = 1000
sequence_length = 32
num_filters = 16

X = torch.randn(num_samples, num_filters, sequence_length)  # Random input
y = torch.sum(X, dim=(1, 2), keepdim=True)  # Simple target (sum of all values)


dataset = TensorDataset(X, y)
dataloader = DataLoader(dataset, batch_size=32, shuffle=True)


model = SimpleModel(num_filters=num_filters, kernel_size=3)
criterion = nn.MSELoss() 
optimizer = torch.optim.Adam(model.parameters(), lr=0.001)


epochs = 5
for epoch in range(epochs):
    model.train()
    epoch_loss = 0.0
    for batch_X, batch_y in dataloader:
        optimizer.zero_grad()
        outputs = model(batch_X)
        loss = criterion(outputs, batch_y)
        loss.backward()
        optimizer.step()
        epoch_loss += loss.item()
    print(f""Epoch {epoch+1}/{epochs}, Loss: {epoch_loss/len(dataloader):.4f}"")

print(""Training complete!"")

","The inplace operation is this:
out += shortcut

The relu needs its own output to compute its gradient! Thus you are doing an inplace operation on the output of the relu, which it needed to compute its gradient in the backwards pass.
replacing it with
out = out + shortcut

should solve your problem.
In general, try to avoid using inplace functions in pytorch (such as += unless you know what you are doing

more details:
if you look at the pytorch code, the backwards pass for the relu is auto-generated from the following bit of code in pytorch/tools/autograd/derivatives.yaml
- name: relu(Tensor self) -> Tensor
  self: threshold_backward(grad, result, 0)
  result: auto_element_wise

What this does is it

takes the gradient of the output as grad
and the result of the relu as result
and returns the contents of grad where result is bigger than 0 and 0 otherwise.

Thus it does need the output (technically, it could have stored its input instead, but this is how it is implemented)
",pytorch
how to do backpropagation in pytorch when training alphazero,"I'm trying to implement my version of AlphaZero for Connect Four. I have implemented a convolutional network using PyTorch and can get (random) value- and policy outputs from the model for given boardstates. Now I would like to simulate some games and train the model using them. However, I have encountered a problem:
As far as I understand, the training consists of basically two steps: a step in which selfplay is used to gather game data, and after that a step where the collected data is used to train the model using backpropagation.
In the selfplay step, the network is used to get an evaluation of a position and a policy on how to choose the next move. The policy is then improved upon using a version of the MCTS algorithm.
After a game is finished, all the moves and the result is saved.
For simplicity, assume that I only play a single game and then want to update the model. If I save the MCTS policies and the network policies I can now calculate the loss. But I can't backpropagate through the model, since the forward pass happened during the collection step. I could in theory forward the same position through the model again, but that sounds not only inefficient, but since my architecture uses dropout layers I would not even get the same results.
So how can I solve this problem in PyTorch? Can I somehow save a model together with the dropout configuration that was used to create a policy? Then I could at least just forward the position again and use backprop afterwards, even if that would be inefficient.
","In general it is not the practice to use gradients from self-play to backprop during training (for many reasons). It would be rather in-efficient to store gradients for later backprop. Plus there is exploration noise in self-play. Re-running is normal in RL training phase.
In self-play, you will likely use eval mode to be on-policy. Drop out is only used in training for regularization purpose. In a sense drop-out can be helpful for exploration, but I think more apt exploration is using parameter noise.
I don't know about AlphaZero, but IMHO it makes less sense to store dropout noise. If you want to do that, use replay buffer to store drop activation which you capture using register_forward_hook.
",pytorch
real time object detection with yolo model not working,"I have trained a custom yolo model to detect square slots on a board, and is working with more than 95 % accuracy on images.
But as soon as I switch to video detection it seems to not detect even a single thing
I am using the following code to run real time object detection
cap = cv2.VideoCapture('../video/1st/output.mp4')
while cap.isOpened():
    ret, frame = cap.read()

    results = model(frame)
    final_img = np.squeeze(results.render())


    cv2.imshow(""YOLO"", final_img)
    if cv2.waitKey(10) & 0XFF == ord(""q""):
        break
cap.release()
cv2.destroyAllWindows()


I load the model using this code
model = torch.hub.load(""ultralytics/yolov5"", ""custom"", path=""yolov5/runs/train/exp36/weights/best.pt"",force_reload=True)
model.conf = 0.6

I have even tried splitting the available video into jpegs and running the model on individual images, saving the output and then merging the output images into a new video file.
that works perfectly, so the model is detecting something.
but as soon as I switch to video it seems to go back to nothing.
","Yes, You cannot see anything due to the none type of object return in the results.renders()/
You can change the code script like this
cap = cv2.VideoCapture('../video/1st/output.mp4')
while cap.isOpened():
    ret, frame = cap.read()

    results = model(frame)
    bboxes = results.xyxy[0].cpu().tolist()
    for bbox in bboxes:
      conf = f'{bbox[4]:.4f}' #Confidance of that prediction
      bbox = list(map(lambda x: int(x), bbox)) #To convert float to integer
      class_id = bbox[5] #Class_id 
      bbox =bbox[:4] 
      cv2.rectangle(frame,(bbox[0],bbox[1]),(bbox[2],bbox[3]),color=(255,255,255),thickness=3)
    cv2.imshow(""YOLO"", frame)
    if cv2.waitKey(10) & 0XFF == ord(""q""):
        break
cap.release()
cv2.destroyAllWindows()

and write the frames in the video
the full code should look like this
input_video_path = #Enter your video path
cap = cv2.VideoCapture(input_video_path)
fps = int(cap.get(cv2.CAP_PROP_FPS))
frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))

output_video_path = 'output_video.mp4' # Output video path
fourcc = cv2.VideoWriter_fourcc(*'XVID')
out = cv2.VideoWriter(output_video_path, fourcc, fps, (frame_width, frame_height))

model = #Here load your model
while True:
    ret, frame = cap.read()
    if not ret:
      break
    results = model(frame)
    bboxes = results.xyxy[0].cpu().tolist()
    for bbox in bboxes:
      conf = f'{bbox[4]:.4f}' #Confidance of that prediction
      bbox = list(map(lambda x: int(x), bbox)) #To convert float to integer
      class_id = bbox[5] #Class_id 
      bbox =bbox[:4] 
      cv2.rectangle(frame,(bbox[0],bbox[1]),(bbox[2],bbox[3]),color=(255,255,255),thickness=3)
    out.write(frame)
    #cv2_imshow(frame)
    if cv2.waitKey(10) & 0XFF == ord(""q""):
        break

cap.release()
out.release()

# Close all OpenCV windows
cv2.destroyAllWindows()

References:
https://docs.ultralytics.com/ 
https://docs.ultralytics.com/yolov5/ 
https://docs.ultralytics.com/yolov5/tutorials/pytorch_hub_model_loading/
",pytorch
using positional encoding in pytorch,"Is there any built-in positional encoding in pytorch? Basically, I want to be able to specify the dimension of the encoding, and then be able to get the i'th encoding for every i.
","There isn't, as far as I'm aware.
However, you can use an implementation from PyTorch's documentation:
class PositionalEncoding(nn.Module):

    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000):
        super().__init__()
        self.dropout = nn.Dropout(p=dropout)

        position = torch.arange(max_len).unsqueeze(1)
        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))
        pe = torch.zeros(max_len, 1, d_model)
        pe[:, 0, 0::2] = torch.sin(position * div_term)
        pe[:, 0, 1::2] = torch.cos(position * div_term)
        self.register_buffer('pe', pe)

    def forward(self, x: Tensor) -> Tensor:
        """"""
        Arguments:
            x: Tensor, shape ``[seq_len, batch_size, embedding_dim]``
        """"""
        x = x + self.pe[:x.size(0)]
        return self.dropout(x)

You can find it here:
",pytorch
keep training pytorch model on new data,"I'm working on a text classification task and have decided to use a PyTorch model for this purpose. The process mainly involves the following steps:

Load and process the text.
Use a TF-IDF Vectorizer.
Build the neural network and save the TF-IDF Vectorizer and model to predict new data.

However, every day I need to classify new comments and correct any wrong classifications.
Currently, my approach is to add the new comments with the correct classification to the dataset and retrain the entire model. This process is time-consuming, and the new comments can be lost during validation. I would like to create a new dataset with the newly classified texts and continue training over this new data (the new comments are classified manually, so each label is correct).
Using GPT and some online code, i write the desired process, however, im not sure if its working as expected, or im making some silly mistakes that should not happen.
So the mains questions are:

How could i check if the propossed way to solve this problem work as i expect?
What can i do with the vectorizer when it face new tokens, can i just do a .fit_transform() or i would loose the original vectorizer?

Here its the full training process:
import torch
from torch import nn
from torch.utils.data import Dataset, DataLoader, random_split
from sklearn.preprocessing import LabelEncoder
import polars as pl
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
import joblib

set1 = (
    pl
    .read_csv(
        ""set1.txt"",
        separator="";"",
        has_header=False,
        new_columns=[""text"",""label""]
    )
)

# since the dateset its unbalanced, im going to force to have more balance

fear_df = set1.filter(pl.col(""label"") == ""fear"")
joy_df = set1.filter(pl.col(""label"") == ""joy"").sample(n=2500)
sadness_df = set1.filter(pl.col(""label"") == ""sadness"").sample(n=2500)
anger_df = set1.filter(pl.col(""label"") == ""anger"")

train_df = pl.concat([fear_df,joy_df,sadness_df,anger_df])

""""""
The text its already clean, so im going to change the labels to numeric
and then split it on train, test ,val
""""""

label_mapping = {
    ""anger"": 0,
    ""fear"": 1,
    ""joy"": 2,
    ""sadness"": 3
}

train_mapped = (
    train_df
    .with_columns(
        pl.col(""label"").replace_strict(label_mapping, default=""other"").cast(pl.Int16)
    )
   
)

train_set, pre_Test = train_test_split(train_mapped,
                                    test_size=0.4,
                                    random_state=42,
                                    stratify=train_mapped[""label""])

test_set, val_set = train_test_split(pre_Test,
                                    test_size=0.5,
                                    random_state=42,
                                    stratify=pre_Test[""label""]) 

# Vectorize text data using TF-IDF
vectorizer = TfidfVectorizer(max_features=30000, ngram_range=(1, 2))

X_train_tfidf = vectorizer.fit_transform(train_set['text']).toarray()
X_val_tfidf = vectorizer.transform(val_set['text']).toarray()
X_test_tfidf = vectorizer.transform(test_set['text']).toarray()

y_train = train_set['label']
y_val = val_set['label']
y_test = test_set['label']

class TextDataset(Dataset):
    def __init__(self, texts, labels):
        self.texts = texts
        self.labels = labels
    
    def __len__(self):
        return len(self.texts)
    
    def __getitem__(self, idx):
        text = self.texts[idx]
        label = self.labels[idx]
        return text, label
    
train_dataset = TextDataset(X_train_tfidf, y_train)
val_dataset = TextDataset(X_val_tfidf, y_val)
test_dataset = TextDataset(X_test_tfidf, y_test)

batch_size = 32
train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=batch_size)
test_loader = DataLoader(test_dataset, batch_size=batch_size)

class TextClassificationModel(nn.Module):
    def __init__(self, input_dim, num_classes):
        super(TextClassificationModel, self).__init__()
        self.fc1 = nn.Linear(input_dim, 64)
        self.dropout1 = nn.Dropout(0.5)
        self.fc2 = nn.Linear(64, 32)
        self.dropout2 = nn.Dropout(0.5)
        self.fc3 = nn.Linear(32, num_classes)

    def forward(self, x):
        x = torch.relu(self.fc1(x))
        x = self.dropout1(x)
        x = torch.relu(self.fc2(x))
        x = self.dropout2(x)
        x = torch.softmax(self.fc3(x), dim=1)
        return x
    
input_dim = X_train_tfidf.shape[1]
model = TextClassificationModel(input_dim, 4)

# Define loss and optimizer
criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.Adamax(model.parameters())

# Training loop
num_epochs = 17
best_val_acc = 0.0
best_model_path = ""modelbest.pth""

for epoch in range(num_epochs):
    model.train()
    for texts, labels in train_loader:
        texts, labels = texts.float(), labels.long()
        outputs = model(texts)
        loss = criterion(outputs, labels)
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

    # Validation
    model.eval()
    correct, total = 0, 0
    with torch.no_grad():
        for texts, labels in val_loader:
            texts, labels = texts.float(), labels.long()
            outputs = model(texts)
            _, predicted = torch.max(outputs.data, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()
    val_acc = correct / total
    if val_acc > best_val_acc:
        best_val_acc = val_acc
        torch.save(model.state_dict(), best_model_path)

    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}, Val Acc: {val_acc:.4f}')

# Load the best model
model.load_state_dict(torch.load(best_model_path))

# Load the best model
model.load_state_dict(torch.load(best_model_path))

# Test the model
model.eval()
correct, total = 0, 0
with torch.no_grad():
    for texts, labels in test_loader:
        texts, labels = texts.float(), labels.long()
        outputs = model(texts)
        _, predicted = torch.max(outputs.data, 1)
        total += labels.size(0)
        correct += (predicted == labels).sum().item()
test_acc = correct / total
print(f'Test Acc: {test_acc:.3f}')


# Save the TF-IDF vectorizer
vectorizer_path = ""tfidf_vectorizer.pkl""
joblib.dump(vectorizer, vectorizer_path)

# Save the PyTorch model
model_path = ""text_classification_model.pth""
torch.save(model.state_dict(), model_path)


Proposed code:
import torch
import joblib
import polars as pl
from sklearn.model_selection import train_test_split
from torch import nn
from torch.utils.data import Dataset, DataLoader

# Load the saved TF-IDF vectorizer
vectorizer_path = ""tfidf_vectorizer.pkl""
vectorizer = joblib.load(vectorizer_path)

input_dim = len(vectorizer.get_feature_names_out())

class TextClassificationModel(nn.Module):
    def __init__(self, input_dim, num_classes):
        super(TextClassificationModel, self).__init__()
        self.fc1 = nn.Linear(input_dim, 64)
        self.dropout1 = nn.Dropout(0.5)
        self.fc2 = nn.Linear(64, 32)
        self.dropout2 = nn.Dropout(0.5)
        self.fc3 = nn.Linear(32, num_classes)

    def forward(self, x):
        x = torch.relu(self.fc1(x))
        x = self.dropout1(x)
        x = torch.relu(self.fc2(x))
        x = self.dropout2(x)
        x = torch.softmax(self.fc3(x), dim=1)
        return x
    
# Load the saved PyTorch model
model_path = ""text_classification_model.pth""
model = TextClassificationModel(input_dim, 4)
model.load_state_dict(torch.load(model_path))

# Map labels to numeric values
label_mapping = {""anger"": 0, ""fear"": 1, ""joy"": 2, ""sadness"": 3}
sentiments = [""fear"",""joy"",""sadness"",""anger""]

new_data = (
    pl
    .read_csv(
        ""set2.txt"",
        separator="";"",
        has_header=False,
        new_columns=[""text"",""label""]
    )
    .filter(pl.col(""label"").is_in(sentiments))
    .with_columns(
        pl.col(""label"").replace_strict(label_mapping, default=""other"").cast(pl.Int16)
    )
    
)
# Vectorize the new text data using the loaded TF-IDF vectorizer
X_new = vectorizer.transform(new_data['text']).toarray()
y_new = new_data['label']

class TextDataset(Dataset):
    def __init__(self, texts, labels):
        self.texts = texts
        self.labels = labels
    
    def __len__(self):
        return len(self.texts)
    
    def __getitem__(self, idx):
        text = self.texts[idx]
        label = self.labels[idx]
        return text, label

batch_size = 10
   
# Create DataLoader for the new training data
new_train_dataset = TextDataset(X_new, y_new)
new_train_loader = DataLoader(new_train_dataset, batch_size=batch_size, shuffle=True)

# Define loss and optimizer
criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.Adamax(model.parameters())

num_epochs = 5
new_best_model_path = ""modelbest.pth""
for epoch in range(num_epochs):
    model.train()
    for texts, labels in new_train_loader:
        texts, labels = texts.float(), labels.long()
        outputs = model(texts)
        loss = criterion(outputs, labels)
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        torch.save(model.state_dict(), new_best_model_path)
        
print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')

# Save the PyTorch model
new_best_model_path = ""new_moedl.pth""
torch.save(model.state_dict(), new_best_model_path)

The dataset can be found here
","use  pre-trained word embeddings like BertForSequenceClassification.  These embeddings can handle unseen tokens more gracefully since they map words to continuous vectors based on semantic meaning, reducing the impact of unseen words.
Model Training with BERT
import torch
from torch import nn, optim
from torch.utils.data import DataLoader, Dataset
from transformers import BertTokenizer, BertModel, BertForSequenceClassification
from transformers import Trainer, TrainingArguments
from sklearn.model_selection import train_test_split
import polars as pl

# Load and prepare data
set1 = pl.read_csv(""set1.txt"", separator="";"", has_header=False, new_columns=[""text"", ""label""])

# Balance dataset
fear_df = set1.filter(pl.col(""label"") == ""fear"")
joy_df = set1.filter(pl.col(""label"") == ""joy"").sample(n=2500)
sadness_df = set1.filter(pl.col(""label"") == ""sadness"").sample(n=2500)
anger_df = set1.filter(pl.col(""label"") == ""anger"")
train_df = pl.concat([fear_df, joy_df, sadness_df, anger_df])

label_mapping = {""anger"": 0, ""fear"": 1, ""joy"": 2, ""sadness"": 3}
train_df = train_df.with_columns(pl.col(""label"").replace_strict(label_mapping, default=""other"").cast(pl.Int16))

# Split dataset
train_set, test_val_set = train_test_split(train_df, test_size=0.4, random_state=42, stratify=train_df[""label""])
test_set, val_set = train_test_split(test_val_set, test_size=0.5, random_state=42, stratify=test_val_set[""label""])

# Dataset class
class TextDataset(Dataset):
    def __init__(self, texts, labels, tokenizer, max_length=128):
        self.texts = texts
        self.labels = labels
        self.tokenizer = tokenizer
        self.max_length = max_length

    def __len__(self):
        return len(self.texts)

    def __getitem__(self, idx):
        text = self.texts[idx]
        label = self.labels[idx]
        encoding = self.tokenizer.encode_plus(
            text,
            add_special_tokens=True,
            max_length=self.max_length,
            padding='max_length',
            truncation=True,
            return_tensors='pt'
        )
        return {
            'input_ids': encoding['input_ids'].flatten(),
            'attention_mask': encoding['attention_mask'].flatten(),
            'labels': torch.tensor(label, dtype=torch.long)
        }

# Initialize tokenizer and datasets
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
train_dataset = TextDataset(train_set['text'], train_set['label'], tokenizer)
val_dataset = TextDataset(val_set['text'], val_set['label'], tokenizer)
test_dataset = TextDataset(test_set['text'], test_set['label'], tokenizer)

# Initialize BERT model for classification
model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=4)

# Training arguments
training_args = TrainingArguments(
    output_dir='./results',
    num_train_epochs=3,
    per_device_train_batch_size=16,
    per_device_eval_batch_size=16,
    evaluation_strategy='epoch',
    save_strategy='epoch',
    logging_dir='./logs',
    learning_rate=2e-5,
    load_best_model_at_end=True
)

# Define Trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=val_dataset
)

# Train model
trainer.train()

# Evaluate model
results = trainer.evaluate(test_dataset)
print(f""Test Accuracy: {results['eval_accuracy']:.4f}"")

# Save the model and tokenizer
model.save_pretrained(""saved_model"")
tokenizer.save_pretrained(""saved_tokenizer"")

Incremental training with least effort
# Load the saved model and tokenizer
model = BertForSequenceClassification.from_pretrained(""saved_model"")
tokenizer = BertTokenizer.from_pretrained(""saved_tokenizer"")

# Load new data
new_data = (
    pl.read_csv(""set2.txt"", separator="";"", has_header=False, new_columns=[""text"", ""label""])
    .filter(pl.col(""label"").is_in([""fear"", ""joy"", ""sadness"", ""anger""]))
    .with_columns(pl.col(""label"").replace_strict(label_mapping, default=""other"").cast(pl.Int16))
)

# Create new dataset
new_dataset = TextDataset(new_data['text'], new_data['label'], tokenizer)

# Update training arguments for incremental training
new_training_args = TrainingArguments(
    output_dir='./results_incremental',
    num_train_epochs=2,  # Fewer epochs since it's incremental
    per_device_train_batch_size=16,
    evaluation_strategy='epoch',
    logging_dir='./logs_incremental',
    learning_rate=2e-5,
    load_best_model_at_end=True
)

# Define new trainer
new_trainer = Trainer(
    model=model,
    args=new_training_args,
    train_dataset=new_dataset,
    eval_dataset=val_dataset  # Validate on previous validation set
)

# Train on new data
new_trainer.train()

# Evaluate after retraining
new_results = new_trainer.evaluate(test_dataset)
print(f""Test Accuracy After Incremental Training: {new_results['eval_accuracy']:.4f}"")

# Save the updated model
model.save_pretrained(""saved_model_incremental"")

",scikit-learn
how to pass gpus all option to docker with go sdk,"I have seen how to do some basic commands such as running a container, pulling images, listing images, etc from the SDK examples.
I am working on a project where I need to use the GPU from within the container.
My system has GPU, I have installed the drivers, and I have also installed the nvidia-container-runtime.
If we remove Go SDK from the scene for a moment, I can run the following command to get the nvidia-smi output on my host system:
docker run -it --rm --gpus all nvidia/cuda:10.0-base nvidia-smi

I have to do this via the SDK. Here is the code to start with. This code prints ""hello world"". But in actual I will be running nvidia-smi command at that place:
package main

import (
    ""context""
    ""os""

    ""github.com/docker/docker/api/types""
    ""github.com/docker/docker/api/types/container""
    ""github.com/docker/docker/client""
    ""github.com/docker/docker/pkg/stdcopy""
)

func main() {
    ctx := context.Background()
    cli, err := client.NewClientWithOpts(client.FromEnv, client.WithAPIVersionNegotiation())
    if err != nil {
        panic(err)
    }

    RunContainer(ctx, cli)
}

func RunContainer(ctx context.Context, cli *client.Client) {
    reader, err := cli.ImagePull(ctx, ""nvidia/cuda:10.0-base"", types.ImagePullOptions{})
    if err != nil {
        panic(err)
    }

    defer reader.Close()
    // io.Copy(os.Stdout, reader)

    resp, err := cli.ContainerCreate(ctx, &container.Config{
        Image: ""nvidia/cuda:10.0-base"",
        Cmd:   []string{""echo"", ""hello world""},
        // Tty:   false,
    }, nil, nil, nil, """")

    if err != nil {
        panic(err)
    }

    if err := cli.ContainerStart(ctx, resp.ID, types.ContainerStartOptions{}); err != nil {
        panic(err)
    }

    statusCh, errCh := cli.ContainerWait(ctx, resp.ID, container.WaitConditionNotRunning)

    select {
    case err := <-errCh:
        if err != nil {
            panic(err)
        }
    case <-statusCh:
    }

    out, err := cli.ContainerLogs(ctx, resp.ID, types.ContainerLogsOptions{ShowStdout: true})
    if err != nil {
        panic(err)
    }

    stdcopy.StdCopy(os.Stdout, os.Stderr, out)
}

","see: https://github.com/docker/cli/blob/9ac8584acfd501c3f4da0e845e3a40ed15c85041/cli/command/container/opts.go#L594
import ""github.com/docker/cli/opts""

// ...

gpuOpts := opts.GpuOpts{}
gpuOpts.Set(""all"")

resp, err := cli.ContainerCreate(ctx, &container.Config{
    Image: ""nvidia/cuda:10.0-base"",
    Cmd:   []string{""echo"", ""hello world""},
    // Tty:   false,
}, &container.HostConfig{Resources: container.Resources{DeviceRequests: gpuOpts.Value()}}, nil, nil, """")

",tensorflow
what does finetuning a multilingual checkpoint mean,"I'm fine-tuning a SetFit model on a French dataset and following the guide in huggingface. They mention this point on the site that I didn't quite understand

""🌎 Multilingual support: SetFit can be used with any Sentence
Transformer on the Hub, which means you can classify text in multiple
languages by simply fine-tuning a multilingual checkpoint.""

Does that mean I must find an already finetuned SetFit model in French when loading the model? As in replace ""paraphrase-mpnet-base-v2"" below with a French one?
model = SetFitModel.from_pretrained(""sentence-transformers/paraphrase-mpnet-base-v2"")

","What the point in the guide suggests is that multilingual models fine-tuned using SetFit method generalize well even on languages they did not see during the SetFit fine-tuning process. This seems to be generally true for multilingual language models but it probably does not do any damage to mention it explicitly, particularly when discussing SetFit, which is a method which usually works with a very small dataset (i.e. the dataset that might not be multilingual).
The finding is supported by the paper mentioned in the guide, where researchers show that model fine-tuned on English data using SetFit performs well on variety of languages (see table 4).
What I would take from it is this: if you fine-tune multilingual checkpoint (e.g. sentence-transformers/paraphrase-multilingual-mpnet-base-v2) and fine-tune it on French, it will perform well on French and probably will also perform well on other languages. If you plan to use the fine-tuned model only on French texts, you certainly can and try to fine-tune a specifically French model - however, it's certainly not true that you must do this.
However, if there exists a specifically French sentence transformer and you want to use your model only on French texts, I would recommend using the French model. Not because you must, but because it might perform better than the multilingual model.
",fine-tune
cvinteger vs predefined splits in gridsearchcv,"What's the difference between setting CV=some integer vs cv=PredefinedSplit(test_fold=your_test_fold)?
Is there any advantage of one over the other? Does CV=some integer sets the splits randomly?
","Specifying an integer will produce kfold cross-validation without shuffling, as described in the documentation for sklearn.model_selection.KFold. Shuffling before splitting may or may not be preferred; if your data is sorted, shuffling is necessary to randomize the distribution of samples, while if the samples are simply correlated due to spatial or temporal sampling effects, shuffling may provide an optimistic view of performance.
I would avoid using PredefinedSplit unless you have a very good reason to predefine your splits. There are other CV generators that can probably meet your needs, like StratifiedKFold if you want to maintain your class distribution (for example.)
",cross-validation
python multiprocessing on multiple cpus gpus,"I have 8 GPUs, 64 CPU cores (multiprocessing.cpu_count()=64)
I am trying to get inference of multiple video files using a deep learning model. I want some files to get processed on each of the 8 GPUs. For each GPU, I want a different 6 CPU cores utilized.
Below python filename: inference_{gpu_id}.py
Input1: GPU_id
Input2: Files to process for GPU_id
from torch.multiprocessing import Pool, Process, set_start_method
try:
     set_start_method('spawn', force=True)
except RuntimeError:
    pass

model = load_model(device='cuda:' + gpu_id) 

def pooling_func(file):
    preds = []
    cap = cv2.VideoCapture(file)
    while(cap.isOpened()):
          ret, frame = cap.read()
          count += 1
          if ret == True:
                frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
                pred = model(frame)[0]
                preds.append(pred)
          else:
                break
    cap.release()
    np.save(file[:-4]+'.npy', preds)

def process_files():

    # all files to process on gpu_id
    files = np.load(gpu_id + '_files.npy') 

    # I am hoping to use 6 cores for this gpu_id, 
    # and a different 6 cores for a different GPU id
    pool = Pool(6) 

    r = list(tqdm(pool.imap(pooling_func, files), total = len(files)))
    pool.close()
    pool.join()

if __name__ == '__main__':
    import multiprocessing
    multiprocessing.freeze_support()
    process_files()

I am hoping to run inference_{gpu_id}.py files on all GPUs simultaneously
Currently, I am able to successfully run it on one GPU, 6 cores, But when I try to run it on all GPUs together, only GPU 0 runs, all others stop giving below error message.
RuntimeError: CUDA error: invalid device ordinal.
The script I am running:
CUDA_VISIBLE_DEVICES=0 inference_0.py

CUDA_VISIBLE_DEVICES=1 inference_1.py

...

CUDA_VISIBLE_DEVICES=7 inference_7.py

","Consider this, if you are not using the CUDA_VISIBLE_DEVICES flag, then all GPUs will be available to your PyTorch process. This means torch.cuda.device_count will return 8 (assuming your version setup is valid). And you will be able to get access to each one of those 8 GPUs with torch.device, via torch.device('cuda:0'), torch.device('cuda:1'), ..., and torch.device('cuda:8').
Now if you are only planning on using one and want to restrict your process to one. then CUDA_VISIBLE_DEVICES=i (where i is the device ordinal) will make it so. In this case torch.cuda will only have access to a single device through torch.device('cuda:0'). It doesn't matter what the actual device ordinal is, the way you access it is through torch.device('cuda:0').
If you allow access to more than one device: let's say n°0, n°4, and n°2, then you would use CUDA_VISIBLE_DEVICES=0,4,2. Consequently you refer to your cuda devices via d0 = torch.device('cuda:0'), d1 = torch.device('cuda:1'), and d2 = torch.device('cuda:2'). In the same order as you defined them with the flag, i.e.:

d0 -> GPU n°0, d1 -> GPU n°4, and d2 -> GPU n°2.

This makes it so you can use the same code and run it on different GPUs without having to change the underlying code where you are referring to the device ordinal.
In summary, what you need to look at is the number of devices you need to run your code. In your case: 1 is enough. You will refer to it with torch.device('cuda:0'). When running your code, however, you will need to specify what that cuda:0 device is, with the flag:
> CUDA_VISIBLE_DEVICES=0 inference.py
> CUDA_VISIBLE_DEVICES=1 inference.py
  ...
> CUDA_VISIBLE_DEVICES=7 inference.py

Do note 'cuda' will default to 'cuda:0'.
",pytorch
inconsistent numbers of samples error in multiclass  multilabel machine learning model,"I have a OneVsOne model running fine with textual feature and target fields. To progress to a multi-class model (i.e. with multiple textual feature fields), I believe OneVsRest with Logistic Regression is suitable.
However, when I use the following pipeline:
model = Pipeline([
    ('vect', CountVectorizer()),
    ('tfidfT', TfidfTransformer()),
    ('clf', OneVsRestClassifier(LogisticRegression(), n_jobs = 1))
    ])

I am getting the following error when trying to run a OneVsRest classifier with Logistic Regression:
ValueError: Found input variables with inconsistent numbers of samples: [3, 224]

The feature fields are in a pandas dataframe of 224 rows and the target field is a pandas series of length 224. There are no nulls in the data.
Here is the full traceback:
ValueError                                Traceback (most recent call last)
File c:\Users\u363028\OneDrive - IBERDROLA S.A\Tools\sklearn\defect_autocategorisation_main9.py:127
    119 model = Pipeline([
    120     ('vect', CountVectorizer()),
    121     ('tfidfT', TfidfTransformer()),
    122     ('clf', OneVsRestClassifier(LogisticRegression(), n_jobs = 1))
    123     ])
    124 #model = OneVsRestClassifier(LogisticRegression())
    125 
    126 # Initialize the classifier
--> 127 model.fit(X,y)
    128 predicted = model.predict(X_test)
    129 #predicted = model.predict(X_test, Y_test)
    130    
    131 # creating a confusion matrix  
   (...)
    140 
    141 # Generate classification report

File c:\Users\u363028\OneDrive - IBERDROLA S.A\Tools\sklearn\sk_learn\Lib\site-packages\sklearn\base.py:1473, in _fit_context.<locals>.decorator.<locals>.wrapper(estimator, *args, **kwargs)
   1466     estimator._validate_params()
   1468 with config_context(
   1469     skip_parameter_validation=(
   1470         prefer_skip_nested_validation or global_skip_validation
   1471     )
   1472 ):
-> 1473     return fit_method(estimator, *args, **kwargs)

File c:\Users\u363028\OneDrive - IBERDROLA S.A\Tools\sklearn\sk_learn\Lib\site-packages\sklearn\pipeline.py:473, in Pipeline.fit(self, X, y, **params)
    471     if self._final_estimator != ""passthrough"":
    472         last_step_params = routed_params[self.steps[-1][0]]
--> 473         self._final_estimator.fit(Xt, y, **last_step_params[""fit""])
    475 return self

File c:\Users\u363028\OneDrive - IBERDROLA S.A\Tools\sklearn\sk_learn\Lib\site-packages\sklearn\base.py:1473, in _fit_context.<locals>.decorator.<locals>.wrapper(estimator, *args, **kwargs)
   1466     estimator._validate_params()
   1468 with config_context(
   1469     skip_parameter_validation=(
   1470         prefer_skip_nested_validation or global_skip_validation
   1471     )
   1472 ):
-> 1473     return fit_method(estimator, *args, **kwargs)

File c:\Users\u363028\OneDrive - IBERDROLA S.A\Tools\sklearn\sk_learn\Lib\site-packages\sklearn\multiclass.py:370, in OneVsRestClassifier.fit(self, X, y, **fit_params)
    366 columns = (col.toarray().ravel() for col in Y.T)
    367 # In cases where individual estimators are very fast to train setting
    368 # n_jobs > 1 in can results in slower performance due to the overhead
    369 # of spawning threads.  See joblib issue #112.
--> 370 self.estimators_ = Parallel(n_jobs=self.n_jobs, verbose=self.verbose)(
    371     delayed(_fit_binary)(
    372         self.estimator,
    373         X,
    374         column,
    375         fit_params=routed_params.estimator.fit,
    376         classes=[
    377             ""not %s"" % self.label_binarizer_.classes_[i],
    378             self.label_binarizer_.classes_[i],
    379         ],
    380     )
    381     for i, column in enumerate(columns)
    382 )
    384 if hasattr(self.estimators_[0], ""n_features_in_""):
    385     self.n_features_in_ = self.estimators_[0].n_features_in_

File c:\Users\u363028\OneDrive - IBERDROLA S.A\Tools\sklearn\sk_learn\Lib\site-packages\sklearn\utils\parallel.py:74, in Parallel.__call__(self, iterable)
     69 config = get_config()
     70 iterable_with_config = (
     71     (_with_config(delayed_func, config), args, kwargs)
     72     for delayed_func, args, kwargs in iterable
     73 )
---> 74 return super().__call__(iterable_with_config)

File c:\Users\u363028\OneDrive - IBERDROLA S.A\Tools\sklearn\sk_learn\Lib\site-packages\joblib\parallel.py:1918, in Parallel.__call__(self, iterable)
   1916     output = self._get_sequential_output(iterable)
   1917     next(output)
-> 1918     return output if self.return_generator else list(output)
   1920 # Let's create an ID that uniquely identifies the current call. If the
   1921 # call is interrupted early and that the same instance is immediately
   1922 # re-used, this id will be used to prevent workers that were
   1923 # concurrently finalizing a task from the previous call to run the
   1924 # callback.
   1925 with self._lock:

File c:\Users\u363028\OneDrive - IBERDROLA S.A\Tools\sklearn\sk_learn\Lib\site-packages\joblib\parallel.py:1847, in Parallel._get_sequential_output(self, iterable)
   1845 self.n_dispatched_batches += 1
   1846 self.n_dispatched_tasks += 1
-> 1847 res = func(*args, **kwargs)
   1848 self.n_completed_tasks += 1
   1849 self.print_progress()

File c:\Users\u363028\OneDrive - IBERDROLA S.A\Tools\sklearn\sk_learn\Lib\site-packages\sklearn\utils\parallel.py:136, in _FuncWrapper.__call__(self, *args, **kwargs)
    134     config = {}
    135 with config_context(**config):
--> 136     return self.function(*args, **kwargs)

File c:\Users\u363028\OneDrive - IBERDROLA S.A\Tools\sklearn\sk_learn\Lib\site-packages\sklearn\multiclass.py:93, in _fit_binary(estimator, X, y, fit_params, classes)
     91 else:
     92     estimator = clone(estimator)
---> 93     estimator.fit(X, y, **fit_params)
     94 return estimator

File c:\Users\u363028\OneDrive - IBERDROLA S.A\Tools\sklearn\sk_learn\Lib\site-packages\sklearn\base.py:1473, in _fit_context.<locals>.decorator.<locals>.wrapper(estimator, *args, **kwargs)
   1466     estimator._validate_params()
   1468 with config_context(
   1469     skip_parameter_validation=(
   1470         prefer_skip_nested_validation or global_skip_validation
   1471     )
   1472 ):
-> 1473     return fit_method(estimator, *args, **kwargs)

File c:\Users\u363028\OneDrive - IBERDROLA S.A\Tools\sklearn\sk_learn\Lib\site-packages\sklearn\linear_model\_logistic.py:1223, in LogisticRegression.fit(self, X, y, sample_weight)
   1220 else:
   1221     _dtype = [np.float64, np.float32]
-> 1223 X, y = self._validate_data(
   1224     X,
   1225     y,
   1226     accept_sparse=""csr"",
   1227     dtype=_dtype,
   1228     order=""C"",
   1229     accept_large_sparse=solver not in [""liblinear"", ""sag"", ""saga""],
   1230 )
   1231 check_classification_targets(y)
   1232 self.classes_ = np.unique(y)

File c:\Users\u363028\OneDrive - IBERDROLA S.A\Tools\sklearn\sk_learn\Lib\site-packages\sklearn\base.py:650, in BaseEstimator._validate_data(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)
    648         y = check_array(y, input_name=""y"", **check_y_params)
    649     else:
--> 650         X, y = check_X_y(X, y, **check_params)
    651     out = X, y
    653 if not no_val_X and check_params.get(""ensure_2d"", True):

File c:\Users\u363028\OneDrive - IBERDROLA S.A\Tools\sklearn\sk_learn\Lib\site-packages\sklearn\utils\validation.py:1320, in check_X_y(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)
   1301 X = check_array(
   1302     X,
   1303     accept_sparse=accept_sparse,
   (...)
   1315     input_name=""X"",
   1316 )
   1318 y = _check_y(y, multi_output=multi_output, y_numeric=y_numeric, estimator=estimator)
-> 1320 check_consistent_length(X, y)
   1322 return X, y

File c:\Users\u363028\OneDrive - IBERDROLA S.A\Tools\sklearn\sk_learn\Lib\site-packages\sklearn\utils\validation.py:457, in check_consistent_length(*arrays)
    455 uniques = np.unique(lengths)
    456 if len(uniques) > 1:
--> 457     raise ValueError(
    458         ""Found input variables with inconsistent numbers of samples: %r""
    459         % [int(l) for l in lengths]
    460     )

ValueError: Found input variables with inconsistent numbers of samples: [3, 224]

There is this similar Stack Overflow question:
ValueError: Number of features of the model must match the input
but neither the suggestions in this nor in any of the few other similar questions work for me.
Although my data is textual, for info the above pipeline causes the same error when using the Iris dataset but it completes successfully when just running the classifier (i.e. omitting the vectorizer and transformer). However, just running the classifier on my textual data doesn't work, giving the following expected error:
ValueError: could not convert string to float: 'Jacket'

I am aware of OneHot encoding but this 'inconsistent numbers of samples' problem seems irrespective of any encoding issue and I would like to solve this before tackling any other issues.
Edit 22/10/24:
Here is a Minimal Reproducible Example geared to use the iris dataset:
import pandas as pd
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.feature_extraction.text import TfidfTransformer
from sklearn.pipeline import Pipeline
from sklearn.model_selection import train_test_split
from sklearn.multiclass import OneVsRestClassifier
from sklearn.linear_model import LogisticRegression

# Read in the dataset to train the model
training_data = pd.read_csv('iris_dataset.csv')   
print(training_data) 

# Load feature data
X = training_data[['sepal.length', 'sepal.width','petal.length','petal.width']]
    
# Load target data
y = training_data['variety']

# Split training data into training and test portions
X_train, X_test, y_train, y_test \
    = train_test_split(X, y, test_size=0.5, random_state=42)
    
# Create the pipeline composed of vectoriser, transformer and classifier
model = Pipeline([
    ('vect', CountVectorizer()),
    ('tfidfT', TfidfTransformer()),
    ('clf', OneVsRestClassifier(LogisticRegression(), n_jobs = 1))
    ])

# Initialize the classifier
model.fit(X,y)
predicted = model.predict(X_test)

Edit 23/10/24: Here is the MRE again, self-contained with textual data inputs:
import pandas as pd
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.feature_extraction.text import TfidfTransformer
from sklearn.pipeline import Pipeline
from sklearn.model_selection import train_test_split
from sklearn.multiclass import OneVsRestClassifier
from sklearn.linear_model import LogisticRegression

# Read in the dataset to train the model
training_data = pd.read_csv('iris_dataset.csv')   
print(training_data)

training_data = pd.DataFrame({
                'Location': ['Structure', 'Stucture', 'Structure', 'Access systems'],\
                'Component': ['Mid bay brace12', 'Mid bay brace10', 'Mid bay brace07', 'First stage ladder'],\
                'Defect Description': ['Surface corrosion', 'Coating delamination with minor surface corrosion', 'Corrosion', 'Entangled rope'],\
                'Failure Mode': ['Corrosion', 'Corrosion','Corrosion', 'Debris']
                })


# Load feature data
X = training_data[['Location', 'Component','Defect Description']]
    
# Load target data
y = training_data['Failure Mode']

# Split training data into training and test portions
X_train, X_test, y_train, y_test \
    = train_test_split(X, y, test_size=0.5, random_state=42)
    
# Create the pipeline composed of vectoriser, transformer and classifier
model = Pipeline([
    ('vect', CountVectorizer()),
    ('tfidfT', TfidfTransformer()),
    ('clf', OneVsRestClassifier(LogisticRegression(), n_jobs = 1))
    ])

# Initialize the classifier
model.fit(X,y)
predicted = model.predict(X_test)

","I'll expand this later, but the main solution is to use a ColumnTransformer to run separate text transformers per column (I've also consolidated CountVectorizer+TfidfTransformer=TfidfVectorizer):
preproc = ColumnTransformer([
    (col+""_tfidf"", TfidfVectorizer(), col)
    for col in X.columns
])

model = Pipeline([
    (""preproc"", preproc),
    ('clf', LogisticRegression()),
])

model.fit(X,y)

",scikit-learn
how can i get the history of the different fits when using cross vaidation over a kerasregressor,"I have a regression problem and I am using a keras fully connected layer to model my problem. I am using cross_val_score and my question is: how can I extract the model and the history of each train/validation combination the cross_val_score does?
Assuming this example:
from sklearn import datasets
from sklearn.model_selection import cross_val_score, KFold
from keras.models import Sequential
from keras.layers import Dense
from keras.wrappers.scikit_learn import KerasRegressor
seed = 1

diabetes = datasets.load_diabetes()
X = diabetes.data[:150]
y = diabetes.target[:150]

def baseline_model():
    model = Sequential()
    model.add(Dense(10, input_dim=10, activation='relu'))
    model.add(Dense(1))
    model.compile(loss='mean_squared_error', optimizer='adam')
    return model

estimator = KerasRegressor(build_fn=baseline_model, nb_epoch=100, batch_size=100, verbose=False)
kfold = KFold(n_splits=10, random_state=seed)
results = cross_val_score(estimator, X, y, cv=kfold)
print(""Results: %.2f (%.2f) MSE"" % (results.mean(), results.std()))

My understanding is that I only get the overall mse over each fold, so to say.
But I want to compare the train to validation mse over the epochs of the model for each fold, i.e. for 10 in this case.
When not using kfold, but simple train/validation split, then one can do:
hist = model.fit(X_tr, y_tr, validation_data=val_data,
                  epochs=100, batch_size=100,
                  verbose=1)

plt.plot(history.history['loss'])
plt.plot(history.history['loss'])

This would return a plot representing the evolution of the mse w.r.t. to the epochs for the train and validation datasets, allowing to spot over/underfitting.
How to do this for each fold when using cross validation?
","You can go for a ""manual"" CV procedure, and plot the loss (or any other available metric you might want to use) for each fold, i.e. something like this:
from sklearn.metrics import mean_squared_error
cv_mse = []

for train_index, val_index in kfold.split(X):
    history = estimator.fit(X[train_index], y[train_index])
    pred = estimator.predict(X[val_index])
    err = mean_squared_error(y[val_index], pred)
    cv_mse.append(err)
    plt.plot(history.history['loss'])

In which case, the cv_mse list will contain the final MSE for each fold, and you also get the respective plots for its evolution per epoch for each fold.
",cross-validation
what is the difference between various backends in torchnnattentionsdpbackend and what do they mean,"In the pytorch docs on SDPBackend there are a few enums available to be used with the context manager,
ERROR: An error occurred when trying to determine the backend.
MATH: The math backend for scaled dot product attention.
FLASH_ATTENTION: The flash attention backend for scaled dot product attention.
EFFICIENT_ATTENTION: The efficient attention backend for scaled dot product attention.
CUDNN_ATTENTION: The cuDNN backend for scaled dot product attention.
What do they mean and how are they different?
What exactly is the EFFICIENT ATTENTION backend? And another is I checked with  torch.backends.cuda.flash_sdp_enabled() on a machine without GPU and it is true but isn't flash attention only supposed to be for GPU's and it is based on using GPU cache memory? Is efficient attention just flash attention 2?
","MATH is the pytorch C++ attention implementation
FLASH_ATTENTION is the attention implementation from the flash attention paper
EFFICIENT_ATTENTION is the implementation from the facebook xformers library
CUDNN_ATTENTION is the implementation from the Nvidia CuDNN library
You can read more about the differences here
",pytorch
implementing gridsearchcv with scorer for leave one out crossvalidation,"I am attempting to implement scikit-learn's GridSearchCV for Gaussian Process Regression (GPR). I'm using a small dataset of ~200 points, and would like to use LOOCV as a performance evaluator for my model. My setup is:
from sklearn.model_selection import *
from sklearn.ensemble import *
from sklearn.gaussian_process import *

param_grid = {
    'kernel':[kernels.RBF(),kernels.Matern(length_scale=0.1)],
    'n_restarts_optimizer':[5,10,20,25],
    'random_state':[30]
}
res_GPR = GridSearchCV(estimator=GaussianProcessRegressor(),param_grid=param_grid,cv=LeaveOneOut(),verbose=20,n_jobs=-1)
res_GPR.fit(X,y)

where X and y are my data points and target values respectively.
I know that the scoring method returned by GPR is r^2, which is undefinable for the LOOCV case (since there is only one test element) - this is verified by obtaining NaN for the .best_score_ attribute of the fitted model.
As such, I would like the model to be scored with just the Root Mean Squared Error (RMSE) for each test case, averaged over all the iterations. How can I do that?
","GridSearchCV includes a scoring argument, which you may use to set your score to negative RMSE:
res_GPR = GridSearchCV(estimator=GaussianProcessRegressor(),
                       param_grid=param_grid,
                       cv=LeaveOneOut(),
                       verbose=20,
                       n_jobs=-1, 
                       scoring = 'neg_root_mean_squared_error')

See the documentation and the list of available scores for more.
",cross-validation
how to extend the vocabulary of a pretrained transformer model,"I would like to extend a zero-shot text classification (NLI) model's vocabulary, to include domain-specific vocabulary or just to keep it up-to-date. For example, I would like the model to know the names of the latest COVID-19 variants are related to the topic 'Healthcare'.
I've added the tokens to the tokenizer and resized the token embeddings. However, I don't know how to finetune the weights in the embedding layer, as suggested here.
To do the finetuning, can I use simply use texts containing a mixture of new vocabulary and existing vocabulary, and have the tokenizer recognise the relations between tokens through co-occurrences in an unsupervised fashion?
Any help is appreciated, thank you!
","If you resized the corresponding embedding weights with resize_token_embeddings, they will be initialised randomly.
Technically, you can fine-tune the model on your target task (NLI, in your case), without touching the embedding weights. In practice, it will be harder for your model to learn anything meaningful about the newly added tokens, since their embeddings are randomly initialised.
To learn the embedding weights you can do further pre-training, before fine-tuning on the target task. This is done by training the model on the pre-training objective(s) (such as Masked Language Modelling). Pre-training is more expensive than fine-tuning of course, but remember that you aren't pre-training from scratch, since you start pre-training from the checkpoint of the already pre-trained model. Therefore, the number of epochs/steps will be significantly less than what was used in the original pre-training setup.
When doing pre-training it will be beneficial to include in-domain documents, so that it can learn the newly added tokens. Depending on whether you want the model to be more domain specific or remain varied so as to not ""forget"" any previous domains, you might also want to include documents from a variety of domains.
The Don't Stop Pretraining paper might also be an interesting reference, which delves into specifics regarding the type of data used as well as training steps.
",fine-tune
what is the rationale behind transformedtargetregressor always cloning the given regressor and how to prevent this behavior,"The docs of sklearn.compose.TransformedTargetRegressor state that:

regressor object, default=None
Regressor object such as derived from RegressorMixin. This regressor will automatically be cloned each time prior to fitting. If regressor is None, LinearRegression is created and used.

What is the rationale behind cloning the given regressor each time prior to fitting? Why would this be useful?
This behavior prevents, for example, the following code from working:
import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestRegressor
from sklearn.pipeline import Pipeline
from sklearn.compose import TransformedTargetRegressor


X = np.random.default_rng(seed=1).normal(size=(100,3))
y = np.random.default_rng(seed=1).normal(size=100)

model = RandomForestRegressor()
pipeline = Pipeline(
    steps=[
        ('normalize', StandardScaler()),
        ('model', model),
    ],
)
tt = TransformedTargetRegressor(regressor=pipeline, transformer=StandardScaler())
tt.fit(X, y)

print(model.feature_importances_)

It results in:
Traceback (most recent call last):
  File ""/tmp/test.py"", line 21, in <module>
    print(model.feature_importances_)
[...]
sklearn.exceptions.NotFittedError: This RandomForestRegressor instance is not fitted yet. Call 'fit' with appropriate arguments before using this estimator.

which is not surprising given that the model object is cloned by the TransformedTargetRegressor.
So, is there a way to prevent this cloning behavior and make the above code work?
","All sklearn meta-estimators (except Pipeline) clone their base estimators; I can't confidently answer why the developers chose that paradigm.
But the fitted base estimators are always made available in new attributes: instead of
model.feature_importances_

use
tt.regressor_['model'].feature_importances_

",scikit-learn
crossvalidation of neural network how to treat the number of epochs,"I'm implementing a pytorch neural network (regression) and want to identify the best network topology, optimizer etc.. I use cross validation, because I have x databases of measurements and I want to evaluate whether I can train a neural network with a subset of the x databases and apply the neural network to the unseen databases. Therefore, I also introduce a test database, which I doesn't use in the phase of the hyperparameter identification.
I am confused on how to treat the number of epochs in cross validation, e.g. I have a number of epochs = 100. There are two options:

The number of epochs is a hyperparameter to tune. In each epoch, the mean error across all cross validation iterations is determined. After models are trained with all network topologies, optimizers etc. the model with the smallest mean error is determined and has parameters like: -network topology: 1
-optimizer: SGD
-number of epochs: 54
To calculate the performance on the test set, a model is trained with exactly these parameters (number of epochs = 54) on the training and the validation data. Then it is applied and evaluated on the test set.
The number of epochs is NOT a hyperparameter to tune. Models are trained with all the network topologies, optimizers etc. For each model, the number of epochs, where the error is the smallest, is used. The models are compared and the best model can be determined with parameters like:
-network topology: 1 
-optimizer: SGD
To calculate the performance on the test data, a “simple” training and validation split is used (e.g. 80-20). The model is trained with the above parameters and 100 epochs on the training and validation data. Finally, a model with a number of epochs yielding the smallest validation error, is evaluated on the test data.

Which option is the correct or the better one?
","The number of epochs is better not to be fine-tuned.
Option 2 is a better option.
Actually, if the # of epochs is fixed, you need not to have validation set. Validation set gives you the optimal epoch of the saved model.
",cross-validation
why is numpy native on m1 max greatly slower than on old intel i5,"I just got my new MacBook Pro with M1 Max chip and am setting up Python. I've tried several combinational settings to test speed - now I'm quite confused. First put my questions here:

Why python run natively on M1 Max is greatly (~100%) slower than on my old MacBook Pro 2016 with Intel i5?
On M1 Max, why there isn't significant speed difference between native run (by miniforge) and run via Rosetta (by anaconda) - which is supposed to be slower ~20%?
On M1 Max and native run, why there isn't significant speed difference between conda installed Numpy and TensorFlow installed Numpy - which is supposed to be faster?
On M1 Max, why run in PyCharm IDE is constantly slower ~20% than run from terminal, which doesn't happen on my old Intel Mac.

Evidence supporting my questions is as follows:

Here are the settings I've tried:
1. Python installed by

Miniforge-arm64, so that python is natively run on M1 Max Chip. (Check from Activity Monitor, Kind of python process is Apple).
Anaconda. Then python is run via Rosseta. (Check from Activity Monitor, Kind of python process is Intel).

2. Numpy installed by

conda install numpy: numpy from original conda-forge channel, or pre-installed with anaconda.
Apple-TensorFlow: with python installed by miniforge, I directly install tensorflow, and numpy will also be installed. It's said that, numpy installed in this way is optimized for Apple M1 and will be faster. Here is the installation commands:

conda install -c apple tensorflow-deps
python -m pip install tensorflow-macos
python -m pip install tensorflow-metal

3. Run from

Terminal.
PyCharm (Apple Silicon version).


Here is the test code:
import time
import numpy as np
np.random.seed(42)
a = np.random.uniform(size=(300, 300))
runtimes = 10

timecosts = []
for _ in range(runtimes):
    s_time = time.time()
    for i in range(100):
        a += 1
        np.linalg.svd(a)
    timecosts.append(time.time() - s_time)

print(f'mean of {runtimes} runs: {np.mean(timecosts):.5f}s')

and here are the results:
+-----------------------------------+-----------------------+--------------------+
|   Python installed by (run on)→   | Miniforge (native M1) | Anaconda (Rosseta) |
+----------------------+------------+------------+----------+----------+---------+
| Numpy installed by ↓ | Run from → |  Terminal  |  PyCharm | Terminal | PyCharm |
+----------------------+------------+------------+----------+----------+---------+
|          Apple Tensorflow         |   4.19151  |  4.86248 |     /    |    /    |
+-----------------------------------+------------+----------+----------+---------+
|        conda install numpy        |   4.29386  |  4.98370 |  4.10029 | 4.99271 |
+-----------------------------------+------------+----------+----------+---------+

This is quite slow. For comparison,

run the same code on my old MacBook Pro 2016 with i5 chip - it costs 2.39917s.
another post (but not in English) reports that run with M1 chip (not Pro or Max), miniforge+conda_installed_numpy is 2.53214s, and miniforge+apple_tensorflow_numpy is 1.00613s.
you may also try on it your own.

Here is the CPU information details:

My old i5:

$ sysctl -a | grep -e brand_string -e cpu.core_count
machdep.cpu.brand_string: Intel(R) Core(TM) i5-6360U CPU @ 2.00GHz
machdep.cpu.core_count: 2


My new M1 Max:

% sysctl -a | grep -e brand_string -e cpu.core_count
machdep.cpu.brand_string: Apple M1 Max
machdep.cpu.core_count: 10


I follow instructions strictly from tutorials - but why would all these happen? Is it because of my installation flaws, or because of M1 Max chip? Since my work relies heavily on local runs, local speed is very important to me. Any suggestions to possible solution, or any data points on your own device would be greatly appreciated :)
","Update Mar 28 2022: Please see @AndrejHribernik's comment below.

How to install numpy on M1 Max, with the most accelerated performance (Apple's vecLib)? Here's the answer as of Dec 6 2021.

Steps
I. Install miniforge
So that your Python is run natively on arm64, not translated via Rosseta.

Download Miniforge3-MacOSX-arm64.sh, then
Run the script, then open another shell

$ bash Miniforge3-MacOSX-arm64.sh


Create an environment (here I use name np_veclib)

$ conda create -n np_veclib python=3.9
$ conda activate np_veclib

II. Install Numpy with BLAS interface specified as vecLib

To compile numpy, first need to install cython and pybind11:

$ conda install cython pybind11


Compile numpy by (Thanks @Marijn's answer) - don't use conda install!

$ pip install --no-binary :all: --no-use-pep517 numpy


An alternative of 2. is to build from source

$ git clone https://github.com/numpy/numpy
$ cd numpy
$ cp site.cfg.example site.cfg
$ nano site.cfg

Edit the copied site.cfg: add the following lines:
[accelerate]
libraries = Accelerate, vecLib

Then build and install:
$ NPY_LAPACK_ORDER=accelerate python setup.py build
$ python setup.py install


After either 2 or 3, now test whether numpy is using vecLib:

>>> import numpy
>>> numpy.show_config()

Then, info like /System/Library/Frameworks/vecLib.framework/Headers should be printed.
III. For further installing other packages using conda
Make conda recognize packages installed by pip
conda config --set pip_interop_enabled true

This must be done, otherwise if e.g. conda install pandas, then numpy will be in The following packages will be installed list and installed again. But the new installed one is from conda-forge channel and is slow.

Comparisons to other installations:
1. Competitors:
Except for the above optimal one, I also tried several other installations

A. np_default: conda create -n np_default python=3.9 numpy
B. np_openblas: conda create -n np_openblas python=3.9 numpy blas=*=*openblas*
C. np_netlib: conda create -n np_netlib python=3.9 numpy blas=*=*netlib*

The above ABC options are directly installed from conda-forge channel. numpy.show_config() will show identical results. To see the difference, examine by conda list - e.g. openblas packages are installed in B. Note that mkl or blis is not supported on arm64.

D. np_openblas_source: First install openblas by brew install openblas. Then add [openblas] path /opt/homebrew/opt/openblas to site.cfg and build Numpy from source.
M1 and i9–9880H in this post.
My old i5-6360U 2cores on MacBook Pro 2016 13in.

2. Benchmarks:
Here I use two benchmarks:

mysvd.py: My SVD decomposition

import time
import numpy as np
np.random.seed(42)
a = np.random.uniform(size=(300, 300))
runtimes = 10

timecosts = []
for _ in range(runtimes):
    s_time = time.time()
    for i in range(100):
        a += 1
        np.linalg.svd(a)
    timecosts.append(time.time() - s_time)

print(f'mean of {runtimes} runs: {np.mean(timecosts):.5f}s')


dario.py: A benchmark script by Dario Radečić at the post above.

3. Results:
+-------+-----------+------------+-------------+-----------+--------------------+----+----------+----------+
|  sec  | np_veclib | np_default | np_openblas | np_netlib | np_openblas_source | M1 | i9–9880H | i5-6360U |
+-------+-----------+------------+-------------+-----------+--------------------+----+----------+----------+
| mysvd |  1.02300  |   4.29386  |   4.13854   |  4.75812  |      12.57879      |  / |     /    |  2.39917 |
+-------+-----------+------------+-------------+-----------+--------------------+----+----------+----------+
| dario |     21    |     41     |      39     |    323    |         40         | 33 |    23    |    78    |
+-------+-----------+------------+-------------+-----------+--------------------+----+----------+----------+

",tensorflow
difference between scikitlearn and sklearn now deprecated,"On OS X 10.11.6 and python 2.7.10 I need to import from sklearn  manifold. 
I have numpy 1.8 Orc1, scipy .13 Ob1 and scikit-learn 0.17.1 installed.
I used pip to install sklearn(0.0), but when I try to import from sklearn manifold I get the following: 

Traceback (most recent call last):   File """", line 1, in
     File
  ""/Library/Python/2.7/site-packages/sklearn/init.py"", line 57, in
       from .base import clone   File
  ""/Library/Python/2.7/site-packages/sklearn/base.py"", line 11, in
       from .utils.fixes import signature   File
  ""/Library/Python/2.7/site-packages/sklearn/utils/init.py"", line
  10, in      from .murmurhash import murmurhash3_32   File
  ""numpy.pxd"", line 155, in init sklearn.utils.murmurhash
  (sklearn/utils/murmurhash.c:5029) ValueError: numpy.dtype has the
  wrong size, try recompiling.

What is the difference between scikit-learn and sklearn?  Also, 
I cant import scikit-learn because of a syntax error
","You might need to reinstall numpy. It doesn't seem to have been installed correctly.
sklearn is how you type the scikit-learn name in python (only the latter should be installed, the former is now deprecated).
Also, try running the standard tests in scikit-learn and check the output. You will have detailed error information there.

As a side note, do you have nosetests installed? Try: nosetests -v sklearn. You type this in bash, not in the python interpreter.
",scikit-learn
