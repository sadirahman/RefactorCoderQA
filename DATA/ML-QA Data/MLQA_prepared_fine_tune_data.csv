instruction,input,output,Categories
what is oof approach in machine learning,"I have seen in many kaggle notebooks people talk about oof approach when they do machine learning with K-Fold validation. What is oof and is it related to k-fold validation ? Also can you suggest some useful resources for it to get the concept in detail
Thanks for helping!
","OOF simply stands for ""Out-of-fold"" and refers to a step in the learning process when using k-fold validation in which the predictions from each set of folds are grouped together into one group of 1000 predictions. These predictions are now ""out-of-the-folds"" and thus error can be calculated on these to get a good measure of how good your model is. 
In terms of learning more about it, there's really not a ton more to it than that, and it certainly isn't its own technique to learning or anything. If you have a follow up question that is small, please leave a comment and I will try and update my answer to include this.
EDIT: While ambling around the inter-webs I stumbled upon this relatively similar question from Cross-Validated (with a slightly more detailed answer), perhaps it will add some intuition if you are still confused.
",cross-validation
feature selection using bootstrap resampling lasso and stepwise regression,"In this paper, the authors perform radiomics feature selection for survival prediction by:

Bootstrap resampling the dataset x 1000
Fitting cross-validated LASSO models to each the resampled data sets
Retaining the 10 most common features with non-zero coefficients across all 1000 models
Fitting reverse stepwise regression using the ten selected features to the resampled datasets ( the same data sets as generated in step 1)
Choosing the final features based on the most common cox-regression model.

I would like to replicate this approach (albiet for logistic regression rather than cox-regression).
I am able to use the following R code to obtain the top K features from the Lasso models using the 'boot' library:
lasso_Select <- function(x, indices){ 
   x <- x[indices,]
   y <- x$Outcome
   x = subset(x, select = -Outcome)
   x2 <- as.matrix(x)
   fit <- glmnet(x2, y , family=""binomial"",alpha=1, standardize=TRUE)
   cv <- cv.glmnet(x2, y, family=""binomial"",alpha=1,  standardize=TRUE)
   fit <- glmnet(x2, y, family=""binomial"",alpha=1, lambda=cv$lambda.min,  standardize=TRUE)
     return(coef(fit)[,1])
   }

myBootstrap <- boot(scaled_train, lasso_Select, R = 1000, parallel = ""multicore"", ncpus=5)

However, I don't believe I can access the individual resampled datasets to then run the multiple logistic regression models and choose the most common.
Any advice on how to approach this?
","As the manual page for boot() explains:

For most of the boot methods the resampling is done in the master process, but not if simple = TRUE nor sim = ""parametric"".

As you are not doing parametric bootstrapping and you don't need to specify simple = TRUE, the code displayed when you type boot::boot at the R prompt  shows how the resampled data indices are generated. The critical code is:
if (!simple) 
            i <- index.array(n, R, sim, strata, m, L, weights)

where n is the number of data rows, R is the number of bootstrap samples, and the other arguments are defined in the call to boot() and don't seem to apply to your situation. Typing boot:::index.array shows the code for that function, which in turn calls boot:::ordinary.array for your situation. In your situation, i is just a matrix showing which data rows to use for each bootstrap sample.
It should be reasonably straightforward to tweak the code for boot() to return that matrix of indices along with the other values the function normally returns.
An alternative might be to return indices directly in your lasso_Select() function, although I'm not sure how well the boot() function would handle that.
",cross-validation
optimizing a gam for smoothness,"I am currently trying to generate a general additive model in R using a response variable and three predictor variables.
One of the predictors is linear, and the dataset consists of 298 observations.
I have run the following code to generate a basic GAM:
GAM <- gam(response~ linearpredictor+ s(predictor2) + s(predictor3), data = data[2:5])


This produces a model with 18 degrees of freedom and seems to substantially overfit the data.
I'm wondering how I might generate a GAM that maximizes smoothness and predictive error.
I realize that each of these features is going to come at the expense of the other, but is there good a way to find the optimal model that doesn't overfit?
Additionally, I need to perform leave one out cross validation LOOCV, and I am not sure how to make sure that gam() does this in the MGCV package.
Any help on either of these problems would be greatly appreciated. Thank you.
I've run this to generate a GAM, but it overfits the data.
I have also generated 1,000,000 GAMs with varying combinations of smoothing parameters and ranged the maximum degrees of freedom allowed from 10 (as shown in the code below) to 19.
The variable combinations2 is a list of all 1,000,000 combinations of smoothers I selected.
This code is designed to try and balance degrees of freedom and AIC score. It does function, but I'm not sure that I'm actually going to be able to find the optimal model from this. I also cannot tell how to make sure that it uses LOOCV.
BestGAM <- gam(response~ linearpredictor+ predictor2+ predictor3, data = data[2:5])

for(i in 1:100000){
    PotentialGAM <- gam(response~ linearpredictor+ s(predictor2) + s(predictor3), data = data[2:5], sp=c(combinations2[i,]$Var1,combinations2[i,]$Var2))
    
    if (AIC(PotentialGAM,BestGAM)$df[1] <= 10 & AIC(PotentialGAM,BestGAM)$AIC[1] < AIC(PotentialGAM,BestGAM)$AIC[2]){
        
        BestGAM <<- PotentialGAM
        listNumber <- i
    }
}

","You are fitting your GAM using generalised cross validation (GCV) smoothness selection. GCV is a way to get around the invariance problem of ordinary cross validation (OCV; what you also call LOOCV) when estimating GAMs. Note that GCV is the same as OCV on a rotated version of the fitting problem (rotating y - Xβ by Q, any orthogonal matrix), and while when fitting with GCV {mgcv} doesn't actually need to do the rotation and the expected GCV score isn't affected by the rotation, GCV is just OCV (wood 2017, p. 260)
It has been shown that GCV can undersmooth (resulting in more wiggly models) as the objective function (GCV profile) can become flat around the optimum. Instead it is preferred to estimate GAMs (with penalized smooths) using REML or ML smoothness selection; add method = ""REML"" (or ""ML"") to your gam() call.
If the REML or ML fit is as wiggly as the GCV one with your data, then I'd be likely to presume gam() is not overfitting, but that there is something about your response data that hasn't been explained here (are the data ordered in time, for example?)
As to your question

how I might generate a GAM that maximizes smoothness and [minimize?] predictive error,

you are already doing that using GCV smoothness selection and for a particular definition of ""smoothness"" (in this case it is squared second derivatives of the estimated smooths, integrated over the range of the covariates, and summed over smooths).
If you want GCV but smoother models, you can increase the gamma argument above 1; gamma 1.4 is often used for example, which means that each EDF costs 40% more in the GCV criterion.
FWIW, you can get the LOOCV (OCV) score for your model without actually fitting 288 GAMs through the use of the influence matrix A. Here's a reproducible example using my {gratia} package:
library(""gratia"")
library(""mgcv"")

df <- data_sim(""eg1"", seed = 1)
m <- gam(y ~ s(x0) + s(x1) + s(x2) + s(x3), data = df, method = ""REML"")
A <- influence(m)
r <- residuals(m, type = ""response"")

ocv_score <- mean(r^2 / (1 - A))

",cross-validation
using explicit predefined validation set for grid search with sklearn,"I have a dataset, which has previously been split into 3 sets: train, validation and test. These sets have to be used as given in order to compare the performance across different algorithms.
I would now like to optimize the parameters of my SVM using the validation set. However, I cannot find how to input the validation set explicitly into sklearn.grid_search.GridSearchCV(). Below is some code I've previously used for doing K-fold cross-validation on the training set. However, for this problem I need to use the validation set as given. How can I do that?
from sklearn import svm, cross_validation
from sklearn.grid_search import GridSearchCV

# (some code left out to simplify things)

skf = cross_validation.StratifiedKFold(y_train, n_folds=5, shuffle = True)
clf = GridSearchCV(svm.SVC(tol=0.005, cache_size=6000,
                             class_weight=penalty_weights),
                     param_grid=tuned_parameters,
                     n_jobs=2,
                     pre_dispatch=""n_jobs"",
                     cv=skf,
                     scoring=scorer)
clf.fit(X_train, y_train)

","Use PredefinedSplit
ps = PredefinedSplit(test_fold=your_test_fold)

then set cv=ps in GridSearchCV

test_fold : “array-like, shape (n_samples,)
test_fold[i] gives the test set fold of sample i. A value of -1 indicates that the corresponding sample is not part of any test set folds, but will instead always be put into the training fold.

Also see here

when using a validation set, set the test_fold to 0 for all samples that are part of the validation set, and to -1 for all other samples.

",cross-validation
knn gridsearchcv taking extremely long time to execute,"I am attempting to use sklearn to train a KNN model on the MNIST classification task. When I try to tune my parameters using either sklearn's GridSearchCV or RandomisedSearchCV classes, my code is taking an extremely long time to execute.
As an experiment, I created a KNN model using KNeighborsClassifier() with the default parameters and passed these same parameters to GridSearchCV. Afaik, this should mean GridSearchCV only has single set of parameters and so should effectively not perform a ""search"". I then called the .fit() methods of both on the training data and timed their execution (see code below). The KNN model's .fit() method took about 11 seconds to run, whereas the GridSearchCV model took over 20 minutes.
I understand that GridSearchCV should take slightly longer as it is performing 5-fold cross validation, but the difference in execution time seems too large for it to be explained by that.
Am I doing something with my GridSearchCV call that it causing it to take such a long time to execute? And is there anything that I can do to accelerate it?
import sklearn
import time 

# importing models
from sklearn.model_selection import StratifiedShuffleSplit
from sklearn.model_selection import GridSearchCV
from sklearn.neighbors import KNeighborsClassifier

# Importing data
from sklearn.datasets import fetch_openml
mnist = fetch_openml(name='mnist_784')

print(""data loaded"")

# splitting the data into stratified train & test sets
X, y = mnist.data, mnist.target # mnist mj.data.shape is (n_samples, n_features)
sss = StratifiedShuffleSplit(n_splits = 1, test_size = 0.2, random_state = 0)
for train_index, test_index in sss.split(X,y):
    X_train, y_train = X[train_index], y[train_index]
    X_test, y_test = X[test_index], y[test_index]

print(""data split"")

# Data has no missing values and is preprocessed, so no cleaing needed.

# using a KNN model, as recommended
knn = KNeighborsClassifier()

print(""model created"")


print(""training model"")
start = time.time()
knn.fit(X_train, y_train)
end = time.time()
print(f""Execution time for knn paramSearch was: {end-start}"")

# Parameter tuning.
# starting by performing a broad-range search on n_neighbours to work out the 
# rough scale the parameter should be on 
print(""beginning param tuning"")
params = {'n_neighbors':[5],
           'weights':['uniform'],
           'leaf_size':[30]
           }

paramSearch = GridSearchCV(
    estimator = knn,
    param_grid = params,
    cv=5,
    n_jobs = -1)

start = time.time()
paramSearch.fit(X_train, y_train)
end = time.time()
print(f""Execution time for knn paramSearch was: {end-start}"")

","With vanilla KNN, the costly procedure is predicting, not fitting: fitting just saves a copy of the data, and then predicting has to do the work of finding nearest neighbors.  So since your search involves scoring on each test fold, that's going to take a lot more time than just fitting.  A better comparison would have you predict on the training set in the no-search section.
However, sklearn does have different options for the algorithm parameter, which aim to trade away some of the prediction complexity for added training time, by building a search structure so that fewer comparisons are needed at prediction time.  With the default algorithm='auto', you're probably building a ball tree, and so the effect of the first paragraph won't be so profound.  I suspect this is still the issue though: now the training time will be non-neglibible, but the scoring portion in the search is what takes most of the time.
",cross-validation
matlab cross validation and knn,"I am trying to build a knn clasiffier with cross validation in Matlab. Because of my MATLAB version I have used knnclassify() in order to build the classifier (classKNN = knnclassify (sample_test, sample_training, training_label)).
I am not capable to use crossval() with that.
","There are two ways to perform the K-Nearest Neighbour in Matlab. The first one is by using knnclassify() as you did. However, this function will return the predicted labels and you cannot use crossval() with this. The cross-validation is performed on a model, not on its results. In Matlab, the model is described by an object.  
crossval() only works with objects (classifier objects, be it K-NN, SVM and so on...). In order to create the so-called nearest-neighbor classification object you need to use the fitcknn() function. Given the Training Set and the Training Labels as input (in this order), such function will return your object, which you can give as input in crossval().
There's only one thing left though: how do I predict the labels for my validation set? In order to do this, you need to use the predict() function. Given the model (kNN object) and the Validation Set as input (again, in this order), such function will return (as in knnclassify()) the predicted labels vector.
",cross-validation
question regarding k fold cross validation for knn using r,"I am trying to fit 5 fold cross validation for several values of k. I used the OJ data set in ISLR package.
my code so far as follows,
library(ISLR)
library(class)
ks=c(1:5)
err.rate.test <- numeric(length = 5)
folds <- cut(seq(1,nrow(OJ)),breaks=5,labels=FALSE)

for (j in seq(along = ks)) {
  set.seed(123)
  cv.knn <- sapply(1:5, FUN = function(i) {
    testID <- which(folds == i, arr.ind = TRUE)
    test.X <- OJ[testID, 3]
    test.Y <- OJ[testID, 1]
    train.X <- OJ[-testID, 3]
    train.Y <- OJ[-testID, 1]
    knn.test <- knn(data.frame(train.X), data.frame(test.X), train.Y, k = ks[j])
    cv.test.est <- mean(knn.test != test.Y)
    return(cv.test.est)
  })
  err.rate.test[j] <- mean(cv.knn)
  
}
   

err.rate.test
[1] 0.3757009 0.3757009 0.3757009 0.3757009 0.3757009

The code doesn't give any errors. But for some reason, my test error rate for each value of k is same. This seems to be weird for me. So i assume there is something wrong with my code.
Can anyone help me to figure that out?
","remove set.seed(123), this causes the repeat error rates.
set.seed is used for reproducibility, ensuring that any random grid searches or parameter estimates remain constant, meaning all of the parameter estimates that go into fitting the knn model will be the same across executions, resulting in the same predictions and therefore the same error rates.
",cross-validation
knn classifier using cross validation,"I am trying to implement KNN classifier using the cross validation approach where I have different images of a certain character for training(e.g 5 images) and another two for testing. Now I get the idea of the cross validation by simply choosing the K with the least error value when training & then using it with the test data to find how accurate my results are.
How do I train images in matlab to get my K value? Do I compare them and try to find mismatch or what?!
","First of you need to define your task precisely. F.ex Given an image I in R^(MxN) we wish to classify I as an image containing faces or an image without faces.
I often work with pixel classifiers, where the task is something like: For an image I decide if each pixel is a face pixel or a non-face pixel.
An important part of defining the task is to make a hypotheses that can be used as basis for training a classifier. F.ex We believe that the distribution of pixel intensities can be used to discriminate images of faces from images not containing faces. 
Then you need to select some features that define your image. This can be done in many ways and you should search for what other people do when they analyse the same type of images you are working with.
One widely used method in pixel classification is to use pixel intensity values and do a multi-scale analysis of the image. The idea in multi-scale analysis is that different structures are most evident at different level of blurring called scales. As an illustration consider an image of a tree. Without blurring we notice the fine structure, such as small branches and leafs. When we blur the image we notice the trunk and major branches. This is often used as part of segmentation methods.
When you know your task and the features, you can train a classifier. If you use kNN and cross-validation to find the best k, you should split you dataset in train/testing and then split the training set in train/validate sets. You then train using the reduced training set and use the validation set to decide which k is the best. In the case of binary classification e.g face vs non-face the error rate is often used as a measure of performance.
Finally you use the parameters to train the classifier on the full dataset and estimate its performance on the test set.
A classification example: With or without milk?
As a full example, consider images of a cup of coffee taken from above so it shows the rim of the cup surrounding a brownly colored disk. Further assume that all images are scaled and cropped so the diameter of the disk is the same and dimensions of the image are the same. To simplify the task, we convert the color image to grayscale and scale the pixel intensities to the range [0,1].
We want to train a classifier so it can distinguish coffee with milk from coffee without milk. From inspection of histograms of some of the coffee images, we see that each image has two ""bumps"" in the histogram that are clearly separated. We believe that these bumps correspond to foreground (coffee) and background. Now we make the hypothesis that the average intensity of the foreground can be used to distinguish between coffee+milk/coffee.
To find the foreground pixels we observe that because the foreground/background ratio is the same (by design) we can just find the intensity value that gives us that ratio for each image. Then we calculate the average intensity of the foreground pixels and use this value as a feature for each image.
If we have N images that we have manually labeled, we split this into training and test set. We then calculate the average foreground intensity for each image in the training set, giving us a set of (average foreground intensity, label) values. We want to use kNN where an image is assigned the same class as the majority class of the k closest images. We measure the distance as the absolute value of the difference in average foreground pixel intensity.
We search for the optimal k with cross validation. We use 2-fold cross validation (aka holdout) to find the best k. We test k = {1,3,5} and select the k that gives the least prediction error on the validation set.
",cross-validation
super low accuracy for neural network model,"I followed a tutorial on neural network model evaluation using cross-validation with code:
# Multiclass Classification with the Iris Flowers Dataset 
import numpy 
import pandas 
from keras.models import Sequential 
from keras.layers import Dense 
from keras.wrappers.scikit_learn import KerasClassifier 
from keras.utils import np_utils 
from sklearn.model_selection import cross_val_score 
from sklearn.model_selection import KFold 
from sklearn.preprocessing import LabelEncoder 
from sklearn.pipeline import Pipeline 
# fix random seed for reproducibility 
seed = 7 
numpy.random.seed(seed) 
# load dataset 
dataframe = pandas.read_csv(""/content/drive/My Drive/iris.data"", header=None) 
dataset = dataframe.values 
X = dataset[:,0:4].astype(float) 
Y = dataset[:,4] 

# encode class values as integers 
encoder = LabelEncoder() 
encoder.fit(Y) 
encoded_Y = encoder.transform(Y) 

# convert integers to dummy variables (i.e. one hot encoded) 
dummy_y = np_utils.to_categorical(encoded_Y) 

# define baseline model 
def baseline_model():

# create model
  model = Sequential()
  model.add(Dense(4, input_dim=4, activation=""relu"", kernel_initializer=""normal""))
  model.add(Dense(3, activation=""sigmoid"", kernel_initializer=""normal""))

# Compile model
  model.compile(loss= 'categorical_crossentropy' , optimizer= 'adam' , metrics=[ 'accuracy' ])

  return model 
estimator = KerasClassifier(build_fn=baseline_model, nb_epoch=200, batch_size=5, verbose=0) 
kfold = KFold(n_splits=10, shuffle=True, random_state=seed) 
results = cross_val_score(estimator, X, dummy_y, cv=kfold) 
print(""Accuracy: %.2f%% (%.2f%%)"" % (results.mean()*100, results.std()*100))

The accuracy was supposed to be around 95.33% (4.27%) but I got ~Accuracy: 34.00% (13.15%) on a few attempts. The model code seems exactly the same. I downloaded the data from here as instructed. What could go wrong?
","Replace this:
model.add(Dense(4, input_dim=4, activation=""relu"", kernel_initializer=""normal""))

With this:
model.add(Dense(16, activation=""relu""))
model.add(Dense(32, activation=""relu""))

Then, your output layer as:
model.add(Dense(3, activation=""softmax"", kernel_initializer=""normal""))

Your hidden layers were minuscule, and your activation function was wrong. For 3+ classes, it must be softmax.
FULL working code:
import numpy
from keras.models import Sequential
from keras.layers import Dense
from keras.wrappers.scikit_learn import KerasClassifier
from keras.utils import np_utils
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import KFold
from sklearn.preprocessing import MinMaxScaler

seed = 7
numpy.random.seed(seed)

from sklearn.datasets import load_iris

X, encoded_Y = load_iris(return_X_y=True)
mms = MinMaxScaler()
X = mms.fit_transform(X)

dummy_y = np_utils.to_categorical(encoded_Y)

def baseline_model():

    model = Sequential()
    model.add(Dense(4, input_dim=4, activation=""relu"", kernel_initializer=""normal""))
    model.add(Dense(8, activation=""relu"", kernel_initializer=""normal""))
    model.add(Dense(3, activation=""softmax"", kernel_initializer=""normal""))

    model.compile(loss= 'categorical_crossentropy' , optimizer='adam', metrics=[
        'accuracy' ])

    return model

estimator = KerasClassifier(build_fn=baseline_model, epochs=200, verbose=0)
kfold = KFold(n_splits=10, shuffle=True, random_state=seed)
results = cross_val_score(estimator, X, dummy_y, cv=kfold)
print(results)

Out[5]: 
array([0.60000002, 0.93333334, 1.        , 0.66666669, 0.80000001,
       1.        , 1.        , 0.93333334, 0.80000001, 0.86666667])

",cross-validation
what is 39minibatch39 in deep learning,"I'm taking the fast-ai course, and in ""Lesson 2 - SGD"" it says:

Mini-batch: a random bunch of points that you use to update your weights

And it also says that gradient descent uses mini-batches.
What is a mini-batch?  What's the difference between a mini-batch and a regular batch?
","Both are approaches to gradient descent. But in a batch gradient descent you process the entire training set in one iteration. Whereas, in a mini-batch gradient descent you process a small subset of the training set in each iteration.
Also compare stochastic gradient descent, where you process a single example from the training set in each iteration.
Another way to look at it: they are all examples of the same approach to  gradient descent with a batch size of m and a training set of size n. For stochastic gradient descent, m=1. For batch gradient descent, m = n. For mini-batch, m=b and b < n, typically b is small compared to n.
Mini-batch adds the question of determining the right size for b, but finding the right b may greatly improve your results.
",cross-validation
scikitlearn loocv vs doing it manually give different results why,"So i have built a model for a small dataset and since it was a small dataset, i made a Leave-One-out Cross-Validation (LOOCV) check for its accuracy. so in short, i would remove one sample manually, train the model, predict the left out sample and save the prediction and repeat the process for all the samples. then i would use the list of predictions and the actual values to get a RMSE and R2.
and today i found out that there was a Scikit-Learn implementation sklearn.model_selection.LeaveOneOut, however, when i tried it, it gave me different results for the RMSE, and refused to use R-squared as accuracy in the LOOCV method (it seems to calculate the accuracy per sample which does not work with R2).
here is a brief example of the code:
from numpy import mean
from numpy import std
from sklearn.datasets import make_blobs
from sklearn.model_selection import LeaveOneOut
from sklearn.model_selection import cross_val_score
from sklearn.ensemble import RandomForestClassifier

cv = LeaveOneOut()
model = RandomForestRegressor(n_estimators=200, max_depth=6,n_jobs=40, random_state=0)

scores = cross_val_score(model, data2SN, labelCL, scoring='neg_root_mean_squared_error', cv=cv, n_jobs=-1)
# report performance
print('Accuracy: %.3f (%.3f)' % (mean(scores), std(scores)))

my guess is that I'm calculating the RMSE for the whole dataset, while the LOOCV is doing it per sample and eventually i would take the mean and this is what causes the discrepancy between the two codes output, however, when i tried to calculate the RMSE per sample it failed (citing this TypeError: Singleton array 3021.0 cannot be considered a valid collection).   so I'm not sure how the RMSE is calculated inside the LOOCV. and I'm not sure to trust my code or just blindly use scikit-learn implementation.
I'm lost at what to do and chatGPT was just confusing as hell, so my human brethren please help
","cross_val_score average the scores across folds, so with cv=LeaveOneOut() yes, it's computing the score per row (by a model trained on all other rows).  With RMSE, that's equivalent to MAE; and R2 will just fail.
You could use cross_val_predict to get the individual predictions, then score that collection all at once, to reproduce your manual work.
",cross-validation
sklearn cross_val_score returns nan values,"i'm trying to predict next customer purchase to my job. I followed a guide, but when i tried to use cross_val_score() function, it returns NaN values.Google Colab notebook screenshot
Variables: 

X_train is a dataframe
X_test is a dataframe
y_train is a list
y_test is a list

Code:
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=50)
X_train = X_train.reset_index(drop=True)
X_train
X_test = X_test.reset_index(drop=True)

y_train = y_train.astype('float')
y_test = y_test.astype('float')

models = []
models.append((""LR"",LogisticRegression()))
models.append((""NB"",GaussianNB()))
models.append((""RF"",RandomForestClassifier()))
models.append((""SVC"",SVC()))
models.append((""Dtree"",DecisionTreeClassifier()))
models.append((""XGB"",xgb.XGBClassifier()))
models.append((""KNN"",KNeighborsClassifier()))´

for name,model in models:
   kfold = KFold(n_splits=2, random_state=22)
   cv_result = cross_val_score(model,X_train,y_train, cv = kfold,scoring = ""accuracy"")
   print(name, cv_result)
>>
LR [nan nan]
NB [nan nan]
RF [nan nan]
SVC [nan nan]
Dtree [nan nan]
XGB [nan nan]
KNN [nan nan]

help me please!
","Well thanks everyone for your answers. The answer of Anna helped me a lot!, but i don't used X_train.values, instead i assigned an unique ID to the Customers, then dropped Customers column and it works!
Now the models has this output :)
LR [0.73958333 0.74736842]
NB [0.60416667 0.71578947]
RF [0.80208333 0.82105263]
SVC [0.79166667 0.77894737]
Dtree [0.82291667 0.83157895]
XGB [0.85416667 0.85263158]
KNN [0.79166667 0.75789474]

",cross-validation
randomisation behaviour after cloning and fitting randomizedsearchcv,"I have a basic nested CV loop, where an outer loop goes over an inner model-tuning step. My expectation is that each fold should draw a different random sample of hyperparameter values. However, in the example below, each fold ends up sampling the same values.
Imports and make dataset:
from sklearn.model_selection import RandomizedSearchCV, KFold, cross_validate
from sklearn.ensemble import RandomForestClassifier
from sklearn.datasets import make_classification
from sklearn.base import clone

from scipy.stats import uniform
import numpy as np

X, y = make_classification(n_features=10, random_state=np.random.RandomState(0))

Nested CV loop:
#Used for tuning the random forest:
rf_tuner = RandomizedSearchCV(
    RandomForestClassifier(random_state=np.random.RandomState(0)),
    param_distributions=dict(min_samples_split=uniform(0.1, 0.9)),
    n_iter=5,
    cv=KFold(n_splits=2, shuffle=False),
    random_state=np.random.RandomState(0),
    n_jobs=1,
)

#Nested CV
for trn_idx, tst_idx in KFold(3).split(X, y):
    #'cloned' will now share the same RNG as 'rf_tuner'
    cloned = clone(rf_tuner)
    
    #This should be consuming the RNG of 'rf_tuner'
    cloned.fit(X[trn_idx], y[trn_idx])
    
    #Report hyperparameter values sampled in this fold
    display(cloned.cv_results_['params'])

    #<more code for nested CV, not shown>

Output:
Fold 1/3:
[{'min_samples_split': 0.593},
 {'min_samples_split': 0.743},
 {'min_samples_split': 0.642},
 {'min_samples_split': 0.590},
 {'min_samples_split': 0.481}]

Fold 2/3:
[{'min_samples_split': 0.593},
 {'min_samples_split': 0.743},
 {'min_samples_split': 0.642},
 {'min_samples_split': 0.590},
 {'min_samples_split': 0.481}]

Fold 3/3:
[{'min_samples_split': 0.593},
 {'min_samples_split': 0.743},
 {'min_samples_split': 0.642},
 {'min_samples_split': 0.590},
 {'min_samples_split': 0.481}]

I start by instantiating a RandomizedSearchCV with a RandomForestClassifier. I set the random_state= of the search to a random state instance np.random.RandomState(0).
For each pass of the outer loop, I clone() and fit() the search object - cloned should thus be using the same RNG as the original, mutating it at each pass. Each loop ought to yield a different sampling of hyperparameter values. However, as shown above, the hyperparameters sampled at each pass are identical. This suggests that each loop is starting with the same unmodified RNG rather than a mutated one.
The docs say that clones of estimators share the same random state instance:

b = clone(a) [...] calling a.fit will consume b’s RNG, and calling b.fit will consume a’s RNG, since they are the same

What explains the absence of randomisation between folds?

Update
The accepted answer clarifies that the RNG is simply duplicated rather than mutated.
If I want randomness between folds, whilst keeping the script repeatable, one approach would be to remove random_state= from the search object and instead globally set the random seed before running the nested CV.
I think a more canonical approach would be to instantiate an RNG with a fixed seed rng=np.random.RandomState(0), and then in each fold set a new random seed derived from rng:
cloned.set_params(**dict(
  estimator__random_state=rng.randint(10**9),
  random_state=rng.randint(10**9),
)

","clone performs a deepcopy on each non-estimator parameter (source), and so in the case of a RandomState the clones will all have different RandomState objects all starting from the same state (in the sense of get_state()). So your example is expected.
I don't know offhand if this used to behave differently, or if the documentation has always been wrong in this point.
",cross-validation
why when i use gridsearchcv with roc_auc scoring the score is different for grid_searchscorexy and roc_auc_scorey y_predict,"I am using stratified 10-fold cross validation to find model that predicts y (binary outcome) from X (X has 34 labels) with the highest auc. I set the GridSearchCV:
log_reg = LogisticRegression()
parameter_grid = {'penalty' : [""l1"", ""l2""],'C': np.arange(0.1, 3, 0.1),}
cross_validation = StratifiedKFold(n_splits=10,shuffle=True,random_state=100)
grid_search = GridSearchCV(log_reg, param_grid = parameter_grid,scoring='roc_auc',
                          cv = cross_validation)

And then do the cross-validation:
grid_search.fit(X, y)
y_pr=grid_search.predict(X)

I do not understand the following:
why grid_search.score(X,y) and roc_auc_score(y, y_pr) give different results (the former is 0.74 and the latter is 0.63)? Why do not these commands do the same thing in my case?
","This is due to different initialization of roc_auc when used in GridSearchCV.
Look at the source code here
roc_auc_scorer = make_scorer(roc_auc_score, greater_is_better=True,
                             needs_threshold=True)

Observe the third parameter needs_threshold. When true, it will require the continous values for y_pred such as probabilities or confidence scores which in gridsearch will be calculated from log_reg.decision_function().
When you explicitly call roc_auc_score with y_pr, you are using .predict() which will output the resultant predicted class labels of the data and not probabilities. That should account for the difference.
Try :
y_pr=grid_search.decision_function(X)
roc_auc_score(y, y_pr)

If still not same results, please update the question with complete code and some sample data.
",cross-validation
crossvalidation function returns quotunknown label type array00 10 dtypeobjectquot,"Here is the full error:
`---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
Cell In[33], line 2
      1 gnb = GaussianNB()
----> 2 cv = cross_val_score(gnb,X_train,y_train,cv=5, error_score = 'raise')
      3 print(cv)
      4 print(cv.mean())

File /opt/conda/lib/python3.10/site-packages/sklearn/model_selection/_validation.py:515, in cross_val_score(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, error_score)
    512 # To ensure multimetric format is not supported
    513 scorer = check_scoring(estimator, scoring=scoring)
--> 515 cv_results = cross_validate(
    516     estimator=estimator,
    517     X=X,
    518     y=y,
    519     groups=groups,
    520     scoring={""score"": scorer},
    521     cv=cv,
    522     n_jobs=n_jobs,
    523     verbose=verbose,
    524     fit_params=fit_params,
    525     pre_dispatch=pre_dispatch,
    526     error_score=error_score,
    527 )
    528 return cv_results[""test_score""]

File /opt/conda/lib/python3.10/site-packages/sklearn/model_selection/_validation.py:266, in cross_validate(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, return_train_score, return_estimator, error_score)
    263 # We clone the estimator to make sure that all the folds are
    264 # independent, and that it is pickle-able.
    265 parallel = Parallel(n_jobs=n_jobs, verbose=verbose, pre_dispatch=pre_dispatch)
--> 266 results = parallel(
    267     delayed(_fit_and_score)(
    268         clone(estimator),
    269         X,
    270         y,
    271         scorers,
    272         train,
    273         test,
    274         verbose,
    275         None,
    276         fit_params,
    277         return_train_score=return_train_score,
    278         return_times=True,
    279         return_estimator=return_estimator,
    280         error_score=error_score,
    281     )
    282     for train, test in cv.split(X, y, groups)
    283 )
    285 _warn_or_raise_about_fit_failures(results, error_score)
    287 # For callabe scoring, the return type is only know after calling. If the
    288 # return type is a dictionary, the error scores can now be inserted with
    289 # the correct key.

File /opt/conda/lib/python3.10/site-packages/sklearn/utils/parallel.py:63, in Parallel.__call__(self, iterable)
     58 config = get_config()
     59 iterable_with_config = (
     60     (_with_config(delayed_func, config), args, kwargs)
     61     for delayed_func, args, kwargs in iterable
     62 )
---> 63 return super().__call__(iterable_with_config)

File /opt/conda/lib/python3.10/site-packages/joblib/parallel.py:1918, in Parallel.__call__(self, iterable)
   1916     output = self._get_sequential_output(iterable)
   1917     next(output)
-> 1918     return output if self.return_generator else list(output)
   1920 # Let's create an ID that uniquely identifies the current call. If the
   1921 # call is interrupted early and that the same instance is immediately
   1922 # re-used, this id will be used to prevent workers that were
   1923 # concurrently finalizing a task from the previous call to run the
   1924 # callback.
   1925 with self._lock:

File /opt/conda/lib/python3.10/site-packages/joblib/parallel.py:1847, in Parallel._get_sequential_output(self, iterable)
   1845 self.n_dispatched_batches += 1
   1846 self.n_dispatched_tasks += 1
-> 1847 res = func(*args, **kwargs)
   1848 self.n_completed_tasks += 1
   1849 self.print_progress()

File /opt/conda/lib/python3.10/site-packages/sklearn/utils/parallel.py:123, in _FuncWrapper.__call__(self, *args, **kwargs)
    121     config = {}
    122 with config_context(**config):
--> 123     return self.function(*args, **kwargs)

File /opt/conda/lib/python3.10/site-packages/sklearn/model_selection/_validation.py:686, in _fit_and_score(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, split_progress, candidate_progress, error_score)
    684         estimator.fit(X_train, **fit_params)
    685     else:
--> 686         estimator.fit(X_train, y_train, **fit_params)
    688 except Exception:
    689     # Note fit time as time until error
    690     fit_time = time.time() - start_time

File /opt/conda/lib/python3.10/site-packages/sklearn/naive_bayes.py:267, in GaussianNB.fit(self, X, y, sample_weight)
    265 self._validate_params()
    266 y = self._validate_data(y=y)
--> 267 return self._partial_fit(
    268     X, y, np.unique(y), _refit=True, sample_weight=sample_weight
    269 )

File /opt/conda/lib/python3.10/site-packages/sklearn/naive_bayes.py:427, in GaussianNB._partial_fit(self, X, y, classes, _refit, sample_weight)
    424 if _refit:
    425     self.classes_ = None
--> 427 first_call = _check_partial_fit_first_call(self, classes)
    428 X, y = self._validate_data(X, y, reset=first_call)
    429 if sample_weight is not None:

File /opt/conda/lib/python3.10/site-packages/sklearn/utils/multiclass.py:420, in _check_partial_fit_first_call(clf, classes)
    413             raise ValueError(
    414                 ""`classes=%r` is not the same as on last call ""
    415                 ""to partial_fit, was: %r"" % (classes, clf.classes_)
    416             )
    418     else:
    419         # This is the first call to partial_fit
--> 420         clf.classes_ = unique_labels(classes)
    421         return True
    423 # classes is None and clf.classes_ has already previously been set:
    424 # nothing to do

File /opt/conda/lib/python3.10/site-packages/sklearn/utils/multiclass.py:107, in unique_labels(*ys)
    105 _unique_labels = _FN_UNIQUE_LABELS.get(label_type, None)
    106 if not _unique_labels:
--> 107     raise ValueError(""Unknown label type: %s"" % repr(ys))
    109 if is_array_api:
    110     # array_api does not allow for mixed dtypes
    111     unique_ys = xp.concat([_unique_labels(y) for y in ys])

ValueError: Unknown label type: (array([0.0, 1.0], dtype=object),)`

I am trying to implement cross-validation in the Spaceship Titanic Project on Kaggle, you can find my full notebook here. I have not been able to get the cross-validation function to run. I have been fiddling with it for hours, and I have not been able to find the solution.
I followed Ken Jee's Titanic Project Example to complete my own Titanic Prediction submission. In this code, the cross-validation function can successfully run, and I tried to re-purpose this to the Spaceship Titanic project.
Please let me know if you need any other details. I am quite new to Machine Learning, so I thank you in advance for your patience.
I've tried getting rid of null values, verified that the data types for my training and test lists are the same, and I've tried mapping categorical variables to numerical variables.
","Your target (y_train) is of type object
Casting y_train before doing CV to int32 fixes the error.
y_train = y_train.astype(""int32"")

",cross-validation
kfold cross validation on segmentation task in deep learning,"I'm new to deep learning and i wanted to do semantic segmentation task with U-Net, i heared one of the strategies to make my results better is to use cross-validation which is not very popular in deep learning.
I made some reasearch to find out how to implement it on my dataset but i couldn't find a reliable answer.
can you help me how should i implement 5-fold cross validation? should i train 1 model on 5 folds? this seems not correct to me because why shouldn't i just train the model on the whole training dataset? or should i train 5 models on each fold and for inference average the 5 outputs? this might work but the inference time will be overwhelmingly high
Thank you
","K-fold cross-validation (cv) can be used to obtain better insight into how the model will generalize on unseen data.
To perform 5-fold cv, first separate your data into five folds.  Then set the first fold aside and train a model on the remaining four folds.  Now evaluate the trained model on the fold that was set aside.  Next take the five folds, set aside the second fold, and train a new model on the remaining four folds; evaluate this model on the second fold.  Repeat this process, setting aside each other fold, until you have created five models, each which has a single validation score.  Take the mean of these five validation scores, and that is your cross-validation score, which is an estimate of the performance of using the model building process (e.g. the fixed preprocessing, hyperparameters, deep learning algorithm).
None of these five models will be your final model.  Instead, rerun the model building process (not cross-validation), using all the data to train it.  The result of this will be the final model.  And the estimate of that model's performance is the cross-validation score previously found.
Why did we perform cross-validation?  The idea is that there is randomness present in the data or model building process.  Given such randomness (or noise), when we create a model on a training set and evaluate it on held-out set, the performance might -- just by luck -- be better or worse than what we will see when we go off and deploy our model.  If instead we look at how our model building process performed on combinations of training data and evaluation data, we will get a better indication of how the model will perform.
Besides using cv to estimate the performance of model that is going to be deployed, another place it is common to use cv is in model selection.  Come up with a series of different model building processes (for instance, different number of layers in a neural net), and select the one with the highest cv score.  (Note, this cv score is an optimistic indicator of how well the model will perform on new data; related terms are ""winners curse"", ""multiple induction problem"", ""multi-hypothesis testing problem"", ""overhyping"").
The reason cross-validation is not popular in deep learning is that it time consuming, because instead of building one model, there is requirements to build multiple.  And also deep learning is often used in problems where there is lots of data, and so a train-validation-test split method is hoped to be sufficient enough for modeling building, model selection, and model validation respectively.
",cross-validation
how to get the accuracy of an evaluation object in weka java,"I am using weka with java for classification purposes, I did manually the cross validation and I would like to know if there is a way to get the accuracy of an evaluation object i don't want the whole metrics like:
 System.out.println(eval.toSummaryString(""=== "" + folds + ""-fold Cross-validation ==="", false));

Thank you
","To get the accuracy you just have to use the function pctCorrect(), which gives you the percentage of instances correctly classified by your model.
double accuracy = eval.pctCorrect();
System.out.println(""Accuracy = "" + accuracy);

",cross-validation
evaluating logistic regression with cross validation,"I would like to use cross validation to test/train my dataset and evaluate the performance of the logistic regression model on the entire dataset and not only on the test set (e.g. 25%). 
These concepts are totally new to me and am not very sure if am doing it right. I would be grateful if anyone could advise me on the right steps to take where I have gone wrong. Part of my code is shown below.
Also, how can I plot ROCs for ""y2"" and ""y3"" on the same graph with the current one?
Thank you
import pandas as pd 
Data=pd.read_csv ('C:\\Dataset.csv',index_col='SNo')
feature_cols=['A','B','C','D','E']
X=Data[feature_cols]

Y=Data['Status'] 
Y1=Data['Status1']  # predictions from elsewhere
Y2=Data['Status2'] # predictions from elsewhere

from sklearn.linear_model import LogisticRegression
logreg=LogisticRegression()
logreg.fit(X_train,y_train)

from sklearn.cross_validation import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)

from sklearn import metrics, cross_validation
predicted = cross_validation.cross_val_predict(logreg, X, y, cv=10)
metrics.accuracy_score(y, predicted) 

from sklearn.cross_validation import cross_val_score
accuracy = cross_val_score(logreg, X, y, cv=10,scoring='accuracy')
print (accuracy)
print (cross_val_score(logreg, X, y, cv=10,scoring='accuracy').mean())

from nltk import ConfusionMatrix 
print (ConfusionMatrix(list(y), list(predicted)))
#print (ConfusionMatrix(list(y), list(yexpert)))

# sensitivity:
print (metrics.recall_score(y, predicted) )

import matplotlib.pyplot as plt 
probs = logreg.predict_proba(X)[:, 1] 
plt.hist(probs) 
plt.show()

# use 0.5 cutoff for predicting 'default' 
import numpy as np 
preds = np.where(probs > 0.5, 1, 0) 
print (ConfusionMatrix(list(y), list(preds)))

# check accuracy, sensitivity, specificity 
print (metrics.accuracy_score(y, predicted)) 

#ROC CURVES and AUC 
# plot ROC curve 
fpr, tpr, thresholds = metrics.roc_curve(y, probs) 
plt.plot(fpr, tpr) 
plt.xlim([0.0, 1.0]) 
plt.ylim([0.0, 1.0]) 
plt.xlabel('False Positive Rate') 
plt.ylabel('True Positive Rate)') 
plt.show()

# calculate AUC 
print (metrics.roc_auc_score(y, probs))

# use AUC as evaluation metric for cross-validation 
from sklearn.cross_validation import cross_val_score 
logreg = LogisticRegression() 
cross_val_score(logreg, X, y, cv=10, scoring='roc_auc').mean() 

","You got it almost right. cross_validation.cross_val_predict gives you predictions for the entire dataset. You just need to remove logreg.fit earlier in the code. Specifically, what it does is the following:
It divides your dataset in to n folds and in each iteration it leaves one of the folds out as the test set and trains the model on the rest of the folds (n-1 folds). So, in the end you will get predictions for the entire data. 
Let's illustrate this with one of the built-in datasets in sklearn, iris. This dataset contains 150 training samples with 4 features. iris['data'] is X and iris['target'] is y
In [15]: iris['data'].shape
Out[15]: (150, 4)

To get predictions on the entire set with cross validation you can do the following:
from sklearn.linear_model import LogisticRegression
from sklearn import metrics, cross_validation
from sklearn import datasets
iris = datasets.load_iris()
predicted = cross_validation.cross_val_predict(LogisticRegression(), iris['data'], iris['target'], cv=10)
print metrics.accuracy_score(iris['target'], predicted)

Out [1] : 0.9537

print metrics.classification_report(iris['target'], predicted) 

Out [2] :
                     precision    recall  f1-score   support

                0       1.00      1.00      1.00        50
                1       0.96      0.90      0.93        50
                2       0.91      0.96      0.93        50

      avg / total       0.95      0.95      0.95       150

So, back to your code. All you need is this:
from sklearn import metrics, cross_validation
logreg=LogisticRegression()
predicted = cross_validation.cross_val_predict(logreg, X, y, cv=10)
print metrics.accuracy_score(y, predicted)
print metrics.classification_report(y, predicted) 

For plotting ROC in multi-class classification, you can follow this tutorial which gives you something like the following:

In general, sklearn has very good tutorials and documentation. I strongly recommend reading their tutorial on cross_validation.
",cross-validation
finding training mse from cvglmnet r package,"I've been trying to find the MSE for the training dataset when using the cv.glmnet function from the glmnet package in R.
Any help would be appreciated.
Thanks
The best I've come up with is below but I'm not sure if it's correct
set.seed(34064064)
library(ISLR2)
Library(glmnet)

x<- matrix.model(Salary~.,data=Hitters)[,-1]
y<- Hitters$Salary

cv<-cv.glmnet(x,y,lambda=exp(seq(-2, 4, length.out = 30)),k=10,alpha=1,standardize = TRUE,type.measure = ""mse"")

best.lambda <- cv$lambda.min

fit <- glmnet(x, y, lambda=best.lambda, alpha=0, standardize=TRUE)

y.pred <- predict(fit, newx=x)

training.mse <- mean((y - y.pred)^2)

print(training.mse)

","You can use the glmnet::assess.glmnet function, which provides summary performance measures for the glmnet model.
library(ISLR2)
library(glmnet)

Hitters <- na.omit(Hitters)
x <- model.matrix(Salary~., data=Hitters)[,-1]
y <- Hitters$Salary

cv <- cv.glmnet(x,y, lambda=exp(seq(-2, 4, length.out = 30)), k=10, 
                alpha=1, standardize = TRUE, type.measure = ""mse"")

best.lambda <- cv$lambda.min
fit <- glmnet(x, y, lambda=best.lambda, alpha=1, standardize=TRUE)

# The method based on ""predict""
y.pred <- predict(fit, newx=x)
training.mse <- mean((y - y.pred)^2)
print(training.mse)

[1] 94310.44

# The method based on ""assess.glmnet""    
perf <- assess.glmnet(fit, newx=x, newy=y)
print(perf$mse)

      s0 
94310.44
attr(,""measure"")
[1] ""Mean-Squared Error""

",cross-validation
why does my crossvalidation consistently perform better than traintest split,"I have the code below (using sklearn) that first uses the training set for cross-validation, and for a final check, uses the test set. However, the cross-validation consistently performs better, as shown below. Am I over-fitting on the training data? If so, which hyper parameter(s) would be best to tune to avoid this?
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)
#Cross validation
rfc = RandomForestClassifier()
cv = RepeatedKFold(n_splits=10, n_repeats=5)   
scoring = {'accuracy', 'precision', 'recall', 'f1', 'roc_auc' }
scores = cross_validate(rfc, X_train, y_train, scoring=scoring, cv=cv)
print(mean(scores['test_accuracy']),
      mean(scores['test_precision']),
      mean(scores['test_recall']),
      mean(scores['test_f1']),
      mean(scores['test_roc_auc'])
      )

Which gives me:
0.8536558341101569 0.8641939667622551 0.8392201023654705 0.8514895113569482 0.9264002192260914

Re-train the model now with the entire training+validation set, and test it with never-seen-before test-set
RFC = RandomForestClassifier()

RFC.fit(X_train, y_train)
y_pred = RFC.predict(X_test)

accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)
y_pred_proba = RFC.predict_proba(X_test)[::,1] 
auc = roc_auc_score(y_test, y_pred_proba)

print(accuracy,
      precision,
      recall,
      f1,
      auc
      )

Now it gives me the numbers below, which are clearly worse:
0.7809788654060067 0.5113236034222446 0.5044687189672294 0.5078730317420644 0.7589037004728368

","I am able to reproduce your scenario with Pima Indians Diabetes Dataset.
The difference you see in the prediction metrics is not consistence and in some runs you may even notice the opposite, because it depends on the selection of the X_test during the split - some of the cases will be easier to predict and will give better metrics and vice versa. While Cross-validation runs predictions on the whole set you have in rotation and aggregates this effect, the single X_test set will suffer from effects of random splits. 
In order to have better visibility on what is happening here, I have modified your experiment and split in two steps:
1. Cross-validation step:
I use the whole of the X and y sets and run rest of the code as it is
rfc = RandomForestClassifier()
cv = RepeatedKFold(n_splits=10, n_repeats=5)
# cv = KFold(n_splits=10)
scoring = {'accuracy', 'precision', 'recall', 'f1', 'roc_auc'}
scores = cross_validate(rfc, X, y, scoring=scoring, cv=cv)
print(mean(scores['test_accuracy']),
      mean(scores['test_precision']),
      mean(scores['test_recall']),
      mean(scores['test_f1']),
      mean(scores['test_roc_auc'])
      )

Output:
0.768257006151743 0.6943032069967433 0.593436328663432 0.6357667086829574 0.8221242747913622

2. Classic train-test step:
Next I run the plain train-test step, but I do it 50 times with the different train_test splits, and average the metrics (similar to Cross-validation step).
accuracies = []
precisions = []
recalls = []
f1s = []
aucs = []

for i in range(50):
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)
    RFC = RandomForestClassifier()

    RFC.fit(X_train, y_train)
    y_pred = RFC.predict(X_test)

    accuracy = accuracy_score(y_test, y_pred)
    precision = precision_score(y_test, y_pred)
    recall = recall_score(y_test, y_pred)
    f1 = f1_score(y_test, y_pred)
    y_pred_proba = RFC.predict_proba(X_test)[::, 1]
    auc = roc_auc_score(y_test, y_pred_proba)
    accuracies.append(accuracy)
    precisions.append(precision)
    recalls.append(recall)
    f1s.append(f1)
    aucs.append(auc)

print(mean(accuracies),
      mean(precisions),
      mean(recalls),
      mean(f1s),
      mean(aucs)
      )

Output:
0.7606926406926405 0.7001931059992001 0.5778712922956755 0.6306501622080503 0.8207846633339568

As expected the prediction metrics are similar. However, the Cross-validation runs much faster and uses each data point of the whole data set for testing (in rotation) by a given number of times.
",cross-validation
classification report with nested cross validation in sklearn averageindividual values,"Is it possible to get classification report from cross_val_score through some workaround? I'm using nested cross-validation and I can get various scores here for a model, however, I would like to see the classification report of the outer loop. Any recommendations? 
# Choose cross-validation techniques for the inner and outer loops,
# independently of the dataset.
# E.g ""LabelKFold"", ""LeaveOneOut"", ""LeaveOneLabelOut"", etc.
inner_cv = KFold(n_splits=4, shuffle=True, random_state=i)
outer_cv = KFold(n_splits=4, shuffle=True, random_state=i)

# Non_nested parameter search and scoring
clf = GridSearchCV(estimator=svr, param_grid=p_grid, cv=inner_cv)

# Nested CV with parameter optimization
nested_score = cross_val_score(clf, X=X_iris, y=y_iris, cv=outer_cv)

I would like to see a classification report here along side the score values. 
http://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html
","We can define our own scoring function as below:
from sklearn.metrics import classification_report, accuracy_score, make_scorer

def classification_report_with_accuracy_score(y_true, y_pred):

    print classification_report(y_true, y_pred) # print classification report
    return accuracy_score(y_true, y_pred) # return accuracy score

Now, just call cross_val_score with our new scoring function, using make_scorer:  
# Nested CV with parameter optimization
nested_score = cross_val_score(clf, X=X_iris, y=y_iris, cv=outer_cv, \
               scoring=make_scorer(classification_report_with_accuracy_score))
print nested_score 

It will print the classification report as text at the same time return the nested_score as a number.
http://scikit-learn.org/stable/auto_examples/model_selection/plot_nested_cross_validation_iris.html example when run with this new scoring function, the last few lines of the output will be as follows:
#   precision    recall  f1-score   support    
#0       1.00      1.00      1.00        14
#1       1.00      1.00      1.00        14
#2       1.00      1.00      1.00         9

#avg / total       1.00      1.00      1.00        37

#[ 0.94736842  1.          0.97297297  1. ]

#Average difference of 0.007742 with std. dev. of 0.007688.

",cross-validation
how can i do repeated cross validation for an elasticnet penalized cox model in r,"I would like to know what would be the best method to do 10x repeated 10-fold cross validation for an elastic-net penalized cox model in R. I am currently using the package glmnet, tuning the alpha first and second the lambda. I would like to use the C-index as a performance metric to choose the best lambda, but I have not been able to find where it is stored in my glmnet object. I was thinking to make a loop and set a seed inside the loop to repeat the CV 10 times, but without the C-index for the different lambda values, I am not sure how to choose the best lambda across models. I get visualize the C-index for the different lambda values using plot(glmnetfit), but I would have to visually inspect the plots for each repeats, which does not sound very accurate to me and becomes difficult if there are more than 10 repeats.
Here is the code I have been using:
library(glmnet)
library(survival)

data(""CoxExample"")
x <- CoxExample$x
y <- CoxExample$y

alphas <- seq(0, 1, by = 0.1) 

# Fit models with different alpha values
models <- lapply(alphas, function(alpha) {
  glmnet(x = x, y = y, family = ""cox"", alpha = alpha)
})

# Optimize alpha
set.seed(123)
cv_results <- lapply(seq_along(models), function(i) {
  alpha <- alphas[[i]]
  print(alpha)
  lambda <- models[[i]]$lambda
  if (is.null(alpha)) {
    alpha <- 1  # Default to lasso if alpha is missing
  }
  cv.glmnet(x = x, y = y, family = ""cox"", 
            alpha = alpha, lambda = lambda)
})

optimal_alpha_index <- sapply(cv_results, function(cv_result) which.min(cv_result$cvm))
optimal_cvm <- vector(""list"", length = length(cv_results))
for (i in seq_along(cv_results)) {
  optimal_index <- which.min(cv_results[[i]]$cvm)  # Get the index of the optimal model for alpha i
  optimal_cvm[[i]] <- cv_results[[i]]$cvm[optimal_index]  # Extract the corresponding cross-validation error
}


optimal_alpha = alphas[which.min(optimal_cvm)]
optimal_alpha #1

#Optimize lambda
models <- list()

for (i in 1:10) {
  set.seed(i*123)
  models[[i]] <- cv.glmnet(x=x, 
                           y = y, 
                           family = ""cox"", 
                           alpha = optimal_alpha, 
                           type.measure = ""C""
  )
  print(models[[i]]$lambda.min)
}

optimal_lambda_index <- sapply(models, function(models) which.min(models$cvm))

I end up with the best lambda for each of the 10 repeats. I would like to extract the C-index for the best lambda in each model, or if not possible hear what would be the best approach to CV repeats. Any help would be appreciated.
Thanks!
","You are setting type.measure to ""C"" for C-index in the cross-validation procedure, but the extraction and usage can be refined.
library(glmnet)
library(survival)
library(foreach)
library(doParallel)

data(""CoxExample"")
x <- CoxExample$x
y <- CoxExample$y

alphas <- seq(0, 1, by = 0.1) 
n_repeats <- 10

results <- vector(""list"", length = n_repeats)

num_cores <- detectCores()-1  
registerDoParallel(cores = num_cores)

# parallel execution using foreach package
results <- foreach(r = 1:n_repeats, .packages = 'glmnet') %dopar% {
  set.seed(r * 123)
  
  # Using different alpha values to fit the model
  models <- lapply(alphas, function(alpha) {
    cv.glmnet(x = x, y = y, family = ""cox"", alpha = alpha, type.measure = ""C"")
  })
  
  # Identifies the best lambda for each alpha based on minimum C-index
  best_lambda_per_alpha <- sapply(models, function(model) {
    lambda_min <- model$lambda.min
    c_index <- max(model$cvm)  
    return(c(lambda = lambda_min, c_index = c_index))
  })
  
  # Finding the alpha and lambda combination that gives the best C-index
  optimal_alpha_index <- which.max(best_lambda_per_alpha[""c_index"",])
  optimal_alpha <- alphas[optimal_alpha_index]
  optimal_lambda <- best_lambda_per_alpha[""lambda"", optimal_alpha_index]
  
  list(alpha = optimal_alpha, lambda = optimal_lambda, c_index = best_lambda_per_alpha[""c_index"", optimal_alpha_index])
}

do.call(rbind, results)

",cross-validation
why do i get the same results with different crossvalidation specifications in caret for lm,"I am using the caret package for fitting different models with the same data. I am using cross-validation for all of them; however, when I use different number of folds with the lm method, I get the same coefficients, I was expecting at least small differences. What is the reason? Is this expected?
Thanks for your time!
Here is a reprex
library(caret)
#> Loading required package: ggplot2
#> Loading required package: lattice

{
set.seed(123)
Xs <- matrix(rnorm(300*20),nrow = 300)
Y <- rnorm(300)
data <- cbind(Xs,Y) |> as.data.frame()
}

ctrlspecs_2 <- trainControl(method=""cv"", number=2)
ctrlspecs_10 <- trainControl(method=""cv"", number=10)

set.seed(123)
model_2 <- train(Y~.,
                 data = data,
                 method = ""lm"",
                 trControl = ctrlspecs_2)

set.seed(123)
model_10 <- train(Y~.,
                 data = data,
                 method = ""lm"",
                 trControl = ctrlspecs_10)

summary(model_2)
#> 
#> Call:
#> lm(formula = .outcome ~ ., data = dat)
#> 
#> Residuals:
#>     Min      1Q  Median      3Q     Max 
#> -3.5934 -0.6277 -0.0082  0.7448  2.2594 
#> 
#> Coefficients:
#>              Estimate Std. Error t value Pr(>|t|)   
#> (Intercept) -0.044073   0.060499  -0.728  0.46692   
#> V1          -0.129567   0.065772  -1.970  0.04984 * 
#> V2          -0.002505   0.061859  -0.040  0.96773   
#> V3          -0.046897   0.059486  -0.788  0.43115   
#> V4           0.044195   0.061427   0.719  0.47245   
#> V5           0.086981   0.064085   1.357  0.17579   
#> V6           0.014166   0.061001   0.232  0.81653   
#> V7          -0.077959   0.060911  -1.280  0.20165   
#> V8           0.017661   0.065486   0.270  0.78759   
#> V9          -0.096562   0.060567  -1.594  0.11200   
#> V10          0.164024   0.060858   2.695  0.00746 **
#> V11         -0.028008   0.060869  -0.460  0.64577   
#> V12          0.034027   0.062118   0.548  0.58428   
#> V13         -0.066028   0.066681  -0.990  0.32294   
#> V14          0.142444   0.061319   2.323  0.02090 * 
#> V15         -0.129046   0.060109  -2.147  0.03267 * 
#> V16         -0.020873   0.061512  -0.339  0.73462   
#> V17          0.046835   0.063381   0.739  0.46056   
#> V18          0.035570   0.066567   0.534  0.59353   
#> V19         -0.016253   0.060039  -0.271  0.78682   
#> V20         -0.082083   0.060843  -1.349  0.17840   
#> ---
#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
#> 
#> Residual standard error: 1.033 on 279 degrees of freedom
#> Multiple R-squared:  0.1041, Adjusted R-squared:  0.03986 
#> F-statistic: 1.621 on 20 and 279 DF,  p-value: 0.04731
summary(model_10)
#> 
#> Call:
#> lm(formula = .outcome ~ ., data = dat)
#> 
#> Residuals:
#>     Min      1Q  Median      3Q     Max 
#> -3.5934 -0.6277 -0.0082  0.7448  2.2594 
#> 
#> Coefficients:
#>              Estimate Std. Error t value Pr(>|t|)   
#> (Intercept) -0.044073   0.060499  -0.728  0.46692   
#> V1          -0.129567   0.065772  -1.970  0.04984 * 
#> V2          -0.002505   0.061859  -0.040  0.96773   
#> V3          -0.046897   0.059486  -0.788  0.43115   
#> V4           0.044195   0.061427   0.719  0.47245   
#> V5           0.086981   0.064085   1.357  0.17579   
#> V6           0.014166   0.061001   0.232  0.81653   
#> V7          -0.077959   0.060911  -1.280  0.20165   
#> V8           0.017661   0.065486   0.270  0.78759   
#> V9          -0.096562   0.060567  -1.594  0.11200   
#> V10          0.164024   0.060858   2.695  0.00746 **
#> V11         -0.028008   0.060869  -0.460  0.64577   
#> V12          0.034027   0.062118   0.548  0.58428   
#> V13         -0.066028   0.066681  -0.990  0.32294   
#> V14          0.142444   0.061319   2.323  0.02090 * 
#> V15         -0.129046   0.060109  -2.147  0.03267 * 
#> V16         -0.020873   0.061512  -0.339  0.73462   
#> V17          0.046835   0.063381   0.739  0.46056   
#> V18          0.035570   0.066567   0.534  0.59353   
#> V19         -0.016253   0.060039  -0.271  0.78682   
#> V20         -0.082083   0.060843  -1.349  0.17840   
#> ---
#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
#> 
#> Residual standard error: 1.033 on 279 degrees of freedom
#> Multiple R-squared:  0.1041, Adjusted R-squared:  0.03986 
#> F-statistic: 1.621 on 20 and 279 DF,  p-value: 0.04731

identical(model_2$finalModel$coefficients,model_10$finalModel$coefficients)
#> [1] TRUE

Created on 2024-03-20 with reprex v2.1.0
","The coefficients are the same because summary is giving you the results of a linear model fitted on the entire dataset.
The cross-validation is done separately to work out how well the model will work on unseen data. You can see the cross-validation results using model2$resample and model10$resample
",cross-validation
how to split a data into kfolds not randomly in matlab,"I have a dataset, for simplicity let's say it has 1000 samples (each is a vector).
I want to split my data for cross validation, for train and test, NOT randomly1, so for example if I want 4-fold cross validation, I should get:
fold1: train = 1:250; test= 251:1000
fold2: train = 251:500, test = [1:250 ; 501:1000]
fold3: train = 501:750, test = [1:500; 751:1000]
fold4: train = 751:1000, test = 1:750
I am aware of CVPARTITION, but AFAIK - it splits the data randomly - which is not what I need.
I guess I can write the code for it, but I figured there is probably a function I could use.

(1) The data is already shuffled and I need to be able to easily reproduce the experiments.
","Here is a function that does it in general:
function [test, train] = kfolds(data, k)

  n = size(data,1);

  test{k,1} = [];
  train{k,1} = [];

  chunk = floor(n/k);

  test{1} = data(1:chunk,:);
  train{1} = data(chunk+1:end,:);

  for f = 2:k
      test{f} = data((f-1)*chunk+1:(f)*chunk,:);
      train{f} = [data(1:(f-1)*chunk,:); data(f*chunk+1:end, :)];
  end
end

It's not an elegant 1 liner, but it's fairly robust, doesn't need k to be a factor of your number of samples, works on a 2D matrix and outputs the actual sets rather than indices.
",cross-validation
implementing gridsearchcv with scorer for leave one out crossvalidation,"I am attempting to implement scikit-learn's GridSearchCV for Gaussian Process Regression (GPR). I'm using a small dataset of ~200 points, and would like to use LOOCV as a performance evaluator for my model. My setup is:
from sklearn.model_selection import *
from sklearn.ensemble import *
from sklearn.gaussian_process import *

param_grid = {
    'kernel':[kernels.RBF(),kernels.Matern(length_scale=0.1)],
    'n_restarts_optimizer':[5,10,20,25],
    'random_state':[30]
}
res_GPR = GridSearchCV(estimator=GaussianProcessRegressor(),param_grid=param_grid,cv=LeaveOneOut(),verbose=20,n_jobs=-1)
res_GPR.fit(X,y)

where X and y are my data points and target values respectively.
I know that the scoring method returned by GPR is r^2, which is undefinable for the LOOCV case (since there is only one test element) - this is verified by obtaining NaN for the .best_score_ attribute of the fitted model.
As such, I would like the model to be scored with just the Root Mean Squared Error (RMSE) for each test case, averaged over all the iterations. How can I do that?
","GridSearchCV includes a scoring argument, which you may use to set your score to negative RMSE:
res_GPR = GridSearchCV(estimator=GaussianProcessRegressor(),
                       param_grid=param_grid,
                       cv=LeaveOneOut(),
                       verbose=20,
                       n_jobs=-1, 
                       scoring = 'neg_root_mean_squared_error')

See the documentation and the list of available scores for more.
",cross-validation
r cvglmnet error doesn39t match manual calculation,"The measure reported by the model doesn't match the manual error I calculate. I am using the cv.glmnet function from the R package glmnet. I believe when I set the function variable keep to True, the cross validation fits will be kept in fit.preval. However, my manual calculation of mean squared error using fit.preval and the index of the minimum cv error doesn't match what the model summary is saying. My manual calculation is always much less.
library(glmnet)

x <- data.frame(runif(100),runif(100),runif(100),runif(100),runif(100))
y <- data.frame(runif(100),runif(100),runif(100))

cvfit <- cv.glmnet(x = as.matrix(x), y = as.matrix(y), keep = T, family = ""mgaussian"")
mean(unlist((cvfit$fit.preval[,,cvfit$index[""min"",]] - y)^2))
## [1] 0.08803571
cvfit
## Call:  cv.glmnet(x = as.matrix(x), y = as.matrix(y), keep = T, family = ""mgaussian"") 

## Measure: Mean-Squared Error 

##      Lambda Index Measure      SE Nonzero
## min 0.06003     1  0.2641 0.01158       1
## 1se 0.06003     1  0.2641 0.01158       1

My manually calculated mean-squared error doesn't match the models measure.
","For mgaussian response, cv computes the mean squared Frobenius norm of the error.
So
errormin = (cvfit$fit.preval[,,cvfit$index[""min"",]] - y)^2
mean(rowSums(errormin))

will match the reported error
",cross-validation
getting different score values between manual cross validation and cross_val_score,"I created a python for loop to split the training dataset into stratified KFolds and used a classifier inside the loop to train it. Then used the trained model to predict with the validation data. The metrics achieved using this process where quite different to that achieved with the cross_val_score function. I expected the same results using both methods.
This code is for text classification and I use TF-IDF to vectorize the text
Code for manual implementation of cross validation:
#Importing metrics functions to measure performance of a  model
from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score
from sklearn.model_selection import StratifiedKFold
data_validation = []  # list used to store the results of model validation using cross validation
skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
accuracy_val = []
f1_val = []

# use ravel function to flatten the multi-dimensional array to a single dimension
for train_index, val_index in (skf.split(X_train, y_train)):
    X_tr, X_val = X_train.ravel()[train_index], X_train.ravel()[val_index] 
    y_tr, y_val  = y_train.ravel()[train_index] , y_train.ravel()[val_index]
    tfidf=TfidfVectorizer()
    X_tr_vec_tfidf = tfidf.fit_transform(X_tr) # vectorize the training folds
    X_val_vec_tfidf = tfidf.transform(X_val) # vectorize the validation fold    
    #instantiate model 
    model= MultinomialNB(alpha=0.5, fit_prior=False) 
    #Training the empty model with our training dataset
    model.fit(X_tr_vec_tfidf, y_tr)  
    predictions_val = model.predict(X_val_vec_tfidf) # make predictions with the validation dataset
    acc_val = accuracy_score(y_val, predictions_val)
    accuracy_val.append(acc_val)
    f_val=f1_score(y_val, predictions_val)
    f1_val.append(f_val)

avg_accuracy_val = np.mean(accuracy_val)
avg_f1_val = np.mean(f1_val)

# temp list to store the metrics 
temp = ['NaiveBayes']
temp.append(avg_accuracy_val)   #validation accuracy score 
temp.append(avg_f1_val)         #validation f1 score
data_validation.append(temp)    
#Create a table ,using dataframe, which contains the metrics for all the trained and tested ML models
result = pd.DataFrame(data_validation, columns = ['Algorithm','Accuracy Score : Validation','F1-Score  : Validation'])
result.reset_index(drop=True, inplace=True)
result      

Output:
    Algorithm   Accuracy Score : Validation     F1-Score : Validation
0   NaiveBayes  0.77012                      0.733994

Now code to use cross_val_score function:
from sklearn.model_selection import cross_val_score
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
scores = ['accuracy', 'f1']
#Text vectorization of training and testing datasets using NLP technique TF-IDF
tfidf=TfidfVectorizer()
X_tr_vec_tfidf = tfidf.fit_transform(X_train)
skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
nb=MultinomialNB(alpha=0.5, fit_prior=False) 
for score in [""accuracy"", ""f1""]:
    print (f'{score}: {cross_val_score(nb,X_tr_vec_tfidf,y_train,cv=skf,scoring=score).mean()} ')

Output:
accuracy: 0.7341283583255231 
f1: 0.7062017090972422 

As can be seen the accuracy and f1 metrics are quite different using the two methods. The difference in metrics is much worse when I use the KNeighborsClassfier.
","TL;DR: The two ways of calculation are not equivalent due to the different way you handle the TF-IDF transformation; the first calculation is the correct one.

In the first calculation you correctly apply fit_transform only to the training data of each fold, and transform to the validation data fold:
X_tr_vec_tfidf = tfidf.fit_transform(X_tr) # vectorize the training folds
X_val_vec_tfidf = tfidf.transform(X_val) # vectorize the validation fold    

But in the second calculation you do not do that; instead, you apply fit_transform to the whole of the training data, before it is split to training and validation folds:
X_tr_vec_tfidf = tfidf.fit_transform(X_train)

hence the difference. The fact that you seem to get a better accuracy with the second, wrong way of calculation, is due to information leakage (your validation data is not actually unseen, they have participated in the TF-IDF transformation).

The correct way to use cross_val_score when we have transformations is via a pipeline (API, User's Guide):
from sklearn.pipeline import Pipeline

tfidf = TfidfVectorizer()
nb = MultinomialNB(alpha=0.5, fit_prior=False) 

pipeline = Pipeline([('transformer', tfidf), ('estimator', nb)])

skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
scores = cross_val_score(pipeline, X_train, y_train, cv = skf)

",cross-validation
key error when implementing cross validation with groupkfold,"I have a df with 3 main columns 'label', 'embeddings' (features), 'chr'. I am trying to do a 10-fold cross validation by grouping the chromosomes such that the chr1 rows are all either in the train or test (not split across the train/test).
I have a df that looks like:

I believe I did it correctly in my code, but I keep running into this Key Error:

Here's my code:
import numpy as np
from sklearn.model_selection import GroupKFold

X = np.array([np.array(x) for x in mini_df['embeddings']])
y = mini_df['label']
groups = mini_df['chromosome']
group_kfold = GroupKFold(n_splits=10)

# Initialize figure for plotting
plt.figure(figsize=(10, 6))

# Perform cross-validation and plot ROC curves for each fold
for i, (train_idx, val_idx) in enumerate(group_kfold.split(X, y, groups)):
    X_train_fold, X_val_fold = X[train_idx], X[val_idx]
    y_train_fold, y_val_fold = y[train_idx], y[val_idx]
    
    # Initialize classifier
    rf_classifier = RandomForestClassifier(n_estimators=n_trees, random_state=42, max_depth=max_depth, n_jobs=-1)
    
    # Train the classifier on this fold
    rf_classifier.fit(X_train_fold, y_train_fold)
    
    # Make predictions on the validation set
    y_pred_proba = rf_classifier.predict_proba(X_val_fold)[:, 1]
    
    # Calculate ROC curve
    fpr, tpr, _ = roc_curve(y_val_fold, y_pred_proba)
    
    # Calculate AUC
    roc_auc = auc(fpr, tpr)
    
    # Plot ROC curve for this fold
    plt.plot(fpr, tpr, lw=1, alpha=0.7, label=f'ROC Fold {i+1} (AUC = {roc_auc:.2f})')

# Plot ROC for random classifier
plt.plot([0, 1], [0, 1], linestyle='--', lw=2, color='r', label='Random', alpha=0.8)

# Add labels and legend
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curves for Random Forest Classifier')
plt.legend(loc='lower right')
plt.show()

","The error appears on the y object, and not on the X object. This means that the X[train_idx] and X[val_idx] operations are executed successfully.
I see that X is a NumPy array, while y is probably a Pandas dataframe or series. You can try converting the Pandas object to a NumPy object (https://pandas.pydata.org/pandas-docs/version/0.24.0rc1/api/generated/pandas.Series.to_numpy.html):
y = mini_df['label'].to_numpy()

or if you want to keep y as a Pandas object then you should access the rows in y by index with iloc[]:
y_train_fold, y_val_fold = y.iloc[train_idx], y.iloc[val_idx]

",cross-validation
right way to use rfecv and permutation importance  sklearn,"There is a proposal to implement this in Sklearn #15075, but in the meantime, eli5 is suggested as a solution. However, I'm not sure if I'm using it the right way. This is my code:
from sklearn.datasets import make_friedman1
from sklearn.feature_selection import RFECV
from sklearn.svm import SVR
import eli5
X, y = make_friedman1(n_samples=50, n_features=10, random_state=0)
estimator = SVR(kernel=""linear"")
perm = eli5.sklearn.PermutationImportance(estimator,  scoring='r2', n_iter=10, random_state=42, cv=3)
selector = RFECV(perm, step=1, min_features_to_select=1, scoring='r2', cv=3)
selector = selector.fit(X, y)
selector.ranking_

There are a few issues:

I am not sure if I am using cross-validation the right way. PermutationImportance is using cv to validate importance on the validation set, or cross-validation should be only with RFECV? (in the example, I used cv=3 in both cases, but not sure if that's the right thing to do)

If I run eli5.show_weights(perm), I'll get: AttributeError: 'PermutationImportance' object has no attribute 'feature_importances_'. Is this because I fit using RFECV? what I'm doing is similar to the last snippet here: https://eli5.readthedocs.io/en/latest/blackbox/permutation_importance.html

as a less important issue, this gives me a warning when I set cv in eli5.sklearn.PermutationImportance :


.../lib/python3.8/site-packages/sklearn/utils/validation.py:68: FutureWarning: Pass classifier=False as keyword args. From version 0.25 passing these as positional arguments will result in an error warnings.warn(""Pass {} as keyword args. From version 0.25 ""
The whole process is a bit vague. Is there a way to do it directly in Sklearn? e.g. by adding a feature_importances attribute?
","Since the objective is to select the optimal number of features with permutation importance and recursive feature elimination, I suggest using RFECV and PermutationImportance in conjunction with a CV splitter like KFold. The code could then look like this:
import warnings
from eli5 import show_weights
from eli5.sklearn import PermutationImportance
from sklearn.datasets import make_friedman1
from sklearn.feature_selection import RFECV
from sklearn.model_selection import KFold
from sklearn.svm import SVR


warnings.filterwarnings(""ignore"", category=FutureWarning)

X, y = make_friedman1(n_samples=50, n_features=10, random_state=0)

splitter = KFold(n_splits=3) # 3 folds as in the example

estimator = SVR(kernel=""linear"")
selector = RFECV(
    PermutationImportance(estimator,  scoring='r2', n_iter=10, random_state=42, cv=splitter),
    cv=splitter,
    scoring='r2',
    step=1
)
selector = selector.fit(X, y)
selector.ranking_

show_weights(selector.estimator_)

Regarding your issues:

PermutationImportance will calculate the feature importance and RFECV the r2 scoring with the same strategy according to the splits provided by KFold.

You called show_weights on the unfitted PermutationImportance object. That is why you got an error. You should access the fitted object with the estimator_ attribute instead.

Can be ignored.


",cross-validation
crossvalidation of neural network how to treat the number of epochs,"I'm implementing a pytorch neural network (regression) and want to identify the best network topology, optimizer etc.. I use cross validation, because I have x databases of measurements and I want to evaluate whether I can train a neural network with a subset of the x databases and apply the neural network to the unseen databases. Therefore, I also introduce a test database, which I doesn't use in the phase of the hyperparameter identification.
I am confused on how to treat the number of epochs in cross validation, e.g. I have a number of epochs = 100. There are two options:

The number of epochs is a hyperparameter to tune. In each epoch, the mean error across all cross validation iterations is determined. After models are trained with all network topologies, optimizers etc. the model with the smallest mean error is determined and has parameters like: -network topology: 1
-optimizer: SGD
-number of epochs: 54
To calculate the performance on the test set, a model is trained with exactly these parameters (number of epochs = 54) on the training and the validation data. Then it is applied and evaluated on the test set.
The number of epochs is NOT a hyperparameter to tune. Models are trained with all the network topologies, optimizers etc. For each model, the number of epochs, where the error is the smallest, is used. The models are compared and the best model can be determined with parameters like:
-network topology: 1 
-optimizer: SGD
To calculate the performance on the test data, a “simple” training and validation split is used (e.g. 80-20). The model is trained with the above parameters and 100 epochs on the training and validation data. Finally, a model with a number of epochs yielding the smallest validation error, is evaluated on the test data.

Which option is the correct or the better one?
","The number of epochs is better not to be fine-tuned.
Option 2 is a better option.
Actually, if the # of epochs is fixed, you need not to have validation set. Validation set gives you the optimal epoch of the saved model.
",cross-validation
multiple metrics for neural network model with cross validation,"I am trying to get F1, precision and recall of cross validation for an LSTM model.
I know how to show the accuracies, but when I try to show the other metrics using cross_validate I get many different errors.
My code is the following:
def nn_model():
    model_lstm1 = Sequential()
    model_lstm1.add(Embedding(20000, 100, input_length=49))
    model_lstm1.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))
    model_lstm1.add(Dense(2, activation='sigmoid'))
    model_lstm1.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
    return model_lstm1

classifier = KerasClassifier(build_fn=nn_model, batch_size=10,nb_epoch=10)

scoring = {'precision' : make_scorer(precision_score),
           'recall' : make_scorer(recall_score), 
           'f1_score' : make_scorer(f1_score)}

results = cross_validate(classifier, X_train, y_train, cv=skf, scoring = scoring)

print(""F1 score SVM: %0.2f (+/- %0.2f)"" % (np.mean(results[f1_score]), np.std(results[f1_score])))

print(""precision score SVM: %0.2f (+/- %0.2f)"" % (np.mean(results[precision]), np.std(results[precision])))
print(""recall macro SVM: %0.2f (+/- %0.2f)"" % (np.mean(results[recall]), np.std(results[recall])))


The error I get is the following:
Epoch 1/1 1086/1086 [==============================] - 18s 17ms/step - loss: 0.6014 - acc: 0.7035
--------------------------------------------------------------------------- ValueError                                Traceback (most recent call last) <ipython-input-40-5afe62c11676> in <module>
      6            'f1_score' : make_scorer(f1_score)}
      7 
----> 8 results = cross_validate(classifier, X_train, y_train, cv=skf, scoring = scoring)
      9 
     10 print(""F1 score SVM: %0.2f (+/- %0.2f)"" % (np.mean(results[f1_score]), np.std(results[f1_score])))

/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/model_selection/_validation.py in cross_validate(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, return_train_score, return_estimator, error_score)
    229             return_times=True, return_estimator=return_estimator,
    230             error_score=error_score)
--> 231         for train, test in cv.split(X, y, groups))
    232 
    233     zipped_scores = list(zip(*scores))

/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/joblib/parallel.py in __call__(self, iterable)
    919             # remaining jobs.
    920             self._iterating = False
--> 921             if self.dispatch_one_batch(iterator):
    922                 self._iterating = self._original_iterator is not None
    923 

/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/joblib/parallel.py in dispatch_one_batch(self, iterator)
    757                 return False
    758             else:
--> 759                 self._dispatch(tasks)
    760                 return True
    761 

/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/joblib/parallel.py in _dispatch(self, batch)
    714         with self._lock:
    715             job_idx = len(self._jobs)
--> 716             job = self._backend.apply_async(batch, callback=cb)
    717             # A job can complete so quickly than its callback is
    718             # called before we get here, causing self._jobs to

/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/joblib/_parallel_backends.py in apply_async(self, func, callback)
    180     def apply_async(self, func, callback=None):
    181         """"""Schedule a func to be run""""""
--> 182         result = ImmediateResult(func)
    183         if callback:
    184             callback(result)

/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/joblib/_parallel_backends.py in __init__(self, batch)
    547         # Don't delay the application, to avoid keeping the input
    548         # arguments in memory
--> 549         self.results = batch()
    550 
    551     def get(self):

/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/joblib/parallel.py in __call__(self)
    223         with parallel_backend(self._backend, n_jobs=self._n_jobs):
    224             return [func(*args, **kwargs)
--> 225                     for func, args, kwargs in self.items]
    226 
    227     def __len__(self):

/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/joblib/parallel.py in <listcomp>(.0)
    223         with parallel_backend(self._backend, n_jobs=self._n_jobs):
    224             return [func(*args, **kwargs)
--> 225                     for func, args, kwargs in self.items]
    226 
    227     def __len__(self):

/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/model_selection/_validation.py in _fit_and_score(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, error_score)
    552         fit_time = time.time() - start_time
    553         # _score will return dict if is_multimetric is True
--> 554         test_scores = _score(estimator, X_test, y_test, scorer, is_multimetric)
    555         score_time = time.time() - start_time - fit_time
    556         if return_train_score:

/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/model_selection/_validation.py in _score(estimator, X_test, y_test, scorer, is_multimetric)
    595     """"""
    596     if is_multimetric:
--> 597         return _multimetric_score(estimator, X_test, y_test, scorer)
    598     else:
    599         if y_test is None:

/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/model_selection/_validation.py in _multimetric_score(estimator, X_test, y_test, scorers)
    625             score = scorer(estimator, X_test)
    626         else:
--> 627             score = scorer(estimator, X_test, y_test)
    628 
    629         if hasattr(score, 'item'):

/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/metrics/scorer.py in __call__(self, estimator, X, y_true, sample_weight)
     95         else:
     96             return self._sign * self._score_func(y_true, y_pred,
---> 97                                                  **self._kwargs)
     98 
     99 

/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/metrics/classification.py in precision_score(y_true, y_pred, labels, pos_label, average, sample_weight)    1567                                                 average=average,    1568                                               warn_for=('precision',),
-> 1569                                                  sample_weight=sample_weight)    1570     return p    1571 

/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/metrics/classification.py in precision_recall_fscore_support(y_true, y_pred, beta, labels, pos_label, average, warn_for, sample_weight)    1413         raise ValueError(""beta should be >0 in the F-beta score"")    1414     labels
= _check_set_wise_labels(y_true, y_pred, average, labels,
-> 1415                                     pos_label)    1416     1417     # Calculate tp_sum, pred_sum, true_sum ###

/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/metrics/classification.py in _check_set_wise_labels(y_true, y_pred, average, labels, pos_label)  1237                          str(average_options))    1238 
-> 1239     y_type, y_true, y_pred = _check_targets(y_true, y_pred)    1240     present_labels = unique_labels(y_true, y_pred)    1241     if average == 'binary':

/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/metrics/classification.py in _check_targets(y_true, y_pred)
     79     if len(y_type) > 1:
     80         raise ValueError(""Classification metrics can't handle a mix of {0} ""
---> 81                          ""and {1} targets"".format(type_true, type_pred))
     82 
     83     # We can't have more than one value on y_type => The set is no more needed

ValueError: Classification metrics can't handle a mix of multilabel-indicator and binary targets

What am I doing wrong?
","Issue in your code

You cant use hot-one-encoded labels link. Use raw labels. You can use sparse_categorical_crossentropy loss with raw labels.
cross_validate returns scores as test_scores. For train scores set return_train_score

Corrected code
def nn_model():
    model_lstm1 = Sequential()
    model_lstm1.add(Embedding(200, 100, input_length=10))
    model_lstm1.add(LSTM(10, dropout=0.2, recurrent_dropout=0.2))
    model_lstm1.add(Dense(2, activation='sigmoid'))
    model_lstm1.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
    return model_lstm1

classifier = KerasClassifier(build_fn=nn_model, batch_size=10,nb_epoch=10)

scoring = {'precision' : make_scorer(precision_score),
           'recall' : make_scorer(recall_score), 
           'f1_score' : make_scorer(f1_score)}

results = cross_validate(classifier, np.random.randint(0,100,(1000,10)), 
                         np.random.np.random.randint(0,2,1000), scoring = scoring, cv=3, return_train_score=True)

print(""F1 score SVM: %0.2f (+/- %0.2f)"" % (np.mean(results['test_f1_score']), np.std(results['test_f1_score'])))
print(""precision score SVM: %0.2f (+/- %0.2f)"" % (np.mean(results['test_precision']), np.std(results['test_precision'])))
print(""recall macro SVM: %0.2f (+/- %0.2f)"" % (np.mean(results['test_recall']), np.std(results['test_recall'])))

Output
Epoch 1/1
666/666 [==============================] - 5s 7ms/step - loss: 0.6932 - acc: 0.5075
Epoch 1/1
667/667 [==============================] - 5s 7ms/step - loss: 0.6929 - acc: 0.5127
Epoch 1/1
667/667 [==============================] - 5s 7ms/step - loss: 0.6934 - acc: 0.5007
F1 score SVM: 0.10 (+/- 0.09)
precision score SVM: 0.43 (+/- 0.07)
recall macro SVM: 0.06 (+/- 0.06)

You might get
UndefinedMetricWarning: ....

warnings in initials epochs (if data is low), which you can ignore. This is because the classifier is classifying all the data to one class and no data into the  another class.
",cross-validation
using groupshufflesplit with gridsearchcv and cross_val_score for nested crossvalidation,"I am trying to perform nested cross-validation while also incorporating group-based splitting using the GroupShuffleSplit class. However, I'm encountering a ""TypeError: cannot pickle 'generator' object"" when trying to use a custom cross-validation object with GridSearchCV. As fas as i know this Error occurs because group_split.split(...) returns an generator which cant be used in the cross_val_score function. Therefore i want to ask if there is a way to easily use GroupShuffleSplit for nested cross-validation.
Regarding my simplified sample code:
I have a dataset with features X, labels y, and group labels groups. The goal is to perform nested cross-validation, where both the inner and outer loops split the data based on the group labels. I would like to use GridSearchCV for hyperparameter tuning and cross_val_score for evaluating the performance.
import numpy as np
from sklearn.model_selection import GroupShuffleSplit, GridSearchCV, cross_val_score
from sklearn.ensemble import RandomForestClassifier

X = np.random.rand(100, 10)
y = np.random.randint(2, size=100)
groups = np.random.randint(4, size=100)  # Example group labels

rf_classifier = RandomForestClassifier()
param_grid = {'n_estimators': [50, 100, 200]}

inner_cv = GroupShuffleSplit(n_splits=5, test_size=0.2)
outer_cv = GroupShuffleSplit(n_splits=5, test_size=0.2)

grid_search = GridSearchCV(estimator=rf_classifier, param_grid=param_grid, cv=inner_cv.split(X, y, groups=groups))
nested_scores = cross_val_score(estimator=grid_search, X=X, y=y, cv=outer_cv.split(X, y, groups=groups))

Resulting in the following Stacktrace Error:
---------------------------------------------------------------------------
Empty                                     Traceback (most recent call last)
File c:\Anaconda3_x64\lib\site-packages\joblib\parallel.py:825, in Parallel.dispatch_one_batch(self, iterator)
    824 try:
--> 825     tasks = self._ready_batches.get(block=False)
    826 except queue.Empty:
    827     # slice the iterator n_jobs * batchsize items at a time. If the
    828     # slice returns less than that, then the current batchsize puts
   (...)
    831     # accordingly to distribute evenly the last items between all
    832     # workers.

File c:\Anaconda3_x64\lib\queue.py:168, in Queue.get(self, block, timeout)
    167     if not self._qsize():
--> 168         raise Empty
    169 elif timeout is None:

Empty: 

During handling of the above exception, another exception occurred:

TypeError                                 Traceback (most recent call last)
Cell In[29], line 16
     13 outer_cv = GroupShuffleSplit(n_splits=5, test_size=0.2)
     15 grid_search = GridSearchCV(estimator=rf_classifier, param_grid=param_grid, cv=inner_cv.split(X, y, groups=groups))
---> 16 nested_scores = cross_val_score(estimator=grid_search, X=X, y=y, cv=outer_cv.split(X, y, groups=groups))
     18 print(nested_scores)

File c:\Anaconda3_x64\lib\site-packages\sklearn\model_selection\_validation.py:515, in cross_val_score(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, error_score)
    512 # To ensure multimetric format is not supported
    513 scorer = check_scoring(estimator, scoring=scoring)
--> 515 cv_results = cross_validate(
    516     estimator=estimator,
    517     X=X,
    518     y=y,
    519     groups=groups,
    520     scoring={""score"": scorer},
    521     cv=cv,
    522     n_jobs=n_jobs,
    523     verbose=verbose,
    524     fit_params=fit_params,
    525     pre_dispatch=pre_dispatch,
    526     error_score=error_score,
    527 )
    528 return cv_results[""test_score""]

File c:\Anaconda3_x64\lib\site-packages\sklearn\model_selection\_validation.py:266, in cross_validate(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, return_train_score, return_estimator, error_score)
    263 # We clone the estimator to make sure that all the folds are
    264 # independent, and that it is pickle-able.
    265 parallel = Parallel(n_jobs=n_jobs, verbose=verbose, pre_dispatch=pre_dispatch)
--> 266 results = parallel(
    267     delayed(_fit_and_score)(
    268         clone(estimator),
    269         X,
    270         y,
    271         scorers,
    272         train,
    273         test,
    274         verbose,
    275         None,
    276         fit_params,
    277         return_train_score=return_train_score,
    278         return_times=True,
    279         return_estimator=return_estimator,
    280         error_score=error_score,
    281     )
    282     for train, test in cv.split(X, y, groups)
    283 )
    285 _warn_or_raise_about_fit_failures(results, error_score)
    287 # For callabe scoring, the return type is only know after calling. If the
    288 # return type is a dictionary, the error scores can now be inserted with
    289 # the correct key.

File c:\Anaconda3_x64\lib\site-packages\sklearn\utils\parallel.py:63, in Parallel.__call__(self, iterable)
     58 config = get_config()
     59 iterable_with_config = (
     60     (_with_config(delayed_func, config), args, kwargs)
     61     for delayed_func, args, kwargs in iterable
     62 )
---> 63 return super().__call__(iterable_with_config)

File c:\Anaconda3_x64\lib\site-packages\joblib\parallel.py:1048, in Parallel.__call__(self, iterable)
   1039 try:
   1040     # Only set self._iterating to True if at least a batch
   1041     # was dispatched. In particular this covers the edge
   (...)
   1045     # was very quick and its callback already dispatched all the
   1046     # remaining jobs.
   1047     self._iterating = False
-> 1048     if self.dispatch_one_batch(iterator):
   1049         self._iterating = self._original_iterator is not None
   1051     while self.dispatch_one_batch(iterator):

File c:\Anaconda3_x64\lib\site-packages\joblib\parallel.py:836, in Parallel.dispatch_one_batch(self, iterator)
    833 n_jobs = self._cached_effective_n_jobs
    834 big_batch_size = batch_size * n_jobs
--> 836 islice = list(itertools.islice(iterator, big_batch_size))
    837 if len(islice) == 0:
    838     return False

File c:\Anaconda3_x64\lib\site-packages\sklearn\utils\parallel.py:59, in <genexpr>(.0)
     54 # Capture the thread-local scikit-learn configuration at the time
     55 # Parallel.__call__ is issued since the tasks can be dispatched
     56 # in a different thread depending on the backend and on the value of
     57 # pre_dispatch and n_jobs.
     58 config = get_config()
---> 59 iterable_with_config = (
     60     (_with_config(delayed_func, config), args, kwargs)
     61     for delayed_func, args, kwargs in iterable
     62 )
     63 return super().__call__(iterable_with_config)

File c:\Anaconda3_x64\lib\site-packages\sklearn\model_selection\_validation.py:268, in <genexpr>(.0)
    263 # We clone the estimator to make sure that all the folds are
    264 # independent, and that it is pickle-able.
    265 parallel = Parallel(n_jobs=n_jobs, verbose=verbose, pre_dispatch=pre_dispatch)
    266 results = parallel(
    267     delayed(_fit_and_score)(
--> 268         clone(estimator),
    269         X,
    270         y,
    271         scorers,
    272         train,
    273         test,
    274         verbose,
    275         None,
    276         fit_params,
    277         return_train_score=return_train_score,
    278         return_times=True,
    279         return_estimator=return_estimator,
    280         error_score=error_score,
    281     )
    282     for train, test in cv.split(X, y, groups)
    283 )
    285 _warn_or_raise_about_fit_failures(results, error_score)
    287 # For callabe scoring, the return type is only know after calling. If the
    288 # return type is a dictionary, the error scores can now be inserted with
    289 # the correct key.

File c:\Anaconda3_x64\lib\site-packages\sklearn\base.py:89, in clone(estimator, safe)
     87 new_object_params = estimator.get_params(deep=False)
     88 for name, param in new_object_params.items():
---> 89     new_object_params[name] = clone(param, safe=False)
     90 new_object = klass(**new_object_params)
     91 params_set = new_object.get_params(deep=False)

File c:\Anaconda3_x64\lib\site-packages\sklearn\base.py:70, in clone(estimator, safe)
     68 elif not hasattr(estimator, ""get_params"") or isinstance(estimator, type):
     69     if not safe:
---> 70         return copy.deepcopy(estimator)
     71     else:
     72         if isinstance(estimator, type):

File c:\Anaconda3_x64\lib\copy.py:161, in deepcopy(x, memo, _nil)
    159 reductor = getattr(x, ""__reduce_ex__"", None)
    160 if reductor is not None:
--> 161     rv = reductor(4)
    162 else:
    163     reductor = getattr(x, ""__reduce__"", None)

TypeError: cannot pickle 'generator' object

","I'm not sure that this is possible before version 1.3 without writing a manual loop to replace cross_val_score.  Besides the generator issue, you're trying to tell the grid search object that it should split all of X, but it won't see all of X (it having already been split by the outer splitter).
In 1.3, we get metadata routing which automatically routes groups to group splitters.  Then we can do e.g.
from sklearn import set_config
set_config(enable_metadata_routing=True)

grid_search = GridSearchCV(estimator=rf_classifier, param_grid=param_grid, cv=inner_cv)
nested_scores = cross_val_score(estimator=grid_search, X=X, y=y, cv=outer_cv, params={'groups': groups})

Just to check that this really routes to both splitters, here's a modified version of your script:
import numpy as np
import pandas as pd
from sklearn.model_selection import GroupShuffleSplit, GridSearchCV, cross_val_score
from sklearn.base import BaseEstimator, ClassifierMixin
from sklearn import set_config

set_config(enable_metadata_routing=True)

X = np.random.rand(100, 10)
y = np.random.randint(2, size=100)
groups = np.random.randint(4, size=100)  # Example group labels

X = pd.DataFrame(X)


class MyClassifier(ClassifierMixin, BaseEstimator):
    def __init__(self, n_estimators=1):
        self.n_estimators = n_estimators

    def fit(self, X, y):
        print(""train: "", groups[X.index])
        return self
    
    def predict(self, X):
        print(""test: "", groups[X.index])
        return np.random.randint(2, size=len(X))


rf_classifier = MyClassifier()
param_grid = {'n_estimators': [50, 100]}

inner_cv = GroupShuffleSplit(n_splits=2, test_size=0.33)
outer_cv = GroupShuffleSplit(n_splits=2, test_size=0.25)

grid_search = GridSearchCV(estimator=rf_classifier, param_grid=param_grid, cv=inner_cv, verbose=10)
nested_scores = cross_val_score(estimator=grid_search, X=X, y=y, cv=outer_cv, params={'groups': groups}, verbose=10)

print(nested_score)

The outer splits put a single group in the test set, then the inner splits pick one of the remaining three as test and the last two in train.  Here's my output:
[CV] START .....................................................................
Fitting 2 folds for each of 2 candidates, totalling 4 fits
[CV 1/2; 1/2] START n_estimators=50.............................................
train:  [1 3 3 3 1 3 3 1 1 1 1 1 3 1 1 1 3 3 1 3 3 3 3 1 1 1 3 3 3 3 3 3 3 3 3 1 3
 3 3 3 1 3 1 1 1 3 3 1 1 3 1 1 1 1 1 1]
test:  [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]
[CV 1/2; 1/2] END ..............n_estimators=50;, score=0.353 total time=   0.0s
[CV 2/2; 1/2] START n_estimators=50.............................................
train:  [3 3 3 3 3 0 0 3 3 3 3 3 3 0 3 0 0 0 3 3 0 0 0 3 3 0 0 3 3 3 3 3 3 3 3 3 0
 0 0 0 3 3 3 0 0 3]
test:  [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]
[CV 2/2; 1/2] END ..............n_estimators=50;, score=0.407 total time=   0.0s
[CV 1/2; 2/2] START n_estimators=100............................................
train:  [1 3 3 3 1 3 3 1 1 1 1 1 3 1 1 1 3 3 1 3 3 3 3 1 1 1 3 3 3 3 3 3 3 3 3 1 3
 3 3 3 1 3 1 1 1 3 3 1 1 3 1 1 1 1 1 1]
test:  [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]
[CV 1/2; 2/2] END .............n_estimators=100;, score=0.412 total time=   0.0s
[CV 2/2; 2/2] START n_estimators=100............................................
train:  [3 3 3 3 3 0 0 3 3 3 3 3 3 0 3 0 0 0 3 3 0 0 0 3 3 0 0 3 3 3 3 3 3 3 3 3 0
 0 0 0 3 3 3 0 0 3]
test:  [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]
[CV 2/2; 2/2] END .............n_estimators=100;, score=0.333 total time=   0.0s
train:  [1 3 3 3 1 3 3 1 1 1 0 1 1 0 3 1 1 1 3 3 1 3 3 3 0 3 1 1 0 1 0 0 3 3 0 0 0
 3 3 0 0 3 3 3 3 3 1 3 3 3 3 0 0 1 0 0 3 1 1 1 3 3 1 1 0 0 3 1 1 1 1 1 1]
test:  [2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2]
[CV] END ................................ score: (test=0.481) total time=   0.0s
[CV] START .....................................................................
Fitting 2 folds for each of 2 candidates, totalling 4 fits
[CV 1/2; 1/2] START n_estimators=50.............................................
train:  [3 3 3 3 2 3 2 2 2 3 2 3 2 3 2 2 2 3 3 2 3 3 2 2 2 3 3 2 2 3 3 2 2 2 3 3 3
 3 3 3 3 2 3 3 2 2 3 2 2 2 2 3 3 2 3 2]
test:  [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]
[CV 1/2; 1/2] END ..............n_estimators=50;, score=0.588 total time=   0.0s
[CV 2/2; 1/2] START n_estimators=50.............................................
train:  [3 3 3 3 2 3 2 2 2 3 2 3 2 3 2 2 2 3 3 2 3 3 2 2 2 3 3 2 2 3 3 2 2 2 3 3 3
 3 3 3 3 2 3 3 2 2 3 2 2 2 2 3 3 2 3 2]
test:  [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]
[CV 2/2; 1/2] END ..............n_estimators=50;, score=0.588 total time=   0.0s
[CV 1/2; 2/2] START n_estimators=100............................................
train:  [3 3 3 3 2 3 2 2 2 3 2 3 2 3 2 2 2 3 3 2 3 3 2 2 2 3 3 2 2 3 3 2 2 2 3 3 3
 3 3 3 3 2 3 3 2 2 3 2 2 2 2 3 3 2 3 2]
test:  [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]
[CV 1/2; 2/2] END .............n_estimators=100;, score=0.647 total time=   0.0s
[CV 2/2; 2/2] START n_estimators=100............................................
train:  [3 3 3 3 2 3 2 2 2 3 2 3 2 3 2 2 2 3 3 2 3 3 2 2 2 3 3 2 2 3 3 2 2 2 3 3 3
 3 3 3 3 2 3 3 2 2 3 2 2 2 2 3 3 2 3 2]
test:  [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]
[CV 2/2; 2/2] END .............n_estimators=100;, score=0.471 total time=   0.0s
train:  [3 3 3 3 2 3 2 2 0 2 0 3 2 3 2 3 2 2 2 3 3 2 3 0 3 2 2 2 0 0 0 3 3 2 0 0 2
 0 3 3 0 0 2 2 2 3 3 3 3 3 3 3 2 3 3 2 2 0 0 0 0 3 2 2 2 2 3 3 2 0 0 3 2]
test:  [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]
[CV] END ................................ score: (test=0.593) total time=   0.0s
[0.48148148 0.59259259]
[Parallel(n_jobs=1)]: Done   1 tasks      | elapsed:    0.0s

",cross-validation
application and deployment of kfold crossvalidation,"K-Fold Cross Validation is a technique applied for splitting up the data into K number of Folds for testing and training. The goal is to estimate the generalizability of a machine learning model. The model is trained K times, once on each train fold and then tested on the corresponding test fold.
Suppose I want to compare a Decision Tree and a Logistic Regression model on some arbitrary dataset with 10 Folds. Suppose after training each model on each of the 10 folds and obtaining the corresponding test accuracies, Logistic Regression has a higher mean accuracy across the test folds, indicating that it is the better model for the dataset.
Now, for application and deployment. Do I retrain the Logistic Regression model on all the data, or do I create an ensemble from the 10 Logistic Regression models that were trained on the K-Folds?
","The main goal of CV is to validate that we did not get the numbers by chance. So, I believe you can just use a single model for deployment.
If you are already satisfied with hyper-parameters and model performance one option is to train on all data that you have and deploy that model.
And, the other option is obvious that you can deploy one of the CV models.
About the ensemble option, I believe it should not give significant better results than a model trained on all data; as each model train for same amount of time with similar paparameters and they have similar architecture; but train data is slightly different. So, they shouldn't show different performance. In my experience, ensemble helps when the output of models are different due to architecture or input data (like different image sizes).
",cross-validation
scikitlearn scores are different when using cross_val_predict vs cross_val_score,"I expected both methods to return rather similar errors, can someone point me to the mistake please?
Calculating RMSE...
rf = RandomForestRegressor(random_state=555, n_estimators=100, max_depth=8)
rf_preds = cross_val_predict(rf, train_, targets, cv=7, n_jobs=7) 
print(""RMSE Score using cv preds: {:0.5f}"".format(metrics.mean_squared_error(targets, rf_preds, squared=False)))

scores = cross_val_score(rf, train_, targets, cv=7, scoring='neg_root_mean_squared_error', n_jobs=7)
print(""RMSE Score using cv_score: {:0.5f}"".format(scores.mean() * -1))


RMSE Score using cv preds: 0.01658
RMSE Score using cv_score: 0.01073

","There are two issues here, both of which are mentioned in the documentation of cross_val_predict:

Results can differ from cross_validate and cross_val_score unless all tests sets have equal size and the metric decomposes over samples.

The first is to make all sets (training and test) the same in both cases, which is not the case in your example. To do so, we need to employ the kfold method in order to define our CV folds, and then use these same folds in both cases. Here is an example with dummy data:
from sklearn.datasets import make_regression
from sklearn.model_selection import KFold, cross_val_score, cross_val_predict
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error

X, y = make_regression(n_samples=2000, n_features=4, n_informative=2,
                      random_state=42, shuffle=False)

rf = RandomForestRegressor(max_depth=2, random_state=0)
kf = KFold(n_splits=5)

rf_preds = cross_val_predict(rf, X, y, cv=kf, n_jobs=5) 
print(""RMSE Score using cv preds: {:0.5f}"".format(mean_squared_error(y, rf_preds, squared=False)))

scores = cross_val_score(rf, X, y, cv=kf, scoring='neg_root_mean_squared_error', n_jobs=5)
print(""RMSE Score using cv_score: {:0.5f}"".format(scores.mean() * -1))

The result of the above code snippet (fully reproducible, since we have explicitly set all the necessary random seeds) is:
RMSE Score using cv preds: 15.16839
RMSE Score using cv_score: 15.16031

So, we can see that the two scores are indeed similar, but still not identical.
Why is that? The answer lies in the rather cryptic second part of the quoted sentence above, i.e. the RMSE score does not decompose over samples (to be honest, I don't know any ML score that it does).
In simple words, while cross_val_predict computes the RMSE strictly according to its definition, i.e. (pseudocode):
RMSE = square_root([(y[1] - y_pred[1])^2 + (y[2] - y_pred[2])^2 + ... + (y[n] - y_pred[n])^2]/n)

where n is the number of samples, the cross_val_score method does not do exactly that; what it does instead is that it computes the RMSE for each one of the k CV folds, and then averages these k values, i.e. (pseudocode again):
RMSE = (RMSE[1] + RMSE[2] + ... + RMSE[k])/k

And exactly because the RMSE is not decomposable over the samples, these two values, although close, are not identical.
We can actually demonstrate that this is the case indeed, by doing the CV procedure manually and emulating the RMSE calculation as done by cross_val_score and described above, i.e.:
import numpy as np
RMSE__cv_score = []

for train_index, val_index in kf.split(X):
    rf.fit(X[train_index], y[train_index])
    pred = rf.predict(X[val_index])
    err = mean_squared_error(y[val_index], pred, squared=False)
    RMSE__cv_score.append(err)

print(""RMSE Score using manual cv_score: {:0.5f}"".format(np.mean(RMSE__cv_score)))

The result being:
RMSE Score using manual cv_score: 15.16031

i.e. identical with the one returned by cross_val_score above.
So, if we want to be very precise, the truth is that the correct RMSE (i.e. calculated exactly according to its definition) is the one returned by cross_val_predict; cross_val_score returns an approximation of it. But in practice, we often find that the difference is not that significant, so we can also use cross_val_score if it is more convenient.
",cross-validation
target transformation and feature selection valueerror input x contains nan,"I am using RFECV for feature selection in scikit-learn. I would like to run an XGBoost model with log(y) because I have been able to demonstrate that this is performing better than just y.
Simple model without transformation: no problem, RFECV works fine and I can get the number of features.
Log-transformed model = problem: I have an error saying:

""ValueError: Input X contains NaN; RFECV does not accept missing
values encoded as NaN natively. For supervised learning, you might
want to consider sklearn.ensemble.HistGradientBoostingClassifier and
Regressor which accept missing values encoded as NaNs natively.
Alternatively, it is possible to preprocess the data, for instance by
using an imputer transformer in a pipeline or drop samples with
missing values. See
https://scikit-learn.org/stable/modules/impute.html You can find a
list of all estimators that handle NaN values at the following page:
https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values""

What I don't understand is that I do not have a NaN issue with the simple model but I do with the log-transformed one. I do not have NaN in the target y.
How can I solve my problem and be able to run RFECV with a log-transformed target?
# Base estimator
rs = 45
xgboost_reg = xgb.XGBRegressor(random_state = rs, 
                                grow_policy = ""depthwise"", 
                                booster = ""gbtree"", # gblinear or dart; gbtree and dart use tree based models while gblinear uses linear functions.
                                tree_method = ""auto"", # pick best option between hist, exact and approx
                                n_estimators = randint(300,500).rvs(random_state = rs),
                                subsample = uniform(0.5, 0.5).rvs(random_state = rs),
                                max_depth = randint(3,10).rvs(random_state = rs),
                                learning_rate = loguniform(0.05, 0.2).rvs(random_state = rs),
                                colsample_bytree = uniform(0.5, 0.5).rvs(random_state = rs),
                                min_child_weight =  randint(1,20).rvs(random_state = rs),
                                gamma = uniform(0.5, 1).rvs(random_state = rs),
                                reg_alpha = uniform(0.0, 1.0).rvs(random_state = rs),
                                reg_lambda = uniform(0.0, 1.0).rvs(random_state = rs),
                                max_delta_step = randint(1,10).rvs(random_state = rs)
)

# RFECV settings
n_features = 89
step = 20
n_scores = 2
min_features_to_select = 9

# Simple model = working
rfecv = RFECV(
    xgboost_reg,
    step=step,
    cv=4,
    scoring=""neg_root_mean_squared_error"",
    min_features_to_select= min_features_to_select,
    n_jobs=-1, 
)
rfecv.fit(x, y)
print(rfecv.n_features_)

# Log-transformed model = error
log_estimator = TransformedTargetRegressor(regressor=xgboost_reg,
                                             func=np.log,
                                             inverse_func=np.exp)
rfecv_log = RFECV(
    estimator= log_estimator,
    step=step,
    cv=4,
    scoring=""neg_root_mean_squared_error"",
    min_features_to_select= min_features_to_select,
    n_jobs=-1, 
)
rfecv_log.fit(x, y)
print(rfecv_log.n_features_)

","Revised Answer: The error is due to x containing Nan values. The problem can be resolved by updating scikit-learn to the current version 1.4.0, which allows Nan values in RFECV.
Note: The original suggestion was to check for negative values in the target, as np.log() for negative numbers produces nan values, which was not the cause of the problem.
",cross-validation
how is scikitlearn cross_val_predict accuracy score calculated,"Does the cross_val_predict (see doc,  v0.18) with k-fold method as shown in the code below calculate accuracy for each fold and average them finally or not? 
cv = KFold(len(labels), n_folds=20)
clf = SVC()
ypred = cross_val_predict(clf, td, labels, cv=cv)
accuracy = accuracy_score(labels, ypred)
print accuracy

","No, it does not!
According to cross validation doc page, cross_val_predict does not return any scores but only the labels based on a certain strategy which is described here:

The function cross_val_predict has a similar interface to
  cross_val_score, but returns, for each element in the input, the
  prediction that was obtained for that element when it was in the test
  set. Only cross-validation strategies that assign all elements to a
  test set exactly once can be used (otherwise, an exception is raised).

And therefore by calling accuracy_score(labels, ypred) you are just calculating accuracy scores of labels predicted by aforementioned particular strategy compared to the true labels. This again is specified in the same documentation page:

These prediction can then be used to evaluate the classifier:
predicted = cross_val_predict(clf, iris.data, iris.target, cv=10) 
metrics.accuracy_score(iris.target, predicted)

Note that the result of this computation may be slightly different
  from those obtained using cross_val_score as the elements are grouped
  in different ways.

If you need accuracy scores of different folds you should try:
>>> scores = cross_val_score(clf, X, y, cv=cv)
>>> scores                                              
array([ 0.96...,  1.  ...,  0.96...,  0.96...,  1.        ])

and then for the mean accuracy of all folds use scores.mean():
>>> print(""Accuracy: %0.2f (+/- %0.2f)"" % (scores.mean(), scores.std() * 2))
Accuracy: 0.98 (+/- 0.03)


How to calculate Cohen kappa coefficient and confusion matrix for each fold?
For calculating Cohen Kappa coefficient and confusion matrix I assumed you mean kappa coefficient and confusion matrix between true labels and each fold's predicted labels:
from sklearn.model_selection import KFold
from sklearn.svm.classes import SVC
from sklearn.metrics.classification import cohen_kappa_score
from sklearn.metrics import confusion_matrix

cv = KFold(len(labels), n_folds=20)
clf = SVC()
for train_index, test_index in cv.split(X):
    clf.fit(X[train_index], labels[train_index])
    ypred = clf.predict(X[test_index])
    kappa_score = cohen_kappa_score(labels[test_index], ypred)
    confusion_matrix = confusion_matrix(labels[test_index], ypred)


What does cross_val_predict return?
It uses KFold to split the data to k parts and then for i=1..k iterations:

takes i'th part as the test data and all other parts as training data
trains the model with training data (all parts except i'th)
then by using this trained model, predicts labels for i'th part (test data)

In each iteration, label of i'th part of data gets predicted. In the end cross_val_predict merges all partially predicted labels and returns them as the final result.
This code shows this process step by step:
X = np.array([[0], [1], [2], [3], [4], [5]])
labels = np.array(['a', 'a', 'a', 'b', 'b', 'b'])

cv = KFold(len(labels), n_folds=3)
clf = SVC()
ypred_all = np.chararray((labels.shape))
i = 1
for train_index, test_index in cv.split(X):
    print(""iteration"", i, "":"")
    print(""train indices:"", train_index)
    print(""train data:"", X[train_index])
    print(""test indices:"", test_index)
    print(""test data:"", X[test_index])
    clf.fit(X[train_index], labels[train_index])
    ypred = clf.predict(X[test_index])
    print(""predicted labels for data of indices"", test_index, ""are:"", ypred)
    ypred_all[test_index] = ypred
    print(""merged predicted labels:"", ypred_all)
    i = i+1
    print(""====================================="")
y_cross_val_predict = cross_val_predict(clf, X, labels, cv=cv)
print(""predicted labels by cross_val_predict:"", y_cross_val_predict)

The result is:
iteration 1 :
train indices: [2 3 4 5]
train data: [[2] [3] [4] [5]]
test indices: [0 1]
test data: [[0] [1]]
predicted labels for data of indices [0 1] are: ['b' 'b']
merged predicted labels: ['b' 'b' '' '' '' '']
=====================================
iteration 2 :
train indices: [0 1 4 5]
train data: [[0] [1] [4] [5]]
test indices: [2 3]
test data: [[2] [3]]
predicted labels for data of indices [2 3] are: ['a' 'b']
merged predicted labels: ['b' 'b' 'a' 'b' '' '']
=====================================
iteration 3 :
train indices: [0 1 2 3]
train data: [[0] [1] [2] [3]]
test indices: [4 5]
test data: [[4] [5]]
predicted labels for data of indices [4 5] are: ['a' 'a']
merged predicted labels: ['b' 'b' 'a' 'b' 'a' 'a']
=====================================
predicted labels by cross_val_predict: ['b' 'b' 'a' 'b' 'a' 'a']

",cross-validation
how does scikit39s rfecv class compute cv_results_,"My understanding of Recursive Feature Elimination Cross Validation: (sklearn.feature_selection.RFECV) You provide an algorithm which is trained on the entire dataset and creates a feature importance ranking using attributes coef_ or feature_importances_. Now with all features included, this algorithm is evaluated by cross validation. Then the feature ranked at the bottom is removed and the model is retrained on the dataset and creates a new ranking, once again assessed by cross validation. This continues until all but one feature remain (or as specified by min_features_to_select), and the final number of features chosen depends on what yielded the highest CV score. (Source)
Question: The CV score for each number of features is stored in rfecv.cv_results_[""mean_test_score""], and I've been facing trouble trying to replicate these scores without using scikit's built in method.
This is what I have tried to obtain the score for n-1 features, where n is the total number of features.
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import StratifiedKFold
from sklearn.model_selection import cross_validate
from sklearn.feature_selection import RFECV

alg = DecisionTreeClassifier(random_state = 0)
cv_split = StratifiedKFold(5)
# train is a pandas dataframe, x_var and y_var are both lists containing variable strings
X = train[x_var]
y = np.ravel(train[y_var])

alg.fit(X, y)
lowest_ranked_feature = np.argmin(alg.feature_importances_)
x_var.pop(lowest_ranked_feature)

one_removed_feature = train[x_var]
alg.fit(one_removed_feature, y)
cv_score = cross_validate(alg, one_removed_feature, y, cv=cv_split, scoring=""accuracy"")
np.mean(cv_score[""test_score""])

And this is the inbuilt method that provides a different score:
rfecv = RFECV(
    estimator=alg,
    step=1,
    cv=cv_split,
    scoring=""accuracy"",
)

rfecv.fit(X, y)
rfecv.cv_results_[""mean_test_score""][-2]

How do I get the exact scores as calculated in the inbuilt method?
I would also like to mention that I did try this first with all n features, and my method matched with
rfecv.cv_results_[""mean_test_score""][-1].
","As pointed out by Ben, the reason you are finding different answers is because your understanding of RFECV is fundamentally flawed. Cross validation is not implemented at each step within RFE, but rather RFE is implemented within each fold of cross validation.
Your method currently removes a feature from the data and then performs CV to score it, and essentially performs a different CV to score each subset of features. However, RFECV performs CV only a single time at the start, and then cycles between removing a feature and scoring the model within a singular fold.
Note: From experimentation, I have found that sklearn RFECV computes the score on a singular fold directly by using whichever evaluation metric you specify, and not another layer of cross validation or anything. In your case, you have chosen scoring=""accuracy"". So within a fold, the model will score the test split based on accuracy, and repeat this for each subset of features.
You can implement RFECV from scratch as follows:
import numpy as np
import pandas as pd
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import StratifiedKFold
from sklearn.metrics import accuracy_score

def score(alg, x_train, y_train, x_test, y_test):
    """"""Calculate the accuracy score of the algorithm.""""""
    alg.fit(x_train, y_train)
    y_pred = alg.predict(x_test)
    return accuracy_score(y_test, y_pred)

def rfecv_function(alg, x_var, X, y, cv):
    """"""Perform RFECV and return a dictionary to store an array of test scores
    for each cv, where the array contains test scores for each number of 
    features selected 1,2,...,n (n is total number of features).
    """"""
    dic = {}
    # Iterate through folds
    for fold_index, (train_index, test_index) in enumerate(cv.split(X, y)):
        x_train_fold, y_train_fold = X.iloc[train_index], y.iloc[train_index]
        x_test_fold, y_test_fold = X.iloc[test_index], y.iloc[test_index]
        features = x_var.copy()
        
        # Array to store test scores for each feature subset
        scores_array = np.empty(len(x_var))
        
        # Iterate through the feature subsets
        for i in range(len(x_var)):
            # Calculate and store the scores in the array
            scores = score(alg, x_train_fold[features], y_train_fold, 
                           x_test_fold[features], y_test_fold)
            scores_array[-i-1] = scores 
            # Find and remove the lowest ranked feature
            alg.fit(x_train_fold[features], y_train_fold)
            lowest_rank = features[np.argmin(alg.feature_importances_)]
            features.remove(lowest_rank)
            
        dic[f""split{fold_index}_test_score""] = scores_array
    
    return dic

dtree = DecisionTreeClassifier(random_state = 0)
cv_split = StratifiedKFold(5, shuffle=True, random_state=0)

# Assume train is a pandas dataframe
x_var = [""var1"", ""var2"", ""var3""]
y_var = [""target_var""]
rfecv_scores = rfecv_function(dtree, x_var, train[x_var], train[y_var], cv_split)

The rfecv_scores provide identical values to those computed in the inbuilt class RFECV.cv_results_
",cross-validation
how to perform crossvalidation with lightgbmlgbmranker while keeping groups together,"I'm on a search problem, I have a dataset of queries and urls. Each couple (query, url) has a relevance (the target), a float which should preserve the order of the urls, for a given query.
I would like to perform cross validation for my lightgbm.LGBMRanker model, with the objective as ndcg.
I went through the documentation and saw that it is important to keep the instances in the same group, because an instance is actually a query with all its associated urls.
I however have an issue regarding this, as I get the following error :
ValueError: Computing NDCG is only meaningful when there is more than 1 document. Got 1 instead.

I used the debugger, and while I do not have any group which size is inferior to 2 in my dataset, I have groups which are smaller in the _feval function, meaning the cv() fucntion did not actually keep the groups together.
In the lightgbm.cv I see no sign of the group argument which is used in the LGBMRanker.
But I can see that the function lightbm.cv precises that Values passed through params take precedence over those supplied via arguments. My understanding was that this value is passed to the underlying model of the cv function.
Here is the code that I have so far :
def eval_model(
    self,
    model: lightgbm.LGBMRanker,
    k_fold: int = 3,
    seed: int = 42,
):
    """"""Evaluates with NDCG""""""

    def _feval(y_pred: np.ndarray, lgb_dataset: lightgbm.basic.Dataset):
        y_true = lgb_dataset.get_label()
        serp_sizes = lgb_dataset.get_group()

        ndcg_values = []
        start = 0
        for size in serp_sizes:
            end = start + size
            y_true_serp, y_pred_serp = y_true[start:end], y_pred[start:end]
            ndcg_serp = sklearn.metrics.ndcg_score(
                [y_true_serp], [y_pred_serp], k=10
            )
            ndcg_values.append(ndcg_serp)
            start = end

        eval_name = ""my-ndcg""
        eval_result = np.mean(ndcg_values)
        greater_is_better = True
        return eval_name, eval_result, greater_is_better

    lgb_dataset = lightgbm.Dataset(data=self.X, label=self.y, group=self.serp_sizes)
    cv_results = lightgbm.cv(
        params={**model.get_params(), ""group"": self.serp_sizes},
        train_set=lgb_dataset,
        num_boost_round=1_000,
        nfold=k_fold,
        stratified=False,
        seed=seed,
        feval=_feval,
    )
    ndcg = np.mean(cv_results[""my-ndcg""])

    return ndcg

Where is my mistake/misunderstanding ?
is there a simple workaround to perform cross-validation using a lightgbm.LGBMRanker, and keeping the groups together ?
","
I would like to perform cross validation for my lightgbm.LGBMRanker model, with the objective as ndcg.

As of lightgbm==4.1.0 (the latest version as of this writing), lightgbm.sklearn.LGBMRanker cannot be used with scikit-learn's cross-validation APIs.
It also cannot be passed to lightgbm.cv().

In the lightgbm.cv I see no sign of the group argument which is used in the LGBMRanker

As described in LightGBM's documentation (link), lightgbm.cv() expects to be passed a lightgbm.Dataset object.
group is an attribute of the Dataset object.
To perform cross-validation of a LightGBM learning-to-rank model, use lightgbm.cv() instead of lightgbm.sklearn.LGBMRanker().
Here's a minimal, reproducible example using 3.11.7 and lightgbm==4.1.0.
import lightgbm as lgb
import numpy as np
import requests
from sklearn.datasets import load_svmlight_file
from tempfile import NamedTemporaryFile

# get training data from LightGBM examples
data_url = ""https://raw.githubusercontent.com/microsoft/LightGBM/master/examples/lambdarank""
with NamedTemporaryFile(mode=""w"") as f:
    train_data_raw = requests.get(f""{data_url}/rank.train"").text
    f.write(train_data_raw)
    X, y = load_svmlight_file(f.name)

group = np.loadtxt(f""{data_url}/rank.train.query"")

# create a LightGBM Dataset
dtrain = lgb.Dataset(
    data=X,
    label=y,
    group=group
)

# perform LambdaRank 3-fold cross-validation with 1 set of hyperparameters
cv_results = lgb.cv(
    train_set=dtrain,
    params={
        ""objective"": ""lambdarank"",
        ""eval_at"": 2,
        ""num_iterations"": 10
    },
    nfold=3,
    return_cvbooster=True
)

# check metrics
np.round(cv_results[""valid ndcg@2-mean""], 3)
# array([0.593, 0.597, 0.64 , 0.632, 0.64 , 0.636, 0.655, 0.655, 0.653, 0.669])

lightgbm.cv() will correctly preserve query groups when creating cross-validation folds.

Values passed through params take precedence over those supplied via arguments

In LightGBM's documentation, ""param"" refers specifically to the configuration described at https://lightgbm.readthedocs.io/en/v4.1.0/Parameters.html.
The statement you've quoted does not apply to data like group, init_score, and label, and those things should not be passed through the params keyword argument in any of LightGBM's interfaces.
",cross-validation
plotting the roc curve of kfold cross validation,"I am working with an imbalanced dataset. I have applied SMOTE Algorithm to balance the dataset after splitting the dataset into test and training set before applying ML models. I want to apply cross-validation and plot the ROC curves of each folds showing the AUC of each fold and also display the mean of the AUCs in the plot. I named the resampled training set variables as X_train_res and y_train_res and following is the code:
cv = StratifiedKFold(n_splits=10)
classifier = SVC(kernel='sigmoid',probability=True,random_state=0)

tprs = []
aucs = []
mean_fpr = np.linspace(0, 1, 100)
plt.figure(figsize=(10,10))
i = 0
for train, test in cv.split(X_train_res, y_train_res):
    probas_ = classifier.fit(X_train_res[train], y_train_res[train]).predict_proba(X_train_res[test])
    # Compute ROC curve and area the curve
    fpr, tpr, thresholds = roc_curve(y_train_res[test], probas_[:, 1])
    tprs.append(interp(mean_fpr, fpr, tpr))
    tprs[-1][0] = 0.0
    roc_auc = auc(fpr, tpr)
    aucs.append(roc_auc)
    plt.plot(fpr, tpr, lw=1, alpha=0.3,
             label='ROC fold %d (AUC = %0.2f)' % (i, roc_auc))

    i += 1
plt.plot([0, 1], [0, 1], linestyle='--', lw=2, color='r',
         label='Chance', alpha=.8)

mean_tpr = np.mean(tprs, axis=0)
mean_tpr[-1] = 1.0
mean_auc = auc(mean_fpr, mean_tpr)
std_auc = np.std(aucs)
plt.plot(mean_fpr, mean_tpr, color='b',
         label=r'Mean ROC (AUC = %0.2f $\pm$ %0.2f)' % (mean_auc, std_auc),
         lw=2, alpha=.8)

std_tpr = np.std(tprs, axis=0)
tprs_upper = np.minimum(mean_tpr + std_tpr, 1)
tprs_lower = np.maximum(mean_tpr - std_tpr, 0)
plt.fill_between(mean_fpr, tprs_lower, tprs_upper, color='grey', alpha=.2,
                 label=r'$\pm$ 1 std. dev.')

plt.xlim([-0.01, 1.01])
plt.ylim([-0.01, 1.01])
plt.xlabel('False Positive Rate',fontsize=18)
plt.ylabel('True Positive Rate',fontsize=18)
plt.title('Cross-Validation ROC of SVM',fontsize=18)
plt.legend(loc=""lower right"", prop={'size': 15})
plt.show()

following is the output:

Please tell me whether the code is correct for plotting ROC curve for the cross-validation or not.
","
The problem is that I do not clearly understand cross-validation. In the for loop range, I have passed the training sets of X and y variables. Does cross-validation work like this?

Leaving SMOTE and the imbalance issue aside, which are not included in your code, your procedure looks correct.
In more detail, for each one of your n_splits=10:

you create train and test folds

you fit the model using the train fold:
classifier.fit(X_train_res[train], y_train_res[train])


and then you predict probabilities using the test fold:
predict_proba(X_train_res[test])



This is exactly the idea behind cross-validation.
So, since you have n_splits=10, you get 10 ROC curves and respective AUC values (and their average), exactly as expected.
However:
The need for (SMOTE) upsampling due to the class imbalance changes the correct procedure, and turns your overall process incorrect: you should not upsample your initial dataset; instead, you need to incorporate the upsampling procedure into the CV process.
So, the correct procedure here for each one of your n_splits becomes (notice that starting with a stratified CV split, as you have done, becomes essential in class imbalance cases):

create train and test folds
upsample your train fold with SMOTE
fit the model using the upsampled train fold
predict probabilities using the test fold (not upsampled)

For details regarding the rationale, please see own answer in the Data Science SE thread Why you shouldn't upsample before cross validation.
",cross-validation
should i first train_test_split and then use cross validation,"If I plan to use cross validation (KFold), should I still split the dataset into training and test data and perform my training (including cross valid) only on the training set? Or will CV do everything for me? E.g.
Option 1
X_train, X_test, y_train, y_test = train_test_split(X,y)
clf = GridSearchCV(... cv=5) 
clf.fit(X_train, y_train)

Option 2
clf = GridSearchCV(... cv=5) 
clf.fit(X y)

","CV is good, but it's better to have train/test split to provide independent score estimation on the untouched data.
If your CV and test data shows about the same score, then you can drop train/test split phase and CV on whole data to achieve slightly better model score. But don't do it before you sure your split and CV score is consistent.
",cross-validation
sklearn cross_val_score gives significantly different number than modelscore,"I have a binary classification problem
First I train test split my data as:
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)

I checked the y_train and it had basically a 50/50 split of the two classes (1,0) which is how the dataset it
when I try a base model such as:
model = RandomForestClassifier()
model.fit(X_train, y_train)
model.score(X_train, y_train)

the output is 0.98 or something 1% different depending on the random state of the train test split. 
HOWEVER, when I try a cross_val_score such as:
cross_val_score(model, X_train, y_train, cv=StratifiedKFold(shuffle=True), scoring='accuracy')

the output is 
array([0.65      , 0.78333333, 0.78333333, 0.66666667, 0.76666667])

none of the scores in the array are even close to 0.98?
and when I tried scoring = 'r2' I got
>>>cross_val_score(model, X_train, y_train, cv=StratifiedKFold(shuffle=True), scoring='r2')
array([-0.20133482, -0.00111235, -0.2       , -0.2       , -0.13333333])

Does anyone know why this is happening? I have tried Shuffle = True and False but it doesn't help.
Thanks in advance
","In your base model, you compute your score on the training corpus. While this is a proper way to ensure your model has actually learnt from the data you fed it, it doesn't ensure the final accuracy of your model on new and unseen data.
Not 100% sure (I don't know well scikit-learn), but I'd expect cross_val_score to actually split the X_train and y_train into a training and a testing set.
So as you compute a score on data unseen during the training, the accuracy will be much lower. Try to compare these results with model.score(X_test, y_test), it should be much closer.
",cross-validation
how to standardize data with sklearn39s cross_val_score,"Let's say I want to use a LinearSVC to perform k-fold-cross-validation on a dataset. How would I perform standardization on the data?
The best practice I have read is to build your standardization model on your training data then apply this model to the testing data.
When one uses a simple train_test_split(), this is easy as we can just do:
X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y)

clf = svm.LinearSVC()

scalar = StandardScaler()
X_train = scalar.fit_transform(X_train)
X_test = scalar.transform(X_test)

clf.fit(X_train, y_train)
predicted = clf.predict(X_test)

How would one go about standardizing data while doing k-fold-cross-validation? The problem comes from the fact that every data point will be for training/testing so you cannot standardize everything before cross_val_score(). Wouldn't you need a different standardization for each cross validation? 
The docs do not mention standardization happening internally within the function. Am I SOL? 
EDIT: This post is super helpful: Python - What is exactly sklearn.pipeline.Pipeline?
","You can use a Pipeline to combine both of the processes and then send it into the cross_val_score().
When the fit() is called on the pipeline, it will fit all the transforms one after the other and transform the data, then fit the transformed data using the final estimator. And during predict() (Only available if last object in pipeline is an estimator, otherwise transform()) it will apply transforms to the data, and predict with the final estimator.
Like this:
scalar = StandardScaler()
clf = svm.LinearSVC()

pipeline = Pipeline([('transformer', scalar), ('estimator', clf)])

cv = KFold(n_splits=4)
scores = cross_val_score(pipeline, X, y, cv = cv)

Check out various examples of pipeline to understand it better:

http://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html#examples-using-sklearn-pipeline-pipeline

",cross-validation
how to plot a decision tree from gridsearchcv,"i was trying to plot the decision tree which is formed with GridSearchCV, but its giving me an Attribute error.
AttributeError: 'GridSearchCV' object has no attribute 'n_features_'

However if i try to plot a normal decision tree without GridSearchCv, then it successfully prints.
code [decision tree without gridsearchcv]
# dtc_entropy : decison tree classifier based on entropy/information Gain
#plotting : decision tree on information/entropy  based

from sklearn.tree import export_graphviz
import graphviz

feature_names = x.columns

dot_data = export_graphviz(dtc_entropy, out_file=None, filled=True, rounded=True,
                                feature_names=feature_names,  
                                class_names=['0','1','2'])
graph = graphviz.Source(dot_data)  
graph                           ### --------------> WORKS 

code [decision tree with gridsearchcv]
#plotting : decision tree with GRIDSEARCHCV (dtc_gscv)  on information/entropy  based
from sklearn.tree import export_graphviz
import graphviz

feature_names = x.columns

dot_data = export_graphviz(dtc_gscv, out_file=None, filled=True, rounded=True,
                                feature_names=feature_names,  
                                class_names=['0','1','2'])
graph = graphviz.Source(dot_data)  
graph                            ##### ------------> ERROR

Error 
---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
<ipython-input-201-603524707f02> in <module>()
      6 dot_data = export_graphviz(dtc_gscv, out_file=None, filled=True, rounded=True,
      7                                 feature_names=feature_names,
----> 8                                 class_names=['0','1','2'])
      9 graph = graphviz.Source(dot_data)
     10 graph

1 frames
/usr/local/lib/python3.6/dist-packages/sklearn/tree/_export.py in export(self, decision_tree)
    393         # n_features_ in the decision_tree
    394         if self.feature_names is not None:
--> 395             if len(self.feature_names) != decision_tree.n_features_:
    396                 raise ValueError(""Length of feature_names, %d ""
    397                                  ""does not match number of features, %d""

AttributeError: 'GridSearchCV' object has no attribute 'n_features_'

code for decision-tree based on GridSearchCV
dtc=DecisionTreeClassifier()

#use gridsearch to test all values for n_neighbors
dtc_gscv = gsc(dtc, parameter_grid, cv=5,scoring='accuracy',n_jobs=-1)

#fit model to data
dtc_gscv.fit(x_train,y_train)

One solution is taking the best parameters from gridsearchCV and then form a decision tree with those parameters and plot the tree. 
However is there any way to print the decision-tree based on GridSearchCV.
","You may try:
dot_data = export_graphviz(dtc_gscv.best_estimator_, out_file=None, 
            filled=True, rounded=True, feature_names=feature_names, class_names=['0','1','2'])

",cross-validation
using sample weights through metadata routing in scikitlearn in nested crossvalidation,"I am using the sklearn version ""1.4.dev0"" to weight samples in the fitting and scoring process as described in this post and in this documentation.
https://scikit-learn.org/dev/metadata_routing.html
sklearn GridSearchCV not using sample_weight in score function
I am trying to use this in a nested cross validation scheme, where hyperparmeter are tuned in a inner loop using ""GridSearchCV"" and performance is evaluated in the outer loop using ""cross_validate"". In both loops, the samples should be weighted for fitting and scoring.
I got confused because if I use or dont use sample_weights in the inner loop (thus, in GridSearchCV) seems not to have an effect on the results of crossvalidate, although the fitting time implicates that the two functions calls of cross_validate differ. Maybe I have mistaken something but for me this seems rather unexpected and not right. Here is a reproducable example. I would like to know

If my suggestion is right that the weighted cross_validate scores of the weighted and unweighted gridsearch estimator should differ
How could I implement it in a way that I get the expected difference in the cross_validate scores

# sklearn version is 1.4.dev0
from sklearn.datasets import make_regression
from sklearn.linear_model import Lasso
from sklearn.model_selection import GridSearchCV, cross_validate, KFold
import numpy as np

np.random.seed(42)
sklearn.set_config(enable_metadata_routing=True)

X, y = make_regression(n_samples=100, n_features=5, noise=0.5)
sample_weights = np.random.rand(len(y))
estimator = Lasso().set_fit_request(sample_weight=True)
hyperparameter_grid = {'alpha': [0.1, 0.5, 1.0, 2.0]}
scoring_inner_cv = 'neg_mean_squared_error'
inner_cv = KFold(n_splits=5, shuffle=True, random_state=42)

grid_search_weighted = GridSearchCV(estimator=estimator, param_grid=hyperparameter_grid, cv=inner_cv,
                             scoring=scoring_inner_cv)
grid_search_unweighted = GridSearchCV(estimator=estimator, param_grid=hyperparameter_grid, cv=inner_cv,
                             scoring=scoring_inner_cv)
grid_search_weighted.fit(X, y, sample_weight=sample_weights)
grid_search_unweighted.fit(X, y)

est_weighted = grid_search_weighted.best_estimator_
est_unweighted = grid_search_unweighted.best_estimator_

weighted_score = grid_search_weighted.best_score_
unweighted_score = grid_search_unweighted.best_score_

predictions_weighted = grid_search_weighted.best_estimator_.predict(X)[:5]  # these are differents depending on the use of sample weights
predictions_unweighted = grid_search_unweighted.best_estimator_.predict(X)[:5]

print('predictions weighted:', predictions_weighted)
print('predictions unweighted:', predictions_unweighted)
print('best grid search score weighted:', weighted_score)
print('best grid search score unweighted:', unweighted_score)


# Setting up outer cross-validation
outer_cv = KFold(n_splits=5, shuffle=True, random_state=43)
scorers = {'mse': 'neg_mean_squared_error'}
results_weighted = cross_validate(est_weighted.set_score_request(sample_weight=True), 
                                  X, 
                                  y, 
                                  cv=outer_cv, 
                                  scoring=scorers, 
                                  return_estimator=True,
                                  params={""sample_weight"": sample_weights})
results_unweighted = cross_validate(est_unweighted.set_score_request(sample_weight=True), 
                                    X, 
                                    y, 
                                    cv=outer_cv, 
                                    scoring=scorers, 
                                    return_estimator=True,
                                    params={""sample_weight"": sample_weights})

print('cv fit time weighted:', results_weighted['fit_time'])
print('cv fit_time unweighted', results_unweighted['fit_time'])
print('cv score weighted:', results_weighted['test_mse'])
print('cv score unweighted:', results_unweighted['test_mse'])

Out:
predictions weighted: [ -56.75523055  -46.40853794 -257.61879983  115.33482089 -123.2799114 ]
predictions unweighted: [ -56.80695125  -46.46115926 -257.55129719  115.29365222 -123.17923488]
best grid search score weighted: -0.28206979708971763
best grid search score unweighted: -0.2959277881104643
cv fit time weighted: [0.00086832 0.00075293 0.00104165 0.00075936 0.000736  ]
cv fit_time unweighted [0.00077033 0.00074911 0.00076008 0.00075603 0.00073433]
cv score weighted: [-0.29977789 -0.19323401 -0.3599154  -0.29672299 -0.42656506]
cv score unweighted: [-0.29977789 -0.19323401 -0.3599154  -0.29672299 -0.42656506]

Edit: Sorry, still a bit sleepy, I corrected the code
","cross_validate trains and scores the estimator, which means if the hyperparameters of the estimators are the same, then the trained versions inside cross_validate would also be the same, which is the case here since both est_weighted and est_unweighted use alpha=0.1.
There are a few issues here though, first, you're not using sample_weight in your scorer, which you should if you're using sample_weight. Second, for a nested cross validation, you should pass the GridSearchCV object to cross_validate. Here's the updated script:
import sklearn
from sklearn.metrics import get_scorer
from sklearn.datasets import make_regression
from sklearn.linear_model import Lasso
from sklearn.model_selection import GridSearchCV, cross_validate, KFold
import numpy as np

np.random.seed(42)
sklearn.set_config(enable_metadata_routing=True)

X, y = make_regression(n_samples=100, n_features=5, noise=0.5)
sample_weights = np.random.rand(len(y))
estimator = Lasso().set_fit_request(sample_weight=True)
hyperparameter_grid = {""alpha"": [0.1, 0.5, 1.0, 2.0]}
scoring_inner_cv = get_scorer(""neg_mean_squared_error"").set_score_request(
    sample_weight=True
)
inner_cv = KFold(n_splits=5, shuffle=True, random_state=42)

grid_search_weighted = GridSearchCV(
    estimator=estimator,
    param_grid=hyperparameter_grid,
    cv=inner_cv,
    scoring=scoring_inner_cv,
)
grid_search_unweighted = GridSearchCV(
    estimator=estimator,
    param_grid=hyperparameter_grid,
    cv=inner_cv,
    scoring=scoring_inner_cv,
)
grid_search_weighted.fit(X, y, sample_weight=sample_weights)
grid_search_unweighted.fit(X, y)

est_weighted = grid_search_weighted.best_estimator_
est_unweighted = grid_search_unweighted.best_estimator_

print(""best estimator weighted:"", est_weighted)
print(""best estimator unweighted:"", est_unweighted)

weighted_score = grid_search_weighted.best_score_
unweighted_score = grid_search_unweighted.best_score_

predictions_weighted = grid_search_weighted.best_estimator_.predict(X)[
    :5
]  # these are different depending on the use of sample weights
predictions_unweighted = grid_search_unweighted.best_estimator_.predict(X)[:5]

print(""predictions weighted:"", predictions_weighted)
print(""predictions unweighted:"", predictions_unweighted)
print(""best grid search score weighted:"", weighted_score)
print(""best grid search score unweighted:"", unweighted_score)


# Setting up outer cross-validation
outer_cv = KFold(n_splits=5, shuffle=True, random_state=43)
scorers = {
    ""mse"": get_scorer(""neg_mean_squared_error"").set_score_request(sample_weight=True)
}
results_weighted = cross_validate(
    grid_search_weighted,
    X,
    y,
    cv=outer_cv,
    scoring=scorers,
    return_estimator=True,
    params={""sample_weight"": sample_weights},
)
results_unweighted = cross_validate(
    grid_search_unweighted,
    X,
    y,
    cv=outer_cv,
    scoring=scorers,
    return_estimator=True,
)

print(""cv fit time weighted:"", results_weighted[""fit_time""])
print(""cv fit_time unweighted"", results_unweighted[""fit_time""])
print(""cv score weighted:"", results_weighted[""test_mse""])
print(""cv score unweighted:"", results_unweighted[""test_mse""])

And the output:
best estimator weighted: Lasso(alpha=0.1)
best estimator unweighted: Lasso(alpha=0.1)
predictions weighted: [ -56.75523055  -46.40853794 -257.61879983  115.33482089 -123.2799114 ]
predictions unweighted: [ -56.80695125  -46.46115926 -257.55129719  115.29365222 -123.17923488]
best grid search score weighted: -0.30694013415226884
best grid search score unweighted: -0.2959277881104613
cv fit time weighted: [0.02880669 0.02938795 0.02891922 0.02823281 0.02768564]
cv fit_time unweighted [0.02526283 0.0255146  0.0250349  0.02500224 0.02558732]
cv score weighted: [-0.34250528 -0.21293099 -0.41301416 -0.36952952 -0.43474412]
cv score unweighted: [-0.28752003 -0.20898288 -0.40011525 -0.28467415 -0.41647231]

",cross-validation
why does my cross validation model give 100 validation accuracy,"I have been trying to classify autism and have a CNN model. The best accuracies so far from papers is around 70-73%~ and my model has been getting around 65-70% with different parameters. I have finally found a hyper parameter combination that gives a 70%+ accuracy when tested with a test set (around 10% of the data set, 10% used on validation and 80% for training). I decided to do a 10 fold cross validation and check with verbose 1 for each epoch. The first run gave around 68-76% validation accuracy per epoch (25 epochs in total) and a 72% on score. However, from the second batch of 25 epochs, the val accuracy is around 98-100% and accuracy keeps being at 1.000. Third batch is similar with 100% popping up. Is this normal? I haven't worked with this so far, the code I used is a template for CNN k-Fold cross validation.
from sklearn.model_selection import KFold
import numpy as np


# data should be of shape (838, 392, 392, num_channels)
data = conn_matrices


# labels should be of shape (838,)
labels = y

# Initialize 10-fold cross-validation
kf = KFold(n_splits=10, shuffle=True, random_state=42)

# Create lists to store the results of each fold
fold_accuracies = []

# Perform cross-validation and store the results
for train_index, test_index in kf.split(data):
    X_train, X_test = data[train_index], data[test_index]
    y_train, y_test = labels[train_index], labels[test_index]

    # Define and compile your Keras-based CNN model
    # Replace 'your_cnn_model' with your actual model
    your_cnn_model = model

    # Train the model on the training data
    your_cnn_model.fit(X_train, y_train, epochs=25,
                      batch_size=32, validation_data=(X_test, y_test), verbose=1)

    # Evaluate the model on the test data
    accuracy = your_cnn_model.evaluate(X_test, y_test)[1]
    fold_accuracies.append(accuracy)

# Print the accuracy of each fold
for i, accuracy in enumerate(fold_accuracies):
    print(f""Fold {i+1} Accuracy: {accuracy:.4f}"")

# Calculate and print the mean accuracy and standard deviation of the results
mean_accuracy = np.mean(fold_accuracies)
std_deviation = np.std(fold_accuracies)
print(f""Mean Accuracy: {mean_accuracy:.4f}"")
print(f""Standard Deviation: {std_deviation:.4f}"")

Expected each runs to have similar accuracies of around 70 to maximum of 76-77%
","You are giving the model the test data when training which is likely using the test data to fit some model parameters/hyperparameters, so of course it will overfit on that and give over-optimistic scores when testing on the same data it already knows:
# Train the model on the training data
your_cnn_model.fit(X_train, y_train, epochs=25, batch_size=32, validation_data=(X_test, y_test), verbose=1)

You need to use nested cross validation to find hyperparameters:
https://scikit-learn.org/stable/auto_examples/model_selection/plot_nested_cross_validation_iris.html
",cross-validation
scikit learn rfecv valueerror continuous is not supported,"I am trying to use scikit learn RFECV for feature selection in a given dataset using the code below:
import pandas as pd
from sklearn.ensemble import RandomForestRegressor
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.feature_selection import RFECV

# Data Processing
df = pd.read_csv('Combined_Data_final_2019H2_10min.csv')
X, y = (df.drop(['TimeStamp','Power_kW'], axis=1)), df['Power_kW']
SEED = 10
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=SEED)

# The ""accuracy"" scoring is proportional to the number of correct classifications
clf_rf_4 = RandomForestRegressor()
rfecv = RFECV(estimator=clf_rf_4, step=1, cv=4,scoring='accuracy')   #4-fold cross-validation (cv=4)

rfecv = rfecv.fit(X_train, y_train)

print('Optimal number of features :', rfecv.n_features_)
print('Best features :', X.columns[rfecv.support_])

# Plot number of features VS. cross-validation scores
plt.figure()
plt.xlabel(""Number of features selected"")
plt.ylabel(""Cross validation score of number of selected features"")
plt.plot(range(1, len(rfecv.grid_scores_) + 1), rfecv.grid_scores_)
plt.show()

I have tried a number of different solutions but I continuously get the following error code:
ValueError: continuous is not supported

Any ideas?
Any help would be very much appreciated!
","I believe your error is due to these 2 lines:
clf_rf_4 = RandomForestRegressor()
rfecv = RFECV(estimator=clf_rf_4, step=1, cv=4,scoring='accuracy')

accuracy is not defined for continuous outputs. Try changing it to something like:
rfecv = RFECV(estimator=clf_rf_4, step=1, cv=4,scoring='r2')

For a full list of regression scoring metrics see here, note the Regression line.
",cross-validation
difference in cross validation and testing performance,"I am using sklearn with cross validation(5 fold). 
Cross validation.
I get my data set and use it in 5 fold cross validation.
The returned scores (all 5) are in the range .80 to .85 
Direct Training
If I use the same data set with train test split (0.2 test portion) and directly fit and predict, I get around .70 accuracy. (recall and ROC AUC are also less than that).
So,
In cross validation, a single combination of folds is equal to what we do directly in train test split right ? Then why is there a huge difference? I have read that the reason is the cross validation is over fitting to training data. But when a single setting(combination) of cross validation is considered, isnt it the same as a direct fit and predict ? If somehow I know the exact way a particular combination in cross validation splits the data, and use that exact splitting method to be use in direct approach, shouldn't I get the same accuracy ?
","Without looking at the codes and your data, I can only give an educated guess. First of all, the reason we need validation dataset is to tune hyperparameters. Using cross validation, we try to find the best hyperparameters that give us the best prediction accuracy on the validation set. Thus the final model with the hyperparameters it chose overfits the validation dataset. So the prediction accuracy on the validation dataset is not a true measurement of the performance of your model. You need to have a hold-out never touched testing dataset to evaluate your model performance. 
If you use train/test split only without validation set, the performance on test dataset could be worse due to

Your hyperparameters are not tuned since you do not have validation
dataset 
Because your model never sees the test dataset, it is not
overfitting to it.

",cross-validation
specify columns to be selected in parameter grid for gridsearchcv,"I want to train models with certain sets of features as hyperparameters, using sklearn's GridSearchCV.
An example parameter grid would be:
[
    {
        'clf': [LogisticRegression()],
        'clf__C': [0.5, 0.1, 0.05, 0.01],
        'coltrans__feature_selector__feature_names': [
            ['COUNT(activities)', 'COUNT(events WHERE device_category = desktop)'], 
            ['COUNT(activities)']
        ]
    },
    {
        'clf': [DummyClassifier()],
        'clf__strategy': ['prior', 'most_frequent'],
        'coltrans__feature_selector__feature_names': [
            ['COUNT(activities)', 'COUNT(events WHERE device_category = desktop)'], 
            ['COUNT(activities)']
        ]
    }
]

This means I'd like GridSearchCV to train 4 logistic regressions (one for each value of C) using the set of features ['COUNT(activities)', 'COUNT(events WHERE device_category = desktop)'] and 4 logistic regressions using the set of features ['COUNT(activities)'].
The same goes for the dummy model.
Here's what I've tried
import pandas as pd
from typing import List, Dict
from functools import reduce
from utils import ClfSwitcher, update_pgrid

from optbinning import BinningProcess
from sklearn.model_selection import cross_validate, GridSearchCV, KFold
from sklearn.pipeline import make_pipeline, Pipeline
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.impute import SimpleImputer
from sklearn.compose import ColumnTransformer, make_column_selector
from sklearn.dummy import DummyClassifier
from sklearn.base import BaseEstimator, TransformerMixin
from sklearn.linear_model import LogisticRegression
from sklearn.dummy import DummyClassifier

# 

# feature selector transformer. Given a set of features it will output a datraframe with all columns that contain the names of the features given in the parameter 'feature_names'

class FeatureSelector(BaseEstimator, TransformerMixin):
    def __init__(self, feature_names):
        self.feature_names = feature_names

    def fit(self, X, y=None):
        return self

    def transform(self, X):
        selected_features = [col for col in X.columns if any(name in col for name in self.feature_names)]
        return X[selected_features]


# nested cross validation setup

n_folds = 3
scoring = {'auc': 'roc_auc', 'log_loss': 'neg_log_loss', 'brier_score': 'neg_brier_score'}
p_grid =     [
    {
        'clf': [LogisticRegression()],
        'clf__C': [0.5, 0.1, 0.05, 0.01],
        'coltrans__feature_selector__feature_names': [
            ['COUNT(activities)', 'COUNT(events WHERE device_category = desktop)'], 
            ['COUNT(activities)']
        ]
    },
    {
        'clf': [DummyClassifier()],
        'clf__strategy': ['prior', 'most_frequent'],
        'coltrans__feature_selector__feature_names': [
            ['COUNT(activities)', 'COUNT(events WHERE device_category = desktop)'], 
            ['COUNT(activities)']
        ]
    }
]


inner_cv = KFold(n_splits=n_folds, shuffle=True, random_state=1)
outer_cv = KFold(n_splits=n_folds, shuffle=True, random_state=3)

# get the names of categorical and numerical features
num_vars = []
cat_vars = []
for v, t in zip(X.dtypes.index, X.dtypes):
    if (""int"" in str(t)) or (""float"" in str(t)):
        num_vars.append(v)
    else:
        cat_vars.append(v)

# initialize transfomers that will go in the columntransfomer

imp = SimpleImputer(strategy=""median"")
scl = StandardScaler()
ohe = OneHotEncoder(
    drop=""first"", handle_unknown=""infrequent_if_exist"", min_frequency=0.1
)

feature_selector = FeatureSelector(feature_names=['COUNT(activities)', 'COUNT(events WHERE device_category = desktop)'])

# build columntransfomer

t = [
    (""imp_scale"", make_pipeline(imp, scl), num_vars ),
    (""ohe"", ohe, cat_vars),
    ('feature_selector', feature_selector, cat_vars+num_vars),
]
    
col_transformer = ColumnTransformer(transformers=t, remainder='drop')


# create a pipeline
pipe  = Pipeline([
    ('coltrans', col_transformer),
    ('clf', DummyClassifier()),
                        ])

# run cross-validation

clf = GridSearchCV(estimator=pipe, param_grid=p_grid, cv=inner_cv, refit=True, error_score='raise')

cv_results = cross_validate(
clf,
X,
y,
cv=outer_cv,
scoring=scoring,
return_estimator=False,
)

auc = reduce(lambda x, y: x + y, cv_results[""test_auc""]) / n_folds
log_loss = reduce(lambda x, y: x + y, cv_results[""test_log_loss""]) / n_folds


print(
"" AUC estimate: "",
auc,
""\n"",
""Log loss estimate: "",
log_loss
)

Here's the thing, if modify my column transfomer in the following way:
t = [
    ('feature_selector', feature_selector, cat_vars+num_vars),
]
    
col_transformer = ColumnTransformer(transformers=t, remainder='drop')

And then apply it to X:
col_transformer.fit_transform(X)

I get an array with only two columns, it works perfectly. The catch is that I have to put the feature_selector transfomer inside a ColumnTransformer because it needs the names of the columns to work. What I can't figure out is how to select the features I want and then make sure they go through all the other transfomartions (imputing and one-hot-encoding). The code I wrote works, but after using the column transfomer, I get an array with all my initial numerical features plus all the dummy columns created by one-hot-encoding.
I've tried using mlxtend's feature_selection in the actualy pipeline, but then I don't really know the indices of the features I want to select, since they've gone through one-hot-encoding (is there a way to circumvent this?).
","In your original approach:
t = [
    (""imp_scale"", make_pipeline(imp, scl), num_vars ),
    (""ohe"", ohe, cat_vars),
    ('feature_selector', feature_selector, cat_vars+num_vars),
]
    
col_transformer = ColumnTransformer(transformers=t, remainder='drop')

you end up with every (num+cat) feature included after transformation by the first two transformers, and then the one/two you want to include without transformation via the last transformer.  (See also Consistent ColumnTransformer for intersecting lists of columns and its linked questions.)
It seems like you want to include just the subset of features, and transform them accordingly.  So you should pipeline the selector before the rest of the transformations:
processor = ColumnTransformer(t[:-1], remainder='drop')

pipe = Pipeline([
    ('select', feature_selector),
    ('process', processor),
])

Since your feature selector produces a dataframe, you don't have to worry about the column transformer getting feature names, but you don't know in advance which subset of features are even going to reach it.  But you can use a callable in the column specification instead of a hard list (and you've already got that!):
def num_type_detector(X):
    num_vars = []
    for v, t in zip(X.dtypes.index, X.dtypes):
        if (""int"" in str(t)) or (""float"" in str(t)):
            num_vars.append(v)
    return num_vars

def cat_type_detector(X):
    cat_vars = []
    for v, t in zip(X.dtypes.index, X.dtypes):
        if (""int"" in str(t)) or (""float"" in str(t)):
            cat_vars.append(v)
    return cat_vars

processor = ColumnTransformer(
    [
        (""imp_scale"", make_pipeline(imp, scl), num_type_detector),
        (""ohe"", ohe, cat_type_detector),
    ],
    remainder='drop',
)

pipe = Pipeline([
    ('select', feature_selector),
    ('process', processor),
])

You should consider a more elegant version of num_type_detector, e.g. using make_column_selector (docs).

If you were to use a less-custom feature selector, you could use the pandas-out
functionality included from sklearn v1.2.  That doesn't work for sparse arrays (yet), so you'll need to set sparse=False in the one-hot encoder, and you might have problems with mixed types.
",cross-validation
unsure about the purpose of get_n_splits and why it is necessary,"I'm following a kernel on Kaggle and came across this code:
n_folds = 5

def rmsle_cv(model):
    kf = KFold(n_folds, shuffle=True, random_state=42).get_n_splits(train.values)
    rmse= np.sqrt(-cross_val_score(model, train.values, y_train, scoring=""neg_mean_squared_error"", cv = kf))
    return(rmse)

I understand the purpose and use of KFold and the fact that is used in cross_val_score. What I don't get is why get_n_split is used. As far as I am aware, it returns the number of iterations used for cross validation i.e. returns a value of 5 in this case. Surely for this line:
rmse= np.sqrt(-cross_val_score(model, train.values, y_train, scoring=""neg_mean_squared_error"", cv = kf))

cv = 5? This doesn't make any sense to me. Why is it even necessary to use get_n_splits if it returns an integer? I thought KFold returns a class whereas get_n_splits returns an integer.
Anyone can clear my understanding?
","
I thought KFold returns a class whereas get_n_splits returns an integer. 

Sure, KFold is a class, and one of the class methods is get_n_splits, which returns an integer; your shown kf variable
kf = KFold(n_folds, shuffle=True, random_state=42).get_n_splits(train.values)

is not a KFold class object, it is the result of the KFold().get_n_splits() method, and it is indeed an integer. In fact, if you check the documentation, get_n_splits() does not even need any arguments (they are actually ignored, and exist only for compatibility reasons with other classes and methods).
As for the questioned utility of the get_n_splits method, it is never a bad idea to be able to query such class objects in order to get back their parameter settings (on the contrary); imagine a situation where you have multiple different KFold objects, and you need to get their respective number of CV folds programmatically in the program flow.
",cross-validation
standardize data with kfold cross validation,"I'm using StratifiedKFold so my code looks like this 
def train_model(X,y,X_test,folds,model):
    scores=[]
    for fold_n, (train_index, valid_index) in enumerate(folds.split(X, y)):
        X_train,X_valid = X[train_index],X[valid_index]
        y_train,y_valid = y[train_index],y[valid_index]        
        model.fit(X_train,y_train)
        y_pred_valid = model.predict(X_valid).reshape(-1,)
        scores.append(roc_auc_score(y_valid, y_pred_valid))
    print('CV mean score: {0:.4f}, std: {1:.4f}.'.format(np.mean(scores), np.std(scores)))
folds = StratifiedKFold(10,shuffle=True,random_state=0)
lr = LogisticRegression(class_weight='balanced',penalty='l1',C=0.1,solver='liblinear')
train_model(X_train,y_train,X_test,repeted_folds,lr)

now before train the model I want to standardize the data so which is the correct way?
1)
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

doing this before calling train_model function
2)
doing standardization inside function like this
def train_model(X,y,X_test,folds,model):
    scores=[]
    for fold_n, (train_index, valid_index) in enumerate(folds.split(X, y)):
        X_train,X_valid = X[train_index],X[valid_index]
        y_train,y_valid = y[train_index],y[valid_index]
        scaler = StandardScaler()
        X_train = scaler.fit_transform(X_train)
        X_vaid = scaler.transform(X_valid)
        X_test = scaler.transform(X_test)
        model.fit(X_train,y_train)
        y_pred_valid = model.predict(X_valid).reshape(-1,)

        scores.append(roc_auc_score(y_valid, y_pred_valid))

    print('CV mean score: {0:.4f}, std: {1:.4f}.'.format(np.mean(scores), np.std(scores)))

As per my knowlwdge in 2nd option I'm not leaking the data.so which way is correct if I'm not using pipeline and also how to use pipeline if i want to use cross validation?
","Indeed the second option is better because the scaler does not see the values of X_valid to scale X_train.
Now if you were to use a pipeline, you can do:
from sklearn.pipeline import make_pipeline

def train_model(X,y,X_test,folds,model):
    pipeline = make_pipeline(StandardScaler(), model)
    ...

And then use pipeline instead of model. At every fit or predict call, it will automatically standardize the data at hand.
Note that you can also use the cross_val_score function from scikit-learn, with the parameter scoring='roc_auc'.
",cross-validation
tuned 3 parameters using grid search but the best_estimator_ has only 2 parameters,"I am tuning a gradient boosted classifier using a pipeline and grid search
My pipeline is
pipe = make_pipeline(StandardScaler(with_std=True, with_mean=True), \
    RFE(RandomForestClassifier(), n_features_to_select= 15), \
        GradientBoostingClassifier(random_state=42, verbose=True))

The parameter gri is:
    tuned_parameters = [{'gradientboostingclassifier__max_depth': range(3, 5),\
'gradientboostingclassifier__min_samples_split': range(4,6),\
'gradientboostingclassifier__learning_rate':np.linspace(0.1, 1, 10)}]

The grid search is done as
grid = GridSearchCV(pipe, tuned_parameters, cv=5, scoring='accuracy', refit=True)
grid.fit(X_train, y_train)

After fitting the model in train data, when I check the grid.best_estimator I can only find the 2 parameters(learning_rate and  min_samples_split )that I am fitting. I don't find the max_depth parameter in the best estimator.
grid.best_estimator_.named_steps['gradientboostingclassifier'] =

GradientBoostingClassifier(learning_rate=0.9, min_samples_split=5,
                           random_state=42, verbose=True)

But, if I use the grid.cv_results to find the best 'mean_test_score' and find the corresponding parameters for that test score, then I can find the max_depth in it.
inde = np.where(grid.cv_results_['mean_test_score'] == max(grid.cv_results_['mean_test_score']))

    grid.cv_results_['params'][inde[-1][0]]
{'gradientboostingclas...rning_rate': 0.9, 'gradientboostingclas..._max_depth': 3, 'gradientboostingclas...ples_split': 5}
special variables
function variables
'gradientboostingclassifier__learning_rate':0.9
'gradientboostingclassifier__max_depth':3
'gradientboostingclassifier__min_samples_split':5

My doubt now is, if I use the trained pipeline (name of the object is 'grid' in my case) will it still use the 'max_depth' parameter also or will it not?
Is it then better to use the 'best parameters' which gave me the best 'mean_test_score' taken from the grid.cv_results
","Your pipeline has been tuned on all three parameters that you specified. It is just that the best value for max_depth happens to be the default value. When printing the classifier, default values will not be included. Compare the following outputs:
print(GradientBoostingClassifier(max_depth=3)) # default
# output: GradientBoostingClassifier()

print(GradientBoostingClassifier(max_depth=5)) # not default
# output: GradientBoostingClassifier(max_depth=5)

In general, it is best-practice to access the best parameters by the best_params_ attribute of the fitted GridSearchCV object since this will always include all parameters:
grid.best_params_

",cross-validation
value error x has 24 features but decisiontreeclassifier is expecting 19 features as input,"I'm trying to reproduce this GitHub project on my machine, on Topological Data Analysis (TDA).
My steps:

get best parameters from a cross-validation output
load my dataset feature selection
extract topological features from the dataset for prediction
create a Random Forest Classifier model built on the best parameters
calculate probabilities on test data


Background:

Feature selection


In order to decide which attributes belong to which group, we created a correlation matrix.
From this, we saw that there were two big groups, where player attributes were strongly correlated with each other. Therefore, we decided to split the attributes into two groups,
one to summarise the attacking characteristics of a player while the other one the defensiveness. Finally, since the goalkeeper has completely different statistics with respect to the
other players, we decided to take into account only the overall rating. Below, is possible
to see the 24 features used for each player:


Attack: ""positioning"", ""crossing"", ""finishing"", ""heading_accuracy"", ""short_passing"",
""reactions"", ""volleys"", ""dribbling"", ""curve"", ""free_kick_accuracy"", ""acceleration"",
""sprint_speed"", ""agility"", ""penalties"", ""vision"", ""shot_power"", ""long_shots""
Defense: ""interceptions"", ""aggression"", ""marking"", ""standing_tackle"", ""sliding_tackle"",
""long_passing""
Goalkeeper: ""overall_rating""


From this set of features, the next step we did was to, for each non-goalkeeper player,
compute the mean of the attack attributes and the defensive ones.


Finally, for each team in a given match, we compute the mean and the standard deviation
for the attack and the defense from these stats of the team's players, as well as the best
attack and best defense.


In this way a match is described by 14 features (GK overall value, best attack, std attack,
mean attack, the best defense, std defense, mean defense), that mapped the match in the space,
following the characterizes of the two teams.



Feature extraction


The aim of TDA is to catch the structure of the space underlying the data. In our project, we assume that the neighborhood of a data point hides meaningful information that is correlated with the outcome of the match. Thus, we explored the data space looking for
this kind of correlation.


Methods:
def get_best_params():
    cv_output = read_pickle('cv_output.pickle')
    best_model_params, top_feat_params, top_model_feat_params, *_ = cv_output

    return top_feat_params, top_model_feat_params

def load_dataset():
    x_y = get_dataset(42188).get_data(dataset_format='array')[0]
    x_train_with_topo = x_y[:, :-1]
    y_train = x_y[:, -1]

    return x_train_with_topo, y_train


def extract_x_test_features(x_train, y_train, players_df, pipeline):
    """"""Extract the topological features from the test set. This requires also the train set

    Parameters
    ----------
    x_train:
        The x used in the training phase
    y_train:
        The 'y' used in the training phase
    players_df: pd.DataFrame
        The DataFrame containing the matches with all the players, from which to extract the test set
    pipeline: Pipeline
        The Giotto pipeline

    Returns
    -------
    x_test:
        The x_test with the topological features
    """"""
    x_train_no_topo = x_train[:, :14]
    y_test = np.zeros(len(players_df))  # Artificial y_test for features computation
    print('Y_TEST',y_test.shape)

    x_test_topo = extract_features_for_prediction(x_train_no_topo, y_train, players_df.values, y_test, pipeline)

    return x_test_topo

def extract_topological_features(diagrams):
    metrics = ['bottleneck', 'wasserstein', 'landscape', 'betti', 'heat']
    new_features = []
    for metric in metrics:
        amplitude = Amplitude(metric=metric)
        new_features.append(amplitude.fit_transform(diagrams))
    new_features = np.concatenate(new_features, axis=1)
    return new_features

def extract_features_for_prediction(x_train, y_train, x_test, y_test, pipeline):
    shift = 10
    top_features = []
    all_x_train = x_train
    all_y_train = y_train
    for i in tqdm(range(0, len(x_test), shift)):
        #
        print(range(0, len(x_test), shift) )
        if i+shift > len(x_test):
            shift = len(x_test) - i
        batch = np.concatenate([all_x_train, x_test[i: i + shift]])
        batch_y = np.concatenate([all_y_train, y_test[i: i + shift].reshape((-1,))])
        diagrams_batch, _ = pipeline.fit_transform_resample(batch, batch_y)
        new_features_batch = extract_topological_features(diagrams_batch[-shift:])
        top_features.append(new_features_batch)
        all_x_train = np.concatenate([all_x_train, batch[-shift:]])
        all_y_train = np.concatenate([all_y_train, batch_y[-shift:]])
    final_x_test = np.concatenate([x_test, np.concatenate(top_features, axis=0)], axis=1)
    return final_x_test

def get_probabilities(model, x_test, team_ids):
    """"""Get the probabilities on the outcome of the matches contained in the test set

    Parameters
    ----------
    model:
        The model (must have the 'predict_proba' function)
    x_test:
        The test set
    team_ids: pd.DataFrame
        The DataFrame containing, for each match in the test set, the ids of the two teams
    Returns
    -------
    probabilities:
        The probabilities for each match in the test set
    """"""
    prob_pred = model.predict_proba(x_test)
    prob_match_df = pd.DataFrame(data=prob_pred, columns=['away_team_prob', 'draw_prob', 'home_team_prob'])
    prob_match_df = pd.concat([team_ids.reset_index(drop=True), prob_match_df], axis=1)
    return prob_match_df


Working code:
best_pipeline_params, best_model_feat_params = get_best_params()

# 'best_pipeline_params' -> {'k_min': 50, 'k_max': 175, 'dist_percentage': 0.1}
# best_model_feat_params -> {'n_estimators': 1000, 'max_depth': 10, 'random_state': 52, 'max_features': 0.5}

pipeline = get_pipeline(best_pipeline_params)
# pipeline -> Pipeline(steps=[('extract_point_clouds',
            # SubSpaceExtraction(dist_percentage=0.1, k_max=175, k_min=50)),
            #('create_diagrams', VietorisRipsPersistence(n_jobs=-1))])

x_train, y_train = load_dataset()

# x_train.shape ->  (2565, 19)
# y_train.shape -> (2565,)

x_test = extract_x_test_features(x_train, y_train, new_players_df_stats, pipeline)

# x_test.shape -> (380, 24)

rf_model = RandomForestClassifier(**best_model_feat_params)
rf_model.fit(x_train, y_train)
matches_probabilities = get_probabilities(rf_model, x_test, team_ids)  # <-- breaks here
matches_probabilities.head()
compute_final_standings(matches_probabilities, 'premier league')


But I'm getting the error:
ValueError: X has 24 features, but DecisionTreeClassifier is expecting 19 features as input.


Loaded dataset (X_train):
Data columns (total 20 columns):
 #   Column              Non-Null Count  Dtype  
---  ------              --------------  -----  
 0   home_best_attack    2565 non-null   float64
 1   home_best_defense   2565 non-null   float64
 2   home_avg_attack     2565 non-null   float64
 3   home_avg_defense    2565 non-null   float64
 4   home_std_attack     2565 non-null   float64
 5   home_std_defense    2565 non-null   float64
 6   gk_home_player_1    2565 non-null   float64
 7   away_avg_attack     2565 non-null   float64
 8   away_avg_defense    2565 non-null   float64
 9   away_std_attack     2565 non-null   float64
 10  away_std_defense    2565 non-null   float64
 11  away_best_attack    2565 non-null   float64
 12  away_best_defense   2565 non-null   float64
 13  gk_away_player_1    2565 non-null   float64
 14  bottleneck_metric   2565 non-null   float64
 15  wasserstein_metric  2565 non-null   float64
 16  landscape_metric    2565 non-null   float64
 17  betti_metric        2565 non-null   float64
 18  heat_metric         2565 non-null   float64
 19  label               2565 non-null   float64


Please note that the first 14 columns are the features that describe the match, and that the 5 remaining features (minus label) are the topological ones, that are already extracted.
The problem seems to be when code gets to extract_x_test_features() and extract_features_for_prediction(), which should get the tolopogical features and stack the train dataset with it.
Since X_train already has topological features, it adds 5 more and so I end up with 24 features.
I'm not sure, though. I'm just trying to wrap this project around my head...and how prediction is being made here.

How do I fix the mismatch using the code above?

NOTES:
1- x_train and  y_test are not dataframes but numpy.ndarray
2 - This question is completely reproducible if one clones or downloads the project from the following link:
Github Link
","Returning a slice with 19 features here:
def extract_features_for_prediction(x_train, y_train, x_test, y_test, pipeline):
   (...)
   return final_x_test[:, :19]

Got rid of the error and ran the test.

I still don't get the gist of it, though.
I will grant the bounty to anyone who explains me the idea behind the test set in the context of this project, in the project notebook, which can be found here:
Project Notebook
",cross-validation
print classification result with k fold classification with sklearn package,"I have a dataset that I spilt by the holdout method using sklearn. The following is the procedure
from sklearn.model_selection import train_test_split
(X_train, X_test, y_train, y_test)=train_test_split(X,y,test_size=0.3, stratify=y)

I am using Random forest as classifier. The following is the code for that
clf = RandomForestClassifier(random_state=0 )
clf.fit(X_train, y_train)
R_y_pred = clf.predict(X_test)
target_names = ['Alive', 'Dead']
print(classification_report(y_test, R_y_pred, target_names=target_names))

Now I would like to use stratified kfold cross-validation on the training set. The code that I have written for that
cv_results = cross_validate(clf, X_train, y_train, cv=5)
R_y_pred = cv_results.predict(X_test)
target_names = ['Alive', 'Dead']
print(classification_report(y_test, R_y_pred, target_names=target_names))

I got error as cv_results has no attribute like predict.
I would like to know how could I print the classification result after using k fold cross validation.
Thank you.
","The cv_results is simply returning scores that demonstrate how well the model performs in predicting data across split samples (5 as specified in this case).
It is not a model that can be used for prediction purposes.
For instance, when considering a separate problem of predicting hotel cancellations using a classification model, using 5-fold cross validation with a random forest classifier yields the following test scores:
>>> from sklearn.model_selection import cross_validate
>>> cv_results = cross_validate(clf, x1_train, y1_train, cv=5)
>>> cv_results

{'fit_time': array([1.09486771, 1.13821363, 1.11560798, 1.08220959, 1.06806993]),
 'score_time': array([0.07809329, 0.10946631, 0.09018588, 0.07582998, 0.07735801]),
 'test_score': array([0.84440007, 0.85172242, 0.85322017, 0.84656349, 0.84190381])}

However, when attempting to make predictions using this model, the same error message is returned:
>>> from sklearn.model_selection import cross_validate
>>> cv_results = cross_validate(clf, x1_train, y1_train, cv=5)
>>> cv_results
>>> R_y_pred = cv_results.predict(x1_val)
>>> print(classification_report(y_test, R_y_pred))

---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
Cell In[33], line 4
      2 cv_results = cross_validate(clf, x1_train, y1_train, cv=5)
      3 cv_results
----> 4 R_y_pred = cv_results.predict(x1_val)
      5 print(classification_report(y_test, R_y_pred))

AttributeError: 'dict' object has no attribute 'predict'

",cross-validation
get the best model after cross validation,"How do I get the best model after a training with k-fold cross-validation without grid search?
for example:
model = XGBClassifier(**best_params)
cv_scores = cross_val_score(model, X_train, Y_train, cv=5, scoring='f1')
I am not sure how to get the best model to predict the Y_test, using the X_test data. Thank you
","This isn't the purpose of cross_val_score.
That is to say, cross_val_score doesn't generate multiple models for the purpose of finding the best classifiers to use, but is way of quantifying the generalizability of your model.  It trains and tests over subsets of the data you provided it (X_train, and Y_train in this case) to give a sense of whether your model overfits the training data or generalizes to the data provided.  If values in the returned list are all close to one another, then you can assume your model is generalizable.
Once you get that determination, your next process is to train over all the training data you have and then test it against all your training data, to determine the model's accuracy and precision.
See this article to learn more about how you might use  cross_val_score().
",cross-validation
why is the mean roc score from gridsearchcv using only 1 cv split different from roc calculated with grid_searchscore method or roc_auc_score func,"I was experimenting with sklearn's GridSearchCV, and I don't understand why the mean roc scores I get when using a single split defined with an iterable, are different than what I get running the score method after fitting, or the roc_auc_score function.
This is my data shape:
print(X.shape)
print(X.index)

print(y.shape)
print(y.index)

(31695, 1379)
RangeIndex(start=0, stop=31695, step=1)
(31695,)
RangeIndex(start=0, stop=31695, step=1)

This is how I define the cv_split:
cv_split =[(np.arange(15848), np.arange(15848,31695))]
cv_split

[(array([    0,     1,     2, ..., 15845, 15846, 15847]),
  array([15848, 15849, 15850, ..., 31692, 31693, 31694]))]

Fitting the model and resulting cv_results_:
gs_algorithm = GridSearchCV(estimator=LGBMClassifier(),
                            param_grid=hyperparameter_space,
                            scoring='roc_auc',
                            n_jobs=1,
                            pre_dispatch=1,
                            cv=cv_split,
                            verbose=10,
                            return_train_score=True)
gs_algorithm.fit(X, y)
gs_algorithm.cv_results_

Fitting 1 folds for each of 1 candidates, totalling 1 fits
...
{'mean_fit_time': array([17.40988088]),
 'std_fit_time': array([0.]),
 'mean_score_time': array([1.16691899]),
 'std_score_time': array([0.]),
 'param_colsample_bytree': masked_array(data=[0.2],
              mask=[False],
        fill_value='?',
             dtype=object),
 'param_learning_rate': masked_array(data=[0.1],
              mask=[False],
        fill_value='?',
             dtype=object),
 'param_max_depth': masked_array(data=[-1],
              mask=[False],
        fill_value='?',
             dtype=object),
 'param_min_child_samples': masked_array(data=[3000],
              mask=[False],
        fill_value='?',
             dtype=object),
 'param_min_child_weight': masked_array(data=[0],
              mask=[False],
        fill_value='?',
             dtype=object),
 'param_n_estimators': masked_array(data=[150],
              mask=[False],
        fill_value='?',
             dtype=object),
 'param_num_leaves': masked_array(data=[15000],
              mask=[False],
        fill_value='?',
             dtype=object),
 'param_random_state': masked_array(data=[6],
              mask=[False],
        fill_value='?',
             dtype=object),
 'params': [{'colsample_bytree': 0.2,
   'learning_rate': 0.1,
   'max_depth': -1,
   'min_child_samples': 3000,
   'min_child_weight': 0,
   'n_estimators': 150,
   'num_leaves': 15000,
   'random_state': 6}],
 'split0_test_score': array([0.75898716]),
 'mean_test_score': array([0.75898716]),
 'std_test_score': array([0.]),
 'rank_test_score': array([1], dtype=int32),
 'split0_train_score': array([0.81224109]),
 'mean_train_score': array([0.81224109]),
 'std_train_score': array([0.])}

So it's correctly giving me the same value for split0_test_score and mean_test_score: 0.75898716
But then when I try this:
gs_algorithm.score(X.iloc[cv_split[0][1]],y[cv_split[0][1]])

0.8194048788870386

y_pred = gs_algorithm.predict_proba(X)[:, 1]
print(y_pred[cv_split[0][1]].shape)

roc_auc_score(y[cv_split[0][1]], y_pred[cv_split[0][1]])

(15847,)
0.8194048788870386

Why is the mean_score informed after fitting the model different?
","The score and predict_proba methods of GridSearchCV (your gs_algorithm) rely on a refitted model using the entire training set (recombining the cv split(s)); see the documentation for the parameter refit.
Individual fold-estimator combinations aren't saved, so you would need to manually refit the estimator with the best_params_ on the training set (with random effects controlled) in order to recreate the test fold score.
",cross-validation
grid search parameter and crossvalidated data set in knn classifier in scikitlearn,"I'm trying to perform my first KNN Classifier using SciKit-Learn. I've been following the User Guide and other online examples but there are a few things I am unsure about. For this post lets use the following
X = data
Y = target

In most introduction to machine learning pages that I've read it seems to say you want a training set, a validation set, and a test set. From what I understand, cross validation allows you to combine the training and validations sets to train the model, and then you should test it on the test set to get a score. However, I have seen in papers that in a lot of cases you can just cross validate on the entire data set and then report the CV score as the accuracy. I understand in an ideal world you would want to test on separate data but if this is legitimate I would like to cross-validate on my entire dataset and report those scores

So starting the process


I define my KNN Classifier as follows
knn = KNeighborsClassifier(algorithm = 'brute')

I search for best n_neighbors using
clf = GridSearchCV(knn, parameters, cv=5)

Now if I say
clf.fit(X,Y)

I can check the best parameter using
clf.best_params_

and then I can get a score
clf.score(X,Y)

But - as I understand it, this hasn't cross validated the model, as it only gives 1 score?
If I have seen clf.best_params_ = 14 now could I go on
knn2 = KNeighborsClassifier(n_neighbors = 14, algorithm='brute')
cross_val_score(knn2, X, Y, cv=5)

Now I know the data has been cross validated but I don't know if it is legitimate to use clf.fit to find the best parameter and then use cross_val_score with a new knn model?

I understand that the 'proper' way to do it would be as follows

Split to X_train, X_test, Y_train, Y_test,
Scale train sets -> apply transform to test sets
knn = KNeighborsClassifier(algorithm = 'brute')
clf = GridSearchCV(knn, parameters, cv=5)
clf.fit(X_train,Y_train)
clf.best_params_

and then I can get a score
clf.score(X_test,Y_test)

In this case, is the score calculated using the best parameter?

I hope that this makes sense. I've been trying to find as much as I can without posting but I have come to the point where I think it would be easier to get some direct answers.
In my head I am trying to get some cross-validated scores using the whole dataset but also use a gridsearch (or something similar) to fine tune the parameters.
","
Yes you can CV on your entire dataset it is viable, but I still suggest you to at least split your data into 2 sets one for CV and one for testing.
The .score function is supposed to return a single float value according to the documentation which is the score of the best estimator(which is the best scored estimator you get from fitting your GridSearchCV) on the given X,Y
If you saw that the best parameter is 14 than yes you can go on whith using it in your model, but if you gave it more parameters you should set all of them. 
(- I say that because you haven't given your parameters list) And yes it is legitimate to check your CV once again just in case if this model is as good as it should.

Hope that makes the things clearer :)
",cross-validation
not able to use stratifiedkfold on multi label classifier,"The following code is used to do KFold Validation but I am to train the model as it is throwing the error
ValueError: Error when checking target: expected dense_14 to have shape (7,) but got array with shape (1,)

My target Variable has 7 classes. I am using LabelEncoder to encode the classes into numbers. 
By seeing this error, If I am changing the into MultiLabelBinarizer to encode the classes. I am getting the following error
ValueError: Supported target types are: ('binary', 'multiclass'). Got 'multilabel-indicator' instead.

The following is the code for KFold validation
skf = StratifiedKFold(n_splits=10, shuffle=True)
scores = np.zeros(10)
idx = 0
for index, (train_indices, val_indices) in enumerate(skf.split(X, y)):
    print(""Training on fold "" + str(index+1) + ""/10..."")
    # Generate batches from indices
    xtrain, xval = X[train_indices], X[val_indices]
    ytrain, yval = y[train_indices], y[val_indices]
    model = None
    model = load_model() //defined above

    scores[idx] = train_model(model, xtrain, ytrain, xval, yval)
    idx+=1
print(scores)
print(scores.mean())

I don't know what to do. I want to use Stratified K Fold on my model. Please help me.
","MultiLabelBinarizer returns a vector which is of the length of your number of classes.
If you look at how StratifiedKFold splits your dataset, you will see that it only accepts a one-dimensional target variable, whereas you are trying to pass a target variable with dimensions [n_samples, n_classes]
Stratefied split basically preserves your class distribution. And if you think about it, it does not make a lot of sense if you have a multi-label classification problem.
If you want to preserve the distribution in terms of the different combinations of classes in your target variable, then the answer here explains two ways in which you can define your own stratefied split function.
UPDATE:
The logic is something like this:
Assuming you have n classes and your target variable is a combination of these n classes. You will have (2^n) - 1 combinations (Not including all 0s). You can now create a new target variable considering each combination as a new label.
For example, if n=3, you will have 7 unique combinations:
 1. [1, 0, 0]
 2. [0, 1, 0]
 3. [0, 0, 1]
 4. [1, 1, 0]
 5. [1, 0, 1]
 6. [0, 1, 1]
 7. [1, 1, 1]

Map all your labels to this new target variable. You can now look at your problem as simple multi-class classification, instead of multi-label classification.
Now you can directly use StartefiedKFold using y_new as your target. Once the splits are done, you can map your labels back.
Code sample:
import numpy as np

np.random.seed(1)
y = np.random.randint(0, 2, (10, 7))
y = y[np.where(y.sum(axis=1) != 0)[0]]

OUTPUT:
array([[1, 1, 0, 0, 1, 1, 1],
       [1, 1, 0, 0, 1, 0, 1],
       [1, 0, 0, 1, 0, 0, 0],
       [1, 0, 0, 1, 0, 0, 0],
       [1, 0, 0, 0, 1, 1, 1],
       [1, 1, 0, 0, 0, 1, 1],
       [1, 1, 1, 1, 0, 1, 1],
       [0, 0, 1, 0, 0, 1, 1],
       [1, 0, 1, 0, 0, 1, 1],
       [0, 1, 1, 1, 1, 0, 0]])

Label encode your class vectors:
from sklearn.preprocessing import LabelEncoder

def get_new_labels(y):
    y_new = LabelEncoder().fit_transform([''.join(str(l)) for l in y])
    return y_new

y_new = get_new_labels(y)

OUTPUT:
array([7, 6, 3, 3, 2, 5, 8, 0, 4, 1])

",cross-validation
crossvalidation in scikitlearn mean absolute error of x_test y_test,"Usually we split the original feature and target data (X,y) in (X_train, y_train) and (X_test, y_test).
By using the method:
mae_A = cross_val_score(clf, X_train_scaled, y_train, scoring=""neg_mean_absolute_error"", cv=kfold)

I get the cross validation Mean Absolute Error (MAE) for the (X_train, y_train), right?
How can I get the MAE (from the previous cross-validation models got by using (X_train, y_train)) for the (X_test, y_test)?
","This is the correct approach. As a rule, you should only train your model using training data.
Thus the test_set should remain unseen in the cross-validation process, i.e. by the model's hyperparameters, otherwise you could be biasing the results obtained from the model by adding knowledge from the test sample.

I get the cross validation Mean Absolute Error (MAE) for the (X_train, y_train), right?

Yes, the error displayed by cross_val_score will be only from the training data.
So the idea is that once you are satisfied with the results of cross_val_score, you fit the final model with the whole training set, and perform a prediction on y_test. For that you could use sklearn.metrics. For isntance, if you wanted to obtain the MAE:
from sklearn.metrics import mean_absolute_error as mae
MAE = mae(y_test, y_pred)

",cross-validation
notfittederror instance is not fitted yet after invoked cross_validate,"This is my minimal reproducible example:
import numpy as np
from sklearn.naive_bayes import GaussianNB
from sklearn.model_selection import cross_validate

x = np.array([
   [1, 2],
   [3, 4],
   [5, 6],
   [6, 7]
])  
y = [1, 0, 0, 1]

model = GaussianNB()
scores = cross_validate(model, x, y, cv=2, scoring=(""accuracy""))

model.predict([8,9])

What I intended to do is instantiating a Gaussian Naive Bayes Classifier and use sklearn.model_selection.cross_validate for cross validate my model (I am using cross_validate instead of cross_val_score since in my real project I need precision, recall and f1 as well).
I have read in the doc that cross_validate does ""evaluate metric(s) by cross-validation and also record fit/score times.""
I expected that my model would have been fitted on x (features), y (labels) data but when I invoke model.predict(.) I get:

sklearn.exceptions.NotFittedError: This GaussianNB instance is not fitted yet. Call 'fit' with appropriate arguments before using this estimator.

Of course it says me about invoking model.fit(x,y) before ""using the estimator"" (that is before invoking model.predict(.).
Shouldn't the model have been fitted cv=2 times when I invoke cross_validate(...)?
","A close look at cross_validate documentation reveals that it includes an argument:

return_estimator : bool, default=False
Whether to return the estimators fitted on each split.

So, by default it will not return any fitted estimator (hence it cannot be used to predict).
In order to predict with the fitted estimator(s), you need to set the argument to True; but beware, you will not get a single fitted model, but a number of models equal to your cv parameter value (here 2):
import numpy as np
from sklearn.naive_bayes import GaussianNB
from sklearn.model_selection import cross_validate

x = np.array([
   [1, 2],
   [3, 4],
   [5, 6],
   [6, 7]
])  
y = [1, 0, 0, 1]

model = GaussianNB()
scores = cross_validate(model, x, y, cv=2, scoring=(""accuracy""), return_estimator=True)
scores
# result:
{'fit_time': array([0.00124454, 0.00095725]),
 'score_time': array([0.00090432, 0.00054836]),
 'estimator': [GaussianNB(), GaussianNB()],
 'test_score': array([0.5, 0.5])}

So, in order to get predictions from each fitted model, you need:
scores['estimator'][0].predict([[8,9]])
# array([1])

scores['estimator'][1].predict([[8,9]])
# array([0])

This may look inconvenient, but it is like that by design: cross_validate is generally meant only to return the scores necessary for diagnosis and assessment, not to be used for fitting models which are to be used for predictions.
",cross-validation
logistic regression and crossvalidation,"I am trying to solve a classification problem on a given dataset, through logistic regression (and this is not the problem). To avoid overfitting I'm trying to implement it through cross-validation (and here's the problem): there's something that I'm missing to complete the program. My purpose here is to determine accuracy.
But let me be specific. This is what I've done:

I split the set into train set and test set
I defined the logregression prediction model to be used
I used the cross_val_predict method (in sklearn.cross_validation) to make predictions
Lastly, I measured accuracy

Here is the code:
import pandas as pd
import numpy as np
import seaborn as sns
from sklearn.cross_validation import train_test_split
from sklearn import metrics, cross_validation
from sklearn.linear_model import LogisticRegression
 
# read training data in pandas dataframe
data = pd.read_csv(""./dataset.csv"", delimiter=';')
# last column is target, store in array t
t = data['TARGET']
# list of features, including target
features = data.columns
# item feature matrix in X
X = data[features[:-1]].as_matrix()
# remove first column because it is not necessary in the analysis
X = np.delete(X,0,axis=1)
# divide in training and test set
X_train, X_test, t_train, t_test = train_test_split(X, t, test_size=0.2, random_state=0)

# define method
logreg=LogisticRegression()

# cross valitadion prediction
predicted = cross_validation.cross_val_predict(logreg, X_train, t_train, cv=10)
print(metrics.accuracy_score(t_train, predicted)) 

My problems:

From what I understand the test set should not be considered until the very end and cross-validation should be made on training set. That's why I inserted X_train and t_train in the cross_val_predict method. Thuogh, I get an error saying:
ValueError: Found input variables with inconsistent numbers of samples: [6016, 4812]
where 6016 is the number of samples in the whole dataset, and 4812 is the number of samples in the training set after the dataset has been split

After this, I don't know what to do. I mean: when do the X_test and t_test come into play? I don't get how I should use them after cross-validating and how to get the final accuracy.


Bonus question: I'd also like to perform scaling and reduction of dimensionality (through feature selection or PCA) within each step of the cross-validation. How can I do this? I've seen that defining a pipeline can help with scaling, but I don't know how to apply this to the second problem.
","Here is working code tested on a sample dataframe. The first issue in your code is the target array is not an np.array. You also shouldn't have target data in your features. Below I illustrate how to manually split the training and testing data using train_test_split. I also show how to use the wrapper cross_val_score to automatically split, fit, and score.
random.seed(42)
# Create example df with alphabetic col names.
alphabet_cols = list(string.ascii_uppercase)[:26]
df = pd.DataFrame(np.random.randint(1000, size=(1000, 26)),
                  columns=alphabet_cols)
df['Target'] = df['A']
df.drop(['A'], axis=1, inplace=True)
print(df.head())
y = df.Target.values  # df['Target'] is not an np.array.
feature_cols = [i for i in list(df.columns) if i != 'Target']
X = df.ix[:, feature_cols].as_matrix()
# Illustrated here for manual splitting of training and testing data.
X_train, X_test, y_train, y_test = \
    model_selection.train_test_split(X, y, test_size=0.2, random_state=0)

# Initialize model.
logreg = linear_model.LinearRegression()

# Use cross_val_score to automatically split, fit, and score.
scores = model_selection.cross_val_score(logreg, X, y, cv=10)
print(scores)
print('average score: {}'.format(scores.mean()))

Output
     B    C    D    E    F    G    H    I    J    K   ...    Target
0   20   33  451    0  420  657  954  156  200  935   ...    253
1  427  533  801  183  894  822  303  623  455  668   ...    421
2  148  681  339  450  376  482  834   90   82  684   ...    903
3  289  612  472  105  515  845  752  389  532  306   ...    639
4  556  103  132  823  149  974  161  632  153  782   ...    347

[5 rows x 26 columns]
[-0.0367 -0.0874 -0.0094 -0.0469 -0.0279 -0.0694 -0.1002 -0.0399  0.0328
 -0.0409]
average score: -0.04258093018969249

Helpful references:

Convert from pandas to numpy
Select all but subset of columns of dataframe
sklearn.model_selection.train_test_split
sklearn.model_selection.cross_val_score

",cross-validation
cross validation and model selection,"I am using sklearn for SVM training. I am using the cross-validation to evaluate the estimator and avoid the overfitting model.
I split the data into two parts. Train data and test data. Here is the code:
import numpy as np
from sklearn import cross_validation
from sklearn import datasets
from sklearn import svm

X_train, X_test, y_train, y_test = cross_validation.train_test_split(
    iris.data, iris.target, test_size=0.4, random_state=0
)
clf = svm.SVC(kernel='linear', C=1)
scores = cross_validation.cross_val_score(clf, X_train, y_train, cv=5)
print scores

Now I need to evaluate the estimator clf on X_test.
clf.score(X_test, y_test)

here,  I get an error saying that
the model is not fitted using fit()

but normally, in cross_val_score function the model is fitted? What is the problem?
","cross_val_score is basically a convenience wrapper for the sklearn cross-validation iterators. You give it a classifier and your whole (training + validation) dataset and it automatically performs one or more rounds of cross-validation by splitting your data into random training/validation sets, fitting the training set, and computing the score on the validation set. See the documentation here for an example and more explanation.
The reason why clf.score(X_test, y_test) raises an exception is because cross_val_score performs the fitting on a copy of the estimator rather than the original (see the use of clone(estimator) in the source code here). Because of this, clf remains unchanged outside of the function call, and is therefore not properly initialized when you call clf.fit.
",cross-validation
i can39t execute cross_val_score with keraswrappersscikit_learnkerasregressor,"# Python library

import pandas as pd
import numpy as np
import seaborn as sns
from tensorflow import keras
import matplotlib.pyplot as plt
from keras.wrappers.scikit_learn import KerasRegressor
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import train_test_split

# 1) created from the data 

np.random.seed(0) 
m = 100 

X = np.linspace(0, 10, m).reshape(m,1) 
y = X + np.random.randn(m, 1)  

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)

scaler = MinMaxScaler()
X_train= scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# 2) Data visualization

print('dimensions de X:', X.shape)
print('dimensions de y:', y.shape)
plt.scatter(X,y)
plt.show()

# 3) Configuration of the Neural Network Layers

model = keras.Sequential()
model.add(keras.layers.Dense(100, activation='relu', input_dim=1))
model.add(keras.layers.Dropout(0.5))
model.add(keras.layers.Dense(100, activation='relu'))
model.add(keras.layers.Dropout(0.5))
model.add(keras.layers.Dense(1, activation='relu'))

# 4) Use the validation stick to train the model and display the learning curve   

Model = keras.Sequential([
keras.layers.Dense(4, activation='relu', input_dim=2),
keras.layers.Dense(2, activation='relu'),
keras.layers.Dense(1, activation='relu')])

opt = keras.optimizers.Adam()
Model.compile(opt, loss= 'mse')

Model = KerasRegressor(Model,batch_size=10,verbose=1, epochs=1000)

val_score = cross_val_score(Model, X_train, y_train, cv=10)
#plt.plot(val_score)

When I run the attached code normally it should work but for some reason it displays this error:
<ipython-input-6-041ea9940b77>:14: DeprecationWarning: KerasRegressor is deprecated, use Sci-Keras (https://github.com/adriangb/scikeras) instead. See https://www.adriangb.com/scikeras/stable/migration.html for help migrating.
  Model = KerasRegressor(Model,batch_size=10,verbose=1, epochs=1000)
/usr/local/lib/python3.8/dist-packages/sklearn/model_selection/_validation.py:372: FitFailedWarning: 
10 fits failed out of a total of 10.
The score on these train-test partitions for these parameters will be set to nan.
If these failures are not expected, you can try to debug them by setting error_score='raise'.

Below are more details about the failures:
10 fits failed with the following error:
Traceback (most recent call last):
  File ""/usr/local/lib/python3.8/dist-packages/sklearn/model_selection/_validation.py"", line 680, in _fit_and_score
    estimator.fit(X_train, y_train, **fit_params)
  File ""/usr/local/lib/python3.8/dist-packages/keras/wrappers/scikit_learn.py"", line 152, in fit
    self.model = self.build_fn(
  File ""/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py"", line 67, in error_handler
    raise e.with_traceback(filtered_tb) from None
  File ""/usr/local/lib/python3.8/dist-packages/keras/engine/base_layer.py"", line 3100, in _split_out_first_arg
    raise ValueError(
ValueError: The first argument to `Layer.call` must always be passed.

warnings.warn(some_fits_failed_message, FitFailedWarning)

","I don't know if the warning always triggers an error, but it's informing you that using the built-in wrapper is deprecated. The migrating guide is worth reading, but the basic steps are:

Install scikeras (See also: docs)

pip install scikeras[tensorflow]


Replace keras.wrappers with scikeras.wrappers:

# from keras.wrappers.scikit_learn import KerasRegressor
from scikeras.wrappers import KerasRegressor

Full example:
from tensorflow import keras
from sklearn.model_selection import cross_val_score
from sklearn.datasets import make_regression
from scikeras.wrappers import KerasRegressor

X, y = make_regression(n_samples=10_000)
input_shape = X.shape[1]
model = keras.Sequential([
    keras.layers.Dense(100, activation='relu', input_dim=input_shape),
    keras.layers.Dense(200, activation='relu'),
    keras.layers.Dense(200, activation='relu'),
    keras.layers.Dense(1, activation='linear')])
model.compile(keras.optimizers.Adam(), loss='mse')

model = KerasRegressor(model, batch_size=256, verbose=1, epochs=10)

val_score = cross_val_score(model, X, y, cv=5)

",cross-validation
how to evaluate the performance of different model on one dataset,"I want to evaluate the performance different model such as SVM, RandForest, CNN etc, I only have one dataset. So I split the dataset to training set and testing set and train different model on this dataset with training data and test with testing dataset.
Can I get the real performance of different model on only one dataset? For example: I found SVM model get the best result, So Should I select the SVM as my final classification model?
","Its probably a better idea to cross validate your models with different test samples through cross validation to avoid biases. Also check your models against different evaluation metrics depending upon your application type. For instance use recall, accuracy and AUC for each model if its a classification problem.
Evaluation results can be pretty deceptive and require extensive validation. 
",cross-validation
stratified groupshufflesplit in scikitlearn,"I would like to ask if it is possible to do ""Stratified GroupShuffleSplit"" in scikit-learn which is in other words a combination of GroupShuffleSplit and StratifiedShuffleSplit
Here is a sample of the code I am using:
cv=GroupShuffleSplit(n_splits=n_splits,test_size=test_size,\
    train_size=train_size,random_state=random_state).split(\
    allr_sets_nor[:,:2],allr_labels,groups=allr_groups)
opt=GridSearchCV(SVC(decision_function_shape=dfs,tol=tol),\
    param_grid=param_grid,scoring=scoring,n_jobs=n_jobs,cv=cv,verbose=verbose)
opt.fit(allr_sets_nor[:,:2],allr_labels)

Here I applied the GroupShuffleSplit but I still want to add the startification according to allr_labels
","I solved the problem by applying StratifiedShuffleSplit on the groups and then finding training and testing sets indices manually because they are linked to the groups indices (in my case each group contains 6 successive sets from 6*index to 6*index+5)
as in the following:
sss=StratifiedShuffleSplit(n_splits=n_splits,test_size=test_size,
    train_size=train_size,random_state=random_state).split(all_groups,all_labels) 
        # startified splitting for groups only

i=0
train_is = [np.array([],dtype=int)]*n_splits
test_is = [np.array([],dtype=int)]*n_splits
for train_index,test_index in sss :
        # finding the corresponding indices of reflected training and testing sets
    train_is[i]=np.hstack((train_is[i],np.concatenate([train_index*6+i for i in range(6)])))
    test_is[i]=np.hstack((test_is[i],np.concatenate([test_index*6+i for i in range(6)])))
    i=i+1

cv=[(train_is[i],test_is[i]) for i in range(n_splits)]
        # constructing the final cross-validation iterable: list of 'n_splits' tuples;
        # each tuple contains two numpy arrays for training and testing indices respectively

opt=GridSearchCV(SVC(decision_function_shape=dfs,tol=tol),param_grid=param_grid,
                 scoring=scoring,n_jobs=n_jobs,cv=cv,verbose=verbose)
opt.fit(allr_sets_nor[:,:2],allr_labels)

",cross-validation
cross validation on r ranger library,"Hello I have the following ranger model:
X <- train_df[, -1]
y <- train_df$Price

rf_model <- ranger(Price ~ ., data = train_df, mtry = 11 ,splitrule = ""extratrees"" ,min.node.size = 1, num.trees =100)

I am trying to accomplish two things,

Give me an average performance metric, cross-validating across non intersecting variance data sets, and give me a more stable accuracy metric, despite the change in seed value
Set up cross validation to find the most optimal mtry, and num.trees combo.

What I have tried:
**The following worked for optimizing for mtry,splitrule and min.node.size, but I can not add the number of trees into the equation, as it gives me an error in the case of doing so. **
# define the parameter grid to search over
param_grid <- expand.grid(mtry = c(1:ncol(X)),
splitrule = c( ""variance"", ""extratrees"", ""maxstat""),
min.node.size = c(1, 5, 10))
# set up the cross-validation scheme
cv_scheme <- trainControl(method = ""cv"",
                          number = 5,
                          verboseIter = TRUE)

# perform the grid search using caret
rf_model <- train(x = X,
                  y = y,
                  method = ""ranger"",
                  trControl = cv_scheme,
                  tuneGrid = param_grid)

# view the best parameter values
rf_model$bestTune

","One easy way to do it, is to add a num.trees argument in train and iterate over that argument.
The other way is to create your customized model see this chapter Using Your Own Model
there is an RPubs paper by Pham Dinh Khanh demonstrating that here
library(caret)
library(mlbench)
library(ranger)
data(PimaIndiansDiabetes)
x=PimaIndiansDiabetes[,-ncol(PimaIndiansDiabetes)]
y=PimaIndiansDiabetes[,ncol(PimaIndiansDiabetes)]

param_grid=expand.grid(mtry = c(1:4),
                       splitrule = c( ""variance"", ""extratrees""),
                       min.node.size = c(1, 5))
cv_scheme <- trainControl(method = ""cv"",
                          number = 5,
                          verboseIter = FALSE)
models=list()
for (ntree in c(4,100)){
set.seed(123)
rf_model <- train(x = x,
                  y = y,
                  method = ""ranger"",
                  trControl = cv_scheme,
                  tuneGrid = param_grid,
                  num.trees=ntree)
name=paste0(ntree,""_tr_model"")
models[[name]]=rf_model
}

models[[""4_tr_model""]]
#> Random Forest 
#> 
#> 768 samples
#>   8 predictor
#>   2 classes: 'neg', 'pos' 
#> 
#> No pre-processing
#> Resampling: Cross-Validated (5 fold) 
#> Summary of sample sizes: 614, 615, 614, 615, 614 
#> Resampling results across tuning parameters:
#> 
#>   mtry  splitrule   min.node.size  Accuracy   Kappa    
#>   1     variance    1                    NaN        NaN
#>   1     variance    5                    NaN        NaN
#>   1     extratrees  1              0.6808675  0.2662428
#>   1     extratrees  5              0.6783125  0.2618862
...

models[[""100_tr_model""]]
#> Random Forest 
...
#> 
#>   mtry  splitrule   min.node.size  Accuracy   Kappa    
#>   1     variance    1                    NaN        NaN
#>   1     variance    5                    NaN        NaN
#>   1     extratrees  1              0.7473559  0.3881530
#>   1     extratrees  5              0.7564808  0.4112127
...


Created on 2023-04-19 with reprex v2.0.2
",cross-validation
error during recursive feature elimination using histogram based gbm,"I am implementing Recursive Feature Elimination using the HistGradientBoostingClassifier, but for some reason keeps on getting the following error:
ValueError: when importance_getter=='auto', the underlying estimator HistGradientBoostingClassifier should have coef_ or feature_importances_ attribute. Either pass a fitted estimator to feature selector or call fit before calling transform.
from sklearn.ensemble import HistGradientBoostingClassifier
from sklearn.feature_selection import RFECV
from sklearn.model_selection import cross_val_score, RepeatedStratifiedKFold
from sklearn.datasets import make_classification

X_train, y_train = make_classification(n_samples=1000, n_features=20, n_informative=10,
                           n_redundant=5, random_state=42)

# Create a HistGradientBoostingClassifier estimator
estimator = HistGradientBoostingClassifier().fit(X_train, y_train)

# Create a feature selector object using SelectFromModel
# Create a recursive feature elimination with cross-validation object
rfecv = RFECV(estimator=estimator, step=1, cv=RepeatedStratifiedKFold(n_splits=5, n_repeats=1),
              scoring='roc_auc')

# Fit the recursive feature elimination object to the data
rfecv.fit(X_train, y_train)

# Print the selected features and their ranks
print(""Selected Features: "", X_train.columns[rfecv.support_])
print(""Feature Rankings: "", rfecv.ranking_)

","As the error message indicates, HistGradientBoostingClassifier doesn't have a coef_ or feature_importances_ attribute (even after fitting). There's a github Issue discussing that, but at the moment the core devs are more wary of misleading importance scores than the convenience they would provide.
RFECV allows using a callable as importance_getter, which gets passed the fitted model and needs to return importances. So you could use permutation importance or some other custom importance that way. You may be able to copy or recreate impurity reduction from the older gradient boosting implementation this way.
",cross-validation
what is the best practice to apply crossvalidation using timeseriessplit over dataframe within end2end pipeline in python,"Let's say I have dataset within the following pandas dataframe format with a non-standard timestamp column without datetime format as follows:
+--------+-----+
|TS_24hrs|count|
+--------+-----+
|0       |157  |
|1       |334  |
|2       |176  |
|3       |86   |
|4       |89   |
 ...      ...
|270     |192  |
|271     |196  |
|270     |251  |
|273     |138  |
+--------+-----+
274 rows × 2 columns

I have already applied some regression algorithms after splitting data without using cross-validation (CV) into training-set and test-set and got results like the following:
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

#Load the time-series data as dataframe
df = pd.read_csv('/content/U2996_24hrs_.csv', sep="","")
print(df.shape)

# Split the data into training and testing sets
from sklearn.model_selection import train_test_split
train, test = train_test_split(df, test_size=0.27, shuffle=False)
print(train.shape) #(200, 2)
print(test.shape)  #(74, 2)

#visulize splitted data
train['count'].plot(label='Training-set')
test['count'].plot(label='Test-set')
plt.legend()
plt.show()

#Train and fit the model
from sklearn.ensemble import RandomForestRegressor
rf = RandomForestRegressor().fit(train, train['count']) #X, y
rf.score(train, train['count']) #0.9998644192184375

# Use the forest's model to predict on the test-set
predictions = rf.predict(test)

#convert prediction result into dataframe for plot issue in ease
df_pre = pd.DataFrame({'TS_24hrs':test['TS_24hrs'], 'count_prediction':predictions})

# Calculate the mean absolute errors
from sklearn.metrics import mean_absolute_error
rf_mae = mean_absolute_error(test['count'], df_pre['count_prediction'])

print(train.shape)   #(200, 2)
print(test.shape)    #(74, 2)
print(df_pre.shape)  #(74, 2)

#visulize forecast or prediction of used regressor model
train['count'].plot(label='Training-set')
test['count'].plot(label='Test-set')
df_pre['count_prediction'].plot(label=f'RF_forecast  MAE={rf_mae:.2f}')
plt.legend()
plt.show()


According this answer I noticed:

if your data is already sorted based on time then simply use shuffle=False in
train, test = train_test_split(newdf, test_size=0.3, shuffle=False)

So far, I have used this classic split data method, but I want to experiment with Time-series-based split methods that are summarized here:

Additionally, based on my investigation (please see the references at the end of the post), it is recommended to use the cross-validation method (K-Fold) before applying regression models. explanation: Cross Validation in Time Series
Problem: How can split time-series data with using CV methods for comparable results? (plot the quality of data split for ensure\evaluate the quality of data splitting)

TSS CV method: TimeSeriesSplit()
BTSS CV method: BlockingTimeSeriesSplit()

So far, the closest solution that crossed my mind is to separate the last 74 observations as hold-on test-set a side and do CV on just the first 200 observations. I'm still struggling with playing with these arguments max_train_size=199, test_size=73 to reach desired results, but it's very tricky and I couldn't figure it out. in fact, I applied time-series-based data split using TSS CV methods before training RF regressor to train-set (first 200 days\observations) and fit model over test-set (last 74 days\observations).
I've tried recommended TimeSeriesSplit() as the following unsuccessfully:
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

#Load the time-series data as dataframe
df = pd.read_csv('/content/U2996_24hrs_.csv', sep="","")
print(df.shape)

#Try to split data with CV (K-Fold) by using TimeSeriesSplit() method
from sklearn.model_selection import TimeSeriesSplit
tscv = TimeSeriesSplit(
    n_splits=len(df['TS_24hrs'].unique()) - 1,
    gap=0, # since data alraedy groupedby for 24hours to retrieve daily count there is no need to to have gap
    #max_train_size=199, #here: https://stackoverflow.com/a/43326651/10452700 they recommended to set this argument I'm unsure if it is the case for my problem
    #test_size=73,
)

for train_idx, test_idx in tscv.split(df['TS_24hrs']):
    print('TRAIN: ',    df.loc[df.index.isin(train_idx), 'TS_24hrs'].unique(), 
          'val-TEST: ', df.loc[df.index.isin(test_idx),  'TS_24hrs'].unique())

The following figures for understanding and better alignment of split data could be part of the expected output if one could plot for each method:
expected output:

References:

Using k-fold cross-validation for time-series model selection

Cross validation with time series [duplicate]

Time series k-fold cross validation for classification

How many folds for (time series) cross validation

Cross Validation for Time Series Classification (Not Forecasting!)



Edit1:
I found 3 related  posts:

post1
post2

I decided to apply TimeSeriesSplit() in short TTS cv output within for loop to train\fit regression model over training-set with assist of CV-set then predict() over Hold-on test-set. The current output of my implementation shows slightly improvement in forecasting with or without, which could be due to problems in my implementation.




#Load the time-series data as dataframe
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

df = pd.read_csv('/content/U2996_24hrs_.csv', sep="","")
#print(df.shape) #(274, 2)

#####----------------------------without CV

# Split the data into training and testing sets
from sklearn.model_selection import train_test_split
train, test = train_test_split(df, test_size=0.27, shuffle=False)
print(train.shape) #(200, 2)
print(test.shape)  #(74, 2)

#visulize splitted data
#train['count'].plot(label='Training-set')
#test['count'].plot(label='Test-set')
#plt.legend()
#plt.show()

#Train and fit the model
from sklearn.ensemble import RandomForestRegressor
rf = RandomForestRegressor().fit(train, train['count']) #X, y
rf.score(train, train['count']) #0.9998644192184375

# Use the forest's model to predict on the test-set
predictions = rf.predict(test)

#convert prediction result into dataframe for plot issue in ease
df_pre = pd.DataFrame({'TS_24hrs':test['TS_24hrs'], 'count_prediction':predictions})

# Calculate the mean absolute errors
from sklearn.metrics import mean_absolute_error
rf_mae = mean_absolute_error(test['count'], df_pre['count_prediction'])

#####----------------------------with CV

df1 = df[:200] #take just first 1st 200 records
#print(df1.shape) #(200, 2)
#print(len(df1)) #200

from sklearn.model_selection import TimeSeriesSplit
tscv = TimeSeriesSplit(
    n_splits=len(df1['TS_24hrs'].unique()) - 1,
    #n_splits=3,
    gap=0, # since data alraedy groupedby for 24hours to retrieve daily count there is no need to to have gap
    #max_train_size=199,
    #test_size=73,
)

#print(type(tscv)) #<class 'sklearn.model_selection._split.TimeSeriesSplit'>

#mae = []
cv = []
TS_24hrs_tss = []
predictions_tss = []
for train_index, test_index in tscv.split(df1):
    cv_train, cv_test = df1.iloc[train_index], df1.iloc[test_index]
    #cv.append(cv_test.index)
    #print(cv_train.shape) #(199, 2)
    #print(cv_test.shape)  #(1, 2)
    TS_24hrs_tss.append(cv_test.values[:,0])
    #Train and fit the model
    from sklearn.ensemble import RandomForestRegressor
    rf_tss = RandomForestRegressor().fit(cv_train, cv_train['count']) #X, y
    # Use the forest's model to predict on the cv_test
    predictions_tss.append(rf_tss.predict(cv_test))
    #print(predictions_tss)
    # Calculate the mean absolute errors
    #from sklearn.metrics import mean_absolute_error
    #rf_tss_mae = mae.append(mean_absolute_error(cv_test, predictions_tss))
    #print(rf_tss_mae)


#print(len(TS_24hrs_tss))    #199
#print(type(TS_24hrs_tss))   #<class 'list'>
#print(len(predictions_tss)) #199

#convert prediction result into dataframe for plot issue in ease
import pandas as pd

df_pre_tss1 = pd.DataFrame(TS_24hrs_tss)
df_pre_tss1.columns =['TS_24hrs_tss']
#df_pre_tss1

df_pre_tss2 = pd.DataFrame(predictions_tss)
df_pre_tss2.columns =['count_predictioncv_tss']
#df_pre_tss2

df_pre_tss= pd.concat([df_pre_tss1,df_pre_tss2], axis=1)
df_pre_tss

# Use the forest's model to predict on the hold-on test-set
predictions_tsst = rf_tss.predict(test)
#print(len(predictions_tsst)) #74

#convert prediction result of he hold-on test-set into dataframe for plot issue in ease
df_pre_test = pd.DataFrame({'TS_24hrs_tss':test['TS_24hrs'], 'count_predictioncv_tss':predictions_tsst})

# Fix the missing record (1st record) 
df_col_merged = df_pre_tss.merge(df_pre_test, how=""outer"")
#print(df_col_merged.shape) #(273, 2) 1st record is missing
ddf = df_col_merged.rename(columns={'TS_24hrs_tss': 'TS_24hrs', 'count_predictioncv_tss': 'count'})
df_first= df.head(1)
df_merged_pred = df_first.merge(ddf, how=""outer"") #insert first record from original df to merged ones
#print(df_merged_pred.shape) #(274, 2)

print(train.shape)   #(200, 2)
print(test.shape)    #(74, 2)
print(df_pre_test.shape)  #(74, 2)

# Calculate the mean absolute errors
from sklearn.metrics import mean_absolute_error
rf_mae_tss = mean_absolute_error(test['count'], df_pre_test['count_predictioncv_tss'])

#visulize forecast or prediction of used regressor model
train['count'].plot(label='Training-set', alpha=0.5)
test['count'].plot(label='Test-set', alpha=0.5)
#cv['count'].plot(label='cv TSS', alpha=0.5)
df_pre['count_prediction'].plot(label=f'RF_forecast  MAE={rf_mae:.2f}', alpha=0.5)
df_pre_test['count_predictioncv_tss'].plot(label=f'RF_forecast_tss  MAE={rf_mae_tss:.2f}', alpha=0.5 , linestyle='--')
plt.legend()
plt.title('Plot forecast results with & without cross-validation (K-Fold)')
plt.show()


post3 sklearn

(I couldn't implement it, one can try this) using make_pipeline() and use def evaluate(model, X, y, cv): function but still confusing if I want to collect the results in the form of dataframe for visualizing case and what is the best practice to pass cv result to regressor and compare the results.



Edit2:
In the spirit of DRY, I tried to build an end-to-end pipeline without/with CV methods, load a dataset, perform feature scaling and supply the data into a regression model:
#Load the time-series data as dataframe
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

df = pd.read_csv('/content/U2996_24hrs_.csv', sep="","")
#print(df.shape) #(274, 2)

#####--------------Create pipeline without CV------------

# Split the data into training and testing sets for just visualization sense
from sklearn.model_selection import train_test_split
train, test = train_test_split(df, test_size=0.27, shuffle=False)
print(train.shape) #(200, 2)
print(test.shape)  #(74, 2)

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler
from sklearn.ensemble import RandomForestRegressor
from sklearn.pipeline import Pipeline

# Split the data into training and testing sets without CV
X = df['TS_24hrs'].values
y = df['count'].values

print(X_train.shape) #(200, 1)
print(y_train.shape) #(200,)
print(X_test.shape)  #(74, 1)
print(y_test.shape)  #(74,)

# Here is the trick
X = X.reshape(-1,1)
X_train, X_test, y_train, y_test = train_test_split(X, y , test_size=0.27, shuffle=False, random_state=0)

print(X_train.shape) #(200, 1)
print(y_train.shape) #(1, 200)
print(X_test.shape)  #(74, 1)
print(y_test.shape)  #(1, 74)

#build an end-to-end pipeline, and supply the data into a regression model. It avoids leaking the test set into the train set
rf_pipeline = Pipeline([('scaler', MinMaxScaler()),('RF', RandomForestRegressor())])
rf_pipeline.fit(X_train, y_train)

#Displaying a Pipeline with a Preprocessing Step and Regression
from sklearn import set_config
set_config(display=""diagram"")
rf_pipeline  # click on the diagram below to see the details of each step


r2 = rf_pipeline.score(X_test, y_test)
print(f""RFR: {r2}"") # -0.3034887940244342

# Use the Randomforest's model to predict on the test-set
y_predictions = rf_pipeline.predict(X_test.reshape(-1,1))

#convert prediction result into dataframe for plot issue in ease
df_pre = pd.DataFrame({'TS_24hrs':test['TS_24hrs'], 'count_prediction':y_predictions})

# Calculate the mean absolute errors
from sklearn.metrics import mean_absolute_error
rf_mae = mean_absolute_error(y_test, df_pre['count_prediction'])

print(train.shape)   #(200, 2)
print(test.shape)    #(74, 2)
print(df_pre.shape)  #(74, 2)

#visulize forecast or prediction of used regressor model
train['count'].plot(label='Training-set')
test['count'].plot(label='Test-set')
df_pre['count_prediction'].plot(label=f'RF_forecast  MAE={rf_mae:.2f}')
plt.legend()
plt.title('Plot results without cross-validation (K-Fold) using pipeline')
plt.show()


#####--------------Create pipeline with TSS CV------------



#####--------------Create pipeline with BTSS CV------------




The results got worse using the pipeline, based on MAE score comparing implementation when separating the steps outside of the pipeline!
","Considering the argues in the comments and assist of @igrinis and found a possible solution addressed in Edit1/post2, I came up with the following implementation to:

meet the declared forecasting strategy:


... training RF regressor to train-set (first 200 days\observations) and fit model over test-set (last 74 days\observations).


use TSS class: TimeSeriesSplit()

#Load the time-series data as dataframe
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

df = pd.read_csv('/content/U2996_24hrs_.csv', sep="","")
# Select the first 200 observations
df200 = df[:200]

# Split the data into training and testing sets
from sklearn.model_selection import train_test_split
#X_train, X_test, y_train, y_test = train_test_split(X, y , test_size=0.27, shuffle=False, random_state=0)
train, validation = train_test_split(df200 , test_size=0.2, shuffle=False)
test = df[200:]
#print(train.shape)       #(160, 2)
#print(validation.shape)  #(40, 2)
#print(test.shape)        #(74, 2) #hold-on (unseen data)

#Train and fit the RF model
from sklearn.ensemble import RandomForestRegressor
#rf_model = RandomForestRegressor().fit(train, train['count']) #X, y

# calculate R2 score using model
#r2_train = rf_model.score(train, train['count'])
#print(f""RFR_train: {r2_train:.4f}"")              #RFR_train: 0.9995
#r2_validation = rf_model.score(validation, validation['count'])
#print(f""RFR_val: {r2_validation:.4f}"")           #RFR_val:   0.9972
#r2_test = rf_model.score(test, test['count'])
#print(f""RFR_test: {r2_test:.4f}"")                #RFR_test:  0.5967

# Use the forest's model to predict on the validation-set and test-set
#predictions_val  = rf_model.predict(validation)
#predictions_test = rf_model.predict(test)

#build an end-to-end pipeline, and supply the data into a regression model and train within pipeline. It avoids leaking the test\val-set into the train-set
from sklearn.preprocessing import MinMaxScaler
from sklearn.ensemble import RandomForestRegressor
from sklearn.pipeline import Pipeline, make_pipeline
rf_pipeline = Pipeline([('scaler', MinMaxScaler()),('RF', RandomForestRegressor())]).fit(train, train['count']) #X, y

#Displaying a Pipeline with a Preprocessing Step and Regression
from sklearn import set_config
set_config(display=""text"")
#print(rf_pipeline)  # Pipeline(steps=[('scaler', MinMaxScaler()), ('RF', RandomForestRegressor())])

# calculate R2 score using pipeline
#r2_train = rf_pipeline.score(train, train['count'])
#print(f""RFR_train: {r2_train:.4f}"")              #RFR_train: 0.9995
#r2_validation = rf_pipeline.score(validation, validation['count'])
#print(f""RFR_val: {r2_validation:.4f}"")           #RFR_val:   0.9972
#r2_test       = rf_pipeline.score(test, test['count'])
#print(f""RFR_test: {r2_test:.4f}"")                #RFR_test:  0.5967

# Use the pipeline to predict over the validation-set and test-set
y_predictions_val  = rf_pipeline.predict(validation)
y_predictions_test = rf_pipeline.predict(test)


from sklearn.model_selection import TimeSeriesSplit
tscv = TimeSeriesSplit(n_splits = 5)
rf_pipeline_tss = Pipeline([('scaler', MinMaxScaler()),('RF', RandomForestRegressor())])
rf_mae_test_tss  = []
tss_cv_test_index = []
for train_index, test_index in tscv.split(df200):
    cv_train, cv_test = df200.iloc[train_index], df200.iloc[test_index]
    #print(f""cv_train: {cv_train.shape}"")
    #print(f""cv_test: {cv_test.shape}"")
    #print(f""cv_test_index: {cv_test.index}"")
    rf_pipeline_tss.fit(cv_train, cv_train['count'])
    predictions_tss = rf_pipeline_tss.predict(cv_test)
    rf_mae_test_tss.append(mean_absolute_error(cv_test['count'], predictions_tss))
    tss_cv_test_index.append(list(cv_test.index))
print(rf_mae_test_tss)
print(tss_cv_test_index)

# Use the TSS-based pipeline to predict over the hold-on (unseen) test-set
y_predictions_test_tss = rf_pipeline_tss.predict(test) 


Similarly, one can use BTSS class within the for-loop to train the model in the pipeline. The following visualisation of the final forecast :

Note: I calculate the mean of splits (K-folds): np.mean(rf_mae_test_tss) and reflect in legend in the plot.
",cross-validation
can any sklearn module return average precision and recall scores for negative class in kfold cross validation,"I am trying to get the mean of precision and recall for BOTH positive and negative class in a 10-fold cross validation. My model is a binary classifier.
I ran the codes below and unfortunately it only returned the mean precision and recall for the positive class. How can I tell the algorithm to return the mean precision and recall scores for the negative class as well?
from sklearn.metrics import make_scorer, accuracy_score, precision_score, recall_score, f1_score
from sklearn.model_selection import cross_validate

scoring = {'accuracy' : make_scorer(accuracy_score), 
           'precision' : make_scorer(precision_score),
           'recall' : make_scorer(recall_score), 
           'f1_score' : make_scorer(f1_score)}

results = cross_validate(model_unbalanced_data_10_times_weight, X, Y, cv=10, scoring=scoring)

np.mean(results['test_precision'])
np.mean(results['test_recall'])

I've also tried printing the classification report using the command ""classification_report(y_test, predictions)"" which resulted in the printout in screenshot below. However, I believe the precision/recall scores from the classification report is based on 1 run only and not the average over 10 folds (correct me if I am wrong).

","Based on our discussion above, I do believe that computing predictions for every cv fold and computing cross_validation_report on them should be the right way to go. Results should now take the number of cv folds into account:
>>> from sklearn.metrics import classification_report
>>> from sklearn.datasets import load_iris
>>> from sklearn.ensemble import RandomForestClassifier
>>> from sklearn.model_selection import cross_val_predict
>>> 
>>> iris = load_iris()
>>> 
>>> rf_clf = RandomForestClassifier()
>>> 
>>> preds = cross_val_predict(estimator=rf_clf,
...                           X=iris[""data""],
...                           y=iris[""target""],
...                           cv=15)
>>> 
>>> print(classification_report(iris[""target""], preds))
              precision    recall  f1-score   support

           0       1.00      1.00      1.00        50
           1       0.92      0.94      0.93        50
           2       0.94      0.92      0.93        50

    accuracy                           0.95       150
   macro avg       0.95      0.95      0.95       150
weighted avg       0.95      0.95      0.95       150

",cross-validation
how to compute precisionrecall and f1 score of an imbalanced dataset for k fold cross validation,"I have an imbalanced dataset containing a binary classification problem. I have built Random Forest Classifier and used k-fold cross-validation with 10 folds.
kfold = model_selection.KFold(n_splits=10, random_state=42)
model=RandomForestClassifier(n_estimators=50) 

I got the results of the 10 folds
results = model_selection.cross_val_score(model,features,labels, cv=kfold)
print results
[ 0.60666667  0.60333333  0.52333333  0.73        0.75333333  0.72        0.7
  0.73        0.83666667  0.88666667]

I have calculated accuracy by taking mean and standard deviation of the results
print(""Accuracy: %.3f%% (%.3f%%)"") % (results.mean()*100.0, results.std()*100.0)
Accuracy: 70.900% (10.345%)

I have computed my predictions as follows
predictions = cross_val_predict(model, features,labels ,cv=10)

Since this is an imbalanced dataset, I would like to calculate the precision, recall, and f1 score of each fold and average the results.
How to calculate the values in python?
","When you use cross_val_score method, you can specify, which scorings you can calculate on each fold:
from sklearn.metrics import make_scorer, accuracy_score, precision_score, recall_score, f1_score

scoring = {'accuracy' : make_scorer(accuracy_score), 
           'precision' : make_scorer(precision_score),
           'recall' : make_scorer(recall_score), 
           'f1_score' : make_scorer(f1_score)}

kfold = model_selection.KFold(n_splits=10, random_state=42)
model=RandomForestClassifier(n_estimators=50) 

results = model_selection.cross_val_score(estimator=model,
                                          X=features,
                                          y=labels,
                                          cv=kfold,
                                          scoring=scoring)

After cross validation, you will get results dictionary with keys: 'accuracy', 'precision', 'recall', 'f1_score', which store metrics values on each fold for certain metric. For each metric you can calculate mean and std value by using np.mean(results[value]) and np.std(results[value]), where value - one of your specified metric name.
",cross-validation
how to use timeseriessplit in cv as mentioned in the documentation of scikitlearn,"Trying to use 10 fold TimeSeriesSplit(), but in the documentation of cross_val_score, it is given that we need to pass a cross-validation generator or an iterable.
tss = TimeSeriesSplit(max_train_size=None, n_splits=10)
l =[]
neighb = [1,3,5,7,9,11,13,12,23,19,18]
for k in neighb:
    knn = KNeighborsClassifier(n_neighbors=k, algorithm='brute')
    sc = cross_val_score(knn, X1, y1, cv=tss, scoring='accuracy')
    l.append(sc.mean())

How should I pass it after the time-series split into train and test data to cv?
TypeError                   
 Traceback (most recent call last)
<ipython-input-44-acf06bc7340e> in <module>()
     14 for k in neighb:
     15     knn = KNeighborsClassifier(n_neighbors=k, algorithm='brute')
---> 16     sc = cross_val_score(knn, X1, y1, cv=tss, scoring='accuracy')
     17     l.append(sc.mean())
     18 ~\Anaconda3\lib\site-packages\sklearn\cross_validation.py in cross_val_score(estimator, X, y, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch)
   1579                                               train, test, verbose, None,
   1580                                               fit_params)
-> 1581                       for train, test in cv)
   1582     return np.array(scores)[:, 0]
   1583 
TypeError: 'TimeSeriesSplit' object is not iterable

","Just pass tss to cv.
scores = cross_val_score(knn, X_train, y_train, cv=tss , scoring='accuracy')

No need to call tss.split().
Update:
The above method is tested on scikit-learn v0.19.1 . So make sure you have the latest version. Also I am using TimeSeriesSplit from model_selection module.
Edit 1:
You are using this now:
tss = TimeSeriesSplit(n_splits=10).split(X_1)
kn = KNeighborsClassifier(n_neighbors=5, algorithm='brute') 
sc = cross_val_score(kn, X1, y1, cv=tss, scoring='accuracy') 

But in the question you posted you did this:
tss = TimeSeriesSplit(n_splits=10)

See the difference between them (split() is not present). I am using this tss in the cross_val_score() without the split() as you posted in the question.
Edit 2:
Dude you are using the deprecated class. 
Currently you are doing this:
from sklearn.cross_validation import cross_val_score

This is wrong. You should get a warning like this:

DeprecationWarning: This module was deprecated in version 0.18 in favor
  of the model_selection module into which all the refactored classes
  and functions are moved. Also note that the interface of the new CV
  iterators are different from that of this module. This module will be
  removed in 0.20.

Pay attention to that, and use the model_selection module like this:
from sklearn.model_selection import cross_val_score

Then you will not get error with my code.
",cross-validation
custom crossvalidation and pycaret,"I've been working with hierarchical time series, and, as a result, I needed to create my own CV to make sure that all timestamps and products are included evenly in the test (validation) set. It worked just fine for sklearn, but I can't make it work in pycaret: best = compare_models() yields nothing at all. Here is the custom CV I used:
class custom_cv:
    def __init__(self, train_end, test_size, n_splits): # val_size
        self.train_end = train_end
        # self.val_size = val_size
        self.test_size = test_size
        self.n_splits = n_splits
        
    def split(self, X):
        self.X = X
        
        for i in range(self.n_splits, 0, -1): # range(start, stop, step)
            tr_threshol = self.train_end - self.test_size*i
            te_threshol = tr_threshol + self.test_size
    
            tr_idx = np.array(self.X.reset_index(drop = True).index[self.X['N_month'] <= tr_threshol])
            te_idx = np.array(self.X.index[(self.X['N_month'] > tr_threshol) & (self.X['N_month'] <= te_threshol)])
        
            yield(tr_idx, te_idx)

    custom_CV = custom_cv(train_end = 365, test_size = 28, n_splits = 5)
    # custom_CV = custom_CV.split(X = df)

My Data looks like this: 1
For sklearn I used the following loop:
def custom_cv(df, train_end = 36, test_size = 4, n_splits = 4):
    cv_idx = []

    for i in range(n_splits, 0, -1): # range(start, stop, step)
        tr_threshol = train_end - test_size*i
        te_threshol = tr_threshol + test_size
    
        tr_idx = list(df.reset_index(drop = True).index[df['N_month'] <= tr_threshol])
        te_idx = list(df.index[(df['N_month'] > tr_threshol) & (df['N_month'] <= te_threshol)])
    
        cv_idx.append((tr_idx, te_idx))
    
    return cv_idx

custom_CV = custom_cv(df = df, train_end = 365, test_size = 28, n_splits = 5)

However, pycaret requires a custom CV generator object compatible with scikit-learn (something I've never dealt with before). I can't figure out what's wrong exactly, and I hope you can kindly help me out.
","Your class for Pycaret is probably missing the get_n_splits method.I had similar problem and solved with the class structure like here:
How to generate a custom cross-validation generator in scikit-learn?
",cross-validation
sklearn grid search with grouped k fold cv generator,"I am trying to implement a grid search over parameters in sklearn using randomized search and a grouped k fold cross-validation generator.  The following works:
skf=StratifiedKFold(n_splits=5,shuffle=True,random_state=0)
rs=sklearn.model_selection.RandomizedSearchCV(clf,parameters,scoring='roc_auc',cv=skf,n_iter=10)
rs.fit(X,y)

This doesn't
gkf=GroupKFold(n_splits=5)
rs=sklearn.model_selection.RandomizedSearchCV(clf,parameters,scoring='roc_auc',cv=gkf,n_iter=10)
rs.fit(X,y)

#ValueError: The groups parameter should not be None

How do I indicate the groups parameter?
Neither does this 
gkf=GroupKFold(n_splits=5)
fv = gkf.split(X, y, groups=groups)
rs=sklearn.model_selection.RandomizedSearchCV(clf,parameters,scoring='roc_auc',cv=gkf,n_iter=10)
rs.fit(X,y)

#TypeError: object of type 'generator' has no len()

","For reference, this is done via 
rs.fit(X,y,groups=groups)

for 
rs=sklearn.model_selection.RandomizedSearchCV(forest,parameters,scoring='roc_auc',cv=gkf,n_iter=10)

",cross-validation
scikitlearn cross validation function not allowing custom folds when indices are not sequential,"Attempting to pass in custom cross validation folds to sklearn's cross validate function.
The cross validate function seems to be triggering an error because it's insisting on using position-based indexing, rather than label-based indexing. The indices I'm passing in my cv_folds argument are consistent with the original dataframe's indices. The reason this is relevant is because I want to use a hash function value to select subsets for my train-test split, as well as my cv folds. I get the following error: IndexError: indices are out-of-bounds
df2 = pd.DataFrame(np.random.rand(8, 3), columns=['feature_1', 'feature_2', 'feature_3'])

train_index_list = [0,1,2,5,6,7]
test_index_list = [3,4]
X_train = df2.loc[train_index_list].drop(columns='feature_3').copy()
y_train = df2.loc[train_index_list]['feature_3'].copy()

# 2-fold cross validation
cv_folds = [ ([0,1,2,],[5,6,7]), ([5,6,7], [0,1,2])]

cv_output = cross_validate(model, X_train, y_train,  scoring=['neg_mean_squared_error'], cv=cv_folds) 

This triggers an error. But what puzzles me is that the following lines run just fine
X_train.loc[train_index_list]
y_train.loc[train_index_list]

How do I resolve this so I can pass in my custom-defined cv folds into Scikit-Learn?
","You can use a workaround by using a Index.get_indexer to convert labels to index positions:
def cv_folds(df, labels):
    for i, j in labels:
        i = df.index.get_indexer(i)
        j = df.index.get_indexer(j)
        yield (i.tolist(), j.tolist())

labels = [([0, 1, 2], [5, 6, 7]), ([5, 6, 7], [0, 1, 2])]
cv = cv_folds(X_train, labels)
cv_output = cross_validate(model, X_train, y_train, cv=cv,
                           scoring=['neg_mean_squared_error'])

Test:
>>> list(cv_folds(X_train, labels))
    [([0, 1, 2], [3, 4, 5]), ([3, 4, 5], [0, 1, 2])]  # <- positions
#   [([0, 1, 2], [5, 6, 7]), ([5, 6, 7], [0, 1, 2])]  # <- labels

",cross-validation
continuous data y response not support in the cross_val_score binarymulticlass for iterativeimputer for bayesianridge,"Problem Defined, Continuous Challenge
This new imputer_bayesian_ridge() function is for Iterative Imputer to impute training data. Sending in data frame training data, then immediately get data.values for numpy array variable. This send or passes a training data with many features, and Y response variable. This effort is only seeking to impute on one single feature.
Apparently my continuous data, Y response data, which is price $$$$ continuous data, is not supported in the cross_val_score(interative_imputer, data_array).
So what advise on how to work with continuous data in Y response variable to work with Iterative Imputer and satisfy the cross_val_score for the object 'interativea_imputer'
To support the target type, should I cast my continuous data in Y response variable to binary? No. For this is not a binary classification, so multiclass is more in line. So how to handle price data when it is the response variable?
Error Received
ValueError: Supported target types are: ('binary', 'multiclass'). Got 'continuous' instead.
CODE
   
def imputer_regressor_bay_ridge(data, y):
    data_array = data.values. ##looks OK

    interative_imputer = IterativeImputer(BayesianRidge()). ## runs OK

    interative_imputer_fit = interative_imputer.fit(data_array) ## runs OK

    data_imputed = interative_imputer_fit.transform(data_array) ## runs OK

    cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1) ## runs OK

    scores = cross_val_score(interative_imputer, data_array, y, 
                             scoring='accuracy', cv=cv, n_jobs=-1, error_score='raise')
    
    return scores, data_imputed

DATA SAMPLE
print(train_data.shape)
data_array = train_data.values
data_array

(1460, 250)
array([[-1.73086488, -0.20803433, -0.20714171, ..., -0.11785113,
         0.4676514 , -0.30599503],
       [-1.7284922 ,  0.40989452, -0.09188637, ..., -0.11785113,
         0.4676514 , -0.30599503],
       [-1.72611953, -0.08444856,  0.07347998, ..., -0.11785113,
         0.4676514 , -0.30599503],
       ...,
       [ 1.72611953, -0.16683907, -0.14781027, ..., -0.11785113,
         0.4676514 , -0.30599503],
       [ 1.7284922 , -0.08444856, -0.08016039, ..., -0.11785113,
         0.4676514 , -0.30599503],
       [ 1.73086488,  0.20391824, -0.05811155, ..., -0.11785113,
         0.4676514 , -0.30599503]])

y = train_data['ResponseY'].values
y.shape  

(1460,)
array([ 0.34727322,  0.00728832,  0.53615372, ...,  1.07761115,
       -0.48852299, -0.42084081])

Value Error
Apparently my continuous data, which is price $ data, is not supported in  cross_val_score(interative_imputer, data_array on:
ValueError: Supported target types are: ('binary', 'multiclass'). Got 'continuous' instead.
Empty                                     Traceback (most recent call last)
File ~/opt/anaconda3/lib/python3.9/site-packages/joblib/parallel.py:820, in Parallel.dispatch_one_batch(self, iterator)
    819 try:
--> 820     tasks = self._ready_batches.get(block=False)
    821 except queue.Empty:
    822     # slice the iterator n_jobs * batchsize items at a time. If the
    823     # slice returns less than that, then the current batchsize puts
   (...)
    826     # accordingly to distribute evenly the last items between all
    827     # workers.

File ~/opt/anaconda3/lib/python3.9/queue.py:168, in Queue.get(self, block, timeout)
    167     if not self._qsize():
--> 168         raise Empty
    169 elif timeout is None:

Empty: 

During handling of the above exception, another exception occurred:

ValueError                                Traceback (most recent call last)
Cell In[27], line 5
      3 #train_data, test_data = minmaxscaler(train_data, test_data)  # alternate run for min-max scaler
      4 columns, imputed_df = imputer_regressor(train_data)
----> 5 scores, data_imputed = imputer_regressor_bay_ridge(train_data, y)
      7 misTrain = whichColumnsMissing(train_data)
      8 misTest = whichColumnsMissing(test_data)

Cell In[24], line 110, in imputer_regressor_bay_ridge(data, y)
    108 data_imputed = interative_imputer_fit.transform(data_array)
    109 cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)
--> 110 scores = cross_val_score(interative_imputer, data_array, 
    111                          y, scoring='accuracy', cv=cv, n_jobs=-1, error_score='raise')
    113 return scores, data_imputed

File ~/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:509, in cross_val_score(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, error_score)
    506 # To ensure multimetric format is not supported
    507 scorer = check_scoring(estimator, scoring=scoring)
--> 509 cv_results = cross_validate(
    510     estimator=estimator,
    511     X=X,
    512     y=y,
    513     groups=groups,
    514     scoring={""score"": scorer},
    515     cv=cv,
    516     n_jobs=n_jobs,
    517     verbose=verbose,
    518     fit_params=fit_params,
    519     pre_dispatch=pre_dispatch,
    520     error_score=error_score,
    521 )
    522 return cv_results[""test_score""]

File ~/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:267, in cross_validate(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, return_train_score, return_estimator, error_score)
    264 # We clone the estimator to make sure that all the folds are
    265 # independent, and that it is pickle-able.
    266 parallel = Parallel(n_jobs=n_jobs, verbose=verbose, pre_dispatch=pre_dispatch)
--> 267 results = parallel(
    268     delayed(_fit_and_score)(
    269         clone(estimator),
    270         X,
    271         y,
    272         scorers,
    273         train,
    274         test,
    275         verbose,
    276         None,
    277         fit_params,
    278         return_train_score=return_train_score,
    279         return_times=True,
    280         return_estimator=return_estimator,
    281         error_score=error_score,
    282     )
    283     for train, test in cv.split(X, y, groups)
    284 )
    286 _warn_about_fit_failures(results, error_score)
    288 # For callabe scoring, the return type is only know after calling. If the
    289 # return type is a dictionary, the error scores can now be inserted with
    290 # the correct key.

File ~/opt/anaconda3/lib/python3.9/site-packages/joblib/parallel.py:1041, in Parallel.__call__(self, iterable)
   1032 try:
   1033     # Only set self._iterating to True if at least a batch
   1034     # was dispatched. In particular this covers the edge
   (...)
   1038     # was very quick and its callback already dispatched all the
   1039     # remaining jobs.
   1040     self._iterating = False
-> 1041     if self.dispatch_one_batch(iterator):
   1042         self._iterating = self._original_iterator is not None
   1044     while self.dispatch_one_batch(iterator):

File ~/opt/anaconda3/lib/python3.9/site-packages/joblib/parallel.py:831, in Parallel.dispatch_one_batch(self, iterator)
    828 n_jobs = self._cached_effective_n_jobs
    829 big_batch_size = batch_size * n_jobs
--> 831 islice = list(itertools.islice(iterator, big_batch_size))
    832 if len(islice) == 0:
    833     return False

File ~/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_validation.py:267, in <genexpr>(.0)
    264 # We clone the estimator to make sure that all the folds are
    265 # independent, and that it is pickle-able.
    266 parallel = Parallel(n_jobs=n_jobs, verbose=verbose, pre_dispatch=pre_dispatch)
--> 267 results = parallel(
    268     delayed(_fit_and_score)(
    269         clone(estimator),
    270         X,
    271         y,
    272         scorers,
    273         train,
    274         test,
    275         verbose,
    276         None,
    277         fit_params,
    278         return_train_score=return_train_score,
    279         return_times=True,
    280         return_estimator=return_estimator,
    281         error_score=error_score,
    282     )
    283     for train, test in cv.split(X, y, groups)
    284 )
    286 _warn_about_fit_failures(results, error_score)
    288 # For callabe scoring, the return type is only know after calling. If the
    289 # return type is a dictionary, the error scores can now be inserted with
    290 # the correct key.

File ~/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_split.py:1411, in _RepeatedSplits.split(self, X, y, groups)
   1409 for idx in range(n_repeats):
   1410     cv = self.cv(random_state=rng, shuffle=True, **self.cvargs)
-> 1411     for train_index, test_index in cv.split(X, y, groups):
   1412         yield train_index, test_index

File ~/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_split.py:340, in _BaseKFold.split(self, X, y, groups)
    332 if self.n_splits > n_samples:
    333     raise ValueError(
    334         (
    335             ""Cannot have number of splits n_splits={0} greater""
    336             "" than the number of samples: n_samples={1}.""
    337         ).format(self.n_splits, n_samples)
    338     )
--> 340 for train, test in super().split(X, y, groups):
    341     yield train, test

File ~/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_split.py:86, in BaseCrossValidator.split(self, X, y, groups)
     84 X, y, groups = indexable(X, y, groups)
     85 indices = np.arange(_num_samples(X))
---> 86 for test_index in self._iter_test_masks(X, y, groups):
     87     train_index = indices[np.logical_not(test_index)]
     88     test_index = indices[test_index]

File ~/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_split.py:709, in StratifiedKFold._iter_test_masks(self, X, y, groups)
    708 def _iter_test_masks(self, X, y=None, groups=None):
--> 709     test_folds = self._make_test_folds(X, y)
    710     for i in range(self.n_splits):
    711         yield test_folds == i

File ~/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_split.py:652, in StratifiedKFold._make_test_folds(self, X, y)
    650 allowed_target_types = (""binary"", ""multiclass"")
    651 if type_of_target_y not in allowed_target_types:
--> 652     raise ValueError(
    653         ""Supported target types are: {}. Got {!r} instead."".format(
    654             allowed_target_types, type_of_target_y
    655         )
    656     )
    658 y = column_or_1d(y)
    660 _, y_idx, y_inv = np.unique(y, return_index=True, return_inverse=True)

ValueError: Supported target types are: ('binary', 'multiclass'). Got 'continuous' instead.

","In cross_val_score, the use of scoring='accuracy' is only for binary or multiclass targets.
You should use instead scoring=None or some other scoring adequate for continuous targets. See regression ('neg_mean_absolute_error', 'neg_mean_squared_error', ...).
",cross-validation
error quotboolean array expected for the condition not float64quot during stratifiedkfold,"i'm trying to use the stratifid k- fold for cross validation on my dataset but there is the error ""Boolean array expected for the condition, not float64"" (in the heading code below). Does anyone know the reason?
This is the code:
import pandas as pd
import numpy as np
from sklearn.model_selection import StratifiedKFold
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score

from imblearn.over_sampling import SMOTE
cleanedDataset = `pd.read_csv(""train_numeric_shuffled_50000_cleaned_90.csv"")`

#providing input and output features
x=cleanedDataset.drop(['Id','Response'], axis=1)
y=cleanedDataset['Response']

#applico Stratified K-fold con K=4
skf = StratifiedKFold(n_splits=4)

#stampo risultati dei 4 fold
for i, (train_index, test_index) in enumerate(skf.split(x, y)):
        print(f""Fold {i}:"")
        print(f""  Train: index={train_index}"")
        print(f""  Test:  index={test_index}"")

#uso la colonna response come Target
target = cleanedDataset.loc[:,'Response']

#definizione train model
model = LogisticRegression()
def train_model(train, test, fold_no):

    x_train = train[x]
    y_train = train[y]
    x_test = test[x]
    x_test = test[y]
    model.fit(X_train,y_train)
    predictions = model.predict(X_test)
    print('Fold',str(fold_no),'Accuracy:',accuracy_score(y_test,predictions))

#stampo valori accuratezza algoritmo
fold_no =1
for train_index, test_index in skf.split(cleanedDataset, target):
    train = cleanedDataset.loc[train_index,:]
    test = cleanedDataset.loc[test_index,:]
    train_model(train,test,fold_no)
    fold_no += 1


This is the error traceback from the last few line:
 ValueError  Traceback (most recent call last)
    ~\AppData\Local\Temp\ipykernel_8004\1316530102.py in <module>
          4     train = cleanedDataset.loc[train_index,:]
          5     test = cleanedDataset.loc[test_index,:]
    ----> 6     train_model(train,test,fold_no)
          7     fold_no += 1
    
    ~\AppData\Local\Temp\ipykernel_8004\3643313375.py in train_model(train, test, fold_no)
          3 def train_model(train, test, fold_no):
          4 
    ----> 5     X_train = train[x]
          6     y_train = train[y]
          7     X_test = test[x]

~\anaconda3\lib\site-packages\pandas\core\frame.py in __getitem__(self, key)
   3490         # Do we have a (boolean) DataFrame?
   3491         if isinstance(key, DataFrame):
-> 3492             return self.where(key)
   3493 
   3494         # Do we have a (boolean) 1d indexer?

~\anaconda3\lib\site-packages\pandas\util\_decorators.py in wrapper(*args, **kwargs)
    309                     stacklevel=stacklevel,
    310                 )
--> 311             return func(*args, **kwargs)
    312 
    313         return wrapper

~\anaconda3\lib\site-packages\pandas\core\frame.py in where(self, cond, other, inplace, axis, level, errors, try_cast)
  10962         try_cast=lib.no_default,
  10963     ):
> 10964         return super().where(cond, other, inplace, axis, level, errors, try_cast)
  10965 
  10966     @deprecate_nonkeyword_arguments(

~\anaconda3\lib\site-packages\pandas\core\generic.py in where(self, cond, other, inplace, axis, level, errors, try_cast)
   9313             )
   9314 
-> 9315         return self._where(cond, other, inplace, axis, level, errors=errors)
   9316 
   9317     @doc(

~\anaconda3\lib\site-packages\pandas\core\generic.py in _where(self, cond, other, inplace, axis, level, errors)
   9074                 for dt in cond.dtypes:
   9075                     if not is_bool_dtype(dt):
-> 9076                         raise ValueError(msg.format(dtype=dt))
   9077         else:
   9078             # GH#21947 we have an empty DataFrame/Series, could be object-dtype

ValueError: Boolean array expected for the condition, not float64

What i suppose to modify?
","Apparently your x and y here
#providing input and output features
x=cleanedDataset.drop(['Id','Response'], axis=1)
y=cleanedDataset['Response']

should contain column names only, not column contents. it needs to be changed to
x = [c for c in cleanedDataset.columns if c not in {'Id','Response'}]
y = 'Response'

Then train[x], train[y] etc can take correct columns in the dataframe cleanedDataset.
",cross-validation
resamples folds for crossvalidation in r,"I'm using tidymodels framework for creating stratified resample folds for cross-validation in a random forest model. Is it possible to actually access and view / plot the data within each of these folds? Reproducible code below:
library(tidyverse)
library(tidymodels)

df_cv <- vfold_cv(iris, v = 10, strata =Species)

","The output of vfold_cv is an rsplit object. You can run split1 <- get_rsplit(df_cv, index = 1) to get the split. analysis(split1) will give you the analysis data frame and assessment(split1) will get you the assessment data frame.
You can also run tidy(split1) to get information about which rows went to the analysis set vs. which rows went to the assessment set.
This reference gives a little bit more information about what you do with an rsplit object.
For a more in-depth understanding of the rsplit class, you can check out the code here.
",cross-validation
why sklearn39s kfold can only be enumerated once also on using it in xgboostcv,"Trying to create a KFold object for my xgboost.cv, and I have
import pandas as pd
from sklearn.model_selection import KFold

df = pd.DataFrame([[1,2,3,4,5],[6,7,8,9,10]])

KF = KFold(n_splits=2)
kf = KF.split(df)

But it seems I can only enumerate once:
for i, (train_index, test_index) in enumerate(kf):
    print(f""Fold {i}"")

for i, (train_index, test_index) in enumerate(kf):
    print(f""Again_Fold {i}"")

gives output of
Fold 0
Fold 1

The second enumerate seems to be on an empty object.
I am probably fundamentally understanding something wrong, or completed messed up somewhere, but could someone explain this behavior?
[Edit, adding follow up question] This behavior seems to cause passing KFold object to xgboost.cv setting xgboost.cv(..., folds = KF.split(df)) to have index out of range error. My fix is to recreate the list of tuples with
kf = []
for i, (train_index, test_index) in enumerate(KF.split(df)):
    this_split = (list(train_index), list(test_index))
    kf.append(this_split)

xgboost.cv(..., folds = kf)

looking for smarter solutions.
","Using an example:
from sklearn.model_selection import KFold
import xgboost as xgb
import numpy as np

data = np.random.rand(5, 10)  # 5 entities, each contains 10 features
label = np.random.randint(2, size=5)  # binary target
dtrain = xgb.DMatrix(data, label=label)

param = {'max_depth': 2, 'eta': 1, 'objective': 'binary:logistic'}

If we run your code :
KF = KFold(n_splits=2)
xgboost.cv(params= param,dtrain=dtrain, folds = KF.split(df))

I get the error :
IndexError                                Traceback (most recent call last)
Cell In[51], line 2
      1 KF = KFold(n_splits=2)
----> 2 xgboost.cv(params= param,dtrain=dtrain, folds = KF.split(df))
[..]

IndexError: list index out of range

In the documentation, it ask for a KFold instance, so you just need to do:
KF = KFold(n_splits=2)
xgb.cv(params= param,dtrain=dtrain, folds = KF)

You can check out the source code and see that it will call the split method, so you don't need to provide KF.split(..) .
",cross-validation
error in newdata objectmodellistvariables  subscript out of boundsquot,"When I am running this code, I am getting this error
""Error in newdata[, object$model.list$variables] : subscript out of bounds""
I am not getting how to solve it.
install.packages(""boot"")
install.packages(""plyr"")

library(boot)
library(plyr)

set.seed(50)
k=10
RMSE.NN=NULL

List=list( )

for(j in 10:65){
  for(i in 1:k){
    index=sample(1:nrow(data2),j)
    
    trainNN=d2[index,]
    testNN=d1[-index,]
    dataset=data2[-index,]
    
    NN=neuralnet(quality~.,d2,hidden=10,linear.output = T)
    predict_testNN=compute(NN,d1[,c(1:5)])
    predict_testNN=(predict_testNN$net.result*(max(data2$quality)-min(data2$quality)))+min(data2$quality)
    
    RMSE.NN[i]<-(sum((data2$quality-predict_testNN)^2)/nrow(dataset))^0.5
  }
  List[[j]]=RMSE.NN
}

(matrixRMSE=do.call(cbind, List))
matrixRMSE

","The wine data you shared has variable names with spaces in them.
R can get confused when variables have spaces in them; such names require quoting with backticks etc to refer to
# i.e. my_num would be simple but `my num` would be needed to maintain the space.

Easiest thing for you to do is use library(janitor) and clean_names on the dataset you want to work with.
",cross-validation
is there any downside in using multiple quotn_jobs1quot statements,"In the context of model selection for a classification problem, while running cross validation, is it ok to specify n_jobs=-1 both in model specification and cross validation function in order to take full advantage of the power of the machine?
For example, comparing sklearn RandomForestClassifier and xgboost XGBClassifier:
RF_model = RandomForestClassifier( ..., n_jobs=-1)
XGB_model = XGBClassifier( ..., n_jobs=-1)

RF_cv = cross_validate(RF_model, ..., n_jobs=-1)
XGB_cv = cross_validate(XGB_model, ..., n_jobs=-1)

is it ok to specify the parameters in both? Or should I specify it only once? And in which of them, model or cross validation statement?
I used for the example models from two different libraries (sklearn and xgboost) because maybe there is a difference in how it works, also cross_validate function is from sklearn.
","Specifying n_jobs twice does have an effect, though whether it has a positive or negative effect is complicated.
When you specify n_jobs twice, you get two levels of parallelism. Imagine you have N cores. The cross-validation function creates N copies of your model. Each model creates N threads to run fitting and predictions. You then have N*N threads.
This can blow up pretty spectacularly. I once worked on a program which needed to apply ARIMA to tens of thousands of time-series. Since each ARIMA is independent, I parallelized it and ran one ARIMA on each core of a 12-core CPU. I ran this, and it performed very poorly. I opened up htop, and was surprised to find 144 threads running. It turned out that this library, pmdarima, internally parallelized ARIMA operations. (It doesn't parallelize them well, but it does try.) I got a massive speedup just by turning off this inner layer of parallelism. Having two levels of parallelism is not necessarily better than having one.
In your specific case, I benchmarked a random forest with cross validation, and I benchmarked four configurations:

No parallelism
Parallelize across different CV folds, but no model parallelism
Parallelize within the model, but not on CV folds
Do both


(Error bars represent 95% confidence interval. All tests used RandomForestClassifier. Test was performed using cv=5, 100K samples, and 100 trees. Test system had 4 cores with SMT disabled. Scores are mean duration of 7 runs.)
This graph shows that no parallelism is the slowest, CV parallelism is third fastest, and model parallelism and combined parallelism are tied for first place.
However, this is closely tied to what classifiers I'm using - a benchmark for pmdarima, for example, would find that cross-val parallelism is faster than model parallelism or combined parallelism. If you don't know which one is faster, then test it.
",cross-validation
scikitlearn cross_validate reveal test set indices,"In sklearn.model_selection.cross_validate , is there a way to output the samples / indices which were used as test set by the CV splitter for each fold?
","There's an option to specify the cross-validation generator, using cv option :

cv int, cross-validation generator or an iterable, default=None
Determines the cross-validation splitting strategy. Possible inputs
for cv are:
None, to use the default 5-fold cross validation,
int, to specify the number of folds in a (Stratified)KFold,
CV splitter,
An iterable yielding (train, test) splits as arrays of indices.
For int/None inputs, if the estimator is a classifier and y is either
binary or multiclass, StratifiedKFold is used. In all other cases,
KFold is used. These splitters are instantiated with shuffle=False so
the splits will be the same across calls.

If you provide it as an input to cross_validate :
from sklearn import datasets, linear_model
from sklearn.model_selection import cross_validate
from sklearn.model_selection import KFold
from sklearn.svm import LinearSVC
diabetes = datasets.load_diabetes()
X = diabetes.data[:150]
y = diabetes.target[:150]
lasso = linear_model.Lasso()

kf = KFold(5, random_state = 99, shuffle = True)
cv_results = cross_validate(lasso, X, y, cv=kf)

You can extract the index like this:
idx = [test_index for train_index, test_index in kf.split(X)]

Where the first in the list will be the test index for the 1st fold and so on..
",cross-validation
why is this deep learning convolutional model not generalizing,"I am training a convolutional network using pytorch that works on 3D medical raster images (.nrrd files) to get estimated volume measurements from very noisy ultrasound images.
I have around 200 individual raster images of 30 patients, and have augmented them to over 5000 applying all kind of transforms and noise in all 3 axis (chosen randomly). All the rasters are resized to 128x128x128 before being used.
I am doing 6-fold cross validation, where I make sure that the validation set is composed of entirely different patients from those in the training set. I think this helps see if the model is actually generalizing and is capable of estimating rasters of unseen patients.
Problem is, the model is failing to generalize or learn at all. See the results I get for 2 test runs I have made (10 hours processing each):
First Training Failure
Second Training Failure
The architecture used is just 6 convolutional layers followed by 2 densely connected ones, nothing too fancy. What could be causing this? Could it be I don't have enough data for my model to learn?
I tried lowering the learning rate and raising weight decay, no luck. I haven't tried using other criterions and optimizers (currently using MSE Loss and Adam).
*Edit: Added code:
class RasterNet(nn.Module):
    def __init__(self):
        super(RasterNet, self).__init__()

        self.conv0 = nn.Sequential( # 128x128x128 -> 256x32x32
            nn.Conv2d(128, 256, kernel_size=7, stride=2, padding=3),
            nn.BatchNorm2d(256),
            nn.ReLU(),
            nn.MaxPool2d(kernel_size=2, stride=2)
        )

        self.conv1 = nn.Sequential( # 256x32x32 -> 512x16x16
            nn.Conv2d(256, 512, kernel_size=3, stride=1, padding=1),
            nn.BatchNorm2d(512),
            nn.ReLU(),
            nn.MaxPool2d(kernel_size=2, stride=2)
        )

        self.conv2 = nn.Sequential( # 512x16x16 -> 1024x8x8
            nn.Conv2d(512, 1024, kernel_size=3, stride=1, padding=1),
            nn.BatchNorm2d(1024),
            nn.ReLU(),
            nn.MaxPool2d(kernel_size=2, stride=2)
        )

        self.conv3 = nn.Sequential( # 1024x8x8 -> 2048x4x4
            nn.Conv2d(1024, 2048, kernel_size=3, stride=1, padding=1),
            nn.BatchNorm2d(2048),
            nn.ReLU(),
            nn.MaxPool2d(kernel_size=2, stride=2)
        )

        self.conv4 = nn.Sequential( # 2048x4x4 -> 4096x2x2
            nn.Conv2d(2048, 4096, kernel_size=3, stride=1, padding=1),
            nn.BatchNorm2d(4096),
            nn.ReLU(),
            nn.MaxPool2d(kernel_size=2, stride=2)
        )

        self.conv5 = nn.Sequential( # 4096x2x2 -> 8192x1x1
            nn.Conv2d(4096, 8192, kernel_size=3, stride=1, padding=1),
            nn.BatchNorm2d(8192),
            nn.ReLU(),
            nn.MaxPool2d(kernel_size=2, stride=2)
        )

        self.linear = nn.Sequential(
            nn.Linear(8192, 4096),
            nn.ReLU(),
            nn.Linear(4096, 1)
        )

    def forward(self, base):
        base = base.squeeze().float().to(dml)

        # View from y axis (Coronal, as this is the clearest view)
        base = torch.transpose(base, 2, 1)

        x = self.conv0(base)
        x = self.conv1(x)
        x = self.conv2(x)
        x = self.conv3(x)
        x = self.conv4(x)
        x = self.conv5(x)
        x = x.view(x.size(0), -1)
        return self.linear(x)

","Ok a few notes which are not an ""answer"" per se but are too extended for comments:
First, the fact that your training loss converges to a low value, but your validation loss is high, means that your model is overfit to the training distribution. This could mean:

Your model architecture is not expressive enough to meaningfully distill high-level information from low-level (pixel/voxel) information so instead learns training-set wide bias terms that bring the loss relatively low. This could indicate that your validation and training split are from different distributions, or else that your loss function is not well-chosen for the task.
Your model is too expressive (high variance) such that it can learn the exact training examples (classic overfitting)

Second, an almost-ubiquitous trick for NN training is to use at-runtime data augmentation. This means that, rather then generating a set of augmented images before training, you instead generate a set of augmenting functions which apply data transformations randomly. This set of functions is used to transform the data batch at each training epoch, such that the model never sees exactly the same data example twice.
Third, this model architecture is relatively simplistic (simpler than AlexNet, the first modern deep CNN.) Far greater performance has been achieved by making  much deeper architectures and using residual layers to (see ResNet) to deal with the vanishing gradient problem. I'd be somewhat surprised if you could achieve good performance on this task with this architecture.
It is normal for the validation loss to be higher on average than the training loss. It is possible that your model is learning to some extent but the loss curve is relatively shallow when compared to the (likely overfit) training curve. I suggest also computing epoch-wide validation accuracy and reporting this value across epochs. You should see training accuracy increase, and possibly validation accuracy as well.
Do note that cross-validation is not quite exactly meant to determine whether the model generalizes to unseen patients. That is the purpose of the validation set. Instead, cross-validation ensures that the training - validation performance is valid across multiple data partitions, and isn't simply the result of selecting an ""easy"" validation set.
Purely for speed/simplicity, I recommend training the model first without cross-validation (i.e. use a single training-testing partition. Once you achieve good performance on the whole dataset, you can retrain with k-fold to ensure the above, but this should make your debug cycles a bit faster.
",cross-validation
cross validation metrics for h2o,"I'm having a hard time understanding why the output for various metrics on my models differs when I use h2o.  
For example,  if I use 'h2o.grid' then the logloss measure is different when I look at the mean model$cross_validation_metrics_summary.  It is the same as   model$cross_validation_metrics_summary.   What is the reasoning behind this difference?  What one should I report on?
library(mlbench) 
  library(h2o)
 data(Sonar)

h2o.init() Sonarhex <- as.h2o(Sonar) h2o.grid(""gbm"", grid_id = ""gbm_grid_id0"", x = c(1:50), y = 'Class',
         training_frame = Sonarhex, hyper_params = list(ntrees = 50, learn_rate = c(.1, .2, .3)), nfolds = 5, seed=1234)

grid <- h2o.getGrid(""gbm_grid_id0"", sort_by = 'logloss')

first_model = h2o.getModel(grid@model_ids[[1]]) first_model@model$cross_validation_metrics_summary first_model@model$cross_validation_metrics

","This inconsistency is an issue that has been documented and explained here and will be resolved in a future release.  The model$cross_validation_metrics_summary metrics are the correct CV metrics.  The metrics that appear in the Grid table or by using the utility functions like h2o.logloss(model, xval = TRUE) are slightly different because they aggregate the CV predictions and then compute the loss (instead of computing the loss separately across K folds and then taking the average).  This can lead to slight numerical differences.
",cross-validation
retrieve cross validation performance auc on h2o automl for holdout dataset,"I am training a binary classification model with h2o AutoML using the default cross-validation (nfolds=5). I need to obtain the AUC score for each holdout fold in order to compute the variability.
This is the code I am using:
h2o.init()

prostate = h2o.import_file(""https://h2o-public-test-data.s3.amazonaws.com/smalldata/prostate/prostate.csv"")
# convert columns to factors
prostate['CAPSULE'] = prostate['CAPSULE'].asfactor()
prostate['RACE'] = prostate['RACE'].asfactor()
prostate['DCAPS'] = prostate['DCAPS'].asfactor()
prostate['DPROS'] = prostate['DPROS'].asfactor()

# set the predictor and response columns
predictors = [""AGE"", ""RACE"", ""VOL"", ""GLEASON""]
response_col = ""CAPSULE""

# split into train and testing sets
train, test = prostate.split_frame(ratios = [0.8], seed = 1234)


aml = H2OAutoML(seed=1, max_runtime_secs=100, exclude_algos=[""DeepLearning"", ""GLM""],
                    nfolds=5, keep_cross_validation_predictions=True)

aml.train(predictors, response_col, training_frame=prostate)

leader = aml.leader

I check that leader is not a StackedEnsamble model (for which the validation metrics are not available). Anyway, I am not able to retrieve the five AUC scores.
Any idea on how to do so?
","Here's how it's done:
import h2o
from h2o.automl import H2OAutoML

h2o.init()

# import prostate dataset
prostate = h2o.import_file(""https://h2o-public-test-data.s3.amazonaws.com/smalldata/prostate/prostate.csv"")
# convert columns to factors
prostate['CAPSULE'] = prostate['CAPSULE'].asfactor()
prostate['RACE'] = prostate['RACE'].asfactor()
prostate['DCAPS'] = prostate['DCAPS'].asfactor()
prostate['DPROS'] = prostate['DPROS'].asfactor()

# set the predictor and response columns
predictors = [""AGE"", ""RACE"", ""VOL"", ""GLEASON""]
response_col = ""CAPSULE""

# split into train and testing sets
train, test = prostate.split_frame(ratios = [0.8], seed = 1234)

# run AutoML for 100 seconds
aml = H2OAutoML(seed=1, max_runtime_secs=100, exclude_algos=[""DeepLearning"", ""GLM""],
                    nfolds=5, keep_cross_validation_predictions=True)
aml.train(x=predictors, y=response_col, training_frame=prostate)

# Get the leader model
leader = aml.leader

There is a caveat to mention here about cross-validated AUC -- H2O currently stores two computations of CV AUC.  One is an aggregated version (take the AUC of aggregated CV predictions), and the other is the ""true"" definition of cross-validated AUC (an average of the k AUCs from k-fold cross-validation).  The latter is stored in an object which also contains the individual fold AUCs, as well as the standard deviation across the folds.
If you're wondering why we do this, there's some historical & technical reasons why we have two versions, as well as a ticket open to only every report the latter.
The first one is what you get when you do this (and also what appears on the AutoML Leaderboard).
# print CV AUC for leader model
print(leader.model_performance(xval=True).auc())

If you want the fold-wise AUCs so you can compute or view their mean and variability (standard deviation), you can do that by looking here:
# print CV metrics summary
leader.cross_validation_metrics_summary()

Output:
Cross-Validation Metrics Summary:
             mean        sd           cv_1_valid    cv_2_valid    cv_3_valid    cv_4_valid    cv_5_valid
-----------  ----------  -----------  ------------  ------------  ------------  ------------  ------------
accuracy     0.71842104  0.06419111   0.7631579     0.6447368     0.7368421     0.7894737     0.65789473
auc          0.7767409   0.053587236  0.8206676     0.70905924    0.7982079     0.82538515    0.7303846
aucpr        0.6907578   0.0834025    0.78737605    0.7141305     0.7147677     0.67790955    0.55960524
err          0.28157896  0.06419111   0.23684211    0.35526314    0.2631579     0.21052632    0.34210527
err_count    21.4        4.8785243    18.0          27.0          20.0          16.0          26.0
---          ---         ---          ---           ---           ---           ---           ---
precision    0.61751753  0.08747421   0.675         0.5714286     0.61702126    0.7241379     0.5
r2           0.20118153  0.10781976   0.3014902     0.09386432    0.25050205    0.28393403    0.07611712
recall       0.84506994  0.08513061   0.84375       0.9142857     0.9354839     0.7241379     0.8076923
rmse         0.435928    0.028099842  0.41264254    0.47447023    0.42546       0.41106534    0.4560018
specificity  0.62579334  0.15424488   0.70454544    0.41463414    0.6           0.82978725    0.58

See the whole table with table.as_data_frame()

Here's what the leaderboard looks like (storing aggregated CV AUCs).  In this case, because the data is so small (300 rows), there's a noticeable difference between the two reported between the two reported CV AUC values, however for larger datasets, they should be much closer estimates.
# print the whole Leaderboard (all CV metrics for all models)
lb = aml.leaderboard
print(lb)

That will print the top of the leaderboard:
model_id                                                  auc    logloss     aucpr    mean_per_class_error      rmse       mse
---------------------------------------------------  --------  ---------  --------  ----------------------  --------  --------
XGBoost_grid__1_AutoML_20200924_200634_model_2       0.769716   0.565326  0.668827                0.290806  0.436652  0.190665
GBM_grid__1_AutoML_20200924_200634_model_4           0.762993   0.56685   0.666984                0.279145  0.437634  0.191524
XGBoost_grid__1_AutoML_20200924_200634_model_9       0.762417   0.570041  0.645664                0.300121  0.440255  0.193824
GBM_grid__1_AutoML_20200924_200634_model_6           0.759912   0.572651  0.636713                0.30097   0.440755  0.194265
StackedEnsemble_BestOfFamily_AutoML_20200924_200634  0.756486   0.574461  0.646087                0.294002  0.441413  0.194845
GBM_grid__1_AutoML_20200924_200634_model_7           0.754153   0.576821  0.641462                0.286041  0.442533  0.195836
XGBoost_1_AutoML_20200924_200634                     0.75411    0.584216  0.626074                0.289237  0.443911  0.197057
XGBoost_grid__1_AutoML_20200924_200634_model_3       0.753347   0.57999   0.629876                0.312056  0.4428    0.196072
GBM_grid__1_AutoML_20200924_200634_model_1           0.751706   0.577175  0.628564                0.273603  0.442751  0.196029
XGBoost_grid__1_AutoML_20200924_200634_model_8       0.749446   0.576686  0.610544                0.27844   0.442314  0.195642

[28 rows x 7 columns]

",cross-validation
nlp neural net validation accuracy increases too much  between folds in cross validation,"I'm training a model with BERT for classification with two labels. I'd like to use cross validation, as I want to get an out of sample prediction of each observations in the data set to use later in linear regressions. I have 5 EPOCHS.
The behavior of the first fold is as expected: the validation accuracy increases across EPOCHS and converges to the accuracy I get when running the neural net with the usual 80-10-10 split and the whole sample (about .86).
The strange part of it is that for the subsequent folds (2 to 5), the validation accuracy keeps increasing---to .90, .95, .98 and 1.0.
I believe the code is right, as I re-run all the model from scratch for each fold. I've also manually checked and the split seems to be ok. Each fold's validation set is random, unique, and not overlaps with its corresponding training dataset A possible explanation could be that the weights are not reinitialized between folds?? That would cause the new observations from the new folds to have been used before to compute the weights. But that looks strange to me.
I copy my code. Any help or ideas would be much appreciated. Thank you!
%%time



# Create K train/test folds
n_folds = 5
kf = KFold(n_splits=n_folds, random_state=RANDOM_SEED, shuffle=True)

# Initialize matrix to store results across folds
train_acc_mat = [[0 for _ in range(EPOCHS)] for _ in range(n_folds)]
train_loss_mat = [[0 for _ in range(EPOCHS)] for _ in range(n_folds)]
val_acc_mat = [[0 for _ in range(EPOCHS)] for _ in range(n_folds)]
val_loss_mat = [[0 for _ in range(EPOCHS)] for _ in range(n_folds)]

# Create fold index to store results in matrix
fold_index = 1

# Loop across folds
for train_index, val_index in kf.split(df):
  df_train = df.iloc[train_index]
  df_val = df.iloc[val_index]
  
  # Run Data Loader function for each training data set - fold
  train_data_loader = create_data_loader(df_train, tokenizer, MAX_LEN, BATCH_SIZE)
  # Run Data Loader function for each validation data set - fold
  val_data_loader = create_data_loader(df_val, tokenizer, MAX_LEN, BATCH_SIZE)

  # Train data loader
  data = next(iter(train_data_loader))

  # Create input_ids and attention_mask
  input_ids = data['input_ids'].to(device)
  attention_mask = data['attention_mask'].to(device)

  # Set last layer classification function, other config
  F.sigmoid(model(input_ids, attention_mask))
  optimizer = AdamW(model.parameters(), lr=2e-5, correct_bias=False)
  total_steps = len(train_data_loader) * EPOCHS

  scheduler = get_linear_schedule_with_warmup(
    optimizer,
    num_warmup_steps=0,
    num_training_steps=total_steps
  )

  loss_fn = nn.CrossEntropyLoss().to(device)


  ###### TRAIN MODEL

  # Set accuracy to 0 to store fold best model results
  best_accuracy = 0

  # Create epoch index to store results in matrix
  EPOCH_index = 1

  # Iterate over EPOCHS, TRAIN MODEL
  for epoch in range(EPOCHS):

    print(f'Epoch {epoch + 1}/{EPOCHS}')
    print('-' * 10)

    train_acc, train_loss = train_epoch(
      model,
      train_data_loader,    
      loss_fn, 
      optimizer, 
      device, 
      scheduler, 
      len(df_train)
    )

    print(f'Train loss {train_loss} accuracy {train_acc}')

    val_acc, val_loss = eval_model(
      model,
      val_data_loader,
      loss_fn, 
      device, 
      len(df_val)
    )

    print(f'Val   loss {val_loss} accuracy {val_acc}')
    print()

    # For fold_index, store results from EPOCH_index iteration
    train_acc_mat[fold_index-1][EPOCH_index-1] = train_acc.item()
    train_loss_mat[fold_index-1][EPOCH_index-1] = train_loss.item()
    val_acc_mat[fold_index-1][EPOCH_index-1] = val_acc.item()
    val_loss_mat[fold_index-1][EPOCH_index-1] = val_loss.item()

    # Save fold_index best model
    if val_acc > best_accuracy:
      torch.save(model.state_dict(), 'best_model_state_%s.bin' % fold_index)
      best_accuracy = val_acc
    
    # Update index for next EPOCH iteration
    EPOCH_index = EPOCH_index + 1

  # Store fold results
  globals()['train_fold_%s' % fold_index] = np.asmatrix(train_index)  
    
  y_review_texts, y_pred, y_pred_probs, y_test = get_predictions(
  model,
  val_data_loader
  )

  globals()['results_fold_%s' % fold_index] = np.asmatrix(val_index)
  globals()['results_fold_%s' % fold_index] = np.vstack([globals()['results_fold_%s' % fold_index], y_review_texts])
  globals()['results_fold_%s' % fold_index] = np.vstack([globals()['results_fold_%s' % fold_index], y_pred.detach().numpy()])
  globals()['results_fold_%s' % fold_index] = np.vstack([globals()['results_fold_%s' % fold_index], y_test.detach().numpy()])
  globals()['results_fold_%s' % fold_index] = np.vstack([globals()['results_fold_%s' % fold_index], y_pred_probs.detach().numpy().T[1]])

  # Update index for next FOLD iteration
  fold_index = fold_index + 1


","You're resetting the optimizer but not resetting the model. With K folds you should have K models.
Imagine you have 5 pieces of data [1, 2, 3, 4, 5]
On the first fold you train [1, 2, 3, 4] and test [5].
Second fold you train [2, 3, 4, 5] test on [1]
Third fold you train [3, 4, 5, 1] test on [2]
Fourth fold you train [4, 5, 1, 2] test on [3]
Fifth fold you train [5, 1, 2, 3] test on [4].
By the time you get to the fifth fold since you're not resetting the model, it's already seen 4 in 4 of the training sets, so it's going to perform very well on it.
This is also why your accuracy continues to increase after each fold. On each fold you're leaking data to the model by testing on an already seen fold. The purpose of K fold cross validation is to see how the model would perform on unseen data.
",cross-validation
month by month rolling cv in mlr3,"My goal is to create Resampling using mlr3 package that uses some sort of rolling CV. More concretely, I want to use n months of data in training set (say 6 months) and one month of data in test set.
Here is example of my data set:
DT <- structure(list(Not_FLS_positive = c(0.408197129345391, 0.765784452003651, 
                                          0.44694266987472, 0.261843524433751, 0.823612378660914, 0.463701982908819, 
                                          0.50286235791919, 0.202937028125778, 0.728864183190907, 0.396498796980005, 
                                          0.0645482452501452, 0.386210901850162, 0.518874968887414, 0.748527337592301, 
                                          0.453414087778976, 0.758566332033519, 0.544926574296856, 0.758151497552477, 
                                          0.641583008379657, 0.15000414834481, 0.271384717497718, 0.516634862689787, 
                                          0.379988384634531, 0.220277109433336, 0.368373019165353, 0.367294449514644, 
                                          0.924583091346553, 0.702895544677674, 0.560192483199204, 0.61212976022567, 
                                          0.0189164523355181, 0.308139052518045), Not_FLS_negative = c(0.690284576453995, 
                                                                                                       0.406288890732598, 0.965402804281092, 0.981830249730358, 0.750850410686136, 
                                                                                                       0.884676014270306, 0.978760474570646, 0.846013440637186, 0.319754417987223, 
                                                                                                       0.70256367709284, 0.0308636853895296, 0.247905085870738, 0.886999087364142, 
                                                                                                       0.28017920849581, 0.697253795735502, 0.720069692192815, 0.838131585497387, 
                                                                                                       0.967559943582511, 0.755745457562433, 0.97593960009956, 0.886833153571725, 
                                                                                                       0.587156724466938, 0.959097320169252, 0.0548411183937609, 0.957769849829918, 
                                                                                                       0.479382726292209, 0.626897867750767, 0.772670704388949, 0.9822450842114, 
                                                                                                       0.736829005226914, 0.420642163776653, 0.723886169418402), bin_aroundzero_ret_excess_stand_22 = structure(c(2L, 
                                                                                                                                                                                                                  1L, 3L, 1L, 1L, 3L, 1L, 1L, 2L, 2L, 2L, 1L, 3L, 1L, 2L, 2L, 1L, 
                                                                                                                                                                                                                  1L, 1L, 3L, 2L, 1L, 3L, 2L, 2L, 2L, 3L, 2L, 1L, 2L, 3L, 2L), levels = c(""0"", 
                                                                                                                                                                                                                                                                                          ""1"", ""-1""), class = ""factor""), monthid = c(""20141"", ""20141"", 
                                                                                                                                                                                                                                                                                                                                     ""20141"", ""20141"", ""20141"", ""20141"", ""20141"", ""20141"", ""20141"", 
                                                                                                                                                                                                                                                                                                                                     ""20141"", ""20142"", ""20142"", ""20142"", ""20142"", ""20142"", ""20142"", 
                                                                                                                                                                                                                                                                                                                                     ""20142"", ""20142"", ""20142"", ""20142"", ""20142"", ""20143"", ""20143"", 
                                                                                                                                                                                                                                                                                                                                     ""20143"", ""20143"", ""20143"", ""20143"", ""20143"", ""20143"", ""20143"", 
                                                                                                                                                                                                                                                                                                                                     ""20143"", ""20143"")), row.names = c(NA, -32L), class = c(""data.table"", 
                                                                                                                                                                                                                                                                                                                                                                                            ""data.frame""))



Now, I want to use months 20141 and 20142 in train set and 20143 in the test set. I thougt the best way is to set monthid group role:
task <- as_task_classif(DT, id = ""aroundzero"", target = ""bin_aroundzero_ret_excess_stand_22"")
task$set_col_roles(""monthid"", ""group"")

and now use ResamplingRollingWindowCV from mlr3temporal package:
resampling = rsmp(""forecast_cv"", folds = 5, fixed_window = TRUE, horizon = 1L, window_size = 6)
resampling$instantiate(task)

but this return an error:
Error in max(ids) - self$param_set$values$horizon : 
  non-numeric argument to binary operator

I have then tried to use custom resampling:
custom = rsmp(""custom"")
train_sets = list(1:2)
test_sets = list(3)
custom$instantiate(task, train_sets, test_sets)
custom$train_set(1)
custom$test_set(1)

but this returns only 1,2 in train and 3 in test set. It seems it does't use groups bt individual observations.
Is it possible to vreate my by month rolling cv in current Resampling scheme or should I create completely new class for it?
","Grouping is not supported by ResamplingForecastCV. You could try Leave-time-out resampling and tag monthid with the ""time"" row role. But with this method you also predict months in the past e.g. training set is 20143 and 20142 and test set is 20141. If that's not working for you, you need to use ResamplingCustom.

this returns only 1,2 in train and 3 in test set

This is the correct behavior. You pass the row ids of the task. You have to do the grouping yourself before and pass the grouped IDs to the resampling.
Update
With you data:
custom = rsmp(""custom"")
train_sets = list(1:21)
test_sets = list(22:32)
custom$instantiate(task, train_sets, test_sets)

",cross-validation
why is cross_val_score not producing consistent results,"When this code executes the results are not consistent.
Where is the randomness coming from?
from sklearn import datasets
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.tree import DecisionTreeClassifier
from sklearn.pipeline import Pipeline
from sklearn.model_selection import KFold
from sklearn.model_selection import cross_val_score

seed = 42
iris = datasets.load_iris()
X = iris.data
y = iris.target

pipeline = Pipeline([('std', StandardScaler()), 
                     ('pca', PCA(n_components = 4)), 
                     ('Decision_tree', DecisionTreeClassifier())], 
                    verbose = False)

kfold = KFold(n_splits = 10, random_state = seed, shuffle = True)
results = cross_val_score(pipeline, X, y, cv = kfold)
print(results.mean())


0.9466666666666667
0.9266666666666665
0.9466666666666667
0.9400000000000001
0.9266666666666665

","DecisionTreeClassifier does not use all columns, but by default the sqrt of the number of columns for each split. You assigned the seed to KFold, but not to DecisionTreeClassifier. So different columns will be selected each run. PCA also accepts a random state.
See DecisionTreeClassifier and PCA
",cross-validation
get outoffold predictions from xgboostcv in python,"In the R xgboost package, I can specify predictions=TRUE to save the out-of-fold predictions during cross-validation, e.g.:
library(xgboost)
data(mtcars)
xgb_params = list(
  max_depth = 1,
  eta = 0.01
)
x = model.matrix(mpg~0+., mtcars)
train = xgb.DMatrix(x, label=mtcars$mpg)
res = xgb.cv(xgb_params, train, 100, prediction=TRUE, nfold=5)
print(head(res$pred))

How would I do the equivalent in the python package?  I can't find a prediction argument for xgboost.cvin python.
","I'm not sure if this is what you want, but you can accomplish this by using the sklearn wrapper for xgboost: (I know I'm using iris dataset as regression problem -- which it isn't but this is for illustration).
import xgboost as xgb
from sklearn.cross_validation import cross_val_predict as cvp
from sklearn import datasets
X = datasets.load_iris().data[:, :2]
y = datasets.load_iris().target
xgb_model = xgb.XGBRegressor()
y_pred = cvp(xgb_model, X, y, cv=3, n_jobs = 1)
y_pred


array([  9.07209516e-01,   1.84738374e+00,   1.78878939e+00,
         1.83672094e+00,   9.07209516e-01,   9.07209516e-01,
         1.77482617e+00,   9.07209516e-01,   1.75681138e+00,
         1.83672094e+00,   9.07209516e-01,   1.77482617e+00,
         1.84738374e+00,   1.84738374e+00,   1.12216723e+00,
         9.96944368e-01,   9.07209516e-01,   9.07209516e-01,
         9.96944368e-01,   9.07209516e-01,   9.07209516e-01,
         9.07209516e-01,   1.77482617e+00,   8.35850239e-01,
         1.77482617e+00,   9.87186074e-01,   9.07209516e-01,
         9.07209516e-01,   9.07209516e-01,   1.78878939e+00,
         1.83672094e+00,   9.07209516e-01,   9.07209516e-01,
         8.91427517e-01,   1.83672094e+00,   9.09049034e-01,
         8.91427517e-01,   1.83672094e+00,   1.84738374e+00,
         9.07209516e-01,   9.07209516e-01,   1.01038718e+00,
         1.78878939e+00,   9.07209516e-01,   9.07209516e-01,
         1.84738374e+00,   9.07209516e-01,   1.78878939e+00,
         9.07209516e-01,   8.35850239e-01,   1.99947178e+00,
         1.99947178e+00,   1.99947178e+00,   1.94922602e+00,
         1.99975276e+00,   1.91500926e+00,   1.99947178e+00,
         1.97454870e+00,   1.99947178e+00,   1.56287444e+00,
         1.96453893e+00,   1.99947178e+00,   1.99715066e+00,
         1.99947178e+00,   2.84575284e-01,   1.99947178e+00,
         2.84575284e-01,   2.00303388e+00,   1.99715066e+00,
         2.04597521e+00,   1.99947178e+00,   1.99975276e+00,
         2.00527954e+00,   1.99975276e+00,   1.99947178e+00,
         1.99947178e+00,   1.99975276e+00,   1.99947178e+00,
         1.99947178e+00,   1.91500926e+00,   1.95735490e+00,
         1.95735490e+00,   2.00303388e+00,   1.99975276e+00,
         5.92201948e-04,   1.99947178e+00,   1.99947178e+00,
         1.99715066e+00,   2.84575284e-01,   1.95735490e+00,
         1.89267385e+00,   1.99947178e+00,   2.00303388e+00,
         1.96453893e+00,   1.98232651e+00,   2.39597082e-01,
         2.39597082e-01,   1.99947178e+00,   1.97454870e+00,
         1.91500926e+00,   9.99531507e-01,   1.00023842e+00,
         1.00023842e+00,   1.00023842e+00,   1.00023842e+00,
         1.00023842e+00,   9.22234297e-01,   1.00023842e+00,
         1.00100708e+00,   1.16144836e-01,   1.00077248e+00,
         1.00023842e+00,   1.00023842e+00,   1.00100708e+00,
         1.00023842e+00,   1.00077248e+00,   1.00023842e+00,
         1.13711983e-01,   1.00023842e+00,   1.00135887e+00,
         1.00077248e+00,   1.00023842e+00,   1.00023842e+00,
         1.00023842e+00,   9.99531507e-01,   1.00077248e+00,
         1.00023842e+00,   1.00023842e+00,   1.00023842e+00,
         1.00023842e+00,   1.00023842e+00,   1.13711983e-01,
         1.00023842e+00,   1.00023842e+00,   1.00023842e+00,
         1.00023842e+00,   9.78098869e-01,   1.00023842e+00,
         1.00023842e+00,   1.00023842e+00,   1.00023842e+00,
         1.00023842e+00,   1.00023842e+00,   1.00077248e+00,
         9.99531507e-01,   1.00023842e+00,   1.00100708e+00,
         1.00023842e+00,   9.78098869e-01,   1.00023842e+00], dtype=float32)

",cross-validation
can39t get logisticregressioncv to converge for any other cs then 1,"I'm trying to build a logistic regression model with an array of hyperparameter values such as:
lambdas = [0.001, 0.01, 0.05, 0.1, 1., 100.]
However, the model won't converge unless i have Cs = 1.Here is my code:
X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2,random_state=42)
lambdas = [0.001, 0.01, 0.05, 0.1, 1., 100.]

RidgeCV = LogisticRegressionCV(Cs = lambdas,penalty =""l2"",cv=10,solver=""saga"",max_iter=1000)
RidgeCV.fit(X_train, y_train)

Does anyone know how to solve this?
I tried to change the solver, inrease max_iter, change the cross validation ammount. Different scaling of the data.The data looks as follows before applying a standard scaler: data head screenshot
","Cs = 1 means you have C as 0.0001. See https://github.com/scikit-learn/scikit-learn/blob/98cf537f5c538fdbc9d27b851cf03ce7611b8a48/sklearn/linear_model/_logistic.py#L266
It seems like your data need stronger regularization. You may try grid search for even lower lambda, such as [0.0001, 0.00001, 0.000001].
",cross-validation
sklearn randomizedsearchcv extract confusion matrix for different folds,"I try to calculate an aggregated confusion matrix to evaluate my model:
cv_results = cross_validate(estimator, dataset.data, dataset.target, scoring=scoring,
                cv=Config.CROSS_VALIDATION_FOLDS, n_jobs=N_CPUS, return_train_score=False)

But I don't know how to extract the single confusion matrices of the different folds. In a scorer I can compute it:
scoring = {
'cm': make_scorer(confusion_matrix)
}

, but I cannot return the comfusion matrix, because it has to return a number instead of an array. If I try it I get the following error: 
ValueError: scoring must return a number, got [[...]] (<class 'numpy.ndarray'>) instead. (scorer=cm)

I wonder if it is possible to store the confusion matrices in a global variable, but had no success using
global cm_list
cm_list.append(confusion_matrix(y_true,y_pred))

in a custom scorer.
Thanks in advance for any advice.
","The problem was, that I could not get access to the estimator after RandomizedSearchCV was finished, because I did not know RandomizedSearchCV implements a predict method. Here is my personal solution:
r_search = RandomizedSearchCV(estimator=estimator, param_distributions=param_distributions,
                          n_iter=n_iter, cv=cv, scoring=scorer, n_jobs=n_cpus,
                          refit=next(iter(scorer)))
r_search.fit(X, y_true)
y_pred = r_search.predict(X)
cm = confusion_matrix(y_true, y_pred)

",cross-validation
how to remove crossvalidation with train_test_split,"My code:
X = data['text_with_tokeniz_lemmatiz']
y = data['toxic']
X_train, X_tmp, y_train, y_tmp = train_test_split(X, y, train_size=0.8, test_size=0.2, shuffle=False, random_state=12345)
X_valid, X_test, y_valid, y_test = train_test_split(X_tmp, y_tmp, test_size=0.5, shuffle=False, random_state=12345)
The inspector wrote to me: ""You use both validation sampling and cross-validation at the same time. It would be better to transfer the entire project to cross-validation and increase the amount of data in training.""
How to fix it?
i dont know(((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((((
","When using a validation dataset, we usually train a model on the training data and evaluate its performance on the validation data.
Cross-validation is essentially the same thing, but done multiple times, with different splits.
As your inspector suggests, it is not necessary to split the validation data yourself, as this is already done during cross-validation.
It is hard to say how you can fix it when we don't see how you use the validation data in the code. From what I see, you first need to get rid of the validation data entirely, so the code would look like:
X = data['text_with_tokeniz_lemmatiz']
y = data['toxic']
X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.9, test_size=0.1, shuffle=False, random_state=12345)

If latter in the code you use validation data to measure the performance of your learning algorithm, you can replace that with cross-validation, for instance using Scikit Learn's cross_val_score.
",cross-validation
kfolds crossvalidator show keyerror none of int64index,"I try to use K-Folds cross-validator with dicision tree. I use for loop to train and test data from KFOLD like this code.
df = pd.read_csv(r'C:\\Users\data.csv')
    
# split data into X and y
X = df.iloc[:,:200]
Y = df.iloc[:,200]

X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2)

clf = DecisionTreeClassifier()

kf =KFold(n_splits=5, shuffle=True, random_state=3)

cnt = 1

# Cross-Validate
for train, test in kf.split(X, Y):
    print(f'Fold:{cnt}, Train set: {len(train)}, Test set:{len(test)}')
    cnt += 1
    
    X_train = X[train]
    y_train = Y[train]
    X_test = X[test]
    y_test = Y[test]

    clf = clf.fit(X_train,y_train)

    predictions = clf.predict(X_test)
    accuracy = accuracy_score(y_test, predictions)

    print(""test"")
    print(y_test)
    print(""predict"")
    print(predictions)
    print(""Accuracy: %.2f%%"" % (accuracy * 100.0))

when I run it show error like this.
KeyError: ""None of [Int64Index([  0,   1,   2,   5,   7,   8,   9,  10,  11,  12,\n            ...\n            161, 164, 165, 166, 167, 168, 169, 170, 171, 173],\n           dtype='int64', length=120)]

How to fix it?
","The issue is here:
X_train = X[train]
y_train = Y[train]
X_test = X[test]
y_test = Y[test]

To access some parts/slices of your dataframe, you should use the iloc property. This should solve your problem:
X_train = X.iloc[train]
y_train = Y.iloc[train]
X_test = X.iloc[test]
y_test = Y.iloc[test]

",cross-validation
cross validation on train validation amp test set,"In the scenario of having three sets

A train set of e.g. 80% (for model training)
A validation set of e.g. 10% (for model training)
A test set of e.g. 10% (for final model testing)

let's say I perform k-fold cross validation (CV) on the example dataset of [1,2,3,4,5,6,7,8,9,10]. Let's also say

10 is the test set in this example
the remaining [1,2,3,4,5,6,7,8,9] will be used for training and validation

leave-one-out CV would than look something like this
# Fold 1
[2, 3, 4, 5, 6, 7, 8, 9] # train
[1]                      # validation
# Fold 2
[1, 3, 4, 5, 6, 7, 8, 9] # train
[2]                      # validation
# Fold 3
[1, 2, 4, 5, 6, 7, 8, 9] # train
[3]                      # validation
# Fold 4
[1, 2, 3, 5, 6, 7, 8, 9] # train
[4]                      # validation
# Fold 5
[1, 2, 3, 4, 6, 7, 8, 9] # train
[5]                      # validation
# Fold 6
[1, 2, 3, 4, 5, 7, 8, 9] # train
[6]                      # validation
# Fold 7
[1, 2, 3, 4, 5, 6, 8, 9] # train
[7]                      # validation
# Fold 8
[1, 2, 3, 4, 5, 6, 7, 9] # train
[8]                      # validation
# Fold 9
[1, 2, 3, 4, 5, 6, 7, 8] # train
[9]                      # validation

Great, now the model has been built and validation using each data point of the combined train and validation set once.
Next, I would test my model on the test set (10) and get some performance.
What I was wondering now is why we not also perform CV using the test set and average the result to see the impact of different test sets? Meaning why we don't do the above process 10 times such that we have each data point also in the test set?
It would be obviously computationally extremely expensive but I was thinking about that cause it seemed difficult to choose an appropriate test set. For example, it could be that my model from above would have performed much differently when I would have chosen 1 as the test set and trained and validated on the remaining points.
I wondered about this in scenarios where I have groups in my data. For example

[1,2,3,4] comes from group A,
[5,6,7,8] comes from group B and
[9,10] comes from group C.

In this case when choosing 10 as the test set, it could perform much differently than choosing 1 right, or am I missing something here?
","All your train-validation-test splits should be randomly sampled and sufficiently big. Hence if your data comes from different groups you should have roughly the same distribution of groups across train, validation and test pools. If your test performance varies based on the sampling seed you're definitely doing something wrong.
As to why not use test set for cross-validation, this would result in overfitting. Usually you would run your cross-validation many times with different hyperparameters and use cv score to select best models. If you don't have a separate test set to evaluate your model at the end of model selection you would never know if you overfitted to the training pool during model selection iterations.
",cross-validation
why does logisticregressioncv39s score differ from cross_val_score,"I was using LogisticRegressionCV's .score() method to yield an accuracy score for my model.
I also used cross_val_score to yield an accuracy score with the same cv split (skf), expecting the same score to show up.
But alas, they were different and I'm confused.
I first did a StratifiedKFold:
skf = StratifiedKFold(n_splits = 5,
                      shuffle = True,
                      random_state = 708)

After which I instantiated a LogisticRegressionCV() with the skf as an argument for the CV parameter, fitted, and scored on the training set.
logreg = LogisticRegressionCV(cv=skf, solver='liblinear')

logreg.fit(X_train_sc, y_train)
logreg.score(X_train_sc, y_train)

This gave me a score of 0.849507735583685, which was accuracy by default. Since this is LogisticRegressionCV, this score is actually the mean accuracy score right?
Then I used cross_val_score:
cross_val_score(logreg, X_train_sc, y_train, cv=skf).mean()

This gave me a mean accuracy score of 0.8227814439082044.
I'm kind of confused as to why the scores differ, since I thought I was basically doing the same thing.
","
[.score] is actually the mean accuracy score right?

No. The score method here is the accuracy score of the final classifier (which was retrained on the entire training set, using the optimal value of the regularization strength). By evaluating it on the training set again, you're getting an optimistically-biased estimate of future performance.
To recover the cross-validation scores, you can use the attribute scores_.  Even with the same folds, these may be slightly different from cross_val_score due to randomness in the solver, if it doesn't converge completely.
",cross-validation
i can39t execute cross_val_score with scikeraswrapperskerasregressor,"from tensorflow import keras
from sklearn.model_selection import cross_val_score
from sklearn.datasets import make_regression
from scikeras.wrappers import KerasRegressor
X, y = make_regression(n_samples=10_000)
input_shape = X.shape[1]
model = keras.Sequential([
    keras.layers.Dense(100, activation='relu', input_dim=input_shape),
    keras.layers.Dense(200, activation='relu'),
    keras.layers.Dense(200, activation='relu'),
    keras.layers.Dense(1, activation='linear')])
model.compile(keras.optimizers.Adam(), loss='mse')

model = KerasRegressor(model, batch_size=256, verbose=1, epochs=10)

val_score = cross_val_score(model, X, y, cv=5)
plt.plot(val_score)

when I run the attached code normally it should work but for some reason it displays this error :
----------------------------------------------------------------------------------------------
Empty                                     Traceback (most recent call last)
/usr/local/lib/python3.8/dist-packages/joblib/parallel.py in dispatch_one_batch(self, iterator)
861             try:
--> 862                 tasks = self._ready_batches.get(block=False)
863             except queue.Empty:
13 frames
Empty:
During handling of the above exception, another exception occurred:
AttributeError                            Traceback (most recent call last)
/usr/local/lib/python3.8/dist-packages/keras/optimizers/optimizer_v2/optimizer_v2.py in _getattribute_(self, name)
864     """"""Overridden to support hyperparameter access.""""""
865     try:
--> 866       return super(OptimizerV2, self)._getattribute_(name)
867     except AttributeError as e:
868       # Needed to avoid infinite recursion with _setattr_.
AttributeError: 'Adam' object has no attribute 'build'
","(TensorFlow 2.11) Make sure you're doing:
from tensorflow import keras

There is a difference between import keras and from tensorflow import keras:
>>> import keras
>>> keras.optimizers.Adam.build
AttributeError: type object 'Adam' has no attribute 'build'

>>> from tensorflow import keras
>>> keras.optimizers.Adam.build
<function Adam.build at 0x7f1ff29e7b50>


(TensorFlow 2.9)
Boilerplate wrapping in a get_model function appears to resolve this:
from tensorflow import keras
from sklearn.model_selection import cross_val_score
from sklearn.datasets import make_regression
from scikeras.wrappers import KerasRegressor
X, y = make_regression(n_samples=10_000)

def get_model(meta):
  X_shape_ = meta[""X_shape_""]
  model = keras.Sequential()
  model.add(keras.layers.Dense(100, activation='relu', input_shape=X_shape_[1:]))
  model.add(keras.layers.Dense(200, activation='relu'))
  model.add(keras.layers.Dense(200, activation='relu'))
  model.add(keras.layers.Dense(1, activation='linear'))
  return model

model = KerasRegressor(model=get_model, loss=""mse"", batch_size=256, verbose=1, epochs=10)

cross_val_score(model, X, y, cv=5)

",cross-validation
topic models cross validation with loglikelihood or perplexity,"I'm clustering documents using topic modeling. I need to come up with the optimal topic numbers. So, I decided to do ten fold cross validation with topics 10, 20, ...60.
I have divided my corpus into ten batches and set aside one batch for a holdout set. I have ran latent dirichlet allocation (LDA) using nine batches (total 180 documents) with topics 10 to 60. Now, I have to calculate perplexity or log likelihood for the holdout set.
I found this code from one of CV's discussion sessions. I really don't understand several lines of code below. I have dtm matrix using the holdout set (20 documents). But I don't know how to calculate the perplexity or log likelihood of this holdout set.

Questions:

Can anybody explain to me what seq(2, 100, by =1) mean here? Also, what AssociatedPress[21:30] mean? What function(k) is doing here?
 best.model <- lapply(seq(2, 100, by=1), function(k){ LDA(AssociatedPress[21:30,], k) })


If I want to calculate perplexity or log likelihood of the holdout set called dtm, is there better code? I know there are perplexity() and logLik() functions but since I'm new I can not figure out how to implement it with my holdout matrix, called dtm.

How can I do ten fold cross validation with my corpus, containing 200 documents? Is there existing code that I can invoke? I found caret for this purpose, but again cannot figure that out either.


","The accepted answer to this question is good as far as it goes, but it doesn't actually address how to estimate perplexity on a validation dataset and how to use cross-validation.
Using perplexity for simple validation
Perplexity is a measure of how well a probability model fits a new set of data.  In the topicmodels R package it is simple to fit with the perplexity function, which takes as arguments a previously fit topic model and a new set of data, and returns a single number.  The lower the better.
For example, splitting the AssociatedPress data into a training set (75% of the rows) and a validation set (25% of the rows):
# load up some R packages including a few we'll need later
library(topicmodels)
library(doParallel)
library(ggplot2)
library(scales)

data(""AssociatedPress"", package = ""topicmodels"")

burnin = 1000
iter = 1000
keep = 50

full_data  <- AssociatedPress
n <- nrow(full_data)
#-----------validation--------
k <- 5

splitter <- sample(1:n, round(n * 0.75))
train_set <- full_data[splitter, ]
valid_set <- full_data[-splitter, ]

fitted <- LDA(train_set, k = k, method = ""Gibbs"",
                          control = list(burnin = burnin, iter = iter, keep = keep) )
perplexity(fitted, newdata = train_set) # about 2700
perplexity(fitted, newdata = valid_set) # about 4300

The perplexity is higher for the validation set than the training set, because the topics have been optimised based on the training set.
Using perplexity and cross-validation to determine a good number of topics
The extension of this idea to cross-validation is straightforward.  Divide the data into different subsets (say 5), and each subset gets one turn as the validation set and four turns as part of the training set.  However, it's really computationally intensive, particularly when trying out the larger numbers of topics.  
You might be able to use caret to do this, but I suspect it doesn't handle topic modelling yet.  In any case, it's the sort of thing I prefer to do myself to be sure I understand what's going on.
The code below, even with parallel processing on 7 logical CPUs, took 3.5 hours to run on my laptop:
#----------------5-fold cross-validation, different numbers of topics----------------
# set up a cluster for parallel processing
cluster <- makeCluster(detectCores(logical = TRUE) - 1) # leave one CPU spare...
registerDoParallel(cluster)

# load up the needed R package on all the parallel sessions
clusterEvalQ(cluster, {
   library(topicmodels)
})

folds <- 5
splitfolds <- sample(1:folds, n, replace = TRUE)
candidate_k <- c(2, 3, 4, 5, 10, 20, 30, 40, 50, 75, 100, 200, 300) # candidates for how many topics

# export all the needed R objects to the parallel sessions
clusterExport(cluster, c(""full_data"", ""burnin"", ""iter"", ""keep"", ""splitfolds"", ""folds"", ""candidate_k""))

# we parallelize by the different number of topics.  A processor is allocated a value
# of k, and does the cross-validation serially.  This is because it is assumed there
# are more candidate values of k than there are cross-validation folds, hence it
# will be more efficient to parallelise
system.time({
results <- foreach(j = 1:length(candidate_k), .combine = rbind) %dopar%{
   k <- candidate_k[j]
   results_1k <- matrix(0, nrow = folds, ncol = 2)
   colnames(results_1k) <- c(""k"", ""perplexity"")
   for(i in 1:folds){
      train_set <- full_data[splitfolds != i , ]
      valid_set <- full_data[splitfolds == i, ]

      fitted <- LDA(train_set, k = k, method = ""Gibbs"",
                    control = list(burnin = burnin, iter = iter, keep = keep) )
      results_1k[i,] <- c(k, perplexity(fitted, newdata = valid_set))
   }
   return(results_1k)
}
})
stopCluster(cluster)

results_df <- as.data.frame(results)

ggplot(results_df, aes(x = k, y = perplexity)) +
   geom_point() +
   geom_smooth(se = FALSE) +
   ggtitle(""5-fold cross-validation of topic modelling with the 'Associated Press' dataset"",
           ""(ie five different models fit for each candidate number of topics)"") +
   labs(x = ""Candidate number of topics"", y = ""Perplexity when fitting the trained model to the hold-out set"")

We see in the results that 200 topics is too many and has some over-fitting, and 50 is too few.  Of the numbers of topics tried, 100 is the best, with the lowest average perplexity on the five different hold-out sets.

",cross-validation
cross validation for split test and train datasets,"Unlike standart data, I have dataset contain separetly as train, test1 and test2.  I implemented ML algorithms and got performance metrics. But when i apply cross validation, it's getting complicated.. May be someone help me.. Thank you..
It's my code..
train = pd.read_csv('train-alldata.csv',sep="";"")
test = pd.read_csv('test1-alldata.csv',sep="";"")
test2 = pd.read_csv('test2-alldata.csv',sep="";"")

X_train = train_pca_son.drop('churn_yn',axis=1)
y_train = train_pca_son['churn_yn']

X_test = test_pca_son.drop('churn_yn',axis=1)
y_test = test_pca_son['churn_yn']

X_test_2 = test2_pca_son.drop('churn_yn',axis=1)
y_test_2 = test2_pca_son['churn_yn']

For example, KNN Classifier.
knn_classifier = KNeighborsClassifier(n_neighbors =7,metric='euclidean')
knn_classifier.fit(X_train, y_train)

For K-Fold.
from sklearn import datasets
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import KFold, cross_val_score


dtc = DecisionTreeClassifier(random_state=42)

k_folds = KFold(n_splits = 5)

scores = cross_val_score(dtc, X, y, cv = k_folds)

print(""Cross Validation Scores: "", scores)
print(""Average CV Score: "", scores.mean())
print(""Number of CV Scores used in Average: "", len(scores))

","This is a variation on the ""holdout test data"" pattern (see also: Wikipedia: Training, Validation, Test / Confusion in terminology). For churn prediction: this may arise if you have two types of customers, or are evaluating on two time frames.
X_train, y_train    ← perform training and hyperparameter tuning with this
X_test1, y_test1    ← test on this
X_test2, y_test2    ← test on this as well

Cross validation estimates holdout error using the training data—it may come up if you estimate hyperparameters with GridSearchCV. Final evaluation involves estimating performance on two test sets, separately or averaged over the two:
from sklearn.datasets import make_classification
from sklearn.model_selection import train_test_split
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import f1_score

X, y = make_classification(n_samples=1000, random_state=42)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.4)
X_test1, X_test2, y_test1, y_test2 = train_test_split(X_test, y_test, test_size=.5)

print(y_train.shape, y_test1.shape, y_test2.shape)
# (600,) (200,) (200,)

clf = KNeighborsClassifier(n_neighbors=7).fit(X_train, y_train)

print(f1_score(y_test1, clf.predict(X_test1)))
print(f1_score(y_test2, clf.predict(X_test2)))
# 0.819
# 0.805

",cross-validation
how to perform smote with cross validation in sklearn in python,"I have a highly imbalanced dataset and would like to perform SMOTE to balance the dataset and perfrom cross validation to measure the accuracy. However, most of the existing tutorials make use of only single training and testing iteration to perfrom SMOTE.
Therefore, I would like to know the correct procedure to perfrom SMOTE using cross-validation.
My current code is as follows. However, as mentioned above it only uses single iteration.
from imblearn.over_sampling import SMOTE
from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)
sm = SMOTE(random_state=2)
X_train_res, y_train_res = sm.fit_sample(X_train, y_train.ravel())
clf_rf = RandomForestClassifier(n_estimators=25, random_state=12)
clf_rf.fit(x_train_res, y_train_res)

I am happy to provide more details if needed.
","You need to perform SMOTE within each fold. Accordingly, you need to avoid train_test_split in favour of KFold:
from sklearn.model_selection import KFold
from imblearn.over_sampling import SMOTE
from sklearn.metrics import f1_score

kf = KFold(n_splits=5)

for fold, (train_index, test_index) in enumerate(kf.split(X), 1):
    X_train = X[train_index]
    y_train = y[train_index]  # Based on your code, you might need a ravel call here, but I would look into how you're generating your y
    X_test = X[test_index]
    y_test = y[test_index]  # See comment on ravel and  y_train
    sm = SMOTE()
    X_train_oversampled, y_train_oversampled = sm.fit_sample(X_train, y_train)
    model = ...  # Choose a model here
    model.fit(X_train_oversampled, y_train_oversampled )  
    y_pred = model.predict(X_test)
    print(f'For fold {fold}:')
    print(f'Accuracy: {model.score(X_test, y_test)}')
    print(f'f-score: {f1_score(y_test, y_pred)}')

You can also, for example, append the scores to a list defined outside.
",cross-validation
hurdle models  gridsearchcv,"I am currently trying to build a hurdle model - zero inflated regressor to predict the revenue from each of out customers.
We use zero inflated regressor because most (80%) of our customers have 0 as revenue and only 20% have revenue > 0.
So, we build two models like as shown below
zir = ZeroInflatedRegressor(
    classifier=ExtraTreesClassifier(),
    regressor=RandomForestRegressor()
)

And I do gridsearchCV to improve the performance of our model. So, I do the below
from sklearn.model_selection import GridSearchCV

grid = GridSearchCV(
    estimator=zir,
    param_grid={
        'classifier__n_estimators': [100,200,300,400,500],
        'classifier__bootstrap':[True, False],
        'classifier__max_features': ['sqrt','log2',None],
        'classifier__max_depth':[2,4,6,8,None],
        'regressor__n_estimators': [100,200,300,400,500],
        'regressor__bootstrap':[True, False],
        'regressor__max_features': ['sqrt','log2',None],
        'regressor__max_depth':[2,4,6,8,None]  
    },
    scoring = 'neg_mean_squared_error'
)

Now my question is on how does gridsearchCV work in the case of hurdle models?
Does hyperparameters from classifier combine with regressor as well to generate a pair? Or only hypaprameters within the same model type combine to generate new pairs?
Put simply, would classifier have 150 combinations of hyperparameters and regressor seperately have 150?
","In your code snippet, there are 150*150 hyperparameter combinations to try.  (You can check this easily by starting to fit; it will print out the number of model fittings.)  This is just how GridSearchCV works, not anything specific to ZeroInflatedRegressor.
If you want different behavior, you can wrap the individual estimators in grid searches.  For example,
clf = GridSearchCV(
    estimator=ExtraTreesClassifier(),
    param_grid={
        'classifier__n_estimators': [100,200,300,400,500],
        'classifier__bootstrap':[True, False],
        'classifier__max_features': ['sqrt','log2',None],
        'classifier__max_depth':[2,4,6,8,None],
    },
    scoring='roc_auc',
)

reg = GridSearchCV(
    estimator=RandomForestRegressor(),
    param_grid={
        'regressor__n_estimators': [100,200,300,400,500],
        'regressor__bootstrap':[True, False],
        'regressor__max_features': ['sqrt','log2',None],
        'regressor__max_depth':[2,4,6,8,None],
    },
    scoring = 'neg_mean_squared_error',
)
       
zir = ZeroInflatedRegressor(
    classifier=clf,
    regressor=reg,
)

Now we need to know a bit more about the ZeroInflatedRegressor.  It fits its classifier on all the data with target ""is it nonzero?""; in this case, that's a grid search, so we'll search the 150 candidate hyperparameter combinations, choosing the one that performs best in terms of ROC AUC.  Then among the nonzero (predicted) datapoints it fits the regressor, and now again that's 150 hyperparameter points selecting for optimal MSE.
So this version will be much faster, in exchange for less optimality: you optimize the classifier for ROC AUC, not for how it works with the regressor's predictions and final MSE.
",cross-validation
how to run svc classifier after running 10fold cross validation in sklearn,"I'm relatively new to machine learning and would like some help in the following:
I ran a Support Vector Machine Classifier (SVC) on my data with 10-fold cross validation and calculated the accuracy score (which was around 89%). I'm using Python and scikit-learn to perform the task. Here's a code snippet:
def get_scores(features,target,classifier):
  X_train, X_test, y_train, y_test =train_test_split(features, target , 
    test_size=0.3)
    scores = cross_val_score(
    classifier,
    X_train,
    y_train,
    cv=10,
    scoring='accuracy',
    n_jobs=-1)
 return(scores)

get_scores(features_from_df,target_from_df,svm.SVC())

Now, how can I use my classifier (after running the 10-folds cv) to test it on X_test and compare the predicted results to y_test? As you may have noticed, I only used X_train and y_train in the cross validation process.
I noticed that sklearn have cross_val_predict: 
http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_predict.html should I replace my cross_val_score by cross_val_predict? just FYI: my target data column is binarized (have values of 0s and 1s).
If my approach is wrong, please advise me with the best way to proceed with.
Thanks!
","You're almost there: 
# Build your classifier
classifier = svm.SVC()

# Train it on the entire training data set
classifier.fit(X_train, y_train)

# Get predictions on the test set
y_pred = classifier.predict(X_test)

At this point, you can use any metric from the sklearn.metrics module to determine how well you did. For example:
from sklearn.metrics import accuracy_score
print(accuracy_score(y_test, y_pred))

",cross-validation
how to extract model hyperparameters from sparkml in pyspark,"I'm tinkering with some cross-validation code from the PySpark documentation, and trying to get PySpark to tell me what model was selected:
from pyspark.ml.classification import LogisticRegression
from pyspark.ml.evaluation import BinaryClassificationEvaluator
from pyspark.mllib.linalg import Vectors
from pyspark.ml.tuning import ParamGridBuilder, CrossValidator

dataset = sqlContext.createDataFrame(
    [(Vectors.dense([0.0]), 0.0),
     (Vectors.dense([0.4]), 1.0),
     (Vectors.dense([0.5]), 0.0),
     (Vectors.dense([0.6]), 1.0),
     (Vectors.dense([1.0]), 1.0)] * 10,
    [""features"", ""label""])
lr = LogisticRegression()
grid = ParamGridBuilder().addGrid(lr.regParam, [0.1, 0.01, 0.001, 0.0001]).build()
evaluator = BinaryClassificationEvaluator()
cv = CrossValidator(estimator=lr, estimatorParamMaps=grid, evaluator=evaluator)
cvModel = cv.fit(dataset)

Running this in PySpark shell, I can get the linear regression model's coefficients, but I can't seem to find the value of lr.regParam selected by the cross validation procedure. Any ideas?
In [3]: cvModel.bestModel.coefficients
Out[3]: DenseVector([3.1573])

In [4]: cvModel.bestModel.explainParams()
Out[4]: ''

In [5]: cvModel.bestModel.extractParamMap()
Out[5]: {}

In [15]: cvModel.params
Out[15]: []

In [36]: cvModel.bestModel.params
Out[36]: []

","Ran into this problem as well. I found out you need to call the java property for some reason I don't know why. So just do this:
from pyspark.ml.tuning import TrainValidationSplit, ParamGridBuilder, CrossValidator
from pyspark.ml.regression import LinearRegression
from pyspark.ml.evaluation import RegressionEvaluator

evaluator = RegressionEvaluator(metricName=""mae"")
lr = LinearRegression()
grid = ParamGridBuilder().addGrid(lr.maxIter, [500]) \
                                .addGrid(lr.regParam, [0]) \
                                .addGrid(lr.elasticNetParam, [1]) \
                                .build()
lr_cv = CrossValidator(estimator=lr, estimatorParamMaps=grid, \
                        evaluator=evaluator, numFolds=3)
lrModel = lr_cv.fit(your_training_set_here)
bestModel = lrModel.bestModel

Printing out the parameters you want:
>>> print 'Best Param (regParam): ', bestModel._java_obj.getRegParam()
0
>>> print 'Best Param (MaxIter): ', bestModel._java_obj.getMaxIter()
500
>>> print 'Best Param (elasticNetParam): ', bestModel._java_obj.getElasticNetParam()
1

This applies to other methods like extractParamMap() as well. They should fix this soon.
",cross-validation
regression with random forest on imbalanced data,"I'm using r package of random forest to predict the distances between pairs of proteins based on their amino acid sequence, the main interest is the proteins that are close (has smaller distance). my training dataset consist of 10k pair of proteins and the actual distance between them. however, very few pairs of protein (less than 0.2%) has small distances between them, and the problem is that the trained random forest became very accurate in predicting the distance between proteins with large distances and very bad for proteins that have small distances between them. I tried to down-sample the proteins with the large distances in my training data, but the results are still not good. I'm more interested in close proteins (those pairs who have small distance between them). 
there is a very clear signal of over-fitting since my training accuracy is 78 and my testing accuracy is 51% 
any suggestions are highly appreciated
","A couple suggestions: 
1) Look at GBM's from the gbm package.
2) Create more features to help the RF understand what drives distance.  
3) Plot errors vs individual variables to look for what is driving relationships. (ggplot2 is great for this especially using the colour and size options.) 
4) You could also assign 1 or 0 to y-variables based on distance (ie if distance < x; set to 1 / if distance >= x; set to 0).  Once you have two classes you can use the strata argument in RF to create uniformly balanced samples and see what variables are driving the difference in distance using the importance() and varImpPlot() functions of RF.
5) Try using log of distance-related variables.  RF is usually pretty good about compensating for non-linearity but it can't hurt to try.
My guess is that #2 is where you want to spend your time though it is also the hardest and requires the most thought.  
",cross-validation
why does calling the kfold generator with shuffle give the same indices,"With sklearn, when you create a new KFold object and shuffle is true, it'll produce a different, newly randomized fold indices. However, every generator from a given KFold object gives the same indices for each fold even when shuffle is true. Why does it work like this?
Example: 
from sklearn.cross_validation import KFold
X = np.array([[1, 2], [3, 4], [1, 2], [3, 4]])
y = np.array([1, 2, 3, 4])
kf = KFold(4, n_folds=2, shuffle = True)
​
for fold in kf:
    print fold
​
print '---second round----'
​
for fold in kf:
    print fold

Output:
(array([2, 3]), array([0, 1]))
(array([0, 1]), array([2, 3]))
---second round----#same indices for the folds
(array([2, 3]), array([0, 1]))
(array([0, 1]), array([2, 3]))

This question was motivated by a comment on this answer. I decided to split it into a new question to prevent that answer from becoming too long.
","A new iteration with the same KFold object will not reshuffle the indices, that only happens during instantiation of the object. KFold() never sees the data but knows number of samples so it uses that to shuffle the indices. From the code during instantiation of KFold:
if shuffle:
    rng = check_random_state(self.random_state)
    rng.shuffle(self.idxs)

Each time a generator is called to iterate through the indices of each fold, it will use same shuffled indices and divide them the same way.
Take a look at the code for the base class of KFold _PartitionIterator(with_metaclass(ABCMeta)) where __iter__ is defined. The __iter__ method in the base class calls _iter_test_indices in KFold to divide and yield the train and test indices for each fold.
",cross-validation
creating function to run kfold cross validation on glmer object leave one out crossvalidation,"I am trying to create a function to run a k-fold cross validation on a glmer object.
This is just data I got online (my dataset is quite large) so the model isn't the best but if I can get this to work using this data I should be able to switch it to my dataset quite easily.
I want to do a LOOCV(Leave One Out Cross-Validation)
""LOOCV(Leave One Out Cross-Validation) is a type of cross-validation approach in which each observation is considered as the validation set and the rest (N-1) observations are considered as the training set.""
The outline I got was from Caroline's answer on this researchgate thread.
https://www.researchgate.net/post/Does_R_code_for_k-fold_cross_validation_of_a_nested_glmer_model_exist

#load libraries 
library(tidyverse)
library(optimx)
library(lme4)

#add example data 
Data <- read.csv(""https://stats.idre.ucla.edu/stat/data/hdp.csv"")
Data <- select(Data, remission, IL6, CRP, DID)
Data
Data$remission<- as.factor(Data$remission)
Data$DID<- as.factor(Data$DID)

#add ROW column  
Data <- Data %>% mutate(ROW = row_number())
head(Data)
PTOT=NULL
for (i in 1:8825) { # i in total number of observations in dataset 
  ##Data that will be predicted
  DataC1=Data[unique(Data$ROW)==i,]
  ###To train the model
  DataCV=Data[unique(DataC1$ROW)!=i,]
  M1 <- glmer(remission ~ 1 + IL6 + CRP + ( 1 | DID ), data = DataCV, family = binomial, control = glmerControl(optimizer ='optimx', optCtrl=list(method='L-BFGS-B')))
  P1=predict(M1, DataC1)
  names(P1)=NULL
  P1
  PTOT= c(PTOT, P1)
}

R2cv=1-(sum((remission-PTOT)^2)/(length(PTOT))/(var(remission)))



This is the error I get
""Error: Invalid grouping factor specification, DID""
","DataCV is empty.
For example:
i <- 1  ## first time through the loop
DataCV=Data[unique(DataC1$ROW)!=i,]

I think that should have been DataC$ROW), not DataC1$ROW.
A few other comments: a more compact version of your code would look something like this:
## fit the full model
M1 <- glmer(remission ~ 1 + IL6 + CRP + ( 1 | DID ), data = DataC, 
   family = binomial, control = glmerControl(optimizer ='optimx', optCtrl=list(method='L-BFGS-B')))
res <- numeric(nrow(DataCV))
for (i in 1:nrow(DataCV)) {
   new_fit <- update(M1, data = dataC[-i,]
   res[i] <- (predict(new_fit, newdata=dataC[i,]) - remission[i])^2
}

For a well-specified model LOOCV is asymptotically equivalent to AIC, so you might be doing a lot of work to get something that's not very different from the AIC (which you can get directly from a single model fit) ...
",cross-validation
difference between cross_val_score and kfold,"I am learning Machine learning and I am having this doubt. Can anyone tell me what is the difference between:-
from sklearn.model_selection import cross_val_score

and
from sklearn.model_selection import KFold

I think both are used for k fold cross validation, but I am not sure why to use two different code for same function.
If there is something I am missing please do let me know. ( If possible please explain difference between these two methods)
Thanks,
","
cross_val_score is a function which evaluates a data and returns the score.

On the other hand, KFold is a class, which lets you to split your data to K folds.


So, these are completely different. Yo can make K fold of data and use it on cross validation like this:
# create a splitter object
kfold = KFold(n_splits = 10)       
# define your model (any model)
model = XGBRegressor(**params)     
# pass your model and KFold object to cross_val_score
# to fit and get the mse of each fold of data
cv_score = cross_val_score(model,
                           X, y, 
                           cv=kfold, 
                           scoring='neg_root_mean_squared_error')
print(cv_score.mean(), cv_score.std())

",cross-validation
how to split data on balanced training set and test set on sklearn,"I am using sklearn for multi-classification task. I need to split alldata into train_set and test_set. I want to take randomly the same sample number from each class.
Actually, I amusing this function
X_train, X_test, y_train, y_test = cross_validation.train_test_split(Data, Target, test_size=0.3, random_state=0)

but it gives unbalanced dataset! Any suggestion.
","You can use StratifiedShuffleSplit to create datasets featuring the same percentage of classes as the original one:
import numpy as np
from sklearn.model_selection import StratifiedShuffleSplit
X = np.array([[1, 3], [3, 7], [2, 4], [4, 8]])
y = np.array([0, 1, 0, 1])
stratSplit = StratifiedShuffleSplit(y, n_iter=1, test_size=0.5, random_state=42)
for train_idx, test_idx in stratSplit:
    X_train=X[train_idx]
    y_train=y[train_idx]

print(X_train)
# [[3 7]
#  [2 4]]
print(y_train)
# [1 0]

",cross-validation
target transformation and feature selection in scikitlearn,"I am using RFECV for feature selection in scikit-learn. I would like to compare the result of a  simple linear model (X,y) with that of a log transformed model (using X, log(y))
Simple Model:
RFECV and cross_val_score provide the same result (we need to compare the average score of cross-validation across all folds with the score of RFECV for all features: 0.66 = 0.66, no problem, results are reliable)
Log Model:
the Problem: it seems that RFECV does not provide a way to trasnform the y. the scores in this case are 0.55 vs 0.53. This is quite expected though, because I had to manually apply np.log to fit the data: log_seletor = log_selector.fit(X,np.log(y)). This r2 score is for y = log(y), with no inverse_func, while what we need is a way to fit the model on the log(y_train) and calculate the score using exp(y_test). Alternatively, if I try to use the TransformedTargetRegressor, I get the error shown in the code: The classifier does not expose ""coef_"" or ""feature_importances_"" attributes
How do I resolve the problem and make sure that the feature selection process is reliable?
from sklearn.datasets import make_friedman1
from sklearn.feature_selection import RFECV
from sklearn import linear_model
from sklearn.model_selection import cross_val_score
from sklearn.compose import TransformedTargetRegressor
import numpy as np

X, y = make_friedman1(n_samples=50, n_features=10, random_state=0)
estimator = linear_model.LinearRegression()
log_estimator = TransformedTargetRegressor(regressor=linear_model.LinearRegression(),
                                                func=np.log,
                                                inverse_func=np.exp)
selector = RFECV(estimator, step=1, cv=5, scoring='r2')
selector = selector.fit(X, y)
###
# log_selector = RFECV(log_estimator, step=1, cv=5, scoring='r2')
# log_seletor = log_selector.fit(X,y) 
# #RuntimeError: The classifier does not expose ""coef_"" or ""feature_importances_"" attributes
###
log_selector = RFECV(estimator, step=1, cv=5, scoring='r2')
log_seletor = log_selector.fit(X,np.log(y))

print(""**Simple Model**"")
print(""RFECV, r2 scores: "", np.round(selector.grid_scores_,2))
scores = cross_val_score(estimator, X, y, cv=5)
print(""cross_val, mean r2 score: "", round(np.mean(scores),2), "", same as RFECV score with all features"") 
print(""no of feat: "", selector.n_features_ )

print(""**Log Model**"")
log_scores = cross_val_score(log_estimator, X, y, cv=5)
print(""RFECV, r2 scores: "", np.round(log_selector.grid_scores_,2))
print(""cross_val, mean r2 score: "", round(np.mean(log_scores),2)) 
print(""no of feat: "", log_selector.n_features_ )

Output: 
**Simple Model**
RFECV, r2 scores:  [0.45 0.6  0.63 0.68 0.68 0.69 0.68 0.67 0.66 0.66]
cross_val, mean r2 score:  0.66 , same as RFECV score with all features
no of feat:  6

**Log Model**
RFECV, r2 scores:  [0.39 0.5  0.59 0.56 0.55 0.54 0.53 0.53 0.53 0.53]
cross_val, mean r2 score:  0.55
no of feat:  3

","All you need to do is to add such properties to the TransformedTargetRegressor:
class MyTransformedTargetRegressor(TransformedTargetRegressor):
    @property
    def feature_importances_(self):
        return self.regressor_.feature_importances_

    @property
    def coef_(self):
        return self.regressor_.coef_

Then in you code, use that:
log_estimator = MyTransformedTargetRegressor(regressor=linear_model.LinearRegression(),
                                             func=np.log,
                                             inverse_func=np.exp)

",cross-validation
gridsearchcv amp randomizedsearchcv  do you refit the model after running,"I have some test and train data, the test data does not have any dependant variables.
I'm currently running a GridSearchCV or RandomizedSearchCV to find the best paramaters.
Should I pass all of my ""test"" X & y values into a GridSearchCV or RandomizedSearchCV? 
I understand it does a cross validation, so I believe its fine to? 
But if this is the case, what data has the best_estimator been fit with? All of it? Or data from one of the folds?
Do I need to refit the full set of test data after?
","There are quite a lot of questions being asked here, I will try and answer one by one.


Should I pass all of my ""test"" X & y values into a GridSearchCV or RandomizedSearchCV?


You mentioned that you don't have the dependent variable for your test data, in that case, you cannot pass it to your model. Even if you have access to the values of the dependent variable you should not send them to your GridSearchCV or RandomSearchCV, these methods will internally create a validation set on which your model is tested for each hyperparameter setting.


what data has the best_estimator been fit with?


It depends on how you have initialized your GridSearchCV or RandomizedSearchCV object, both these methods have a parameter called refit which when set to TRUE (by default) will refit the model with entire data.


Do I need to refit the full set of test data after?


Generally, you don't use your test data to tune your hyperparameters. You do it using the validation set, and once you have frozen your model, you use the test set to check the performance of the model which will be an unbiased estimation of the model performance.
",cross-validation
adjust threshold cros_val_score sklearn,"There is a way to set the threshold cross_val_score sklearn?
I've trained a model, then I adjust the threshold to 0.22. The model in the following below :
# Try with Threshold
pred_proba = LGBM_Model.predict_proba(X_test)


# Adjust threshold for predictions proba
prediction_with_threshold = []
for item in pred_proba[:,0]:
    if item > 0.22 :
        prediction_with_threshold.append(0)
    else:
        prediction_with_threshold.append(1)

print(classification_report(y_test,prediction_with_threshold))

then I want to validate this model using cross_val_score. I've searched but can't find the method to set threshold for cross_val_score. The cross_val_score that I've used like the following below :
F1Scores = cross_val_score(LGBMClassifier(random_state=101,learning_rate=0.01,max_depth=-1,min_data_in_leaf=60,num_iterations=200,num_leaves=70),X,y,cv=5,scoring='f1')
F1Scores

### how to adjust threshold to 0.22 ??

Or there is other method to validate this model using threshold?
","Assuming that you are working with a two-class classification problem you could override the predict method of LGBMClassifier object with your thresholding approach as shown below:
import numpy as np
from lightgbm import LGBMClassifier
from sklearn.datasets import make_classification

X, y = make_classification(n_features=10, random_state=0, n_classes=2, n_samples=1000, n_informative=8)

class MyLGBClassifier(LGBMClassifier):
    def predict(self,X, threshold=0.22,raw_score=False, num_iteration=None,
                pred_leaf=False, pred_contrib=False, **kwargs):
        result = super(MyLGBClassifier, self).predict_proba(X, raw_score, num_iteration,
                                    pred_leaf, pred_contrib, **kwargs)
        predictions = [1 if p>threshold else 0 for p in result[:,0]]
        return predictions

clf = MyLGBClassifier()
clf.fit(X,y)
clf.predict(X,threshold=2)  # just testing the implementation
# [0,0,0,0,..,0,0,0]        # we get all zeros since we have set threshold as 2

F1Scores = cross_val_score(MyLGBClassifier(random_state=101,learning_rate=0.01,max_depth=-1,min_data_in_leaf=60,num_iterations=2,num_leaves=5),X,y,cv=5,scoring='f1')
F1Scores
#array([0.84263959, 0.83333333, 0.8       , 0.78787879, 0.87684729])

",cross-validation
scaling and data leakage on cross validation and test set,"I have more of a best practice question.
I am scaling my data and I understand that I should fit_transform on my training set and transform on my test set because of potential data leakage.
Now if I want to use both (5 fold) Cross validation on my training set but I use a holdout test set anyway is it necessary to scale each fold independently?
My problem is that I want to use Feature Selection like this:
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler
from mlxtend.feature_selection import ExhaustiveFeatureSelector as EFS

scaler = MinMaxScaler()
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

efs = EFS(clf_tmp, 
        min_features=min,
        max_features=max,
        cv=5,
        n_jobs = n_jobs)

efs = efs.fit(X_train, y_train)

Right now I am scaling X_train and X_test independently. But when the whole training set goes into the feature selector there will be some data leakage. Is this a problem for evaluation?
","It's definitely best practice to include everything within your cross-validation loop to avoid data leakage. Any scaling should be done on the training set and then applied to the test set within each CV loop.
",cross-validation
how to perform stratifiedgroupkfold based on id that should not be part of training,"I am trying to perform logistic regression using StratifiedGroupKFold as shown in the following code.
grid={'C':np.logspace(-3,3,7)}
grkf_cv = StratifiedGroupKFold(n_splits=10)
id_ls = X_train_df['ID'].to_list()  

log_reg = LogisticRegression(max_iter=100, random_state=42)
logreg_cv = GridSearchCV(log_reg, grid, cv=grkf_cv, scoring='roc_auc')
logreg_cv.fit(X_train_df, y_train_df, groups=id_ls)

This causes a conflict as the model is training with the group ID which is incorrect and it appears as a feature. My issue is I need to pass id_ls with X_train_df (which contains the ID). I am not sure how splits would be performed if X_train_df did not contain the ID.
I can drop the ID from X_train_df and then train but I do not think the splits would be performed based on groups.
Is there a way around this problem.
","In the example in sklearn documentation (found here), you can see that they define the groups parameter separately, without it ever being a part of the training dataset.
I am assuming this is because the groups parameter does not have to be checked against a column, as it already contains the group label for each sample in order.
It makes sense, the function knows that the first row of X_train has group id the first element of id_ls (which you are passing at the groups parameter), the second row is matched to the second element of the list etc.
",cross-validation
python  lightgbm with gridsearchcv is running forever,"Recently, I am doing multiple experiments to compare Python XgBoost and LightGBM. It seems that this LightGBM is a new algorithm that people say it works better than XGBoost in both speed and accuracy.
This is LightGBM GitHub.
This is LightGBM python API documents, here you will find python functions you can call. It can be directly called from LightGBM model and also can be called by LightGBM scikit-learn.
This is the XGBoost Python API I use. As you can see, it has very similar data structure as LightGBM python API above.
Here are what I tried:

If you use train() method in both XGBoost and LightGBM, yes lightGBM works faster and has higher accuracy. But this method, doesn't have cross validation.
If you try cv() method in both algorithms, it is for cross validation. However, I didn't find a way to use it return a set of optimum parameters.
if you try scikit-learn GridSearchCV() with LGBMClassifier and XGBClassifer. It works for XGBClassifer, but for LGBClassifier, it is running forever.

Here are my code examples when using GridSearchCV() with both classifiers:
XGBClassifier with GridSearchCV
param_set = {
 'n_estimators':[50, 100, 500, 1000]
}
gsearch = GridSearchCV(estimator = XGBClassifier( learning_rate =0.1, 
n_estimators=100, max_depth=5,
min_child_weight=1, gamma=0, subsample=0.8, colsample_bytree=0.8, 
nthread=7,
objective= 'binary:logistic', scale_pos_weight=1, seed=410), 
param_grid = param_set, scoring='roc_auc',n_jobs=7,iid=False, cv=10)

xgb_model2 = gsearch.fit(features_train, label_train)
xgb_model2.grid_scores_, xgb_model2.best_params_, xgb_model2.best_score_

This works very well for XGBoost, and only tool a few seconds.
LightGBM with GridSearchCV
param_set = {
 'n_estimators':[20, 50]
}

gsearch = GridSearchCV(estimator = LGBMClassifier( boosting_type='gbdt', num_leaves=30, max_depth=5, learning_rate=0.1, n_estimators=50, max_bin=225, 
 subsample_for_bin=0.8, objective=None, min_split_gain=0, 
 min_child_weight=5, 
 min_child_samples=10, subsample=1, subsample_freq=1, 
colsample_bytree=1, 
reg_alpha=1, reg_lambda=0, seed=410, nthread=7, silent=True), 
param_grid = param_set, scoring='roc_auc',n_jobs=7,iid=False, cv=10)

lgb_model2 = gsearch.fit(features_train, label_train)
lgb_model2.grid_scores_, lgb_model2.best_params_, lgb_model2.best_score_

However, by using this method for LightGBM, it has been running the whole morning today still nothing generated.
I am using the same dataset, a dataset contains 30000 records.
I have 2 questions:

If we just use cv() method, is there anyway to tune optimum set of parameters?
Do you know why GridSearchCV() does not work well with LightGBM? I'm wondering whether this only happens on me all it happened on others to?

","Try to use n_jobs = 1 and see if it works.
In general, if you use n_jobs = -1 or n_jobs > 1 then you should protect your script by using if __name__=='__main__': :
Simple Example:
import ...

if __name__=='__main__':

    data= pd.read_csv('Prior Decompo2.csv', header=None)
    X, y = data.iloc[0:, 0:26].values, data.iloc[0:,26].values
    param_grid = {'C' : [0.01, 0.1, 1, 10], 'kernel': ('rbf', 'linear')}
    classifier = SVC()
    grid_search = GridSearchCV(estimator=classifier, param_grid=param_grid, scoring='accuracy', n_jobs=-1, verbose=42)
    grid_search.fit(X,y)

Finally, can you try to run your code using  n_jobs = -1 and including if __name__=='__main__': as I explained and see if it works?
",cross-validation
saving a crossvalidation trained model in scikit,"I have trained a model in scikit-learn using Cross-Validation and Naive Bayes classifier. How can I persist this model to later run against new instances?
Here is simply what I have, I can get the CV scores but I don't know how to have access to the trained model
gnb = GaussianNB() 
scores = cross_validation.cross_val_score(gnb, data_numpy[0],data_numpy[1], cv=10)

","cross_val_score doesn't changes your estimator, and it will not return fitted estimator. It just returns score of estimator of cross validation.
To fit your estimator - you should call fit on it explicitly with provided dataset.
To save (serialize) it - you can use pickle:
# To fit your estimator
gnb.fit(data_numpy[0], data_numpy[1])
# To serialize
import pickle
with open('our_estimator.pkl', 'wb') as fid:
    pickle.dump(gnb, fid)
# To deserialize estimator later
with open('our_estimator.pkl', 'rb') as fid:
    gnb = pickle.load(fid)

",cross-validation
return coefficients from pipeline object in sklearn,"I've fit a Pipeline object with RandomizedSearchCV
pipe_sgd = Pipeline([('scl', StandardScaler()),
                    ('clf', SGDClassifier(n_jobs=-1))])

param_dist_sgd = {'clf__loss': ['log'],
                 'clf__penalty': [None, 'l1', 'l2', 'elasticnet'],
                 'clf__alpha': np.linspace(0.15, 0.35),
                 'clf__n_iter': [3, 5, 7]}

sgd_randomized_pipe = RandomizedSearchCV(estimator = pipe_sgd, 
                                         param_distributions=param_dist_sgd, 
                                         cv=3, n_iter=30, n_jobs=-1)

sgd_randomized_pipe.fit(X_train, y_train)

I want to access the coef_ attribute of the best_estimator_ but I'm unable to do that. I've tried accessing coef_ with the code below.
sgd_randomized_pipe.best_estimator_.coef_
However I get the following AttributeError... 
AttributeError: 'Pipeline' object has no attribute 'coef_'
The scikit-learn docs say that coef_ is an attribute of SGDClassifier, which is the class of my base_estimator_. 
What am I doing wrong?
","You can always use the names you assigned to them while making the pipeline by using the named_steps dict.
scaler = sgd_randomized_pipe.best_estimator_.named_steps['scl']
classifier = sgd_randomized_pipe.best_estimator_.named_steps['clf']

and then access all the attributes like coef_, intercept_ etc. which are available to corresponding fitted estimator.
This is the formal attribute exposed by the Pipeline as specified in the documentation:

named_steps : dict
Read-only attribute to access any step parameter by user given name. Keys are step names and values are steps parameters.

",cross-validation
how to correctly evaluate model performance of a regressor in gridsearchcv sklearn,"I'm trying to found a set of best hyperparameters for my Gradient Boosting Regressor with Grid Search CV. But I have difficulties getting the performance of the best model.
My code is as follows, this function is expected to return an optimized model.
def parameter_tuning_Gradient_Boost(X,
                                    y,
                                    ):

    model = GradientBoostingRegressor()

    param_grid = {  'learning_rate': [0.005, 0.01, 0.02, 0.05, 0.1],
                    'subsample'    : [1.0, 0.8, 0.6],
                    'n_estimators' : [100, 200, 500, 1000],
                    'max_depth'    : [2, 4, 6, 8, 10]
                    }

    grid_search = GridSearchCV( model,
                                param_grid,
                                cv = 5,
                                n_jobs = 8,
                                verbose = 0)

    grid_search.fit(X = X,
                    y = y,)

    print('Best Parameters by Searching: %s' % grid_search.best_params_)

    best_parameters = grid_search.best_estimator_.get_params()

    model = GradientBoostingRegressor(  learning_rate = best_parameters['learning_rate'],
                                        subsample = best_parameters['subsample'],
                                        n_estimators = best_parameters['n_estimators'],
                                        max_depth = best_parameters['max_depth'],
                                        )
    
    return model


In general, I have the following questions：

Do I have to use train_test_split function to split X and y, and then feed X_train and y_train to grid_search.fit function? Some said GridSearchCV will automatically split data into train and test if you set cv = 5. But I saw some online tutorial will do something like this:

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)
grid_search.fit(X_train, y_train)


What is the metrics score for a regressor in GridSearchCV? after fitting GridSearchCV, I run the follwing commands and get way different score. I am wondering what is the correct way to get model cv perforamce for a regressor in GridSearchCV sklearn.

print(""Best Score:"", grid_search.score(X, y))
print(""Best Score: %.3f"" % grid_search.best_score_)

I tried to apply GridSearchCV method to perform parameter tuning for a regressor and get its cross-validation performance, I want to know what is the default evaluation metrics here, and I want to know do I have to split the data into train and test set when I set cv parameter in GridSearchCV.
","
Yes GridSearchCV will split the data into 5 train/test splits. It will then use these splits to find the optimal hyperparameters. However, it's also good practice to set aside a completely unseen split of the data. That you score the model(s) on when you are completely done with training. Take a look at this article to read more on this. Remember, after evaluating the model on the unseen data set. You are not ""allowed"" to improve your model.

The scoring metric for GridSearchCV can be user defined. But as a standard it will use the scoring parameter from the estimator. In this case squared_error which is the default scoring for GradientBoostingRegressor, the parameter is named loss=squared_error.


To answer the second part of the question, we need to understand what happens in GridSearchCV when fitting. When the optimal hyperparameters are found, a model is refitted on all the data with the optimal hyperparameters, if refit=True, which it is by default.
grid_search.score(X,y) scores the refitted model on all of the data while
grid_search._best_score returns the average score of the models with the optimal hyperparameters on the five splits.
",cross-validation
combination of calibratedclassifiercv with randomoversampler,"When using a classifier like GaussianNB(), the resulting .predict_proba() values are sometimes poorly calibrated; that's why I'd like to wrap this classifier into sklearn's CalibratedClassifierCV.
I have now a binary classification problem with only a very few positive samples - so few that CalibratedClassifierCV fails because there are less samples than folds (the resulting error is then Requesting 5-fold cross-validation but provided less than 5 examples for at least one class.). Thus, I'd like to upsample the minority class before applying the classifier. I use imblearn's pipeline for this as it ensures that resampling takes place only during fit and not during inference.
However, I do not find a way to upsample my training data and combine it with CalibratedClassifierCV while ensuring that upsampling only takes place during fit and not during inference.
I tried the following reproducible example, but it seems that CalibratedClassifierCV wants to split the data first, prior to upsampling - and it fails.
Is there a way to correctly upsample data while using CalibratedClassifierCV?
from sklearn.calibration import CalibratedClassifierCV
from sklearn.naive_bayes import GaussianNB
from sklearn.datasets import make_classification
from sklearn.model_selection import train_test_split
from imblearn.over_sampling import RandomOverSampler
from imblearn.pipeline import Pipeline

X, y = make_classification(
    n_samples = 100,
    n_features = 10,
    n_classes = 2,
    weights = (0.95,), # 5% of samples are of class 1
    random_state = 10,
    shuffle = True
)

X_train, X_val, y_train, y_val = train_test_split(
    X,
    y,
    test_size = 0.2,
    random_state = 10,
    shuffle = True,
    stratify = y
)

pipeline = Pipeline([
    (""resampling"", RandomOverSampler(
        sampling_strategy=0.2,
        random_state=10
    )),
    (""model"", GaussianNB())
])

m = CalibratedClassifierCV(
    base_estimator=pipeline,
    method=""isotonic"",
    cv=5,
    n_jobs=-1
)

m.fit(X_train, y_train) # results in error

","I guess I understand my conceptual error: the cross-validation split has to happen BEFORE upsampling and not after (otherwise there would be information leakage from validation to training). But if it happens before, I cannot have more folds than samples of the positive class... Thus, oversampling does not save me from having not enough samples for CalibratedClassifierCV.
So I indeed have to reduce the number of folds, as @NMH1013 suggests.
",cross-validation
getting quottypeerror fit missing 1 required positional argument 39y39quot for linear regression in python,"What I'm trying to do:

Fit a linear regression model on data from PCA transformation
Use that linear regression model to perform cross-validation

ks = [1,2,3,4,5,6,8,10,12,15,20]
mean_val_mse = []
# loop through all k values
for k in ks:
    # instantiate pca and set n_components = k
    pca = PCA(n_components=k)
    # fit and transform
    # fit learns the number of components, transform actually reduces the dimensions
    x_train_pca = pca.fit_transform(X_train_scaled)
    
    # create linreg and fit it on pca data
    lr = LinearRegression().fit(x_train_pca, y_train)
    cv = cross_validate(lr, X=x_train_pca, cv=10, return_train_score=True, scoring='neg_mean_squared_error', error_score='raise')

When I do so, my cross-validation is throwing the error:
TypeError: fit() missing 1 required positional argument: 'y'
from line: cv = cross_validate(lr, X=x_train_pca, cv=10, return_train_score=True, scoring='neg_mean_squared_error', error_score='raise')

I've checked that my y_train is the correct shape and run through the syntax but I'm not seeing a reason why it would give this error.  Am I instantiating the LinearRegression wrong or is the error somewhere else?
","I have never dealt with Machine Learning, but the error message says you didn't supply y into that last line, so maybe try
cv = cross_validate(lr, X=x_train_pca, y=y_train, cv=10, return_train_score=True, scoring='neg_mean_squared_error', error_score='raise')

source:
https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_validate.html#sklearn.model_selection.cross_validate
",cross-validation
split time series with multiple records per day,"I have a dataset of daily transactions where there are multiple records per day. I need to split it into different cross validation folds to train a ML model, however I can't user TimeSeriesSplit from sklearn as there are multiple transactions per day. Do you know how can I do this in python?
","Input data:
import numpy as np
import pandas as pd
data = np.array(
    [['DAY_1','afds',5],
     ['DAY_1','rtws', 4],
     ['DAY_1','gtssd', 2],
     ['DAY_2','ititl', 4],
     ['DAY_2','uius', 7],
     ['DAY_3','hyaah', 6],
     ['DAY_4','apsaj', 9]])
df = pd.DataFrame(data,columns=['DATEDAY','TRANSACTION_ID','PRICE'])

Resulting df:
        TRANSACTION_ID PRICE
DATEDAY
DAY_1             afds     5
DAY_1             rtws     4
DAY_1            gtssd     2
DAY_2            ititl     4
DAY_2             uius     7
DAY_3            hyaah     6
DAY_4            apsaj     9

Solution:
from sklearn.model_selection import TimeSeriesSplit

df = df.set_index('DATEDAY')
days = np.sort(df.index.unique())
tscv = TimeSeriesSplit(2)
for train_index, test_index in tscv.split(days):
    print ('------------------------------')
    train_days, test_days = days[train_index], days[test_index]
    X_train, X_test = df.loc[train_days], df.loc[test_days]
    print ('train:', X_train, '\n')
    print ('test:', X_test, '\n')

Output:
------------------------------
train:         TRANSACTION_ID PRICE
DATEDAY
DAY_1             afds     5
DAY_1             rtws     4
DAY_1            gtssd     2
DAY_2            ititl     4
DAY_2             uius     7

test:         TRANSACTION_ID PRICE
DATEDAY
DAY_3            hyaah     6

------------------------------
train:         TRANSACTION_ID PRICE
DATEDAY
DAY_1             afds     5
DAY_1             rtws     4
DAY_1            gtssd     2
DAY_2            ititl     4
DAY_2             uius     7
DAY_3            hyaah     6

test:         TRANSACTION_ID PRICE
DATEDAY
DAY_4            apsaj     9


Note 1: we assume that the date column can be sorted. In this example, DAY_X doesn't sort well, since DAY_11 would be placed before DAY_2, for instance. If we only know the number X of the day, then we need to put X in the column, instead of DAY_X, e.g., we might do something like:

df['DATEDAY'] = [int(x.split('_')[1]) for x in df['DATEDAY']]


Note 2: if we want to avoid having DATEDAY as index of the dataframe, we can simply reset the index for X_train and X_test:

for train_index, test_index in tscv.split(days):
    print ('------------------------------')
    train_days, test_days = days[train_index], days[test_index]
    X_train, X_test = df.loc[train_days].reset_index(), df.loc[test_days].reset_index()
    print ('train:\n', X_train, '\n')
    print ('test:\n', X_test, '\n')

Output:
------------------------------
train:
   DATEDAY TRANSACTION_ID PRICE
0   DAY_1           afds     5
1   DAY_1           rtws     4
2   DAY_1          gtssd     2
3   DAY_2          ititl     4
4   DAY_2           uius     7

test:
   DATEDAY TRANSACTION_ID PRICE
0   DAY_3          hyaah     6

------------------------------
train:
   DATEDAY TRANSACTION_ID PRICE
0   DAY_1           afds     5
1   DAY_1           rtws     4
2   DAY_1          gtssd     2
3   DAY_2          ititl     4
4   DAY_2           uius     7
5   DAY_3          hyaah     6

test:
   DATEDAY TRANSACTION_ID PRICE
0   DAY_4          apsaj     9

",cross-validation
model wrapper for sklearn cross_val_score,"This is an minimal example using XGBClassifier, but am interested how this would work in general. I am trying to wrap the model class in order to use it in cross validation. In this case I am only weighing the imbalanced classes, but my ultimate goal is a bit broader change in the pipeline.
My first try was to simply override the fit function:
from sklearn import metrics
from sklearn.utils.class_weight import compute_sample_weight
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import RepeatedStratifiedKFold
from sklearn.model_selection import train_test_split
from xgboost import XGBClassifier
from sklearn.base import BaseEstimator, ClassifierMixin

class WeightedXGBClassifier(XGBClassifier, BaseEstimator, ClassifierMixin):
    
    @staticmethod
    def get_weights(y):
        sample_weights = compute_sample_weight(class_weight='balanced', y=y)
        return sample_weights
    
    def fit(self, X, y, **kwargs):
        weights = self.get_weights(y)
        super(XGBClassifier, self).fit(X, y, sample_weight=weights, **kwargs)

which works fine, when I'm trying to fit the model, use predictions etc.. But using this in sklearn cross_val_score
xgb_model_cv = WeightedXGBClassifier(n_estimators=100, max_depth=4, alpha=100, use_label_encoder=False)

cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)
auc_scorer = metrics.make_scorer(metrics.roc_auc_score, needs_proba=True)
scores = cross_val_score(xgb_model_cv, X, y, scoring=auc_scorer, cv=cv, n_jobs=-1, verbose=1)

throws an error
File ""/home/ubuntu/anaconda3/envs/pyTF/lib/python3.9/site-packages/sklearn/model_selection/_validation.py"", line 767, in _score
    scores = scorer(estimator, X_test, y_test)
  File ""/home/ubuntu/anaconda3/envs/pyTF/lib/python3.9/site-packages/sklearn/metrics/_scorer.py"", line 106, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File ""/home/ubuntu/anaconda3/envs/pyTF/lib/python3.9/site-packages/sklearn/metrics/_scorer.py"", line 306, in _score
    y_pred = self._select_proba_binary(y_pred, clf.classes_)
AttributeError: 'WeightedXGBClassifier' object has no attribute 'classes_'

Now, it is my understanding the classes_ attribute is created, when the model is fitted, but I am not sure how to then properly wrap the model to capture this. Note that running
model = XGBClassifier(use_label_encoder=False, scale_pos_weight=(~y).sum()/y.sum())
cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)
scores = cross_val_score(model, X, y, scoring='roc_auc', cv=cv, n_jobs=-1)

works fine. My second try was:
class XGBClassifierWrapper(BaseEstimator, ClassifierMixin):
    def __init__(self, **kwargs):
#         super(BaseEstimator).__init__()
#         super(ClassifierMixin).__init__()
        self.xgb_classifier_obj = XGBClassifier(**kwargs)
    
    @staticmethod
    def get_weights(y):
        sample_weights = compute_sample_weight(class_weight='balanced', y=y)
        return sample_weights
    
    def fit(self, X, y, **kwargs):
        weights = self.get_weights(y)
        self.xgb_classifier_obj.fit(X, y, sample_weight=weights, **kwargs)
        return self
    
    def predict(self, X, **kwargs):
        return self.xgb_classifier_obj.predict(X, **kwargs)
    
    def predict_proba(self, X, **kwargs):
        return self.xgb_classifier_obj.predict_proba(X, **kwargs)

which again resulted in the same error as in the case above, i.e., missing classes_ attribute.
","(I don't actually get an error when I run any of your code; however, I do get a scores consisting only of nan, and adding error_score='raise' I get your error message.)
In the first approach, I believe the only real problem is in your initialization. super(XGBClassifier, self): that's looking for a parent class of XGBClassifier, and not XGBClassifier itself, as I assume you want.  Replacing with just the vanilla super() and everything works.
You should also add return self to the end of fit in your first attempt, but it's not important here.  You can probably safely drop BaseEstimator and ClassifierMixin from the inheritance, since XGBClassifier already inherits from them.
Your second, wrapper, approach just fails because the wrapped xgb_classifier_obj has all the fitted attributes, including classes_, but your wrapper doesn't expose that directly.  You can just set self.classes_ = self.xgb_classifier_obj.classes_ in fit, or perhaps define a @property delegation.
You should also consider that your __init__ this time doesn't meet the sklearn API, so cloning won't work correctly.  I'd advise using the first approach for this reason (fixing it requires rather more tedious work, in my opinion).
",cross-validation
is it necessary to use crossvalidation after data is split using stratifiedshufflesplit,"I used StratifiedShuffleSplit to split the data and now I am wondering if I need to use cross-validation again as I go for building the classification model(Logistic Regression,KNN,Random Forest etc.) I am confused about it because reading the documentation in Sklearn I get the impression that StratifiedShuffleSplit is a mix of splitting the data and cross-validating it at the same time.
","StratifiedShuffleSplit provides you just a list with train/test indices. How it will be used depends on you.

You can fit the model with train set and predict on the test and calculate the score manually - so implementing cross validation by yourself
Or you can use cross_val_score and pass StratifiedShuffleSplit() to it and cross_val_score will do the same thing.

Example:
import numpy as np
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score
from sklearn.model_selection import StratifiedShuffleSplit, cross_val_score

X = np.array([[1, 2], [3, 4], [1, 2], [3, 4], [1, 2], [3, 4]])
y = np.array([0, 0, 0, 1, 1, 1])

sss = StratifiedShuffleSplit(n_splits=5, test_size=0.5, random_state=0)
model = RandomForestClassifier(n_estimators=1, random_state=1)

# Calculate scores automatically
accuracy_per_split = cross_val_score(model, X, y, scoring=""accuracy"", cv=sss, n_jobs=1)
print(f""Accuracies per splits: {accuracy_per_split}"")

# Calculate scores manually
accuracy_per_split = []
for train_index, test_index in sss.split(X, y):
    X_train, X_test = X[train_index], X[test_index]
    y_train, y_test = y[train_index], y[test_index]

    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)

    acc = accuracy_score(y_test, y_pred)
    accuracy_per_split.append(acc)

print(f""Accuracies per splits: {accuracy_per_split}"")


",cross-validation
using gridsearchcv for randomforestregressor,"I'm trying to use GridSearchCV for RandomForestRegressor, but always get ValueError: Found array with dim 100. Expected 500. Consider this toy example:
import numpy as np

from sklearn import ensemble
from sklearn.cross_validation import train_test_split
from sklearn.grid_search import GridSearchCV
from sklearn.metrics import r2_score

if __name__ == '__main__':

    X = np.random.rand(1000, 2)
    y = np.random.rand(1000)

    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.5, random_state=1)

    # Set the parameters by cross-validation
    tuned_parameters = {'n_estimators': [500, 700, 1000], 'max_depth': [None, 1, 2, 3], 'min_samples_split': [1, 2, 3]}

    # clf = ensemble.RandomForestRegressor(n_estimators=500, n_jobs=1, verbose=1)
    clf = GridSearchCV(ensemble.RandomForestRegressor(), tuned_parameters, cv=5, scoring=r2_score, n_jobs=-1, verbose=1)
    clf.fit(X_train, y_train)
    print clf.best_estimator_

This is what I get:
Fitting 5 folds for each of 36 candidates, totalling 180 fits
Traceback (most recent call last):
  File ""C:\Users\abudis\Dropbox\machine_learning\toy_example.py"", line 21, in <module>
    clf.fit(X_train, y_train)
  File ""C:\Users\abudis\AppData\Local\Enthought\Canopy\User\lib\site-packages\sklearn\grid_search.py"", line 596, in fit
    return self._fit(X, y, ParameterGrid(self.param_grid))
  File ""C:\Users\abudis\AppData\Local\Enthought\Canopy\User\lib\site-packages\sklearn\grid_search.py"", line 378, in _fit
    for parameters in parameter_iterable
  File ""C:\Users\abudis\AppData\Local\Enthought\Canopy\User\lib\site-packages\sklearn\externals\joblib\parallel.py"", line 653, in __call__
    self.dispatch(function, args, kwargs)
  File ""C:\Users\abudis\AppData\Local\Enthought\Canopy\User\lib\site-packages\sklearn\externals\joblib\parallel.py"", line 400, in dispatch
    job = ImmediateApply(func, args, kwargs)
  File ""C:\Users\abudis\AppData\Local\Enthought\Canopy\User\lib\site-packages\sklearn\externals\joblib\parallel.py"", line 138, in __init__
    self.results = func(*args, **kwargs)
  File ""C:\Users\abudis\AppData\Local\Enthought\Canopy\User\lib\site-packages\sklearn\cross_validation.py"", line 1240, in _fit_and_score
    test_score = _score(estimator, X_test, y_test, scorer)
  File ""C:\Users\abudis\AppData\Local\Enthought\Canopy\User\lib\site-packages\sklearn\cross_validation.py"", line 1296, in _score
    score = scorer(estimator, X_test, y_test)
  File ""C:\Users\abudis\AppData\Local\Enthought\Canopy\User\lib\site-packages\sklearn\metrics\metrics.py"", line 2324, in r2_score
    y_type, y_true, y_pred = _check_reg_targets(y_true, y_pred)
  File ""C:\Users\abudis\AppData\Local\Enthought\Canopy\User\lib\site-packages\sklearn\metrics\metrics.py"", line 65, in _check_reg_targets
    y_true, y_pred = check_arrays(y_true, y_pred)
  File ""C:\Users\abudis\AppData\Local\Enthought\Canopy\User\lib\site-packages\sklearn\utils\validation.py"", line 254, in check_arrays
    % (size, n_samples))
ValueError: Found array with dim 100. Expected 500

For some reason GridSearchCV thinks that n_estimators parameter should be equal to the size of each fold. If I change the first value of the n_estimators in the tuned_parameters list I get ValueError with another expected value.
Training just one model using clf = ensemble.RandomForestRegressor(n_estimators=500, n_jobs=1, verbose=1) works fine though, so not sure if I'm doing something wrong or there's a bug in scikit-learn somewhere.
","Looks like a bug, but in your case it should work if you use RandomForestRegressor's own scorer (which coincidentally is R^2 score) by not specifying any scoring function in GridSearchCV:
clf = GridSearchCV(ensemble.RandomForestRegressor(), tuned_parameters, cv=5, 
                   n_jobs=-1, verbose=1)

EDIT: As mentioned by @jnothman in #4081 this is the real problem:

scoring does not accept a metric function. It accepts a function of signature (estimator, > X, y_true=None) -> float score. You can use scoring='r2' or scoring=make_scorer(r2_score).

",cross-validation
split tensor into training and test sets,"Let's say I've read in a textfile using a TextLineReader. Is there some way to split this into train and test sets in Tensorflow? Something like:
def read_my_file_format(filename_queue):
  reader = tf.TextLineReader()
  key, record_string = reader.read(filename_queue)
  raw_features, label = tf.decode_csv(record_string)
  features = some_processing(raw_features)
  features_train, labels_train, features_test, labels_test = tf.train_split(features,
                                                                            labels,
                                                                            frac=.1)
  return features_train, labels_train, features_test, labels_test

","Something like the following should work:
tf.split_v(tf.random_shuffle(...
Edit: For tensorflow>0.12 This should now be called as tf.split(tf.random.shuffle(...
Reference
See docs for tf.split and for tf.random.shuffle for examples.
",cross-validation
understanding xgboost cross validation and auc output results,"I have the following XGBoost C.V. model.
xgboostModelCV <- xgb.cv(data =  dtrain, 
                             nrounds = 20, 
                             nfold = 3, 
                             metrics = ""auc"", 
                             verbose = TRUE, 
                             ""eval_metric"" = ""auc"",
                             ""objective"" = ""binary:logistic"", 
                             ""max.depth"" = 6, 
                             ""eta"" = 0.01,                               
                             ""subsample"" = 0.5, 
                             ""colsample_bytree"" = 1,
                             print_every_n = 1, 
                             ""min_child_weight"" = 1,
                             booster = ""gbtree"",
                             early_stopping_rounds = 10,
                             watchlist = watchlist,
                             seed = 1234)

My question is regarding the output and nfold of the model, I set nfold to 3
The output of the evaluation log looks as follows;
   iter train_auc_mean train_auc_std test_auc_mean test_auc_std
1     1      0.8852290  0.0023585703     0.8598630  0.005515424
2     2      0.9015413  0.0018569007     0.8792137  0.003765109
3     3      0.9081027  0.0014307577     0.8859040  0.005053600
4     4      0.9108463  0.0011838160     0.8883130  0.004324113
5     5      0.9130350  0.0008863908     0.8904100  0.004173123
6     6      0.9143187  0.0009514359     0.8910723  0.004372844
7     7      0.9151723  0.0010543653     0.8917300  0.003905284
8     8      0.9162787  0.0010344935     0.8929013  0.003582747
9     9      0.9173673  0.0010539116     0.8935753  0.003431949
10   10      0.9178743  0.0011498505     0.8942567  0.002955511
11   11      0.9182133  0.0010825702     0.8944377  0.003051411
12   12      0.9185767  0.0011846632     0.8946267  0.003026969
13   13      0.9186653  0.0013352629     0.8948340  0.002526793
14   14      0.9190500  0.0012537195     0.8954053  0.002636388
15   15      0.9192453  0.0010967155     0.8954127  0.002841402
16   16      0.9194953  0.0009818501     0.8956447  0.002783787
17   17      0.9198503  0.0009541517     0.8956400  0.002590862
18   18      0.9200363  0.0009890185     0.8957223  0.002580398
19   19      0.9201687  0.0010323405     0.8958790  0.002508695
20   20      0.9204030  0.0009725742     0.8960677  0.002581329

However I set nrounds = 20 but cross validation nfolds = 3 so should I have an output of 60 results and not 20? 
Or is the above output just as the column names suggest, the mean score of the AUC at each round...
So at nround = 1 for the training set the train_auc_mean is the result 0.8852290 which would be the average of the 3 cross validation nfolds?
So if I plot these AUC scores then I would be plotting the average AUC score over the 3 fold cross validation?
Just want to make sure everything is clear.
","You are correct that the output is the average of the fold auc. However if you wish to extract the individual fold auc for the best/last iteration you can proceed as follows: 
An example using the Sonar data set from mlbench
library(xgboost)
library(tidyverse)
library(mlbench)

data(Sonar)

xgb.train.data <- xgb.DMatrix(as.matrix(Sonar[,1:60]), label = as.numeric(Sonar$Class)-1)
param <- list(objective = ""binary:logistic"")

in xgb.cv set prediction = TRUE
model.cv <- xgb.cv(param = param,
                   data = xgb.train.data,
                   nrounds = 50,
                   early_stopping_rounds = 10,
                   nfold = 3,
                   prediction = TRUE,
                   eval_metric = ""auc"")

now go over the folds and connect the predictions with the true lables and corresponding indexes:
z <- lapply(model.cv$folds, function(x){
  pred <- model.cv$pred[x]
  true <- (as.numeric(Sonar$Class)-1)[x]
  index <- x
  out <- data.frame(pred, true, index)
  out
})

give the folds names:
names(z) <- paste(""folds"", 1:3, sep = ""_"")

z %>%
  bind_rows(.id = ""id"") %>%
  group_by(id) %>%
  summarise(auroc = roc(true, pred) %>%
           auc())
#output
# A tibble: 3 x 2
  id      auroc
  <chr>   <dbl>
1 folds_1 0.944
2 folds_2 0.900
3 folds_3 0.899

the mean of these values is the same as the mean auc at best iteration:
z %>%
  bind_rows(.id = ""id"") %>%
  group_by(id) %>%
  summarise(auroc = roc(true, pred) %>%
           auc()) %>%
  pull(auroc) %>%
  mean
#output
[1] 0.9143798

model.cv$evaluation_log[model.cv$best_iteration,]
#output
   iter train_auc_mean train_auc_std test_auc_mean test_auc_std
1:   48              1             0       0.91438   0.02092817

You can of course do much more like plot auc curves for each fold and so on. 
",cross-validation
why does shuffling training data for cross validation increase performance,"I am working on unbalanced dataset and I noticed that strangely if I shuffle the data during cross validation I get a high value of the f1 score while if i do not shuffle it f1 is low. Here is the function I use for cross validation:
def train_cross_v(md,df_train,n_folds=5,shuffl=False):

        X,y=df_train.drop([variable],axis=1),df_train[variable]
    
        cv =StratifiedKFold(n_splits=n_folds,shuffle=shuffl)

        scores = cross_val_score(md,X,y, scoring='f1', cv=cv, n_jobs=-1)

        y_pred=cross_val_predict(md,X,y, cv=cv, n_jobs=-1)
        print(' f1: ',scores,np.mean(scores))
        print(confusion_matrix(y_pred,y))
        return np.mean(scores)

Now shuffling I get f1 around 0.82:
nfolds=5
train_cross_v(XGBClassifier(),df_train,n_folds=nfolds,shuffl=True)
f1:  [0.81469793 0.82076749 0.82726257 0.82379249 0.82484862] 0.8222738195197493
[[23677  2452]
[ 1520  9126]]
0.8222738195197493

While not shuffling leads to:
nfolds=5
train_cross_v(XGBClassifier(),df_train,n_folds=nfolds,shuffl=False) 

f1:  [0.67447073 0.55084022 0.4166443  0.52759421 0.64819164] 0.5635482198057791
[[21621  5624]
[ 3576  5954]]
0.5635482198057791

As I understand it, shuffling is preferred to assess the real performance of the model as it allows us to neglect any dependencies related to the ordering of the data, and usually the post shuffling value of the performance metric is lower than that without shuffling. In my case however the behavior is the exact opposite and I get a high value if I shuffle, and the values ​​of the predictions on the test set remain unchanged. What could be the problem here?
","Because the order of your data is important.
Let's consider the following example:

Suppose we have completely balanced labels:

[0, 1, 0, 1, 0, 1, 0, 1, 0, ...]

And the features matrix that matches the labels, i.e:

[
[0, 1, 0, 1, ..],
]


Suppose the first 25% of the data are noisy and have incorrect labels:

n_noisy = int(n_examples * 0.25)
X[:n_noisy] = 1 - X[:n_noisy]

So we have:
[25% noisy, 25% normal, 25% normal, 25% normal]

Now we are using 2-fold cross validation (2 for simplicity).

4.1 without shuffling we will have the following metrics:
 f1:  [0.5 0. ] 0.25  # metrics for the second fold is zero

The first fold will be trained on the second half of data ([25% normal, 25% normal]) which have no noise in it and tested on the first half ([25%noisy, 25% normal]) which have 50% of noise in it which results in f1=0.5.
The second fold will be trained on the first half of data which were inverted as a result f1=0
4.2 with shuffling:
 f1:  [0.74903475 0.75103734] 0.7500360467165447

As expected we have f1=75% because 25% are noise.
Source code:
from xgboost import XGBClassifier
import numpy as np
from sklearn.model_selection import StratifiedKFold, cross_val_score, cross_val_predict
from sklearn.metrics import confusion_matrix


def train_cross_v(md, X, y, n_folds=5, shuffl=False):

    cv = StratifiedKFold(n_splits=n_folds, shuffle=shuffl)

    scores = cross_val_score(md, X, y, scoring=""f1"", cv=cv, n_jobs=-1)

    y_pred = cross_val_predict(md, X, y, cv=cv, n_jobs=-1)
    print("" f1: "", scores, np.mean(scores))
    print(confusion_matrix(y_pred, y))
    return np.mean(scores)


nfolds = 2
n_examples = 1000

y = np.tile([0, 1], 500)
X = y.copy().reshape(-1, 1)

n_noisy = int(n_examples * 0.25)
X[:n_noisy] = 1 - X[:n_noisy]


train_cross_v(XGBClassifier(), X, y, n_folds=nfolds, shuffl=False)
train_cross_v(XGBClassifier(), X, y, n_folds=nfolds, shuffl=True)

So the order matters and shuffling can both increase or decrease performance.
",cross-validation
apply a cross validated ml model to unseen data,"I would like to use scikit learn to predict with X a variable y. I would like to train a classifier on a training dataset using cross validation and then to apply this classifier to an unseen test dataset (as in https://www.nature.com/articles/s41586-022-04492-9)
from sklearn import datasets
from sklearn.model_selection import cross_validate
from sklearn.model_selection import train_test_split
from sklearn.svm import SVC

# Import dataset
X, y = datasets.load_iris(return_X_y=True)

# Create binary variable y
y[y == 0] = 1

# Divide in train and test set 
x_train, x_test, y_train, y_test = train_test_split(X, y,test_size=75, random_state=4, stratify=y)

# Cross validation on the train data  
cv_model = cross_validate(model, x_train, y_train, cv=5)

Now I would like to use this cross validated model and to apply it to the unseen test set. I am unable to find how.
It would be something like
result = cv_model.score(x_test, y_test)

Except this does not work
","You cannot do that; you need to fit the model before using it to predict new data. cross_validate is just a convenience function to get the scores; as clearly mentioned in the documentation, it returns just that, i.e. scores, and not a (fitted) model:

Evaluate metric(s) by cross-validation and also record fit/score times.
[...]
Returns:   scores : dict of float arrays of shape (n_splits,)
Array of scores of the estimator for each run of the cross validation.
A dict of arrays containing the score/time arrays for each scorer is returned.

",cross-validation
from train test split to cross validation in sklearn using pipeline,"I have the following piece of code:
from sklearn import model_selection
from sklearn.ensemble import RandomForestClassifier
import pandas as pd
from sklearn.pipeline import Pipeline
...
x_train, x_test, y_train, y_test= model_selection.train_test_split(dataframe[features_],dataframe[labels], test_size=0.30,random_state=42, shuffle=True)
classifier = RandomForestClassifier(n_estimators=11)
pipe = Pipeline([('feats', feature), ('clf', classifier)])
pipe.fit(x_train, y_train)
predicts = pipe.predict(x_test)

Instead of train test split, I want to use k-fold cross validation to train my model. However, I do not know how can make it by using pipeline structure. I came across this: https://scikit-learn.org/stable/modules/compose.html but I could not fit to my code.
I want to use from sklearn.model_selection import StratifiedKFold if possible. I can use it without pipeline structure but I can not use it with pipeline.
Update:
I tried this but it generates me error.
x_train = dataframe[features_]
y_train = dataframe[labels]

skf = StratifiedKFold(n_splits=3, shuffle=True, random_state=42) 
classifier = RandomForestClassifier(n_estimators=11)
     
#pipe = Pipeline([('feats', feature), ('clf', classifier)])
#pipe.fit(x_train, y_train)
#predicts = pipe.predict(x_test)

predicts = cross_val_predict(classifier, x_train , y_train , cv=skf)

","Pipeline is used to assemble several steps such as preprocessing, transformations, and modeling. StratifiedKFold is used to split your dataset to assess the performance of your model. It is not meant to be used as a part of the Pipeline as you do not want to perform it on new data.
Therefore it is normal to perform it out of the pipeline's structure.
",cross-validation
xgboost  the least populated class in y has only 1 members which is too few,"Im using Xgboost implementation on sklearn for a kaggle's competition.
However, im getting this 'warning' message :
$ python Script1.py
/home/sky/private/virtualenv15.0.1dev/myVE/local/lib/python2.7/site-packages/sklearn/cross_validation.py:516: 

Warning: The least populated class in y has only 1 members, which is too few. The minimum number of labels for any class cannot be less than n_folds=3.
  % (min_labels, self.n_folds)), Warning)

According to another question on stackoverflow :

Check that you have at least 3 samples per class to be able to do StratifiedKFold cross validation with k == 3 (I think this is the default CV used by GridSearchCV for classification).""

And well, i dont have at least 3 samples per class.
So my questions are:

what are the alternatives?

Why can't i use cross validation?

What can i use instead?


...
param_test1 = {
    'max_depth': range(3, 10, 2),
    'min_child_weight': range(1, 6, 2)
}

grid_search = GridSearchCV(

estimator=
XGBClassifier(
    learning_rate=0.1,
    n_estimators=3000,
    max_depth=15,
    min_child_weight=1,
    gamma=0,
    subsample=0.8,
    colsample_bytree=0.8,
    objective='multi:softmax',
    nthread=42,
    scale_pos_weight=1,
    seed=27),

    param_grid=param_test1, scoring='roc_auc', n_jobs=42, iid=False, cv=None, verbose=1)
...

grid_search.fit(train_x, place_id)

References:
One-shot learning with scikit-learn
Using a support vector classifier with polynomial kernel in scikit-learn
","If you have a target/class with only one sample, thats too few for any model. What you can do is get another dataset, preferably as balanced as possible, since most models behave better in balanced sets.
If you cannot have another dataset, you will have to play with what you have. I would suggest you remove the sample that has the lonely target. So you will have a model which does not cover that target. If that does not fit you requirements, you need a new dataset.
",cross-validation
using kfold cross validation to get mae for each data split,"I want to get mean absolute error (MAE) for each split of data using 5-fold cross validation. I have built a custom model using Xception.
Hence, to try this, I coded the following:
# Data Generators:
train_gen = flow_from_dataframe(core_idg, train_df, 
                                path_col = 'path',
                                y_col = 'boneage_zscore', 
                                target_size = IMG_SIZE,
                                color_mode = 'rgb',
                                batch_size = 32,
                                shuffle = True)

X_train, Y_train = next(train_gen)

#-----------------------------------------------------------------------
# Custom Model initiation:
    
base_model = Xception(input_shape = X_train.shape[1:], include_top = False, weights = 'imagenet')
base_model.trainable = True

model = Sequential()
model.add(base_model)
model.add(GlobalMaxPooling2D())
model.add(Flatten())

model.add(Dense(16, activation = 'relu'))
model.add(Dense(1, activation = 'linear'))

def mae_months(in_gt, in_pred):
    return mean_absolute_error(boneage_div * in_gt, boneage_div * in_pred) 

# Compile model
adam = Adam(learning_rate = 0.0005)
model.compile(loss = 'mse', optimizer = adam, metrics = [mae_months])

#-----------------------------------------------------------------------
# KFold
n_splits = 5
kf = KFold(n_splits = n_splits, shuffle = True, random_state = 42)

I coded up to KFold, but now I am stuck with proceeding to the cross validation step to get MAE for each data splits?
A post here suggests a for loop for each Kfold splits, but that's only if the model such as DecisionTreeRegressor() is used instead of a custom model using Xception like mine?
UPDATE
After referring to the suggestion below, I applied the code as follows after the using KFold:
# Data Generators:
train_gen = flow_from_dataframe(core_idg, train_df, 
                                path_col = 'path',
                                y_col = 'boneage_zscore', 
                                target_size = IMG_SIZE,
                                color_mode = 'rgb',
                                batch_size = 1024,
                                shuffle = True)
...
...
...

mae_list = []
n_splits = 5
kf = KFold(n_splits = n_splits, shuffle = True, random_state = 42)
split = kf.split(X_train, Y_train) # X_train, Y_train = next(train_gen) from above

for train, test in split:
    x_train, x_test, y_train, y_test = X_train[train], X_train[test], Y_train[train], Y_train[test]
    history = model.fit(x_train, y_train, validation_data = (x_test, y_test), batch_size = 16)
    pred = model.predict(x_test, batch_size = 8)
    err = mean_absolute_error(y_test, pred)
    mae_list .append(err)

I set the batch size of train_gen to like 1024 first then run the code above, however, I get the following error:
52/52 [==============================] - 16s 200ms/step - loss: 0.9926 - mae_months: 31.5353 - val_loss: 4.4153 - val_mae_months: 81.5463
52/52 [==============================] - 9s 172ms/step - loss: 0.4185 - mae_months: 21.4242 - val_loss: 0.7401 - val_mae_months: 29.3815
52/52 [==============================] - 9s 172ms/step - loss: 0.2930 - mae_months: 17.3729 - val_loss: 0.5628 - val_mae_months: 23.9055
 9/52 [====>.........................] - ETA: 7s - loss: 0.2355 - mae_months: 16.7444

ResourceExhaustedError                    Traceback (most recent call last)
Input In [11], in <cell line: 9>()
     10 x_train, x_test, y_train, y_test = X_train[train], X_train[test], Y_train[train], Y_train[test]
     11 # model = boneage_model()
     12 # history = model.fit(train_gen, validation_data = (x_test, y_test))
---> 13 history = model.fit(x_train, y_train, validation_data = (x_test, y_test), batch_size = 16)
     14 pred = model.predict(x_test, batch_size = 8)
     15 err = mean_absolute_error(y_test, pred)

ResourceExhaustedError: Graph execution error:

....
....
....

Node: 'gradient_tape/sequential/xception/block14_sepconv2/separable_conv2d/Conv2DBackpropFilter'
OOM when allocating tensor with shape[2048,1536,1,1] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc
     [[{{node gradient_tape/sequential/xception/block14_sepconv2/separable_conv2d/Conv2DBackpropFilter}}]]

The memory allocation looks like this from the prompt (hopefully this makes sense):
total_region_allocated_bytes_: 5769199616 
memory_limit_: 5769199616 
available bytes: 0 
curr_region_allocation_bytes_: 8589934592

Stats:
Limit:                      5769199616
InUse:                      5762760448
MaxInUse:                   5769190400
NumAllocs:                      192519
MaxAllocSize:               2470510592
Reserved:                            0
PeakReserved:                        0
LargestFreeBlock:                    0

Is it because my GPU cannot take the batch_size?
UPDATE 2
I have decreased the batch_size of the train_gen to 32. Took out the batch_size from the fit() and predict() method. Is this the right way to determine the MAE for each data split?
Code:
# Data Generators:
train_gen = flow_from_dataframe(core_idg, train_df, 
                                path_col = 'path',
                                y_col = 'boneage_zscore', 
                                target_size = IMG_SIZE,
                                color_mode = 'rgb',
                                batch_size = 32,
                                shuffle = True)

X_train, Y_train = next(train_gen)
...
...
...

mae_list = []
n_splits = 5
kf = KFold(n_splits = n_splits, shuffle = True, random_state = 42)
split = kf.split(X_train, Y_train) # X_train, Y_train = next(train_gen) from above

for train, test in split:
    x_train, x_test, y_train, y_test = X_train[train], X_train[test], Y_train[train], Y_train[test]
    history = model.fit(x_train, y_train, validation_data = (x_test, y_test))
    pred = model.predict(x_test)
    err = mean_absolute_error(y_test, pred)
    mae_list.append(err)

UPDATE 3
According to the suggestions from the comments:

Edited the batch_size of the train_gen to 64.
Added valid_gen to use X_valid and y_valid as validation data of the fit() method.
Used x_test for the predict() method.
Added a method for limiting GPU memory growth.

Code:

# Checking the GPU availability
print(""Num GPUs Available: "", len(tf.config.list_physical_devices('GPU')))

physical_devices = tf.config.list_physical_devices('GPU')
tf.config.experimental.set_memory_growth(physical_devices[0], True)

...
...
...

# Data Generators:
train_gen = flow_from_dataframe(core_idg, train_df, 
                                path_col = 'path',
                                y_col = 'boneage_zscore', 
                                target_size = IMG_SIZE,
                                color_mode = 'rgb',
                                batch_size = 64,
                                shuffle = True)

X_train, Y_train = next(train_gen)

valid_gen = flow_from_dataframe(core_valid, valid_df, 
                                path_col = 'path',
                                y_col = 'boneage_zscore', 
                                target_size = IMG_SIZE,
                                color_mode = 'rgb',
                                batch_size = 64,
                                shuffle = True)

X_valid, y_valid = next(valid_gen)


# Getting MAE for each data split using 5-fold (KFold)

cv_mae = []
n_splits = 5
kf = KFold(n_splits = n_splits, shuffle = True, random_state = 42)
split = kf.split(X_train, Y_train)

for train, test in split:
    x_train, x_test, y_train, y_test = X_train[train], X_train[test], Y_train[train], Y_train[test]
    history = model.fit(x_train, y_train, validation_data = (X_valid, y_valid))
    pred = model.predict(x_test)
    err = mean_absolute_error(y_test, pred)
    cv_mae.append(err)

cv_mae

The output:
2/2 [==============================] - 8s 2s/step - loss: 3.6179 - mae_months: 66.8136 - val_loss: 2.1544 - val_mae_months: 47.2171
2/2 [==============================] - 1s 394ms/step - loss: 1.0826 - mae_months: 36.3370 - val_loss: 1.6431 - val_mae_months: 40.9770
2/2 [==============================] - 1s 344ms/step - loss: 0.6129 - mae_months: 23.0258 - val_loss: 1.8911 - val_mae_months: 45.6456
2/2 [==============================] - 1s 360ms/step - loss: 0.4500 - mae_months: 22.6450 - val_loss: 1.3592 - val_mae_months: 36.7073
2/2 [==============================] - 1s 1s/step - loss: 0.4222 - mae_months: 20.2543 - val_loss: 1.1010 - val_mae_months: 32.8488

[<tf.Tensor: shape=(13,), dtype=float32, numpy=
 array([1.4442804, 1.3981661, 1.5037801, 2.2199252, 1.7645894, 1.4836203,
        1.7916738, 1.3967942, 1.4069557, 2.516875 , 1.4077926, 1.4342965,
        1.9279695], dtype=float32)>,
 <tf.Tensor: shape=(13,), dtype=float32, numpy=
 array([1.8153722, 1.9236553, 1.3917867, 1.5313213, 1.387209 , 1.3831038,
        1.4519565, 1.4680854, 1.7810788, 2.5733376, 1.4269204, 1.3751   ,
        1.446231 ], dtype=float32)>,
 <tf.Tensor: shape=(13,), dtype=float32, numpy=
 array([1.6616   , 1.6529323, 1.9181525, 2.536807 , 1.6306267, 2.856683 ,
        2.113724 , 1.5543866, 1.9128528, 3.218016 , 1.4112593, 1.4043481,
        3.229338 ], dtype=float32)>,
 <tf.Tensor: shape=(13,), dtype=float32, numpy=
 array([2.1295295, 1.8527019, 1.9779519, 3.1390932, 1.5525225, 2.0811615,
        1.6279813, 1.87973  , 1.5029857, 1.6502519, 2.3677726, 1.8570358,
        1.7251074], dtype=float32)>,
 <tf.Tensor: shape=(12,), dtype=float32, numpy=
 array([1.3926607, 1.7088655, 1.7379242, 3.5756006, 1.5988973, 1.3926607,
        1.4928951, 1.4665956, 1.3926607, 1.4575896, 3.146022 , 1.3926607],
       dtype=float32)>]

Does this mean that I have MAEs for 5 data splits? (where it says numpy = array[....] in the output?)
","Ideally, you'd split train and test sets together from the kfold split, but it doesn't matter if you use the same seed. kfold split just returns indices to select train and test elements. So you need to get those indices from the split from the original dataset.
Answer based on OP comment and question:
from sklearn.model_selection import StratifiedKFold as kfold

x, y = # images, labels
cvscores = []
kf = kfold(n_splits = n_splits, shuffle = True, random_state = 42)
split = kf.split(x, y)

for train, test in split
    x_train, x_test, y_train, y_test = x[train], x[test], y[train], y[test]
    model = # do model stuff
    _ = model.fit()
    result = mode.evaluate()
    #depending on how you want to handle the results
    cvscores.append(result)
# do stuff with cvscores

I'm not sure if that would work with an object from flow_fromdataframe()` because that wouldn't be an array or array-like, although you should be able to get the arrays within.
",cross-validation
matlab39s treebagger and kfold cross validation,"I am trying to get the 5-fold cross validation error of a model created with TreeBagger using the function crossval but I keep getting an error

Error using crossval>evalFun
The function 'regrTree' generated the following error:
Too many input arguments.

My code is below. Can anyone point me in the right direction? Thanks
%Random Forest
%%XX is training data matrix, Y is training labels vector
XX=X_Tbl(:,2:end);
Forest_Mdl = TreeBagger(1000,XX,Y,'Method','regression');

err_std = crossval('mse',XX,Y,'Predfun',@regrTree, 'kFold',5);


function yfit_std = regrTree(Forest_Mdl,XX) 
yfit_std = predict(Forest_Mdl,XX);
end

","Reading the documentation helps a lot!:
The function has to be defined as:
(note that it takes 3 arguments, not 2)

function yfit = myfunction(Xtrain,ytrain,Xtest)
% Calculate predicted response
...
end

Xtrain — Subset of the observations in X used as training predictor
data. The function uses Xtrain and ytrain to construct a
classification or regression model.
ytrain — Subset of the responses in y used as training response data.
The rows of ytrain correspond to the same observations in the rows of
Xtrain. The function uses Xtrain and ytrain to construct a
classification or regression model.
Xtest — Subset of the observations in X used as test predictor data.
The function uses Xtest and the model trained on Xtrain and ytrain to
compute the predicted values yfit.
yfit — Set of predicted values for observations in Xtest. The yfit
values form a column vector with the same number of rows as Xtest.

",cross-validation
does stratifiedkfold splits the same each time a for loop is called,"I use StratifiedKFold and a form of grid search for my Logistic Regression.
skf = StratifiedKFold(n_splits=6, shuffle=True, random_state=SEED)

I call this for loop for each combination of parameters:
for fold, (trn_idx, test_idx) in enumerate(skf.split(X, y)):

My question is, are trn_idx and test_idx the same for each fold every time I run the loop?
For example, if fold0 contains trn_dx = [1,2,5,7,8] and test_idx = [3,4,6], is fold0 going to contain the same trn_idx and test_idx the next 5 times I run the loop?
","Yes, the stratified k-fold split is fixed if random_state=SEED is fixed. The shuffle only shuffles the dataset along with their targets before the k-fold split.
This means that each fold will always have their indexes:

x = list(range(10))
y = [1]*5 + [2]*5

from sklearn.model_selection import StratifiedKFold

skf = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)

for fold, (trn_idx, test_idx) in enumerate(skf.split(x, y)):
    print(trn_idx, test_idx)


Output:
[1 2 4 5 7 9] [0 3 6 8]
[0 1 3 5 6 8 9] [2 4 7]
[0 2 3 4 6 7 8] [1 5 9]


No matter how may times I run this code.
",cross-validation
difference between stratifiedkfold and stratifiedshufflesplit in sklearn,"As from the title I am wondering what is the difference between
StratifiedKFold with the parameter shuffle=True
StratifiedKFold(n_splits=10, shuffle=True, random_state=0)

and
StratifiedShuffleSplit
StratifiedShuffleSplit(n_splits=10, test_size=’default’, train_size=None, random_state=0)

and what is the advantage of using StratifiedShuffleSplit
","In stratKFolds, each test set should not overlap, even when shuffle is included. With stratKFolds and shuffle=True, the data is shuffled once at the start, and then divided into the number of desired splits. The test data is always one of the splits, the train data is the rest.
In ShuffleSplit, the data is shuffled every time, and then split. This means the test sets may overlap between the splits.
See this block for an example of the difference. Note the overlap of the elements in the test sets for ShuffleSplit.
splits = 5

tx = range(10)
ty = [0] * 5 + [1] * 5

from sklearn.model_selection import StratifiedShuffleSplit, StratifiedKFold
from sklearn import datasets

stratKfold = StratifiedKFold(n_splits=splits, shuffle=True, random_state=42)
shufflesplit = StratifiedShuffleSplit(n_splits=splits, random_state=42, test_size=2)

print(""stratKFold"")
for train_index, test_index in stratKfold.split(tx, ty):
    print(""TRAIN:"", train_index, ""TEST:"", test_index)

print(""Shuffle Split"")
for train_index, test_index in shufflesplit.split(tx, ty):
    print(""TRAIN:"", train_index, ""TEST:"", test_index)

Output:
stratKFold
TRAIN: [0 2 3 4 5 6 7 9] TEST: [1 8]
TRAIN: [0 1 2 3 5 7 8 9] TEST: [4 6]
TRAIN: [0 1 3 4 5 6 8 9] TEST: [2 7]
TRAIN: [1 2 3 4 6 7 8 9] TEST: [0 5]
TRAIN: [0 1 2 4 5 6 7 8] TEST: [3 9]
Shuffle Split
TRAIN: [8 4 1 0 6 5 7 2] TEST: [3 9]
TRAIN: [7 0 3 9 4 5 1 6] TEST: [8 2]
TRAIN: [1 2 5 6 4 8 9 0] TEST: [3 7]
TRAIN: [4 6 7 8 3 5 1 2] TEST: [9 0]
TRAIN: [7 2 6 5 4 3 0 9] TEST: [1 8]

As for when to use them, I tend to use stratKFolds for any cross validation, and I use ShuffleSplit with a split of 2 for my train/test set splits. But I'm sure there are other use cases for both.
",cross-validation
what39s the mistake i am doing in this cv code,"I am trying to do CV for my training and testing datasets. I am using LinearRegressor. However, when I run the code, I get the error below. But when I run the code on Decision Trees I don't get any errors and the code works. How to fix this? Is my code for the CV section correct? Thank you for your help.......................................................
Reference for the CV code: scikit-learn cross_validation over-fitting or under-fitting
data_set = pd.read_excel(""NEW Collected Data for Preliminary Results Independant variables ONLY_NO AREA_NO_INFILL_DENSITY_no_printing_temperature.xlsx"")
pd.set_option('max_columns', 35)
pd.set_option('max_rows', 300)
data_set.head(300)


X, y = data_set[[ ""Part's Z-Height (mm)"",""Part's Solid Volume (cm^3)"",""Layer Height (mm)"",""Printing/Scanning Speed (mm/s)"",""Part's Orientation (Support's volume) (cm^3)""]], data_set [[""Climate change (kg CO2 eq.)"",""Climate change, incl biogenic carbon (kg CO2 eq.)"",""Fine Particulate Matter Formation (kg PM2.5 eq.)"",""Fossil depletion (kg oil eq.)"",""Freshwater Consumption (m^3)"",""Freshwater ecotoxicity (kg 1,4-DB eq.)"",""Freshwater Eutrophication (kg P eq.)"",""Human toxicity, cancer (kg 1,4-DB eq.)"",""Human toxicity, non-cancer (kg 1,4-DB eq.)"",""Ionizing Radiation (Bq. C-60 eq. to air)"",""Land use (Annual crop eq. yr)"",""Marine ecotoxicity (kg 1,4-DB eq.)"",""Marine Eutrophication (kg N eq.)"",""Metal depletion (kg Cu eq.)"",""Photochemical Ozone Formation, Ecosystem (kg NOx eq.)"",""Photochemical Ozone Formation, Human Health (kg NOx eq.)"",""Stratospheric Ozone Depletion (kg CFC-11 eq.)"",""Terrestrial Acidification (kg SO2 eq.)"",""Terrestrial ecotoxicity (kg 1,4-DB eq.)""]]

   scaler = preprocessing.MinMaxScaler()
    names = data_set.columns
    d = scaler.fit_transform(data_set)
    scaled_df = pd.DataFrame(d, columns=names)
    X_normalized, y_for_normalized = scaled_df[[ ""Part's Z-Height (mm)"",""Part's Solid Volume (cm^3)"",""Layer Height (mm)"",""Printing/Scanning Speed (mm/s)"",""Part's Orientation (Support's volume) (cm^3)""]], scaled_df [[""Climate change (kg CO2 eq.)"",""Climate change, incl biogenic carbon (kg CO2 eq.)"",""Fine Particulate Matter Formation (kg PM2.5 eq.)"",""Fossil depletion (kg oil eq.)"",""Freshwater Consumption (m^3)"",""Freshwater ecotoxicity (kg 1,4-DB eq.)"",""Freshwater Eutrophication (kg P eq.)"",""Human toxicity, cancer (kg 1,4-DB eq.)"",""Human toxicity, non-cancer (kg 1,4-DB eq.)"",""Ionizing Radiation (Bq. C-60 eq. to air)"",""Land use (Annual crop eq. yr)"",""Marine ecotoxicity (kg 1,4-DB eq.)"",""Marine Eutrophication (kg N eq.)"",""Metal depletion (kg Cu eq.)"",""Photochemical Ozone Formation, Ecosystem (kg NOx eq.)"",""Photochemical Ozone Formation, Human Health (kg NOx eq.)"",""Stratospheric Ozone Depletion (kg CFC-11 eq.)"",""Terrestrial Acidification (kg SO2 eq.)"",""Terrestrial ecotoxicity (kg 1,4-DB eq.)""]]
    scaled_df.head(200) 

Part's Z-Height (mm)    Part's Solid Volume (cm^3)  Layer Height (mm)   Printing/Scanning Speed (mm/s)  Part's Orientation (Support's volume) (cm^3)    Climate change (kg CO2 eq.) Climate change, incl biogenic carbon (kg CO2 eq.)   Fine Particulate Matter Formation (kg PM2.5 eq.)    Fossil depletion (kg oil eq.)   Freshwater Consumption (m^3)    Freshwater ecotoxicity (kg 1,4-DB eq.)  Freshwater Eutrophication (kg P eq.)    Human toxicity, cancer (kg 1,4-DB eq.)  Human toxicity, non-cancer (kg 1,4-DB eq.)  Ionizing Radiation (Bq. C-60 eq. to air)    Land use (Annual crop eq. yr)   Marine ecotoxicity (kg 1,4-DB eq.)  Marine Eutrophication (kg N eq.)    Metal depletion (kg Cu eq.) Photochemical Ozone Formation, Ecosystem (kg NOx eq.)   Photochemical Ozone Formation, Human Health (kg NOx eq.)    Stratospheric Ozone Depletion (kg CFC-11 eq.)   Terrestrial Acidification (kg SO2 eq.)  Terrestrial ecotoxicity (kg 1,4-DB eq.)
0   0.258287    0.005030    0.0 0.666667    0.040088    0.069825    0.056976    0.083205    0.010373    0.113808    0.104798    0.086400    0.110358    0.012836    0.091120    0.108676    0.090401    0.087426    0.125608    0.079028    0.080495    0.078380    0.082404    0.045040
1   0.258287    0.005030    0.2 0.666667    0.036597    0.041682    0.022880    0.074884    0.004841    0.045640    0.102285    0.082884    0.044202    0.005414    0.086700    0.105749    0.087161    0.084130    0.060373    0.072878    0.073529    0.074829    0.075438    0.018122
2   0.258287    0.009557    0.4 0.666667    0.031013    0.033310    0.012113    0.073035    0.003458    0.023401    0.102914    0.082494    0.022690    0.003231    0.086279    0.105749    0.086937    0.084130    0.039708    0.071341    0.071981    0.074698    0.073447    0.009856
3   0.258287    0.009054    0.6 0.666667    0.031013    0.029213    0.006954    0.072111    0.002766    0.012936    0.102914    0.082103    0.012524    0.001921    0.086069    0.105423    0.086602    0.084130    0.029579    0.070572    0.071207    0.074435    0.072452    0.005723
4   0.258287    0.010060    1.0 0.666667    0.031711    0.025650    0.001795    0.071803    0.003458    0.002180    0.103542    0.082884    0.002063    0.001048    0.086490    0.106074    0.087049    0.084542    0.019449    0.070572    0.071207    0.074961    0.072452    0.001908
5   0.258287    0.005030    0.0 0.000000    0.040088    0.074279    0.062360    0.084129    0.011065    0.125000    0.104798    0.086790    0.121114    0.014146    0.091330    0.108676    0.091519    0.087426    0.136143    0.080566    0.081269    0.078511    0.083400    0.049385
6   0.258287    0.038226    0.0 0.666667    0.040088    0.097791    0.074249    0.109091    0.038036    0.135174    0.129299    0.111788    0.132164    0.024625    0.116582    0.133725    0.116102    0.112970    0.154781    0.105166    0.106037    0.104419    0.108280    0.064222
7   0.137212    0.004527    0.0 0.666667    0.030314    0.058247    0.046433    0.076117    0.003458    0.095349    0.099144    0.080150    0.092382    0.008907    0.084806    0.102821    0.084702    0.081246    0.106159    0.072878    0.073529    0.072199    0.075438    0.035608
8   0.137212    0.004527    0.2 0.666667    0.029616    0.035269    0.017721    0.069954    0.000000    0.037355    0.098516    0.078197    0.036246    0.002794    0.082281    0.101520    0.082803    0.080010    0.051053    0.068266    0.068885    0.070489    0.070462    0.013247
9   0.137212    0.010060    0.4 0.666667    0.028918    0.031706    0.010543    0.072111    0.002766    0.020494    0.102285    0.081712    0.019891    0.002358    0.085438    0.104773    0.086043    0.083306    0.036467    0.070572    0.071207    0.073908    0.072452    0.008372
10  0.137212    0.010060    0.6 0.666667    0.028220    0.027431    0.005384    0.070878    0.001383    0.010320    0.101657    0.080931    0.010019    0.001484    0.084806    0.104448    0.085373    0.082894    0.026742    0.069803    0.070433    0.073251    0.071457    0.004345
11  0.137212    0.009557    1.0 0.666667    0.027522    0.022800    0.000000    0.069029    0.000000    0.000000    0.101029    0.080150    0.000000    0.000000    0.083754    0.103472    0.084367    0.081658    0.016613    0.068266    0.068885    0.072330    0.070462    0.000000
12  0.137212    0.004527    0.0 0.000000    0.030314    0.062879    0.052266    0.077042    0.004149    0.107122    0.099144    0.080541    0.103875    0.010217    0.085227    0.102821    0.085037    0.081658    0.117099    0.073647    0.074303    0.072462    0.076433    0.040165
13  0.137212    0.037723    0.0 0.666667    0.030314    0.085857    0.063257    0.102003    0.031120    0.116134    0.123645    0.105929    0.112568    0.020695    0.110269    0.127544    0.110515    0.106790    0.134522    0.098247    0.099071    0.097843    0.101314    0.053624
14  0.077118    0.004527    0.0 0.666667    0.054050    0.080335    0.064827    0.091217    0.018672    0.126453    0.111709    0.093821    0.122145    0.016766    0.098485    0.115833    0.098223    0.094842    0.139789    0.087485    0.088235    0.085876    0.090366    0.052777
15  0.077118    0.004527    0.0 0.000000    0.054050    0.085144    0.070884    0.092450    0.019364    0.138081    0.111709    0.094211    0.133638    0.018075    0.099116    0.116158    0.098223    0.094842    0.151135    0.088253    0.089009    0.086139    0.091361    0.057864
16  0.077118    0.004527    0.0 0.333333    0.054050    0.082472    0.067519    0.091834    0.019364    0.132267    0.111709    0.094211    0.127744    0.017639    0.098695    0.116158    0.098223    0.094842    0.144652    0.087485    0.088235    0.086007    0.091361    0.054684

     lin_regressor = LinearRegression()
    
    # pass the order of your polynomial here  
    poly = PolynomialFeatures(1)
    
    # convert to be used further to linear regression
    X_transform = poly.fit_transform(x_train)
    
    # fit this to Linear Regressor
    linear_regg=lin_regressor.fit(X_transform,y_train).                                               
                 

    import numpy as np
    from sklearn.metrics import SCORERS
    from sklearn.model_selection import KFold
    
    scorer = SCORERS['r2']
    
    cv = KFold(n_splits=5, random_state=0,shuffle=True)
    train_scores, test_scores = [], []
    
    for train, test in cv.split(X_normalized):
        X_transform2 = poly.fit_transform(X_normalized)
        OL=lin_regressor.fit(X_transform2.iloc[train], y_for_normalized.iloc[train])
        tr_21 = OL.score(X_train, y_train)
        ts_21 = OL.score(X_test, y_test)
        print (""Train score:"", tr_21) # from documentation .score returns r^2
        print (""Test score:"", ts_21)   # from documentation .score returns r^2
        
        train_scores.append(tr_21)
        test_scores.append(ts_21)


    
    print (""The Mean for Train scores is:"",(np.mean(train_scores)))
        
    print (""The Mean for Test scores is:"",(np.mean(test_scores)))

Error message:

        --------------------------------------------------------------------------
    AttributeError                            Traceback (most recent call last)
    /var/folders/mm/r4gnnwl948zclfyx12w803040000gn/T/ipykernel_73165/2276765730.py in <module>
         10 for train, test in cv.split(X_normalized):
         11     X_transform2 = poly.fit_transform(X_normalized)
    ---> 12     OL=lin_regressor.fit(X_transform2.iloc[train], y_for_normalized.iloc[train])
         13     tr_21 = OL.score(X_train, y_train)
         14     ts_21 = OL.score(X_test, y_test)
    
    AttributeError: 'numpy.ndarray' object has no attribute 'iloc'


Decision Trees
    new_model = DecisionTreeRegressor(max_depth=9,
                                      min_samples_split=10,random_state=0)


    import numpy as np
    from sklearn.metrics import SCORERS
    from sklearn.model_selection import KFold
     
    scorer = SCORERS['r2']
     
    cv = KFold(n_splits=5, random_state=0,shuffle=True)
    train_scores, test_scores = [], []
     
    for train, test in cv.split(X_normalized):
     
        OO=new_model.fit(X_normalized.iloc[train], y_for_normalized.iloc[train])
        tr_2 = OO.score(X_train, y_train)
        ts_2 = OO.score(X_test, y_test)
        print (""Train score:"", tr_2) # from documentation .score returns r^2
        print (""Test score:"", ts_2)   # from documentation .score returns r^2
         
        train_scores.append(tr_2)
        test_scores.append(ts_2)
     
         
         
    print (""The Mean for Train scores is:"",(np.mean(train_scores)))
         
    print (""The Mean for Test scores is:"",(np.mean(test_scores)))

Output
    Train score: 0.8960560474997927
    Test score: -0.15521696464773224
    Train score: 0.8852795454592853
    Test score: 0.17650772852710495
    Train score: 0.5825347735306872
    Test score: 0.34789159049344665
    Train score: 0.8549575808716975
    Test score: 0.7615265842042157
    Train score: 0.8340261480334055
    Test score: 0.14011826401728472
    The Mean for Train scores is: 0.8105708190789735
    The Mean for Test scores is: 0.2541654405188639

#Trial 1
import numpy as np
from sklearn.metrics import SCORERS
from sklearn.model_selection import KFold

scorer = SCORERS['r2']

cv = KFold(n_splits=5, random_state=0,shuffle=True)
train_scores, test_scores = [], []

for train, test in cv.split(X_normalized):
    X_transform2 = poly.fit_transform(X_normalized)
    OL=lin_regressor.fit(X_transform2[train], y_for_normalized[train])
    tr_21 = OL.score(X_train, y_train)
    ts_21 = OL.score(X_test, y_test)
    print (""Train score:"", tr_21) # from documentation .score returns r^2
    print (""Test score:"", ts_21)   # from documentation .score returns r^2
    
    train_scores.append(tr_21)
    test_scores.append(ts_21)


    
print (""The Mean for Train scores is:"",(np.mean(train_scores)))
    
print (""The Mean for Test scores is:"",(np.mean(test_scores)))

Error message:
---------------------------------------------------------------------------
KeyError                                  Traceback (most recent call last)
/var/folders/mm/r4gnnwl948zclfyx12w803040000gn/T/ipykernel_90924/12176184.py in <module>
     10 for train, test in cv.split(X_normalized):
     11     X_transform2 = poly.fit_transform(X_normalized)
---> 12     OL=lin_regressor.fit(X_transform2[train], y_for_normalized[train])
     13     tr_21 = OL.score(X_train, y_train)
     14     ts_21 = OL.score(X_test, y_test)

~/opt/anaconda3/lib/python3.9/site-packages/pandas/core/frame.py in __getitem__(self, key)
   3462             if is_iterator(key):
   3463                 key = list(key)
-> 3464             indexer = self.loc._get_listlike_indexer(key, axis=1)[1]
   3465 
   3466         # take() does not accept boolean indexers

~/opt/anaconda3/lib/python3.9/site-packages/pandas/core/indexing.py in _get_listlike_indexer(self, key, axis)
   1312             keyarr, indexer, new_indexer = ax._reindex_non_unique(keyarr)
   1313 
-> 1314         self._validate_read_indexer(keyarr, indexer, axis)
   1315 
   1316         if needs_i8_conversion(ax.dtype) or isinstance(

~/opt/anaconda3/lib/python3.9/site-packages/pandas/core/indexing.py in _validate_read_indexer(self, key, indexer, axis)
   1372                 if use_interval_msg:
   1373                     key = list(key)
-> 1374                 raise KeyError(f""None of [{key}] are in the [{axis_name}]"")
   1375 
   1376             not_found = list(ensure_index(key)[missing_mask.nonzero()[0]].unique())

KeyError: ""None of [Int64Index([ 0,  1,  3,  4,  5,  6,  9, 10, 11, 12, 14, 15, 17, 18, 19, 20, 21,\n            23, 25, 27, 28, 29, 31, 32, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43,\n            44, 45, 46, 47, 48, 49, 50, 51, 52, 56, 57, 58, 59, 60, 61, 62, 63,\n            64, 65, 66, 67, 68, 69, 70, 71, 72, 74, 76, 77, 79, 80, 81, 82, 83,\n            84, 85, 87, 88, 89, 90, 91, 94, 96, 97, 98, 99],\n           dtype='int64')] are in the [columns]""

","Understanding

poly.fit_transform will return numpy.ndarray so here your X_normalized is being transformed from pandas.core.frame.DataFrame to numpy.ndarray.
But your y_for_normalized is still pandas.core.frame.DataFrame.
So in numpy.ndarray you pass indexes as numpy.ndarray[indexes] and for pandas.core.frame.DataFrame you will pass indexes in .iloc[indexes] respectively.

Solution

For X_transform2 use [] for getting data as it's numpy.ndarray
For y_for_normalized use .iloc[] as it's pandas.core.frame.DataFrame

Code
train_scores, test_scores = [], []

for train, test in cv.split(X_normalized):
    X_transform2 = poly.fit_transform(X_normalized)
    # [] for X_transform2, .iloc[] for y_for_normalized
    OL = lin_regressor.fit(X_transform2[train], y_for_normalized.iloc[train])
    tr_21 = OL.score(X_transform2[train], y_for_normalized.iloc[train])
    ts_21 = OL.score(X_transform2[test], y_for_normalized.iloc[test])
    print(""Train score:"", tr_21)  # from documentation .score returns r^2
    print(""Test score:"", ts_21)  # from documentation .score returns r^2

    train_scores.append(tr_21)
    test_scores.append(ts_21)


print(""The Mean for Train scores is:"", (np.mean(train_scores)))

print(""The Mean for Test scores is:"", (np.mean(test_scores)))

PS:

Don't know why are you using X_train, y_train and X_test, y_test in OL.score. It should be the dataset with index of train and test generated by cv. Same is reflected in a above code snippet.

If you have X_train, y_train and X_test, y_test defined for specific reason then you are good to use.


Why are you using PolynomialFeatures() when you want all your feature to be of 1 degree which it's already is so it's making no difference to use PolynomialFeatures() for 1 degree.
Also check for deprecation warning for SCORER if you are using new version of sklearn.

",cross-validation
how can i get the history of the different fits when using cross vaidation over a kerasregressor,"I have a regression problem and I am using a keras fully connected layer to model my problem. I am using cross_val_score and my question is: how can I extract the model and the history of each train/validation combination the cross_val_score does?
Assuming this example:
from sklearn import datasets
from sklearn.model_selection import cross_val_score, KFold
from keras.models import Sequential
from keras.layers import Dense
from keras.wrappers.scikit_learn import KerasRegressor
seed = 1

diabetes = datasets.load_diabetes()
X = diabetes.data[:150]
y = diabetes.target[:150]

def baseline_model():
    model = Sequential()
    model.add(Dense(10, input_dim=10, activation='relu'))
    model.add(Dense(1))
    model.compile(loss='mean_squared_error', optimizer='adam')
    return model

estimator = KerasRegressor(build_fn=baseline_model, nb_epoch=100, batch_size=100, verbose=False)
kfold = KFold(n_splits=10, random_state=seed)
results = cross_val_score(estimator, X, y, cv=kfold)
print(""Results: %.2f (%.2f) MSE"" % (results.mean(), results.std()))

My understanding is that I only get the overall mse over each fold, so to say.
But I want to compare the train to validation mse over the epochs of the model for each fold, i.e. for 10 in this case.
When not using kfold, but simple train/validation split, then one can do:
hist = model.fit(X_tr, y_tr, validation_data=val_data,
                  epochs=100, batch_size=100,
                  verbose=1)

plt.plot(history.history['loss'])
plt.plot(history.history['loss'])

This would return a plot representing the evolution of the mse w.r.t. to the epochs for the train and validation datasets, allowing to spot over/underfitting.
How to do this for each fold when using cross validation?
","You can go for a ""manual"" CV procedure, and plot the loss (or any other available metric you might want to use) for each fold, i.e. something like this:
from sklearn.metrics import mean_squared_error
cv_mse = []

for train_index, val_index in kfold.split(X):
    history = estimator.fit(X[train_index], y[train_index])
    pred = estimator.predict(X[val_index])
    err = mean_squared_error(y[val_index], pred)
    cv_mse.append(err)
    plt.plot(history.history['loss'])

In which case, the cv_mse list will contain the final MSE for each fold, and you also get the respective plots for its evolution per epoch for each fold.
",cross-validation
why do i have a label problem when using crossvalidator,"I'm new to spark :) I try to use CrossValidator. My model is as follows :
training
#training data - several repartition have been tested, 50/50 seems the best
(trainData, testData) = modelData.randomSplit([0.5, 0.5])

#counting data used
print(""Training dataset count : "" +str(trainData.count()))
print(""Test dataset count : "" +str(testData.count()))
trainData.cache()
testData.cache()

Model
from pyspark.ml.classification import LogisticRegression
lr = LogisticRegression(featuresCol = 'features', labelCol = 'v4_Indexer', maxIter = 5)
lrModel = lr.fit(trainData)
predictions = lrModel.transform(testData)
predictions.select('v4_Indexer','features','rawPrediction', 'prediction', 'probability').toPandas().head(2500)

I try this code for crossvalidation :
from pyspark.ml.tuning import ParamGridBuilder, CrossValidator
from pyspark.ml import Pipeline
pipeline = Pipeline(stages=[lr])
paramGrid = (ParamGridBuilder() .addGrid(lr.regParam, [0,0.5,1]).addGrid(lr.elasticNetParam, [0,0.5,1]).addGrid(lr.maxIter,[1,10]).build())
cv = CrossValidator(estimator=lr, estimatorParamMaps=paramGrid, evaluator=evaluator, numFolds=5)
cvModel = cv.fit(trainData)
trainingSummary = cvModel.bestModel

I have a warning /databricks/spark/python/pyspark/ml/util.py:92: UserWarning: CrossValidator_7ba8c8c903af fit call failed but some spark jobs may still running for unfinished trials. To address this issue, you should enable pyspark pinned thread mode.
warnings.warn(""{} fit call failed but some spark jobs ""
And an error : IllegalArgumentException: label does not exist. Available: v4_Indexer, features, CrossValidator_7ba8c8c903af_rand
this model worked for a while. I do not understand why it doesn't now.
Thx in advance for any help you could bring me =)
","I've solved the issue by changing entirely my code. This is what it looks like now (prerequisite %pip install mlflow):
from pyspark.ml.classification import DecisionTreeClassifier, DecisionTreeClassificationModel
from pyspark.ml.feature import StringIndexer
from pyspark.ml import Pipeline

# StringIndexer: Convert the input column ""label"" (digits) to categorical values
indexer = StringIndexer(inputCol=""v4_Indexer"", outputCol=""indexedLabel"")

# Create an evaluator.  In this case, use ""weightedPrecision"".
from pyspark.ml.evaluation import MulticlassClassificationEvaluator
evaluator = MulticlassClassificationEvaluator(labelCol=""v4_Indexer"", metricName=""weightedPrecision"")

from pyspark.ml.tuning import CrossValidator, ParamGridBuilder

# DecisionTreeClassifier: Learn to predict column ""indexedLabel"" using the ""features"" column

dtc = DecisionTreeClassifier(labelCol=""indexedLabel"")

# Chain indexer + dtc together into a single ML Pipeline
pipeline = Pipeline(stages=[indexer, dtc])
# Define the parameter grid to examine.
grid = ParamGridBuilder().addGrid(dtc.maxDepth, [2, 3, 4, 5, 6, 7, 8]).addGrid(dtc.maxBins, [2, 4, 8]).build()

# Create a cross validator, using the pipeline, evaluator, and parameter grid you created in previous steps.
cv = CrossValidator(estimator=pipeline, evaluator=evaluator, estimatorParamMaps=grid, numFolds=3)

# Explicitly create a new run.
# This allows this cell to be run multiple times.
# If you omit mlflow.start_run(), then this cell could run once, but a second run would hit conflicts when attempting to overwrite the first run.
import mlflow
import mlflow.spark

with mlflow.start_run():
    # Run the cross validation on the training dataset. The cv.fit() call returns the best model it found.
    cvModel = cv.fit(train)
    
    # Evaluate the best model's performance on the test dataset and log the result.
    test_metric = evaluator.evaluate(cvModel.transform(test))
    mlflow.log_metric('test_' + evaluator.getMetricName(), test_metric) 
  
    # Log the best model.
    mlflow.spark.log_model(spark_model=cvModel.bestModel, artifact_path='best-model')

",cross-validation
caret combine the stratified createmultifolds repeatedcv and groupkfold,"My question is very similar to the one asked in 
caret: combine createResample and groupKFold
The only difference: I need to create stratified folds (also repeated 10 times) after grouping instead of bootstrapped resamples (which are not stratified as far as I know) for using it with caret's trainControl.
The following code is working with 10-fold repeated CV but I couldn't include the grouping of the data based on an ""ID"" (df$ID). 
# creating indices
cv.10.folds <- createMultiFolds(rf_label, k = 10, times = 10)
# creating folds    
ctrl.10fold <- trainControl(method = ""repeatedcv"", number = 10, repeats = 10, index = cv.10.folds)
# train
rf.ctrl10 <- train(rf_train, y = rf_label, method = ""rf"", tuneLength = 6,
                       ntree = 1000, trControl = ctrl.10fold, importance = TRUE)

That's my actual problem: My data contains many groups composed of 20 instances each, having the same ""ID"". So, when using the 10-fold CV repeated 10 times I get some instances of a group in the training and some in the validation set. This I want to avoid, but overall I need a stratified partitioning for the prediction value (df$Label). (All instances having the same ""ID"" also have the same prediction/label value.)
In the provided and accepted answer from the link above (see parts below) I guess I have to modify the folds2 line to contain the stratified 10-fold CV instead of the bootstrapped
folds <- groupKFold(x)
folds2 <- lapply(folds, function(x) lapply(1:10, function(i) sample(x, size = length(x), replace = TRUE)))

but unfortunately I cannot figure out how exactly. Could you help me with that?
","Here is an approach to perform stratified repeated K-fold CV with blocking.
library(caret)
library(tidyverse)

some fake data where id will be the blocking factor:
id <- sample(1:55, size = 1000, replace = T)
y <- rnorm(1000)
x <- matrix(rnorm(10000), ncol = 10)
df <- data.frame(id, y, x)

summarise the observations by the blocking factor:
df %>%
  group_by(id) %>%
  summarise(mean = mean(y)) %>%
  ungroup() -> groups1 

create the stratified folds based on the grouped data:
folds <- createMultiFolds(groups1$mean, 10, 3)

back join the original df to the group data and take the df row id's
folds <- lapply(folds, function(i){
  data.frame(id = i) %>%
    left_join(df %>%
                rowid_to_column()) %>%
    pull(rowid) 
})

check if the data id's in the test are not in the train:
lapply(folds, function(i){
  sum(df[i,1] %in% df[-i,1])
})

output is a bunch of zeros, meaning no id's in the test folds are in the train folds.
If your group id's are not numeric there are two approaches to make this work:
1 convert them to numeric:  
first some data
id <- sample(1:55, size = 1000, replace = T)
y <- rnorm(1000)
x <- matrix(rnorm(10000), ncol = 10)
df <- data.frame(id = paste0(""id_"", id), y, x) #factor id's

df %>%
  mutate(id = as.numeric(id)) %>% #convert to numeric
  group_by(id) %>%
  summarise(mean = mean(y)) %>%
  ungroup() -> groups1 

folds <- createMultiFolds(groups1$mean, 10, 3)

folds <- lapply(folds, function(i){
  data.frame(id = i) %>%
    left_join(df %>%
                mutate(id = as.numeric(id)) %>% #also need to convert to numeric in the original data frame
                rowid_to_column()) %>%
    pull(rowid) 
})  

2 filter the id's in grouped data according to fold indexes and then join by id's  
df %>%
  group_by(id) %>%
  summarise(mean = mean(y)) %>%
  ungroup() -> groups1 

folds <- createMultiFolds(groups1$mean, 10, 3)

folds <- lapply(folds, function(i){
  groups1 %>% #start from grouped data
    select(id) %>% #select id's
    slice(i) %>% #filter id's according to fold index
    left_join(df %>% #join by id 
               rowid_to_column()) %>%
    pull(rowid) 
})

Will it work for caret?
ctrl.10fold <- trainControl(method = ""repeatedcv"", number = 10, repeats = 3, index = folds)

rf.ctrl10 <- train(x = df[,-c(1:2)], y = df$y, data = df, method = ""rf"", tuneLength = 1,
                   ntree = 20, trControl = ctrl.10fold, importance = TRUE)

rf.ctrl10$results
#output
  mtry     RMSE    Rsquared       MAE     RMSESD  RsquaredSD      MAESD
1    3 1.041641 0.007534611 0.8246514 0.06953668 0.009488169 0.05934975

Also I suggest you check out library mlr, it has many nice features including blocking - here is one answer on SO. It has very nice tutorials on many things. For a long time I thought you either use caret or mlr but they complement each other very nicely.
",cross-validation
gaussian process regression tune hyperparameters based on validation set,"In the standard scikit-learn implementation of Gaussian-Process Regression (GPR), the hyper-parameters (of the kernel) are chosen based on the training set.
Is there an easy to use implementation of GPR (in python), where the hyperparemeters (of the kernel) are chosen based on a separate validation set? Or cross-validation would also be a nice alternative to find suitable hyperparameters (that are optimized to perform well on mutliple train-val splits). (I would prefer a solution that builds on the scikit-learn GPR.)
In detail: a set of hyperparameters theta should be found, that performs well in the following metric:
Calculate the posterior GP based on the training data (given the prior GP with hyperparameters theta). Then evaluate the negative log likelihood of the validation data with respect to the posterior.
This negative log likelihood should be minimal for theta.
In other words I want to find theta such ""P[ valData | trainData, theta ]"" is maximal. A non-exact approximation that might be sufficient would be to find theta such that sum_i log(P[ valData_i | trainData, theta ] is maximal, where P[ valData_i | trainData, theta ] is the Gaussian marginal posterior density of a validation data-point valData_i given the training-data set given the prior GP with hyperparameters theta.Edit: Since P[ valData | trainData, theta ] has been implemented recently (see my answer), the easier to implement approximation of P[ valData | trainData, theta ] is not needed.

","Two days ago a paper has been presented at ICML that implements my suggestion of splitting the training data into a hyperparameter training set D<m and a hyperparameter validation set D>=m and selecting hyperparameter theta which optimize max p(D>=m|D<m, theta):
https://proceedings.mlr.press/v162/lotfi22a.html.
This paper won an ICML outstanding paper award. They discuss the advantages compared to standars maximization of marginal liklihood and provide some code: https://github.com/Sanaelotfi/Bayesian_model_comparison
I hope that somone implements this (often superior) option for hyperparameter tuning into standard GPR implementation such as the one in scikit-learn.
",cross-validation
sklearnmodel_selectioncross_val_score has different results from a manual calculation done on a confusion matrix,"TL;DR When I calculate precision, recall, and f1 through CV cross_val_score(), it gives me different results than when I calculate through the confusion matrix. Why does it give different precision, recall, and f1 scores?
I'm learning SVM in machine learning and I wanted to compare the result returned by cross_val_score and the result I get from manually calculating the metrics from the confusion matrix. However, I have different result.
To start, I have written the code below using cross_val_score.
clf = svm.SVC()
kfold = KFold(n_splits = 10)

accuracy = metrics.make_scorer(metrics.accuracy_score)
precision = metrics.make_scorer(metrics.precision_score, average = 'macro')
recall = metrics.make_scorer(metrics.recall_score, average = 'macro')
f1 = metrics.make_scorer(metrics.f1_score, average = 'macro')

accuracy_score = cross_val_score(clf, X, y, scoring = accuracy, cv = kfold)
precision_score = cross_val_score(clf, X, y, scoring = precision, cv = kfold)
recall_score = cross_val_score(clf, X, y, scoring = recall, cv = kfold)
f1_score = cross_val_score(clf, X, y, scoring = f1, cv = kfold)

print(""accuracy score:"", accuracy_score.mean())
print(""precision score:"", precision_score.mean())
print(""recall score:"",recall_score.mean())
print(""f1 score:"", f1_score.mean())

The result for each metric is shown below:
accuracy score: 0.97
precision score: 0.96
recall score: 0.97
f1 score: 0.96

In addition, I created a Confusion Matrix so that I can manually calculate the accuracy, precision, recall, and f1 score based on the values on the matrix. I manually created the Confusion Matrix because I am using K-Fold Cross Validation. To do that, I have to get the actual classes and predicted classes for each iteration of the Cross Validation and so I have this code:
def cross_val_predict(model, kfold : KFold, X : np.array, y : np.array) -> Tuple[np.array, np.array]:
    
    model_ = cp.deepcopy(model)
    
    # gets the number of classes in the column/attribute
    no_of_classes = len(np.unique(y))
    
    # initializing empty numpy arrays to be returned
    actual_classes = np.empty([0], dtype = int)
    predicted_classes = np.empty([0], dtype = int)

    for train_index, test_index in kfold.split(X):

        X_train, X_test = X[train_index], X[test_index]
        y_train, y_test = y[train_index], y[test_index]
        
        # append the actual classes for this iteration
        actual_classes = np.append(actual_classes, y_test)
        
        # fit the model
        model_.fit(X_train, y_train)
        
        # predict
        predicted_classes = np.append(predicted_classes, model_.predict(X_test))
        
    return actual_classes, predicted_classes


Afterwards, I created my confusion matrix after calling the above function.
actual_classes, predicted_classes = cross_val_predict(clf, kfold, X, y)
cm = metrics.confusion_matrix(y_true = actual_classes, y_pred = predicted_classes)
cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = cm, display_labels = [2,4])
cm_display.plot()

Now, my confusion matrix looks like the below:
where: col is the predicted label, and row is the true label.

    |------|------|
 2  | 431  |  13  |
    |------|------|
 4  |  9   | 230  |
    |------|------|
       2      4

If I manually calcuate the accuracy, precision, recall, and f1 score from that matrix, I have the ff:
confusion matrix accuracy: 0.97
confusion matrix precision: 0.95
confusion matrix recall: 0.96
confusion matrix f1 score: 0.95

My question is that why did I get different result from manually calculating the metrics from the confusion matrix and the result from calling cross_val_score while specifying which scorer to use, i.e., [accuracy, precision, recall, fscore].
I hope you guys can help me understand why. Thank you very much for your responses!
","With cross_val_score you take the mean of the metrics calculated over each fold, but when you do it manually you concatenate the predictions before calculating the scores. Because of that, the F1 score, precision are modified, while the accuracy and recall are not affected.
Accuracy
If n is the number of samples you have, and k the number of folds, then you can write:

From this equation, you can see that in the case where each fold is the same size, averaging the accuracies is equivalent to calculating the global average. However that is not true if some folds have a different size. But because the difference is only of one participant maximum, and one participant is often small compared to the dataset size, the difference between mean over folds and computed over the whole prediction accuracies is different.
Precision
Now let's consider precision (but this is also true for f1 score or ROC AUC which are calculated from both precision); which is the ratio of true positive/positive predictions
Let's say you have 3 folds:

Fold 1: 4 positive predictions, 1 true positive: precision=1/4

Fold 2: 2 positive predictions, 2 true positive: precision=1

Fold 3: 3 positive predictions, 1 true positive: precision=1/3


Now if you take the average you will get a precision of 19/36=0.527. However if you some the number of positive predictions and true positives you get 4/9=0.44 which is quite different.
The difference comes from the fact that the denominator, i.e. your number of positive predictions, is not constant over folds. Extreme values have more influence when averaging.
Recall
Recall is the rate of True positive/Positive samples. In the case where you stratified your k-fold, the averaged value should be the same as your concatenated metric.
Which to use?
This is a question I have not yet find any clear answer to. Most frameworks or definition use the averaging of the score, but I did not find any comparison with computing the metrics over all the predictions, so here are some personal observations:

computed the metric over each fold seems to be the most used method. The advantage is that you can get the mean, but also the std/quartiles of the metrics. With this you can really assess the performances of the learning process, on the condition that you have enough data.
for a low sample size, the evaluation folds can be really short (few samples). In that case the metrics are less stable (more prone to reach extreme values) and I find the average to still be sensible to extreme values. There I would recommend that you concatenate the predictions/targets to compute the score, but again this is just a personal observation and opinion

",cross-validation
how to cross validate pca in sklearn pipeline without overfitting,"My input is time series data. I want to decompose the dataset with PCA (I dont want to do PCA on the entire dataset first because that would be overfitting) and then use feature selection on each component (fitted on a KNN Regressor model).
This is my code so far:
tscv = TimeSeriesSplit(n_splits=10)
pca = PCA(n_components=.5,svd_solver='full').fit_transform()
knn = KNeighborsRegressor(n_jobs=-1)
sfs = SequentialFeatureSelector(estimator=knn,n_features_to_select='auto',tol=.001,scoring=custom_scorer,n_jobs=-1)
pipe = Pipeline(steps=[(""pca"", pca), (""sfs"", sfs), (""knn"", knn)])
cv_score = cross_val_score(estimator=pipe,X=X,y=y,scoring=custom_scorer,cv=tscv,verbose=10)
print(np.average(cv_score),' +/- ',np.std(cv_score))
print(X.columns)

The problem is I want to make sure PCA isnt looking over the entire dataset when it calculates which features variance. I also want it to be fit transformed, but it doesnt work. With the following error codes:
TypeError: All intermediate steps should be transformers and implement fit and transform or be the string 'passthrough' '<bound method PCA.fit_transform of PCA(svd_solver='full')>' (type <class 'method'>) doesn't

or
TypeError: fit_transform() missing 1 required positional argument: 'X'

","You should not use pca = PCA(...).fit_transform nor pca = PCA(...).fit_transform() when defining your pipeline.
Instead, you should use pca = PCA(...). The fit_transform method is automatically called within the pipeline during the model fitting (in cross_val_score).
",cross-validation
cross validation with grid search returns worse results than default,"I'm using scikitlearn in Python to run some basic machine learning models. Using the built in GridSearchCV() function, I determined the ""best"" parameters for different techniques, yet many of these perform worse than the defaults. I include the default parameters as an option, so I'm surprised this would happen.
For example:
from sklearn import svm, grid_search
from sklearn.ensemble import GradientBoostingClassifier
gbc = GradientBoostingClassifier(verbose=1)
parameters = {'learning_rate':[0.01, 0.05, 0.1, 0.5, 1],  
              'min_samples_split':[2,5,10,20], 
              'max_depth':[2,3,5,10]}
clf = grid_search.GridSearchCV(gbc, parameters)
t0 = time()
clf.fit(X_crossval, labels)
print ""Gridsearch time:"", round(time() - t0, 3), ""s""
print clf.best_params_
# The output is: {'min_samples_split': 2, 'learning_rate': 0.01, 'max_depth': 2}

This is the same as the defaults, except max_depth is 3. When I use these parameters, I get an accuracy of 72%, compared to 78% from the default.
One thing I did, that I will admit is suspicious, is that I used my entire dataset for the cross validation. Then after obtaining the parameters, I ran it using the same dataset, split into 75-25 training/testing.
Is there a reason my grid search overlooked the ""superior"" defaults?
","Running cross-validation on your entire dataset for parameter and/or feature selection can definitely cause problems when you test on the same dataset.  It looks like that's at least part of the problem here.  Running CV on a subset of your data for parameter optimization, and leaving a holdout set for testing, is good practice.  
Assuming you're using the iris dataset (that's the dataset used in the example in your comment link), here's an example of how GridSearchCV parameter optimization is affected by first making a holdout set with train_test_split:  
from sklearn import datasets
from sklearn.model_selection import GridSearchCV
from sklearn.ensemble import GradientBoostingClassifier

iris = datasets.load_iris()
gbc = GradientBoostingClassifier()
parameters = {'learning_rate':[0.01, 0.05, 0.1, 0.5, 1], 
              'min_samples_split':[2,5,10,20], 
              'max_depth':[2,3,5,10]}

clf = GridSearchCV(gbc, parameters)
clf.fit(iris.data, iris.target)

print(clf.best_params_)
# {'learning_rate': 1, 'max_depth': 2, 'min_samples_split': 2}

Now repeat the grid search using a random training subset:  
from sklearn.model_selection import train_test_split

X_train,X_test,y_train,y_test = train_test_split(iris.data, iris.target, 
                                                 test_size=0.33, 
                                                 random_state=42)

clf = GridSearchCV(gbc, parameters)
clf.fit(X_train, y_train)

print(clf.best_params_)
# {'learning_rate': 0.01, 'max_depth': 5, 'min_samples_split': 2}

I'm seeing much higher classification accuracy with both of these approaches, which makes me think maybe you're using different data - but the basic point about performing parameter selection while maintaining a holdout set is demonstrated here.  Hope it helps.
",cross-validation
cross validation with coco data format json files,"I am a newbie ML learner and trying semantic image segmentation on google colab with COCO data format json and lots of images on google drive.
update
I borrowed this code as a starting point. So my code on colab is pretty much like this.
https://github.com/akTwelve/tutorials/blob/master/mask_rcnn/MaskRCNN_TrainAndInference.ipynb
/update
I am splitting an exported json file into 2 jsons (train/validate with 80/20 ratio) every time I receive new annotation data. But this is getting tiring since I have more than 1000 annotations in a file and I do it manually with replace function of VS code.
Is there a better way to do this programatically on google colab?
So what I like to do is rotating annotation data without spitting a json file manually.
Say, I have 1000 annotations in ONE json file on my google drive, I would like to use the 1-800 annotations for training and the 801-1000 annotations for validating for the 1st train session, then for the next train session I would like to use the 210-1000 annotations for training and 1-200 annotations for validating. Like selecting a part of data in json from code on colab.
Or if I can rotate the data during one train session (K-Fold Cross Validation?), that is even better but I have no clue to do this.
Here is parts of my code on the colab.
Loading json files
dataset_train = CocoLikeDataset()
dataset_train.load_data('PATH_TO_TRAIN_JSON', 'PATH_TO_IMAGES')
dataset_train.prepare()

dataset_val = CocoLikeDataset()
dataset_val.load_data('PATH_TO_VALIDATE_JSON', 'PATH_TO_IMAGES')
dataset_val.prepare()

Initializing model
model = modellib.MaskRCNN(mode=""training"", config=config, model_dir=MODEL_DIR)

init_with = ""coco""

if init_with == ""imagenet"":
    model.load_weights(model.get_imagenet_weights(), by_name=True)
elif init_with == ""coco"":
    model.load_weights(COCO_MODEL_PATH, by_name=True,
                       exclude=[""mrcnn_class_logits"", ""mrcnn_bbox_fc"", 
                                ""mrcnn_bbox"", ""mrcnn_mask""])
elif init_with == ""last"":
    model.load_weights(model.find_last(), by_name=True)

train
start_train = time.time()
model.train(dataset_train, dataset_val, 
            learning_rate=config.LEARNING_RATE, 
            epochs=30, 
            layers='heads')
end_train = time.time()
minutes = round((end_train - start_train) / 60, 2)
print(f'Training took {minutes} minutes')

validate 
start_train = time.time()
model.train(dataset_train, dataset_val, 
            learning_rate=config.LEARNING_RATE / 10,
            epochs=10, 
            layers=""all"")
end_train = time.time()
minutes = round((end_train - start_train) / 60, 2)
print(f'Training took {minutes} minutes')

json
{
  ""info"": {
    ""year"": 2020,
    ""version"": ""1"",
    ""description"": ""Exported using VGG Image Annotator (http://www.robots.ox.ac.uk/~vgg/software/via/)"",
    ""contributor"": """",
    ""url"": ""http://www.robots.ox.ac.uk/~vgg/software/via/"",
    ""date_created"": ""Tue Jan 21 2020 16:18:14""
  },
  ""images"": [
    {
      ""id"": 0,
      ""width"": 2880,
      ""height"": 2160,
      ""file_name"": ""sample01.jpg"",
      ""license"": 1,
      ""flickr_url"": ""sample01.jpg"",
      ""coco_url"": ""sample01.jpg"",
      ""date_captured"": """"
    }
  ],
   ""annotations"": [
    {
      ""id"": 0,
      ""image_id"": ""0"",
      ""category_id"": 1,
      ""segmentation"": [
        588,
        783,
        595,
        844,
        607,
        687,
        620,
        703,
        595,
        722,
        582,
        761
      ],
      ""area"": 108199,
      ""bbox"": [
        582,
        687,
        287,
        377
      ],
      ""iscrowd"": 0
    }
  ],
  ""licenses"": [
    {
      ""id"": 1,
      ""name"": ""Unknown"",
      ""url"": """"
    }
  ],
  ""categories"": [
    {
      ""id"": 1,
      ""name"": ""nail"",
      ""supercategory"": ""type""
    }
  ]
}

FYI, My workflow is like

Label images with VIA annotation tool
Export annotations in coco format json
Modify the json and save to my google drive
Load the json on colab and start training

","There's a very good utility function in the sklearn library for doing exactly what you want here. It's called train_test_split.
Now, it's hard to understand what your data structures are, but I am assuming that this code:
dataset_train = CocoLikeDataset()
dataset_train.load_data('PATH_TO_TRAIN_JSON', 'PATH_TO_IMAGES')
dataset_train.prepare()

populates dataset_train with some kind of array of images, or else an array of the paths to the images. sklearn's train_test_split function is able to accept pandas DataFrames as well as numpy arrays.
I am usually very comfortable with pandas DataFrames, so I would suggest you combine the training and validation data into one DataFrame using the pandas function concat, then create a random split using the sklearn function train_test_split at the beginning of every training epoch. It would look something like the following:
import pandas as pd
from sklearn.model_selection import train_test_split

# Convert the data into a DataFrame
master_df = pd.concat([pd.DataFrame(dataset_train), pd.DataFrame(dataset_val)], ignore_index=True)

# Separate out the data and targets DataFrames' (required by train_test_split)
data_df = master_df[['image_data_col_1','image_data_col_2','image_data_col_3']]
targets_df = master_df[['class_label']]

# Split the data into a random train/test (or train/val) split
data_train, data_val, targets_train, targets_val = train_test_split(data_df, targets_df, test_size=0.2)

# Training loop
# If the training function requires the targets to be present in the same DataFrame, you can do this before beginning training:
dataset_train_df = pd.concat([data_train, targets_train], axis=1)
dataset_val_df = pd.concat([data_val, targets_val], axis=1)
##################################
# Continue with training loop...
##################################


Just one last note: ideally, you should have three sets - train, test, and validation. So separate out a testing set beforehand, and then do the train_test_split at the beginning of every iteration of the training loop to obtain your train-validation split from the remaining data.
",cross-validation
time series split in python taking into account different products,"I have df (pandas) containing temporal data for several products (see below). The products may not start or finish in the same date as the others (eg. prod 1 and 2 series finished before dn, while prod 4 started somewhere between d3 and dn). I want to do a time series split, taking into account each produt. By doing that, I can have the same date on training and test, depending on the product. How do I do that?
date  prod  value
d1    p1    10
d1    p2    10
d2    p1    15
d2    p2    12
d3    p1    8
d3    p2    5
d3    p3    7
.
dn    p2    20
dn    p4    10

","you could use:
d = {prod: group.set_index('date')['value'] for prod, group in df.groupby('prod')}

if you want all products have the same index:
d = {prod: group.set_index('date')['value'].reindex(df['date'].unique()) 
     for prod, group in df.groupby('prod')}

",cross-validation
custom scoring function in sklearn cross validate,"I would like to use a custom function for cross_validate which uses a specific y_test to compute precision, this is a different y_test than the actual target y_test.
I have tried a few approaches with make_scorer but I don't know how to actually pass my alternative y_test:
scoring = {'prec1': 'precision',
     'custom_prec1': make_scorer(precision_score()}

scores = cross_validate(pipeline, X, y, cv=5,scoring= scoring)

Can any suggest an approach?
","Found this way. Maybe the code is not optimal, sorry for this.
Okay, let we start:
import numpy as np
import pandas as pd

from sklearn.linear_model import LogisticRegression

from sklearn.model_selection import GridSearchCV
from sklearn.metrics import make_scorer

xTrain = np.random.rand(100, 100)
yTrain = np.random.randint(1, 4, (100, 1))

yTrainCV = np.random.randint(1, 4, (100, 1))

model = LogisticRegression()

yTrainCV will be used here as the custom scorer.
def customLoss(xArray, yArray):
    indices = xArray.index.values
    tempArray = [1 if value1 != value2 else 0 for value1, value2 in zip(xArray.values, yTrainCV[[indices]])]
    
    return sum(tempArray)

scorer = {'main': 'accuracy',
          'custom': make_scorer(customLoss, greater_is_better=True)}

Few tricks here:

you need to pass to customLoss 2 values (predictions from the model + real values; we do not use the second parameter though)
there is some game with greater_is_better: True/False will return either positive or negative number
indices we get from CV in GridSearchCV

And...
grid = GridSearchCV(model,
                    scoring=scorer,
                    cv=5,
                    param_grid={'C': [1e0, 1e1, 1e2, 1e3],
                                'class_weight': ['balanced', None]},
                    refit='custom')
    
 grid.fit(xTrain, pd.DataFrame(yTrain))
 print(grid.score(xTrain, pd.DataFrame(yTrain)))


do not forget refit parameter in GridSearchCV
we pass target array as DataFrame here - it will help us to detect indices in the custom loss function

",cross-validation
how to perform stratified 10 fold cross validation for classification in matlab,"My implementation of usual K-fold cross-validation is pretty much like:
K = 10;
CrossValIndices = crossvalind('Kfold', size(B,2), K);

for i = 1: K
    display(['Cross validation, folds ' num2str(i)])
    IndicesI = CrossValIndices==i;
    TempInd = CrossValIndices;
    TempInd(IndicesI) = [];
    xTraining = B(:, CrossValIndices~=i);
    tTrain = T_new1(:, CrossValIndices~=i);

    xTest = B(:, CrossValIndices ==i);
    tTest = T_new1(:, CrossValIndices ==i);
end

But To ensure that the training, testing, and validating dataset have similar proportions of classes (e.g., 20 classes).I want use stratified sampling
technique.Basic purpose is to avoid class imbalance problem.I know about SMOTE technique but i want to apply this one.
","You can simply use crossvalind('Kfold', Group, K), where Group is the vector containing the class label for each observation. This will lead to sets where each group is proportionally abundant. 
",cross-validation
how to do kfold cross validation with txt files,"for example I have 10  .txt  files, in order to divide in test and train data.
(test_rate = 0.2 which means 2 test data and 8 train data)
In that case, the whole KFold cross validation should run 45 times (C[10,2])
how to do this in python? using sklearn's KFold function(code below) or other methods.
Much thanks for your reply.
import pandas as pd
import numpy as np
from sklearn.model_selection import KFold
KFold(n_splits=2, random_state=None, shuffle=False)

","Yes, you can use sklearn. You should use 5-Fold cross validation if you want your test data to be 0.2 of the whole dataset. Because in 5-Fold CV, you divide your data into 5 splits and use 4 of them for training, remaining 1 for testing, each time. So, n_splits should be 5.
fnames = np.array([
  ""1.txt"", 
  ""2.txt"", 
  ""3.txt"", 
  ""4.txt"", 
  ""5.txt"", 
  ""6.txt"", 
  ""7.txt"", 
  ""8.txt"", 
  ""9.txt"", 
  ""10.txt""
])
kfold = KFold(n_splits=5)

for i, (train_idx, test_idx) in enumerate(kfold.split(fnames)):
  print(f""Fold {i}"")
  train_fold, test_fold = fnames[train_idx], fnames[test_idx]
  print(f""\tlen train fold: {len(train_fold)}"")
  print(f""\tTrain fold: {train_fold}"")
  print(f""\tlen test fold: {len(test_fold)}"")
  print(f""\tTest fold: {test_fold}"")

This prints
Fold 0
    len train fold: 8
    Train fold: ['3.txt' '4.txt' '5.txt' '6.txt' '7.txt' '8.txt' '9.txt' '10.txt']
    len test fold: 2
    Test fold: ['1.txt' '2.txt']
Fold 1
    len train fold: 8
    Train fold: ['1.txt' '2.txt' '5.txt' '6.txt' '7.txt' '8.txt' '9.txt' '10.txt']
    len test fold: 2
    Test fold: ['3.txt' '4.txt']
Fold 2
    len train fold: 8
    Train fold: ['1.txt' '2.txt' '3.txt' '4.txt' '7.txt' '8.txt' '9.txt' '10.txt']
    len test fold: 2
    Test fold: ['5.txt' '6.txt']
Fold 3
    len train fold: 8
    Train fold: ['1.txt' '2.txt' '3.txt' '4.txt' '5.txt' '6.txt' '9.txt' '10.txt']
    len test fold: 2
    Test fold: ['7.txt' '8.txt']
Fold 4
    len train fold: 8
    Train fold: ['1.txt' '2.txt' '3.txt' '4.txt' '5.txt' '6.txt' '7.txt' '8.txt']
    len test fold: 2
    Test fold: ['9.txt' '10.txt']

You may want to give shuffle=True and a random_state in KFold for reproducibility.
",cross-validation
ordinal logistic regression or beta regression with a lasso regularization in r,"I was wondering if someone would know an R package that would allow me to fit an Ordinal Logistic regression with a LASSO regularization or, alternatively, a Beta regression still with the LASSO? And if you also know of a nice tutorial to help me code that in R (with appropriate cross-validation), that would be even better!
Some context: My response variable is a satisfaction score between 0 and 10 (actually, values lie between 2 and 10) so I can model it with a Beta regression or I can convert its values into ranked categories. My interest is to identify important variables explaining this score but as I have too many potential explanatory variables (p = 12) compared to my sample size (n = 105), I need to use a penalized regression method for model selection, hence my interest in the LASSO.
","The ordinalNet package does this. There's a paper with example here:
https://www.jstatsoft.org/article/download/v099i06/1440
Also the glmnetcr package: https://cran.r-project.org/web/packages/glmnetcr/vignettes/glmnetcr.pdf
",cross-validation
xgbregressor with weights and base_margin out of sample validation possible,"I have an old linear model which I wish to improve using XGBoost. I have the predictions from the old model, which I wish to use as a base margin. Also, due to the nature of what I'm modeling, I need to use weights. My old glm is a poisson regression with formula number_of_defaults/exposure ~ param_1 + param_2 and weights set to exposure (same as denominator in response variable). When training the new XGBoost model on data, I do this:
xgb_model = xgb.XGBRegressor(n_estimators=25,
                             max_depth=100,
                             max_leaves=100,
                             learning_rate=0.01,
                             n_jobs=4,
                             eval_metric=""poisson-nloglik"",
                             nrounds=50)

model = xgb_model.fit(X=X_train, y=y_train, sample_weight=_WEIGHT, base_margin=_BASE_MARGIN)

, where _WEIGHT and _BASE_MARGIN are the weights and predictions (popped out of X_train).
But how do I do cross validation or out of sample analysis when I need to specify weights and base margin?
As far as I see I can use sklearn and GridSearchCV, but then I would need to specify weights and base margin in XGBRegressor() (instead of in fit() as above). The equivalent of base_margin in XGBRegressor() is the argument base_score, but there is no argument for weight.
Also, I could potentially forget about doing cross-validation, and just use a training and test dataset, and I would then use eval_set argument in XGBRegressor(), but if I did that there is no way of specifying what is weight and what is base margin in the different sets.
Any guidance in the right direction is much appreciated!
","You can use cross_val_predict with fit_params argument, or GridSearchCV.fit with **fit_params.
Here is a working proof of concept
import xgboost as xgb
from sklearn import datasets
from sklearn.model_selection import cross_val_predict, GridSearchCV
import numpy as np

# Sample dataset
diabetes = datasets.load_diabetes()
X = diabetes.data[:150]
y = diabetes.target[:150]

xgb_model = xgb.XGBRegressor(n_estimators=5)
fit_params = dict(sample_weight=np.abs(X[:, 0]), base_margin=np.abs(X[:, 1]))

# Simple fit
xgb_model.fit(X, y, **fit_params)

# cross_val_predict
y_pred = cross_val_predict(xgb_model, X, y, cv=3, fit_params=fit_params)
print(y_pred.shape, y.shape)

# grid search
grid = GridSearchCV(xgb_model, param_grid={""n_estimators"": [5, 10, 15]})
grid.fit(X, y, **fit_params)

You can see what happen in the code source: here, here and here. The last link is where fit_params get indexing following cross validation splits.
",cross-validation
setting hidden layers and neurons in neuralnet and caret r,"I would like to cross-validate a neural network using the package neuralnet and caret.
The data df can be copied from this post. 
When running the neuralnet() function, there is an argument called hidden where you can set the hidden layers and neurons in each. Let's say I want 2 hidden layers with 3 and 2 neurons respectively. It would be written as hidden = c(3, 2).
However, as I want to cross-validate it, I decided to use the fantastic caret package. But when using the function train(), I do not know how to set the number of layers and neurons.
Does anyone know where can I add these numbers?
This is the code I ran:
nn <- caret::train(DC1 ~ ., data=df, 
                   method = ""neuralnet"", 
                   #tuneGrid = tune.grid.neuralnet,
                   metric = ""RMSE"",
                   trControl = trainControl (
                     method = ""cv"", number = 10,
                     verboseIter = TRUE
))

By the way, I am getting some warnings with the previous code:
predictions failed for Fold01: layer1=3, layer2=0, layer3=0 Error in cbind(1, pred) %*% weights[[num_hidden_layers + 1]] : 
  requires numeric/complex matrix/vector arguments

Ideas on how to solve it?
","When using neural net model in caret in order to specify the number of hidden units in each of the three supported layers you can use the parameters layer1, layer2 and layer3. I found out by checking the source.
library(caret)

grid <-  expand.grid(layer1 = c(32, 16),
                     layer2 = c(32, 16),
                     layer3 = 8)

Use case with BostonHousing data:
library(mlbench)

data(BostonHousing)

lets just select numerical columns for the example to make it simple:
BostonHousing[,sapply(BostonHousing, is.numeric)] -> df

nn <- train(medv ~ ., 
            data = df, 
            method = ""neuralnet"", 
            tuneGrid = grid,
            metric = ""RMSE"",
            preProc = c(""center"", ""scale"", ""nzv""), #good idea to do this with neural nets - your error is due to non scaled data
            trControl = trainControl(
              method = ""cv"",
              number = 5,
              verboseIter = TRUE)
            )

The part
preProc = c(""center"", ""scale"", ""nzv"")

is essential for the algorithm to converge, neural nets don't like unscaled features
Its super slow though.
nn
#output
Neural Network 

506 samples
 12 predictor

Pre-processing: centered (12), scaled (12) 
Resampling: Cross-Validated (5 fold) 
Summary of sample sizes: 405, 404, 404, 405, 406 
Resampling results across tuning parameters:

  layer1  layer2  RMSE      Rsquared   MAE     
  16      16           NaN        NaN       NaN
  16      32      4.177368  0.8113711  2.978918
  32      16      3.978955  0.8275479  2.822114
  32      32      3.923646  0.8266605  2.783526

Tuning parameter 'layer3' was held constant at a value of 8
RMSE was used to select the optimal model using the smallest value.
The final values used for the model were layer1 = 32, layer2 = 32 and layer3 = 8.

",cross-validation
how to refit a holtwinters exponential smoothing model on test data without reestimation of parameters,"I try to generate multiple one-step forecasts by using the HoltWinters method for exponential smoothing. I, therefore, split my data into training and testing and fit a HoltWinters model on my training set. Now I want to produce multiple one-step forecasts for the length of my test data, in a way that the model is ""updated"" with every new observation of the test set, without re-estimation of the parameters. Kind of in a way rolling origin cross-validation would be done.
I found the refit()-function, but that only seems to work with ARIMA models. Does someone know some code that could work? Or somewhere I could read about it?
","The Holt-Winters method is a special case of an ETS model. So you can use the ETS function from the fable package like this:
library(fable)
library(dplyr)

fit <- as_tsibble(USAccDeaths) %>%
  filter(lubridate::year(index) < 1977) %>%
  model(ets = ETS(value))

fit %>%
  refit(as_tsibble(USAccDeaths)) %>%
  report()
#> Series: value 
#> Model: ETS(A,N,A) 
#>   Smoothing parameters:
#>     alpha = 0.6570182 
#>     gamma = 0.004485907 
#> 
#>   Initial states:
#>      l[0]      s[0]     s[-1]    s[-2]     s[-3]    s[-4]    s[-5]    s[-6]
#>  9753.881 -30.20712 -286.0886 227.9668 -65.55104 977.0047 1673.521 806.8883
#>     s[-7]     s[-8]     s[-9]    s[-10]   s[-11]
#>  327.0867 -529.6172 -754.7921 -1551.379 -794.833
#> 
#>   sigma^2:  80017.07
#> 
#>      AIC     AICc      BIC 
#> 1131.232 1137.507 1160.828

Created on 2022-06-02 by the reprex package (v2.0.1)
If you specifically wanted the Holt-Winters variant, you could use
model(ets = ETS(value ~ trend(""A"") + season(""M"")))

for a multiplicative Holt-Winters model or
model(ets = ETS(value ~ trend(""A"") + season(""A"")))

for an additive Holt-Winters model
",cross-validation
is kfolds cross validation a smarter idea than using a validation set instead,"I have a somewhat large (~2000) set of medical images I plan to use to train a CV model (using efficentnet architecture) in my workplace. In preparation for this, I was reading up on some good practices for training medical images. I have split the dataset by patients to prevent leakages and split my data in train:test:val in the order of 60:20:20. However, I read that k-folds cross validation was a newer practice then using a validation set, but I was recommended away from doing so as k-folds is supposed to be far more complicated. What would you recommend in this instance, and are there any other good practices to adopt?
","Common Practice
A train:test split with cross-validation on the training set is part of the standard workflow in many machine learning modules. For an example and further details, I recommend the excellent sklearn article on it.
Implementation
The implementation may be a little trickier but should not be prohibitive given the many code examples assuming you are using TF or Pytorch (see e.g. this SO question).
Should you be using k-fold cross validation?
Compared to a single validation set, k-fold cross-validation avoids over-fitting hyperparameters to a fixed validation set and makes better use of the available data by utilizing the entire training set across the folds, albeit at greater computational cost. Whether or not this makes a big difference depends on your task. 2000 images does not sound like a lot in computer vision terms, so making good use of the data may be relevant to you, especially if you plan on tuning hyperparameters.
",cross-validation
sklearn pipeline  trying to count the number of times an estimator is called,"I'm trying to count the number of times LogisticRegression is called in this pipeline, so I extended the class and overrode .fit(). It was supposed to be simple but it generates this weird error:
TypeError: float() argument must be a string or a number, not 'MyLogistic'
where MyLogistic is the new class. You should be able to reproduce the whole thing if you copy and paste the code.
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import (GridSearchCV, StratifiedKFold)
from sklearn.feature_selection import SelectFromModel
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.datasets import load_iris
import numpy as np

class MyLogistic(LogisticRegression):
    __call_counter = 0
    def fit(X, y, sample_weight=None):
        print(""MyLogistic fit is called."")
        MyLogistic._MyLogistic__call_counter += 1
        # fit() returns self.
        return super().fit(X, y, sample_weight)

# If I use this ""extension"", everything works fine.
#class MyLogistic(LogisticRegression):
#    pass
    
initial_logistic = MyLogistic(solver=""liblinear"", random_state = np.random.RandomState(18))
final_logistic = LogisticRegression(solver=""liblinear"", random_state = np.random.RandomState(20))
# prefit = False by default
select_best = SelectFromModel(estimator = initial_logistic, threshold = -np.inf)

select_k_best_pipeline = Pipeline(steps=[
    ('first_scaler', StandardScaler(with_mean = False)),
    # initial_logistic will be called from select_best, prefit = false by default.
    ('select_k_best', select_best),
    ('final_logit', final_logistic)
])

select_best_grid = {'select_k_best__estimator__C' : [0.02, 0.03],
                    'select_k_best__max_features': [1, 2],
                    'final_logit__C' : [0.01, 0.5, 1.0]}

skf = StratifiedKFold(n_splits = 3, shuffle = True, random_state = 17)

logit_best_searcher = GridSearchCV(estimator = select_k_best_pipeline, param_grid = select_best_grid, cv = skf, 
                               scoring = ""roc_auc"", n_jobs = 6, verbose = 4)

X, y = load_iris(return_X_y=True)
logit_best_searcher.fit(X, y > 0)
print(""Best hyperparams: "", logit_best_searcher.best_params_)

","You just forgot to put self as the first parameter of the fit signature.  So the call is getting X=self, and when trying to check the input X it at some point tries to convert to float, hence the error message.
There's still some weirdness around the parallelization; I get the counter equal to 1. Setting n_jobs=1 instead, I get the correct 37 for the counter (2x2x3 hyperparameter candidates on x3 folds, +1 for the final refit).
",cross-validation
metric for ml algorithm evaluation,"I have a question. Is the best score from GridSearchCV, which corresponds to mean cross-validation score, the right metric to evaluate an algorithm trained with unbalanced data?
","GridSearchCV can be used to find appropriate parameter values for your model.
For the right metric to evaluate an algorithm trained with unbalanced data, you want to look at the area under the precision-recall curve (PR AUC) or 'average precision' or maybe even a cost-sensitive one (Jason Brownlee has a bunch of blogs on this topic).
",cross-validation
trying to understand fb prophet cross validation,"I have a dataset with 84 Monthly Sales (from 01/2013 to 12/2019) - just months, not days.
Month 01 | Sale 1

Month 02 | Sale 2

Month 03 | Sale 3

....     |   ...

Month 84 | Sale 84

By visualization it looks like that the model fits very well... but I need to check it....
So what I understood is that cross val does not support Months, and so what I did was convert to use it w/ days(although there is no day info into my original df)...
I wanted to try my model w/ the first five years(60 months) and leave the 2 remaining years(24 months) to see how well the model is predicting....
So i did something like:
cv_results = cross_validation( model = prophet, initial='1825 days', period='30 days', horizon = '60 days')

Does this make sense?
I did not get the concept of cut off dates and forecast periods
","I struggled with this for a while as well.  But here is how it works.  The initial model will be trained on the first 1,825 days of data.  It will forecast the next 60 days of data (because horizon is set to 60).  The model will then train on the initial period + the period (1,825 + 30 days in this case) and forecast the next 60 days.  It will continued like this, adding another 30 days to the training data and then forecasting for the next 60 until there is no longer enough data to do this.
In summary, period is how much data to add to the training data set in every iteration of cross-validation, and horizon is how far out it will forecast.
",cross-validation
how to access scikit learn nested crossvalidation scores,"I'm using python and I would like to use nested cross-validation with scikit learn. I have found a very good example:
NUM_TRIALS = 30
non_nested_scores = np.zeros(NUM_TRIALS)
nested_scores = np.zeros(NUM_TRIALS)
# Choose cross-validation techniques for the inner and outer loops,
# independently of the dataset.
# E.g ""LabelKFold"", ""LeaveOneOut"", ""LeaveOneLabelOut"", etc.
inner_cv = KFold(n_splits=4, shuffle=True, random_state=i)
outer_cv = KFold(n_splits=4, shuffle=True, random_state=i)

# Non_nested parameter search and scoring
clf = GridSearchCV(estimator=svr, param_grid=p_grid, cv=inner_cv)
clf.fit(X_iris, y_iris)
non_nested_scores[i] = clf.best_score_

# Nested CV with parameter optimization
nested_score = cross_val_score(clf, X=X_iris, y=y_iris, cv=outer_cv)
nested_scores[i] = nested_score.mean()

How can the best set of parameters as well as all set of parameters (with their corresponding score) from the nested cross-validation be accessed?
","You cannot access individual params and best params from cross_val_score. What cross_val_score does internally is clone the supplied estimator and then call fit and score methods on it with given X, y on individual estimators.
If you want to access the params at each split you can use:
#put below code inside your NUM_TRIALS for loop
cv_iter = 0
temp_nested_scores_train = np.zeros(4)
temp_nested_scores_test = np.zeros(4)
for train, test in outer_cv.split(X_iris):
    clf.fit(X_iris[train], y_iris[train])
    temp_nested_scores_train[cv_iter] = clf.best_score_
    temp_nested_scores_test[cv_iter] = clf.score(X_iris[test], y_iris[test])
    #You can access grid search's params here
nested_scores_train[i] = temp_nested_scores_train.mean()
nested_scores_test[i] = temp_nested_scores_test.mean()

",cross-validation
stratifiedkfold vs kfold in scikitlearn,"I use this code to test KFold and StratifiedKFold.
import numpy as np
from sklearn.model_selection import KFold,StratifiedKFold

X = np.array([
    [1,2,3,4],
    [11,12,13,14],
    [21,22,23,24],
    [31,32,33,34],
    [41,42,43,44],
    [51,52,53,54],
    [61,62,63,64],
    [71,72,73,74]
])

y = np.array([0,0,0,0,1,1,1,1])

sfolder = StratifiedKFold(n_splits=4,random_state=0,shuffle=False)
floder = KFold(n_splits=4,random_state=0,shuffle=False)

for train, test in sfolder.split(X,y):
    print('Train: %s | test: %s' % (train, test))
print(""StratifiedKFold done"")

for train, test in floder.split(X,y):
    print('Train: %s | test: %s' % (train, test))
print(""KFold done"")

I found that StratifiedKFold can keep the proportion of labels, but KFold can't.
Train: [1 2 3 5 6 7] | test: [0 4]
Train: [0 2 3 4 6 7] | test: [1 5]
Train: [0 1 3 4 5 7] | test: [2 6]
Train: [0 1 2 4 5 6] | test: [3 7]
StratifiedKFold done
Train: [2 3 4 5 6 7] | test: [0 1]
Train: [0 1 4 5 6 7] | test: [2 3]
Train: [0 1 2 3 6 7] | test: [4 5]
Train: [0 1 2 3 4 5] | test: [6 7]
KFold done

It seems that StratifiedKFold is better, so should KFold not be used?
When to use KFold instead of StratifiedKFold?
","I think you should ask ""When to use StratifiedKFold instead of KFold?"".
You need to know what ""KFold"" and ""Stratified"" are first.

KFold is a cross-validator that divides the dataset into k folds.


Stratified is to ensure that each fold of dataset has the same
proportion of observations with a given label.

So, it means that StratifiedKFold is the improved version of KFold
Therefore, the answer to this question is we should prefer StratifiedKFold over KFold when dealing with classification tasks with imbalanced class distributions.

FOR EXAMPLE
Suppose that there is a dataset with 16 data points and imbalanced class distribution. In the dataset, 12 of data points belong to class A and the rest (i.e. 4) belong to class B. The ratio of class B to class A is 1/3. If we use StratifiedKFold and set k = 4, then, in each iteration, the training sets will include 9 data points from class A and 3 data points from class B while the test sets include 3 data points from class A and 1 data point from class B.
As we can see, the class distribution of the dataset is preserved in the splits by StratifiedKFold while KFold does not take this into consideration.
",cross-validation
get all prediction values for each cv in gridsearchcv,"I have a time-dependent data set, where I (as an example) am trying to do some hyperparameter tuning on a Lasso regression.
For that I use sklearn's TimeSeriesSplit instead of regular Kfold CV, i.e. something like this:
tscv = TimeSeriesSplit(n_splits=5)

model = GridSearchCV(
    estimator=pipeline,
    param_distributions= {""estimator__alpha"": np.linspace(0.05, 1, 50)},
    scoring=""neg_mean_absolute_percentage_error"",
    n_jobs=-1,
    cv=tscv,
    return_train_score=True,
    max_iters=10,
    early_stopping=True,
)

model.fit(X_train, y_train)

With this I get a model, which I can then use for predictions etc. The idea behind that cross validation is based on this:

However, my issue is that I would actually like to have the predictions from all the test sets from all cv's. And I have no idea how to get that out of the model ?
If I try the cv_results_ I get the score (from the scoring parameter) for each split and each hyperparameter. But I don't seem to be able to find the prediction values for each value in each test split. And I actually need that for some backtesting. I don't think it would be ""fair"" to use the final model to predict the previous values. I would imagine there would be some kind of overfitting in that case.
So yeah, is there any way for me to extract the predicted values for each split ?
","You can have custom scoring functions in GridSearchCV.With that you can predict outputs with the estimator given to the GridSearchCV in that particular fold.
from the documentation scoring parameter is

Strategy to evaluate the performance of the cross-validated model on the test set.

from sklearn.metrics import mean_absolute_percentage_error
def custom_scorer(clf, X, y):
     y_pred = clf.predict(X)
     # save y_pred somewhere
     return -mean_absolute_percentage_error(y, y_pred)

model = GridSearchCV(estimator=pipeline,
                     scoring=custom_scorer)

The input X and y in the above code came from the test set. clf is the given pipeline to the estimator parameter.
Obviously your estimator should implement the predict method (should be a valid model in scikit-learn). You can add other scorings to the custom one to avoid non-sense scores from the custom function.
",cross-validation
39pca39 object has no attribute 39predict39,"I have an issue with my code:
PCA runs fine, but when I combine it with grid search I get this error:
error
error
This is my code:
pca = PCA()
param_grid = {'n_components': range(1,50)}
GridSearchCV(pca, param_grid, cv=5 ,scoring='f1', 
return_train_score=True, n_jobs=-1, 
error_score=""raise"").fit(prepared_data_train, y_train)

","checkout this:
remove scoring as F1 does not make sense for PCA by default.
Also, make sure to normalize your data before performing PCA; the reason is explained here.
pca = PCA()

scaler = StandardScaler()


pipe = Pipeline(steps=[(""scaler"", scaler), (""pca"", pca)])

param_grid = {'pca__n_components': range(1,50)}

search = GridSearchCV(pipe, param_grid, cv=5 , return_train_score=True, n_jobs=-1, error_score=""raise"").fit(X_digits, y_digits)

print(""Best parameter (CV score=%0.3f):"" % search.best_score_)
print(search.best_params_)

here is a good example from documentation for further exploration.
",cross-validation
pandas create crossvalidation based on specific columns,"I have a dataframe of few hundreds rows , that can be grouped to ids as follows:
df = Val1 Val2 Val3 Id
      2     2   8    b
      1     2   3    a
      5     7   8    z
      5     1   4    a
      0     9   0    c
      3     1   3    b
      2     7   5    z
      7     2   8    c
      6     5   5    d
...
      5     1   8    a
      4     9   0    z
      1     8   2    z

I want to use GridSearchCV , but with a custom CV that will assure that all the rows from the same ID will always be on the same set. 
So either all the rows if a are in the test set , or all of them are in the train set - and so for all the different IDs.
I want to have 5 folds - so 80% of the ids will go to the train and 20% to the test.
I understand that it can't guarentee that all folds will have the exact same amount of rows - since one ID might have more rows than the other.
What is the best way to do so?
","As stated, you can provide cv with an iterator. You can use GroupShuffleSplit(). For example, once you use it to split your dataset, you can put the result within GridSearchCV() for the cv parameter.
",cross-validation
hyperparameter tuning job in sagemaker with cross valdiation,"I managed to get something along those lines to work. This is great but to be more on the save side (i.e. not rely too much on the train validation split) one should really use cross validation. I am curious, if this can also be achieved via Sagemaker hyperparameter tuning jobs? I googled extensively ...
","It is not possible through HPO.
You need to add additional step in your workflow to achieve cross-validation.
",cross-validation
ltrandomizedsearchcvgt pass the estimator obtained after fitting to scoring function as a parameter,"Suppose I want to do a RandomizedSearchCV with custom both estimator and scorer :
RandomizedSearchCV(cxCustomLogReg(), search_space,
                   n_iter=50, scoring=scorer) 

May it be possible, during randomized search, to pass the estimator obtained after fitting cxCustomLogReg() to scorer function as a parameter (and not only y_true/y_pred values as usual)?
","This is actually the expected format. When scoring is a callable, it should have signature (fitted_estimator, X_test, y_test).
You may be using the helper make_scorer, which converts a metric function with signature (y_true, y_pred) into a scoring function as described above. Just don't use that helper, and define your scorer directly.
",cross-validation
gridsearchcv on logisticregression in scikitlearn,"I am trying to optimize a logistic regression function in scikit-learn by using a cross-validated grid parameter search, but I can't seem to implement it. 
It says that Logistic Regression does not implement a get_params() but on the documentation it says it does. How can I go about optimizing this function on my ground truth? 
>>> param_grid = {'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000] }
>>> clf = GridSearchCV(LogisticRegression(penalty='l2'), param_grid)
>>> clf
GridSearchCV(cv=None,
       estimator=LogisticRegression(C=1.0, intercept_scaling=1, dual=False, fit_intercept=True,
          penalty='l2', tol=0.0001),
       fit_params={}, iid=True, loss_func=None, n_jobs=1,
       param_grid={'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000]},
       pre_dispatch='2*n_jobs', refit=True, score_func=None, verbose=0)
>>> clf = clf.fit(gt_features, labels)
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""/Library/Python/2.7/site-packages/scikit_learn-0.14_git-py2.7-macosx-10.8-x86_64.egg/sklearn/grid_search.py"", line 351, in fit
    base_clf = clone(self.estimator)
  File ""/Library/Python/2.7/site-packages/scikit_learn-0.14_git-py2.7-macosx-10.8-x86_64.egg/sklearn/base.py"", line 42, in clone
    % (repr(estimator), type(estimator)))
TypeError: Cannot clone object 'LogisticRegression(C=1.0, intercept_scaling=1, dual=False, fit_intercept=True,
          penalty='l2', tol=0.0001)' (type <class 'scikits.learn.linear_model.logistic.LogisticRegression'>): it does not seem to be a scikit-learn estimator a it does not implement a 'get_params' methods.
>>> 

","The class name scikits.learn.linear_model.logistic.LogisticRegression refers to a very old version of scikit-learn. The top level package name is now sklearn since at least 2 or 3 releases. It's very likely that you have old versions of scikit-learn installed concurrently in your python path. Uninstall them all, then reinstall 0.14 or later and try again.
",cross-validation
difference between groupsplitshuffle and groupkfolds,"As the title says, I want to know the difference between sklearn's GroupKFold and GroupShuffleSplit.
Both make train-test splits given for data that has a group ID, so the groups don't get separated in the split. I checked on one train/test set for each function and they both look like they make a pretty good stratification, but if someone could confirm that all splits do that, it would be great.
I made a test with both, for 10 splits:
gss = GroupShuffleSplit(n_splits=10, train_size=0.8, random_state=42)

 

for train_idx, test_idx in gss.split(X,y,groups):

    print(""train:"", train_idx, ""test:"", test_idx)

train: [ 1  2  3  4  5 11 12 13 14 15 16 17 19 20] test: [ 0  6  7  8  9 10 18]

train: [ 1  2  3  4  5  6  7  8  9 10 12 13 14 18 19 20] test: [ 0 11 15 16 17]

train: [ 0  1  3  4  5  6  7  8  9 10 12 13 14 18 19 20] test: [ 2 11 15 16 17]

train: [ 0  2  3  4 11 12 13 14 15 16 17 18 19 20] test: [ 1  5  6  7  8  9 10]

train: [ 0  1  3  4  5  6  7  8  9 10 11 15 16 17 19 20] test: [ 2 12 13 14 18]

train: [ 1  2  3  4  5  6  7  8  9 10 11 15 16 17 18] test: [ 0 12 13 14 19 20]

train: [ 0  1  2  3  4  6  7  8  9 10 11 12 13 14 15 16 17] test: [ 5 18 19 20]

train: [ 0  1  3  4  6  7  8  9 10 11 15 16 17 18 19 20] test: [ 2  5 12 13 14]

train: [ 0  1  3  4  5 12 13 14 15 16 17 18 19 20] test: [ 2  6  7  8  9 10 11]

train: [ 0  2  3  4  5 11 12 13 14 15 16 17 19 20] test: [ 1  6  7  8  9 10 18]

 

group_kfold = GroupKFold(n_splits=10)

 

for train_idx, test_idx in group_kfold.split(X,y,groups):

    print(""train:"", train_idx, ""test:"", test_idx)

train: [ 0  1  2  3  4  5 11 12 13 14 15 16 17 18 19 20] test: [ 6  7  8  9 10]

train: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 18 19 20] test: [15 16 17]

train: [ 0  1  2  3  4  5  6  7  8  9 10 11 15 16 17 18 19 20] test: [12 13 14]

train: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18] test: [19 20]

train: [ 0  1  2  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20] test: [3 4]

train: [ 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 19 20] test: [ 0 18]

train: [ 0  1  2  3  4  5  6  7  8  9 10 12 13 14 15 16 17 18 19 20] test: [11]

train: [ 0  1  2  3  4  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20] test: [5]

train: [ 0  1  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20] test: [2]

train: [ 0  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20] test: [1]

","The documentation for the un-Group versions make this clearer.  KFold splits into k folds and then lumps those together into different train/test splits, whereas ShuffleSplit repeatedly makes the train/test splits directly. In particular, each sample is tested on exactly once in KFold, but can be tested on zero or multiple times in ShuffleSplit.
",cross-validation
randomforestclassifier instance not fitted yet call 39fit39 with appropriate arguments before using this method,"I am trying to train a decision tree model, save it, and then reload it when I need it later. However, I keep getting the following error: 

This DecisionTreeClassifier instance is not fitted yet. Call 'fit'
  with appropriate arguments before using this method.

Here is my code: 
X_train, X_test, y_train, y_test = train_test_split(data, label, test_size=0.20, random_state=4)

names = [""Decision Tree"", ""Random Forest"", ""Neural Net""]

classifiers = [
    DecisionTreeClassifier(),
    RandomForestClassifier(),
    MLPClassifier()
    ]

score = 0
for name, clf in zip(names, classifiers):
    if name == ""Decision Tree"":
        clf = DecisionTreeClassifier(random_state=0)
        grid_search = GridSearchCV(clf, param_grid=param_grid_DT)
        grid_search.fit(X_train, y_train_TF)
        if grid_search.best_score_ > score:
            score = grid_search.best_score_
            best_clf = clf
    elif name == ""Random Forest"":
        clf = RandomForestClassifier(random_state=0)
        grid_search = GridSearchCV(clf, param_grid_RF)
        grid_search.fit(X_train, y_train_TF)
        if grid_search.best_score_ > score:
            score = grid_search.best_score_
            best_clf = clf

    elif name == ""Neural Net"":
        clf = MLPClassifier()
        clf.fit(X_train, y_train_TF)
        y_pred = clf.predict(X_test)
        current_score = accuracy_score(y_test_TF, y_pred)
        if current_score > score:
            score = current_score
            best_clf = clf


pkl_filename = ""pickle_model.pkl""  
with open(pkl_filename, 'wb') as file:  
    pickle.dump(best_clf, file)

from sklearn.externals import joblib
# Save to file in the current working directory
joblib_file = ""joblib_model.pkl""  
joblib.dump(best_clf, joblib_file)

print(""best classifier: "", best_clf, "" Accuracy= "", score)

Here is how I load the model and test it: 
#First method
with open(pkl_filename, 'rb') as h:
    loaded_model = pickle.load(h) 
#Second method 
joblib_model = joblib.load(joblib_file)

As you can see, I have tried two ways of saving it but none has worked.
Here is how I tested: 
print(loaded_model.predict(test)) 
print(joblib_model.predict(test)) 

You can clearly see that the models are actually fitted and if I try with any other models such as SVM, or Logistic regression the method works just fine. 
","The problem is in this line:
best_clf = clf

You have passed clf to grid_search, which clones the estimator and fits the data on those cloned models. So your actual clf remains untouched and unfitted.
What you need is 
best_clf = grid_search

to save the fitted grid_search model.
If you dont want to save the entire contents of grid_search, you can use the best_estimator_ attribute of grid_search to get the actual cloned fitted model.
best_clf = grid_search.best_estimator_

",cross-validation
how to use nls with caret to do crossvalidation,"I have fit several models using nls to the same data and am trying to figure out how to use caret to do K-fold cross-validation (eg., here). This SO question asked a general question about using nls in caret for a single model. However, the answer referred them to this resource, which is beyond my understanding (ie, how to adapt for nls), especially for fitting several different nls functions.
Here is example data and examples of the model objects fit with nls:
library(""caret"")

df <-
  structure(list(c = c(123.86, 208.75, 141.5, 230.73, 143.4, 209.31, 
  161.15, 130.87, 232.05, 121.61, 176.31, 139.01, 131.92, 156.61, 
  150.05, 121, 134.12, 146.83, 181.39, 115, 147.87, 161.49, 107.65, 
  115.51, 144.11), q = c(0.028, 0.004, 0.049, 0.001, 0.049, 0.004, 
  0.016, 0.015, 0.003, 0.026, 0.002, 0.009, 0.148, 0.012, 0.017, 
  0.086, 0.02, 0.038, 0.003, 0.031, 0.011, 0.032, 0.132, 0.093, 
  0.026)), row.names = c(NA, -25L), class = c(""tbl_df"", ""tbl"", 
  ""data.frame""))

# Model 1
eq1 <- function(q,a,n) (a*q**(-1/n))
eq1_fit <- nls(c ~ eq1(q,a,n), data=df,start=list(a=380, n=5))

# Model 2
eq2 <- function(q,h,g,n,c0) (h*exp(-g*q**(1/n))+c0)
eq2_fit <- nls(c ~ eq2(q,h,g,n=5,c0=6), data=df,start=list(h=100,g=1))

There are several additional models all fit using nls on the same data that I would like to do 5-fold or 10-fold CV on following the example or something similar to shown here. A solution using tidymodels and workflowsets would also work too. Any help is appreciated!
","You could do this with the vfold_cv() function in the rsample package, which is in the tidymodels ecosystem.
df <-
  structure(list(c = c(123.86, 208.75, 141.5, 230.73, 143.4, 209.31, 
                       161.15, 130.87, 232.05, 121.61, 176.31, 139.01, 131.92, 156.61, 
                       150.05, 121, 134.12, 146.83, 181.39, 115, 147.87, 161.49, 107.65, 
                       115.51, 144.11), q = c(0.028, 0.004, 0.049, 0.001, 0.049, 0.004, 
                                              0.016, 0.015, 0.003, 0.026, 0.002, 0.009, 0.148, 0.012, 0.017, 
                                              0.086, 0.02, 0.038, 0.003, 0.031, 0.011, 0.032, 0.132, 0.093, 
                                              0.026)), row.names = c(NA, -25L), class = c(""tbl_df"", ""tbl"", 
                                                                                          ""data.frame""))


library(rsample)
library(tidyverse)
## define equations 
eq1 <- function(q,a,n) (a*q**(-1/n))
eq2 <- function(q,h,g,n,c0) (h*exp(-g*q**(1/n))+c0)


eq1_fit <- nls(c ~ eq1(q,a,n), data=df,start=list(a=380, n=5))
eq2_fit <- nls(c ~ eq2(q,h,g,n=5,c0=6), data=df,start=list(h=100,g=1))

y <- df$c
## create the squared errors getting model predictions
## from the assessment partition
r1 <- (y - predict(eq1_fit, newdata=df))
r2 <- (y - predict(eq2_fit, newdata=df))


e1 <- (y - predict(eq1_fit, newdata=df))^2
e2 <- (y - predict(eq2_fit, newdata=df))^2





## define function that will do the model fitting and assessment
cv_mods <- function(split, ...){
  ## fit models with the analysis partition
  eq1_fit <- nls(c ~ eq1(q,a,n), data=analysis(split),start=list(a=380, n=5))
  eq2_fit <- nls(c ~ eq2(q,h,g,n=5,c0=6), data=analysis(split),start=list(h=100,g=1))
  ## take the dependent variable from the assessment partition
  y <- assessment(split)$c
  ## create the residuals  getting model predictions
  ## from the assessment partition
  e1 <- (y - predict(eq1_fit, newdata=assessment(split)))
  e2 <- (y - predict(eq2_fit, newdata=assessment(split)))
  ## return the cross-validated residuals from both models as 
  ## a data frame. 
  data.frame(e1 = e1, e2 = e2)
}

## estimate the cross-validation 
## the vfold_cv function sets up the cross-validation partitions
## I used 10 repeats here for speed, but in the ""real world"" you 
## would probably want lots more 
out <- vfold_cv(df,
                v=10,
                repeats = 10) %>%
  ## estimate the cv on all of the partitions
  mutate(err = map(splits,
                   cv_mods)) %>% 
  ## unnest the error column to turn it into two 
  ## columns in your data frame
  unnest(err)  

## First analysis: Pr(mod1 better than mod2)
out1 <- out %>% 
  ## group by repeat 
  group_by(id) %>% 
  ## calculate the standard deviation of the errors for each level of id
  summarise(across(e1:e2, ~sd(.x))) %>% 
  ## calculate the probability across repeats that model 1 is better
  ## than model 2
  summarise(p_eq1_better = mean(e1<e2))

out1
#> # A tibble: 1 × 1
#>   p_eq1_better
#>          <dbl>
#> 1            1


## alternatively, follow similar steps to above, 
## but get the average cross-validation error 
## for each model: 
out2 <- out %>% group_by(id) %>% 
  ## sum up the sums of squared errors across partitions
  summarise(across(e1:e2, ~sd(.x))) %>% 
  ## calculate average CV error:
  summarise(across(e1:e2, ~mean(.x)))

out2
#> # A tibble: 1 × 2
#>      e1    e2
#>   <dbl> <dbl>
#> 1  19.7  21.2

Created on 2022-04-12 by the reprex package (v2.0.1)
",cross-validation
how to use an explicit validation set with predefined split fold,"I have explicit train, test and validation sets as 2d arrays:
X_train.shape
(1400, 38785)
X_val.shape
(200, 38785)
X_test.shape
(400, 38785)

I am tuning the alpha parameter and need advice about how I can use the predefined validation set in it:

from sklearn.naive_bayes import MultinomialNB
from sklearn.model_selection import GridSearchCV, PredefinedSplit

nb = MultinomialNB()
nb.fit(X_train, y_train)

params = {'alpha': [0.1, 1, 3, 5, 10,12,14]}
# how to use on my validation set?
# ps = PredefinedSplit(test_fold=?)

gs = GridSearchCV(nb, param_grid=params, cv = ps,  return_train_score=True, scoring='f1')

gs.fit(X_train, y_train)


My results are as following so far.
# on my validation set, alpha = 5
gs.fit(X_val, y_val)
print('Grid best parameter', gs.best_params_)
Grid best parameter:  {'alpha': 5}

# on my training set, alpha = 10
Grid best parameter:  {'alpha': 10}

I have read the following questions and documentation yet I am not sure how to use PredefinedSplit() in my case. Thank you.
Order between using validation, training and test sets
https://scikit-learn.org/stable/modules/cross_validation.html#predefined-fold-splits-validation-sets
","You can achieve your desired outcome by merging X_train and X_val, and passing PredefinedSplit a list of labels, with -1 indicating training data and 1 indicating validation data. IE,

X = np.concatenate((X_train, X_val))
y = np.concatenate((y_train, y_val))
ps = PredefinedSplit(np.concatenate((np.zeros(len(x_train) - 1, np.ones(len(x_val))))

gs = GridSearchCV(nb, param_grid=params, cv = ps,  return_train_score=True, scoring='f1')

gs.fit(X, y)  # not X_train, y_train

However, unless there is very a good reason for you holding out a separate validation set, you will likely have less overfitting if you use k-fold cross validation for your hyperparameter tuning rather than using a dedicated validation set.
",cross-validation
r caret train failed for repeatedcv with factor predictors,"The following function shall be used with Caret's train() function. Without any factor variables or without cross-validation it works fine.
The problems appear when using factors as predictors and repeatedcv, because in the folds not all the factors are present but still appear within the factor levels:
Consider the following adapted cforest model (from the package partykit):
cforest_partykit <- list(label = ""Conditional Inference Random Forest with partykit"",
          library = c(""partykit"", ""party""),
          loop = NULL,
          type = c(""Classification"", ""Regression""),
          parameters = data.frame(parameter = 'mtry',
                                  class = 'numeric',
                                  label = ""#Randomly Selected Predictors""),
          grid = function(x, y, len = NULL, search = ""grid""){
            if(search == ""grid"") {
              out <- data.frame(mtry = caret::var_seq(p = ncol(x), 
                                                      classification = is.factor(y), 
                                                      len = len))
            } else {
              out <- data.frame(mtry = unique(sample(1:ncol(x), replace = TRUE, size = len)))
            }
            out
          },
          fit = function(x, y, wts, param, lev, last, classProbs, ...) {
            
             # make consistent factor levels
                if(any(sapply(x, is.factor))){                      
                  fac_col_names <- names(grep(""factor"", sapply(x, class), value=TRUE))
                  # assign present levels to each subset
                  for (i in 1:length(fac_col_names)) {                        
                    x[, which(names(x) == fac_col_names[i])] <- factor(x[, which(names(x) == fac_col_names[i])], 
                                                                       levels = as.character(unique(x[, which(names(x) == fac_col_names[i])])))                       
                  }              
                }
                 

            dat <- if(is.data.frame(x)) x else as.data.frame(x, stringsAsFactors = TRUE)
            dat$.outcome <- y
            theDots <- list(...)
            
            if(any(names(theDots) == ""mtry"")) # # change controls to mtry?
            {
              theDots$mtry <- as.integer(param$mtry) # remove gtcrl 
              theDots$mtry
              theDots$mtry <- NULL
              
            } else mtry <- min(param$mtry, ncol(x))
            
            ## pass in any model weights
            if(!is.null(wts)) theDots$weights <- wts
            
            modelArgs <- c(list(formula = as.formula(.outcome ~ .),
                                data = dat,
                                mtry = mtry), # change controls to mtry?
                           theDots)
            
            out <- do.call(partykit::cforest, modelArgs)
            out
          },
          predict = function(modelFit, newdata = NULL, submodels = NULL) {
            if(!is.null(newdata) && !is.data.frame(newdata)) newdata <- as.data.frame(newdata, stringsAsFactors = TRUE)

            # make consistent factor levels
                if(any(sapply(newdata, is.factor))){                      
                  fac_col_names <- names(grep(""factor"", sapply(newdata, class), value=TRUE))
                  # assign present levels to each subset
                  for (i in 1:length(fac_col_names)) {                       
                    newdata[, which(names(newdata) == fac_col_names[i])] <- factor(newdata[, which(names(newdata) == fac_col_names[i])], 
                                                                       levels = as.character(unique(newdata[, which(names(newdata) == fac_col_names[i])])))                      
                  }                     
                }
                

            ## party builds the levels into the model object, so I'm
            ## going to assume that all the levels will be passed to
            ## the output
            out <- partykit:::predict.cforest(modelFit, newdata = newdata, OOB = TRUE) # predict_party, id?
            if(is.matrix(out)) out <- out[,1]
            if(!is.null(modelFit$'(response)')) out <- as.character(out) #  if(!is.null(modelFit@responses@levels$.outcome)) out <- as.character(out)
            
            out
          },
          prob = function(modelFit, newdata = NULL, submodels = NULL) { # submodels ?
            if(!is.null(newdata) && !is.data.frame(newdata)) newdata <- as.data.frame(newdata, stringsAsFactors = TRUE)
            obsLevels <- levels(modelFit$'(response)')
            rawProbs <- partykit::predict.cforest(modelFit, newdata = newdata, OOB = TRUE) # predict(, type=""prob) ? id?
            probMatrix <- matrix(unlist(rawProbs), ncol = length(obsLevels), byrow = TRUE)
            out <- data.frame(probMatrix)
            colnames(out) <- obsLevels
            rownames(out) <- NULL
            out
          },
          predictors = function(x, ...) {
            vi <- partykit::varimp(x, ...)
            names(vi)[vi != 0]
          },
          varImp = function(object, ...) {
            variableImp <- partykit::varimp(object, ...)
            out <- data.frame(Overall = variableImp)
            out
          },
          tags = c(""Random Forest"", ""Ensemble Model"", ""Bagging"", ""Implicit Feature Selection"", ""Accepts Case Weights""),
          levels = function(x) levels(x@data@get(""response"")[,1]),
          sort = function(x) x[order(x[,1]),],
          oob = function(x) {
            obs <- x@data@get(""response"")[,1]
            pred <- partykit:::predict.cforest(x, OOB = TRUE, newdata = NULL)
            postResample(pred, obs)
          })

When applying it within train and repeatedcv using a data frame with a factor predictor variable, an error occurs:
library(caret)
library(party)
library(partykit)

dat <- as.data.frame(ChickWeight)[1:20,]
dat$class <- as.factor(rep(letters[seq( from = 1, to = 20)], each=1))

# specifiy folds with CreateMultiFolds
set.seed(43, kind = ""Mersenne-Twister"", normal.kind = ""Inversion"")
folds_train <- caret::createMultiFolds(y = dat$weight,
                                   k = 3,   
                                   times = 2)

# specifiy trainControl for tuning mtry and with specified folds
finalcontrol <- caret::trainControl(search = ""grid"", method = ""repeatedcv"", number = 3, repeats = 2, 
                                    index = folds_train, 
                                    savePred = T)

preds <- dat[,2:5]
response <- dat[,1]

# tune hyperparameter mtry and build final model
tunegrid <- expand.grid(mtry=c(1,2,3,4)) 
#set.seed(42, kind = ""Mersenne-Twister"", normal.kind = ""Inversion"")
model <- caret::train(x = preds, # predictors
                      y = response, # response
                      method = cforest_partykit,
                      metric = ""RMSE"", 
                      tuneGrid = tunegrid, 
                      trControl = finalcontrol,
                      ntree = 150)

warnings()
1: predictions failed for Fold1.Rep1: mtry=1 Error in model.frame.default(object$predictf, data = newdata, na.action = na.pass, : factor class has new levels a, c, g, k, m, p, s, t
The aim is to identify the levels of each fold.rep and assign only those, which are present in the respective fold:
for (i in 1:length(folds_train)) {

  preds_temp <- preds[folds_train[[i]],]
  # check levels 
  levels(preds_temp$class)
  # which are actually present
  unique(preds_temp$class)
  # assign present levels to each subset
  preds_temp$class <- factor(preds_temp$class, levels = as.character(unique(preds_temp$class)))

}

I tried to include the assignment of the right factor levels within the cforest_partykit function (# make consistent factor levels), but it seems to have no effect.
How could I implement this in the caret train() or trainControl() or createDataPartition() function?
","To make sure cforest_partykit treats categorical variables appropriately, it is best to create the design matrix explicitly through the model.matrix command.
For example
# Create a formula for the model
model_formula <- as.formula(""y_column ~ . -1"")

# Then create the design matrix
model_train.design.matrix <- model.matrix(model_formula, data = dat)

# Add in the y-variable
model_train.design.data <- cbind(y_column = data$y_column, model_train.design.matrix)

",cross-validation
how to do repeatedkfold cv the right way,"I am working on a binary classification using random forest with a dataset size of 977 records and 6 columns. class ratio is 77:23 (imbalanced dataset)
Since, my dataset is small, I learnt that it is not advisable to split using regular train_test split of 70 and 30.
So, I was thinking to do repeatedKfold CV. Please find my code below
Approach 1 - Full data - X, y
rf_boruta = RandomForestClassifier(class_weight='balanced',max_depth=3,max_features='sqrt',n_estimators=300)
cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=100)
scores = cross_val_score(rf_boruta,X,y, scoring='f1', cv=cv)
print('mean f1: %.3f' % mean(scores))

But I see that we have full input data X passed at once to the model. Doesn't this lead to data leakage? Meaning, if I am doing categorical encoding, we have to do based on all categories encountered in full dataset. Similarly, consider if a dataset ranges from the year 2017 to 2022. It is possible that model uses 2021 data in one of the folds and validate it on the 2020 data.
So, is it right to use repeatedKfold like the below?
Approach 2 - only train data - X_train, y_train
rf_boruta = RandomForestClassifier(class_weight='balanced',max_depth=3,max_features='sqrt',n_estimators=300)
cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=100)
scores = cross_val_score(rf_boruta,X_train,y_train, scoring='f1', cv=cv)
print('mean f1: %.3f' % mean(scores))

Can help me understand which will be the best approach to use?
","I'd say that there are two ways to do it. The first way is to write the code for training and validating manually. Here is an example of a code for it:
scores = []
folds = RepeatedStratifiedKFold(n_splits=10, n_repeats=100)
for fold_n, (train_index, valid_index) in enumerate(folds.split(train, y, groups=train['breath_id'])):
    X_train, X_valid = X[columns].iloc[train_index], X[columns].iloc[valid_index]
    y_train, y_valid = y.iloc[train_index], y.iloc[valid_index]
    
    encoder = LabelEncoder()
    encoder.fit(X_train[:, 0])
    X_train[:, 0] = encoder.transform(X_train[:, 0])
    X_valid [:, 0] = encoder.transform(X_valid [:, 0])
    rf_boruta = RandomForestClassifier(class_weight='balanced',max_depth=3,max_features='sqrt',n_estimators=300)

    rf_boruta .fit(X_train, y_train)
    score = metrics.f1_score(y_valid, rf_boruta .predict(X_valid))
    
    scores.append(score)

The second way is to use Pipeline from sklearn:
import numpy as np
from sklearn.model_selection import RepeatedStratifiedKFold
from sklearn.ensemble import RandomForestClassifier
from sklearn.datasets import make_classification
from sklearn.pipeline import Pipeline
from sklearn.model_selection import cross_val_score

# creating artificial data
X, y = make_classification(n_samples=1000, n_features=6, n_informative=4, n_redundant=2)
# making one of the column categorical
X[:, 0] = np.random.randint(0, 10, 1000)
# converting into DataFrame so that we can use column names
X = pd.DataFrame(X, columns = [str(i) for i in range(6)])

preprocessor = ColumnTransformer(
    transformers=[
        ('num', StandardScaler(), ['1', '2']),
        ('cat', OneHotEncoder(), ['0']),
    ]
)                                  
                                  
rf = RandomForestClassifier(class_weight='balanced', max_depth=3, max_features='sqrt', n_estimators=300)
cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=10)
pipe = Pipeline([('transformer', preprocessor), ('rf', rf)])
scores = cross_val_score(rf, X, y, scoring='f1', cv=cv)
print(f'Mean f1: {np.mean(scores):.3f}')

",cross-validation
cvinteger vs predefined splits in gridsearchcv,"What's the difference between setting CV=some integer vs cv=PredefinedSplit(test_fold=your_test_fold)?
Is there any advantage of one over the other? Does CV=some integer sets the splits randomly?
","Specifying an integer will produce kfold cross-validation without shuffling, as described in the documentation for sklearn.model_selection.KFold. Shuffling before splitting may or may not be preferred; if your data is sorted, shuffling is necessary to randomize the distribution of samples, while if the samples are simply correlated due to spatial or temporal sampling effects, shuffling may provide an optimistic view of performance.
I would avoid using PredefinedSplit unless you have a very good reason to predefine your splits. There are other CV generators that can probably meet your needs, like StratifiedKFold if you want to maintain your class distribution (for example.)
",cross-validation
why such different answers for the xgboost scikitlearn interface,"I am using xgboost for the first time and trying the two different interfaces. First I get the data:
import xgboost as xgb
import dlib
import pandas as pd
import numpy as np
from sklearn.model_selection import cross_val_score
data_url = ""http://lib.stat.cmu.edu/datasets/boston""
raw_df = pd.read_csv(data_url, sep=""\s+"", skiprows=22, header=None)
X = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])
y = raw_df.values[1::2, 2]
dmatrix = xgb.DMatrix(data=X, label=y)

Now the scikit-learn interface:
xgbr = xgb.XGBRegressor(objective='reg:squarederror', seed=20)
print(cross_val_score(xgbr, X, y, cv=5))

This outputs:
[0.73438184 0.84902986 0.82579692 0.52374618 0.29743001]

Now the xgboost native interface:
dmatrix = xgb.DMatrix(data=X, label=y)
params={'objective':'reg:squarederror'}
cv_results =  xgb.cv(dtrain=dmatrix, params=params, nfold=5, metrics={'rmse'},  seed=20)
print('RMSE: %.2f' % cv_results['test-rmse-mean'].min())

This gives 3.50.
Why are the outputs so different? What am I doing wrong?
","First of all, you didn't specify the metric in cross_val_score, therefore you are not calculating RMSE, but rather the estimator's default metric, which is usually just its loss function. You need to specify it for comparable results:
cross_val_score(xgbr, X, y, cv=5, scoring = 'neg_root_mean_squared_error')

Second, you need to match sklearn's CV procedure exactly. For that, you can pass folds argument to XGBoost's cv method:
from sklearn.model_selection import KFold

cv_results =  xgb.cv(dtrain=dmatrix, params=params, metrics={'rmse'}, folds = KFold(n_splits=5))

Finally, you need to ensure that XGBoost's cv procedure actually converges. For some reason it only does 10 boosting rounds by default, which is too low to converge on your dataset. This is done via nrounds argument (num_boost_round if you're on an older version), I found that 100 rounds work just fine on this dataset:
cv_results =  xgb.cv(dtrain=dmatrix, params=params, metrics={'rmse'}, folds = KFold(n_splits=5), nrounds = 100)

Now you will get matching results.
On a side note, it's interesting how you say it's your first time using XGBoost, but you actually have a question on XGBoost dating back to 2017.
",cross-validation
how can do crossvalidation for a attributeselectedclassifier model,"I did a model like that:
base = Classifier(classname=""weka.classifiers.trees.ADTree"", 
                  options=[""-B"", ""10"", ""-E"", ""-3"", ""-S"", ""1""])

CostS_cls = SingleClassifierEnhancer(classname=""weka.classifiers.meta.CostSensitiveClassifier"", 
                                options =[""-cost-matrix"", ""[0.0 1.0; 1.0 0.0]"", ""-S"", ""1""])
CostS_cls.classifier = base
smote = Filter(classname=""weka.filters.supervised.instance.SMOTE"", 
               options=[""-C"", ""0"", ""-K"", ""3"", ""-P"", ""250.0"", ""-S"", ""1""])
fc = FilteredClassifier(options=[""-S"",""1""])
fc.filter = smote
fc.classifier = CostS_cls
bagging_cls = SingleClassifierEnhancer(classname=""weka.classifiers.meta.Bagging"",
                         options=[""-P"", ""100"", ""-S"", ""1"", ""-num-slots"", ""1"", ""-I"", ""100""])
bagging_cls.classifier = fc
multisearch_cls = MultiSearch(options = [""-S"", ""1""])
multisearch_cls.evaluation = ""FM""
multisearch_cls.search = [""-sample-size"", ""100"", ""-initial-folds"", ""2"", ""-subsequent-folds"", ""10"",
                          ""-initial-test-set"", ""."", ""-subsequent-test-set"", ""."", ""-num-slots"", ""1""]                        
mparam = MathParameter()
mparam.prop = ""numOfBoostingIterations""
mparam.minimum = 5.0
mparam.maximum = 50.0
mparam.step = 1.0
mparam.base = 10.0
mparam.expression = ""I""
multisearch_cls.parameters = [mparam]
multisearch_cls.classifier = bagging_cls
AttS_cls = AttributeSelectedClassifier()
AttS_cls.search = from_commandline('weka.attributeSelection.GreedyStepwise -B -T -1.7976931348623157E308 -N -1 -num-slots 1', classname=get_classname(ASSearch))
AttS_cls.evaluation = from_commandline('weka.attributeSelection.CfsSubsetEval -P 1 -E 1', classname=get_classname(ASEvaluation))
AttS_cls.classifier = multisearch_cls
train, test = data_modelos_1_2.train_test_split(70.0, Random(1))
AttS_cls.build_classifier(train)

and I'm trying to validate it with cross-validation but when I do that:
train, test = data_modelos_1_2.train_test_split(70.0, Random(1))
AttS_cls.build_classifier(train)
evl = Evaluation(test)
evl.crossvalidate_model(AttS_cls, test, 10, Random(1))

Im getting this error:
---------------------------------------------------------------------------
JavaException                             Traceback (most recent call last)
/tmp/ipykernel_50548/1197040560.py in <module>
     47 print(AttS_cls.to_commandline())
     48 evl = Evaluation(test)
---> 49 evl.crossvalidate_model(AttS_cls, test, 10, Random(1))
     50 print(AttS_cls)
     51 print(""----------------------------------------------------------------------------"")

/usr/local/lib/python3.8/dist-packages/weka/classifiers.py in crossvalidate_model(self, classifier, data, num_folds, rnd, output)
   1289         else:
   1290             generator = [output.jobject]
-> 1291         javabridge.call(
   1292             self.jobject, ""crossValidateModel"",
   1293             ""(Lweka/classifiers/Classifier;Lweka/core/Instances;ILjava/util/Random;[Ljava/lang/Object;)V"",

~/.local/lib/python3.8/site-packages/javabridge/jutil.py in call(o, method_name, sig, *args)
    890     ret_sig = sig[sig.find(')')+1:]
    891     nice_args = get_nice_args(args, args_sig)
--> 892     result = fn(*nice_args)
    893     x = env.exception_occurred()
    894     if x is not None:

~/.local/lib/python3.8/site-packages/javabridge/jutil.py in fn(*args)
    857             x = env.exception_occurred()
    858             if x is not None:
--> 859                 raise JavaException(x)
    860             return result
    861     else:

JavaException: Thread-based execution of evaluation tasks failed!

So i don't know what I'm doing wrong, because i know that using weka u can crossvalidate this types of model but I'm trying on pyweka and have that problem.
","I have turned your code snippet into one with imports and fixed the MultiSearch setup for Bagging (mparam.prop = ""numIterations"" instead of mparam.prop = ""numOfBoostingIterations""), allowing it to be executed.
Since I do not have access to your data, I just used the UCI dataset vote.arff.
Your code was a bit odd, as it did a 70/30 train/test split, trained the classifier and then performed cross-validation on the test data. For cross-validation you do not train the classifier, as this happens within the internal cross-validation loop (each trained classifier inside that loop gets discarded, as cross-validation is only used for gathering statistics).
The code below has therefore three parts:

your original evaluation code, but commented out
performing proper cross-validation
performing train/test evaluation

I do not use Jupyter notebooks and tested the code successfully in a regular virtual environment on my Linux Mint:

Python: 3.8.10
Output of pip freeze:
numpy==1.22.3
packaging==21.3
pyparsing==3.0.7
python-javabridge==4.0.3
python-weka-wrapper3==0.2.7



The modified code itself:
import weka.core.jvm as jvm
from weka.core.converters import load_any_file
from weka.classifiers import Classifier, SingleClassifierEnhancer, FilteredClassifier, MultiSearch, AttributeSelectedClassifier, Evaluation
from weka.core.classes import MathParameter, from_commandline, Random, get_classname
from weka.filters import Filter
from weka.attribute_selection import ASEvaluation, ASSearch

jvm.start(packages=True)

# the dataset/path needs adjusting
data_modelos_1_2 = load_any_file(""/some/where/vote.arff"")
data_modelos_1_2.class_is_last()

base = Classifier(classname=""weka.classifiers.trees.ADTree"",
                  options=[""-B"", ""10"", ""-E"", ""-3"", ""-S"", ""1""])

CostS_cls = SingleClassifierEnhancer(classname=""weka.classifiers.meta.CostSensitiveClassifier"",
                                     options=[""-cost-matrix"", ""[0.0 1.0; 1.0 0.0]"", ""-S"", ""1""])
CostS_cls.classifier = base
smote = Filter(classname=""weka.filters.supervised.instance.SMOTE"",
               options=[""-C"", ""0"", ""-K"", ""3"", ""-P"", ""250.0"", ""-S"", ""1""])
fc = FilteredClassifier(options=[""-S"", ""1""])
fc.filter = smote
fc.classifier = CostS_cls
bagging_cls = SingleClassifierEnhancer(classname=""weka.classifiers.meta.Bagging"",
                                       options=[""-P"", ""100"", ""-S"", ""1"", ""-num-slots"", ""1"", ""-I"", ""100""])
bagging_cls.classifier = fc
multisearch_cls = MultiSearch(options=[""-S"", ""1""])
multisearch_cls.evaluation = ""FM""
multisearch_cls.search = [""-sample-size"", ""100"", ""-initial-folds"", ""2"", ""-subsequent-folds"", ""10"",
                          ""-initial-test-set"", ""."", ""-subsequent-test-set"", ""."", ""-num-slots"", ""1""]
mparam = MathParameter()
mparam.prop = ""numIterations""
mparam.minimum = 5.0
mparam.maximum = 50.0
mparam.step = 1.0
mparam.base = 10.0
mparam.expression = ""I""
multisearch_cls.parameters = [mparam]
multisearch_cls.classifier = bagging_cls

AttS_cls = AttributeSelectedClassifier()
AttS_cls.search = from_commandline('weka.attributeSelection.GreedyStepwise -B -T -1.7976931348623157E308 -N -1 -num-slots 1', classname=get_classname(ASSearch))
AttS_cls.evaluation = from_commandline('weka.attributeSelection.CfsSubsetEval -P 1 -E 1', classname=get_classname(ASEvaluation))
AttS_cls.classifier = multisearch_cls

# original
# train, test = data_modelos_1_2.train_test_split(70.0, Random(1))
# AttS_cls.build_classifier(train)
# evl = Evaluation(test)
# evl.crossvalidate_model(AttS_cls, test, 10, Random(1))
# print(evl.summary())

# cross-validation
print(""\ncross-validation\n"")
evl = Evaluation(data_modelos_1_2)
evl.crossvalidate_model(AttS_cls, data_modelos_1_2, 10, Random(1))
print(evl.summary())

# train/test split
print(""\ntrain/test split\n"")
train, test = data_modelos_1_2.train_test_split(70.0, Random(1))
AttS_cls.build_classifier(train)
evl = Evaluation(test)
evl.test_model(AttS_cls, test)
print(evl.summary())

jvm.stop()

This generated the following output:
cross-validation


Correctly Classified Instances         416               95.6322 %
Incorrectly Classified Instances        19                4.3678 %
Kappa statistic                          0.9094
Mean absolute error                      0.0737
Root mean squared error                  0.1778
Relative absolute error                 15.5353 %
Root relative squared error             36.5084 %
Total Number of Instances              435     


train/test split


Correctly Classified Instances         126               96.1832 %
Incorrectly Classified Instances         5                3.8168 %
Kappa statistic                          0.9216
Mean absolute error                      0.0735
Root mean squared error                  0.1649
Relative absolute error                 15.3354 %
Root relative squared error             33.6949 %
Total Number of Instances              131  

",cross-validation
split data in 5 subsets with choosekn amp not with sample,"I want to split train and  test but with choose() function not with sample() in R.
I have 58 rows and 28 columns on my dataset (a csv file ) and I want to do a 10-fold or 5-fold CV on this dataset.
How am I going to write the code down for this task ?
I`ve tried:
set.seed(1)
smp_size=choose(58,5, name_dataset) # which is totally wrong but ... 
# I haven't figured out yet how to take 5 subsets from 58 observations
# each time I do a 5/10 -fold  CV

train_ind=sample(seq_len(nrow(name_dataset)),size=smp_size) # I think sample here is wrong too
train=name_dataset[train_ind,]
test=name_dataset[-train_ind,]

","I don't know what you mean by every possible combination of 5-subset. That seems like an incredibly large amount of possibilities. I assume you mean that you want a subset of 5 datasets that contain all of the samples in your dataset. I would probably do something like this. We first make a vector of groups that is the number of k and the length of the dataset. We then sample the groups randomly and split the dataset by these groupings.
library(tidyverse)

set.seed(3465)
test_data <- tibble(A = runif(58),
                    B = runif(58))


k_split <- function(dat,k, seed = 1){
  set.seed(seed)
  grp <- rep(1:k, length.out = nrow(dat))
  dat |>
    mutate(grp = sample(grp, nrow(dat), replace = F)) |>
    group_split(grp)|>
    map(\(d) select(d, -grp))
}

k_split(test_data, 5)
#> [[1]]
#> # A tibble: 12 x 2
#>        A      B
#>    <dbl>  <dbl>
#>  1 0.476 0.468 
#>  2 0.636 0.639 
#>  3 0.334 0.0269
#>  4 0.668 0.220 
#>  5 0.398 0.919 
#>  6 0.343 0.748 
#>  7 0.799 0.526 
#>  8 0.710 0.759 
#>  9 0.737 0.927 
#> 10 0.819 0.441 
#> 11 0.852 0.656 
#> 12 0.416 0.541 
#> 
#> [[2]]
#> # A tibble: 12 x 2
#>         A      B
#>     <dbl>  <dbl>
#>  1 0.0107 0.905 
#>  2 0.109  0.539 
#>  3 0.715  0.778 
#>  4 0.523  0.416 
#>  5 0.609  0.357 
#>  6 0.152  0.0972
#>  7 0.919  0.450 
#>  8 0.866  0.510 
#>  9 0.0347 0.0890
#> 10 0.862  0.465 
#> 11 0.364  0.765 
#> 12 0.789  0.601 
#> 
#> [[3]]
#> # A tibble: 12 x 2
#>         A      B
#>     <dbl>  <dbl>
#>  1 0.580  0.228 
#>  2 0.201  0.0418
#>  3 0.0359 0.417 
#>  4 0.521  0.758 
#>  5 0.534  0.974 
#>  6 0.580  0.563 
#>  7 0.844  0.781 
#>  8 0.756  0.271 
#>  9 0.211  0.533 
#> 10 0.851  0.764 
#> 11 0.885  0.150 
#> 12 0.262  0.371 
#> 
#> [[4]]
#> # A tibble: 11 x 2
#>         A     B
#>     <dbl> <dbl>
#>  1 0.556  0.313
#>  2 0.353  0.821
#>  3 0.0959 0.861
#>  4 0.759  0.261
#>  5 0.207  0.772
#>  6 0.668  0.527
#>  7 0.150  0.788
#>  8 0.0939 0.257
#>  9 0.0913 0.817
#> 10 0.294  0.790
#> 11 0.0224 0.253
#> 
#> [[5]]
#> # A tibble: 11 x 2
#>          A      B
#>      <dbl>  <dbl>
#>  1 0.0893  0.665 
#>  2 0.966   0.142 
#>  3 0.672   0.0849
#>  4 0.641   0.155 
#>  5 0.490   0.187 
#>  6 0.00394 0.295 
#>  7 0.126   0.813 
#>  8 0.202   0.474 
#>  9 0.0740  0.107 
#> 10 0.412   0.709 
#> 11 0.509   0.253

",cross-validation
xgboost cross validation  accuracy metrics,"I'm doing cross validation on xgboost. Here is my code.
from xgboost import cv

xgb_cv = cv(dtrain=data_dmatrix, 
            params=params, 
            nfold=10,
            num_boost_round=50, 
            early_stopping_rounds=10, 
            metrics=""auc"", 
            as_pandas=True, 
            seed=1)

Is there any way to have ""accuracy"" metrics?
I couldn't find any ""accuracy"" option for ""metrics"" in the documentation.
","The accuracy is 1 - the error rate (metrics = 'error').
",cross-validation
random forest further improvement,"Following Jason Brownlee's tutorials, I developed my own Random forest classifier code. I paste it below, I would like to know what further improvements can I do to improve the accuracy to my code
from numpy import mean
from numpy import std
from sklearn.datasets import make_classification
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import RepeatedStratifiedKFold
from sklearn.ensemble import RandomForestClassifier
from matplotlib import pyplot


X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.05, shuffle = True, random_state=0)

scaler = StandardScaler()
x_train = scaler.fit_transform(X_train)

x_test = scaler.transform(X_test)



# get a list of models to evaluate
def get_models():
    models = dict()
    # consider tree depths from 1 to 7 and None=full
    depths = [i for i in range(1,8)] + [None]
    for n in depths:
        models[str(n)] = RandomForestClassifier(max_depth=n)
    return models

# evaluate  model using cross-validation
def evaluate_model(model, X, y):
    # define the evaluation procedure
    cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)
    # evaluate the model and collect the results
    scores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1)
    return scores


# get the models to evaluate
models = get_models()
# evaluate the models and store results
results, names = list(), list()
for name, model in models.items():
    # evaluate the model
    scores = evaluate_model(model, X, y)
    # store the results
    results.append(scores)
    names.append(name)
    # summarize the performance along the way
    print('>%s %.3f (%.3f)' % (name, mean(scores), std(scores)))
# plot model performance for comparison
pyplot.boxplot(results, labels=names, showmeans=True)
pyplot.show()

The data, X is a matrix of (140,20000) and y is (140,) categorical.
I got the following results but would like to explore how to improve accuracy further.
>1 0.573 (0.107)
>2 0.650 (0.089)
>3 0.647 (0.118)
>4 0.676 (0.101)
>5 0.708 (0.103)
>6 0.698 (0.124)
>7 0.726 (0.121)
>None 0.700 (0.107)

","Here's what stands out to me:

You split the data but do not use the splits.
You're scaling the data, but tree-based methods like random forests do not need this step.
You are doing your own tuning loop, instead of using sklearn.model_selection.GridSearchCV. This is fine, but it can get quite fiddly (imagine wanting to step over another hyperparameter).
If you use GridSearchCV you don't need to do your own cross validation.
You're using accuracy for evaluation, which is usually not a great evaluation metric for multi-class classification. Weighted F1 is better.
If you're doing cross validation, you need to put the scaler in the CV loop (e.g. using a pipeline) because otherwise the scaler has seen the validation data... but you don't need a scaler for this learning algorithm so this point is moot.

I would probably do something like this:
import numpy as np
from sklearn.datasets import make_classification
from sklearn.model_selection import RepeatedStratifiedKFold
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import GridSearchCV
from sklearn.model_selection import train_test_split

X, y = make_classification()

# Split the data.
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.05, shuffle=True, random_state=0)

# Make things for the cross validation.
cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)
param_grid = {'max_depth': np.arange(3, 8)}
model = RandomForestClassifier(random_state=1)

# Create and train the cross validation.
clf = GridSearchCV(model, param_grid,
                   scoring='f1_weighted',
                   cv=cv, verbose=3)

clf.fit(X_train, y_train)

Take a look at clf.cv_results_ for the scores etc, which you can plot if you want. By default GridSearchCV trains a final model on the best hyperparameters, so you can make predictions with clf.
Almost forgot... you asked about improving the model :) Here are some ideas:

The above will help you tune on more hyperparameters (eg max_features, n_estimators, and min_samples_leaf). But don't get too carried away with hyperparameter tuning.
You could try transforming some features (columns in X), or adding new ones.
Look for more data, eg more rows, higher quality labels, etc.
Address any issues with class imbalance.
Try a more sophisticated algorithm, like gradient boosted trees (there are models in sklearn, or take a look at xgboost).

",cross-validation
how do i get the training accuracies for each fold in kfold cross validation in r,"I would like to evaluate whether the logistic regression model I created is overfit. I'd like to compare the accuracies of each training fold to the test fold, but I don't know how to view these in R. This is the k-fold cross validation code:
library(caret)
levels(habitatdata$outcome) <- c(""absent"", ""present"") #rename factor levels 
set.seed(12)
cvIndex <- createFolds(factor(habitatdata$outcome), 5, returnTrain = T) #create stratified folds
ctrlspecs <- trainControl(index = cvIndex, 
  method = ""cv"", 
  number = 5, 
  savePredictions = ""all"", 
  classProbs = TRUE) #specify training methods
set.seed(123)
model1 <- train(outcome~ ist + hwt, 
  data=habitatdata,
  method = ""glm"",
  family = binomial, trControl = ctrlspecs) #specify model

How do I view the training accuracies of each fold?
","Look at model1$resample - it should give you a table with Accuracy (and Kappa) for each fold.
",cross-validation
cross_val_score returns nan when put in fit_params,"I am doing SVC for classification task with cross validation using cross_val_score in slearn, but turns out it return list of nan value when I put in parameters for fit_params but working fine if I dont put in the parameters for fit_params.
Code:
# define parameter
param_grid = {
    'C' : [1,5,10,20],
    'gamma' : ['auto','scale']
}

svc = SVC(kernel = ""rbf"")

scores = cross_val_score(svc, x_train, y_train, cv=10, fit_params = param_grid)
# scores output array([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan])

scores = cross_val_score(svc, x_train, y_train, cv=10)
# scores output array([0.95833333, 0.95833333, 0.95454545, 0.93181818, 0.95454545, 0.96197719, 0.96197719, 0.94676806, 0.96197719, 0.95057034])

","fit_params is designated for fit methods (e.g., array of sample weights for training data), but you pass your parameter grid to cross_val_score, which is incompatible with your data (x_train, y_train, etc.). Indeed, if you specify error_score='raise' in your cross_val_score, you will receive the corresponding error. Parameter grids should be used with GridSearchCV or similar tools.
",cross-validation
preparing testtrain sets for cross validaton in a loop,"I am trying to build Test and Train groups for doing the Cross Validation. I have a total individuals pool of 95 invidual IDs and tried to make the task done like this:
# create 95 unique IDs as individuals
set.seed(1)
indv <- stringi::stri_rand_strings(95, 4)

# specify Kfold
n.folds <- 5

folds <- cut(1:length(indv), breaks = n.folds, labels = FALSE)
# randomise the folds
folds <- sample(folds, length(folds)) 

samples.train <- list()
samples.test <- list()
foldSet <- list()

kfold.df <- data.frame(""IID"" = indv)

for (f in 1:n.folds) {
          samples.train[[f]] <- indv[folds != f]
          samples.test[[f]] <- indv[folds == f]

# replace to x (test) if the corresponding value is TRUE, and to y (train) if it is FALSE.
foldSet[[f]] <- ifelse(kfold.df$IID %in% 
                  samples.test[[f]], ""test"", ""train"")

# combine foldSet to datafarme.
kfold.df[[f]] <- cbind(kfold.df, foldSet[[f]])
} 

The goal is preparing 5 testing and training sets of samples to do the modeling. But I have encountered with this error message:
Error in data.frame(..., check.names = FALSE) : 
arguments imply differing number of rows: 95, 2

Besides, the foldSet output is not as expected, although samples.train and samples.test are correct. Could you please help me to make this loop working!
UPDATE:
Here is the for-loop without using wildcards in creating foldSet :
for (f in 1:n.folds) {
samples.train[[f]] <- indv[folds != f]
samples.test[[f]] <- indv[folds == f]

foldSet <<- ifelse(kfold.df$IID %in% samples.test[[f]], ""test"", ""train"")
# combine foldSet to datafarme.
kfold.df <<- cbind(kfold.df, foldSet)
}

By executing the loop you will find kfold.df as a dataframe listing all five folds test/train random sets. I expect for each iteration, creating the testing and training sets corresponding to the f, so, after five iteration, I would have access to each fold's Training/Testing sets for the next operations inside the loop, like kfold.df[foldSet == ""train"", ""IID""]. I need this access bcoz I want to use it for subsetting another bigger matrix based on train and test invd of each fold, preparing it for applying to the regression model. That's why I used the wildcards for foldSet to make the loop able creating all by itself but I failed to manage it.
","I think you may be overcomplicating things (which is something I do all the time...)
You don't need to go to great lengths to make what you are trying to make. This answer is broken down into three parts.

Building the data frame you're looking for (I think!)
Why you really don't need this data frame to be built
Why not use what's already out there?


Part 1
If I understand correctly, this is about what you're looking for (less the strings). I also included how you might use it with your actual data.
library(tidyverse)

giveMe <- function(rowCt, nfolds){
  # set.seed(235) # removed seed after establishing working function to incite
  #  the expected randomness

  folds <- cut(1:rowCt, breaks = nfolds, labels = F)
  # randomise the folds
  folds <- sample(folds, length(folds)) 
  # create the folds' sets
  kfold.df <- map_dfc(1:nfolds,
                      ~ifelse(folds != .x, T, F)) %>% 
  setNames(., paste0(""foldSet_"",1:nfolds)) %>%  # name each field
  add_column(IID = 1:rowCt, .before = 1) # add indices to the left

  return(kfold.df) # return a data frame
}

given <- giveMe(95, 5)

giveMore <- giveMe(nrow(iris), 5) # uses the built-in iris data set


Part 2
You could just create your random fold sequence and use that with a model, you don't need to stack them in a data frame. You have to loop through the model the same number of times, why not do it at the same time?
folds <- sample(cut(1:nrow(iris), 5, # no seed-- random on purpose
                    labels = F))

tellMe <- map(1:5, # the folds start in col 2
              ~lm(Sepal.Length~., 
                  iris[ifelse(folds != .x,
                              T, F), 
                       1:4])) # dropped 'Species' groups' issue

To check out the model performance:
map_dfr(1:5, .f = function(x){
  y = tellMe[[x]]
  sigma = sigma(y)
  rsq = summary(y)$adj.r.squared
  c(sigma = sigma, rsq = rsq)
})
# # A tibble: 5 × 2
#   sigma   rsq
#   <dbl> <dbl>
# 1 0.334 0.844
# 2 0.309 0.869
# 3 0.302 0.846
# 4 0.330 0.847
# 5 0.295 0.872 

Predict and inspect the testing performance
# create a list of the predictec values from the test data
showMe <- map(1:5,
              ~predict(tellMe[[.x]], 
                       iris[ifelse(folds == .x,
                                   T, F), 1:4]))

# Grab comparable metrics like those from the models
map_dfr(1:5,
        .f = function(x){
          A = iris[ifelse(folds == x, T, F), ]$Sepal.Length
          P = showMe[[x]]
          sigma = sqrt(sum((A - P)^2) / length(A))
          rsq = cor(A, P)^2
          c(sigma = sigma, rsq = rsq)
        })
# # A tibble: 5 × 2
#   sigma   rsq
#   <dbl> <dbl>
# 1 0.232 0.919
# 2 0.342 0.774
# 3 0.366 0.884
# 4 0.250 0.906
# 5 0.384 0.790 


Part 3
Here I'm going to use the caret library. However, there are a lot of other options.
library(caret)

set.seed(1)
# split training and testing 70/30%
tr <- createDataPartition(iris$Species, p = .7, list = F)

# set up 5-fold val
trC <- trainControl(method = ""cv"", number = 5)

# train the model
fit <- train(Sepal.Length~., iris[tr, ], 
             method = ""lm"", 
             trControl = trC)
summary(fit)
# truncated results best model:
# Residual standard error: 0.2754 on 39 degrees of freedom
# Multiple R-squared:  0.9062,  Adjusted R-squared:  0.8941 

fit.p <- predict(fit, iris[-tr,])
postResample(fit.p, iris[-tr, ]$Sepal.Length)
#      RMSE  Rsquared       MAE 
# 0.2795920 0.8925574 0.2302402  

If you want to see each of the folds' performance, you can do that, too.
fit$resample
#        RMSE  Rsquared       MAE Resample
# 1 0.3629901 0.7911634 0.2822708    Fold1
# 2 0.3680954 0.8888947 0.2960464    Fold2
# 3 0.3508317 0.8394489 0.2709989    Fold3
# 4 0.2548549 0.8954633 0.1960375    Fold4
# 5 0.3396910 0.8661239 0.3187768    Fold5 

",cross-validation
generate negative predictive value using cross_val_score in sklearn for model performance evaluation,"As part of evaluating a model's metrics, I would like to use cross_val_score in sklearn to generate negative predictive value for a binary classification model.
In example below, I set the 'scoring' parameter within cross_val_score to calculate and print 'precision' (mean and standard deviations from 10-fold cross-validation) for positive predictive value of the model:
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import cross_val_score
log=LogisticRegression()    
log_prec = cross_val_score(log, x, y, cv=10, scoring='precision')
print(""PPV(mean, std): "", np.round(log_prec.mean(), 2), np.round(log_prec.std(), 2))

How can I use something like the above line of code to generate negative predictive value/NPV (likelihood of a predicted negative to be a true negative) from within the cross_val_score method?
sklearn provides many scoring options (eg: roc_auc, recall, accuracy, F1, etc) but unforunately not one for NPV...
","You can use make_scorer to pass in pos_label=0 to the precision score function (metrics.precision_score) to get NPV. Like this:
from sklearn.metrics import make_scorer, precision_score
npv = cross_val_score(log, x, y, cv=10, scoring=make_scorer(precision_score, pos_label=0))

For more details see this sklearn example.
",cross-validation
valueerror requesting 5fold crossvalidation but provided less than 5 examples for at least one class,"I have been training a text classifier to then later use to predict characters of a TV show. So far, my code looks like:
vectorizer = TfidfVectorizer(ngram_range=(1,2),min_df=0.001, max_df=0.75,stop_words='English')
X = vectorizer.fit_transform(data['text'])
y = data['character']
print(X.shape, y.shape) #prints (5999, 1429) (5999,)

# get baseline performance
kf = KFold(n_splits=5)
most_frequent = DummyClassifier(strategy='most_frequent')
print(cross_val_score(most_frequent , X, y=y, cv=kf, n_jobs= -1, scoring=""accuracy"").mean())

# fine-tune classifier
base_clf = CalibratedClassifierCV(cv=kf, base_estimator=LogisticRegression(n_jobs= -1, solver='lbfgs' ))

param_grid = {'base_estimator__C': [0.01, 0.05, 0.1, 0.5, 1.0, 10, 20, 50],
'base_estimator__class_weight': ['balanced', 'auto']}

search = GridSearchCV(base_clf, param_grid, cv=kf, scoring='f1_micro')
search.fit(X, y)

# use best classifier to get performance estimate
clf = search.best_estimator_.base_estimator
print(cross_val_score(clf, X, y=y, cv=kf, n_jobs= -1, scoring='f1_micro').mean())

However, I keep getting the following error:
ValueError                                Traceback (most recent call last)
/var/folders/fv/h7n33cb5227g4t5lxym8g_800000gn/T/ipykernel_2208/2611717736.py in <module>
      6 
      7 search = GridSearchCV(base_clf, param_grid, cv=kf, scoring='f1_micro')
----> 8 search.fit(X, y)
      9 
     10 # use best classifier to get performance estimate

~/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py in inner_f(*args, **kwargs)
     61             extra_args = len(args) - len(all_args)
     62             if extra_args <= 0:
---> 63                 return f(*args, **kwargs)
     64 
     65             # extra_args > 0

~/opt/anaconda3/lib/python3.9/site-packages/sklearn/model_selection/_search.py in fit(self, X, y, groups, **fit_params)
    878             refit_start_time = time.time()
    879             if y is not None:
--> 880                 self.best_estimator_.fit(X, y, **fit_params)
    881             else:
    882                 self.best_estimator_.fit(X, **fit_params)

~/opt/anaconda3/lib/python3.9/site-packages/sklearn/calibration.py in fit(self, X, y, sample_weight)
    301             if n_folds and np.any([np.sum(y == class_) < n_folds
    302                                    for class_ in self.classes_]):
--> 303                 raise ValueError(f""Requesting {n_folds}-fold ""
    304                                  ""cross-validation but provided less than ""
    305                                  f""{n_folds} examples for at least one class."")

ValueError: Requesting 5-fold cross-validation but provided less than 5 examples for at least one class.

I am not quite sure how to resolve this error and would truly appreciate any advice.
Thank you in advance!
","You need to check the distribution of your target value data['character'] : it seems that the number of values in one of the classes in the target column is too small. To do it you can use : data['character'].value_counts()
",cross-validation
why is scikitlearn svm classifier cross validation so slow,"I am trying to compare multiple classifiers on a dataset that I have. To get accurate accuracy scores for the classifiers I am now performing 10 fold cross validation for each classifier. This goes well for all of them except SVM (both linear and rbf kernels). The data is loaded like this:
dataset = pd.read_csv(""data/distance_annotated_indels.txt"", delimiter=""\t"", header=None)

X = dataset.iloc[:, [5,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26]].values
y = dataset.iloc[:, 4].values

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2)

from sklearn.preprocessing import StandardScaler
sc = StandardScaler()
X_train = sc.fit_transform(X_train)
X_test = sc.transform(X_test)

Cross validation for for example a Random Forest works fine:
start = time.time()
classifier = RandomForestClassifier(n_estimators = 100, criterion = 'entropy')
classifier.fit(X_train, y_train)
y_pred = classifier.predict(X_test)
cv = ShuffleSplit(n_splits=10, test_size=0.2)
scores = cross_val_score(classifier, X, y, cv=10)
print(classification_report(y_test, y_pred))
print(""Random Forest accuracy after 10 fold CV: %0.2f (+/- %0.2f)"" % (scores.mean(), scores.std() * 2) + "", "" + str(round(time.time() - start, 3)) + ""s"")

Output:
             precision    recall  f1-score   support

          0       0.97      0.95      0.96      3427
          1       0.95      0.97      0.96      3417

avg / total       0.96      0.96      0.96      6844

Random Forest accuracy after 10 fold CV: 0.92 (+/- 0.06), 90.842s

However for SVM this process takes ages (waited for 2 hours, still nothing). The sklearn website does not make me any wiser. Is there something I should be doing different for SVM classifiers? The SVM code is as follows:
start = time.time()
classifier = SVC(kernel = 'linear')
classifier.fit(X_train, y_train)
y_pred = classifier.predict(X_test)
scores = cross_val_score(classifier, X, y, cv=10)
print(classification_report(y_test, y_pred))
print(""Linear SVM accuracy after 10 fold CV: %0.2f (+/- %0.2f)"" % (scores.mean(), scores.std() * 2) + "", "" + str(round(time.time() - start, 3)) + ""s"")

","If you have a lot of samples the computational complexity of the problem gets in the way, see Training complexity of Linear SVM.
Consider playing with the verbose flag of cross_val_score to see more logs about progress. Also, with n_jobs set to a value  > 1 (or even using all CPUs with  n_jobs set to -1, if memory allows) you could speed up computation via parallelization. http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_score.html can be useful to evaluate these options.
If performance is poor I'd consider reducing the value of cv (see https://stats.stackexchange.com/questions/27730/choice-of-k-in-k-fold-cross-validation for a discussion on this)
",cross-validation
r feature selection with cross validation using caret on logistic regression,"I am currently learning how to implement logistical Regression in R
I have taken a data set and split it into a training and test set and wish to implement forward selection, backward selection and best subset selection using cross validation to select the best features.
I am using caret to implement cross-validation on the training data set and then testing the predictions on the test data.
I have seen the rfe control in caret and had also had a look at the documentation on the caret website as well as following the links on the question How to use wrapper feature selection with algorithms in R?. It isn't apparent to me how to change the type of feature selection as it seems to default to backward selection. Can anyone help me with my workflow. Below is a reproducible example
library(""caret"")

# Create an Example Dataset from German Credit Card Dataset
mydf <- GermanCredit

# Create Train and Test Sets 80/20 split
trainIndex <- createDataPartition(mydf$Class, p = .8, 
                              list = FALSE, 
                              times = 1)

train <- mydf[ trainIndex,]
test  <- mydf[-trainIndex,]


ctrl <- trainControl(method = ""repeatedcv"", 
                 number = 10, 
                 savePredictions = TRUE)

mod_fit <- train(Class~., data=train, 
             method=""glm"", 
             family=""binomial"",
             trControl = ctrl, 
             tuneLength = 5)


# Check out Variable Importance
varImp(mod_fit)
summary(mod_fit)

# Test the new model on new and unseen Data for reproducibility
pred = predict(mod_fit, newdata=test)
accuracy <- table(pred, test$Class)
sum(diag(accuracy))/sum(accuracy)

","You can simply call it in mod_fit. When it comes to backward stepwise the code below is sufficient
trControl <- trainControl(method=""cv"",
                          number = 5,
                          savePredictions = T,
                          classProbs = T,
                          summaryFunction = twoClassSummary)

caret_model <- train(Class~.,
                     train,
                     method=""glmStepAIC"", # This method fits best model stepwise.
                     family=""binomial"",
                     direction=""backward"", # Direction
                     trControl=trControl)

Note that in trControl
method= ""cv"", # No need to call repeated here, the number defined afterward defines the k-fold.
classProbs = T,
summaryFunction = twoClassSummary # Gives back ROC, sensitivity and specifity of the chosen model.

",cross-validation
different results between cross_validate and my own cross validation function,"After validating the performance of my regression model with cross_validate I obtain some results following the 'r2' scoring.
That's what my code is doing
scores = cross_validate(RandomForestRegressor(),X,y,cv=5,scoring='r2')

and what I get is
>>scores['test_score']

array([0.47146303, 0.47492019, 0.49350646, 0.56479323, 0.56897343])

For more flexibility, I've also written my own cross validation function which is the following
def my_cross_val(estimator, X, y):
    
    r2_scores = []
    
    kf = KFold(shuffle=True)
    
    for train_index, test_index in kf.split(X,y):
        
        estimator.fit(X.iloc[train_index].values, y.iloc[train_index].values)
        preds = estimator.predict(X.iloc[test_index].values)
                
        r2 = r2_score(y.iloc[test_index].values, preds)
                    
        r2_scores.append(r2)
        
    return np.array(r2_scores)

Running now
scores = my_cross_val(RandomForestRegressor(),X,y)

I obtain
array([0.6975932 , 0.68211856, 0.62892119, 0.64776752, 0.66046326])

Am I doing something wrong in
my_cross_val()

as the values seem that overestimated compared to cross_validate() ? Maybe putting shuffle=True inside KFold?
","In order to be sure that you are comparing apples to apples, and given that shuffling can have a huge difference in such cases, here is what you should do:
First, shuffle your data manually:
from sklearn.utils import shuffle
X_s, y_s = shuffle(X, y, random_state=42)

Then, run cross_validate with these shuffled data:
scores = cross_validate(RandomForestRegressor(),X_s, y_s, cv=5, scoring='r2')

Change your function to use
kf = KFold(shuffle=False) # no more shuffling (although it should not hurt)

and run it with the already shuffled data:
scores = my_cross_val(RandomForestRegressor(), X_s, y_s)

Now the results should be similar - but not yet identical. You could turn them to identical if you define already kf = KFold(shuffle=False, random_state=0) before (and outside of the function), and run cross_validate as
scores = cross_validate(RandomForestRegressor(), X_s, y_s, cv=kf, scoring='r2') # cv=kf

i.e. using the exact same CV partition in both cases (you should also set the same random_state to the kf definition inside the function).
",cross-validation
r smoothspline loocverror depends on order of datapoints,"I wanted to fit a smoothing spline to some data and I noticed that the internally computed LOOCV-error seems to depend on whether the data is unordered or not. Concretely, I only get the expected result when the data is ordered.
I don't see why this is to be expected? Any help?
set.seed(0)
x <- seq(1:10)
y <- x^2 + rnorm(10,0,2)

fit.ss <- smooth.spline(x=x, y=y,  cv=TRUE)
cat(""CV ordered: "",format(fit.ss$cv.crit))
# CV ordered:  13.46173

xu <- sample(x)
yu <- y[xu]
fit.ss.u <- smooth.spline(x=xu, y=yu,  cv=TRUE)
cat(""CV unorderd: "",format(fit.ss.u$cv.crit))
# CV unorderd:  65552.74

spar.opt <- fit.ss$spar
preds <- rep(NA, 10)
for (i in 1:10){
  ss <- smooth.spline(x=x[-i], y=y[-i],  cv=TRUE, spar=spar.opt)
  preds[i] <- predict(ss,x=x[i])$y
}
cat(""CV manual: "",format(mean((preds - y)**2)))
# CV manual:  13.49424


CV ordered and CV manual are (almost) the same and as expected, whereas the unordered version is completely off.
Note that this is a duplicate of https://stats.stackexchange.com/q/561802/213798, where I don't seem to get any input.
","Looks like a bug in smooth.spline.  When it calculates cv.crit internally, it compares observations in the original order to predictions with x ordered.  (I'm not sure what the exact difference is, but presumably it's some sort of ""leave one out"" calculation.)
Here's the code:
cv.crit <-
    if(is.na(cv)) NA
    else {
        r <- y - fit$ty[ox]
        if(cv) {
            ww <- wbar
            ww[ww == 0] <- 1
            r <- r / (1 - (lev[ox] * w)/ww[ox])
            if(no.wgts) mean(r^2) else weighted.mean(r^2, w)
        } else
            (if(no.wgts) mean(r^2) else weighted.mean(r^2, w)) /
                (1 - (df.offset + penalty * df)/n)^2
    }

On the 4th line, things look wrong.  At this point with your unsorted data, I see
Browse[2]> y
 [1]  47.142866  80.988466 104.809307  25.829283  63.410559   3.525909  32.920100   3.347533  18.544859  11.659599

and
Browse[2]> fit$ty[ox]
 [1]   2.458502   5.274807  11.019719  17.995820  25.281214  34.165585  46.918576  63.054358  82.093996 103.915902

So it looks as though fit$ty[ox] is based on ordered x values, whereas y is in the original order.
Unfortunately, the correction isn't obvious:  ox is TRUE at this point, so it's not doing anything.  What they really need to do is to sort y in the same way as fit$ty got sorted.  But there are probably other problems elsewhere, because when I tried that, it wasn't enough to fix things.
This bug has been reported:  https://bugs.r-project.org/show_bug.cgi?id=18294
",cross-validation
kfold cv in brms,"I am trying to use kfold CV as a means of evaluating a model run using brms and I feel like I'm missing something. As a reproducible example, my data are structured as a binary response (0, 1) dependent on the length of an individual. Here is some code to generate and plot data similar to those I am working with:
library(brms)
library(tidyverse)
library(loo)

length <- seq(0, 100, by = 1)
n_fish_per_length <- 10

a0 <- -48
a1 <- 2
a2 <- -0.02

prob <- plogis(a0 + a1 * length + a2 * length^2)

plot(length, prob , type = 'l')

sim_data <-
  expand_grid(fish_id = seq_len(n_fish_per_length),
              length = length) %>%
  mutate(prob_use =  plogis(a0 + a1 * length + a2 * length^2)) %>%
  mutate(is_carp = rbinom(n = n(), size = 1, prob= prob_use))

ggplot(sim_data, aes(x = length, y = is_carp)) +
  geom_jitter(width = 0, height = 0.05) +
  geom_smooth(method = ""glm"", formula = y ~ x + I(x^2),
              method.args = list(family = binomial(link = ""logit"")))

I then use brms to run my model.
Bayes_Model_Binary <- brm(formula = is_carp ~ length + I(length^2),  
                          data=sim_data, 
                          family = bernoulli(link = ""logit""),
                          warmup = 2500, 
                          iter = 5000, 
                          chains = 4, 
                          inits= ""0"", 
                          cores=4,
                          seed = 123)

summary(Bayes_Model_Binary)

I'd like to use kfold CV to evaluate the model. I can use something like this:
kfold(Bayes_Model_Binary, K = 10, chains = 1, save_fits = T)

but the response in my data is highly imbalanced (~18% = 1, ~82% = 0) and my reading suggests that  I need to used stratified kfold cv to account for this. If I use:
sim_data$fold <- kfold_split_stratified(K = 10, x = sim_data$is_carp)

the data are split the way I would expect but I'm not sure what the best way is to move forward with the CV process from here. I saw this post https://mc-stan.org/loo/articles/loo2-elpd.html, but I'm not sure how to modify this to work with a brmsfit object. Alternatively, it appears that I should be able to use:
kfold(Bayes_Model_Binary, K = 10, folds = 'stratified', group = sim_data$is_carp)

but this throws an error. Likely because is_carp is the response rather than a predictor in the model. What would my group be in this context? Am I missing/misinterpreting something here? I'm assuming that there is a very simple solution here that I am overlooking but appreciate any thoughts.
","After some additional digging and learning how to access information about each fold in the analysis, I was able to determine that the structure of the data (proportion of 0s and 1s in the response) is maintained using the default settings in the kfold() function. To do this I used the following code.
First, save the kfold CV analysis as an object.
kfold1 <- kfold(Bayes_Model_Binary, K = 10, save_fits = T)

kfold1$fits is a list of the model fitting results and the observations used in the test data set (omitted) for each fold.
From this information, I created a loop to print the proportion of observations in each training data set where is_carp = 1 (could also do this for each test data set) with the following code.
for(i in 1:10){
    print(length(which(sim_data$is_carp[-kfold1$fits[i, ]$omitted] == 1)) / 
           nrow(sim_data[-kfold1$fits[i, ]$omitted, ]))
}

[1] 0.1859186
[1] 0.1925193
[1] 0.1991199
[1] 0.1914191
[1] 0.1881188
[1] 0.1848185
[1] 0.1936194
[1] 0.1980198
[1] 0.190319
[1] 0.1870187

and it's easy to then compare these proportions with the proportion of observations where is_carp = 1 from the original data set.
length(which(sim_data$is_carp == 1)) / nrow(sim_data)

[1] 0.1910891

",cross-validation
error requires numericcomplex matrixvector arguments for  cross validating glmmtmb model,"I am adapting some k-fold cross validation code written for glmer/merMod models to a glmmTMB model framework. All seems well until I try and use the output from the model(s) fit with training data to predict and exponentiate values into a matrix (to then break into quantiles/number of bins to assess predictive performance). I can get get this line to work using glmer models, but it seems when I run the same model using glmmTMB I get Error in model.matrix: requires numeric/complex matrix/vector arguments There are many other posts out there discussing this error code and I have tried converting the data frame into matrix form and changing the class of the covariates with no luck. Separately running the parts before and after the %*% works but when combined I get the error. For context, this code is intended to be run with use/availability data so the example variables may not make sense, but the problem gets shown well enough. Any suggestions as to what is going on?
library(lme4)
library(glmmTMB)

# Example with mtcars dataset
data(mtcars)

# Model both with glmmTMB and lme4
m1 <- glmmTMB(am ~ mpg + wt + (1|carb), family = poisson, data=mtcars)
m2 <- glmer(am ~ mpg + wt + (1|carb), family = poisson, data=mtcars)

#--- K-fold code (hashed out sections are original glmer version of code where different)---

# define variables
k <- 5
mod <- m1 #m2
dt <- model.frame(mod) #data used

reg.list <- list() # initialize object to store all models used for cross validation
  
# finds the name of the response variable in the model dataframe
  resp <- as.character(attr(terms(mod), ""variables""))[attr(terms(mod), ""response"") + 1]
    
# define column called sets and populates it with character ""train""
  dt$sets <- ""train""
    
# randomly selects a proportion of the ""used""/am records (i.e. am = 1) for testing data 
    dt$sets[sample(which(dt[, resp] == 1), sum(dt[, resp] == 1)/k)] <- ""test""
    
# updates the original model using only the subset of ""trained"" data
    reg <- glmmTMB(formula(mod), data = subset(dt, sets == ""train""), family=poisson,
                   control = glmmTMBControl(optimizer = optim, optArgs=list(method=""BFGS"")))

    #reg <- glmer(formula(mod), data = subset(dt, sets == ""train""), family=poisson,
    #            control = glmerControl(optimizer = ""bobyqa"", optCtrl=list(maxfun=2e5)))
    
 reg.list[[i]] <- reg # store models
    
# uses new model created with training data (i.e. reg) to predict and exponentiate values 
      predall <- exp(as.numeric(model.matrix(terms(reg), dt) %*% glmmTMB::fixef(reg)))
     #predall <- exp(as.numeric(model.matrix(terms(reg), dt) %*% lme4::fixef(reg)))


","Without looking at the code too carefully: glmmTMB::fixef(reg) returns a list (with elements cond (conditional model parameters), zi (zero-inflation parameters), disp (dispersion parameters) rather than a vector.
If you replace this bit with glmmTMB::fixef(reg)[[""cond""]] it will probably work.
",cross-validation
different confusion matrix with crossvalidation,"I am sorry if this is a long post, but i have some questions related to Confusion Matrix metric and Cross-Validation that i really need help with.
This picture from Sklearn CV link, shows that our whole dataset should be split into train and test. Then, the train set is split again into a validation part and we train our model in k-1 folds and validate in the remaining one (repeat this k times). And lastly, we test our model with the test set from the beggining.

In my problem, i have a dataset for a unbalanced binary classification problem with 42372 samples. 3615 belong to class 1, the rest are class 0.
Since my dataset is unbalanced, i was using StratifiedShuffleSplit with 5 folds, and got this:

As result, using a MLPClassfier i got the following confusion matrix:

As you can see from that matrix, half my dataset is being used for test (19361+19+1782+28 = 21190).
After this, i changed the CV strategy, and tried StratifiedKfold:

And, as Confusion Matrix, i got this:

As you can see from this second confusion matrix, my whole dataset is being used for test (38644+113+3329+286 = 42372).
So, here are my questions:
1 - Do i need to split my whole data into train/test (e.g., using train_test_split), and then feed CV iterators (KFold, StratifiedKFold, StratifiedShuffleSplit, etc) only with the train part? Or should i feed my whole data into the iterators and they will do the job of splitting it into train/test and split again this train into train and validation?
2 - About the CV strategies i tried, why StratifiedShuffleSplit is using half the data? and why StratifiedKFold uses all the data? Any of those CV is wrong? Are both wrong or are both correct? What i am missing here?
EDIT: The original code to generate the Confusion Matrix i found here. I have just modified it a little bit to fit my needs, and here it goes:
import itertools
import time as time
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from matplotlib.patches import Patch
# from sklearn.model_selection import StratifiedKFold
from sklearn.model_selection import StratifiedShuffleSplit
from sklearn.neural_network import MLPClassifier
from sklearn.metrics import confusion_matrix

n_splits = 5  # Num of Folds
stratshufkfold = StratifiedShuffleSplit(n_splits=n_splits, random_state=0)
# stratshufkfold = KFold(n_splits=n_splits)

def generate_confusion_matrix(cnf_matrix, classes, normalize=False, title='Matriz de Confusão'):
    if normalize:
        cnf_matrix = cnf_matrix.astype('float') / cnf_matrix.sum(axis=1)[:, np.newaxis]
        print(""Matriz de confusão normalizada"")
    else:
        print('Matriz de confusão, sem normalização')

    plt.imshow(cnf_matrix, interpolation='nearest', cmap=plt.get_cmap('Blues'))
    plt.title(title)
    plt.colorbar()

    tick_marks = np.arange(len(classes))
    plt.xticks(tick_marks, classes)
    plt.yticks(tick_marks, classes)

    fmt = '.2f' if normalize else 'd'
    thresh = cnf_matrix.max() / 2.

    for i, j in itertools.product(range(cnf_matrix.shape[0]), range(cnf_matrix.shape[1])):
        plt.text(j, i, format(cnf_matrix[i, j], fmt), horizontalalignment=""center"",
                 color=""white"" if cnf_matrix[i, j] > thresh else ""black"")

    plt.tight_layout()
    plt.ylabel('Real')
    plt.xlabel('Predito')

    return cnf_matrix


def plot_confusion_matrix(predicted_labels_list, y_test_list):
    cnf_matrix = confusion_matrix(y_test_list, predicted_labels_list)
    np.set_printoptions(precision=2)

    # Plot non-normalized confusion matrix
    plt.figure()
    generate_confusion_matrix(cnf_matrix, classes=class_names, title='Matriz de confusão, sem normalização')
    plt.show()

    # Plot normalized confusion matrix
    plt.figure()
    generate_confusion_matrix(cnf_matrix, classes=class_names, normalize=True, title='Matriz de confusão normalizada')
    plt.show()

def evaluate_model_MLP(x, y):
    predicted_targets = np.array([])
    actual_targets = np.array([])
    global t_inicial_MLP
    global t_final_MLP
    t_inicial_MLP = time.time()
    for train_ix, test_ix in stratshufkfold.split(x, y):
        train_x, train_y, test_x, test_y = x[train_ix], y[train_ix], x[test_ix], y[test_ix]

        # Fit
        classifier = MLPClassifier(activation='relu', batch_size=56, solver='sgd').fit(train_x, train_y)

        
        predicted_labels = classifier.predict(test_x)

        predicted_targets = np.append(predicted_targets, predicted_labels)
        actual_targets = np.append(actual_targets, test_y)
    t_final_MLP = time.time()
    return predicted_targets, actual_targets


predicted_target_MLP, actual_target_MLP = evaluate_model_MLP(x, y)
plot_confusion_matrix(predicted_target_MLP, actual_target_MLP)
acuracia_MLP = accuracy_score(actual_target_MLP, predicted_target_MLP)

","As specified within the comment, for what concerns the first question, the first option is the way to go. Namely, splitting the whole dataset via train_test_split and then calling method .split() of the chosen cross-validator object on the training set.
For the second point, the issue is hidden behind some default parameters of StratifiedKFold and StratifiedShuffleSplit and on the sligthly different meaning of parameter n_splits.

For what concerns StratifiedKFold, the parameter n_splits identifies the number of folds you're considering as per documentation. Therefore, imposing n_splits=5 means that the model will be trained on 4-folds (80% of the training set) and tested on one fold (20% of the training set), for each possible combination.

For what concerns StratifiedShuffleSplit, the parameter n_splits specifies the number of reshuffling and splitting iterations. On the other side, it is the parameter train_size (together with test_size) to define how big the folds will be (relatively to the size of the training set). In particular, according to the docs, the default setting defines that, if none of them is specified, train_size=0.9 (90% of the training set) and test_size=0.1 (10% of the training set).
Therefore specifying test_size within the StratifiedShuffleSplit constructor - eg - should solve your problem:
stratshufkfold = StratifiedShuffleSplit(n_splits=n_splits, random_state=0, test_size=0.2)


",cross-validation
using sklearn cross_val_score and kfolds to fit and help predict model,"I'm trying to understand using kfolds cross validation from the sklearn python module.
I understand the basic flow:

instantiate a model e.g. model = LogisticRegression()
fitting the model e.g. model.fit(xtrain, ytrain)
predicting e.g. model.predict(ytest)
use e.g. cross val score to test the fitted model accuracy.

Where i'm confused is using sklearn kfolds with cross val score. As I understand it the cross_val_score function will fit the model and predict on the kfolds giving you an accuracy score for each fold.
e.g. using code like this:
kf = KFold(n=data.shape[0], n_folds=5, shuffle=True, random_state=8)
lr = linear_model.LogisticRegression()
accuracies = cross_val_score(lr, X_train,y_train, scoring='accuracy', cv = kf)

So if I have a dataset with training and testing data, and I use the cross_val_score function with kfolds to determine the accuracy of the algorithm on my training data for each fold, is the model now fitted and ready for prediction on the testing data?
So in the case above using lr.predict
","No the model is not fitted. Looking at the source code for cross_val_score:

scores=parallel(delayed(_fit_and_score)(clone(estimator),X,y,scorer,
                                        train,test,verbose,None,fit_params)


As you can see, cross_val_score clones the estimator before fitting the fold training data to it. cross_val_score will give you output an array of scores which you can analyse to know how the estimator performs for different folds of the data to check if it overfits the data or not. You can know more about it here
You need to fit the whole training data to the estimator once you are satisfied with the results of cross_val_score, before you can use it to predict on test data. 
",cross-validation
building ml classifier with imbalanced data,"I have a dataset with 1400 obs and 19 columns. The Target variable has values 1 (value that I am most interested in) and 0. The distribution of classes shows imbalance (70:30).
Using the code below I am getting weird values (all 1s). I am not figuring out if this is due to a problem of overfitting/imbalance data or to feature selection (I used Pearson correlation since all values are numeric/boolean).
I am thinking that the steps followed are wrong.
import numpy as np
import math
import sklearn.metrics as metrics
from sklearn.metrics import f1_score

y = df['Label']
X = df.drop('Label',axis=1)

def create_cv(X,y):
    if type(X)!=np.ndarray:
        X=X.values
        y=y.values
 
    test_size=1/5
    proportion_of_true=y[y==1].shape[0]/y.shape[0]
    num_test_samples=math.ceil(y.shape[0]*test_size)
    num_test_true_labels=math.floor(num_test_samples*proportion_of_true)
    num_test_false_labels=math.floor(num_test_samples-num_test_true_labels)
    
    y_test=np.concatenate([y[y==0][:num_test_false_labels],y[y==1][:num_test_true_labels]])
    y_train=np.concatenate([y[y==0][num_test_false_labels:],y[y==1][num_test_true_labels:]])

    X_test=np.concatenate([X[y==0][:num_test_false_labels] ,X[y==1][:num_test_true_labels]],axis=0)
    X_train=np.concatenate([X[y==0][num_test_false_labels:],X[y==1][num_test_true_labels:]],axis=0)
    return X_train,X_test,y_train,y_test

X_train,X_test,y_train,y_test=create_cv(X,y)
X_train,X_crossv,y_train,y_crossv=create_cv(X_train,y_train)
    
tree = DecisionTreeClassifier(max_depth = 5)
tree.fit(X_train, y_train)       

y_predict_test = tree.predict(X_test)

print(classification_report(y_test, y_predict_test))
f1_score(y_test, y_predict_test)

Output:
     precision    recall  f1-score   support

           0       1.00      1.00      1.00        24
           1       1.00      1.00      1.00        70

    accuracy                           1.00        94
   macro avg       1.00      1.00      1.00        94
weighted avg       1.00      1.00      1.00        94

Has anyone experienced similar issues in building a classifier when data has imbalance, using CV and/or under sampling? Happy to share the whole dataset, in case you might want to replicate the output.
What I would like to ask you for some clear answer to follow that can show me the steps and what I am doing wrong.
I know that, to reduce overfitting and work with balance data, there are some methods such as random sampling (over/under), SMOTE, CV. My idea is

Split the data on train/test taking into account imbalance
Perform CV on trains set
Apply undersampling only on a test fold
After the model has been chosen with the help of CV, undersample the train set and train the classifier
Estimate the performance on the untouched test set
(f1-score)

as also outlined in this question: CV and under sampling on a test fold .
I think the steps above should make sense, but happy to receive any feedback that you might have on this.
","When you have imbalanced data you have to perform stratification. The usual way is to oversample the class that has less values.
Another option is to train your algorithm with less data. If you have a good dataset that should not be a problem. In this case you grab first the samples from the less represented class use the size of the set to compute how many samples to get from the other class:
This code may help you split your dataset that way:
def split_dataset(dataset: pd.DataFrame, train_share=0.8):
    """"""Splits the dataset into training and test sets""""""
    all_idx = range(len(dataset))
    train_count = int(len(all_idx) * train_share)

    train_idx = random.sample(all_idx, train_count)
    test_idx = list(set(all_idx).difference(set(train_idx)))

    train = dataset.iloc[train_idx]
    test = dataset.iloc[test_idx]

    return train, test

def split_dataset_stratified(dataset, target_attr, positive_class, train_share=0.8):
    """"""Splits the dataset as in `split_dataset` but with stratification""""""

    data_pos = dataset[dataset[target_attr] == positive_class]
    data_neg = dataset[dataset[target_attr] != positive_class]

    if len(data_pos) < len(data_neg):
        train_pos, test_pos = split_dataset(data_pos, train_share)
        train_neg, test_neg = split_dataset(data_neg, len(train_pos)/len(data_neg))
        # set.difference makes the test set larger
        test_neg = test_neg.iloc[0:len(test_pos)]
    else:
        train_neg, test_neg = split_dataset(data_neg, train_share)
        train_pos, test_pos = split_dataset(data_pos, len(train_neg)/len(data_pos))
        # set.difference makes the test set larger
        test_pos = test_pos.iloc[0:len(test_neg)]

    return train_pos.append(train_neg).sample(frac = 1).reset_index(drop = True), \
           test_pos.append(test_neg).sample(frac = 1).reset_index(drop = True)

Usage:
train_ds, test_ds = split_dataset_stratified(data, target_attr, positive_class)

You can now perform cross validation on train_ds and evaluate your model in test_ds.
",cross-validation
shuffle split cross validation what are the limitations,"In the sklearn documentation for sklearn.cross_validation.ShuffleSplit, it states:
Note: contrary to other cross-validation strategies, random splits do not guarantee that all folds will be different, although this is still very likely for sizeable datasets.
Is this an issue? If so, why?
","Contrary to the most often used KFold cross validation strategy, the Shuffle Split uses random samples of elements in each iteration. For a working example, let's consider a simple training dataset with 10 observations;
Training data = [1,2,3,4,5,6,7,8,9,10]

KFold (k=5)


Shuffle the data, imagine it is now [6,9,1,4,10,5,7,2,3,8]
Create folds; Fold 1 = [6,9], Fold 2 = [1,4], Fold 3 = [10,5], Fold 4 =
[7,2] and Fold 5 = [3,8]
Train keeping one fold aside each iteration for evaluation and using all others


Shuffle split (n_iter=3, test_size=0.2)

It works iterative manner where you specify number of iterations (default n_iter=10 in sklearn)

Each iteration shuffle the data; [6,9,1,4,10,3,8,2,5,7], [6,2,1,4,10,7,5,9,3,8] and [2,6,1,4,10,5,7,9,3,8]
Split into specified train and evaluation dataset as chosen with the hyper-parameter (test_size); Training data are [6,9,1,4,10,3,8,2], [6,2,1,4,10,7,5,9] and [2,6,1,4,10,5,7,9] respectively. Test data are [5,7], [3,8] and [3,8] respectively.

As you can notice, although the shuffle is different (technically it can be same), the training and testing data for the last two iteration are exactly same. As the number of iterations increase, your chance of fitting the same dataset increases which is counter-intuitive to the cross-validation idea where we would like get an estimate of generalizability of our model with limited amount of data. On the other hand, the datasets usually contains numerous observations so that having the same (or very similar) training and test datasets is not an issue. Keeping number of iterations high enough improves the generalizability of your results.
",cross-validation
calculate the average of each metric in cross validation,"I'm trying to calculate some metrics using StratifiedKFold cross validation.
skfold = StratifiedKFold(n_splits=5, random_state=42, shuffle=True)

dtc_score = cross_validate(models[0], X, y, scoring=('accuracy', 'precision', 'recall', 'f1'), cv=skfold, n_jobs=-1, verbose=1)
rfc_score = cross_validate(models[1], X, y, scoring=('accuracy', 'precision', 'recall', 'f1'), cv=skfold, n_jobs=-1, verbose=1)
abc_score = cross_validate(models[2], X, y, scoring=('accuracy', 'precision', 'recall', 'f1'), cv=skfold, n_jobs=-1, verbose=1)
etc_score = cross_validate(models[3], X, y, scoring=('accuracy', 'precision', 'recall', 'f1'), cv=skfold, n_jobs=-1, verbose=1)
gbc_score = cross_validate(models[4], X, y, scoring=('accuracy', 'precision', 'recall', 'f1'), cv=skfold, n_jobs=-1, verbose=1)
bgc_score = cross_validate(models[5], X, y, scoring=('accuracy', 'precision', 'recall', 'f1'), cv=skfold, n_jobs=-1, verbose=1)
knn_score = cross_validate(models[6], X, y, scoring=('accuracy', 'precision', 'recall', 'f1'), cv=skfold, n_jobs=-1, verbose=1)
logreg_score = cross_validate(models[7], X, y, scoring=('accuracy', 'precision', 'recall', 'f1'), cv=skfold, n_jobs=-1, verbose=1)
nb_score = cross_validate(models[8], X, y, scoring=('accuracy', 'precision', 'recall', 'f1'), cv=skfold, n_jobs=-1, verbose=1)
svm_score = cross_validate(models[9], X, y, scoring=('accuracy', 'precision', 'recall', 'f1'), cv=skfold, n_jobs=-1, verbose=1)
xgb_score = cross_validate(models[10], X, y, scoring=('accuracy', 'precision', 'recall', 'f1'), cv=skfold, n_jobs=-1, verbose=1)
mlp_score = cross_validate(models[11], X, y, scoring=('accuracy', 'precision', 'recall', 'f1'), cv=skfold, n_jobs=-1, verbose=1)

After that I put the results of each calculation into the dataframe, but the results don't match what I want.
cv_result = [
    dtc_score, rfc_score, abc_score, etc_score, gbc_score, bgc_score, 
    knn_score, logreg_score, nb_score, svm_score, xgb_score, mlp_score]


df_cv_result = pd.DataFrame(cv_result, index=model_name)
df_cv_result

The result is like this:





fit_time
score_time
test_accuracy
test_precision
test_recall
test_f1




DecisionTreeClassifier
[0.06297850608825684, 0.06297850608825684, 0.1...
[0.025590181350708008, 0.025590181350708008, 0...
[0.783008658008658, 0.7943722943722944, 0.7662...
[0.7193229901269393, 0.7398843930635838, 0.708...
[0.7162921348314607, 0.7191011235955056, 0.668...
[0.7178043631245602, 0.7293447293447292, 0.687...


RandomForestClassifier
[1.759207010269165, 1.774831771850586, 1.75920...
[0.10936832427978516, 0.10936856269836426, 0.1...
[0.8257575757575758, 0.8138528138528138, 0.806...
[0.7809798270893372, 0.7697947214076246, 0.769...
[0.7612359550561798, 0.7373595505617978, 0.709...
[0.7709815078236132, 0.7532281205164992, 0.738...


AdaBoostClassifier
[0.7297384738922119, 0.7453627586364746, 0.721...
[0.07508277893066406, 0.07508540153503418, 0.0...
[0.8235930735930735, 0.8295454545454546, 0.818...
[0.796923076923077, 0.7976011994002998, 0.8006...
[0.7275280898876404, 0.7471910112359551, 0.705...
[0.7606461086637298, 0.7715736040609138, 0.749...


ExtraTreesClassifier
[1.8575339317321777, 1.888782024383545, 1.8731...
[0.12499260902404785, 0.12499213218688965, 0.1...
[0.808982683982684, 0.8008658008658008, 0.7916...
[0.760522496371553, 0.7478386167146974, 0.7599...
[0.7359550561797753, 0.7289325842696629, 0.671...
[0.7480371163454677, 0.7382645803698435, 0.712...


GradientBoostingClassifier
[2.078220844268799, 2.150218963623047, 2.11822...
[0.02400040626525879, 0.027779102325439453, 0....
[0.8365800865800865, 0.8344155844155844, 0.825...
[0.8005865102639296, 0.8038922155688623, 0.791...
[0.7668539325842697, 0.7542134831460674, 0.742...
[0.7833572453371592, 0.7782608695652172, 0.766...


BaggingClassifier
[0.486358642578125, 0.486358642578125, 0.47073...
[0.015625953674316406, 0.039700984954833984, 0...
[0.8143939393939394, 0.808982683982684, 0.7927...
[0.786046511627907, 0.7765793528505393, 0.7683...
[0.7120786516853933, 0.7078651685393258, 0.661...
[0.7472365512159176, 0.7406318883174137, 0.710...


KNeighborsClassifier
[0.01562809944152832, 0.01562809944152832, 0.0...
[0.9189648628234863, 0.9033389091491699, 0.918...
[0.8041125541125541, 0.8122294372294372, 0.803...
[0.7708978328173375, 0.77526395173454, 0.76687...
[0.699438202247191, 0.7219101123595506, 0.7022...
[0.7334315169366716, 0.7476363636363637, 0.733...


LogisticRegression
[0.12395191192626953, 0.14353656768798828, 0.1...
[0.01958465576171875, 0.04000663757324219, 0.0...
[0.8079004329004329, 0.8181818181818182, 0.811...
[0.7856, 0.7955974842767296, 0.793214862681744...
[0.6896067415730337, 0.7106741573033708, 0.689...
[0.7344801795063575, 0.7507418397626112, 0.737...


GaussianNB
[0.02400040626525879, 0.02400040626525879, 0.0...
[0.02399587631225586, 0.02399587631225586, 0.0...
[0.7738095238095238, 0.7797619047619048, 0.764...
[0.7963709677419355, 0.8105906313645621, 0.812...
[0.5547752808988764, 0.5589887640449438, 0.505...
[0.6539735099337749, 0.6616791354945969, 0.623...


SVC
[5.1321375370025635, 5.216028690338135, 4.9741...
[2.7087018489837646, 2.7156307697296143, 2.633...
[0.8327922077922078, 0.8295454545454546, 0.824...
[0.7933042212518195, 0.794074074074074, 0.7833...
[0.7654494382022472, 0.7528089887640449, 0.751...
[0.7791279485346676, 0.7728911319394376, 0.767...


XGBClassifier
[2.78363299369812, 2.78363299369812, 2.7516334...
[0.03450345993041992, 0.03450345993041992, 0.0...
[0.8262987012987013, 0.8208874458874459, 0.808...
[0.7879234167893961, 0.7822222222222223, 0.775...
[0.7514044943820225, 0.7415730337078652, 0.707...
[0.7692307692307693, 0.7613554434030282, 0.740...


MLPClassifier
[20.106656074523926, 20.14053773880005, 19.947...
[0.023995399475097656, 0.0240020751953125, 0.0...
[0.8327922077922078, 0.8295454545454546, 0.819...
[0.7899280575539568, 0.7815602836879433, 0.773...
[0.7710674157303371, 0.773876404494382, 0.7528...
[0.7803837953091683, 0.7776993648553283, 0.762...




I don't want to include every k-fold and just want to calculate the average. How do I calculate the average for each metric?
","You can simply do as follow to average your score:
cv_result = [
    dtc_score, rfc_score, abc_score, etc_score, gbc_score, bgc_score, 
    knn_score, logreg_score, nb_score, svm_score, xgb_score, mlp_score]

results = [{k : v.mean() for k, v in result.items()} for result in cv_result]
df_cv_result = pd.DataFrame(results, index=model_name)

Note: you might want to calculate the standard deviation as it might help you choose to best model.
EDIT:
It might not be the most optimal solution but you could do as follows to add both mean and std:
results = []
for result in cv_result:
    tmp = dict()
    for k, v in result.items():
        tmp.update({f'{k}_mean()': v.mean(), f'{k}_std' : v.std()})
    results.append(tmp)
df_cv_result = pd.DataFrame(results, index=model_name)

",cross-validation
what is the difference between a quotnormalquot kfold crossvalidation using shuffletrue and a repeated kfold crossvalidation,"could anyone explain the difference between a ""normal"" k-fold cross-validation using the shuffle function, e.g.
kf = KFold(n_splits = 5, shuffle = True)

and a repeated k-fold cross-validation? Shouldn't they return the same results?
Having a hard time understanding the difference.
Any hint is appreciated.
","As its name says, RepeatedKFold is a repeated KFold.
It executes it n_repeats times. When n_repeats=1, the former performs exactly as the latter when shuffle=True.
They do not return the same splits because random_state=None by default, that is, you did not specify it.
Therefore, they use different seeds to (pseudo-)randomly shuffle data.
When they have the same random_state and are repeated once, then both lead the same splits. For a deeper understanding try the following:
import pandas as pd
from sklearn.model_selection import KFold, RepeatedKFold
                     
data = pd.DataFrame([['red', 'strawberry'], # color, fruit
                  ['red', 'strawberry'], 
                  ['red', 'strawberry'],
                  ['red', 'strawberry'],
                  ['red', 'strawberry'],
                  ['yellow', 'banana'],
                  ['yellow', 'banana'],
                  ['yellow', 'banana'],
                  ['yellow', 'banana'],
                  ['yellow', 'banana']])

X = data[0]

# KFold
for train_index, test_index in KFold(n_splits=2, shuffle=True, random_state=1).split(X):
    print(""TRAIN:"", train_index, ""TEST:"", test_index)

# RepeatedKFold
for train_index, test_index in RepeatedKFold(n_splits=2, n_repeats=1, random_state=1).split(X):
    print(""TRAIN:"", train_index, ""TEST:"", test_index)

You should obtain the following:
TRAIN: [1 3 5 7 8] TEST: [0 2 4 6 9]
TRAIN: [0 2 4 6 9] TEST: [1 3 5 7 8]

TRAIN: [1 3 5 7 8] TEST: [0 2 4 6 9]
TRAIN: [0 2 4 6 9] TEST: [1 3 5 7 8]

",cross-validation
how made crossvalidation with python,"Hi i made a neural network and i need to do a cross validation.
I don't know how made that, specifically how train or made that.
if someone knows made that please write or give me some indications.
here is my code:
###Division Train / Test
X = df.drop('Peso secado',axis=1)  #Variables de entrada, menos la variable de salida
y = df['Peso secado']              #Variable de salida

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.3,random_state=101)

###

from sklearn.preprocessing import MinMaxScaler
scaler = MinMaxScaler()
X_train= scaler.fit_transform(X_train)
X_train
X_test = scaler.transform(X_test)
X_test



###Creacion del modelo###
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Activation
from tensorflow.keras.optimizers import Adam
import tensorflow as tf

model = Sequential()
num_neuronas = 50
model.add(tf.keras.layers.Dense(units=6, activation='sigmoid', input_shape=(6, )))
model.add(Dense(num_neuronas,activation='relu'))
model.add(tf.keras.layers.Dense(units=1, activation='linear')) 

#Buscar mejor funcion de activacion para capa de salida sigmoid? o linear?
model.summary()
model.compile(optimizer='adam',loss='mse')

###Entrenamiento###
model.fit(x = X_train, y = y_train.values,
          validation_data=(X_test,y_test.values), batch_size=10, epochs=1000) 

losses = pd.DataFrame(model.history.history)  
losses
losses.plot()
   
###Evaluacion###
from sklearn.metrics import mean_squared_error,mean_absolute_error,explained_variance_score,mean_absolute_percentage_error
X_test
predictions = model.predict(X_test)
mean_absolute_error(y_test,predictions)
mean_absolute_percentage_error(y_test,predictions)

mean_squared_error(y_test,predictions)
explained_variance_score(y_test,predictions)  

mean_absolute_error(y_test,predictions)/df['Peso secado'].mean() 
mean_absolute_error(y_test,predictions)/df['Peso secado'].median()

Some recomendation for training or validation would be helpful
","My first observation is that the code is pretty ugly and unstructured. You should import the modules on the top part of your code
For performing cross validation first import the module from sklearn (and all other modules that you need)
from sklearn.model_selection import StratifiedKFold

I'd put the model definition in a separate function as such:
def get_model():
  model = Sequential()
  model.add(Dense(4, input_dim=8, activation='relu'))
  model.add(Dense(1, activation='sigmoid'))
  model.compile(loss='binary_crossentropy', optimizer='adam')
  return model

Define your variables and if you are working with tensorflow / Keras, do something like this:
BATCH_SIZE = 64  #  128
EPOCHS = 100

k = 10
# Use stratified k-fold if the data is imbalanced
kf = StratifiedKFold(n_splits=k, shuffle=False, random_state=None)

# here comes the Cross validation
fold_index = 1
for train_index, test_index in kf.split(X, y):
            X_train = X[train_index]
            y_train = y[train_index]

            X_test = X[test_index]
            y_test = y[test_index]

            # fit the model on the training set
            model = get_model()

            model.fit(
                X_train,
                y_train,
                batch_size=BATCH_SIZE,
                epochs=EPOCHS,
                verbose=0,
                validation_data=(X_test, y_test),
            )

            # predict values
            # pred_values = model.predict(X_test)
            pred_values_prob = np.array(model(X_test))

Note: when working with tensorflow you need to define a new model every time in the loop. This is not the case with sklearn as sklearn starts with fresh initialized weights when called. Here you need to do that separately.
",cross-validation
deploying and undeploying a finetuned model on demand to avoid hourly costs,"I have a fine-tuned model on Azure OpenAI that I expect to use perhaps during a few minutes every working day. The rest of the week nobody will use it. We are 3 potential users. Our backend are Azure Functions.
Since you pay $1.7 per hour regardless of actual usage, it can get unnecessarily expensive. What is the best way you can think for deploying it and undeploying it on demand, to avoid costs during inactivity times?
I was thinking of detecting 15 min of inactivity and then sending an undeploy call, but since we use Azure Functions, it is not easy to check inactivity. Also, we don't want to undeploy when one of the 3 people is using it.
Perhaps best to use OpenAI instead of Azure OpenAI, since there is no hourly rate?
","For fine tune model, it is mentioned in documentation itself

it continues to incur an hourly cost regardless of whether you're actively using it

Workaround in this case would be, to use function app to deploy and delete the deploy based on the conditions.
First, you need to add diagnostic setting.
Go to your OpenAI resource, under Monitoring select Diagnostic settings.

Next, add below configuration.

Logs needed and archive to storage account.
In storage account the logs will be in path like below.
insights-logs-requestresponse/resourceId=/SUBSCRIPTIONS/0B361xxxxxB-AE97-yyyy/RESOURCEGROUPS/<resourcegrp>/PROVIDERS/MICROSOFT.COGNITIVESERVICES/ACCOUNTS/OPENAI-SOAUTOMATION/y=2024/m=11/d=18/h=10/m=00/PT1H.json

In PT1H.json the content will be like below.
{ ""callerIpAddress"": ""27.63.242.***"", ""category"": ""RequestResponse"", ""correlationId"": ""d7d6xyz6d"", ""durationMs"": 835, ""event"": ""ShoeboxCallResult"", ""location"": ""eastus"", ""operationName"": ""ChatCompletions_Create"", ""properties"": ""{\""apiName\"":\""Azure OpenAI API version 2024-02-15-preview\"",\""requestTime\"":638675238320405867,\""requestLength\"":242,\""responseTime\"":638675238328757797,\""responseLength\"":1419,\""objectId\"":\""34d06566-4703-4953-9bc3-295e54b1b7a2\"",\""streamType\"":\""Non-Streaming\"",\""modelDeploymentName\"":\""gpt35-soautomation\"",\""modelName\"":\""gpt-35-turbo\"",\""modelVersion\"":\""0301\""}"", ""resourceId"": ""/SUBSCRIPTIONS/0B36xyz75DB0/RESOURCEGROUPS/x_y_z/PROVIDERS/MICROSOFT.COGNITIVESERVICES/ACCOUNTS/OPENAI-SOAUTOMATION"", ""resultSignature"": ""200"", ""time"": ""2024-11-18T10:54:59.0810000Z"", ""Tenant"": ""eastus""}
{ ""callerIpAddress"": ""27.63.242.***"", ""category"": ""RequestResponse"", ""correlationId"": ""1813d0xxxyyyzzz6e"", ""durationMs"": 91, ""event"": ""ShoeboxCallResult"", ""location"": ""eastus"", ""operationName"": ""ChatCompletions_Create"", ""properties"": ""{\""apiName\"":\""Azure OpenAI API version 2024-10-01-preview\"",\""requestTime\"":638675232368776560,\""requestLength\"":313,\""responseTime\"":638675232369687659,\""responseLength\"":0,\""objectId\"":\""\"",\""streamType\"":\""Streaming\"",\""modelDeploymentName\"":\""gpt-4o\"",\""modelName\"":\""gpt-4o\"",\""modelVersion\"":\""2024-05-13\""}"", ""resourceId"": ""/SUBSCRIPTIONS/0B361z_y_z/RESOURCEGROUPS/x_y_z/PROVIDERS/MICROSOFT.COGNITIVESERVICES/ACCOUNTS/OPENAI-SOAUTOMATION"", ""resultSignature"": ""200"", ""time"": ""2024-11-18T10:43:16.8470000Z"", ""Tenant"": ""eastus""}

In this you need to extract latest PT1H.json and max time field in it.
Below is the sample code you can try, alter it according to your needs.
import os
import json
from datetime import datetime, timedelta
import pytz

# path till OPENAI-SOAUTOMATION
base_path = ""insights-logs-requestresponse/resourceId=/SUBSCRIPTIONS/0B361xxxxxB-AE97-yyyy/RESOURCEGROUPS/<resourcegrp>/PROVIDERS/MICROSOFT.COGNITIVESERVICES/ACCOUNTS/OPENAI-SOAUTOMATION""

def get_latest_path(base_path):
    # get latest path according to date.
    latest_year = max([int(f.split('=')[1]) for f in os.listdir(base_path) if f.startswith('y=')])
    year_path = os.path.join(base_path, f""y={latest_year}"")

    latest_month = max([int(f.split('=')[1]) for f in os.listdir(year_path) if f.startswith('m=')])
    month_path = os.path.join(year_path, f""m={latest_month}"")

    latest_day = max([int(f.split('=')[1]) for f in os.listdir(month_path) if f.startswith('d=')])
    day_path = os.path.join(month_path, f""d={latest_day}"")

    latest_hour = max([int(f.split('=')[1]) for f in os.listdir(day_path) if f.startswith('h=')])
    hour_path = os.path.join(day_path, f""h={latest_hour}"")

    latest_minute = max([int(f.split('=')[1]) for f in os.listdir(hour_path) if f.startswith('m=')])
    minute_path = os.path.join(hour_path, f""m={latest_minute}"")

    return os.path.join(minute_path, ""PT1H.json"")

def find_latest_time_field(json_path):
 
    with open(json_path, 'r') as file:
        data = json.load(file)

    
    times = []
    for record in data:
        time_str = record.get(""time"") #Default time will be in UTC
        if time_str:
            times.append(datetime.strptime(time_str, ""%Y-%m-%dT%H:%M:%S.%fZ""))
    
    # taking latest time
    if times:
        latest_time = max(times)
        return latest_time
    else:
        print(""No valid `time` fields found."")
        return None

def check_time_difference(latest_time):
    # Current time in UTC
    current_time = datetime.utcnow()

    
    time_difference = current_time - latest_time
    print(f""Current Time (UTC): {current_time}"")
    print(f""Latest Time (UTC): {latest_time}"")
    print(f""Time Difference: {time_difference}"")

    
    if time_difference > timedelta(minutes=15):
        print(""If greater than 15 mintes delete deployment"")
    else:
        print(""If time difference is within 15 minutes don't delete"")

if __name__ == ""__main__"":
    
    latest_json_path = get_latest_path(base_path)
    print(""Latest JSON Path:"", latest_json_path)

    latest_time = find_latest_time_field(latest_json_path)
    if latest_time:
        check_time_difference(latest_time)

",fine-tune
target modules for applying peft  lora on different models,"I am looking at a few different examples of using PEFT on different models. The LoraConfig object contains a target_modules array. In some examples, the target modules are [""query_key_value""], sometimes it is [""q"", ""v""], sometimes something else.
I don't quite understand where the values of the target modules come from. Where in the model page should I look to know what the LoRA adaptable modules are?
One example (for the model Falcon 7B):
peft_config = LoraConfig(
    lora_alpha=lora_alpha,
    lora_dropout=lora_dropout,
    r=lora_r,
    bias=""none"",
    task_type=""CAUSAL_LM"",
    target_modules=[
        ""query_key_value"",
        ""dense"",
        ""dense_h_to_4h"",
        ""dense_4h_to_h"",
    ]

Another example (for the model Opt-6.7B):
config = LoraConfig(
    r=16,
    lora_alpha=32,
    target_modules=[""q_proj"", ""v_proj""],
    lora_dropout=0.05,
    bias=""none"",
    task_type=""CAUSAL_LM""
)

Yet another (for the model Flan-T5-xxl):
lora_config = LoraConfig(
 r=16,
 lora_alpha=32,
 target_modules=[""q"", ""v""],
 lora_dropout=0.05,
 bias=""none"",
 task_type=TaskType.SEQ_2_SEQ_LM
)

","Let's say that you load some model of your choice:
model = AutoModelForCausalLM.from_pretrained(""some-model-checkpoint"")
Then you can see available modules by printing out this model:
print(model)
You will get something like this (SalesForce/CodeGen25):
LlamaForCausalLM(
  (model): LlamaModel(
    (embed_tokens): Embedding(51200, 4096, padding_idx=0)
    (layers): ModuleList(
      (0-31): 32 x LlamaDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
          (act_fn): SiLUActivation()
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
  )
  (lm_head): Linear(in_features=4096, out_features=51200, bias=False)
)

In my case, you can find the LLamaAttention module that contains q_proj, k_proj, v_proj, and o_proj. And this are some modules available for LoRA.
I suggest you reading more about which modules to use in LoRA paper.
",fine-tune
error when pushing llama31 7b finetuned model to huggingface,"I'm having an issue pushing a fine-tuned Llama 3.1 model to Huggingface, getting the error below.  All of the literature that I've read suggests that the code below that I'm using to push is correct, and the model exists on my page, there are just no files or model card associated with it.
model.push_to_hub(NEW_MODEL, token=token, max_shard_size=""10GB"")
The loading of said model code is below, done before model.push:
tokenizer = AutoTokenizer.from_pretrained(NEW_MODEL)

model = AutoModelForCausalLM.from_pretrained(
    MODEL_NAME,
    torch_dtype=torch.float16,
    device_map=""auto"",
)

model.resize_token_embeddings(len(tokenizer), pad_to_multiple_of=8)
model = PeftModel.from_pretrained(model, NEW_MODEL)
model = model.merge_and_unload()


  

Does anyone have a word of advice here?
Cheers!
---------------------------------------------------------------------------
IsADirectoryError                         Traceback (most recent call last)
<ipython-input-86-cac150fe4ba1> in <cell line: 1>()
----> 1 model.push_to_hub(NEW_MODEL, token=token, max_shard_size=""10GB"")

3 frames
/usr/local/lib/python3.10/dist-packages/transformers/utils/hub.py in push_to_hub(self, repo_id, use_temp_dir, commit_message, private, token, max_shard_size, create_pr, safe_serialization, revision, commit_description, tags, **deprecated_kwargs)
    917 
    918         # Create a new empty model card and eventually tag it
--> 919         model_card = create_and_tag_model_card(
    920             repo_id, tags, token=token, ignore_metadata_errors=ignore_metadata_errors
    921         )

/usr/local/lib/python3.10/dist-packages/transformers/utils/hub.py in create_and_tag_model_card(repo_id, tags, token, ignore_metadata_errors)
   1172     try:
   1173         # Check if the model card is present on the remote repo
-> 1174         model_card = ModelCard.load(repo_id, token=token, ignore_metadata_errors=ignore_metadata_errors)
   1175     except EntryNotFoundError:
   1176         # Otherwise create a simple model card from template

/usr/local/lib/python3.10/dist-packages/huggingface_hub/repocard.py in load(cls, repo_id_or_path, repo_type, token, ignore_metadata_errors)
    185 
    186         # Preserve newlines in the existing file.
--> 187         with card_path.open(mode=""r"", newline="""", encoding=""utf-8"") as f:
    188             return cls(f.read(), ignore_metadata_errors=ignore_metadata_errors)
    189 

/usr/lib/python3.10/pathlib.py in open(self, mode, buffering, encoding, errors, newline)
   1117         if ""b"" not in mode:
   1118             encoding = io.text_encoding(encoding)
-> 1119         return self._accessor.open(self, mode, buffering, encoding, errors,
   1120                                    newline)
   1121 

IsADirectoryError: [Errno 21] Is a directory: 'Mottzerella/Llama-3-8B-Instruct-Finance-RAG'

Tried to push a saved model to Huggingface while logged into Google Colab.  Tried different versions of the push including use_temp_dir=True, all with the same result.  The model itself is listed in my Huggingface but none of the files are associated with it.  Expected the model to be pushed, but got the above error.
","according to https://github.com/huggingface/transformers/issues/28543
the issue stems from having a local path that has the same name as NEW_MODEL and a remote repository having the id NEW_MODEL as well with no model card.
to fix this either :

rename the local path to something else
push_to_hub to a repository with a new ID something like NEW_MODEL_2

",fine-tune
openai api returns null when i retrieve the finetuned model,"I'm fine-tuning a model and generating actions from text. I create a train.jsonl file, upload it, and fine-tune the model. However, when I try to get the name of the model I just created, it returns null.
I fine-tune a model like this:
const model = await openai.fineTuning.jobs.create({
  training_file: process.env.FILE_ID,
  model: 'babbage-002',
})

Then I try to retrieve the fine-tuned model like this:
const response = await openai.fineTuning.jobs.retrieve(
    process.env.FINE_TUNE_ID,
)

But this is the response I get from the OpenAI API:
data:  {
  object: 'fine_tuning.job',
  id: 'ftjob-NSFvxzJtTSfR5jcqQTfeDTCo',
  model: 'babbage-002',
  created_at: 1722546992,
  fine_tuned_model: null,
  organization_id: 'org-GLjhkXwkbQrLOvHk0762UcmL',
  result_files: [],
  status: 'running',
  validation_file: null,
  training_file: 'file-Lh1C4Vv1HDIv7LxXUGh9mIL9',
  hyperparameters: { n_epochs: 9, batch_size: 1, learning_rate_multiplier: 16 },
  trained_tokens: null,
  error: {},
  user_provided_suffix: null,
  seed: 1564492262,
  estimated_finish: null,
  integrations: []
}

","The fine-tuning job hasn't finished yet.
The fine-tuning flow is the following:

Create a fine-tuning job.
Fine-tuning is in progress.
Retrieve the fine-tuning job.

Try to run the following code to see if the fine-tuning is still in progress.
import OpenAI from ""openai"";

const client = new OpenAI();

async function main() {
  const list = await client.fineTuning.jobs.list();

  for await (const fineTune of list) {
    console.log(fineTune);
  }
}

main();

",fine-tune
openai api error when finetuning quotthis is not a chat model and thus not supported in the v1chatcompletions endpointquot,"I've fine-tuned a model using the OpenAI API. Now I want to use this fine-tuned model.
This is my code:
 const response = await openai.chat.completions.create({
    model: process.env.FINE_TUNE_MODEL,
    messages: [
      {
        role: ""system"",
        content: prompt,
      },
      {
        role: ""user"",
        content: inputText,
      },
    ],
    temperature: 0,
    top_p: 1,
    frequency_penalty: 0,
    presence_penalty: 0,
});

But there's a 404 error while executing this function.
This is the error message:

NotFoundError: 404 This is not a chat model and thus not supported in
the v1/chat/completions endpoint. Did you mean to  use v1/completions?

How can I use the fine-tuned model to generate a text?
","Problem
You fine-tuned one of the non-chat models but wanted to use the API endpoint, which is compatible only with the chat models.
The API endpoint you need to use for a fine-tuned model depends on the model you fine-tune.
As of today, you can fine-tune the following models:

gpt-4o-mini-2024-07-18 (chat model),
gpt-4o-2024-05-13 (chat model),
gpt-4-0613 (chat model),
gpt-3.5-turbo-0125 (chat model),
gpt-3.5-turbo-1106 (chat model),
gpt-3.5-turbo-0613 (chat model),
babbage-002 (non-chat model), and
davinci-002 (non-chat model).

Solution
Use the following rule:

If you fine-tune a chat model, use the /v1/chat/completions API endpoint.
If you fine-tune a non-chat model, use the /v1/completions API endpoint.

In other words:

If you fine-tune a chat model, use the client.chat.completions.create method.
If you fine-tune a non-chat model, use the client.completions.create method.

",fine-tune
finetuning a pretrained model with quantization and amp scaler error quotattempting to unscale fp16 gradientsquot,"I am trying to fine-tune a pretrained model with limited VRAM. To achieve this, I am using quantization and automatic mixed precision (AMP). However, I am encountering an issue that I can't seem to resolve. Could you please help me identify the problem?
Here is a minimal example:
import os
from transformers import BitsAndBytesConfig, OPTForCausalLM, GPT2TokenizerFast
import torch
from torch.cuda.amp import GradScaler, autocast

model_name = ""facebook/opt-1.3b""
cache_dir = './models'
os.environ[""CUDA_VISIBLE_DEVICES""] = ""7""

quantization_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type=""nf4"",
    bnb_4bit_compute_dtype=torch.float16
)

pretrained_model:OPTForCausalLM = OPTForCausalLM.from_pretrained(model_name, 
                                                    cache_dir=cache_dir,                                                     
                                                    quantization_config=quantization_config)
tokenizer:GPT2TokenizerFast = GPT2TokenizerFast.from_pretrained(model_name,
                                                    cache_dir=cache_dir)
optimizer = torch.optim.AdamW(pretrained_model.parameters(), lr=1e-4)
scaler = GradScaler()
input_ids = torch.LongTensor([[0, 1, 2, 3]]).to(0)
labels = torch.LongTensor([[1, 2, 3, 4]]).to(0)
with torch.autocast(device_type='cuda'):
    out = pretrained_model(input_ids=input_ids, labels=labels)
    loss = out.loss
scaler.scale(out.loss).backward()
scaler.step(optimizer) 
scaler.update()
optimizer.zero_grad()

print(f'End')

At the line scaler.step(optimizer), an error occurs:
Exception has occurred: ValueError: Attempting to unscale FP16 gradients.


Thank you in advance for your help!
","You can't fine-tune a fp16/uint8 model with AMP. AMP uses fp32 parameters. The params are autocast to fp16 for the forward pass, but AMP expects the master set of parameters to be FP32.
You also shouldn't fine-tune a quantized model in the first place. The quantization causes all sorts of numerical issues and instability during training.
What you are supposed to do is keep the quantized model static and train an adapter on top of the quantized model. You can find more details here
",fine-tune
getting peft version error while autotrain finetune on llama 2,"i did some Llama 2 finetuning with autotrain, on google colab. this is a sample text column, for fine tuning
###Human:
Here is the OCR Text extracted from a VHS tape cover. Yes, the text is surely extracted from a VHS tape, but it may have some typos and wrong characters
hi fi EMBASSY HOME ENTERTAINMENT PSYCHIC KILLER A WHOLE NEW KIND OF MURDER Mardi Rustam presents PSYCHIC KILLER Starring PAUL BURKE JIM HUTTON JULIE ADAMS NEHEMIAH PERSOFF NEVILLE BRAND Co starring ALDO RAY WHIT BISSELL ROD CAMERON and DELLA REESE Executive producer MOHAMMED RUSTAM Written by GREYDON CLARK MIKE ANGEL RAYMOND DANTON Produced by MARDI RUSTAM Directed by RAYMOND DANTON An E Embassy Pictures Release
And here is the possibly matched database entry
title Psychic killer, distributor Embassy Home Entertainment, vhs release year 1975, crew Jim Hutton Paul Burke Neville Brand, synopsis A man wrongly accused of murder and locked in a mental institution seeks revenge through his newly learned psychic gifts His deadly revenge is terrifying Jim Hutton stars

Instructions for Matching VHS Tape Details to Database Entry
~*~SURELY~*~  Confirm if both the title and VHS home video distributor from the OCR text match the database entry.
~*~POSSIBLE~*~  Indicate if only the title matches, and you're uncertain about the distributor.
~*~IMPOSSIBLE~*~  Indicate if neither the title nor the distributor matches, or if the tape and distributor are clearly not the same.
Guidelines
Focus on the title of the movie, VHS home video distributor (not theatrical distributor), VHS release year (not theatrical release year), and cast and crew members.
Consider that the synopsis was manually added to the database and may have discrepancies, and some missing information such the crew.
Explain your reasoning briefly, optimized for ML data training enrichment, but avoid rewriting details of all possible matches.

###Assistant:
- ~*~SURELY~*~ The title of the movie ""Psychic Killer"" match both in OCR extracted text and database entry. The distributor ""Embassy Home Entertainment"" also matches in both the sources. Although the release year is not mentioned in the VHS tape, it matches the common names of the cast members such as Jim Hutton and Paul Burke. Therefore, it is a confirmed match.

this is how i do autotrain
!autotrain llm --train --project_name ""llm-vhsfinetune-200s"" \
--model abhishek/llama-2-7b-hf-small-shards \
--data_path . \
--text_column text \
--use_peft \
--use_int4 \
--learning_rate 2e-4 \
--train_batch_size 2 \
--num_train_epochs 1 \
--model_max_length 1024 \
--trainer sft \
--push_to_hub \
--repo_id ""soajan/llm2-vhsfinetune-200s"" \
--block_size 1024 > training.log

after training is done, trying to load & test the model from huggingface:
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig

model_id = ""soajan/llm2-vhsfinetune-200s""
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type=""nf4"",
    bnb_4bit_compute_dtype=torch.bfloat16
)

tokenizer = AutoTokenizer.from_pretrained(model_id)
model = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=bnb_config, device_map={"""":0})

but getting the below error message, stating
ValueError: The version of PEFT you are using is not compatible, 
please use a version that is greater than 0.4.0

Full output & error trace:
Downloading (…)okenizer_config.json: 100%
705/705 [00:00<00:00, 52.3kB/s]
Downloading tokenizer.model: 100%
500k/500k [00:00<00:00, 507kB/s]
Downloading (…)/main/tokenizer.json: 100%
1.84M/1.84M [00:00<00:00, 3.73MB/s]
Downloading (…)in/added_tokens.json: 100%
21.0/21.0 [00:00<00:00, 1.21kB/s]
Downloading (…)cial_tokens_map.json: 100%
435/435 [00:00<00:00, 33.4kB/s]
Downloading (…)/adapter_config.json: 100%
458/458 [00:00<00:00, 35.5kB/s]
Loading checkpoint shards: 100%
10/10 [02:20<00:00, 10.89s/it]
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
<ipython-input-69-1fbd90a0393c> in <cell line: 13>()
     11 
     12 tokenizer = AutoTokenizer.from_pretrained(model_id)
---> 13 model = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=bnb_config, device_map={"""":0})

3 frames
/usr/local/lib/python3.10/dist-packages/transformers/models/auto/auto_factory.py in from_pretrained(cls, pretrained_model_name_or_path, *model_args, **kwargs)
    533         elif type(config) in cls._model_mapping.keys():
    534             model_class = _get_model_class(config, cls._model_mapping)
--> 535             return model_class.from_pretrained(
    536                 pretrained_model_name_or_path, *model_args, config=config, **hub_kwargs, **kwargs
    537             )

/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py in from_pretrained(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)
   3223 
   3224         if has_adapter_config:
-> 3225             model.load_adapter(
   3226                 adapter_model_id,
   3227                 adapter_name=adapter_name,

/usr/local/lib/python3.10/dist-packages/transformers/lib_integrations/peft/peft_mixin.py in load_adapter(self, peft_model_id, adapter_name, revision, token, device_map, max_memory, offload_folder, offload_index)
    114                 `offload_index` argument to be passed to `accelerate.dispatch_model` method.
    115         """"""
--> 116         check_peft_version(min_version=""0.4.0"")
    117 
    118         adapter_name = adapter_name if adapter_name is not None else ""default""

/usr/local/lib/python3.10/dist-packages/transformers/utils/peft_utils.py in check_peft_version(min_version)
     93 
     94     if not is_peft_version_compatible:
---> 95         raise ValueError(
     96             f""The version of PEFT you are using is not compatible, please use a version that is greater""
     97             f"" than {min_version}""

ValueError: The version of PEFT you are using is not compatible, please use a version that is greater than 0.4.0

and i'm checking peft version, it is 0.5.0.dev0. Why may this be happening?
","In  Tranasformers.utils.peft_utils.py ,
 is_peft_version_compatible = version.parse(importlib.metadata.version(""peft"")) <= version.parse(min_version)

The <= should be >=.
enter image description here
",fine-tune
nllb finetuning error missing data_prefix configuration englishgerman translation,"I'm attempting to fine-tune the NLLB model ""facebook/nllb-200-distilled-600M"" for a scientific translation task from English (eng_Latn) to German (deu_Latn). I followed the official guidelines for fine-tuning by authors of nllb.
Documentation: link
This is the code block which is giving error:
DATA_CONFIG = ""/content/sample_data/data_config.json""
OUTPUT_DIR = ""/content/outputs""
MODEL_FOLDER = ""/content/drive/MyDrive/Thesis/nllb-checkpoints""
DROP = 0.1
SRC = ""eng_Latn""
TGT = ""deu_Latn""
!python /content/fairseq/examples/nllb/modeling/train/train_script.py \
    cfg=nllb200_dense3.3B_finetune_on_fbseed \
    cfg/dataset=default \
    cfg.dataset.lang_pairs=""$SRC-$TGT"" \
    cfg.fairseq_root=$(pwd) \
    cfg.output_dir=$OUTPUT_DIR \
    cfg.dropout=$DROP \
    cfg.warmup=10 \
    cfg.finetune_from_model=$MODEL_FOLDER/checkpoint.pt

This is the error:
/content/fairseq/examples/nllb/modeling/train/train_script.py:287: UserWarning: 
The version_base parameter is not specified.
Please specify a compatability version level, or None.
Will assume defaults for version 1.1
  @hydra.main(config_path=""conf"", config_name=""base_config"")
/usr/local/lib/python3.10/dist-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.
See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.
  ret = run_job(
TRAINING DIR:  /content/outputs
Error executing job with overrides: ['cfg=nllb200_dense3.3B_finetune_on_fbseed', 'cfg/dataset=default', 'cfg.dataset.lang_pairs=eng_Latn-deu_Latn', 'cfg.fairseq_root=/content', 'cfg.output_dir=/content/outputs', 'cfg.dropout=0.1', 'cfg.warmup=10', 'cfg.finetune_from_model=/content/drive/MyDrive/LASS_KG_Data/Thesis/nllb-checkpoints/checkpoint.pt']
Traceback (most recent call last):
  File ""/content/fairseq/examples/nllb/modeling/train/train_script.py"", line 289, in main
    train_module = TrainModule(config)
  File ""/content/fairseq/examples/nllb/modeling/train/train_script.py"", line 122, in __init__
    assert cluster_name in cfg.dataset.data_prefix
omegaconf.errors.ConfigAttributeError: Key 'data_prefix' is not in struct
    full_key: cfg.dataset.data_prefix
    object_type=dict

Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.

So far, I understand there is a Missing data_prefix configuration. I created a demo custom data_config.json. Which looks like this:
{
    ""data_prefix"": ""/content/sample_data"",
    ""train_data"": ""train_demo.json"",
    ""test_data"": ""test_demo.json"",
    ""lang_pairs"": ""eng_Latn-deu_Latn""
}

While the official documentation provides some information, I'm encountering difficulties in applying it to my specific use case. Can someone share a detailed guide or point me to helpful resources on fine-tuning NLLB?
","While I can't help you with the concrete error message you are getting (my guess would be issues with structure of the provided JSON files), my personal recommendation would be to fine-tune NLLB in the transformers library, specifically using the Seq2SeqTrainer.
I did this before for multiple models, including NLLB, check out this repository: https://github.com/EliasK93/transformer-models-for-domain-specific-machine-translation/
This way the fine-tuning and inference process for the NLLB model is the same as any bilingual model (you can find guides for those more easiely), with the only exception that you load the tokenizer like so:
tokenizer = NllbTokenizer.from_pretrained(model_path, src_lang=""eng_Latn"", tgt_lang=""deu_Latn"")

and generate translations like this:
model.generate(tokenized_chunk.input_ids, forced_bos_token_id=tokenizer.encode(""deu_Latn"")[1], max_length=512)

",fine-tune
indexerror list index out of range when trying to predict from the fine tuned model using hugginface,"i am trying to learn on how to fine tune a pretrained model and use it. this is my code
from transformers import AutoModelForSequenceClassification, AutoTokenizer, TrainingArguments, Trainer
from datasets import load_dataset
import numpy as np
import torch

# Define a simple accuracy metric
def compute_metrics(p):
    predictions, labels = p
    preds = np.argmax(predictions, axis=1)
    return {""accuracy"": (preds == labels).mean()}

# Load the dataset
dataset = load_dataset(""imdb"", split='train[:1%]')
small_train_dataset = dataset.train_test_split(test_size=0.1)['train']
small_eval_dataset = dataset.train_test_split(test_size=0.1)['test']

# Load the tokenizer and model
model_name = ""bert-base-uncased""
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)

# Tokenize the dataset
def tokenize_function(examples):
    return tokenizer(examples['text'], padding=""max_length"", truncation=True)

small_train_dataset = small_train_dataset.map(tokenize_function, batched=True)
small_eval_dataset = small_eval_dataset.map(tokenize_function, batched=True)
small_train_dataset = small_train_dataset.rename_column(""label"", ""labels"")
small_eval_dataset = small_eval_dataset.rename_column(""label"", ""labels"")
small_train_dataset.set_format(""torch"", columns=[""input_ids"", ""attention_mask"", ""labels""])
small_eval_dataset.set_format(""torch"", columns=[""input_ids"", ""attention_mask"", ""labels""])

# Define training arguments
training_args = TrainingArguments(
    output_dir=""test_trainer"",
    evaluation_strategy=""epoch"",
    per_device_train_batch_size=8,
    per_device_eval_batch_size=8,
    num_train_epochs=3,
    weight_decay=0.01
)

# Initialize the Trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=small_train_dataset,
    eval_dataset=small_eval_dataset,
    compute_metrics=compute_metrics
)

# Train the model
trainer.train()

# Evaluate the model
validation_results = trainer.evaluate()
print(validation_results)

now, i am trying to make a prediction on the fine tuned model, like this
inputs=tokenizer(dataset[0]['text'], padding=""max_length"", truncation=True,return_tensors=""pt"")
predictions = trainer.predict(test_dataset=inputs)

i am getting this error when i am trying to make a prediction,

IndexError Traceback (most recent call last) Cell In[8], line 7 3
inputs=tokenizer(dataset[0][‘text’], padding=“max_length”,
truncation=True,return_tensors=“pt”) 6 # Make predictions
----> 7 predictions = trainer.predict(test_dataset=inputs)
File C:\Python311\Lib\site-packages\transformers\trainer.py:3305, in
Trainer.predict(self, test_dataset, ignore_keys, metric_key_prefix)
3302 start_time = time.time() 3304 eval_loop = self.prediction_loop if
self.args.use_legacy_prediction_loop else self.evaluation_loop → 3305
output = eval_loop( 3306 test_dataloader, description=“Prediction”,
ignore_keys=ignore_keys, metric_key_prefix=metric_key_prefix 3307 )
3308 total_batch_size = self.args.eval_batch_size *
self.args.world_size 3309 if
f""{metric_key_prefix}_jit_compilation_time"" in output.metrics:
File C:\Python311\Lib\site-packages\transformers\trainer.py:3408, in
Trainer.evaluation_loop(self, dataloader, description,
prediction_loss_only, ignore_keys, metric_key_prefix) 3406
observed_num_examples = 0 3407 # Main evaluation loop → 3408 for step,
inputs in enumerate(dataloader): 3409 # Update the observed num
examples 3410 observed_batch_size = find_batch_size(inputs) 3411 if
observed_batch_size is not None:
File C:\Python311\Lib\site-packages\accelerate\data_loader.py:454, in
DataLoaderShard.iter(self) 452 # We iterate one batch ahead to check
when we are at the end 453 try: → 454 current_batch =
next(dataloader_iter) 455 except StopIteration: 456 yield
File
C:\Python311\Lib\site-packages\torch\utils\data\dataloader.py:631, in
_BaseDataLoaderIter.next(self) 628 if self._sampler_iter is None: 629 # TODO(Bug in dataloader iterator found by mypy · Issue #76750 · pytorch/pytorch · GitHub) 630 self._reset() # type: ignore[call-arg] →
631 data = self._next_data() 632 self._num_yielded += 1 633 if
self._dataset_kind == _DatasetKind.Iterable and 634
self._IterableDataset_len_called is not None and 635 self._num_yielded

self._IterableDataset_len_called:

File
C:\Python311\Lib\site-packages\torch\utils\data\dataloader.py:675, in
_SingleProcessDataLoaderIter._next_data(self) 673 def _next_data(self): 674 index = self._next_index() # may raise StopIteration → 675 data = self._dataset_fetcher.fetch(index) # may
raise StopIteration 676 if self._pin_memory: 677 data =
_utils.pin_memory.pin_memory(data, self._pin_memory_device)
File
C:\Python311\Lib\site-packages\torch\utils\data_utils\fetch.py:51, in
_MapDatasetFetcher.fetch(self, possibly_batched_index) 49 data = self.dataset.getitems(possibly_batched_index) 50 else: —> 51 data =
[self.dataset[idx] for idx in possibly_batched_index] 52 else: 53 data
= self.dataset[possibly_batched_index]
File
C:\Python311\Lib\site-packages\torch\utils\data_utils\fetch.py:51, in
(.0) 49 data = self.dataset.getitems(possibly_batched_index) 50 else:
—> 51 data = [self.dataset[idx] for idx in possibly_batched_index] 52
else: 53 data = self.dataset[possibly_batched_index]
File
C:\Python311\Lib\site-packages\transformers\tokenization_utils_base.py:255,
in BatchEncoding.getitem(self, item) 253 return self.data[item] 254
elif self._encodings is not None: → 255 return self._encodings[item]
256 elif isinstance(item, slice): 257 return {key:
self.data[key][item] for key in self.data.keys()}
IndexError: list index out of range

","The error you are encountering is because the trainer.predict method expects a dataset as input, but you are passing a single example that has been tokenized into tensors.
To perform predictions on a single input, you need to prepare it similarly to how the dataset was prepared before training, and then use the model directly for prediction.
Here's how you can modify your code to make predictions on a single input:

Prepare the input correctly
Use the model directly for prediction

Here's the revised code:

from transformers import AutoModelForSequenceClassification, AutoTokenizer, TrainingArguments, Trainer
from datasets import load_dataset
import numpy as np
import torch

# Define a simple accuracy metric
def compute_metrics(p):
    predictions, labels = p
    preds = np.argmax(predictions, axis=1)
    return {""accuracy"": (preds == labels).mean()}

# Load the dataset
dataset = load_dataset(""imdb"", split='train[:1%]')
small_train_dataset = dataset.train_test_split(test_size=0.1)['train']
small_eval_dataset = dataset.train_test_split(test_size=0.1)['test']

# Load the tokenizer and model
model_name = ""bert-base-uncased""
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)

# Tokenize the dataset
def tokenize_function(examples):
    return tokenizer(examples['text'], padding=""max_length"", truncation=True)

small_train_dataset = small_train_dataset.map(tokenize_function, batched=True)
small_eval_dataset = small_eval_dataset.map(tokenize_function, batched=True)
small_train_dataset = small_train_dataset.rename_column(""label"", ""labels"")
small_eval_dataset = small_eval_dataset.rename_column(""label"", ""labels"")
small_train_dataset.set_format(""torch"", columns=[""input_ids"", ""attention_mask"", ""labels""])
small_eval_dataset.set_format(""torch"", columns=[""input_ids"", ""attention_mask"", ""labels""])

# Define training arguments
training_args = TrainingArguments(
    output_dir=""test_trainer"",
    evaluation_strategy=""epoch"",
    per_device_train_batch_size=8,
    per_device_eval_batch_size=8,
    num_train_epochs=3,
    weight_decay=0.01
)

# Initialize the Trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=small_train_dataset,
    eval_dataset=small_eval_dataset,
    compute_metrics=compute_metrics
)

# Train the model
trainer.train()

# Evaluate the model
validation_results = trainer.evaluate()
print(validation_results)

# Make a prediction on a single input
inputs = tokenizer(dataset[0]['text'], padding=""max_length"", truncation=True, return_tensors=""pt"")
model.eval()  # Set the model to evaluation mode
with torch.no_grad():  # Disable gradient calculation
    outputs = model(**inputs)
    predictions = torch.argmax(outputs.logits, dim=-1)

print(f""Predicted label: {predictions.item()}"")

",fine-tune
how to prepare data for batchinference in azure ml,"The data format (.csv) that I am using for inferencing produces the error :
""each data point should be a conversation array"" when running the batch scoring job. All the documentations provided online deal with image data so I am unable to figure out the format for my data.
I am trying to create a job from the ""Create job"" button under the batch endpoint.
I am using Azure ML platform and I have fine-tuned a phi-3-mini-4k-instruct model using the ""eurlex"" data available on huggingface. The training process required a jsonl format for the training data. However, while trying the run the batch inference, the data assets are only stored in csv,png etc. formats.
The training data looked something like the first image. I converted this into jsonl format.

I created a batch endpoint and deployed the model there. I created a job and provided the data that I formatted in the manner shown in the second image. I simple passed the whole prompt as a string in a single column dataframe which I converted into csv before writing it in a .csv file.

I have also tried making a dataframe with 3 columns - system,assistant,user but it doesn't work either.
","You have to give the csv file with columns names which is having correct name or signature as per the deployment.
If you see this example which is the batch deployment on text summarization model
The driver code takes the batch input then loading it using load_dataset  and fetching the column text for predictions.

def run(mini_batch):
    resultList = []

    print(f""[INFO] Reading new mini-batch of {len(mini_batch)} file(s)."")
    ds = load_dataset(""csv"", data_files={""score"": mini_batch})

    start_time = time.perf_counter()
    for idx, text in enumerate(ds[""score""][""text""]):


Also while invoking the batch endpoint it give the data as input type.
input = Input(type=AssetTypes.URI_FOLDER, path=""data"")
job = ml_client.batch_endpoints.invoke(
   endpoint_name=endpoint.name,
   input=input,
)

Here, the input type is of folder type and the csv files or saved in the folder data.
So, check your driver code or scoring script what column it is accessing and give the same in csv.
Also, you invoke the batch endpoint with number of csv files like given in above code.
If you don't find the scoring script or signature deploy the batch endpoint with your custom scoring script with help of code/batch_driver.py  in the above provide documentation.
Create a csv files with column message containing the string then read it in your custom scoring script for prediction.
",fine-tune
fine tuning layoutlmv3 using cordv2 dataset,"I'm working on fine-tuning LayoutLMv3 using the CORD-v2 dataset. I'm struggling with the data preprocessing part, specifically on how to correctly extract the total amount (TTC) from the images. The examples I've found online seem to use the older CORD dataset, which has a different format. The new CORD-v2 dataset only includes images and ground truth labels.
How to approach this?
I've tried examples from YouTube and Hugging Face but haven't had any success.
","I found a solution, you should create a label map for the data you want to extract, then Scale Bounding Boxes after that, and Detect Currency in Text because the problem is the dataset has a lot of different currencies and languages also so this is what I did
label_map = {
""total.total_price"": 1,
""other"": 0
}
def scale_bbox(box, original_size, target_size=(1000, 1000)):
    x_scale = target_size[0] / original_size[0]
    y_scale = target_size[1] / original_size[1]
    return [int(box[0] * x_scale), int(box[1] * y_scale), int(box[2] * x_scale), int(box[3] * y_scale)]

def detect_currency(text):
    currency_symbols = {
        '$': 'USD',
        '€': 'EUR',
        '£': 'GBP',
        '¥': 'JPY',
        '₹': 'INR',
        '₩': 'KRW',
    }
    for symbol, currency in currency_symbols.items():
        if symbol in text:
            return currency
    return None

def preprocess_data(examples):
    images = []
    words = []
    boxes = []
    labels = []
    original_size = (224, 224)
    currency_converter = CurrencyRates()

    for image, gt in zip(examples['image'], examples['ground_truth']):
        img = image.convert(""RGB"").resize(original_size)
        images.append(img)
        gt = json.loads(gt)
        batch_words = []
        batch_boxes = []
        batch_labels = []

        for item in gt['valid_line']:
            for w in item['words']:
                text = w['text']
                quad = w['quad']
                bbox = scale_bbox([quad['x1'], quad['y1'], quad['x3'], quad['y3']], original_size)
                bbox = [min(max(0, coord), 1000) for coord in bbox]
                batch_words.append(text)
                batch_boxes.append(bbox)
                if item['category'] == 'total.total_price':
                    try:
                        total_amount_match = re.findall(r""\d+\.\d{2}"", text)
                        if total_amount_match:
                            total_amount = float(total_amount_match[0])
                            detected_currency = detect_currency(text)
                            if detected_currency and detected_currency != 'USD':
                                total_amount = currency_converter.convert(detected_currency, 'USD', total_amount)
                            text = f""{total_amount:.2f} USD""
                    except Exception as e:
                        print(f""Error processing text: {e}"")
                    batch_labels.append(label_map[""total.total_price""])
                else:
                    batch_labels.append(label_map[""other""])

        words.append(batch_words)
        boxes.append(batch_boxes)
        labels.append(batch_labels)

    encoding = processor(images, words, boxes=boxes, word_labels=labels, truncation=True, padding=""max_length"", max_length=512, return_tensors=""pt"")
    return encoding`enter code here`

",fine-tune
huggingface  finetuning in tensorflow with custom datasets,"I have been battling with my own implementation on my dataset with a different transformer model than the tutorial, and I have been getting this error AttributeError: 'NoneType' object has no attribute 'dtype', when i was starting to train my model. I have been trying to debug for hours, and then I have tried the tutorial from hugging face as it can be found here https://huggingface.co/transformers/v3.2.0/custom_datasets.html. Running this exact code, so I could identify my mistake, also leads to the same error.
!wget http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz
!tar -xf aclImdb_v1.tar.gz

from pathlib import Path
def read_imdb_split(split_dir):
    split_dir = Path(split_dir)
    texts = []
    labels = []
    for label_dir in [""pos"", ""neg""]:
        for text_file in (split_dir/label_dir).iterdir():
            texts.append(text_file.read_text())
            labels.append(0 if label_dir is ""neg"" else 1)

    return texts, labels

train_texts, train_labels = read_imdb_split('aclImdb/train')
test_texts, test_labels = read_imdb_split('aclImdb/test')

from sklearn.model_selection import train_test_split
train_texts, val_texts, train_labels, val_labels = train_test_split(train_texts, train_labels, test_size=.2)

from transformers import DistilBertTokenizerFast
tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')

train_encodings = tokenizer(train_texts, truncation=True, padding=True)
val_encodings = tokenizer(val_texts, truncation=True, padding=True)
test_encodings = tokenizer(test_texts, truncation=True, padding=True)

import tensorflow as tf

train_dataset = tf.data.Dataset.from_tensor_slices((
    dict(train_encodings),
    train_labels
))
val_dataset = tf.data.Dataset.from_tensor_slices((
    dict(val_encodings),
    val_labels
))
test_dataset = tf.data.Dataset.from_tensor_slices((
    dict(test_encodings),
    test_labels
))

from transformers import TFDistilBertForSequenceClassification

model = TFDistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased')

optimizer = tf.keras.optimizers.Adam(learning_rate=5e-5)
model.compile(optimizer=optimizer, loss=model.compute_loss) # can also use any keras loss fn
model.fit(train_dataset.shuffle(1000).batch(16), epochs=3, batch_size=16)

My goal will be to perform multi-label text classification on my own custom dataset, which unfortunately I cannot share for privacy reasons. If anyone could point out what is wrong with this implementation, will be highly appreciated.
","There seems to be an error, when you are passing the loss parameter.
model.compile(optimizer=optimizer, loss=model.compute_loss) # can also use any keras loss fn

You don't need to pass the loss parameter, if you want to use the model's built-in loss function.
I was able to train the model with your provided source code by changing mentioned line to:
model.compile(optimizer=optimizer)

or by passing a loss function
loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)

model.compile(optimizer=optimizer, loss=loss_fn)

transformers version: 4.20.1
Hope it helps.
",fine-tune
openai chat completions api how do i make a finetuned gpt35 model only answer from the finetuned data,"OpenAI now allows us to fine-tune GPT-3.5 models. I have tested and fine-tuned the model with my own dataset but the problem is the fine-tuned model generates the answer randomly, not correct based on my custom dataset.
Is there any way to make the model only answer from my own fine-tuned dataset?
","This is a completely wrong approach, as you've already figured out.
Why doesn't the fine-tuned OpenAI model answer a specific question with a specific answer (i.e., fact) from the fine-tuning dataset?
As stated in the official OpenAI documentation:

Some common use cases where fine-tuning can improve results:

Setting the style, tone, format, or other qualitative aspects
Improving reliability at producing a desired output
Correcting failures to follow complex prompts
Handling many edge cases in
specific ways
Performing a new skill or task that’s hard to articulate
in a prompt


Fine-tuning is not about answering a specific question with a specific answer (i.e., fact) from the fine-tuning dataset.
What's the correct approach, then?
You need to implement a vector similarity search, as stated in the official OpenAI documentation:

When should I use fine-tuning vs embeddings with retrieval?
Embeddings
with retrieval is best suited for cases when you need to have a large
database of documents with relevant context and information.
By default OpenAI’s models are trained to be helpful generalist
assistants. Fine-tuning can be used to make a model which is narrowly
focused, and exhibits specific ingrained behavior patterns. Retrieval
strategies can be used to make new information available to a model by
providing it with relevant context before generating its response.
Retrieval strategies are not an alternative to fine-tuning and can in
fact be complementary to it.

A term that you most likely stumbled upon to this point if you're into AI is RAG (i.e., Retrieval-Augmented Generation). Read Nvidia's RAG explanation to better understand what RAG is:

To understand the latest advance in generative AI, imagine a
courtroom.
Judges hear and decide cases based on their general understanding of
the law. Sometimes a case — like a malpractice suit or a labor dispute
— requires special expertise, so judges send court clerks to a law
library, looking for precedents and specific cases they can cite.
Like a good judge, large language models (LLMs) can respond to a wide
variety of human queries. But to deliver authoritative answers that
cite sources, the model needs an assistant to do some research.
The court clerk of AI is a process called retrieval-augmented
generation, or RAG for short.

Wait, what does a RAG have to do with vector similarity search?
RAGs use vector similarity search under the hood. Take a look at the visual representation of a RAG process below:

Image source: An introduction to RAG and simple/ complex RAG by Chia Jeng Yang
Information is extracted from data sources (A), slashed into chunks (B), transformed into vectors (C), and inserted into a vector database (D). When a user asks a question, its question is transformed into a vector (1). This vector is then compared with vectors that are inside the vector database (2). The most similar vectors (3) are passed to an LLM (4), which then returns an answer to the user (5).
This is how RAGs work. They use a vector similarity search under the hood.
So, how do I achieve my goal?
You have at least the following three options if you want your LLM to answer a specific question with a specific answer (i.e., fact):

Custom solution (see my past StackOverflow answer).
Using LlamaIndex RAG or LangChain RAG (see my YouTube tutorial with corresponsing code).
Using the OpenAI Assistants API (see my YouTube tutorial with corresponding code).

",fine-tune
running batch predictions with finetuned palm model in vertex ai always throws valueerror,"For my code:
import vertexai
from vertexai.preview.language_models import TextGenerationModel

model = TextGenerationModel.get_tuned_model(""projects/PROJECT_ID/locations/us-central1/models/MODEL_ID"")
batch_file=f'gs://my_bucket/test_.jsonl'
batch_file_output=f'gs://my_bucket/response'
batch_prediction_job = model.batch_predict(
  dataset=[batch_file],
  destination_uri_prefix=batch_file_output,,
  model_parameters={
      ""maxOutputTokens"": ""300"",
      ""temperature"": ""0.0"",
      ""topP"": ""0.95"",
      ""topK"": ""1"",
  }
)

does not work and throws this error:
ValueError                                Traceback (most recent call last)
/tmp/ipykernel_1/123140971.py in <module>
     12       ""temperature"": ""0.0"",
     13       ""topP"": ""0.95"",
---> 14       ""topK"": ""1"",
     15   },
     16 )

~/.local/lib/python3.7/site-packages/vertexai/language_models/_language_models.py in batch_predict(self, destination_uri_prefix, dataset, model_parameters, **_kwargs)
    430             dataset=dataset,
    431             destination_uri_prefix=destination_uri_prefix,
--> 432             model_parameters=model_parameters,
    433         )
    434 

~/.local/lib/python3.7/site-packages/vertexai/language_models/_language_models.py in batch_predict(self, dataset, destination_uri_prefix, model_parameters)
    381         # TODO(b/284512065): Batch prediction service does not support
    382         # fully qualified publisher model names yet
--> 383         publishers_index = model_name.index(""/publishers/"")
    384         if publishers_index > 0:
    385             model_name = model_name[publishers_index + 1 :]

ValueError: substring not found

That comment above the error (the TODO) is pretty interesting. Seems to imply it isn't supported.
is there any way to run batch prediction mode with a fine tuned PALM model?
Note this similar code works without any errors.
text_model = TextGenerationModel.from_pretrained(""text-bison"")
batch_prediction_job = text_model.batch_predict(
  dataset=[batch_file],
  destination_uri_prefix=batch_file_output,
  model_parameters={
      ""maxOutputTokens"": ""300"",
      ""temperature"": ""0.0"",
      ""topP"": ""0.95"",
      ""topK"": ""1"",
  },
)

But the big difference is the non-fine-tuned model.
I am on ver 1.30.1 of google-cloud-aiplatform
","From my ticket to Google support on aug 29 2023:

I have a confirmation from the product engineers that we don't support fine-tuned model for batch prediction.

When asked if it would be added in the future:

We don't have plan for fine tuned model yet. Hence, unfortunately I don't have any ETA whether this would be considered in the future.

The answer is that this is not currently supported by google and may or may not be in the future.
",fine-tune
how to fix this keyerror while training my model,"I wrote a code that will fine-tine my QnA model using huggingface. The code will import questions and answers from a json file and then fine tune a HF Question-Answer model with the data from the json file. However, things went awry from then onwards, when I get this traceback:
Traceback (most recent call last):
  File ""C:\Users\Philip Chen\OneDrive\Documents\ML\machineLearning.py"", line 45, in <module>
    trainer.train()
  File ""C:\Users\Philip Chen\AppData\Local\Programs\Python\Python310\lib\site-packages\transformers\trainer.py"", line 1859, in train
    return inner_training_loop(
  File ""C:\Users\Philip Chen\AppData\Local\Programs\Python\Python310\lib\site-packages\transformers\trainer.py"", line 2165, in _inner_training_loop
    for step, inputs in enumerate(epoch_iterator):
  File ""C:\Users\Philip Chen\AppData\Local\Programs\Python\Python310\lib\site-packages\accelerate\data_loader.py"", line 454, in __iter__
    current_batch = next(dataloader_iter)
  File ""C:\Users\Philip Chen\AppData\Local\Programs\Python\Python310\lib\site-packages\torch\utils\data\dataloader.py"", line 631, in __next__
    data = self._next_data()
  File ""C:\Users\Philip Chen\AppData\Local\Programs\Python\Python310\lib\site-packages\torch\utils\data\dataloader.py"", line 675, in _next_data
    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration
  File ""C:\Users\Philip Chen\AppData\Local\Programs\Python\Python310\lib\site-packages\torch\utils\data\_utils\fetch.py"", line 51, in fetch
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File ""C:\Users\Philip Chen\AppData\Local\Programs\Python\Python310\lib\site-packages\torch\utils\data\_utils\fetch.py"", line 51, in <listcomp>
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File ""C:\Users\Philip Chen\AppData\Local\Programs\Python\Python310\lib\site-packages\datasets\dataset_dict.py"", line 81, in __getitem__
    raise KeyError(
KeyError: ""Invalid key: 0. Please first select a split. For example: `my_dataset_dictionary['train'][0]`. Available splits: ['train']""

from the offending line:
trainer.train()

And this is my entire code:
# https://www.mlexpert.io/blog/alpaca-fine-tuning
# https://wellsr.com/python/fine-tuning-huggingface-models-in-tensorflow-keras/
# https://learnopencv.com/fine-tuning-bert/
# https://medium.com/@karary/nlp-fine-tune-question-answering-model-%E5%AF%A6%E4%BD%9C-3-model-training-%E5%84%B2%E5%AD%98%E8%88%87-inference-13d2a5bf5c32
import transformers as tf
import datasets as ds
import pandas as pd
import numpy as np
import torch
import json
 
############## Check if CUDA is enabled. ################
hasCUDA=torch.cuda.is_available()
print(f""CUDA Enabled? {hasCUDA}"")
device=""cuda"" if hasCUDA else ""cpu""      
 
############## Loading file and populating data ################
fileName=""qna.json""
trainDS=ds.load_dataset(""json"", data_files=fileName)
evalDS=ds.load_dataset(""json"", data_files=fileName)
# rawDS=ds.load_dataset('squad')
############## Model ##########################################
modelName=""./distilbert-base-cased""     #or replace the model name with whatever you feel like.
config=tf.AutoConfig.from_pretrained(modelName+""/config.json"")
model=tf.AutoModelForQuestionAnswering.from_pretrained(modelName,config=config)
tokenizer=tf.AutoTokenizer.from_pretrained(modelName)
############## Training #######################################
trnArgs=tf.TrainingArguments(
    output_dir=""./"",
    evaluation_strategy=""epoch"",
    save_strategy=""epoch"",
    learning_rate=2e-5,
    num_train_epochs=3,
    remove_unused_columns=False,
    fp16=True
)
 
trainer=tf.Trainer(
    model=model,
    args=trnArgs,
    train_dataset=trainDS,
    eval_dataset=evalDS,
    tokenizer=tokenizer
)
trainer.train()

This is the JSON file that contains the questions:
{""text"": ""Who wrote Charlie and the Chocolate Factory?"", ""label"": ""Roald Dahl""}
{""text"": ""Name a few ways to treat constipation naturally."", ""label"": ""Exercise regularly, eat more fibers, and drink more water.""}
{""text"": ""Where is the longest roller coaster located?"", ""label"": ""Nagashima, Japan. The name of the coaster is Steel Dragon 2000.""}
{""text"": ""What are the 11 herbs and spices that Colonel Sanders used in KFC?"", ""label"": ""Nobody knows, as it's a secret.""}
{""text"": ""Who wrote Les Miserables?"", ""label"": ""Victor Hugo""}
{""text"": ""What is the Watergate Scandal?"", ""label"": ""The Watergate scandal was a significant political controversy in the United States during the presidency of Richard Nixon from 1972 to 1974, ultimately resulting in Nixon's resignation. It originated from attempts by the Nixon administration to conceal its involvement in the June 17, 1972, break-in at the Democratic National Committee headquarters located in the Watergate Office Building in Washington, D.C.""}
{""text"": ""What is Obama's most famous quote?"", ""label"": ""'Yes we can!'""}

Any suggestions on how I can fix this thing?
","The issue lies in how you create the train and test datasets. Since your dataset is already split into JSON files, you need to specify the split when reading the data using the split parameter. If you don't specify the split every time you run ds.load_dataset, it automatically creates train and test splits for you. According to the function documentation:
split (`Split` or `str`):
Which split of the data to load.

**If `None`, it will return a `dict` with all splits** (typically `datasets.Split.TRAIN` and `datasets.Split.TEST`).

If a split is provided, it will return a single Dataset.

Splits can be combined and specified like in tensorflow-datasets.

To address this issue, use the following code:
trainDS = ds.load_dataset(""json"", data_files=fileName, split='train')
evalDS = ds.load_dataset(""json"", data_files=fileName, split='test')

",fine-tune
unable to save generated data to jsonl file  always resulting in quotwrote 0 examples to finetuning_eventsjsonlquot message,"Issue Description
When attempting to generate JSONL data using Llama Index, the process works well until the final step where the results are saved to a JSONL file. However, every time I try to save the data, it seems to be unsuccessful as I always receive the message ""Wrote 0 examples to finetuning_events.jsonl"". I am unsure of the reason behind this issue.
Steps to Reproduce

Successfully generated JSONL data using Llama Index.
Attempted to save the results to a JSONL file.
Received the message ""Wrote 0 examples to finetuning_events.jsonl"".

Additional Information

Llama Index version used: 0.10.22
Operating System: Windows

Log
Wrote 0 examples to ./dataset_data/finetuning_events.jsonl
My code:
     def jsonl_generation(self):
        """"""
        Generate JSONL file for fine-tuning events and perform model refinement.
        """"""
        # Initialize OpenAI FineTuningHandler and CallbackManager
        finetuning_handler = OpenAIFineTuningHandler()
        callback_manager = CallbackManager([finetuning_handler])

        self.llm.callback_manager = callback_manager

        # Load questions for fine-tuning from a file
        questions = []
        with open(f'{self.dataset_path}/train_questions.txt', ""r"", encoding='utf-8') as f:
            for line in f:
                questions.append(line.strip())

        try:
            # Generate responses to the questions using GPT-4 and save the fine-tuning events to a JSONL file
            index = VectorStoreIndex.from_documents(
                self.documents
            )
            query_engine = index.as_query_engine(similarity_top_k=2, llm=self.llm)
            for question in questions:
                response = query_engine.query(question)
        except Exception as e:
            # Handle the exception here, you might want to log the error or take appropriate action
            print(f""An error occurred: {e}"")
        finally:
            # Save the fine-tuning events to a JSONL file
            finetuning_handler.save_finetuning_events(f'{self.dataset_path}/finetuning_events.jsonl')

","I just solved the problem.
It's my solution. Currently, It's storing the dataset to jsonl data.
    def jsonl_generation(self):
        """"""
        Generate JSONL file for fine-tuning events and perform model refinement.
        """"""
        # Initialize OpenAI FineTuningHandler and CallbackManager
        finetuning_handler = OpenAIFineTuningHandler()
        callback_manager = CallbackManager([finetuning_handler])

        llm = OpenAI(model=""gpt-4"", temperature=0.3)
        Settings.callback_manager, = (callback_manager,)

        # Load questions for fine-tuning from a file
        questions = []
        with open(f'{self.dataset_path}/train_questions.txt', ""r"", encoding='utf-8') as f:
            for line in f:
                questions.append(line.strip())

        try:
            from llama_index.core import VectorStoreIndex
            # Generate responses to the questions using GPT-4 and save the fine-tuning events to a JSONL file
            index = VectorStoreIndex.from_documents(
                self.documents
            )
            query_engine = index.as_query_engine(similarity_top_k=2, llm=llm)
            for question in questions:
                response = query_engine.query(question)
        except Exception as e:
            # Handle the exception here, you might want to log the error or take appropriate action
            print(f""An error occurred: {e}"")
        finally:
            # Save the fine-tuning events to a JSONL file
            finetuning_handler.save_finetuning_events(f'{self.dataset_path}/finetuning_events.jsonl')

",fine-tune
vllm040post1neuron213 modulenotfounderror no module named 39vllm_c39,"My issue is with vLLM running with Neuron on AWS inferentia instance.
I successfully installed vllm-0.4.0.post1+neuron213.
But when I run LLM using vLLM it says ModuleNotFoundError: No module named 'vllm._C'.
I realized there is the following functuion in vllm setup.py,
if not _is_neuron():
    ext_modules.append(CMakeExtension(name=""vllm._C""))

and
cmdclass={""build_ext"": cmake_build_ext} if not _is_neuron() else {},

So, vllm._C won't be created if the device is Neuron (AWS inferentia Instance - inf2). This results in ModuleNotFoundError: No module named 'vllm._C'.
How to fix it?
I tried this example
from vllm import LLM, SamplingParams

prompts = [
    ""Hello, my name is"",
    ""The president of the United States is"",
    ""The capital of France is"",
    ""The future of AI is"",
]
sampling_params = SamplingParams(temperature=0.8, top_p=0.95)

llm = LLM(
    model=""TinyLlama/TinyLlama-1.1B-Chat-v1.0"",
    max_num_seqs=8,
    max_model_len=128,
    block_size=128,
    device=""neuron"",
    tensor_parallel_size=2)

outputs = llm.generate(prompts, sampling_params)

for output in outputs:
    prompt = output.prompt
    generated_text = output.outputs[0].text
    print(f""Prompt: {prompt!r}, Generated text: {generated_text!r}"")

","I just solved it this way.
The problem is with from vllm._C import ops while there is no vllm._C.
We need ops that exists in your_environment_name/lib/python3.10/site-packages/vllm/model_executor/layers/ (see the figure below)

So, what we have to do is to change from vllm._C import ops to from vllm.model_executor.layers import ops in every single file of the package.
This solves the problem :)
",fine-tune
how can i finetune a language model with negative examples using sfttrainer,"I'm using the SFTTrainer from the 'trl' package to fine-tune a language model. I would like to give it some negative examples, but I'm not seeing any built-in methods anywhere. Is there something I'm missing, or some way of implementing this in a custom way?
I tried looking in the documentation but haven't seen anything obvious.
","SFTTrainer is designed for supervised fine-tuning (maximizing likelihood of in-distribution samples), so there is no straightforward way to utilize negative samples.
May be other alignment algorithms like KTO(also implemented in trl) would do the job in your case.
Another possible way is to modify prompt to include negative label in it. For example ""{question} This is the wrong answer: {answer}"".
",fine-tune
what are the differences between fine tuning and few shot learning,"I am trying to understand the concept of fine-tuning and few-shot learning.
I understand the need for fine-tuning. It is essentially tuning a pre-trained model to a specific downstream task. However, recently I have seen a plethora of blog posts stating zero-shot learning, one-shot learning and few-shot learning.

How are they different from fine-tuning? It appears to me that few-shot learning is a specialization of fine-tuning. What am I missing here?

Can anyone please help me?
","Fine tuning - When you already have a model trained to perform the task you want but on a different dataset, you initialise using the pre-trained weights and train it on target (usually smaller) dataset (usually with a smaller learning rate).
Few shot learning - When you want to train a model on any task using very few samples. e.g., you have a model trained on different but related task and you (optionally) modify it and train for target task using small number of examples.
For example:
Fine tuning - Training a model for intent classification and then fine tuning it on a different dataset.
Few shot learning - Training a language model on large text dataset and modifying it (usually last (few) layer) to classify intents by training on small labelled dataset.
There could be many more ways to do few shot learning. For 1 more example, training a model to classify images where some classes have very small (or 0 for zero shot and 1 for one shot) number of training samples. Here in inference, classifying these rare classes (rare in training) correctly becomes the aim of few shot learning.
",fine-tune
how to create a dataset with huggingface from a list of strings to finetune llama 2 with the transformers library,"I have a list of strings and I want to use them to fine tune Llama 2. Every entry of the list contains a couple of sentences.
I need to bring this into the right format to use the Trainer of the transformers library. But I don't seem to find anything online. This should be a really basic problem?
I don't need a validation dataset. Just a way to feed the dataset in to the trainer via
trainer = transformers.Trainer(model=model,train_dataset=dataset,... )

this is what I have tried:
from datasets import Dataset

dataset = Dataset.from_list(list)

","This is what worked for me in the end:
import pandas as pd
df = pd.DataFrame(list)

from datasets import Dataset
dataset = Dataset.from_pandas(df.rename(columns={0: ""train""}), split=""train"")

and then to tokenize the data:
tokenized_dataset = dataset.map(lambda samples: tokenizer(samples[""train""]), batched=True)

",fine-tune
openai api what is the proper format to finetune the openai model,"I am creating an OpenAI model for one of the Catalog Book through which users can ask anything from the book, and the model is able to answer it.
I created a Q&A list from the book (70 QA). ChatGPT gives me the following format to submit it to the fine-tune API endpoint:
""{
    ""context"": ""Introducing BAUTER - an innovative thermal insulation product which has been developed by engineers for many years for the production of paints and other thin-coating materials that results in a unique group of thermo coating solutions. As part of the market demand for a product that will work in conditions where insulation has not yet been possible or difficult, our product resolves many problems in construction and industry. BAUTER is a family of various products with many applications."",
    ""question"": ""What is BAUTER?"",
    ""answer"": ""BAUTER is an innovative thermal insulation product developed by engineers for the production of paints and other thin-coating materials. It offers unique thermo coating solutions and addresses insulation challenges in construction and industry.""
},""

What is the proper format and the proper way to fine-tune the OpenAI model?
","The proper format to fine-tune the OpenAI model depends on the model you want to fine-tune.
As of today, you can fine-tune the following OpenAI models, as stated in the official OpenAI documentation:

gpt-3.5-turbo-0613 (recommended)
babbage-002
davinci-002

Let's say you want to create a chatbot that occasionally gives sarcastic responses, as shown in the official OpenAI documentation.
If you want to fine-tune the gpt-3.5-turbo-0613 model, use the following format:
{""messages"": [{""role"": ""system"", ""content"": ""Marv is a factual chatbot that is also sarcastic.""}, {""role"": ""user"", ""content"": ""What's the capital of France?""}, {""role"": ""assistant"", ""content"": ""Paris, as if everyone doesn't know that already.""}]}
{""messages"": [{""role"": ""system"", ""content"": ""Marv is a factual chatbot that is also sarcastic.""}, {""role"": ""user"", ""content"": ""Who wrote 'Romeo and Juliet'?""}, {""role"": ""assistant"", ""content"": ""Oh, just some guy named William Shakespeare. Ever heard of him?""}]}
{""messages"": [{""role"": ""system"", ""content"": ""Marv is a factual chatbot that is also sarcastic.""}, {""role"": ""user"", ""content"": ""How far is the Moon from Earth?""}, {""role"": ""assistant"", ""content"": ""Around 384,400 kilometers. Give or take a few, like that really matters.""}]}

If you want to fine-tune the babbage-002 or davinci-002 model, use the following format:
{""prompt"": ""What's the capital of France?"", ""completion"": ""Paris, as if everyone doesn't know that already.""}
{""prompt"": ""Who wrote 'Romeo and Juliet'?"", ""completion"": ""Oh, just some guy named William Shakespeare. Ever heard of him?""}
{""prompt"": ""How far is the Moon from Earth?"", ""completion"": ""Around 384,400 kilometers. Give or take a few, like that really matters.""}

",fine-tune
openai api error quotunrecognized request argument suppliedquot,"I'm receiving an error when calling the OpenAI API. It's not recognizing file argument, which I submitted to the API.
Here is my PHP code:
<?php

// Define your OpenAI API key and the endpoint
$apiKey = 'sk-TOh**********************************';
$endpoint = 'https://api.openai.com/v1/engines/davinci/completions';

// File ID of the uploaded data
$fileId = 'file-FlW6jPfNuuq1lTak91AjMj2j';

// Product name
$productName = '6 pack fresh grannies apples';

// Prompt to use the file ID as a reference
$prompt = ""Given the following data from the uploaded file $fileId, categorize the product '$productName':"";

// Prepare the cURL request data
$data = [
    'prompt' => $prompt,
    'max_tokens' => 1, // Adjust the token limit as needed
    'file' => $fileId // Reference the file by ID
];

// Prepare the cURL request
$ch = curl_init($endpoint);
curl_setopt($ch, CURLOPT_RETURNTRANSFER, true);
curl_setopt($ch, CURLOPT_HTTPHEADER, [
    'Authorization: Bearer ' . $apiKey,
    'Content-Type: application/json',
]);
curl_setopt($ch, CURLOPT_POST, 1);
curl_setopt($ch, CURLOPT_POSTFIELDS, json_encode($data));

// Execute the cURL request
$response = curl_exec($ch);

// Check for cURL errors
if (curl_errno($ch)) {
    echo 'cURL error: ' . curl_error($ch);
} else {
    // Parse the API response as JSON
    $responseData = json_decode($response, true);
echo ""<pre>"",print_r($responseData),""</pre>"";
    // Extract and display the category
    $category = $responseData['choices'][0]['text'];
    echo ""Product '$productName' belongs to the category: $category"";
}

// Close the cURL session
curl_close($ch);

?>

Here is the data of the file I uploaded:
{""prompt"": ""fruits"", ""completion"": ""apples, bananas, oranges, grapes, strawberries""}
{""prompt"": ""vegetables"", ""completion"": ""carrots, broccoli, spinach, lettuce, tomatoes""}
{""prompt"": ""dairy"", ""completion"": ""milk, cheese, yogurt, butter, cream""}
{""prompt"": ""meat"", ""completion"": ""chicken, beef, pork, lamb, turkey""}
{""prompt"": ""bakery"", ""completion"": ""bread, muffins, cookies, cakes, pies""}

Here is the error I'm receiving:
[error] => Array
(
  [message] => Unrecognized request argument supplied: file
  [type] => invalid_request_error
  [param] => 
  [code] => 
)

What am I doing wrong? I've tried searching for the answer and also looking at OpenAI documentation.
","Problem
You're trying to pass file as a parameter to the Completions API endpoint, which is not a valid parameter. You can't pass any parameter you make up to the Completions API endpoint.
Solution
See the complete list of parameters you can pass to the Completions API endpoint:

model
prompt
suffix
max_tokens
temperature
top_p
n
stream
logprobs
echo
stop
presence_penalty
frequency_penalty
best_of
logit_bias
user



Also, all Engines API endpoints are deprecated.

Use the Completions API endpoint.
Change the URL from this...
https://api.openai.com/v1/engines/davinci/completions

...to this.
https://api.openai.com/v1/completions

",fine-tune
openai api error when using a finetuned model quotthe model xxxxx does not existquot,"I'm trying to do a chatbot with a fine tuned model.
I'm doing my request like this:
    const API_KEY = ""/"";
    const ORG_ID = ""/"";
    const headers = {
      ""Content-Type"": ""application/json"",
      Authorization: ""Bearer "" + API_KEY,
      ""OpenAI-Organization"": ORG_ID,
    };
    const res = await axios.post(
      ""https://api.openai.com/v1/chat/completions"",
      {
        model: ""ft-modelname"",
        messages: [
          {
            role: ""system"",
            content: ""your name is Name."",
          },
          {
            role: ""user"",
            content: message,
          },
        ],
      },
      { headers }
    );

And after listing my models with the API I can see it, it does exist.
But I have this error in the console:
{
    ""error"": {
        ""message"": ""The model `ft-modelname` does not exist"",
        ""type"": ""invalid_request_error"",
        ""param"": null,
        ""code"": null
    }
}

After looking up, i can't find a way to do this, some people use open ai engine api instead of completion, but it don't work too.
Do you have any ideas ?
Thank you.
Edit:
It works when I'm not using a fine tune model
","UPDATE: 22 August 2023
Fine-tuning for GPT-3.5 is now available, as stated in the official OpenAI blog:

Fine-tuning for GPT-3.5 Turbo is now available, with fine-tuning for
GPT-4 coming this fall.

Consequently, the Chat Completions API (i.e., the GPT-3.5 API) endpoint can be used for a fine-tuned model, but only for a GPT-3.5 fine-tuned model. For a GPT-3 fine-tuned model, use the Completions API (i.e., the GPT-3 API) endpoint.
As stated in the official OpenAI documentation:

When a job has succeeded, you will see the fine_tuned_model field
populated with the name of the model when you retrieve the job
details. You may now specify this model as a parameter to in the Chat
Completions API (for gpt-3.5-turbo) or legacy Completions API (for
babbage-002 and davinci-002), and make requests to it using the
Playground.


Problem
You're using the wrong API endpoint.
Solution
Use the Completions API (i.e., the GPT-3 API) endpoint instead of the Chat Completions API (i.e., the GPT-3.5 API) endpoint.
As stated in the official OpenAI documentation:

When a job has succeeded, the fine_tuned_model field will be populated
with the name of the model. You may now specify this model as a
parameter to our Completions API, and make requests to it using the
Playground.

In general
Python
import os
import openai

openai.api_key = os.getenv(""OPENAI_API_KEY"")

openai.Completion.create(
    model = FINE_TUNED_MODEL,
    prompt = YOUR_PROMPT)

NodeJS
const { Configuration, OpenAIApi } = require(""openai"");

const configuration = new Configuration({
  apiKey: process.env.OPENAI_API_KEY,
});

const openai = new OpenAIApi(configuration);

const response = await openai.createCompletion({
  model: FINE_TUNED_MODEL
  prompt: YOUR_PROMPT,
});

cURL
curl https://api.openai.com/v1/completions \
  -H ""Authorization: Bearer $OPENAI_API_KEY"" \
  -H ""Content-Type: application/json"" \
  -d '{""prompt"": YOUR_PROMPT, ""model"": FINE_TUNED_MODEL}'

OpenAI CLI
openai api completions.create -m <FINE_TUNED_MODEL> -p <YOUR_PROMPT>

Your case
Try this:
const res = await axios.post(
  ""https://api.openai.com/v1/completions"", {
    model: ""ft-modelname"",
    prompt: ""Say this is a test"",
  }, {
    headers
  }
);

",fine-tune
finetuning a huggingface llm on two books using lora,"I have been trying to get into finetuning LLMs on my own hardware (Ryzen 3960x and RTX 3090 64 GB Ram) as efficiently as possible and running into some problems while doing so. As a test, I wanted to train GPT-2 on DavidCopperfield by Charles Dickens to test the result one could expect, so I tokenized the books using pdfReader and autoTokenize from my model. This seemed to work. Then, I wanted to finetune the model on this tokenized dataset, but I ran into some issues with CUDA installation. Every time I run my code, I get this error:
    bin C:\Users\salom\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\LocalCache\local-packages\Python311\site-packages\bitsandbytes\libbitsandbytes_cpu.so
    False
    C:\Users\salom\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\LocalCache\local-packages\Python311\site-packages\bitsandbytes\cextension.py:34: UserWarning: The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.
      warn(""The installed version of bitsandbytes was compiled without GPU support. ""
    'NoneType' object has no attribute 'cadam32bit_grad_fp32'
    CUDA SETUP: Required library version not found: libbitsandbytes_cpu.so. Maybe you need to compile it from source?
    CUDA SETUP: Defaulting to libbitsandbytes_cpu.so...
    
    ================================================ERROR=====================================
    CUDA SETUP: CUDA detection failed! Possible reasons:
    1. CUDA driver not installed
    2. CUDA not installed
    3. You have multiple conflicting CUDA libraries
    4. Required library not pre-compiled for this bitsandbytes release!
    CUDA SETUP: If you compiled from source, try again with `make CUDA_VERSION=DETECTED_CUDA_VERSION` for example, `make CUDA_VERSION=113`.
    CUDA SETUP: The CUDA version for the compile might depend on your conda install. Inspect CUDA version via `conda list | grep cuda`.
    ================================================================================
    
    CUDA SETUP: Problem: The main issue seems to be that the main CUDA library was not detected.
    CUDA SETUP: Solution 1): Your paths are probably not up-to-date. You can update them via: sudo ldconfig.
    CUDA SETUP: Solution 2): If you do not have sudo rights, you can do the following:
    CUDA SETUP: Solution 2a): Find the cuda library via: find / -name libcuda.so 2>/dev/null
    CUDA SETUP: Solution 2b): Once the library is found add it to the LD_LIBRARY_PATH: export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:FOUND_PATH_FROM_2a
    CUDA SETUP: Solution 2c): For a permanent solution add the export from 2b into your .bashrc file, located at ~/.bashrc
    CUDA SETUP: Setup Failed!


This is my code:

import PyPDF2

# Function to extract text from a PDF file
def extract_text_from_pdf(file_path):
    with open(file_path, 'rb') as file:
        pdf_reader = PyPDF2.PdfReader(file)
        text = """"
        for page in pdf_reader.pages:
            text += page.extract_text()
        return text

# Load the PDF file and extract text
pdf_file_path = ""DavidCopperfield.pdf""
book_text = extract_text_from_pdf(pdf_file_path)

import re

# Function to filter and clean the text
def filter_text(text):
    # Remove chapter titles and page numbers
    text = re.sub(r'CHAPTER \d+', '', text)
    text = re.sub(r'\d+', '', text)

    # Remove unwanted characters and extra whitespaces
    text = re.sub(r'[^\w\s\'.-]', '', text)
    text = re.sub(r'\s+', ' ', text)

    # Remove lines with all uppercase letters (potential noise)
    text = '\n'.join(line for line in text.split('\n') if not line.isupper())

    return text

# Apply text filtering to the book text
filtered_text = filter_text(book_text)

# Partition the filtered text into training texts with a maximum size
max_text_size = 150
train_texts = []
current_text = """"
for paragraph in filtered_text.split(""\n\n""):
    if len(current_text) + len(paragraph) < max_text_size:
        current_text += paragraph + ""\n\n""
    else:
        train_texts.append(current_text)
        current_text = paragraph + ""\n\n""
if current_text:
    train_texts.append(current_text)


from transformers import GPT2LMHeadModel, GPT2Tokenizer, GPT2Config
from transformers import AdamW
from torch.utils.data import Dataset, DataLoader
import torch
# Define your dataset class
class TextDataset(Dataset):
    def __init__(self, texts, tokenizer, max_length):
        self.texts = [text for text in texts if len(text) >= max_length]  # Filter out texts shorter than max_length
        self.tokenizer = tokenizer
        self.max_length = max_length

    def __len__(self):
        return len(self.texts)

    def __getitem__(self, idx):
        text = self.texts[idx]
        encoded_input = self.tokenizer.encode_plus(text, max_length=self.max_length, padding='max_length', truncation=True, return_tensors='pt')
        input_ids = encoded_input['input_ids'].squeeze()
        attention_mask = encoded_input['attention_mask'].squeeze()
        return input_ids, attention_mask

# Load pre-trained LM and tokenizer
lm_model = GPT2LMHeadModel.from_pretrained('gpt2')
tokenizer = GPT2Tokenizer.from_pretrained('gpt2')
tokenizer.add_special_tokens({'pad_token': '[PAD]'})  # Add padding token

# Prepare your training data
train_dataset = TextDataset(train_texts, tokenizer, max_length=128)
train_dataloader = DataLoader(train_dataset, batch_size=8, shuffle=True)

# Configure LM training
lm_model.train()
# Replace the optimizer initialization line
optimizer = torch.optim.AdamW(lm_model.parameters(), lr=1e-5)
num_epochs = 10

# Training loop
for epoch in range(num_epochs):
    for batch in train_dataloader:
        input_ids, attention_mask = batch
        outputs = lm_model(input_ids=input_ids, attention_mask=attention_mask, labels=input_ids)
        loss = outputs.loss

        # Backpropagation and optimization
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        # Print loss or other metrics for monitoring

# Save the fine-tuned LM
lm_model.save_pretrained('fine_tuned_lm')
tokenizer.save_pretrained('fine_tuned_lm')


","If anyone else is running into the issue, the fix for me was to remove all versions I had installed and use pytorch installation which aso installs CUDA. Then, I installed bitsandbytes again and ran it. The results from he finetune were mediocre though, as I had expected.
",fine-tune
implement dropout to pretrained resnet model in pytorch,"I am trying to implement Dropout to pretrained Resnet Model in Pytorch, and here is my code
    feats_list = []
    for key, value in model._modules.items():
        feats_list.append(value)

    for feat in feats_list:
        if isinstance(feat, nn.Conv2d) or isinstance(feat, nn.Conv1d):
            feats_list.append(nn.Dropout(p=0.5, inplace=True))

    model.features = nn.Sequential(*feats_list)
    print(model.features)


I think it should apply dropout to all conv2 and conv1 layers, but in reality, only the last AdaptiveAvgPool2d has attached a drop out rate.
This is what I got
...
  (8): AdaptiveAvgPool2d(output_size=(1, 1))
  (9): Linear(in_features=2048, out_features=1000, bias=True)
  (10): Dropout(p=0.5, inplace=True)

Could someone help me? Thank you
Here is the partial code block FYI
def generic_classifier(model, criterion, optimizer, num_epochs=25):
    # try to select specific layers to freeze or unfreeze from the pretrained model
    # true:trainable;  false: freeze, untraibale
    '''
    n = 0
    for param in model.parameters():
        if n  < 7:
            param.requires_grad = True
        else:
            param.requires_grad = False
        n +=1
    '''
    feats_list = []
    for key, value in model._modules.items():
        feats_list.append(value)

    for feat in feats_list:
        if isinstance(feat, nn.Conv2d) or isinstance(feat, nn.Conv1d):
            feats_list.append(nn.Dropout(p=0.5, inplace=True))
            #print(feat)
    #print(feats_list)

    # modify convolution layers
    model.features = nn.Sequential(*feats_list)
    print(model.features)
    #for name, param in model.named_parameters():
    #    print(name, param.requires_grad)

    # remove all the fully connected layers
    model.fc = nn.Sequential()

    # add a number of fully connected layers of our choice right after the convolutional layers
    model.fc = nn.Sequential(
        # need to know the last layer of selected model architecture
        # resnet50:2048, resnet18: 512, resnet34:512
        # did not find a way to automate this part yet.
        nn.Linear(512, 256),
        nn.ReLU(),
        nn.Linear(256, 256),
        nn.ReLU(),
        nn.Linear(256, 3)
    )

    model = model.to(device)

    # train the model
    model_with_pretrained_train_acc = []
    model_with_pretrained_test_acc = []
    start = time.time()

...
...
        return model

","The issue in your code is that you are appending the nn.Dropout layers to the feats_list within the loop that iterates over the model's modules. However, when you append a nn.Dropout layer to feats_list, it immediately gets added to the end of the list, causing the loop to continue iterating over it. So you are appending lot of dropout layers at the end of your architecture.
The following code contains a loop that runs through all the layers of the pretrained network and if it encounters a convolutional layer it creates an exactly equal one and appends it to the list followed by a dropout layer, otherwise it appends the layer as is without adding dropout layers.
model = models.resnet18(pretrained=True)

feats_list = []
for key, value in model.named_children():  
    if isinstance(value, nn.Conv2d) or isinstance(value, nn.Conv1d):
        feats_list.append(nn.Conv2d(
            in_channels=value.in_channels,
            out_channels=value.out_channels,
            kernel_size=value.kernel_size,
            stride=value.stride,
            padding=value.padding,
            bias=value.bias,
        ))
        feats_list.append(nn.Dropout(p=0.5, inplace=True))
    else:
        feats_list.append(value)

# Create a new model with the modified layers
model = nn.Sequential(*feats_list)

",fine-tune
attributeerror module openai has no attribute finetuneingjob,"I am attempting to start a fine-tuning job using GPT 3.5-turbo via a Python call, using the format listed in the fine-tuning reference, essentially:
import os
import openai
openai.api_key = os.getenv(""OPENAI_API_KEY"")
openai.FineTuningJob.create(training_file=""file-abc123"", model=""gpt-3.5-turbo"")

However, I am running into the AttributeError “module ‘openai’ has no attribute ‘FineTuneingJob’”.
I am running openai v0.27.8. In addition, I uploaded my training data successfully using
openai.File.create(
    file=open(""train_chat_gpt.jsonl"", ""rb""),
    purpose='fine-tune'
)

Any suggestions how to fix this?
Thanks.
","Finally I found the solution.
I had an old version (0.27.8) of the openai module; however, the FineTuningJob function is available starting from version 0.27.9 of the module.
So, you need to update the openai module with the following command:
pip install --upgrade openai

",fine-tune
what does finetuning a multilingual checkpoint mean,"I'm fine-tuning a SetFit model on a French dataset and following the guide in huggingface. They mention this point on the site that I didn't quite understand

""🌎 Multilingual support: SetFit can be used with any Sentence
Transformer on the Hub, which means you can classify text in multiple
languages by simply fine-tuning a multilingual checkpoint.""

Does that mean I must find an already finetuned SetFit model in French when loading the model? As in replace ""paraphrase-mpnet-base-v2"" below with a French one?
model = SetFitModel.from_pretrained(""sentence-transformers/paraphrase-mpnet-base-v2"")

","What the point in the guide suggests is that multilingual models fine-tuned using SetFit method generalize well even on languages they did not see during the SetFit fine-tuning process. This seems to be generally true for multilingual language models but it probably does not do any damage to mention it explicitly, particularly when discussing SetFit, which is a method which usually works with a very small dataset (i.e. the dataset that might not be multilingual).
The finding is supported by the paper mentioned in the guide, where researchers show that model fine-tuned on English data using SetFit performs well on variety of languages (see table 4).
What I would take from it is this: if you fine-tune multilingual checkpoint (e.g. sentence-transformers/paraphrase-multilingual-mpnet-base-v2) and fine-tune it on French, it will perform well on French and probably will also perform well on other languages. If you plan to use the fine-tuned model only on French texts, you certainly can and try to fine-tune a specifically French model - however, it's certainly not true that you must do this.
However, if there exists a specifically French sentence transformer and you want to use your model only on French texts, I would recommend using the French model. Not because you must, but because it might perform better than the multilingual model.
",fine-tune
can39t run transformer fine tuning with m1 mac cpu,"Here's the code I use
from transformers import DistilBertForSequenceClassification, Trainer, TrainingArguments

training_args = TrainingArguments(
    output_dir='./results',          # output directory
    num_train_epochs=1,              # total number of training epochs
    per_device_train_batch_size=16,  # batch size per device during training
    per_device_eval_batch_size=64,   # batch size for evaluation
    warmup_steps=500,                # number of warmup steps for learning rate scheduler
    weight_decay=0.01,               # strength of weight decay
    logging_dir='./logs',            # directory for storing logs
    logging_steps=10,
    use_mps_device=False
)

model = DistilBertForSequenceClassification.from_pretrained(""distilbert-base-uncased"")

trainer = Trainer(
    model=model,                         # the instantiated 🤗 Transformers model to be trained
    args=training_args,                  # training arguments, defined above
    train_dataset=train_dataset,         # training dataset
    eval_dataset=val_dataset             # evaluation dataset
)
 
device = torch.device(""cpu"")
model.to(device)
  
trainer.train()

I get this error: message
RuntimeError: Placeholder storage has not been allocated on MPS device!
If I change the argument to use_mps_device=True, it will train with GPU, even though the model.to statement is pointing to CPU?
I could get the training to run with GPU but not with CPU.
","The Trainer automatically moves the model to the appropriate device if the place_model_on_device attribute is True.
My suggestion would be to remove any moving code & do it through the transformers library. You can specify no_cuda in TrainingArguments as False so that the training objects aren't moved to GPU.
",fine-tune
how to create a dataset for a model like falcon7b40b,"I am having the data as docx files and I want to use them to fine-tune the Falcon model. From what I see the data used to train the model was in json format. How can I convert my data in a format to be useful for the model?
Currently I am trying to convert my data in the json format, but it's a tedious work to do by hand.
","To convert your data from DOCX format to JSON format, you can use python-docx library to extract text from a DOCX file and convert it to JSON:
import json
from docx import Document

 # Load the DOCX file
doc = Document('input.docx')

# Extract text content
text = [p.text for p in doc.paragraphs]

# Create JSON objects
json_data = []
for paragraph in text:
    json_data.append({""text"": paragraph})

# Save as JSON
with open('output.json', 'w') as json_file:
    json.dump(json_data, json_file)

",fine-tune
how do i fine tune bert39s self attention mechanism,"My goal is to fine tune BERT's self attention so that I can see to what extent two random sentences in a document (with positional encoding) rely on each other contextually.
Many explanations and article that I see talk about the implementation of self-attention but do not mention how to further train self attention.
Here is what I'm thinking of doing to train BERT's self attention:

Use some kind of word to vector algorithm and vectorize all the words in the article.

Add positional encoding to each sentence [where the sentence is an array of vectors (words)] using a sinusoidal function.

Make matrix of each sentence concatenated with every other sentence

For each sentence-sentence pair, iterate through each words masking them.  The model must guess the word based on context; back prop is based on accuracy of the guess.

The finished model should be able to take in an arbitrary sentence-sentence pair and output an attention matrix.


I'm not sure if such a method is the right one for fine tuning (or if this even counts as continuing pre-training) a self attention mechanism, or if BERT is even the best model to train a self attention function on.
I'm obviously very new to fine tuning LLMs, so any guidance would be greatly appreciated!
","Hugginface provides a model class, that you can use for your task:
https://huggingface.co/docs/transformers/model_doc/bert#transformers.BertForNextSentencePrediction.
See also the small example provided in the link. You can use the logits in the output to create your attention matrix.
This is just an inference task. Fine-tuning means to do further training on custom data.
",fine-tune
gpu out of memory fine tune flanul2,"
OutOfMemoryError: CUDA out of memory. Tried to allocate 256.00 MiB
(GPU 0; 15.78 GiB total capacity; 14.99 GiB already allocated; 3.50
MiB free; 14.99 GiB reserved in total by PyTorch) If reserved memory
is >> allocated memory try setting max_split_size_mb to avoid
fragmentation.  See documentation for Memory Management and
PYTORCH_CUDA_ALLOC_CONF

I have Standard_NC24s_v3 single node GPU with 448GB memory and 4 GPUs. However the error message says the total capacity is 15.78GiB. Is the fine tune not using 4 GPUs? How to get all the 4 GPUs used in the fine tune of Flan-UL2 using huggingface transformers?
","I solve the issue by using the following package versions.
!pip install transformers==4.28.1
!pip install sentencepiece==0.1.97
!pip install accelerate==0.18.0
!pip install bitsandbytes==0.37.2
!pip install torch==1.13.1

",fine-tune
how can i finetune a model from openai39s whisper asr on my own training data,"I use OpenAI's Whisper python lib for speech recognition. I have some training data: either text only, or audio  + corresponding transcription. How can I finetune a model from OpenAI's Whisper ASR on my own training data?
","From https://github.com/openai/whisper/discussions/64, the released code doesn't contain the training/finetuning part. Therefore one would have to write it to be able to train/finetune a model from OpenAI's Whisper ASR on my own training data.
Also, from https://openai.com/blog/whisper/:

We are open-sourcing models and inference code to serve as a foundation for building useful applications and for further research on robust speech processing.

No training code mentioned.

William Castrillon and nizata pointed to the following fine-tuning codes created by third-party developers:

https://huggingface.co/blog/fine-tune-whisper (code)
https://github.com/openai/whisper/discussions/64
https://huggingface.co/spaces/openai/whisper/discussions/6

",fine-tune
how can i finetune mbart50 for machine translation in the transformers python library so that it learns a new word,"I try to fine-tune mBART-50 (paper, pre-trained model on Hugging Face) for machine translation in the transformers Python library. To test the fine-tuning, I am trying to simply teach mBART-50 a new word that I made up.
I use the following code. Over 95% of the code is from the Hugging Face documentation:
from transformers import MBartForConditionalGeneration, MBart50TokenizerFast

print('Model loading started')
model = MBartForConditionalGeneration.from_pretrained(""facebook/mbart-large-50"")
tokenizer = MBart50TokenizerFast.from_pretrained(""facebook/mbart-large-50"", src_lang=""fr_XX"", tgt_lang=""en_XX"")
print('Model loading done')

src_text = "" billozarion ""
tgt_text =  "" plorization ""

model_inputs = tokenizer(src_text, return_tensors=""pt"")
with tokenizer.as_target_tokenizer():
    labels = tokenizer(tgt_text, return_tensors=""pt"").input_ids

print('Fine-tuning started')
for i in range(1000):
    #pass
    model(**model_inputs, labels=labels) # forward pass
print('Fine-tuning ended')
    
# Testing whether the model learned the new word. Translate French to English
tokenizer = MBart50TokenizerFast.from_pretrained(""facebook/mbart-large-50-many-to-many-mmt"")
tokenizer.src_lang = ""fr_XX""
article_fr = src_text
encoded_fr = tokenizer(article_fr, return_tensors=""pt"")
generated_tokens = model.generate(**encoded_fr, forced_bos_token_id=tokenizer.lang_code_to_id[""en_XX""])
translation = tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)
print(translation)

However, the new word wasn't learned. The output is ""billozarion"" instead of ""plorization"". Why?
I'm strictly following the Hugging Face documentation, unless I missed something.  The # forward pass does make me concerned, as one would need a backward pass to update the gradients. Maybe this means that the documentation is incorrect, however I can't test that hypothesis as I don't know how to add the backward pass.

Environment that I used to run the code: Ubuntu 20.04.5 LTS with an NVIDIA A100 40GB GPU (I also tested with an NVIDIA T4 Tensor Core GPU) and CUDA 12.0 with the following conda environment:
conda create --name mbart-python39 python=3.9
conda activate mbart-python39 
pip install transformers==4.28.1
pip install chardet==5.1.0
pip install sentencepiece==0.1.99
pip install protobuf==3.20

","One could add the following to fine-tune mBART-50:
from transformers.optimization import AdamW

# Set up the optimizer and training settings
optimizer = AdamW(model.parameters(), lr=1e-4)
model.train()

print('Fine-tuning started')
for i in range(100):
    optimizer.zero_grad()
    output = model(**model_inputs, labels=labels) # forward pass
    loss = output.loss
    loss.backward()
    optimizer.step()
print('Fine-tuning ended')

Full code:
from transformers import MBartForConditionalGeneration, MBart50TokenizerFast
from transformers.optimization import AdamW
import os
os.environ[""TOKENIZERS_PARALLELISM""] = ""false""


print('Model loading started')
model = MBartForConditionalGeneration.from_pretrained(""facebook/mbart-large-50"")
tokenizer = MBart50TokenizerFast.from_pretrained(""facebook/mbart-large-50"", src_lang=""fr_XX"", tgt_lang=""en_XX"")
print('Model loading done')

src_text = "" billozarion ""
tgt_text =  "" plorizatizzzon ""

model_inputs = tokenizer(src_text, return_tensors=""pt"")
with tokenizer.as_target_tokenizer():
    labels = tokenizer(tgt_text, return_tensors=""pt"").input_ids

# Set up the optimizer and training settings
optimizer = AdamW(model.parameters(), lr=1e-4)
model.train()

print('Fine-tuning started')
for i in range(100):
    optimizer.zero_grad()
    output = model(**model_inputs, labels=labels) # forward pass
    loss = output.loss
    loss.backward()
    optimizer.step()
print('Fine-tuning ended')
    
# translate French to English
tokenizer = MBart50TokenizerFast.from_pretrained(""facebook/mbart-large-50-many-to-many-mmt"")
tokenizer.src_lang = ""fr_XX""
article_fr = src_text
encoded_fr = tokenizer(article_fr, return_tensors=""pt"")
generated_tokens = model.generate(**encoded_fr, forced_bos_token_id=tokenizer.lang_code_to_id[""en_XX""])
translation =tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)
print(translation)

It outputs the correct made up translation ""plorizatizzzon"".
I reported the documentation issue on https://github.com/huggingface/transformers/issues/23185

https://github.com/huggingface/transformers/tree/main/examples/pytorch/translation contains two more advanced scripts to fine-tune mBART and T5 (thanks sgugger for pointing me to it). Here is how to use the script to fine-tune mBART:
Create a new conda environment:
conda create --name mbart-source-transformers-python39 python=3.9
conda activate mbart-source-transformers-python39 
git clone https://github.com/huggingface/transformers.git
cd transformers
pip install git+https://github.com/huggingface/transformers
pip install datasets evaluate accelerate sacrebleu
conda install pytorch torchvision torchaudio pytorch-cuda=11.8 -c pytorch -c nvidia
pip install sentencepiece==0.1.99
pip install protobuf==3.20
pip install --force-reinstall charset-normalizer==3.1.0

Command:
python examples/pytorch/translation/run_translation.py \
    --model_name_or_path facebook/mbart-large-50 \
    --do_train \
    --do_eval \
    --source_lang fr_XX \
    --target_lang en_XX \
    --source_prefix ""translate French to English: "" \
    --train_file finetuning-translation-train.json \
    --validation_file finetuning-translation-validation.json  \
    --test_file finetuning-translation-test.json \
    --output_dir tmp/tst-translation4 \
    --per_device_train_batch_size=4 \
    --per_device_eval_batch_size=4 \
    --overwrite_output_dir \
    --do_predict \
    --predict_with_generate

(Note: the readme seems to have missed --do_predict)
with finetuning-translation-train.json, finetuning-translation-validation.json and finetuning-translation-test.json formatted as follows with the JSON Lines format:
{""translation"": {""en"": ""20 year-old male tennis player."", ""fr"": ""Joueur de tennis de 12 ans""}}
{""translation"": {""en"": ""2 soldiers in an old military Jeep"", ""fr"": ""2 soldats dans une vielle Jeep militaire""}}

(Note: one must use double quotes in the .json files. Single quotes e.g. 'en' will make the script crash.)
I run the code on Ubuntu 20.04.5 LTS with an NVIDIA T4 Tensor Core GPU (16GB memory) and CUDA 12.0. The mBART-50 model takes around 15GB of GPU memory.
",fine-tune
expected file to have jsonl format where every line is a json dictionary openai createfile for fine tune,"I created file with name mydata.jsonl and I put on it these lines
    {
        ""prompt"": ""aa"",
        ""completion"": ""bb""
    }
    {
        ""prompt"": ""cc"",
        ""completion"": ""dd""
    }

then in index.js I did this function
    const { Configuration, OpenAIApi } = require(""openai"");
    const fs = require(""fs"");
    
    const configuration = new Configuration({
        apiKey: process.env.OPENAI_API_KEY,
    });
    async function getAiResponse(topic) {
        const openai = new OpenAIApi(configuration);
        const filename = 'example.txt';
        const fileContents = 'This is some example text.';
        const response = await openai.createFile(
            fs.createReadStream(""mydata.jsonl""),
            ""fine-tune""
        );
        console.log(response)
    }
    getAiResponse(""ee"");

when I run my code I got error
    Expected file to have JSONL format, where every line is a JSON dictionary. Line 1 is not a dictionary (HINT: line starts with: ""{..."").

I can't get where is the exactly the error
","The JSONL format is where the each json object is sperated by a newline characater.
So your output should be:
{ ""prompt"": ""aa"", ""completion"": ""bb"" }
{ ""prompt"": ""cc"", ""completion"": ""dd"" }

i.e. one LINE per json object
",fine-tune
transformers always only use a single linear layer for classification head,"For example, in the class BertForSequenceClassification definition, only one Linear layer is used for the classifier. If just one Linear layer is used, doesn’t it just do linear projection for pooled_out? Will such a classifier produce good predictions? Why not use multiple Linear layers? Does transformers offer any option for using multiple Linear layers as the classification head?
I looked at several other classes. They all use a single Linear layer as the classification head.
","To add onto the previous answer,

Embedding layers (self.bert = BertModel(config) in your case) transform the original data (a sentence, an image etc.) into some semantic-aware vector spaces. This is where all the architecture designs come in (e.g. attention, cnn, lstm etc.), which are all far more superior than a simple FC for their chosen tasks. So if you have the capacity of adding multiple FCs, why not just add another attention block? On the other hand, the embeddings from a decent model should have large inter-class distance and small intra-class variance, which could easily be projected to their corresponding classes in a linear fashion, and a FC is more than enough.

It would be ideal to have the pretrained portion as big as possible such that, as a downstream user, I just have to train/finetune a tiny bit of the model (e.g. the fc classification layer)


",fine-tune
fine tune in using huggingface,"I am totally new bee in NLP and I want to fine tune a pre-trained model (rebel-large) with my onw dataset. After reading some tutorials I frealized there is no difference between training and fine tuning? I mean I understand they are not the same, but in the code there are no difference.
How can I define how I want to fine tune the model? E.g. if I just want to change the weights in the last layer, how should I make the trained to do it?
","You're right, fine-tuning a model is the same as loading a pre-trained model and then training it.
You can use the following snippet, and replace the dataset with yours.
from datasets import load_dataset
from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer
import numpy as np
import evaluate

tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')
model = AutoModelForSequenceClassification.from_pretrained('bert-base-uncased')
metric = evaluate.load('accuracy')


def tokenize_function(examples):
    return tokenizer(examples['text'], padding='max_length', truncation=True)


def compute_metrics(eval_pred):
    logits, labels = eval_pred
    predictions = np.argmax(logits, axis=-1)
    return metric.compute(predictions=predictions, references=labels)


dataset = load_dataset('imdb')
dataset = dataset.map(tokenize_function, batched=True)

# print(model)  # Use to discover your layers and choose which ones to put in `to_freeze`

to_freeze = [model.bert.encoder]

for layer in to_freeze:
    for param in layer.parameters():
        param.requires_grad = False

training_args = TrainingArguments(output_dir='test_trainer')
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=dataset['train'],
    eval_dataset=dataset['test']
)
trainer.train()

",fine-tune
do i need to retrain bert for ner to create new labels,"I am very new to natural language processing and I was thinking about working on named entity recognition NER. A friend of mine who works with NLP advised me to check out BERT, which I did. When reading the documentation and checking out the CoNLL-2003 data set, I noticed that the only labels are person, organization, location, miscellanious and outside. What if instead of outside, I want the model to recognize date, time, and other labels. I get that I would need a dataset labelled as such so, assuming that I have that, do I need to retrain BERT from stratch or can I somehow fine tune the existing model without needing to restart the whole process?
","Yes, you would have to use a model trained using the specific labels you require. The OntoNotes dataset may be better suited for what you are trying to do, as it includes the 18 entity names listed below (see OntoNotes 5.0 Release Notes for further info).
The HuggingFace flair/ner-english-ontonotes-large (here) and flair/ner-english-ontonotes-fast (here) models are trained on this dataset and will likely produce results closer to what you desire. As a demo (make sure to pip install flair first)
from flair.data import Sentence
from flair.models import SequenceTagger

tagger = SequenceTagger.load(""flair/ner-english-ontonotes-large"")  # load tagger
sentence = Sentence(""On September 1st George won 1 dollar while watching Game of Thrones."")  # example sentence
tagger.predict(sentence)  # predict NER tags

# Print sentence and NER spans
print(sentence)
print('The following NER tags are found:')
# iterate over entities and print
for entity in sentence.get_spans('ner'):
    print(entity)

# Output
# Span [2,3]: ""September 1st""   [− Labels: DATE (1.0)]
# Span [4]: ""George""   [− Labels: PERSON (1.0)]
# Span [6,7]: ""1 dollar""   [− Labels: MONEY (1.0)]
# Span [10,11,12]: ""Game of Thrones""   [− Labels: WORK_OF_ART (1.0)

OntoNotes 5.0 Named Entities

PERSON (People, including fictional)
NORP (Nationalities or religious or political groups)
FACILITY (Buildings, airports, highways, bridges, etc.)
ORGANIZATION (Companies, agencies, institutions, etc.)
GPE (Countries, cities, states)
LOCATION (Non-GPE locations, mountain ranges, bodies of water)
PRODUCT (Vehicles, weapons, foods, etc. (Not services))
EVENT (Named hurricanes, battles, wars, sports events, etc.)
WORK OF ART (Titles of books, songs, etc.)
LAW (Named documents made into laws)
LANGUAGE (Any named language)
DATE (Absolute or relative dates or periods)
TIME (Times smaller than a day)
PERCENT (Percentage (including “%”))
MONEY (Monetary values, including unit)
QUANTITY (Measurements, as of weight or distance)
ORDINAL (“first”, “second”)
CARDINAL (Numerals that do not fall under another type)

",fine-tune
what are the differences between adapter tuning and prefix tuning,"I am trying to understand the concept of adapter-tuning, prompt-tuning, and prefix-tuning in the context of few-shot learning.
It appears to me that I can apply prompt tuning to a black box language model.
I read for prompt tuning the entire pre-trained language model is frozen. If that's the case prompt tuning could be applied for an OpenAI model like gpt-3 and Codex.
How could I do prompt tuning with OpenAI Codex? I don't find any way so far.
How these techniques are different than in-context example that could be given by few-shot learning.
Can anyone please guide me in the correct direction?
","These are alternatives to fine-tuning model. They are essentially solutions that reside between few-shot learning and complete fine-tuning of models.
The other answer in this SO post is completely wrong. Fine-tuning has nothing to do with neither prompt tuning nor prefix tuning. These two are completely different techniques than fine-tuning.
Correct reference to prompt tuning and prefix tuning are given below:

Prompt Tuning: For prompt tuning k learnable parameter i.e. continuous token embeddings is appended to the input. But the entire pre-trained language model is frozen.

Prefix Tuning: For k positions prepended to the input, concatenate additional learnable weights for keys and values at every attention layer. Different to prompt tuning (only learnable input vectors).


Papers that introduced these techniques are given below:

Prompt Tuning: https://aclanthology.org/2021.emnlp-main.243/
Prefix-Tuning: https://arxiv.org/abs/2101.00190

",fine-tune
how to extend the vocabulary of a pretrained transformer model,"I would like to extend a zero-shot text classification (NLI) model's vocabulary, to include domain-specific vocabulary or just to keep it up-to-date. For example, I would like the model to know the names of the latest COVID-19 variants are related to the topic 'Healthcare'.
I've added the tokens to the tokenizer and resized the token embeddings. However, I don't know how to finetune the weights in the embedding layer, as suggested here.
To do the finetuning, can I use simply use texts containing a mixture of new vocabulary and existing vocabulary, and have the tokenizer recognise the relations between tokens through co-occurrences in an unsupervised fashion?
Any help is appreciated, thank you!
","If you resized the corresponding embedding weights with resize_token_embeddings, they will be initialised randomly.
Technically, you can fine-tune the model on your target task (NLI, in your case), without touching the embedding weights. In practice, it will be harder for your model to learn anything meaningful about the newly added tokens, since their embeddings are randomly initialised.
To learn the embedding weights you can do further pre-training, before fine-tuning on the target task. This is done by training the model on the pre-training objective(s) (such as Masked Language Modelling). Pre-training is more expensive than fine-tuning of course, but remember that you aren't pre-training from scratch, since you start pre-training from the checkpoint of the already pre-trained model. Therefore, the number of epochs/steps will be significantly less than what was used in the original pre-training setup.
When doing pre-training it will be beneficial to include in-domain documents, so that it can learn the newly added tokens. Depending on whether you want the model to be more domain specific or remain varied so as to not ""forget"" any previous domains, you might also want to include documents from a variety of domains.
The Don't Stop Pretraining paper might also be an interesting reference, which delves into specifics regarding the type of data used as well as training steps.
",fine-tune
fine tuning freezing weights nnunet,"Good morning,
I've followed the instructions in this github issue:
https://github.com/MIC-DKFZ/nnUNet/issues/1108
to fine-tune an nnUNet model (pyTorch) on a pre-trained one, but this method retrain all weights, and i would like to freeze all weigths and retrain only the last layer's weights, changing the number of segmentation classes from 3 to 1.
Do you know a way to do that?
Thank you in advance
","To freeze the weights you need to set parameter.requires_grad = False.
Example:
from nnunet.network_architecture.generic_UNet import Generic_UNet

model = Generic_UNet(input_channels=3, base_num_features=64, num_classes=4, num_pool=3)

for name, parameter in model.named_parameters():
    if 'seg_outputs' in name:
        print(f""parameter '{name}' will not be freezed"")
        parameter.requires_grad = True
    else:
        parameter.requires_grad = False

To check parameter names you can use print:
print(model)

which produces:
Generic_UNet(
  (conv_blocks_localization): ModuleList(
    (0): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (instnorm): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (instnorm): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
  )
  (conv_blocks_context): ModuleList(
    (0): StackedConvLayers(
      (blocks): Sequential(
        (0): ConvDropoutNormNonlin(
          (conv): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (dropout): Dropout2d(p=0.5, inplace=True)
          (instnorm): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
        )
        (1): ConvDropoutNormNonlin(
          (conv): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
          (dropout): Dropout2d(p=0.5, inplace=True)
          (instnorm): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
        )
      )
    )
    (1): Sequential(
      (0): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (dropout): Dropout2d(p=0.5, inplace=True)
            (instnorm): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
      (1): StackedConvLayers(
        (blocks): Sequential(
          (0): ConvDropoutNormNonlin(
            (conv): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            (dropout): Dropout2d(p=0.5, inplace=True)
            (instnorm): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            (lrelu): LeakyReLU(negative_slope=0.01, inplace=True)
          )
        )
      )
    )
  )
  (td): ModuleList(
    (0): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)
  )
  (tu): ModuleList(
    (0): Upsample()
  )
  (seg_outputs): ModuleList(
    (0): Conv2d(64, 4, kernel_size=(1, 1), stride=(1, 1), bias=False)
  )
)


Or you can visualize your network with netron:
https://github.com/lutzroeder/netron
",fine-tune
mxnet  how to add dropout layer to resnet_v1 pretrained model,"I am trying to finetune a pretrained model in mxnet: ResNet50_v1.
This model does not have dropout and I would like to add it to avoid overfitting and make it look similar to the last layers of I3D_Resnet50_v1_Kinetics400.
I tried to do the following but when training I get an error:
Last layers of original network (ResNet50_v1):
...
(8): GlobalAvgPool2D(size=(1, 1), stride=(1, 1), padding=(0, 0), ceil_mode=True, global_pool=True, pool_type=avg, layout=NCHW)
  )
  (output): Dense(2048 -> 1000, linear)

My attempt:
    classes = 2
    model_name = 'ResNet50_v1'
    finetune_net = get_model(model_name, pretrained=True)

    with finetune_net.name_scope():
       finetune_net.output = nn.Dense(2048, in_units=2048)
       finetune_net.head = nn.HybridSequential()
       finetune_net.head.add(nn.Dropout(0.95))
       finetune_net.head.add(nn.Dense(2, in_units=2048))
       finetune_net.fc = nn.Dense(2, in_units=2048)

    finetune_net.output.initialize(init.Xavier(), ctx = ctx)
    finetune_net.head.initialize(init.Xavier(), ctx = ctx)
    finetune_net.fc.initialize(init.Xavier(), ctx = ctx)
    finetune_net.collect_params().reset_ctx(ctx)
    finetune_net.hybridize()

Last layers of the modified network (ResNet50_v1):
...
(8): GlobalAvgPool2D(size=(1, 1), stride=(1, 1), padding=(0, 0), ceil_mode=True, global_pool=True, pool_type=avg, layout=NCHW)
  )
(output): Dense(2048 -> 2048, linear)
  (head): HybridSequential(
    (0): Dropout(p = 0.95, axes=())
    (1): Dense(2048 -> 2, linear)
  )
  (fc): Dense(2048 -> 2, linear)
)

Last layers of I3D_Resnet50_v1_Kinetics400:
...## Heading ## 
(st_avg): GlobalAvgPool3D(size=(1, 1, 1), stride=(1, 1, 1), padding=(0, 0, 0), ceil_mode=True, global_pool=True, pool_type=avg, layout=NCDHW)
    (head): HybridSequential(
      (0): Dropout(p = 0.8, axes=())
      (1): Dense(2048 -> 2, linear)
    )
    (fc): Dense(2048 -> 2, linear)

This is what params of the modifies network look like
Parameter resnetv10_dense1_weight (shape=(2048, 2048), dtype=float32) write
Parameter resnetv10_dense1_bias (shape=(2048,), dtype=float32) write
Parameter resnetv10_dense2_weight (shape=(2, 2048), dtype=float32) write
Parameter resnetv10_dense2_bias (shape=(2,), dtype=float32) write
Parameter resnetv10_dense3_weight (shape=(2, 2048), dtype=float32) write
Parameter resnetv10_dense3_bias (shape=(2,), dtype=float32) write

Error when training:
/usr/local/lib/python3.7/dist-packages/mxnet/gluon/block.py:825: UserWarning: Parameter resnetv10_dense3_bias, resnetv10_dense3_weight, resnetv10_dense2_bias, resnetv10_dense2_weight is not used by any computation. Is this intended?
out = self.forward(*args)
UserWarning: Gradient of Parameter resnetv10_dense2_bias on context gpu(0) has not been updated by backward since last step. This could mean a bug in your model that made it only use a subset of the Parameters (Blocks) for this iteration. If you are intentionally only using a subset, call step with ignore_stale_grad=True to suppress this warning and skip updating of Parameters with stale gradient
dense2 and dense3, the ones I have added as new dense layers are not being updated.
dense1 was already in the model, I just changed the output from 1000 to 2048.
Any help woul be very much appreciated as I am quite stuck ...
","Since you assign new layers to the model, you should reimplement hybrid_forward (or forward) method to include them in computations:
from mxnet.gluon import nn
from mxnet.init import Xavier
from mxnet.gluon.block import HybridBlock

class MyResNet(HybridBlock):
    def __init__(self, **kwargs):
        super().__init__(**kwargs)
        self.finetune_net = get_model('ResNet50_v1', pretrained=True)
        self.finetune_net.output = nn.Dense(2048, in_units=2048)
        self.head = nn.HybridSequential()
        self.head.add(nn.Dropout(0.95))
        self.head.add(nn.Dense(2, in_units=2048))
        self.fc = nn.Dense(2, in_units=2048)

    def hybrid_forward(self, F, x):
        x = self.finetune_net(x)
        x = self.head(x)
        x = self.fc(x)
        return x

    def initialize_outputs(self):
        self.finetune_net.output.initialize(init=Xavier())
        self.head.initialize(init=Xavier())
        self.fc.initialize(init=Xavier())

my_resnet = MyResNet()
my_resnet.initialize_outputs()
my_resnet(x)

",fine-tune
wandb getting logged without initiating,"I do not want to use wandb. I don't even have an account. I am simply following this notebook for finetuning. I am not running the 2nd and 3 cells because I do not want to push the model to the hub.
However, when I do trainer.train() I get the following error : 
I don't understand where wandb.log is being called.
I even tried os.environ[""WANDB_DISABLED""]  = ""true"" but I still get the error.
Please help.
","posting the same message as over on transformers:

You can turn off all external logger logging, including wandb logging by passing report_to=""none"" in your Seq2SeqTrainingArguments.
You might have noticed the following warning when setting up your TrainingArguments:
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-)

Right now the default is to run all loggers that you have installed, so maybe you installed wandb on your machine since the last time you ran the script?
If you would like to log with wandb, best practice would already be to start setting report_to=""wandb""
",fine-tune
pytorch  problem with fine tune training from custom features and classes,"The core of my problem is the fact that my features come from NumPy files (.npy).
Therefore I need the following class in my code
import torch
import torchvision
import torchvision.transforms as transforms
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
import numpy as np
from torch.utils.data import Dataset, DataLoader
from torchvision.models import resnet50
import time
import copy

class MyDataSet(torch.utils.data.Dataset):
    def __init__(self, x, y, transform=None):
        super(MyDataSet, self).__init__()
        # store the raw tensors
        self._x = np.load(x)
        self._y = np.load(y)
        self.transform = transform
    
    def __len__(self):
        # a DataSet must know it size
        return self._x.shape[0]

    def __getitem__(self, index):
        x = self._x[index, :]
        y = self._y[index, :]
        return x, y

To convert my NumPy files to DataLoaders I do the following. The code below seems to work (at least, no errors are returned)
#Transform dataset 
transform = transforms.Compose([transforms.ToTensor()])
dataset = MyDataSet(""train1-features.npy"",""train1-classes.npy"",transform=transform)
dataloader = DataLoader(dataset, batch_size=32)

I am trying to fine-tune a RESNET-50 network in these data with 12 classes. Here is what I do
def set_parameter_requires_grad(model, feature_extracting):
    if feature_extracting:
        for param in model.parameters():
            param.requires_grad = False

feature_extract = True
batch_size = 8
num_epochs = 15
num_classes=12

model_ft = resnet50(pretrained=True)
set_parameter_requires_grad(model_ft, feature_extract)
num_ftrs = model_ft.fc.in_features
model_ft.fc = nn.Linear(num_ftrs, num_classes)
input_size = 224

if torch.cuda.is_available():
    model_ft.cuda()

params_to_update = model_ft.parameters()

print(""Params to learn:"")
if feature_extract:
    params_to_update = []
    for name,param in model_ft.named_parameters():
        if param.requires_grad == True:
            params_to_update.append(param)
            print(""\t"",name)
else:
    for name,param in model_ft.named_parameters():
        if param.requires_grad == True:
            print(""\t"",name)

# Observe that all parameters are being optimized
optimizer_ft = optim.SGD(params_to_update, lr=0.001, momentum=0.9)

# Setup the loss fxn
criterion = nn.CrossEntropyLoss()

Finally, here is the problematic training function
for epoch in range(num_epochs):  # loop over the dataset multiple times

    running_loss = 0.0
    for i, data in enumerate(dataloader, 0):
          
        # get the inputs; data is a list of [inputs, labels]
        inputs, labels = data
            
        #transfer labels and inputs to cuda()
        inputs,labels=inputs.cuda(), labels.cuda()
    
        # zero the parameter gradients
        optimizer.zero_grad()

        # forward + backward + optimize
        outputs = model_ft(inputs)
        loss = loss_func(outputs, labels)
        loss.backward()
        optimizer.step()

        # print statistics
        running_loss += loss.item()
        if i % 2000 == 1999:    # print every 2000 mini-batches
            print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 2000:.3f}')
            running_loss = 0.0

This returns me the following error once I execute the code:
Traceback (most recent call last):
  File ""train_my_data_example.py"", line 89, in <module>
    for i, data in enumerate(dataloader, 0):
  File ""/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py"", line 517, in __next__
    data = self._next_data()
  File ""/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py"", line 557, in _next_data
    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration
  File ""/usr/local/lib/python3.8/dist-packages/torch/utils/data/_utils/fetch.py"", line 44, in fetch
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File ""/usr/local/lib/python3.8/dist-packages/torch/utils/data/_utils/fetch.py"", line 44, in <listcomp>
    data = [self.dataset[idx] for idx in possibly_batched_index]
  File ""train_my_data_example.py"", line 29, in __getitem__
    y = self._y[index, :]
IndexError: too many indices for array: array is 1-dimensional, but 2 were indexed

The error is clearly the dataloader variable, so is this creation ok? I mean, I am loading NumPy data and transforming it to a data loader as below:
transform = transforms.Compose([transforms.ToTensor()])
dataset = MyDataSet(""train1-features.npy"",""train1-classes.npy"",transform=transform)
dataloader = DataLoader(dataset, batch_size=32)

Is there any error in my data loader or is the problem the training loop of Pytorch?
P.s: you can reproduce my code by downloading the classes and features here
","You are trying to index the second axis of an array which only has a single dimension. Simply replace y = self._y[index, :] with y = self._y[index].
Actually when positioned last, : is not required as all dimensions are selected by default.
",fine-tune
tensorflow access to see the layer activation finetuning,"I use fine-tuning. How can I see and access the activations of all layers that are inside of the convolutional base?
conv_base = VGG16(weights='imagenet',
              include_top=False,
              input_shape=(inp_img_h, inp_img_w, 3))

def create_functional_model():
   inp = Input(shape=(inp_img_h, inp_img_w, 3))
   model = conv_base(inp)
   model = Flatten()(model)
   model = Dense(256, activation='relu')(model)
   outp = Dense(1, activation='sigmoid')(model)
   return Model(inputs=inp, outputs=outp)

model = create_functional_model()
model.summary()

The model summary is
Layer (type)                 Output Shape              Param #   
=================================================================
vgg16 (Functional)           (None, 7, 7, 512)         14714688  
_________________________________________________________________
flatten_2 (Flatten)          (None, 25088)             0         
_________________________________________________________________
dense_4 (Dense)              (None, 256)               6422784   
_________________________________________________________________
dense_5 (Dense)              (None, 1)                 257       
=================================================================
Total params: 21,137,729
Trainable params: 21,137,729
Non-trainable params: 0
_________________________________________________________________

Thus, the levels inside the conv_base are not accessible.
","As @Frightera said in comments, you can access the base model summary by:
model.layers[0].summary()

And if you want to access activation functions of its layers you can try this:
print(model.layers[0].layers[index_of_layer].activation)
#or
print(model.layers[0].get_layer(""name_of_layer"").activation)

",fine-tune
using weight from a gensim word2vec model as a starting point of another model,"I have two corpora that are from the same field, but with a temporal shift, say one decade. I want to train Word2vec models on them, and then investigate the different factors affecting the semantic shift.
I wonder how should I initialize the second model with the first model's embeddings to avoid as much as possible the effect of variance in co-occurrence estimates.
","At a naive & easy level, you can just load one existing model, and .train() on new data. But note if doing that:

Any words not already known by the model will be ignored, and the word-frequencies that feed algorithmic steps will only be from the initial survey
While all words in the current corpus will get as many training-updates as their appearances (& your epochs setting) dictate, and thus be nudged arbitrarily-far from their original-model locations, other words from the seed model will stay exactly where they were. But, it's only the interleaved tug-of-war between words in the same training session that makes them usefully comparable. So doing this sequential training – updating only some words in a new training session – is likely to degrade the meaningfulness of word-to-word comparisons, in hard-to-measure ways.

Another approach that might be woth trying could be to train single model over the combined corpus - but transform/repeat the era-specific texts/words in certain ways to be able to distinguish earlier-usages from later-usages. There are more details about this suggestion in the context of word-vectors varying over usage-eras in a couple previous answers:
https://stackoverflow.com/a/57400356/130288
https://stackoverflow.com/a/59095246/130288
",fine-tune
upos mappings  tensorflow datasets tdfs,"I am using the tensorflow tdfs dataset extreme/pos which I retrieve using the code below.  It is annotated with universal part of speech POS labels.  These are int values.  Its fairly easy to map them back to their part of speech by creating my own mapping (0 = ADJ, 7 = NOUN, etc.) but I was wondering if there is a way of retrieving these class mappings from the tdfs dataset?
(orig_train, orig_dev, orig_test), ds_info = tfds.load(
'xtreme_pos/xtreme_pos_en',
split=['train', 'dev', 'test'],
shuffle_files=True,
with_info=True
)

","One way is to dig into Tensorflow code to see where is defined the list of POS and then import it to use in your code.
You can find the list of the POS in the Github code of tensorflow Datasets there (UPOS constant):
https://github.com/tensorflow/datasets/blob/master/tensorflow_datasets/core/dataset_builders/conll/conllu_dataset_builder_utils.py#L31
The item order is their index so with display(pd.Series(UPOS)), you get:


Another way would be to extract the items from the upos column of tfds.as_dataframe (taking a few rows, concatenating the upos values, splitting by the separating character and taking the set() to get the unique values.
",tensorflow
what is the current 171official187 waybest practice to train an object detection model using the tensorflow framework,"My goal is to use transfer learning to train an object detection model with my own data (bounding boxes) and convert the models to tflite-format.
I did it before, using the Tensorflow Object Detection API which was great because it provided a wide range of models to use for transfer learning. But the API is no longer maintained and I want to work with something “up to date”.
So I checked on the TensorFlow page and found this tutorial but it again uses the Object Detection API, which is deprecated, as stated before.
I also found this tutorial which uses the TensorFlow Lite Model Maker library so I gave the linked Collab a try but I don’t even get past the pip-install because there are some errors regarding the required versions of different packages (we are talking about the official tutorial collab here! And I ran into the same errors when using the code on my PC).
Then there is MediaPipe with this tutorial When I run the Collab I get a warning.

“TensorFlow Addons (TFA) has ended development and introduction of new features. TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.”

So again, some kind of dead end. And when I tried to run it on my PC I had some issues with importing parts of mediapipe_model_maker.
At this point, I am running out of ideas to be honest. I’m “just” looking for a way to use transfer learning with object detection models (especially smaller ones like Mobilenet, EfficientDet). Is there really no worthy successor of Tensorflow Object Detection API? Am I missing something? Some new API I didn't find, some package I am not aware of?
","Best thing I could do at this point was to use “tflite_model_maker” which was a challenge in itself because of dependency issues.
It still says “TensorFlow Addons (TFA) has ended development and introduction of new features.TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.”
But at least I can train “efficientdet_lite0” now until a better solution comes up (I’ve seen seen many comments on GitHub saying that MediaPipe should be used but I ran into even more dependency issues there).
So in case someone else wants to train an efficientdet_lite network for object detection, that’s how I set up my Conda-environment on Windows, maybe it helps (for the code itself, see tutorial here).

Start with a clean environment and install Python 3.9

Next (to prevent this issue from happening):
pip install ""cython<3.0.0"" wheel

pip install ""pyyaml==5.4.1"" --no-build-isolation


pip install tflite-model-maker

pip install pycocotools (now you would have everything installed that you need but there will be errors, which I fixed with the following steps)

pip install numpy==1.23.5 (to fix AttributeError: module 'numpy' has no attribute 'object', see here)

pip install --upgrade tensorflow-datasets==4.9.1 (to fix ImportError: cannot import name 'array_record_module' from 'array_record.python', see here)

And finally pip uninstall -y tensorflow && pip install -q tensorflow==2.8.0 (to fix  AttributeError: module 'keras.api._v2.keras.mixed_precision' has no attribute 'experimental', again see here)


After that, I had a working environment to train the models. It’s ugly, but it works for the moment. ;)
I’m still open for better solutions though...
",tensorflow
module 39tensorflow_apiv1compatv239 has no attribute 39__internal__39 google colab error,"I am running a tensorflow model on google colab. Today, I got this error:
 Using TensorFlow backend.
    Traceback (most recent call last):
      File ""train.py"", line 6, in <module>
        from yolo import create_yolov3_model, dummy_loss
      File ""/content/drive/MyDrive/yolo/yolo_plz_work/yolo.py"", line 1, in <module>
        from keras.layers import Conv2D, Input, BatchNormalization, LeakyReLU, ZeroPadding2D, UpSampling2D, Lambda
      File ""/usr/local/lib/python3.7/dist-packages/keras/__init__.py"", line 3, in <module>
        from . import utils
      File ""/usr/local/lib/python3.7/dist-packages/keras/utils/__init__.py"", line 26, in <module>
        from .vis_utils import model_to_dot
      File ""/usr/local/lib/python3.7/dist-packages/keras/utils/vis_utils.py"", line 7, in <module>
        from ..models import Model
      File ""/usr/local/lib/python3.7/dist-packages/keras/models.py"", line 10, in <module>
        from .engine.input_layer import Input
      File ""/usr/local/lib/python3.7/dist-packages/keras/engine/__init__.py"", line 3, in <module>
        from .input_layer import Input
      File ""/usr/local/lib/python3.7/dist-packages/keras/engine/input_layer.py"", line 7, in <module>
        from .base_layer import Layer
      File ""/usr/local/lib/python3.7/dist-packages/keras/engine/base_layer.py"", line 12, in <module>
        from .. import initializers
      File ""/usr/local/lib/python3.7/dist-packages/keras/initializers/__init__.py"", line 124, in <module>
        populate_deserializable_objects()
      File ""/usr/local/lib/python3.7/dist-packages/keras/initializers/__init__.py"", line 49, in populate_deserializable_objects
        LOCAL.GENERATED_WITH_V2 = tf.__internal__.tf2.enabled()
      File ""/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/util/module_wrapper.py"", line 193, in __getattr__
        attr = getattr(self._tfmw_wrapped_module, name)
    AttributeError: module 'tensorflow._api.v1.compat.v2' has no attribute '__internal__'

Previously, things had been running smoothly, so I'm not sure why this happened.
I am using Python 3.7.10, and these are the packages I am supposed to use:
absl-py==0.9.0
astor==0.8.1
gast==0.2.2
google-pasta==0.1.8
grpcio==1.26.0
h5py==2.10.0
Keras==2.3.1
Keras-Applications==1.0.8
Keras-Preprocessing==1.1.0
Markdown==3.1.1
numpy==1.18.1
opencv-contrib-python==4.1.2.30
opt-einsum==3.1.0
protobuf==3.11.2
PyYAML==5.3
scipy==1.4.1
six==1.14.0
tensorboard==1.15.0
tensorflow==1.15.0
tensorflow-estimator==1.15.1
termcolor==1.1.0
tqdm==4.41.1
Werkzeug==0.16.0
wrapt==1.11.2

Perhaps colab recently upgraded some libraries? I am sure that I followed the same installation steps as I usually do.
EDIT:
I think there may be an issue in the keras version.
Here are the first few lines of the file I am running:
from keras.layers import Conv2D, Input, BatchNormalization, LeakyReLU, ZeroPadding2D, UpSampling2D, Lambda
from keras.layers.merge import add, concatenate
from keras.models import Model
from keras.engine.topology import Layer
import tensorflow as tf

If I remove all of the lines starting with ""from keras"", I don't get the error. However, I never touched these lines before, so I don't know why they would suddenly cause an error now. Also, it is not the python version causing this error, because colab changed it to 3.7.10 in April and I had no problem.
","Try these parameters, it works with me
!pip3 uninstall keras-nightly
!pip3 uninstall -y tensorflow
!pip3 install keras==2.1.6
!pip3 install tensorflow==1.15.0
!pip3 install h5py==2.10.0

",tensorflow
how to make tensorflow use 100 of gpu,"I have a laptop that has an RTX 2060 GPU and I am using Keras and TF 2 to train an LSTM on it. I am also monitoring the gpu use by nvidia-smi and I noticed that the jupyter notebook and TF are using maximum 35% and usually the gpu is being used between 10-25%.
With current conditions, it took more than 7 hours to train this model, I want to know if I am doing something wrong or it is a limitation of Keras and TF?
My nvidia-smi output:
Sun Nov  3 00:07:37 2019       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 430.26       Driver Version: 430.26       CUDA Version: 10.2     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|===============================+======================+======================|
|   0  GeForce RTX 2060    Off  | 00000000:01:00.0  On |                  N/A |
| N/A   51C    P3    22W /  N/A |    834MiB /  5931MiB |     24%      Default |
+-------------------------------+----------------------+----------------------+

+-----------------------------------------------------------------------------+
| Processes:                                                       GPU Memory |
|  GPU       PID   Type   Process name                             Usage      |
|=============================================================================|
|    0      1032      G   /usr/lib/xorg/Xorg                           330MiB |
|    0      1251      G   /usr/bin/gnome-shell                         333MiB |
|    0      1758      G   ...equest-channel-token=622209288718607755   121MiB |
|    0      5086      G   ...uest-channel-token=12207632792533837012    47MiB |
+-----------------------------------------------------------------------------+

My LSTM:
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from tensorflow.keras.layers import LSTM
from tensorflow.keras.layers import Dropout

regressor = Sequential()

regressor.add(LSTM(units = 180, return_sequences = True, input_shape = (X_train.shape[1], 3)))
regressor.add(Dropout(0.2))

regressor.add(LSTM(units = 180, return_sequences = True))
regressor.add(Dropout(0.2))

regressor.add(LSTM(units = 180, return_sequences = True))
regressor.add(Dropout(0.2))

regressor.add(LSTM(units = 180, return_sequences = True))
regressor.add(Dropout(0.2))

regressor.add(LSTM(units = 180, return_sequences = True))
regressor.add(Dropout(0.2))

regressor.add(LSTM(units = 180))
regressor.add(Dropout(0.2))

regressor.add(Dense(units = 1))

regressor.compile(optimizer = 'adam', loss = 'mean_squared_error')

regressor.fit(X_train, y_train, epochs = 10, batch_size = 32, callbacks=[cp_callback])

","TensorFlow automatically takes care of optimizing GPU resource allocation via CUDA & cuDNN, assuming latter's properly installed. The usage statistics you're seeing are mainly that of memory/compute resource 'activity', not necessarily utility (execution); see this answer. That your utility is ""only"" 25% is a good thing - otherwise, if you substantially increased your model size (which isn't large as-is), you'd OOM.
To increase usage, increase batch size, model size, or whatever would increase the parallelism of computations; note that making the model deeper would increase GPU's memory utility, but far less so its compute-utility.
Also, consider using CuDNNLSTM instead of LSTM, which can run 10x faster and use less GPU memory (courtesy of algorithmic artisanship), but more compute-utility. Lastly, inserting Conv1D as the first layer with strides > 1 will significantly increase train speed by reducing input size, without necessarily harming performance (it can in fact improve it).

Update: overclocking the GPU is an option, but I'd advise against it as it can wear out the GPU in the long run (and all DL is ""long run""). There's also ""over-volting"" and other hardware tweaks, but all should be used for some short applications. What'll make the greatest difference is your input data pipeline.
",tensorflow
unable to download tensorflow 2150,"I'm working on this at paperspace not jupyterlab or kaggle
I can't seem to download newest version of tensorflow. For some reason giving that there is no such version of it. Any ideas
ERROR: Could not find a version that satisfies the requirement tensorflow==2.15.0 (from versions: 2.2.0, 2.2.1, 2.2.2, 2.2.3, 2.3.0, 2.3.1, 2.3.2, 2.3.3, 2.3.4, 2.4.0, 2.4.1, 2.4.2, 2.4.3, 2.4.4, 2.5.0, 2.5.1, 2.5.2, 2.5.3, 2.6.0rc0, 2.6.0rc1, 2.6.0rc2, 2.6.0, 2.6.1, 2.6.2, 2.6.3, 2.6.4, 2.6.5, 2.7.0rc0, 2.7.0rc1, 2.7.0, 2.7.1, 2.7.2, 2.7.3, 2.7.4, 2.8.0rc0, 2.8.0rc1, 2.8.0, 2.8.1, 2.8.2, 2.8.3, 2.8.4, 2.9.0rc0, 2.9.0rc1, 2.9.0rc2, 2.9.0, 2.9.1, 2.9.2, 2.9.3, 2.10.0rc0, 2.10.0rc1, 2.10.0rc2, 2.10.0rc3, 2.10.0, 2.10.1, 2.11.0rc0, 2.11.0rc1, 2.11.0rc2, 2.11.0, 2.11.1, 2.12.0rc0, 2.12.0rc1, 2.12.0, 2.12.1, 2.13.0rc0, 2.13.0rc1, 2.13.0rc2, 2.13.0, 2.13.1)
ERROR: No matching distribution found for tensorflow==2.15.0


code pic
It should have had it automatically installed like it did with jupyter notebook or kaggle but it seems to not work here on paperspace
","You are using python version 3.8 which does not have tensorflow==2.15.0
Update your python version to python 3.11.^ or just simply use pip install tensorflow to download the module which is compatible with old python version.
",tensorflow
questions about training llms on large text datasets for text generation from scratch,"I made a fully custom made GPT in Jax (with Keras 3), using Tensorflow for the data pipeline.
I've trained the model on the Shakespeare dataset and got good results (so no problem with the model).
Now I want to train it on the Tiny-Stories dataset which is pretty big with GPT of 15M parameters.
Here is the code for loading the data:
def get_dataset_lists(ds_path:str):
    dataset = open(ds_path, ""r"", encoding=""utf-8"").read() # [...]
    dataset = dataset.split(""<|endoftext|>"")
    r.shuffle(dataset)
    dataset:list = spm.Encode( # llama's sentence piece encoder
            tf.strings.strip(dataset).numpy().tolist(), 
            add_bos=True,
            add_eos=False
        ) # [[SOS story], ..., [SOS story]]
    print(""\tNumber of stories:"", len(dataset))
    return dataset

def tf_dataload(
    dataset:list,
    batch_size:int,
    maxlen:int,
    shift:int,
):
    import functools; import operator
    dataset = functools.reduce(operator.iconcat, dataset, [])
    num_tokens = len(dataset); print(""\tNumber of tokens in the dataset is"", num_tokens)
    unique_tok = set(dataset); print(""\tNumber of unique tokens in the dataset is"", len(unique_tok))
    # [SOS story ... SOS story]
    dataset = tf.data.Dataset.from_tensor_slices(dataset)
    dataset = dataset.window(maxlen+1, shift=shift, drop_remainder=True)
    # [[...], [...], [...], ...] shape(m, maxlen+1)
    dataset = dataset.flat_map(lambda window: window.batch(maxlen+1))
    dataset = dataset.shuffle(10_000*batch_size, reshuffle_each_iteration=reshuffle_each_iteration)
    # [ [ [...], [...], [...], ...], ...] shape(m//B, B, maxlen+1)
    dataset = dataset.batch(batch_size, drop_remainder=True, num_parallel_calls=tf.data.AUTOTUNE)
    dataset = dataset.shuffle(batch_size*100)
    dataset = dataset.map(lambda window: (window[:, :-1], window[:, 1:]), num_parallel_calls=tf.data.AUTOTUNE).prefetch(tf.data.AUTOTUNE)
    return dataset # (shape(m//B, B, maxlen) shape(m//B, B, maxlen))

def load_data(
    train_ds_path:str,
    val_ds_path:str,
    batch_size:int,
    maxlen:int,
    shift:int,
):  
    print(""Training Dataset:"")
    train_ds = tf_dataload(get_dataset_lists(train_ds_path), batch_size, maxlen, shift, reshuffle_each_iteration=True)
    print(""Validation Dataset:"")
    val_ds = tf_dataload(get_dataset_lists(val_ds_path), batch_size, maxlen, shift, reshuffle_each_iteration=True)
    print(f""\n{train_ds}\n{val_ds}"")
    datasets = {""train"": train_ds.repeat(), ""val"":val_ds}
    return datasets


I've certain questions regarding
the value of the shift?
First I set it equal to 1, but the training was very slow, even after 100000 steps it didn't converge even though it was decreasing slowly (I think there's no problem with the learning rate as I plotted Loss Vs Lr and selected the max learning rate possible and used cosine decay with warmup)


So I looked into Karpathy's llama-2 repo and the shift was equal to maxlen.
So I set it equal to maxlen and trained it for 100000 steps but the model is learning very slowly, and didn't get a loss even close to what Karpathy got
(I don't know what's the problem, as I've closely followed Karpathy's llama2 repo)
What is shift generally equal to when pre-training an LLM on Language Modelling?
Shouldn't it be 1, because the transformer model is not positionally invariant, and it would affect model performance if shift is not equal to 1? But then the number of samples will be very large...?

And for what number of steps to train a LLM given the number of tokens

You may find the below helpful...
@dataclass
class GPTArgs:
    """"""GPT Configuration""""""
    d_model:int = 288
    num_layers:int = 6
    num_heads:int = 6
    max_context_length:int = 256
    vocab_size:int = VOCAB_SIZE # 32K
    output_units:int = None # equal to vocab_size if None in model init  
    assert d_model % 2 == 0
    assert d_model % num_heads == 0
    dropout_rate:float = 0.1

@dataclass
class TArgs:
    # lr scheduler
    init_lr:float = 1e-7
    max_lr:float = 6.5e-4
    min_lr:float = 0.1*max_lr # The factor is usually 0.1 or 0.0
    num_steps:int = 100_000
    warmup_steps:int = 1000 # 1000, to make training more stable instead of 2000
    decay_steps:int = num_steps

    # optimizer
    beta1:float = 0.9
    beta2:float = 0.95
    weight_decay:float = 1e-1
    clipvalue:float = 1e0
    num_grad_accumalation_steps:int = 4
    # num_tok_per_update = batch_size * maxlen * gradient_accumalation = 128 * 256 * 4 = 131_072

    # training
    checkpoint:str = 'weights/GPTstories/Epoch{epoch}.weights.h5'
    train_ds_path:str = ""TinyStoriesDataset/TinyStories-train.txt""
    val_ds_path:str = ""TinyStoriesDataset/TinyStories-valid.txt""
    steps_per_epoch = eval_freq = 2000
    eval_steps:int = 200
    batch_size:int = 128 
    patience:int = 10 # early stopping with restore best weights

Update 1:
I thought that the model wasn't getting the training samples uniformly so I modified the data pipeline and also increased the number of steps to 200,000.
But there were no significant improvements. The training was still very slow by the end and loss was decreasing by 0.01 every epoch (of 2000 steps)... Got a loss of 1.67 on validation set
def pretokenize_and_save_dataset(dataset_path:str, num_shards:int, shard_dir:str):
    dataset = open(dataset_path, ""r"", encoding=""utf-8"").read() # [...]
    dataset = dataset.split(""<|endoftext|>"")
    r.shuffle(dataset)
    dataset:list = spm.Encode(
            tf.strings.strip(dataset).numpy().tolist(), 
            add_bos=True,
            add_eos=False
        ) # [[SOS story], ..., [SOS story]]
    print(""Dataset:"")
    print(""\tNumber of stories:"", len(dataset))

    # flatten
    dataset = functools.reduce(operator.iconcat, dataset, [])
    num_tokens = len(dataset); print(""\tNumber of tokens in the dataset:"", num_tokens)
    print(""\tNumber of unique tokens in the dataset:"", len(set(dataset)))
    
    dataset = np.asarray(dataset, dtype=np.uint16) # [SOS story ... SOS story]
    print(""\tAvg length of story:"", num_tokens/((dataset==1).sum()))

    # shard and save dataset
    sharded_datasets_list = np.array_split(dataset, num_shards) # [[SOS story...], [...], [...], ...]
    filenames = [os.path.join(shard_dir, f""shard{i+1}.npy"") for i in range(num_shards)]
    
    for filename, sharded_ds in zip(filenames, sharded_datasets_list):
        with open(filename, ""wb"") as f:
            np.save(f, sharded_ds)
    return filenames

def load_data_as_tfds(
    dataset:np.ndarray,
    maxlen:int,
    shift:int,
):
    # [SOS story ... SOS story]
    dataset = tf.data.Dataset.from_tensor_slices(dataset.tolist())
    dataset = dataset.window(maxlen+1, shift=shift, drop_remainder=True)
    # [[...], [...], [...], ...] shape(m, maxlen+1)
    dataset = dataset.flat_map(lambda window: window.batch(maxlen+1))
    dataset = dataset.shuffle(10_000*128)
    return dataset

def batch_tfds(
        dataset:tf.data.Dataset,
        batch_size:int,
):
    dataset = dataset.batch(batch_size, drop_remainder=True, num_parallel_calls=tf.data.AUTOTUNE)
    dataset = dataset.shuffle(batch_size*1000)
    dataset = dataset.map(lambda window: (window[:, :-1], window[:, 1:]), num_parallel_calls=tf.data.AUTOTUNE)
    dataset = dataset.repeat().prefetch(tf.data.AUTOTUNE)
    return dataset

def load_data(
    dataset_path:str,
    batch_size:int,
    maxlen:int,
    shift:int,
    num_shards:int,
    shard_dir:str
):  
    if os.path.exists(shard_dir) and os.listdir(shard_dir):
        filenames = glob.glob(os.path.join(shard_dir, ""*.npy""))
    else:
        os.makedirs(shard_dir)
        filenames = pretokenize_and_save_dataset(dataset_path, num_shards=num_shards, shard_dir=shard_dir)
    r.shuffle(filenames)
    to_tfds = lambda dataset: load_data_as_tfds(dataset, maxlen=maxlen, shift=shift)
    num_train_shards = round(0.9651*num_shards)
    num_val_shards = num_shards-num_train_shards

    print(""Training Dataset:"")
    print(f""\tNumber of files taken for training: {num_train_shards}/{num_shards}"")
    train_datasets_lists = [to_tfds(np.load(filename)) for filename in filenames[:num_train_shards]]
    train_ds = tf.data.Dataset.sample_from_datasets(train_datasets_lists, weights=[1/num_train_shards]*num_train_shards)
    # [ [ [...], [...], [...], ...], ...] shape(m//B, B, maxlen+1)
    train_ds = batch_tfds(train_ds, batch_size=batch_size)

    print(""Validation Dataset:"")
    print(f""\tNumber of files taken for validation: {num_val_shards}/{num_shards}"")
    val_datasets_lists = [to_tfds(np.load(filename)) for filename in filenames[num_train_shards:]]
    val_ds = tf.data.Dataset.sample_from_datasets(val_datasets_lists, weights=[1/num_val_shards]*num_val_shards)
    # [ [ [...], [...], [...], ...], ...] shape(m//B, B, maxlen+1)
    val_ds = batch_tfds(val_ds, batch_size=batch_size)

    print(f""\n{train_ds}\n{val_ds}"")
    datasets = {""train"": train_ds, ""val"":val_ds}
    return datasets

","Replaced keras's gradient accumulation argument in AdamW with a custom implementation like in karpathy's and now the loss is decreasing faster.
If you are using keras's num_grad_accum, increase num_steps to
num_steps *= num_grad_accum
",tensorflow
integrating numericalphysical data for cnn image classification,"I am attempting to use a CNN to classify medical images in python using keras. These medical images also include textual information such as age and gender that can influence the decision of the model. How can I train a CNN that can train using both the images and the real world information so that it can make classifications base of both?
","There are a couple of possibilities that I can think of off the type of my head, but the simplest is to extract some features from the medical images with a CNN, then flatten the result of the CNN, and concatenate the non-image data. Here is an idea supposing you have 512x512 images and 10 classes. This is the functional API which allows you to have multiple inputs.
import tensorflow as tf
import numpy as np

num_classes = 10

H,W = 512, 512
# Define inputs with their shapes
imgs = tf.keras.Input((H,W,3), dtype = tf.float32)
genders = tf.keras.Input(1, dtype = tf.float32)
ages = tf.keras.Input(1, dtype = tf.float32)

# Extract image features
features = tf.keras.layers.Conv2D(64, 4, strides = 4, activation = 'relu')(imgs)
features = tf.keras.layers.MaxPooling2D()(features)
features = tf.keras.layers.Conv2D(128,3, strides = 2, activation = 'relu')(features)
features = tf.keras.layers.MaxPooling2D()(features)
features = tf.keras.layers.Conv2D(256, 3, strides = 2, activation = 'relu')(features)
features = tf.keras.layers.Conv2D(512, 3, strides = 2, activation = 'relu')(features)

# #Flatten output
flat_features = tf.keras.layers.Flatten()(features)

#Concatenate gender and age
flat_features = tf.concat([flat_features, genders, ages], -1)

# Downsample
xx = tf.keras.layers.Dense(2048, activation = 'relu')(flat_features)
xx = tf.keras.layers.Dense(1024, activation = 'relu')(xx)
xx = tf.keras.layers.Dense(512, activation = 'relu')(xx)

#Calculate probabilities for each class
logits = tf.keras.layers.Dense(num_classes)(xx)
probs = tf.keras.layers.Softmax()(logits)

model = tf.keras.Model(inputs = [imgs, genders, ages], outputs = probs)

model.summary()

This architecture is not especially standard, and you might want to make the decoder deeper and/or decrease the number of parameters in the CNN encoder.
",tensorflow
creating an ai chatbot but getting a traceback error,"I'm trying to create an ai chatbox in python. I tried following this tutorial: https://techwithtim.net/tutorials/ai-chatbot/part-1/ but I'm getting a lot of errors of deprecations and getting some Traceback error.
Here's the code:
import json
import random
import tensorflow
import tflearn
import numpy
import sys
import pickle
import nltk
from nltk.stem.lancaster import LancasterStemmer
stemmer = LancasterStemmer()
nltk.download('punkt')


with open(""trainingData.json"") as file:
    data = json.load(file)

try:
    with open(""data.pickle"", ""rb"") as f:
        words, labels, training, output = pickle.load(f)
except:
    words = []
    labels = []
    docs_x = []
    docs_y = []

    for intent in data[""intents""]:
        for pattern in intent[""patterns""]:
            wrds = nltk.word_tokenize(pattern)
            words.extend(wrds)
            docs_x.append(wrds)
            docs_y.append(intent[""tag""])

        if intent[""tag""] not in labels:
            labels.append(intent[""tag""])

    words = [stemmer.stem(w.lower()) for w in words if w != ""?""]
    words = sorted(list(set(words)))

    labels = sorted(labels)

    training = []
    output = []

    out_empty = [0 for _ in range(len(labels))]

    for x, doc in enumerate(docs_x):
        bag = []

        wrds = [stemmer.stem(w.lower()) for w in doc]

        for w in words:
            if w in wrds:
                bag.append(1)
            else:
                bag.append(0)

        output_row = out_empty[:]
        output_row[labels.index(docs_y[x])] = 1

        training.append(bag)
        output.append(output_row)

    training = numpy.array(training)
    output = numpy.array(output)

    with open(""data.pickle"", ""wb"") as f:
        pickle.dump((words, labels, training, output), f)

tensorflow.reset_default_graph()

net = tflearn.input_data(shape=[None, len(training[0])])
net = tflearn.fully_connected(net, 8)
net = tflearn.fully_connected(net, 8)
net = tflearn.fully_connected(net, len(output[0]), activation=""softmax"")
net = tflearn.regression(net)

model = tflearn.DNN(net)

try:
    model.load(""model.tflearn"")
except:
    model.fit(training, output, n_epoch=1000, batch_size=8, show_metric=True)
    model.save(""model.tflearn"")


def bag_of_words(s, words):
    bag = [0 for _ in range(len(words))]

    s_words = nltk.word_tokenize(s)
    s_words = [stemmer.stem(word.lower()) for word in s_words]

    for se in s_words:
        for i, w in enumerate(words):
            if w == se:
                bag[i] = 1

    return numpy.array(bag)


def chat():
    print(""Start talking with the bot (type quit to stop)!"")
    while True:
        inp = input(""You: "")
        if inp.lower() == ""quit"":
            break

        results = model.predict([bag_of_words(inp, words)])
        results_index = numpy.argmax(results)
        tag = labels[results_index]

        for tg in data[""intents""]:
            if tg['tag'] == tag:
                responses = tg['responses']

        print(random.choice(responses))

chat()

Here are the errors I'm getting. How can I fix the deprecation errors, the traceback error?

Here's the text of the error:
Run id: VOB3W4
Log directory: /tmp/tflearn_logs/
---------------------------------
Training samples: 20
Validation samples: 0
--
--
Traceback (most recent call last):
  File ""script.py"", line 91, in <module>
    model.fit(training, output, n_epoch=1000, batch_size=8, show_metric=True)
  File ""/usr/local/lib/python2.7/site-packages/tflearn/models/dnn.py"", line 216, in fit
    callbacks=callbacks)
  File ""/usr/local/lib/python2.7/site-packages/tflearn/helpers/trainer.py"", line 339, in fit
    show_metric)
  File ""/usr/local/lib/python2.7/site-packages/tflearn/helpers/trainer.py"", line 816, in _train
    tflearn.is_training(True, session=self.session)
  File ""/usr/local/lib/python2.7/site-packages/tflearn/config.py"", line 95, in is_training
    tf.get_collection('is_training_ops')[0].eval(session=session)
  File ""/usr/local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py"", line 731, in eval
    return _eval_using_default_session(self, feed_dict, self.graph, session)
  File ""/usr/local/lib/python2.7/site-packages/tensorflow/python/framework/ops.py"", line 5579, in _eval_using_default_session
    return session.run(tensors, feed_dict)
  File ""/usr/local/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 950, in run
    run_metadata_ptr)
  File ""/usr/local/lib/python2.7/site-packages/tensorflow/python/client/session.py"", line 1096, in _run
    raise RuntimeError('Attempted to use a closed Session.')
RuntimeError: Attempted to use a closed Session.

","At start file ""model.tflearn"" doesn't exist and try/except should catch error when code try to load this file and run fit() and save() 
try:
    model.load(""model.tflearn"")
except:
    model.fit(training, output, n_epoch=1000, batch_size=8, show_metric=True)
    model.save(""model.tflearn"")

but it seems this error closes tf.session() so it can't run fit()correctly.
If you remove try/except with load() and keep only fit() and save() then it has no problem to create model and save it in file.
model.fit(training, output, n_epoch=1000, batch_size=8, show_metric=True)
model.save(""model.tflearn"")

After creating file ""model.ftlearn"" you can use again try/except with load() and it should work if you don't delete file with model.

Better solution should check if file exists - but it saves data in few files ""model.tflearn.index"", ""model.tflearn.meta"" and ""model.tflearn.data-00000-of-00001"" so it should check one of this file instead of ""model.tflearn""
Use 
import os

if os.path.exists(""model.tflearn.meta""):
    model.load(""model.tflearn"")
else:
    model.fit(training, output, n_epoch=1000, batch_size=8, show_metric=True)
    model.save(""model.tflearn"")

instead of 
try:
    model.load(""model.tflearn"")
except:
    model.fit(training, output, n_epoch=1000, batch_size=8, show_metric=True)
    model.save(""model.tflearn"")


EDIT: It seems this problem exists at least 2 years: RuntimeError: Attempted to use a closed Session in tflearn
",tensorflow
how to mask inputs with variable size in transformer model when the batches needs to be masked differently,"I'm making a transformer using tensorflow.keras and having issues understanding how the attention_mask works for a MultiHeadAttention layer.
My input is 3-dimensional data. For example, let's assume my whole dataset has 10 elements, each one with length no more than 4:
# whole data
[
  # first item
  [
    [     1,      2,      3],
    [     1,      2,      3],
    [np.nan, np.nan, np.nan],
    [np.nan, np.nan, np.nan],
  ],
  # second item
  [
    [     1,      2,      3],
    [     5,      8,      2],
    [     3,      7,      8],
    [     4,      6,      2],
  ],
  ... # 8 more items
]

So, my mask looks like:
# assume this is a numpy array
mask = [
  [
    [1, 1, 1],
    [1, 1, 1],
    [0, 0, 0],
    [0, 0, 0],
  ],
  [
    [1, 1, 1],
    [1, 1, 1],
    [1, 1, 1],
    [1, 1, 1],
  ],
  ...
]

So the shape of the mask til now is [10, 4, 3]. Let's say I use batch_size = 5. Now, according documentation, attention_mask shape should be [B, T, S] (batch_size, query_size, key_size). In the example case should be [5, 4, 4]?
Question
If the mask is calculated only once, what 5 items should I give as a mask? This sounds counterintuitive to me. How should I build the mask?
According this answer, head_size should be also taken in account, so they also do:
mask = mask[:, tf.newaxis, tf.newaxis, :]

What I've tested
The only time I manage to run the transformer successfully using the attention_mask is when I do:
mask = np.ones((batch_size, data.shape[1], data.shape[2]))
mask = mask[:, tf.newaxis, tf.newaxis, :]

Obviously that mask makes no sense, because it is all ones, but it was just to test if it had the correct shape.
The model
I'm using practically the same code from the keras example transformer for time series classification
def transformer_encoder(inputs, head_size, num_heads, ff_dim, dropout=0.0, mask=None):
    # Normalization and Attention
    x = layers.LayerNormalization(epsilon=1e-6)(inputs)
    x = layers.MultiHeadAttention(
        key_dim=head_size, num_heads=num_heads, dropout=dropout
    )(x, x, attention_mask=mask)
    x = layers.Dropout(dropout)(x)
    res = x + inputs

    # Feed Forward Part
    x = layers.LayerNormalization(epsilon=1e-6)(res)
    x = layers.Conv1D(filters=ff_dim, kernel_size=1, activation=""relu"")(x)
    x = layers.Dropout(dropout)(x)
    x = layers.Conv1D(filters=inputs.shape[-1], kernel_size=1)(x)
    return x + res


def build_model(
    n_classes,
    input_shape,
    head_size,
    num_heads,
    ff_dim,
    num_transformer_blocks,
    mlp_units,
    dropout=0.0,
    mlp_dropout=0.0,
    input_mask=None,
) -> keras.Model:
    inputs = keras.Input(shape=input_shape)
    x = inputs
    for _ in range(num_transformer_blocks):
        x = transformer_encoder(x, head_size, num_heads, ff_dim, dropout, input_mask)

    x = layers.GlobalAveragePooling2D(data_format=""channels_first"")(x)
    for dim in mlp_units:
        x = layers.Dense(dim, activation=""relu"")(x)
        x = layers.Dropout(mlp_dropout)(x)
    outputs = layers.Dense(n_classes, activation=""softmax"")(x)
    return keras.Model(inputs, outputs)

","After a little research and seeing several transformer model examples this is what solved the problem for me.

Create a custom TransformerBlock layer that supports masking
Add a mask parameter in the call method of the TransformerBlock and reshape it there.
Add a Masking layer before the TransformerBlock

Code:
class TransformerBlock(layers.Layer):
    def __init__(self, head_size, num_heads, ff_dim, ff_dim2, rate=0.1):
        super().__init__()
        self.att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=head_size)
        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)
        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)
        self.dropout1 = layers.Dropout(rate)
        self.dropout2 = layers.Dropout(rate)
        self.conv1 = layers.Conv1D(filters=ff_dim, kernel_size=1, activation=""relu"")
        self.conv2 = layers.Conv1D(filters=ff_dim2, kernel_size=1)
        self.supports_masking = True

    def call(self, inputs, training, mask=None):
        padding_mask = None
        if mask is not None:
            padding_mask = tf.cast(mask[:, tf.newaxis, tf.newaxis, :], dtype=""int32"")

        out_norm1 = self.layernorm1(inputs, training=training)
        out_att = self.att(
            out_norm1, out_norm1, training=training, attention_mask=padding_mask
        )
        out_drop1 = self.dropout1(out_att, training=training)
        res = out_drop1 + inputs
        out_norm2 = self.layernorm2(res, training=training)
        out_conv1 = self.conv1(out_norm2, training=training)
        out_drop2 = self.dropout2(out_conv1, training=training)
        out_conv2 = self.conv2(out_drop2, training=training)
        return out_conv2 + res

def build_model(
    n_classes,
    input_shape,
    head_size,
    num_heads,
    ff_dim,
    num_transformer_blocks,
    mlp_units,
    dropout=0.0,
    mlp_dropout=0.0,
    mask=None,
) -> keras.Model:
    inputs = keras.Input(shape=input_shape)
    _x = inputs
    if mask is not None:
        _x = layers.Masking(mask_value=mask)(_x)
    for _ in range(num_transformer_blocks):
        _x = TransformerBlock(
            head_size,
            num_heads,
            ff_dim,
            inputs.shape[-1],
            dropout,
        )(_x)

    _x = layers.GlobalAveragePooling2D(data_format=""channels_first"")(_x)
    for dim in mlp_units:
        _x = layers.Dense(dim, activation=""relu"")(_x)
        _x = layers.Dropout(mlp_dropout)(_x)
    outputs = layers.Dense(n_classes, activation=""softmax"")(_x)
    return keras.Model(inputs, outputs)

",tensorflow
give a list of image path pairs to load using tfdatadatasetmap but it seems to only read 2,"I am learning how to use Tensorflow with a piece of old code provided at a workshop a couple of years back (i.e. it should've been tested and works). However it didn't work, and my investigations led me back to step 1 of loading the data and making sure it has loaded properly.
The data reading pipeline is as follows:

The load function

@tf.function
def load(path_pair):
    image_path = path_pair[0]
    masks_path = path_pair[1]

    image_raw = tf.io.read_file(image_path)
    image = tf.io.decode_image(
        image_raw, channels=1, dtype=tf.uint8
    )

    masks_raw = tf.io.read_file(masks_path)
    masks = tf.io.decode_image(
        masks_raw, channels=NUM_CONTOURS, dtype=tf.uint8
    )

    return image / 255, masks / 255```
2. The function used to create the dataset
```def create_datasets(dataset_type):
    path_pairs = get_path_pairs(dataset_type) # this just gives a list of 2 x 2 tuples containing the image/mask path to load
    dataset = tf.data.Dataset.from_tensor_slices(path_pairs)
    dataset = dataset.shuffle(
        len(path_pairs),
        reshuffle_each_iteration=True,
    )
    dataset = dataset.map(load)

    dataset = dataset.batch(BATCH_SIZE)
    dataset = dataset.prefetch(tf.data.AUTOTUNE)

    return dataset```



When I use the create_datasets function on a dataset that contains 818 data pairs and check the size of the loaded dataset using len(dataset)it tells me there is only 2 items loaded. 

","The problem is, that you are batching your dataset, thus when you use len(dataset), you get the number of batches, not the number of elements in your dataset.
To get them you can, for instance, iterate over your batches:
num_samples = 0
for batch in dataset:
    num_samples += len(batch[0])
print(num_samples)

",tensorflow
compute hessian matrix only diagonal part with respect to a high rank tensor,"I would like to compute the first and the second derivatives (diagonal part of Hessian) of my specified Loss with respect to each feature map of a vgg16 conv4_3 layer's kernel which is a 3x3x512x512 dimensional matrix. I know how to compute derivatives if it is respected to a low-rank one according to How to compute all second derivatives (only the diagonal of the Hessian matrix) in Tensorflow?
However, when it turns to higher-rank, I got completed lost.
# Inspecting variables under Ipython notebook
In  : Loss 
Out : <tf.Tensor 'local/total_losses:0' shape=() dtype=float32>

In  : conv4_3_kernel.get_shape() 
Out : TensorShape([Dimension(3), Dimension(3), Dimension(512), Dimension(512)])

## Compute derivatives
Grad = tf.compute_gradients(Loss, conv4_3_kernel)
Hessian = tf.compute_gradients(Grad, conv4_3_kernel)

In  : Grad 
Out : [<tf.Tensor 'gradients/vgg/conv4_3/Conv2D_grad/Conv2DBackpropFilter:0' shape=(3, 3, 512, 512) dtype=float32>]

In  : Hessian 
Out : [<tf.Tensor 'gradients_2/vgg/conv4_3/Conv2D_grad/Conv2DBackpropFilter:0' shape=(3, 3, 512, 512) dtype=float32>]

Please help me to check my understandings. So, for conv4_3_kernel, each dim stand for [Kx, Ky, in_channels, out_channels], so Grad should be partial derivatives of Loss with respect to each element(pixel) in the each feature maps. And Hessian is the second derivatives.
But, Hessian computes all the derivatives, how can I only compute only the diagonal part? should I use tf.diag_part()?
","tf.compute_gradients computes derivative of a scalar quantity. If the quantity provided isn't scalar, it turns it into scalar by summing up the components which is what's happening in your example
To compute full Hessian you need n calls to tf.gradients, The example is here. If you want just the diagonal part, then modify arguments to ith call to tf.gradients to differentiate with respect to ith variable, rather than all variables.
",tensorflow
batch normalisation during testing,"I am working on a 2d time series problem with vector size 140*6 for binary classification using CNN. I have not used any scaling and normalising techniques instead directly fed data to CNN with 3 hidden layers and Batch Normalisation layers with batch size 256 during training .Since I have to test it at real time as well with batch size 1 how would batch Normalisation work then having not calculated any mean or std deviation for any training layer.And also should batch normalisation later be used for forward pass during final testing or the mean and std deviation only should be calculated for training layers and used.
","Batch normalization is not used during testing. The reason for that being is batch normalization is used to alleviate the problem of covariance shift between different batches in training data. The covariance shift leads to bad models getting trained, thus, we use it. It has no role to play during testing.
And if you have used batch normalization with batch size 1, then, that is simply instance normalization.
",tensorflow
importerror libcudaso1 cannot open shared object file,"When I run my code with TensorFlow directly, everything is normal.
However, when I run it in a screen window, I get the following error.
ImportError: libcuda.so.1: cannot open shared object file: No such file or directory

I have tried the command:
source /etc/profile

But it doesn't work.
Cause I use ssh to connect to the servers, the screen is necessary.
How can I fix it?
","Try to put libcuda.so.1 path to LD_LIBRARY_PATH environment variable.
For example, if your libcuda.so.1 location is /usr/lib64/nvidia/libcuda.so.1 (source: GitHub issue), you need this:
export LD_LIBRARY_PATH=/usr/lib64/nvidia/:$LD_LIBRARY_PATH

",tensorflow
how to have untracked weights in custom keras layer,"I would like to create a custom keras layer (a codebook for a VQVAE model.)  While training I would like to have a tf.Variable which tracks the usage of each code so I can restart unused codes.  So I created my Codebook layer as follows...
class Codebook(layers.Layer): 
     def __init__(self, num_codes, code_reset_limit = None, **kwargs): 
         super().__init__(**kwargs) 
         self.num_codes = num_codes 
         self.code_reset_limit = code_reset_limit 
         if self.code_reset_limit: 
             self.code_counter = tf.Variable(tf.zeros(num_codes, dtype = tf.int32), trainable = False) 
     def build(self, input_shape): 
         self.codes = self.add_weight(name = 'codes',  
                                      shape = (self.num_codes, input_shape[-1]), 
                                      initializer = 'random_uniform',  
                                      trainable = True) 
         super().build(input_shape) 
                                                                                                             

The issue I have is that the Layer class finds the member variable self.code_counter and adds it to the list of weights which are saved with the layer.  It also expects the self.code_counter to be present when weights are loaded which is not the case when I run in inference mode.  How can I make it so keras does not track a variable in my layer.  I do not want it persisted or to be part of the layers.weights.
","I am a bit late with the answer, but I had the same problem and came across the question without an answer. Now, I have found an answer that works for Keras 2 and Keras 3, so I am sharing it here for others encountering the same question.
To prevent TensorFlow and Keras from tracking variables one needs to encapsulate the variable in a class that TensorFlow and Keras do not handle in the tracking module. The list of classes that are automatically tracked for Keras 3 are: keras.Variable, list, dict, tuple, and NamedTuple
(see here). For Keras 2 the list of objects is not so easy to find but appears to include tf.Variable (see the present question), dict, and list.
The solution that did work in my context for keras.Variable and tf.Variable is to create dataclass encapsulating the Variable. Here the setup for tensorflow and keras 2.
import tensorflow as tf
from dataclasses import dataclass

@dataclass
class DoNotTrackContainer:
    data: tf.Variable


In the code of the present question, this would then be used like this
 if self.code_reset_limit: 
     self.code_counter = DoNotTrackContainer(data=tf.Variable(tf.zeros(num_codes, dtype = tf.int32), trainable = False) )

When accessing the counter the data attribute needs to be included in the path
  # for accessing the counter
  self.code_counter.data.assign_add(1) 

For Keras 3 the Container becomes
import keras
from dataclasses import dataclass

@dataclass
class DoNotTrackContainer:
    data: keras.Variable


",tensorflow
cannot augment images with tensorflowkeraspreprocessingimage import imagedatagenerator,"I want to make an augmented version of my dataset. The dataset was manually labelled and downloaded from this RIWA dataset.
I create class with this code
source_dir = r'./river-water-segmentation-dataset/riwa_v2'
subdir = os.listdir(source_dir)

filepaths = []
labels = []

for i in subdir:
    classpath = os.path.join(source_dir, i)

    if os.path.isdir(classpath):
        file_list = os.listdir(classpath)
        for f in file_list:
            file_path = os.path.join(classpath, f)
            filepaths.append(file_path)
            labels.append(i)
paths = pd.Series(filepaths, name='paths')
labels = pd.Series(labels, name='labels')

df = pd.concat([paths, labels], axis=1)

print(df.head())
print(""========================"")
print(df['labels'].value_counts())
print(""========================="")
print('Total data: ', len(df))

Then make them to 700 each for starter, might increase later for bigger dataset
sample_list = []
max_size = 1500# TODO: change this value

grouping = df.groupby('labels')

for label in df['labels'].unique():
    group = grouping.get_group(label)
    group_size = len(group)

    if group_size > max_size:
        samples = group.sample(max_size, replace=False, weights=None, axis=0).reset_index(drop=True)
    else:
        samples = group.sample(frac=1.0, replace=False, axis=0).reset_index(drop=True)
    sample_list.append(samples)

df = pd.concat(sample_list, axis=0).reset_index(drop=True)
print(df['labels'].value_counts())
print('Total data: ', len(df))

From there I create augmented dataset with these
import os
import shutil
from tensorflow.keras.preprocessing.image import ImageDataGenerator

working_dir = r'./river-water-segmentation-dataset/riwa_v2/cropped'

aug_dir = os.path.join(working_dir, 'aug')
if os.path.isdir(aug_dir):
    shutil.rmtree(aug_dir)
os.mkdir(aug_dir)
for label in df['labels'].unique():
    dir_path=os.path.join(aug_dir, label)
    os.mkdir(dir_path)
print(os.listdir(aug_dir))

target = 700 # set the target count for each class in df
gen = ImageDataGenerator(
    rotation_range = 90,
    horizontal_flip = True,
    vertical_flip = True,
)

grouping = df.groupby('labels') # group by class


for label in df['labels'].unique(): # for every class
    group = grouping.get_group(label) # a dataframe holding only rows with the specificied label
    sample_count = len(group) # determine how many samples there are in this class
    # if group.empty:
    #     print(f""No images found for label '{label}'. Skipping augmentation."")
    #     continue
    if sample_count < target: # if the class has less than target number of images
        aug_img_count = 0
        delta = target - sample_count # number of augmented images to create
        target_dir = os.path.join(aug_dir, label) # define where to write the images

        aug_gen = gen.flow_from_dataframe(
            group,
            x_col = 'paths',
            y_col = None,
            target_size = (1420, 1080), # change this target size based on transfer learning model
            class_mode = None,
            batch_size = 1,
            shuffle = False,
            save_to_dir = target_dir,
            save_prefix = 'aug-',
            save_format='jpg'
        )
        images = next(aug_gen)  # Try fetching a batch
        print(f""Generated {len(images)} images."")

        while aug_img_count < delta:
            images = next(aug_gen)
            aug_img_count += len(images)
            

At first the import from tensorflow.keras.preprocessing.image import ImageDataGenerator is actually moved from from tensorflow.preprocessing.image import ImageDataGenerator from other answer I cannot find anymore because for this version of keras it was moved to this tensorflow.keras.preprocessing.image import route.
Ran the code over 10 minutes and still Found 0 validated image filenames as a result, is there something I did wrong? Is this because I download cpu version of tensorflow?
Edit 1: I did think it was because the size problem so I crop all dataset images to the same size and the code still doesn't work.
","The problem lies with the Riwa dataset, which contains images and masks in
separate folders. I moved these two folders (images and masks) under the train
folder, so that the train folder now contains both images and masks. After
using the ImageDataGenerator.flow_from_directory method, I made these
adjustments, and the code works . Please refer to this gist
",tensorflow
how can i return a numpy array using fastapi,"I have a TensorFlow Keras deep learning model in the form of an h5 file.
How can I upload an image and return a NumPy array in FastAPI?
import numpy as np
import cv2
from fastapi import FastAPI, File, UploadFile
import numpy as np
from tensorflow.keras.models import load_model
import tensorflow as tf

model=load_model(""complete_model.h5"")
app = FastAPI()

def prepare(image):
    IMG_SIZE = 224
    new_array = cv2.resize(image, (IMG_SIZE, IMG_SIZE))
    return new_array.reshape(-1, IMG_SIZE,IMG_SIZE,3)

@app.post(""/"")
async def root(file: UploadFile = File(...)):
    global model
    content = await file.read()
    nparr = np.fromstring(content, np.uint8)
    img = cv2.imdecode(nparr, cv2.IMREAD_COLOR).astype(np.float32)
    prediction = model.predict(prepare(img))
    return prediction

When uploading the image using Swagger UI, I get the following error:
line 137, in jsonable_encoder
data = dict(obj)
TypeError: 'numpy.float32' object is not iterable

Working code without FastAPI:
import numpy as np
import numpy as np
from tensorflow.keras.models import load_model
import tensorflow as tf
import cv2

model=load_model(""complete_model.h5"")

def prepare(image):
    IMG_SIZE = 224
    new_array = cv2.resize(image, (IMG_SIZE, IMG_SIZE))
    return new_array.reshape(-1, IMG_SIZE,IMG_SIZE,3)

img = cv2.imread(""./test.jpeg"").astype(np.float32)
prediction = model.predict(prepare(img))
print(prediction)

Result in the terminal:
[[0.25442022 0.74557984]]

How can I get the same result while using FastAPI?
","The error is thrown when returning the response (i.e., prediction in your case) from your endpoint. It looks like FastAPI is trying to convert the NumPy array into a dict, using the jsonable_encoder, which is used internally by FastAPI when returning a value from an endpoint, and which seems to call Python's vars() method, as shown in the error you provided here (have a look at the discussion here, as well as the documentation). Thus, what you could do is to convert the NumPy array into a Python list, then serialize it into a JSON string and return it:
return json.dumps(prediction.tolist())

Note that instead of returning the JSON string in the way it is shown above, which would casue FastAPI to serialize it again behind the scenes, you might consider returning a custom Response directly, as demonstrated here, as well as here and here.
On Swagger UI /docs, you should still be able to see the expected result. However, if you needed to convert it back to a NumPy array, you could parse the JSON string in Python, as shown below.
arr = np.asarray(json.loads(resp.json()))  # resp.json() if using Python requests

If you would like to return the NumPy array as raw bytes and display the image in the browser or download it, have a look at this answer.
",tensorflow
how can i convert the datatype of a numpy array sourced from an awkward array,"I have a numpy array I converted from awkward array by to_numpy() function, and the resulting array has the datatype: dtype=[('phi', '<f8'), ('eta', '<f8')]). I want to make it a regular tuple of (float32, float32) because otherwise this does not convert into a tensorflow tensor
I tried the regular asdtype functions but all I get is errors
>>> array = ak.Array([{""phi"": 1.1, ""eta"": 2.2}, {""phi"": 3.3, ""eta"": 4.4}])
>>> ak.to_numpy(array)
array([(1.1, 2.2), (3.3, 4.4)], dtype=[('phi', '<f8'), ('eta', '<f8')])

","I believe your problem is equivalent to this: you have some Awkward Array with record structure,
>>> array = ak.Array([{""phi"": 1.1, ""eta"": 2.2}, {""phi"": 3.3, ""eta"": 4.4}])

and when you convert that with ak.to_numpy, it turns the record fields into NumPy structured array fields:
>>> ak.to_numpy(array)
array([(1.1, 2.2), (3.3, 4.4)], dtype=[('phi', '<f8'), ('eta', '<f8')])

ML libraries like TensorFlow and PyTorch want the feature vectors to not have fields with names, but instead be 2D arrays in which the second dimension ranges over all of the features. If all of the NumPy structured array dtypes are identical, as they're all <f8 in this example, you could view it:
>>> ak.to_numpy(array).view(""<f8"").reshape(len(array), -1)
array([[1.1, 2.2],
       [3.3, 4.4]])

But this is unsafe. If, for example, some of your fields are 32-bit and others are 64-bit, or some are integers and others are floating-point, view will just reinterpret the memory, losing the meaning of the numbers:
>>> bad = np.array([(1, 2, 3.3), (4, 5, 6.6)], dtype=[(""x"", ""<i4""), (""y"", ""<i4""), (""z"", ""<f8"")])
>>> bad.view(""<f8"").reshape(len(bad), -1)
array([[4.24399158e-314, 3.30000000e+000],
       [1.06099790e-313, 6.60000000e+000]])

(z's 3.3 and 6.6 are preserved, but x and y get merged into a single field and the raw memory gets interpreted as floats.)
Instead, we should make the structure appropriate in Awkward, which has the tools to do exactly this sort of thing, and afterward convert it to NumPy (and from there to TensorFlow or PyTorch).
So, we're starting with an array of records with named fields:
>>> array
<Array [{phi: 1.1, eta: 2.2}, {...}] type='2 * {phi: float64, eta: float64}'>

We want the named fields to go away and make these individual arrays. That's ak.unzip.
>>> ak.unzip(array)
(<Array [1.1, 3.3] type='2 * float64'>, <Array [2.2, 4.4] type='2 * float64'>)

(The first in the tuple is from phi, the second is from eta.)
We want to get values for each field together into the same input vector for the ML model. That is, 1.1 and 2.2 should be in a vector [1.1, 2.2] and 3.3 and 4.4 should be in a vector [3.3, 4.4]. That's a concatenation of the arrays in this tuple, but not an axis=0 concatenation that would make [1.1, 3.3, 2.2, 4.4]; it has to be a concatenation in a higher axis=1. That axis doesn't exist yet, but we can always make length-1 axes with np.newaxis.
>>> ak.unzip(array[:, np.newaxis])
(<Array [[1.1], [3.3]] type='2 * 1 * float64'>, <Array [[2.2], [4.4]] type='2 * 1 * float64'>)

Now ak.concatenate with axis=1 will concatenate [1.1] and [2.2] into [1.1, 2.2], etc.
>>> ak.concatenate(ak.unzip(array[:, np.newaxis]), axis=1)
<Array [[1.1, 2.2], [3.3, 4.4]] type='2 * 2 * float64'>

So in the end, here's a one-liner that you can pass to TensorFlow that will work even if your record fields have different dtypes:
>>> ak.to_numpy(ak.concatenate(ak.unzip(array[:, np.newaxis]), axis=1))
array([[1.1, 2.2],
       [3.3, 4.4]])

Or, actually, maybe you can skip the ak.to_numpy and go straight to ak.to_tensorflow.
",tensorflow
tensorflow does not accept list type for dataset generator,"I am building a neural network. I couldn't load all the training data into memory at once, so I am using TensorFlow's tf.data.Dataset.from_generator function to load data incrementally. However, it throws an error saying it does not accept a list of tensors as a type.
TypeError: `output_signature` must contain objects that are subclass of 
`tf.TypeSpec` but found <class 'list'> which is not.

The input to my neural network is a list of 151 separate tensors. How can I represent this in the generator? My code is below:
def generator(file_paths, batch_size, files_per_batch, tam, value):
    return tf.data.Dataset.from_generator(
        lambda: data_generator(file_paths, batch_size, files_per_batch, tam, value),
        output_signature=(
            [tf.TensorSpec(shape=(batch_size, tam), dtype=tf.float32) for _ in range(tam+1)],  # Lista de 151 tensores
            tf.TensorSpec(shape=(batch_size, tam), dtype=tf.float32)  # Rótulos
        )
    )

inputArray = [Input(shape=(tam,)) for _ in range(tam + 1)]

train_dataset = generator(file_paths, batch_size, files_per_batch, tam, False)
train_dataset = train_dataset.prefetch(tf.data.AUTOTUNE)

model.fit(train_dataset, epochs=1000, validation_split=0.2, verbose=1)

I tried to use tf.data.Dataset.from_generator to feed data into my neural network in batches, since I can't load all the data into memory at once.
However, I encountered an error:
TypeError: output_signature must contain objects that are subclass of tf.TypeSpec but found <class 'list'> which is not.

","I solved the problem using a dictionary instead of a list.
def generator(file_paths, batch_size, files_per_batch, size, value):
    return tf.data.Dataset.from_generator(
        lambda: data_generator(file_paths, batch_size, files_per_batch, size, value),
        output_signature=(
            {f""input_{i}"": tf.TensorSpec(shape=(batch_size, size), dtype=tf.float32) for i in range(size + 1)},  # Inputs
            tf.TensorSpec(shape=(batch_size, size), dtype=tf.float32)  # Labels
        )
    )

To achieve this, I adjusted the input layer to:
inputArray = [Input(shape=(size,), name=f""input_{i}"") for i in range(size + 1)]

This adjustment ensures that the keys from the generator match the keys expected by the model at the input.
",tensorflow
how to read decode tfrecords file see the images inside and do augmentation,"I have a .tfrecords file and I want to extract, see the images in the file and augment them.
I am using https://colab.research.google.com
TensorFlow version: 2.3.0
And for the following code
raw_dataset = tf.data.TFRecordDataset(""*path.tfrecords"")

for raw_record in raw_dataset.take(1):
    example = tf.train.Example()
    example.ParseFromString(raw_record.numpy())
    print(example)

I am facing the following output:
features {
  feature {
    key: ""depth""
    value {
      int64_list {
        value: 3
      }
    }
  }
  feature {
    key: ""height""
    value {
      int64_list {
        value: 333
      }
    }
  }
  feature {
    key: ""image_raw""
    value {
      bytes_list {
        value:
      }
    }
  }
  feature {
    key: ""label""
    value {
      int64_list {
        value: 16
      }
    }
  }
  feature {
    key: ""width""
    value {
      int64_list {
        value: 500
      }
    }
  }
}

","Here is a simple code that can extract your .tfrecord images as .png format.
To run next codes you need to install one time pip modules through pip install tensorflow tensorflow_addons pillow numpy matplotlib.
import os
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' 
import tensorflow as tf, PIL.Image, numpy as np

raw_dataset = tf.data.TFRecordDataset('max_32_set.tfrecords')

for i, raw_record in enumerate(raw_dataset.take(3)):
    example = tf.train.Example()
    example.ParseFromString(raw_record.numpy())
    info = {}
    for k, v in example.features.feature.items():
        if k == 'image_raw':
            info[k] = v.bytes_list.value[0]
        elif k in ['depth', 'height', 'width']:
            info[k] = v.int64_list.value[0]
    img_arr = np.frombuffer(info['image_raw'], dtype = np.uint8).reshape(
        info['height'], info['width'], info['depth']
    )
    # You can use img_arr numpy array above to directly augment/preprocess
    # your image without saving it to .png.
    img = PIL.Image.fromarray(img_arr)
    img.save(f'max_32_set.tfrecords.{str(i).zfill(5)}.png')

First image from dataset:

Below is code for drawing number of images per each label. Labels inside max_32_set.tfrecords file are represented as integers (not string names), probably names of labels are located in separate small file with meta information about dataset.
import os
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' 
import tensorflow as tf, numpy as np, matplotlib.pyplot as plt

raw_dataset = tf.data.TFRecordDataset('max_32_set.tfrecords')

labels_cnts = {}
for i, raw_record in enumerate(raw_dataset.as_numpy_iterator()):
    example = tf.train.Example()
    example.ParseFromString(raw_record)
    info = {}
    for k, v in example.features.feature.items():
        if k == 'label':
            info[k] = v.int64_list.value[0]
    labels_cnts[info['label']] = labels_cnts.get(info['label'], 0) + 1

x, y = zip(*sorted(labels_cnts.items(), key = lambda e: e[0]))
plt.xlabel('label')
plt.ylabel('num images')
plt.plot(x, y)
plt.xticks(x)
plt.show()

Plot for max_32_set.tfrecords:

Next code does augmentation using gaussian noise and gaussian blur, augmented tfrecord dataset is saved to max_32_set.augmented.tfrecords file:
import os
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' 
import tensorflow as tf, tensorflow_addons as tfa, PIL.Image, numpy as np, math

c_inp_fname = 'max_32_set.tfrecords'
c_out_fname = 'max_32_set.augmented.tfrecords'
c_augment_types = ('noise', 'blur', 'noise_blur', 'noise_blur_mirror')
c_res_class_size = None # If None then auto configured to maximal class size

def calc_labels():
    raw_dataset = tf.data.TFRecordDataset(c_inp_fname)
    cnts, labels = {}, []
    for i, raw_record in enumerate(raw_dataset):
        example = tf.train.Example()
        example.ParseFromString(raw_record.numpy())
        label = example.features.feature['label'].int64_list.value[0]
        cnts[label] = cnts.get(label, 0) + 1
        labels.append(label)
    return cnts, labels

def img_gen():
    raw_dataset = tf.data.TFRecordDataset(c_inp_fname)
    for i, raw_record in enumerate(raw_dataset):
        example = tf.train.Example()
        example.ParseFromString(raw_record.numpy())
        info = {}
        for k, v in example.features.feature.items():
            if k == 'image_raw':
                info[k] = v.bytes_list.value[0]
            elif k in ['depth', 'height', 'width']:
                info[k] = v.int64_list.value[0]
        img_arr = np.frombuffer(info['image_raw'], dtype = np.uint8).reshape(
            info['height'], info['width'], info['depth']
        )
        yield example, img_arr
        
def gaussian_noise(inp, stddev):
    noise = tf.random.normal(shape = tf.shape(inp), mean = 0.0, stddev = stddev, dtype = inp.dtype)
    return inp + noise
        
def augment(a, cnt):
    min_noise_stddev, max_noise_stddev = 5., 20.
    blur_kern, min_blur_stddev, max_blur_stddev = 3, 1., 5.
    
    assert cnt >= 1
    pad_a = lambda x: np.pad(x, (
        (0, 2 ** math.ceil(math.log(x.shape[0]) / math.log(2)) - x.shape[0]),
        (0, 2 ** math.ceil(math.log(x.shape[1]) / math.log(2)) - x.shape[1]),
        (0, 0)), constant_values = 0)
    post_a = lambda x: np.clip(x[:a.shape[0], :a.shape[1]], 0, 255).astype(np.uint8)
    yield 'orig', a
    cnt -= 1
    res = []
    fcnt = math.ceil(cnt / len(c_augment_types))
    linsp = lambda l, r, c: [(l + (i + 1) * (r - l) / (c + 1)) for i in range(c)]
    for noise_stddev, blur_stddev in zip(linsp(min_noise_stddev, max_noise_stddev, fcnt), linsp(min_blur_stddev, max_blur_stddev, fcnt)):
        if 'noise' in c_augment_types:
            #yield 'noise', post_a(tf.keras.layers.GaussianNoise(stddev = noise_stddev)(prep_a, training = True).numpy())
            res.append(('noise', post_a(gaussian_noise(a.astype(np.float32), stddev = noise_stddev).numpy())))
        if 'blur' in c_augment_types:
            res.append(('blur', post_a(tfa.image.gaussian_filter2d(pad_a(a).astype(np.float32), filter_shape = blur_kern, sigma = blur_stddev).numpy())))
        if 'noise_blur' in c_augment_types or 'noise_blur_mirror' in c_augment_types:
            nbr = post_a(tfa.image.gaussian_filter2d(
                pad_a(gaussian_noise(a.astype(np.float32), stddev = noise_stddev).numpy()),
                filter_shape = blur_kern, sigma = blur_stddev).numpy())
            if 'noise_blur' in c_augment_types:
                res.append(('noise_blur', nbr))
            if 'noise_blur_mirror' in c_augment_types:
                res.append(('noise_blur_mirror', tf.image.flip_left_right(nbr).numpy().astype(np.uint8)))
    assert cnt <= len(res) <= cnt + len(c_augment_types), (cnt, len(res), len(c_augment_types))
    yield from res[:cnt]

def process():
    labels_cnts, labels = calc_labels()
    max_class_size = max(labels_cnts.values())
    if c_res_class_size is not None:
        assert max_class_size <= c_res_class_size, f'Maximal class size is {max_class_size}, while requested res class size is smaller, {c_res_class_size}!'
        class_size = c_res_class_size
    else:
        class_size = max_class_size
    cur_labels_cnts = {}
    for iimg, (proto, imga) in enumerate(img_gen()):
        label = proto.features.feature['label'].int64_list.value[0]
        cur_labels_cnts[label] = cur_labels_cnts.get(label, 0) + 1
        need_cnt = class_size // labels_cnts[label] + int(cur_labels_cnts[label] <= class_size % labels_cnts[label])
        for iaug, (taug, aug) in enumerate(augment(imga, need_cnt)):
            #PIL.Image.fromarray(aug).save(f'max_32_set.tfrecords.aug.{str(iimg).zfill(5)}.{iaug}_{taug}.png')
            protoc = type(proto)()
            protoc.ParseFromString(proto.SerializeToString())
            protoc.features.feature['image_raw'].bytes_list.value[0] = aug.tobytes()
            yield protoc.SerializeToString()
        if (iimg % 10) == 0:
            print(iimg, ' ', sep = '', end = '', flush = True)
            
def main():
    assert tf.executing_eagerly()
    tf.data.experimental.TFRecordWriter(c_out_fname).write(
        tf.data.TFRecordDataset.from_generator(process, tf.string)
    )

main()

Example augmented images:
Original:

Noised:

Blurred:

Noised-blurred:

Noised-blurred-mirrored:

Number of images per label after augmentation (exactly balanced 30 images per label):


Same augmentation as above but for the case of input and output folders with labeled images, instead of TFRecordDataset, change c_inp_dir and c_out_dir to your folders paths:
import os
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' 
import tensorflow as tf, tensorflow_addons as tfa, PIL.Image, numpy as np, math, matplotlib.pyplot as plt

c_inp_dir = './images/'
c_out_dir = './images_out/'
c_augment_types = ('noise', 'blur', 'noise_blur', 'noise_blur_mirror')
c_res_class_size = None # If None then auto configured to maximal class size

def calc_labels(dirn = None):
    if dirn is None:
        dirn = c_inp_dir
    cnts, labels = {}, []
    for label in sorted(os.listdir(f'{dirn}')):
        label = int(label)
        labels.append(label)
        cnts[label] = len(os.listdir(f'{dirn}/{label}/'))
    return cnts, labels

def img_gen():
    cnts = {}
    for label in sorted(os.listdir(c_inp_dir)):
        label = int(label)
        for fname in sorted(os.listdir(f'{c_inp_dir}/{label}/')):
            img_arr = np.array(PIL.Image.open(f'{c_inp_dir}/{label}/{fname}'))
            yield label, img_arr, fname
        
def gaussian_noise(inp, stddev):
    noise = tf.random.normal(shape = tf.shape(inp), mean = 0.0, stddev = stddev, dtype = inp.dtype)
    return inp + noise
        
def augment(a, cnt):
    min_noise_stddev, max_noise_stddev = 5., 20.
    blur_kern, min_blur_stddev, max_blur_stddev = 3, 1., 5.
    
    assert cnt >= 1
    pad_a = lambda x: np.pad(x, (
        (0, 2 ** math.ceil(math.log(x.shape[0]) / math.log(2)) - x.shape[0]),
        (0, 2 ** math.ceil(math.log(x.shape[1]) / math.log(2)) - x.shape[1]),
        (0, 0)), constant_values = 0)
    post_a = lambda x: np.clip(x[:a.shape[0], :a.shape[1]], 0, 255).astype(np.uint8)
    yield 'orig', a
    cnt -= 1
    res = []
    fcnt = math.ceil(cnt / len(c_augment_types))
    linsp = lambda l, r, c: [(l + (i + 1) * (r - l) / (c + 1)) for i in range(c)]
    for noise_stddev, blur_stddev in zip(linsp(min_noise_stddev, max_noise_stddev, fcnt), linsp(min_blur_stddev, max_blur_stddev, fcnt)):
        if 'noise' in c_augment_types:
            #yield 'noise', post_a(tf.keras.layers.GaussianNoise(stddev = noise_stddev)(prep_a, training = True).numpy())
            res.append(('noise', post_a(gaussian_noise(a.astype(np.float32), stddev = noise_stddev).numpy())))
        if 'blur' in c_augment_types:
            res.append(('blur', post_a(tfa.image.gaussian_filter2d(pad_a(a).astype(np.float32), filter_shape = blur_kern, sigma = blur_stddev).numpy())))
        if 'noise_blur' in c_augment_types or 'noise_blur_mirror' in c_augment_types:
            nbr = post_a(tfa.image.gaussian_filter2d(
                pad_a(gaussian_noise(a.astype(np.float32), stddev = noise_stddev).numpy()),
                filter_shape = blur_kern, sigma = blur_stddev).numpy())
            if 'noise_blur' in c_augment_types:
                res.append(('noise_blur', nbr))
            if 'noise_blur_mirror' in c_augment_types:
                res.append(('noise_blur_mirror', tf.image.flip_left_right(nbr).numpy().astype(np.uint8)))
    assert cnt <= len(res) <= cnt + len(c_augment_types), (cnt, len(res), len(c_augment_types))
    yield from res[:cnt]

def process():
    labels_cnts, labels = calc_labels()
    max_class_size = max(labels_cnts.values())
    if c_res_class_size is not None:
        assert max_class_size <= c_res_class_size, f'Maximal class size is {max_class_size}, while requested res class size is smaller, {c_res_class_size}!'
        class_size = c_res_class_size
    else:
        class_size = max_class_size
    
    cur_labels_cnts = {}
    for iimg, (label, imga, fname) in enumerate(img_gen()):
        os.makedirs(f'{c_out_dir}/{label}/', exist_ok = True)
        cur_labels_cnts[label] = cur_labels_cnts.get(label, 0) + 1
        need_cnt = class_size // labels_cnts[label] + int(cur_labels_cnts[label] <= class_size % labels_cnts[label])
        for iaug, (taug, aug) in enumerate(augment(imga, need_cnt)):
            PIL.Image.fromarray(aug).save(f'{c_out_dir}/{label}/{fname}.{iaug}_{taug}.png')
        if (iimg % 10) == 0:
            print(iimg, ' ', sep = '', end = '', flush = True)
            
def plot_cnts(dirn):
    labels_cnts = calc_labels(dirn)[0]
    x, y = zip(*sorted(labels_cnts.items(), key = lambda e: e[0]))
    plt.xlabel('label')
    plt.ylabel('num images')
    plt.plot(x, y)
    plt.xticks(x)
    plt.show()
            
def main():
    process()
    plot_cnts(c_inp_dir)
    plot_cnts(c_out_dir)

main()

",tensorflow
warning gradients do not exist for variables,"I recently came across a warning in Tensorflow that caused some head-scratching and took a while to fix. Since I didn't find a solution online, I wanted to share.
I am building a transformer (encoder-decoder) architecture. But my training results are really bad. The transformer always gives the same answer no matter the input, although the training accuracy looks very good (above 0.95). On top of that, I get this warning:
WARNING:tensorflow:Gradients do not exist for variables ['embedding/embeddings:0'] when minimizing the loss. If you're using 'model.compile()', did you forget to provide a 'loss' argument?
Both the encoder and decoder have

a token embedding realized through a keras.Embedding layer
a positional embedding, realized through a
keras_nlp.PositionEmbedding layer.

Here is the encoder code:
encoder_inputs = Input(shape=(encoder_inputs_size,), name=""encoder_inputs"")
token_embeddings = Embedding(input_dim=vocabulary_size, output_dim=embedding_dim)   (encoder_inputs)
position_embeddings = PositionEmbedding(sequence_length=encoder_inputs_size)(token_embeddings)
encoder_outputs = TransformerEncoder(intermediate_dim=intermediate_dim, num_heads=num_heads)(inputs=position_embeddings)
encoder = Model(encoder_inputs, encoder_outputs, name=""encoder"")

There is keras_nlp.TokenAndPositionEmbedding that combines two embeddings into a single layer and using it makes the problem disappear. But since I want to use other forms of embedding, like patch embedding for image processing, I can't use this combined layer.
","The solution is that unlike regular keras layers that simply pass the information through when connecting them, the token embedding and the positional embedding must be manually added up, so the following code fixes the problem:
encoder_inputs = Input(shape=(encoder_inputs_size,), name=""encoder_inputs"")
token_embeddings = Embedding(input_dim=vocabulary_size, output_dim=embedding_dim)(encoder_inputs)
position_embeddings = PositionEmbedding(sequence_length=encoder_inputs_size)(token_embeddings)

# this line adds up the embeddings and fixes the problem
embeddings = token_embeddings + position_embeddings

encoder_outputs = TransformerEncoder(intermediate_dim=intermediate_dim, num_heads=num_heads)(inputs=embeddings)
encoder = Model(encoder_inputs, encoder_outputs, name=""encoder"")

",tensorflow
how to specify padding with keras in conv2d layer,"I am trying to implement AlexNet with Keras and was inspecting the network design in MATLAB which is given as follows

As could be seen, the second convolution layer has 256 filters of size 5x5,  48 channels and a padding of [ 2 2 2 2 ]. How could I specify padding of [ 2 2 2 2] with Keras? I went through the documentation of Conv2D. It accepts only 2 values for padding namely valid and same. I could not understand this. For what I know, valid would mean zero padding. How could I specify [2 2 2 2] padding with the second convolution layer? I created the first layer as:
model.add(keras.layers.Conv2D(filters = 96, kernel_size = (11,11), 
 strides = (4,4), padding = ""valid"", input_shape=(227,227,3)))

Also, since in the second layer there are 48 channels, do I need to be explicit about it?
","A specific padding isn't specified in Conv2D but instead a ZeroPadding2D layer.
valid and same are really just shorthands for common paddings - valid means that you don't pad the input and same means you add padding such that the output length is the same as the input length.
In your case if you wanted to add a specific padding of size 2:
model.add(keras.layers.ZeroPadding2D(padding=(2, 2)))
model.add(keras.layers.Conv2D(filters = 96, kernel_size = (11,11), strides = (4,4), padding = ""valid""))

I would also strongly suggest checking out this keras implementation of alexnet. Note that you can also find docs for padding layers in the keras convolutional docs (it's all the way at the bottom).
",tensorflow
how to efficiently represent a matrix product with repeated elements,"I have a tensor a that is of shape (n/f, c, c) that I want to multiply by another tensor b of shape (n, c, 1). Each row of a represents f rows of b, such that the naiive way of implementing this would be to simply repeat each row of a f times before performing the multiplication:
n = 100
c = 5
f = 10

a = tf.constant(np.random.rand(n//f, c, c))
b = tf.constant(np.random.rand(n, c, c))
a_prime = tf.repeat(a, f, 0)

result = a_prime @ b

This works, but for large n and f I'm worried about the memory footprint of the repeat. I could of course loop through each row and perform dot-products manually, but that would have implications on performance. Is there a better way?
","We can do this by reshaping tensors and utilizing broadcasting, We can perform matrix multiplication more efficiently by eliminating the need for explicit repetition.
import tensorflow as tf
import numpy as np
n = 100
c = 5
f = 10

a = tf.constant(np.random.rand(n // f, c, c))
b = tf.constant(np.random.rand(n, c, c))

#Reshape a and b
a_reshaped = tf.reshape(a, (1, n // f, c, c))
b_reshaped = tf.reshape(b, (n, 1, c, c))

# perform matrix multiplication
result = tf.matmul(a_reshaped, b_reshaped)
result = tf.reduce_sum(result, axis=1)
print(result.shape)

output:
(100, 5, 5)

",tensorflow
understanding states of a bidirectional lstm in a seq2seq model tf keras,"I am creating a language model: A seq2seq model with 2 Bidirectional LSTM layers. I have got the model to train and the accuracy seems good, but whilst stuck on figuring out the inference model, I've found myself a bit confused by the states that are returned by each LSTM layer.
I am using this tutorial as a guide, though the example in this link is not using bidriectional layers: https://blog.keras.io/a-ten-minute-introduction-to-sequence-to-sequence-learning-in-keras.html
Note: I am using a pretrained word embedding.
lstm_units = 100

# Set up embedding layer using pretrained weights
embedding_layer = Embedding(total_words+1, emb_dimension, input_length=max_input_len, weights=[embedding_matrix], name=""Embedding"")

# Encoder
encoder_input_x = Input(shape=(None,), name=""Enc_Input"")
encoder_embedding_x = embedding_layer(encoder_input_x)
encoder_lstm_x, enc_state_h_fwd, enc_state_c_fwd, enc_state_h_bwd, enc_state_c_bwd = Bidirectional(LSTM(lstm_units, dropout=0.5, return_state=True, name=""Enc_LSTM1""), name=""Enc_Bi1"")(encoder_embedding_x)
encoder_states = [enc_state_h_fwd, enc_state_c_fwd, enc_state_h_bwd, enc_state_c_bwd]

# Decoder
decoder_input_x = Input(shape=(None,), name=""Dec_Input"")
decoder_embedding_x = embedding_layer(decoder_input_x)
decoder_lstm_layer = Bidirectional(LSTM(lstm_units, return_state=True, return_sequences=True, dropout=0.5, name=""Dec_LSTM1""))
decoder_lstm_x, _, _, _, _= decoder_lstm_layer(decoder_embedding_x, initial_state=encoder_states) 
decoder_dense_layer = TimeDistributed(Dense(total_words+1, activation=""softmax"", name=""Dec_Softmax""))
decoder_output_x = decoder_dense_layer(decoder_lstm_x)

model = Model(inputs=[encoder_input_x, decoder_input_x], outputs=decoder_output_x)

model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])

I believe  diagram of the model looks like this, with 60 time steps.:

I want the encoder to pass the enc_state_h_fwd and enc_state_c_fwd forward to the decoder. This connection is highlighted by the orange arrow.
But since the model is bidirectional, I have some questions:

Do I need to pass the decoder states backwards to the encoder? And how would one possibly do this, it seems like a chicken and egg scenario.
The encoder_states that come from the encoder lstm layer output 4 states. h and c states going forward and backward. I feel like the ""backward"" states are denoted in my diagram by the pink arrow going left out of the encoder. I am passing these to the decoder, but why does it need them? Am I incorrectly connecting the pink arrow on the left to the purple arrow going into the decoder from the right?

","This model is not valid. It is set up as a translation model, which during inference would predict one word at a time, starting with the start of sequence token, to predict y1, then looping and feeding in the start of sequence token, y1 to get y2 etc.
A bidirectional LSTM cannot be used for real time predictions in a many to many prediction unless the entire decoder input is available. In this case, the decoder input is only available after predicting one step at a time, so the first prediction (of y1) is invalid without the rest of the sequence (y2-yt).
The decoder should therefore not be a bidirectional LSTM.
As for the states, the encoder Bidirectional LSTM does indeed output h and c states going forward (orange arrow), and h and c states going backward (pink arrow).
By concatenating these states and feeding them to the decoder, we can give the decoder more information. This is possible as we do have the entire encoder input at time of inference.
Also to be noted is that the bidirectional encoder with lstm_units (eg. 100) effectively has 200 lstm units, half going forward, half going backward. To feed these into the decoder, the decoder must have 200 units too.
",tensorflow
how can i load an image from url into a tensorflow pipeline in r,"I am playing around with image classification using the tensorflow and keras packages for R. I have build and trained a model that does well on the testing validation dataset. I now want to use that model to predict image classes for a lot of images stored online (i have all the URLs in a dataframe in R).
I can write a for loop to do this where i download each image, classify it, record the model prediction, and then delete the downloaded image, but this takes a long time and it would be faster to just read the image into memory instead of downloading each image. I cannot for the life of me figure out how to load an imagine into memory in R and convert it to a datatype that works with the rest of my tensorflow image standardization.
Here is my for loop:
data$score<-NA
for (i in 1:nrow(data)){
  
  img_tensor = 
    get_file(""t"",data$image_url[i]) %>% #download temp file
    tf$io$read_file() %>%
    tf$io$decode_image() %>%
    tf$image$resize(as.integer(image_size)) %>%
    tf$expand_dims(0L) 
  
  #delete temp file
  file.remove(""/Users/me/.keras/datasets/t"")
  
  data$score[i]=model %>% predict(img_tensor, verbose=0)
  
}

Here is an example image URL: https://inaturalist-open-data.s3.amazonaws.com/photos/451526093/medium.jpeg
All i want to do is be able to load that image into R directly from the URL (no writing the file to disk) and then use the tensorflow workflow (decode_image, resize, expand_dims). Any help is appreciated!
To replicate the code just replace data$image_url[i] with the URL i provided. No need to worry about predicting my model, that part is working fine. I just need the image to successfully feed into the rest of the pipe.
","A few notes:

Writing to a temporary directory on macOS and Linux usually has identical performance to keeping everything in memory, since /tmp is usually mounted as a RAM filesystem and never actually touches the disk. (If you're on Windows, or are swapping, the story is different)

As far as I know, TensorFlow doesn't have any graph ops that will fetch content from an http url, so you'll need to do that step using R or Python. If the op needs to live in a tf.data, you'll need to wrap it in tf.py_function.

To fetch a url directly into memory in R, without writing to the filesystem, you can do:
url <- ""https://inaturalist-open- data.s3.amazonaws.com/photos/451526093/medium.jpeg""
bytes <- readBin(url, raw(), 200000)
as_py_bytes <- reticulate::import_builtins(convert = FALSE)$bytes
bytes_tensor <- tf$constant(as_py_bytes(bytes), tf$string)


The bottleneck is most likely the download step, not the ""write to a file"" step. You'll probably see the most significant speedups from rewriting your loop to process batches of images instead of a single image at a time (e.g., using curl::multi_download(), and passing a batch of images to predict())


",tensorflow
what does this tensorflow message mean any side effect was the installation successful,"I just installed tensorflow v2.3 on anaconda python. I tried to test out the installation using the python command below;
$ python -c ""import tensorflow as tf; x = [[2.]]; print('tensorflow version', tf.__version__); print('hello, {}'.format(tf.matmul(x, x)))""

I got the following message;
2020-12-15 07:59:12.411952: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations:  AVX AVX2
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
hello, [[4.]]

From the message, it seems that the installation was installed successfully. But what does This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations:  AVX AVX2 mean exactly?
Am I using a tensorflow version with some limited features? Any side effects?
I am using Windows 10.
","An important part of Tensorflow is that it is supposed to be fast. With a suitable installation, it works with CPUs, GPUs, or TPUs. Part of going fast means that it uses different code depending on your hardware. Some CPUs support operations that other CPUs do not, such as vectorized addition (adding multiple variables at once). Tensorflow is simply telling you that the version you have installed can use the AVX and AVX2 operations and is set to do so by default in certain situations (say inside a forward or back-prop matrix multiply), which can speed things up. This is not an error, it is just telling you that it can and will take advantage of your CPU to get that extra speed out.
Note: AVX stands for Advanced Vector Extensions.
",tensorflow
issue between macos 13 and tensorflow,"I am trying to use PhaseNet and it request the tensorflow==2.14.0. However, when I try to install the tensorflow, the pip does not shows availability of the version 2.14:
pip install tensorflow==2.14.0 
ERROR: Could not find a version that satisfies the requirement tensorflow==2.14.0 (from versions: 2.16.0rc0, 2.16.1, 2.16.2, 2.17.0rc0, 2.17.0rc1, 2.17.0, 2.17.1, 2.18.0rc0, 2.18.0rc1, 2.18.0rc2, 2.18.0)
ERROR: No matching distribution found for tensorflow==2.14.0

I have installed the 2.16.1, but the PhaseNet didn't work when it try to use the tensorflow. It shows the following message:
 File ""/opt/anaconda3/envs/phasenet/lib/python3.12/site-packages/tensorflow/python/util/lazy_loader.py"", line 207, in __getattr__
    raise AttributeError(
AttributeError: `conv2d` is not available with Keras 3.

Does someone have any suggestion how to install the tensorflow==2.14.0 when it is not available on pip?
Thanks in advance,
Guilherme
","tensorflow 2.14.0 requires and provides wheels for Pythons 3.9, 3.10, 3.11.
Use Python 3.11.
",tensorflow
tensorflow docker not using gpu,"I'm trying to get Tensorflow working on my Ubuntu 24.04.1 with a GPU.
According to this page:

Docker is the easiest way to run TensorFlow on a GPU since the host machine only requires the NVIDIA® driver

So I'm trying to use Docker.
I'm checking to ensure my GPU is working with Docker by running docker run --gpus all --rm nvidia/cuda:12.6.2-cudnn-runtime-ubuntu24.04 nvidia-smi. The output of that is:
==========
== CUDA ==
==========

CUDA Version 12.6.2

Container image Copyright (c) 2016-2023, NVIDIA CORPORATION & AFFILIATES. All rights reserved.

This container image and its contents are governed by the NVIDIA Deep Learning Container License.
By pulling and using the container, you accept the terms and conditions of this license:
https://developer.nvidia.com/ngc/nvidia-deep-learning-container-license

A copy of this license is made available in this container at /NGC-DL-CONTAINER-LICENSE for your convenience.

Sat Oct 26 01:16:50 2024
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 560.35.03              Driver Version: 560.35.03      CUDA Version: 12.6     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA TITAN RTX               Off |   00000000:01:00.0 Off |                  N/A |
| 41%   40C    P8             24W /  280W |       1MiB /  24576MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+

+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|  No running processes found                                                             |
+-----------------------------------------------------------------------------------------+

(Side note, I'm not using the command they suggest because docker run --gpus all --rm nvidia/cuda nvidia-smi doesn't work due to nvidia/cuda not having a latest tag anymore)
So it looks to be working. However when I run:
docker run --gpus all -it --rm tensorflow/tensorflow:latest-gpu \
   python -c ""import tensorflow as tf; print(tf.config.list_physical_devices('GPU'))""

The output is:
2024-10-26 01:20:51.021242: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1729905651.033544       1 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1729905651.037491       1 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2024-10-26 01:20:51.050486: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
W0000 00:00:1729905652.350499       1 gpu_device.cc:2344] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.
Skipping registering GPU devices...
[]

Which indicates that there is no GPU detected by Tensorflow.
What am I doing wrong here?
","I don't think you're doing anything wrong, but I'm concerned that the image may be a ""pip install"" short of a complete image.
I'm running a different flavor of linux, but to start off with I had to make sure I had my gpu available to docker (see here Add nvidia runtime to docker runtimes ) and I upgraded my cuda version to the latest.
Even after doing all this I had the same error as you.
So I logged into the container as follows:
docker run -it --rm --runtime=nvidia --gpus all tensorflow/tensorflow:latest-gpu /bin/bash
and ran
pip install tensorflow[and-cuda]
Some of the dependencies were there and some or the dependencies had to be installed because they were missing.  This is undesireable because you'd expect everything necessary to be there to run (maybe they'll fix the image in the future)
After it finished I ran python -c ""import tensorflow as tf; print(tf.config.list_physical_devices('GPU'))"" and it finally found my GPU
You're going to want to create your own docker image using their docker image as a base. So your dockerfile may look like something like:
# Use the official TensorFlow GPU base image
FROM tensorflow/tensorflow:latest-gpu

# Install TensorFlow with CUDA support
RUN pip install tensorflow[and-cuda]

# Shell
CMD [""bash""]

",tensorflow
how to setup tf and torch on one virtual environment with same cuda,"I want to setup tensorflow and pytorch on one virtual environment with same CUDA. However, I cannot find a CUDA version that can support both tensorflow and pytorch: For tensorflow 2.10, I selected CUDA 11.2. But I didn't find this CUDA version in the list for supporting pyTorch. I can only find the CUDA 11.1 in the list for pyTorch. Detailed information is listed below.

To find CUDA version for Tensorflow
https://www.tensorflow.org/install/source_windows#tested_build_configurations


To find CUDA version for PyTorch
https://elenacliu-pytorch-cuda-driver.streamlit.app/



Will there be any problems if I install 2 different CUDA versions if I want to run the codes with GPU card? For example, after I create a virtual environemt by ""conda create --name myenv python=3.10"", I want to run codes with tensorflow for project 1, and codes with pyTorch for project 2.
Do I need to modify the ""CUDA_PATH"" in system variable every time before I ran the codes. i.e., set CUDA_PATH for CUDA 11.1 when I need to use PyTorch, and set CUDA_PATH for CUDA 11.2 when I need to use Tensorflow?
I find there is an option of installing CUDA 11.0, which is compatible with TF-2.4 and PyTorch-1.7.
But there is a problem that it does not support CUDA capability SM_86. Will it be a problem of losing access to new features?

","There are no pre-built binaries of pytorch with cuda-11.2 indeed. If you necessarily want to go with this version of cuda, you have two choices I think:

Use pytorch binaries compiled with cuda-11.1, which should work just fine
Build pytorch from source, as described here

I'm basically repeating what is said on this pytorch thread, you can read it for more details
I would not try to have multiple versions of cuda and manually ""hotswap"" them by tinkering with the cuda paths. (Opinion here) From experience, it can work but is also very error prone and will lead to problems eventually
",tensorflow
unknown image file format one of jpeg png gif bmp required,"I built a simple CNN model and it raised below errors:
Epoch 1/10
235/235 [==============================] - ETA: 0s - loss: 540.2643 - accuracy: 0.4358
---------------------------------------------------------------------------
InvalidArgumentError                      Traceback (most recent call last)
<ipython-input-14-ab88232c98aa> in <module>()
     15     train_ds,
     16     validation_data=val_ds,
---> 17     epochs=epochs
     18 )

7 frames
/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/execute.py in quick_execute(op_name, num_outputs, inputs, attrs, ctx, name)
     58     ctx.ensure_initialized()
     59     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
---> 60                                         inputs, attrs, num_outputs)
     61   except core._NotOkStatusException as e:
     62     if name is not None:

InvalidArgumentError:  Unknown image file format. One of JPEG, PNG, GIF, BMP required.
     [[{{node decode_image/DecodeImage}}]]
     [[IteratorGetNext]] [Op:__inference_test_function_2924]

Function call stack:
test_function

The code I wrote is quite simple and standard. Most of them are just directly copied from the official website. It raised this error before the first epoch finish. I am pretty sure that the images are all png files. The train folder does not contain anything like text, code, except imgages. I am using Colab. The version of tensorlfow is 2.5.0. Appreciate for any help.
data_dir = './train'

train_ds = tf.keras.preprocessing.image_dataset_from_directory(
    data_dir, 
    subset='training',
    validation_split=0.2,
    batch_size=batch_size,
    seed=42
)

val_ds = tf.keras.preprocessing.image_dataset_from_directory(
    data_dir, 
    subset='validation',
    validation_split=0.2,
    batch_size=batch_size,
    seed=42
)

model = Sequential([
    layers.InputLayer(input_shape=(image_size, image_size, 3)),
    layers.Conv2D(32, 3, activation='relu'),
    layers.MaxPooling2D(),
    layers.Flatten(),
    layers.Dense(128, activation='relu'),
    layers.Dense(num_classes)
    ])

optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)
model.compile(
    optimizer=optimizer,
    loss=tf.losses.SparseCategoricalCrossentropy(from_logits=True),
    metrics=['accuracy'])

history = model.fit(
    train_ds,
    validation_data=val_ds,
    epochs=epochs
)

","Some of your files in the validation folder are not in the format accepted by Tensorflow ( JPEG, PNG, GIF, BMP), or may be corrupted. The extension of a file is indicative only, and does not enforce anything on the content of the file.
You might be able to find the culprit using the imghdr module from the python standard library, and a simple loop.
from pathlib import Path
import imghdr

data_dir = ""/home/user/datasets/samples/""
image_extensions = ["".png"", "".jpg""]  # add there all your images file extensions

img_type_accepted_by_tf = [""bmp"", ""gif"", ""jpeg"", ""png""]
for filepath in Path(data_dir).rglob(""*""):
    if filepath.suffix.lower() in image_extensions:
        img_type = imghdr.what(filepath)
        if img_type is None:
            print(f""{filepath} is not an image"")
        elif img_type not in img_type_accepted_by_tf:
            print(f""{filepath} is a {img_type}, not accepted by TensorFlow"")

This should print out whether you have files that are not images, or that are not what their extension says they are, and not accepted by TF. Then you can either get rid of them or convert them to a format that TensorFlow supports.
",tensorflow
how to implement tfgather_nd in pytorch with the argument batch_dims,"I have been doing a project on image matching, so I need to find correspondences between 2 images. To get descriptors, I will need a interpolate function. However, when I read about a equivalent function which is done in Tensorflow, I still don’t get how to implement tf.gather_nd(parmas, indices, barch_dims) in Pytorch. Especially when there is a argument: batch_dims. I have gone through stackoverflow and there is no perfect equivalence yet.
The referred interpolate function in Tensorflow is below and I have been trying to implement this in Pytorch Arguments' information is below:
inputs is a dense feature map[i] from a for loop of batch size, which means it is 3D[H, W, C](in pytorch is [C, H, W])
pos is a set of random point coordinate shapes like [[i, j], [i, j],...,[i, j]], so it is 2D when it goes in interpolate function(in pytorch is [[i,i,...,i], [j,j,...,j]])
and it then expands both of their dimensions when they get into this function
I just want a perfect implement of tf.gather_nd with argument batch_dims. Thank you!
And here's a simple example of using it:
pos = tf.ones((12, 2)) ## stands for a set of coordinates [[i, i,…, i], [j, j,…, j]]
inputs = tf.ones((4, 4, 128)) ## stands for [H, W, C] of dense feature map
outputs = interpolate(pos, inputs, batched=False)
print(outputs.get_shape()) # We get (12, 128) here

interpolate function (tf version):
def interpolate(pos, inputs, nd=True):

    pos = tf.expand_dims(pos, 0)
    inputs = tf.expand_dims(inputs, 0)

    h = tf.shape(inputs)[1]
    w = tf.shape(inputs)[2]

    i = pos[:, :, 0]
    j = pos[:, :, 1]

    i_top_left = tf.clip_by_value(tf.cast(tf.math.floor(i), tf.int32), 0, h - 1)
    j_top_left = tf.clip_by_value(tf.cast(tf.math.floor(j), tf.int32), 0, w - 1)

    i_top_right = tf.clip_by_value(tf.cast(tf.math.floor(i), tf.int32), 0, h - 1)
    j_top_right = tf.clip_by_value(tf.cast(tf.math.ceil(j), tf.int32), 0, w - 1)

    i_bottom_left = tf.clip_by_value(tf.cast(tf.math.ceil(i), tf.int32), 0, h - 1)
    j_bottom_left = tf.clip_by_value(tf.cast(tf.math.floor(j), tf.int32), 0, w - 1)

    i_bottom_right = tf.clip_by_value(tf.cast(tf.math.ceil(i), tf.int32), 0, h - 1)
    j_bottom_right = tf.clip_by_value(tf.cast(tf.math.ceil(j), tf.int32), 0, w - 1)

    dist_i_top_left = i - tf.cast(i_top_left, tf.float32)
    dist_j_top_left = j - tf.cast(j_top_left, tf.float32)
    w_top_left = (1 - dist_i_top_left) * (1 - dist_j_top_left)
    w_top_right = (1 - dist_i_top_left) * dist_j_top_left
    w_bottom_left = dist_i_top_left * (1 - dist_j_top_left)
    w_bottom_right = dist_i_top_left * dist_j_top_left

    if nd:
        w_top_left = w_top_left[..., None]
        w_top_right = w_top_right[..., None]
        w_bottom_left = w_bottom_left[..., None]
        w_bottom_right = w_bottom_right[..., None]

    interpolated_val = (
        w_top_left * tf.gather_nd(inputs, tf.stack([i_top_left, j_top_left], axis=-1), batch_dims=1) +
        w_top_right * tf.gather_nd(inputs, tf.stack([i_top_right, j_top_right], axis=-1), batch_dims=1) +
        w_bottom_left * tf.gather_nd(inputs, tf.stack([i_bottom_left, j_bottom_left], axis=-1), batch_dims=1) +
        w_bottom_right * tf.gather_nd(inputs, tf.stack([i_bottom_right, j_bottom_right], axis=-1), batch_dims=1)
    )

    interpolated_val = tf.squeeze(interpolated_val, axis=0)
    return interpolated_val

","As far as I'm aware there is no directly equivalent of tf.gather_nd in PyTorch and implementing a generic version with batch_dims is not that simple. However, you likely don't need a generic version, and given the context of your interpolate function, a version for [C, H, W] would suffice.
At the beginning of interpolate you add a singular dimension to the front, which is the batch dimension. Setting batch_dims=1 in tf.gather_nd means there is one batch dimension at the beginning, therefore it applies it per batch, i.e. it indexes inputs[0] with pos[0] etc. There is no benefit of adding a singular batch dimension, because you could have just used the direct computation.
# Adding singular batch dimension
# Shape: [1, num_pos, 2]
pos = tf.expand_dims(pos, 0)
# Shape: [1, H, W, C]
inputs = tf.expand_dims(inputs, 0)


batched_result = tf.gather_nd(inputs, pos, batch_dims=1)
single_result = tf.gater_nd(inputs[0], pos[0])

# The first element in the batched result is the same as the single result
# Hence there is no benefit to adding a singular batch dimension.
tf.reduce_all(batched_result[0] == single_result) # => True

Single version
In PyTorch the implementation for [H, W, C] can be done with Python's indexing. While PyTorch usually uses [C, H, W] for images, it's only a matter of what dimension to index, but let's keep them the same as in TensorFlow for the sake of comparison. If you were to index them manually, you would do it as such: inputs[pos_h[0], pos_w[0]],  inputs[pos_h[1], pos_w[1]] and so on. PyTorch allows you to do that automatically by providing the indices as lists: inputs[pos_h, pos_w], where pos_h and pos_w have the same length. All you need to do is split your pos into two separate tensors, one for the indices along the height dimension and the other along the width dimension, which you also did in the TensorFlow version.
inputs = torch.randn(4, 4, 128)
# Random positions 0-3, shape: [12, 2]
pos = torch.randint(4, (12, 2))

# Positions split by dimension
pos_h = pos[:, 0]
pos_w = pos[:, 1]

# Index the inputs with the indices per dimension
gathered = inputs[pos_h, pos_w]

# Verify that it's identical to TensorFlow's output
inputs_tf = tf.convert_to_tensor(inputs.numpy())
pos_tf = tf.convert_to_tensor(pos.numpy())
gathered_tf = tf.gather_nd(inputs_tf, pos_tf)
gathered_tf = torch.from_numpy(gathered_tf.numpy())

torch.equal(gathered_tf, gathered) # => True

If you want to apply it to a tensor of size [C, H, W] instead, you only need to change the dimensions you want to index:
# For [H, W, C]
gathered = inputs[pos_h, pos_w]

# For [C, H, W]
gathered = inputs[:, pos_h, pos_w]

Batched version
Making it a batched batched version (for [N, H, W, C] or [N, C, H, W]) is not that difficult, and using that is more appropriate, since you're dealing with batches anyway. The only tricky part is that each element in the batch should only be applied to the corresponding batch. For this the batch dimensions needs to be enumerated, which can be done with torch.arange. The batch enumeration is just the list with the batch indices, which will be combined with the pos_h and pos_w indices, resulting in inputs[0, pos_h[0, 0], pos_h[0, 0]], inputs[0, pos_h[0, 1], pos_h[0, 1]] ... inputs[1, pos_h[1, 0], pos_h[1, 0]] etc.
batch_size = 3
inputs = torch.randn(batch_size, 4, 4, 128)
# Random positions 0-3, different for each batch, shape: [3, 12, 2]
pos = torch.randint(4, (batch_size, 12, 2))

# Positions split by dimension
pos_h = pos[:, :, 0]
pos_w = pos[:, :, 1]

batch_enumeration = torch.arange(batch_size) # => [0, 1, 2]
# pos_h and pos_w have shape [3, 12], so the batch enumeration needs to be
# repeated 12 times per batch.
# Unsqueeze to get shape [3, 1], now the 1 could be repeated to 12, but
# broadcasting will do that automatically.
batch_enumeration = batch_enumeration.unsqueeze(1)
# Index the inputs with the indices per dimension
gathered = inputs[batch_enumeration, pos_h, pos_w]

# Again, verify that it's identical to TensorFlow's output
inputs_tf = tf.convert_to_tensor(inputs.numpy())
pos_tf = tf.convert_to_tensor(pos.numpy())
# This time with batch_dims=1
gathered_tf = tf.gather_nd(inputs_tf, pos_tf, batch_dims=1)
gathered_tf = torch.from_numpy(gathered_tf.numpy())

torch.equal(gathered_tf, gathered) # => True

Again, for [N, C, H, W], only the dimensions that are indexed need to be changed:
# For [N, H, W, C]
gathered = inputs[batch_enumeration, pos_h, pos_w]

# For [N, C, H, W]
gathered = inputs[batch_enumeration, :, pos_h, pos_w]


Just a little side note on the interpolate implementation, rounding the positions (floor and ceil respectively) doesn't make sense, because indices must be integers, so it has no effect, as long as your positions are actual indices. That also results in i_top_left and i_bottom_left being the same value, but even if they are to be rounded differently, they are always 1 position apart. Furthermore, i_top_left and i_top_right are literally the same. I don't think that this function produces a meaningful output. I don't know what you're trying to achieve, but if you're looking for image interpolation you could have a look at torch.nn.functional.interpolate.
",tensorflow
check tpu workloadutilization,"I am training a model, and when I open the TPU in the Google Cloud Platform console, it shows me the CPU utilization (on the TPU, I suppose). It is really, really, low (like 0.07%), so maybe it is the VM CPU?  I am wondering whether the training is really proper or if the TPUs are just that strong.
Is there any other way to check the TPU usage? Maybe with a ctpu command?
","I would recommend using the TPU profiling tools that plug into TensorBoard. A good tutorial for install and use of these tools can be found here.
You'll run the profiler while your TPU is training. It will add an extra tab to your TensorBoard with TPU-specific profiling information. Among the most useful:

Average step time
Host idle time (how much time the CPU spends idling)
TPU idle time
Utilization of TPU Matrix units

Based on these metrics, the profiler will suggest ways to start optimizing your model to train well on a TPU. You can also dig into the more sophisticated profiling tools like a trace viewer, or a list of the most expensive graph operations.
For some guidelines on performance tuning (in addition to those ch_mike already linked) you can look at the TPU performance guide.
",tensorflow
why is the accuracy for my keras model always 0 when training,"I have built a simple Keras network:
import numpy as np;

from keras.models import Sequential;
from keras.layers import Dense,Activation;

data= np.genfromtxt(""./kerastests/mydata.csv"", delimiter=';')
x_target=data[:,29]
x_training=np.delete(data,6,axis=1)
x_training=np.delete(x_training,28,axis=1)

model=Sequential()
model.add(Dense(20,activation='relu', input_dim=x_training.shape[1]))
model.add(Dense(10,activation='relu'))
model.add(Dense(1));

model.compile(optimizer='adam',loss='mean_squared_error',metrics=['accuracy'])
model.fit(x_training, x_target)

From my source data, I have removed 2 columns, as you can see. One is a column that came with dates in a string format (in the dataset, besides it, I have a column for the day, another for the month, and another for the year, so I don't need that column) and the other column is the column I use as target for the model).
When I train this model I get this output:
32/816 [>.............................] - ETA: 23s - loss: 13541942.0000 - acc: 0.0000e+00
800/816 [============================>.] - ETA: 0s - loss: 11575466.0400 - acc: 0.0000e+00 
816/816 [==============================] - 1s - loss: 11536905.2353 - acc: 0.0000e+00     
Epoch 2/10
 32/816 [>.............................] - ETA: 0s - loss: 6794785.0000 - acc: 0.0000e+00
816/816 [==============================] - 0s - loss: 5381360.4314 - acc: 0.0000e+00     
Epoch 3/10
 32/816 [>.............................] - ETA: 0s - loss: 6235184.0000 - acc: 0.0000e+00
800/816 [============================>.] - ETA: 0s - loss: 5199512.8700 - acc: 0.0000e+00
816/816 [==============================] - 0s - loss: 5192977.4216 - acc: 0.0000e+00     
Epoch 4/10
 32/816 [>.............................] - ETA: 0s - loss: 4680165.5000 - acc: 0.0000e+00
736/816 [==========================>...] - ETA: 0s - loss: 5050110.3043 - acc: 0.0000e+00
816/816 [==============================] - 0s - loss: 5168771.5490 - acc: 0.0000e+00     
Epoch 5/10
 32/816 [>.............................] - ETA: 0s - loss: 5932391.0000 - acc: 0.0000e+00
768/816 [===========================>..] - ETA: 0s - loss: 5198882.9167 - acc: 0.0000e+00
816/816 [==============================] - 0s - loss: 5159585.9020 - acc: 0.0000e+00     
Epoch 6/10
 32/816 [>.............................] - ETA: 0s - loss: 4488318.0000 - acc: 0.0000e+00
768/816 [===========================>..] - ETA: 0s - loss: 5144843.8333 - acc: 0.0000e+00
816/816 [==============================] - 0s - loss: 5151492.1765 - acc: 0.0000e+00     
Epoch 7/10
 32/816 [>.............................] - ETA: 0s - loss: 6920405.0000 - acc: 0.0000e+00
800/816 [============================>.] - ETA: 0s - loss: 5139358.5000 - acc: 0.0000e+00
816/816 [==============================] - 0s - loss: 5169839.2941 - acc: 0.0000e+00     
Epoch 8/10
 32/816 [>.............................] - ETA: 0s - loss: 3973038.7500 - acc: 0.0000e+00
672/816 [=======================>......] - ETA: 0s - loss: 5183285.3690 - acc: 0.0000e+00
816/816 [==============================] - 0s - loss: 5141417.0000 - acc: 0.0000e+00     
Epoch 9/10
 32/816 [>.............................] - ETA: 0s - loss: 4969548.5000 - acc: 0.0000e+00
768/816 [===========================>..] - ETA: 0s - loss: 5126550.1667 - acc: 0.0000e+00
816/816 [==============================] - 0s - loss: 5136524.5098 - acc: 0.0000e+00     
Epoch 10/10
 32/816 [>.............................] - ETA: 0s - loss: 6334703.5000 - acc: 0.0000e+00
768/816 [===========================>..] - ETA: 0s - loss: 5197778.8229 - acc: 0.0000e+00
816/816 [==============================] - 0s - loss: 5141391.2059 - acc: 0.0000e+00    

Why is this happening? My data is a time series. I know that for time series people do not usually use Dense neurons, but it is just a test. What really tricks me is that accuracy is always 0. And, with other tests, I did even lose: gets to a ""NAN"" value.
Could anybody help here?
","Your model seems to correspond to a regression model for the following reasons: 

You are using linear (the default one) as an activation function in the output layer (and relu in the layer before).
Your loss is loss='mean_squared_error'. 

However, the metric that you use- metrics=['accuracy'] corresponds to a classification problem. If you want to do regression, remove metrics=['accuracy']. That is, use
model.compile(optimizer='adam',loss='mean_squared_error')

Here is a list of keras metrics for regression and classification (taken from this blog post):

Keras Regression Metrics
•Mean Squared Error: mean_squared_error, MSE or mse 
•Mean Absolute Error: mean_absolute_error, MAE, mae 
•Mean Absolute Percentage Error: mean_absolute_percentage_error, MAPE,
  mape 
•Cosine Proximity: cosine_proximity, cosine
Keras Classification Metrics
•Binary Accuracy: binary_accuracy, acc
•Categorical Accuracy: categorical_accuracy, acc
•Sparse Categorical Accuracy: sparse_categorical_accuracy
•Top k Categorical Accuracy: top_k_categorical_accuracy (requires you
  specify a k parameter)
•Sparse Top k Categorical Accuracy: sparse_top_k_categorical_accuracy
  (requires you specify a k parameter)

",tensorflow
tensorflow allocation memory allocation of 38535168 exceeds 10 of system memory,"Using ResNet50 pre-trained Weights I am trying to build a classifier. The code base is fully implemented in Keras high-level Tensorflow API. The complete code is posted in the below GitHub Link.
Source Code: Classification Using RestNet50 Architecture
The file size of the pre-trained model is 94.7mb.
I loaded the pre-trained file
new_model = Sequential()

new_model.add(ResNet50(include_top=False,
                pooling='avg',
                weights=resnet_weight_paths))

and fit the model
train_generator = data_generator.flow_from_directory(
    'path_to_the_training_set',
    target_size = (IMG_SIZE,IMG_SIZE),
    batch_size = 12,
    class_mode = 'categorical'
    )

validation_generator = data_generator.flow_from_directory(
    'path_to_the_validation_set',
    target_size = (IMG_SIZE,IMG_SIZE),
    class_mode = 'categorical'
    )

#compile the model

new_model.fit_generator(
    train_generator,
    steps_per_epoch = 3,
    validation_data = validation_generator,
    validation_steps = 1
)

and in the Training dataset, I have two folders dog and cat, each holder almost 10,000 images. When  I compiled the script, I get the following error

Epoch 1/1 2018-05-12 13:04:45.847298: W
tensorflow/core/framework/allocator.cc:101] Allocation of 38535168
exceeds 10% of system memory. 2018-05-12 13:04:46.845021: W
tensorflow/core/framework/allocator.cc:101] Allocation of 37171200
exceeds 10% of system memory. 2018-05-12 13:04:47.552176: W
tensorflow/core/framework/allocator.cc:101] Allocation of 37171200
exceeds 10% of system memory. 2018-05-12 13:04:48.199240: W
tensorflow/core/framework/allocator.cc:101] Allocation of 37171200
exceeds 10% of system memory. 2018-05-12 13:04:48.918930: W
tensorflow/core/framework/allocator.cc:101] Allocation of 37171200
exceeds 10% of system memory. 2018-05-12 13:04:49.274137: W
tensorflow/core/framework/allocator.cc:101] Allocation of 19267584
exceeds 10% of system memory. 2018-05-12 13:04:49.647061: W
tensorflow/core/framework/allocator.cc:101] Allocation of 19267584
exceeds 10% of system memory. 2018-05-12 13:04:50.028839: W
tensorflow/core/framework/allocator.cc:101] Allocation of 19267584
exceeds 10% of system memory. 2018-05-12 13:04:50.413735: W
tensorflow/core/framework/allocator.cc:101] Allocation of 19267584
exceeds 10% of system memory.

Any ideas to optimize the way to load the pre-trained model (or) get rid of this warning message?
Thanks!
","Try reducing batch_size attribute to a small number(like 1,2 or 3).
Example:
train_generator = data_generator.flow_from_directory(
    'path_to_the_training_set',
    target_size = (IMG_SIZE,IMG_SIZE),
    batch_size = 2,
    class_mode = 'categorical'
    )

",tensorflow
tfkerasmetricsmean_absolute_errory_truey_pred no longer work with tensorflow v216,"I have a function that creates a dict for evaluation using metrics like mae, mse,rmse,mape and mase. The error is with ""MAE"". This was working as of Jan 2024; The error screenshot is attached. Code snippet:
def evaluate_preds(y_true, y_pred):
  # Make sure float32 (for metric calculations)
  y_true = tf.cast(y_true, dtype=tf.float32)
  y_pred = tf.cast(y_pred, dtype=tf.float32)

  # Calculate various metrics
  mae = tf.keras.metrics.mean_absolute_error(y_true, y_pred) #error here
  mse = tf.keras.metrics.mean_squared_error(y_true, y_pred) 
  rmse = tf.sqrt(mse)
  mape = tf.keras.metrics.mean_absolute_percentage_error(y_true, y_pred)
  mase = mean_absolute_scaled_error(y_true, y_pred)
  
  return {""mae"": mae.numpy(),
          ""mse"": mse.numpy(),
          ""rmse"": rmse.numpy(),
          ""mape"": mape.numpy(),
          ""mase"": mase.numpy()}

I troubleshoot using stackoverflow, view documentation but no luck. Steps I followed for setup is:

Installed miniforge
conda install <package_name>
packages installed in virtual env are tensorflow-macos,tensorflow-metal,jupyter,pandas,numpy,matplotlib scikit-learn

Error description keras._tf_keras.keras.metrics seem misleading to me.
Any help appreacited!
Thanks,
Jordan
Error screenshot
","Surprisingly, by importing Keras, it worked. I think in tf 2.x, Keras version also upgraded to 3 from 2
On Dec 23', importing keras from tensorflow did the trick: from tensorflow import keras. However, it gave the same error when running the notebook on May 03'24. Luckily, import keras worked.
",tensorflow
how dose the mobilenet in tensorflow preprocess input,"When we use some famous CNN deep neural networks such as MobileNet, it is recommended to preprocess an image before feeding it into the network. I found a sample code that uses MobileNet. In this code, the preprocess on the image is done by the following code in TensorFlow 2.7.0:
tf.keras.applications.mobilenet.preprocess_input(image)

I need to preprocess the input image only using PIL and OpenCV in python. Therefore, I need to know the procedure of MobileNet preprocesses in TensorFlow. I will be grateful to guide.
","As already stated here:

[...] mobilenet.preprocess_input will scale input pixels between -1 and 1.

As already mentioned, you could also check out the source code itself. With opencv, you would just use cv2.resize(*) and cv2.normalize(*).
",tensorflow
custom encoder and decoder layers within keras model show as unbuilt,"I have a subclassed tensorflow.keras.Model Seq2Seq model with custom layers. However, when I try to run a test script to build and compile the model, running model.summary() gives:
Model: ""retrosynthesis_seq2_seq_model""
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓
┃ Layer (type)                    ┃ Output Shape           ┃       Param # ┃
┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩
│ simple_encoder (SimpleEncoder)  │ ?                      │   0 (unbuilt) │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ simple_decoder (SimpleDecoder)  │ ?                      │   0 (unbuilt) │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ enc_state_h (Dense)             │ (1, 128)               │        16,512 │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ enc_state_c (Dense)             │ (1, 128)               │        16,512 │
└─────────────────────────────────┴────────────────────────┴───────────────┘
 Total params: 361,064 (1.38 MB)
 Trainable params: 361,064 (1.38 MB)
 Non-trainable params: 0 (0.00 B)
Model output shape: (1, 20, 1000)

From what I can tell, I have correctly implemented the build() methods for the encoder & decoder layers. I think this is then causing a TypeError: Unsupported integer size (0) serialization error when I attempt to save the model.
I have included the encoder, decoder and Seq2Seq model classes below, along with the test script for replication. I appreciate its quite a lot of code, but pasting it into a single file and running is enough to replicate the error:
import numpy as np
import tensorflow as tf
from tensorflow.keras import Model
from tensorflow.keras.layers import Dense, Layer, Embedding, Bidirectional, LSTM, Dropout
from tensorflow.keras.optimizers import Adam
from typing import Optional, Tuple, Any

""""""
Encoder Layer
""""""
class SimpleEncoder(Layer):
    def __init__(self, vocab_size: int, embedding_dim: int, units: int, dropout_rate: float = 0.2, **kwargs):
        super(SimpleEncoder, self).__init__(**kwargs)
        self.vocab_size = vocab_size
        self.embedding_dim = embedding_dim
        self.units = units
        self.dropout_rate = dropout_rate

        self.embedding = Embedding(input_dim=vocab_size, output_dim=embedding_dim, mask_zero=True, name='simple_embedding')
        self.dense = Dense(units, activation='relu', name='simple_dense')
        self.dropout = Dropout(dropout_rate, name='simple_dropout')

    def build(self, input_shape):
        self.embedding.build(input_shape)

        embedding_output_shape = self.embedding.compute_output_shape(input_shape)
        self.dense.build(embedding_output_shape)

        dense_output_shape = self.dense.compute_output_shape(embedding_output_shape)
        self.dropout.build(dense_output_shape)

        super(SimpleEncoder, self).build(input_shape)

    def call(self, inputs: tf.Tensor, training: Optional[bool] = None) -> Tuple[tf.Tensor, tf.Tensor, tf.Tensor]:
        x = self.embedding(inputs)  # Shape: (batch_size, sequence_length, embedding_dim)

        encoder_output = self.dense(x)  # Shape: (batch_size, sequence_length, units)

        encoder_output = self.dropout(encoder_output, training=training)

        state_h = tf.zeros_like(encoder_output[:, 0, :])  # Shape: (batch_size, units)
        state_c = tf.zeros_like(encoder_output[:, 0, :])  # Shape: (batch_size, units)

        return encoder_output, state_h, state_c

    def compute_mask(self, inputs: tf.Tensor, mask: Optional[tf.Tensor] = None) -> Optional[tf.Tensor]:
        return self.embedding.compute_mask(inputs, mask)

    def get_config(self) -> dict:
        config = super(SimpleEncoder, self).get_config()
        config.update({
            'vocab_size': self.vocab_size,
            'embedding_dim': self.embedding_dim,
            'units': self.units,
            'dropout_rate': self.dropout_rate,
            'embedding': tf.keras.layers.serialize(self.embedding),
            'dense': tf.keras.layers.serialize(self.dense),
            'dropout': tf.keras.layers.serialize(self.dropout),
        })
        return config

    @classmethod
    def from_config(cls, config: dict) -> 'SimpleEncoder':
        config['embedding'] = tf.keras.layers.deserialize(config['embedding'])
        config['dense'] = tf.keras.layers.deserialize(config['dense'])
        config['dropout'] = tf.keras.layers.deserialize(config['dropout'])
        return cls(**config)

""""""
Decoder Layer
""""""
class SimpleDecoder(Layer):
    def __init__(
        self,
        vocab_size: int,
        embedding_dim: int,
        units: int,
        dropout_rate: float = 0.2,
        **kwargs
    ):
        super(SimpleDecoder, self).__init__(**kwargs)
        self.vocab_size = vocab_size
        self.embedding_dim = embedding_dim
        self.units = units
        self.dropout_rate = dropout_rate

        self.embedding = Embedding(
            input_dim=vocab_size,
            output_dim=embedding_dim,
            mask_zero=True,
            name='decoder_embedding'
        )
        self.lstm = LSTM(
            units,
            return_sequences=True,
            return_state=True,
            name='decoder_lstm'
        )
        self.dropout = Dropout(dropout_rate, name='decoder_dropout')
        self.dense = Dense(vocab_size, activation='softmax', name='decoder_dense')

    def build(self, input_shape):
        decoder_input_shape, initial_states_shape = input_shape

        self.embedding.build(decoder_input_shape)

        embedding_output_shape = self.embedding.compute_output_shape(decoder_input_shape)
        self.lstm.build(embedding_output_shape)

        lstm_output_shape = self.lstm.compute_output_shape(embedding_output_shape)
        self.dropout.build(lstm_output_shape)

        dropout_output_shape = self.dropout.compute_output_shape(lstm_output_shape)
        self.dense.build(dropout_output_shape)

        super(SimpleDecoder, self).build(input_shape)

    def call(
        self,
        inputs: Tuple[tf.Tensor, tuple[tf.Tensor, tf.Tensor]],
        training: Optional[bool] = None,
        mask: Optional[tf.Tensor] = None
    ) -> tf.Tensor:
        decoder_input, initial_state = inputs

        if decoder_input is None or initial_state is None:
            raise ValueError('decoder_input and initial_state must be provided to the Decoder.')

        x = self.embedding(decoder_input)

        lstm_output, state_h, state_c = self.lstm(
            x,
            initial_state=initial_state,
            training=training,
            mask=None
        )

        lstm_output = self.dropout(lstm_output, training=training)

        output = self.dense(lstm_output)

        return output

    @staticmethod
    def compute_mask(inputs: Tuple, mask: Optional[tf.Tensor] = None) -> None:
        return None

    def get_config(self) -> dict:
        config = super(SimpleDecoder, self).get_config()
        config.update({
            'vocab_size': self.vocab_size,
            'embedding_dim': self.embedding_dim,
            'units': self.units,
            'dropout_rate': self.dropout_rate,
            'embedding': tf.keras.layers.serialize(self.embedding),
            'lstm': tf.keras.layers.serialize(self.lstm),
            'dropout': tf.keras.layers.serialize(self.dropout),
            'dense': tf.keras.layers.serialize(self.dense),
        })
        return config

    @classmethod
    def from_config(cls, config: dict) -> 'SimpleDecoder':
        config['embedding'] = tf.keras.layers.deserialize(config['embedding'])
        config['lstm'] = tf.keras.layers.deserialize(config['lstm'])
        config['dropout'] = tf.keras.layers.deserialize(config['dropout'])
        config['dense'] = tf.keras.layers.deserialize(config['dense'])
        return cls(**config)

""""""
Seq2Seq Model
""""""
class RetrosynthesisSeq2SeqModel(Model):
    def __init__(self, input_vocab_size: int, output_vocab_size: int, encoder_embedding_dim: int,
                 decoder_embedding_dim: int, units: int, dropout_rate: float = 0.2, *args, **kwargs):
        super(RetrosynthesisSeq2SeqModel, self).__init__(*args, **kwargs)

        self.units: int = units

        self.encoder: SimpleEncoder = SimpleEncoder(
            input_vocab_size, encoder_embedding_dim, units, dropout_rate
        )

        self.decoder: SimpleDecoder = SimpleDecoder(
            output_vocab_size, decoder_embedding_dim, units, dropout_rate
        )

        self.input_vocab_size: int = input_vocab_size
        self.output_vocab_size: int = output_vocab_size

        self.enc_state_h: Dense = Dense(units, name='enc_state_h')
        self.enc_state_c: Dense = Dense(units, name='enc_state_c')

        self.encoder_data_processor: Optional[Any] = None
        self.decoder_data_processor: Optional[Any] = None

        self.dropout_rate: float = dropout_rate

    def build(self, input_shape):
        encoder_input_shape, decoder_input_shape = input_shape

        encoder_dummy = tf.zeros(encoder_input_shape)
        decoder_dummy = tf.zeros(decoder_input_shape)

        self.call((encoder_dummy, decoder_dummy), training=False)

        super(RetrosynthesisSeq2SeqModel, self).build(input_shape)

    def call(self, inputs: Tuple[tf.Tensor, tf.Tensor], training: Optional[bool] = None) -> tf.Tensor:
        encoder_input, decoder_input = inputs

        encoder_output, state_h, state_c = self.encoder.call(encoder_input, training=training)

        decoder_initial_state_h: tf.Tensor = self.enc_state_h(state_h)
        decoder_initial_state_c: tf.Tensor = self.enc_state_c(state_c)
        decoder_initial_state: Tuple[tf.Tensor, tf.Tensor] = (decoder_initial_state_h, decoder_initial_state_c)

        decoder_inputs = (
            decoder_input,
            decoder_initial_state
        )

        encoder_mask: Optional[tf.Tensor] = self.encoder.compute_mask(encoder_input)

        output: tf.Tensor = self.decoder.call(
            decoder_inputs,
            training=training,
            mask=encoder_mask
        )

        return output

    def get_config(self) -> dict:
        config = super(RetrosynthesisSeq2SeqModel, self).get_config()
        config.update({
            'units': self.units,
            'input_vocab_size': self.input_vocab_size,
            'output_vocab_size': self.output_vocab_size,
            'encoder_embedding_dim': self.encoder.embedding.output_dim,
            'decoder_embedding_dim': self.decoder.embedding.output_dim,
            'dropout_rate': self.dropout_rate,
            'encoder': tf.keras.layers.serialize(self.encoder),
            'decoder': tf.keras.layers.serialize(self.decoder),
            'enc_state_h': tf.keras.layers.serialize(self.enc_state_h),
            'enc_state_c': tf.keras.layers.serialize(self.enc_state_c)
        })
        return config

    @classmethod
    def from_config(cls, config: dict) -> 'RetrosynthesisSeq2SeqModel':
        config['encoder'] = tf.keras.layers.deserialize(config['encoder'])
        config['decoder'] = tf.keras.layers.deserialize(config['decoder'])
        config['enc_state_h'] = tf.keras.layers.deserialize(config['enc_state_h'])
        config['enc_state_c'] = tf.keras.layers.deserialize(config['enc_state_c'])
        return cls(**config)

""""""
Test Script
""""""
input_vocab_size = 1000
output_vocab_size = 1000
encoder_embedding_dim = 32
decoder_embedding_dim = 64
units = 128
dropout_rate = 0.2

model = RetrosynthesisSeq2SeqModel(
    input_vocab_size=input_vocab_size,
    output_vocab_size=output_vocab_size,
    encoder_embedding_dim=encoder_embedding_dim,
    decoder_embedding_dim=decoder_embedding_dim,
    units=units,
    dropout_rate=dropout_rate
)

encoder_seq_length = 20
decoder_seq_length = 20
model.build(input_shape=[(1, encoder_seq_length), (1, decoder_seq_length)])

sample_encoder_input = np.random.randint(0, input_vocab_size, size=(1, 20))
sample_decoder_input = np.random.randint(0, output_vocab_size, size=(1, 20))

learning_rate: float = 0.0001
optimizer: Adam = Adam(learning_rate=learning_rate, clipnorm=5.0)

model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])

model.summary()

output = model([sample_encoder_input, sample_decoder_input])
print(""Model output shape:"", output.shape)

model.save('minimal_seq2seq_model.keras')
print(""Model saved successfully."")

","Encoder and decoder were unbuilt as I was invoking the call() method directly:
encoder_output, state_h, state_c = self.encoder.call(encoder_input, training=training)

output: tf.Tensor = self.decoder(
            decoder_inputs,
            training=training,
            mask=encoder_mask
        )

After changing this to invoke the layers directly, the encoder and decoder layers are showing as built in model.summary() output:
encoder_output, state_h, state_c = self.encoder(encoder_input, training=training)

output: tf.Tensor = self.decoder(
            decoder_inputs,
            training=training,
            mask=encoder_mask
        )

From what I can tell, invoking the layer's call() method directly bypasses Keras's internal mechanisms for tracking and building layers, resulting in them not being properly built and tracked.
However the TypeError: Unsupported integer size (0) error when attempting to save the model in .keras format via model.save() persists. Must be separate serialization issue in my model.
",tensorflow
how to stop training when it hits a specific validation accuracy,"I am training a convolutional network and I want to stop training once the validation error hits 90%. I thought about using EarlyStopping and setting baseline to .90 but then it stops training whenever the validation accuracy is below that baseline for given number of epochs(which is just 0 here). So my code is:
es=EarlyStopping(monitor='val_acc',mode='auto',verbose=1,baseline=.90,patience=0)
history = model.fit(training_images, training_labels, validation_data=(test_images, test_labels), epochs=30, verbose=2,callbacks=[es])

When I use this code my training stops after the first epoch with given results:
Train on 60000 samples, validate on 10000 samples

Epoch 1/30
60000/60000 - 7s - loss: 0.4600 - acc: 0.8330 - val_loss: 0.3426 - val_acc: 0.8787

What else can I try to stop my training once the validation accuracy hits 90% or above?
Here is the rest of the code:
  tf.keras.layers.Conv2D(64, (3,3), activation='relu', input_shape=(28, 28, 1)),
  tf.keras.layers.MaxPooling2D(2, 2),
  tf.keras.layers.Conv2D(32, (3,3), activation='relu', input_shape=(28, 28, 1)),
  tf.keras.layers.MaxPooling2D(2, 2),
  tf.keras.layers.Flatten(),
  tf.keras.layers.Dense(152, activation='relu'),
  tf.keras.layers.Dense(10, activation='softmax')
])

model.compile(optimizer=Adam(learning_rate=0.001),loss='sparse_categorical_crossentropy', metrics=['accuracy'])
es=EarlyStopping(monitor='val_acc',mode='auto',verbose=1,baseline=.90,patience=0)
history = model.fit(training_images, training_labels, validation_data=(test_images, test_labels), epochs=30, verbose=2,callbacks=[es])

","Early Stopping Callback will search for a value that stopped increasing (or decreasing) so it's not a good use for your problem. However tf.keras allows you to use custom callbacks.
For your example:
class MyThresholdCallback(tf.keras.callbacks.Callback):
    def __init__(self, threshold):
        super(MyThresholdCallback, self).__init__()
        self.threshold = threshold

    def on_epoch_end(self, epoch, logs=None): 
        val_acc = logs[""val_acc""]
        if val_acc >= self.threshold:
            self.model.stop_training = True

For TF version 2.3 or above, you might have to use ""val_accuracy"" instead of ""val_acc"". Thank you Christian Westbrook for the note in the comments.
The above Callback, on each epoch end, will extract Validation Accuracy from all available logs. Then it will compare it with user defined threshold (in your case 90%). If the criterion is met the training will be stopped.
With that you can simply call:
my_callback = MyThresholdCallback(threshold=0.9)
history = model.fit(training_images, training_labels, validation_data=(test_images, test_labels), epochs=30, verbose=2, callbacks=[my_callback])

Alternatively, you can use def on_batch_end(...) if you want to stop immediately.    This however, requires parameters batch, logs instead of epoch, logs.
",tensorflow
multiclass classifier using keras from 39deep learning with python39 yields very different accuracy compared to what39s in the textbook,"Below is the code for multi class classifier from Chapter 4 in 'Deep Learning with Python' by François Chollet. The textbook mentions this code will yield >95% training accuracy, but my environment seems to yield very low accuracy of <50% compared to the textbook.
Keras version - 3.6
Tensorflow - 2.18
Hardware - Apple M1 Pro
import keras
from tensorflow.keras.datasets import reuters
from tensorflow.keras.utils import to_categorical
from tensorflow.keras import layers
import matplotlib.pyplot as plt
import numpy as np

(train_data, train_labels), (test_data, test_labels) = reuters.load_data(num_words=10000)


def vectorize_sequences(sequences, dimension=10000):
    results = np.zeros((len(sequences), dimension))
    for i, sequence in enumerate(sequences):
        for j in sequence:
            results[i, j] = 1.
        return results


x_train = vectorize_sequences(train_data)
x_test = vectorize_sequences(test_data)

y_train = to_categorical(train_labels)
y_test = to_categorical(test_labels)

model = keras.Sequential([
    layers.Dense(64, activation=""relu""),
    layers.Dense(64, activation=""relu""),
    layers.Dense(46, activation=""softmax"")
])

model.compile(
    optimizer=""rmsprop"",
    loss=""categorical_crossentropy"",
    metrics=[""accuracy""]
)

# setting aside validation set
x_val = x_train[:1000]
partial_x_train = x_train[1000:]
y_val = y_train[:1000]
partial_y_train = y_train[1000:]

# training the model

history = model.fit(
    partial_x_train,
    partial_y_train,
    epochs=10,
    batch_size=512,
    validation_data=(x_val, y_val)
)

# plotting training & validation accuracy
history_dict = history.history
loss_values = history_dict[""loss""]
val_loss_values = history_dict[""val_loss""]
epochs = range(1, len(loss_values) + 1)
acc = history_dict[""accuracy""]
val_acc = history_dict[""val_accuracy""]
plt.plot(epochs, acc, ""bo"", label=""Training acc"")
plt.plot(epochs, val_acc, ""b"", label=""Validation acc"")
plt.xlabel(""Epochs"")
plt.ylabel(""Accuracy"")
plt.legend()
plt.show()


","Firstly,
There is an indentation problem. The vectorize_sequence return is indented incorrectly.
Should be this:
def vectorize_sequences(sequences, dimension=10000):
    results = np.zeros((len(sequences), dimension))
    for i, sequence in enumerate(sequences):
        for j in sequence:
            results[i, j] = 1.
    return results

Secondly:
Import layers:
from tensorflow.keras import layers

Thirdly:
You need to have the labels in categorical format:
y_train = to_categorical(train_labels)
y_test = to_categorical(test_labels)

Then you should get an accuracy of mid 80's as stated in the textbook. I looked at the textbook and it says 95% is for state of the art methods not the naïve approach defined in the book. (Unless you were referring to training accuracy? In which case, yes, >95% is correct)

",tensorflow
typeerror unsupported integer size 0 when attempted to save custom keras model,"I have a subclassed tensorflow.keras.Model Seq2Seq model with custom layers. However when I try to save it via tensorflow.keras.Model.save() method, it throwing the following error:
  File ""/home/anaconda3/envs/aizynth-env/lib/python3.10/site-packages/h5py/_hl/dataset.py"", line 86, in make_new_dset
    tid = h5t.py_create(dtype, logical=1)
  File ""h5py/h5t.pyx"", line 1663, in h5py.h5t.py_create
  File ""h5py/h5t.pyx"", line 1687, in h5py.h5t.py_create
  File ""h5py/h5t.pyx"", line 1705, in h5py.h5t.py_create
  File ""h5py/h5t.pyx"", line 1459, in h5py.h5t._c_int
TypeError: Unsupported integer size (0)

Process finished with exit code 1

From what I understand, this issue stems from the HDF5 format trying to serialize a layer or configuration parameter that it doesn't recognise or cannot handle.
My Tensorflow version is 2.17.0
Code for Minimal Reproducible Example
Encoder Layer
import tensorflow as tf
from tensorflow.keras.layers import Layer, Embedding, Bidirectional, LSTM, Dropout
from typing import Tuple, Optional

class StackedBidirectionalLSTMEncoder(Layer):
    def __init__(self, vocab_size: int, encoder_embedding_dim: int, units: int, dropout_rate: float = 0.2, **kwargs):
        super(StackedBidirectionalLSTMEncoder, self).__init__(**kwargs)
        self.units: int = units
        self.embedding: Embedding = Embedding(vocab_size, encoder_embedding_dim, mask_zero=True)
        self.dropout_rate: float = dropout_rate

        self.bidirectional_lstm_1: Bidirectional = Bidirectional(
            LSTM(units, return_sequences=True, return_state=True),
            name='bidirectional_lstm_1'
        )

        self.dropout_1: Dropout = Dropout(dropout_rate, name='encoder_dropout_1')

        self.bidirectional_lstm_2: Bidirectional = Bidirectional(
            LSTM(units, return_sequences=True, return_state=True),
            name='bidirectional_lstm_2'
        )

        self.dropout_2: Dropout = Dropout(dropout_rate, name='encoder_dropout_2')

    def call(self, encoder_input: tf.Tensor, training: Optional[bool] = None):
        # Embed the input and obtain mask
        encoder_output: tf.Tensor = self.embedding(encoder_input)
        mask = self.embedding.compute_mask(encoder_input)

        # Process through encoder layers
        # First LSTM layer
        encoder_output, forward_h, forward_c, backward_h, backward_c = self.bidirectional_lstm_1(
            encoder_output, mask=mask, training=training
        )
        # Concatenate forward and backward states
        state_h_1: tf.Tensor = tf.concat([forward_h, backward_h], axis=-1)
        state_c_1: tf.Tensor = tf.concat([forward_c, backward_c], axis=-1)

        # Apply dropout
        encoder_output: Optional[tf.Tensor] = self.dropout_1(encoder_output, training=training)

        # Second LSTM layer
        encoder_output, forward_h, forward_c, backward_h, backward_c = self.bidirectional_lstm_2(
            encoder_output, mask=mask, training=training
        )

        # Concatenate forward and backward states
        state_h_2: tf.Tensor = tf.concat([forward_h, backward_h], axis=-1)
        state_c_2: tf.Tensor = tf.concat([forward_c, backward_c], axis=-1)

        # Apply dropout
        encoder_output: tf.Tensor = self.dropout_2(encoder_output, training=training)

        # Final states
        final_state_h: tf.Tensor = state_h_2
        final_state_c: tf.Tensor = state_c_2

        return encoder_output, final_state_h, final_state_c

    def compute_mask(self, inputs: tf.Tensor, mask: Optional[tf.Tensor] = None) -> Optional[tf.Tensor]:
        return self.embedding.compute_mask(inputs, mask)

    def get_config(self) -> dict:
        config = super(StackedBidirectionalLSTMEncoder, self).get_config()
        config.update({
            'vocab_size': self.embedding.input_dim,
            'encoder_embedding_dim': self.embedding.output_dim,
            'units': self.units,
            'dropout_rate': self.dropout_rate,
            'embedding': tf.keras.layers.serialize(self.embedding),
            'bidirectional_lstm_1': tf.keras.layers.serialize(self.bidirectional_lstm_1),
            'dropout_1': tf.keras.layers.serialize(self.dropout_1),
            'bidirectional_lstm_2': tf.keras.layers.serialize(self.bidirectional_lstm_2),
            'dropout_2': tf.keras.layers.serialize(self.dropout_2),
        })
        return config

    @classmethod
    def from_config(cls, config: dict) -> 'StackedBidirectionalLSTMEncoder':
        # Deserialize layers
        config['embedding'] = tf.keras.layers.deserialize(config['embedding'])
        config['bidirectional_lstm_1'] = tf.keras.layers.deserialize(config['bidirectional_lstm_1'])
        config['dropout_1'] = tf.keras.layers.deserialize(config['dropout_1'])
        config['bidirectional_lstm_2'] = tf.keras.layers.deserialize(config['bidirectional_lstm_2'])
        config['dropout_2'] = tf.keras.layers.deserialize(config['dropout_2'])
        return cls(**config)

Decoder Layer
import tensorflow as tf
from tensorflow.keras.layers import Layer, Embedding, LSTM, Dropout, Dense
from typing import List, Optional, Tuple, Union, Any


class StackedLSTMDecoder(Layer):
    def __init__(self, vocab_size: int, decoder_embedding_dim: int, units: int, dropout_rate: float = 0.2,
                 **kwargs) -> None:
        super(StackedLSTMDecoder, self).__init__(**kwargs)
        self.units: int = units
        self.embedding: Embedding = Embedding(vocab_size, decoder_embedding_dim, mask_zero=True)
        self.vocab_size: int = vocab_size
        self.dropout_rate: float = dropout_rate

        # Decoder: 4-layer LSTM without internal Dropout
        # Define LSTM and Dropout layers individually
        self.lstm_decoder_1: LSTM = LSTM(
            units,
            return_sequences=True,
            return_state=True,
            name='lstm_decoder_1'
        )
        self.dropout_1: Dropout = Dropout(dropout_rate, name='decoder_dropout_1')

        self.lstm_decoder_2: LSTM = LSTM(
            units,
            return_sequences=True,
            return_state=True,
            name='lstm_decoder_2'
        )
        self.dropout_2: Dropout = Dropout(dropout_rate, name='decoder_dropout_2')

        self.lstm_decoder_3: LSTM = LSTM(
            units,
            return_sequences=True,
            return_state=True,
            name='lstm_decoder_3'
        )
        self.dropout_3: Dropout = Dropout(dropout_rate, name='decoder_dropout_3')

        self.lstm_decoder_4: LSTM = LSTM(
            units,
            return_sequences=True,
            return_state=True,
            name='lstm_decoder_4'
        )
        self.dropout_4: Dropout = Dropout(dropout_rate, name='decoder_dropout_4')

        # Attention Mechanism
        self.attention: BahdanauAttention = BahdanauAttention(units=units)

        # Output layer
        self.dense: Dense = Dense(vocab_size, activation='softmax')

    def call(self, inputs: Tuple[tf.Tensor, List[tf.Tensor], tf.Tensor], training: Optional[bool] = None,
             mask: Optional[tf.Tensor] = None) -> tf.Tensor:
        # Extract initial state and encoder output from inputs
        decoder_input, initial_state, encoder_output = inputs

        if decoder_input is None or initial_state is None or encoder_output is None:
            raise ValueError('decoder_input, initial_state and encoder_output must be provided to the Decoder.')

        # Embed the input and extract decoder mask
        decoder_output: tf.Tensor = self.embedding(decoder_input)
        decoder_mask: Optional[tf.Tensor] = self.embedding.compute_mask(decoder_input)

        # Process through decoder layers
        # First LSTM layer with initial state
        decoder_output, _, _ = self.lstm_decoder_1(
            decoder_output,
            mask=decoder_mask,
            initial_state=initial_state,
            training=training
        )
        decoder_output: tf.Tensor = self.dropout_1(decoder_output, training=training)

        # Second LSTM layer
        decoder_output, _, _ = self.lstm_decoder_2(
            decoder_output,
            mask=decoder_mask,
            training=training
        )
        decoder_output: tf.Tensor = self.dropout_2(decoder_output, training=training)

        # Third LSTM layer
        decoder_output, _, _ = self.lstm_decoder_3(
            decoder_output,
            mask=decoder_mask,
            training=training
        )
        decoder_output: tf.Tensor = self.dropout_3(decoder_output, training=training)

        # Fourth LSTM layer
        decoder_output, final_state_h, final_state_c = self.lstm_decoder_4(
            decoder_output,
            mask=decoder_mask,
            training=training
        )
        decoder_output: tf.Tensor = self.dropout_4(decoder_output, training=training)

        # Extract only the encoder_mask from the mask list
        if mask is not None and isinstance(mask, (list, tuple)):
            encoder_mask = mask[1]
        else:
            encoder_mask = mask

        # Apply attention
        context_vector, attention_weights = self.attention(
            inputs=[encoder_output, decoder_output],
            mask=encoder_mask
        )

        # Concatenate decoder outputs and context vector
        concat_output: tf.Tensor = tf.concat([decoder_output, context_vector], axis=-1)  # (batch_size, seq_len_dec, units + units_enc)

        # Generate outputs
        decoder_output: tf.Tensor = self.dense(concat_output)  # (batch_size, seq_len_dec, vocab_size)

        return decoder_output

    @staticmethod
    def compute_mask(inputs: Any, mask: Optional[Any] = None) -> None:
        return None

    def get_config(self) -> dict:
        config = super(StackedLSTMDecoder, self).get_config()
        config.update({
            'vocab_size': self.vocab_size,
            'decoder_embedding_dim': self.embedding.output_dim,
            'units': self.units,
            'dropout_rate': self.dropout_rate,
            'embedding': tf.keras.layers.serialize(self.embedding),
            'lstm_decoder_1': tf.keras.layers.serialize(self.lstm_decoder_1),
            'dropout_1': tf.keras.layers.serialize(self.dropout_1),
            'lstm_decoder_2': tf.keras.layers.serialize(self.lstm_decoder_2),
            'dropout_2': tf.keras.layers.serialize(self.dropout_2),
            'lstm_decoder_3': tf.keras.layers.serialize(self.lstm_decoder_3),
            'dropout_3': tf.keras.layers.serialize(self.dropout_3),
            'lstm_decoder_4': tf.keras.layers.serialize(self.lstm_decoder_4),
            'dropout_4': tf.keras.layers.serialize(self.dropout_4),
            'attention': tf.keras.layers.serialize(self.attention),
            'dense': tf.keras.layers.serialize(self.dense),
        })
        return config

    @classmethod
    def from_config(cls, config: dict) -> 'StackedLSTMDecoder':
        # Deserialize layers
        config['embedding'] = tf.keras.layers.deserialize(config['embedding'])
        config['lstm_decoder_1'] = tf.keras.layers.deserialize(config['lstm_decoder_1'])
        config['dropout_1'] = tf.keras.layers.deserialize(config['dropout_1'])
        config['lstm_decoder_2'] = tf.keras.layers.deserialize(config['lstm_decoder_2'])
        config['dropout_2'] = tf.keras.layers.deserialize(config['dropout_2'])
        config['lstm_decoder_3'] = tf.keras.layers.deserialize(config['lstm_decoder_3'])
        config['dropout_3'] = tf.keras.layers.deserialize(config['dropout_3'])
        config['lstm_decoder_4'] = tf.keras.layers.deserialize(config['lstm_decoder_4'])
        config['dropout_4'] = tf.keras.layers.deserialize(config['dropout_4'])
        config['attention'] = tf.keras.layers.deserialize(config['attention'])
        config['dense'] = tf.keras.layers.deserialize(config['dense'])
        return cls(**config)

Attention Layer
import tensorflow as tf
from tensorflow.keras.layers import Layer, Dense
from attention.attention_interface import AttentionInterface
from typing import List, Optional, Tuple, Union


class BahdanauAttention(Layer):
    def __init__(self, units: int, **kwargs):
        super(BahdanauAttention, self).__init__(**kwargs)
        self.units: int = units
        self.attention_dense1: Dense = Dense(units, name='attention_dense1')
        self.attention_dense2: Dense = Dense(units, name='attention_dense2')
        self.attention_v: Dense = Dense(1, name='attention_v')
        self.supports_masking: bool = True

    def call(self, inputs: List[tf.Tensor], mask: Optional[tf.Tensor] = None,
             training: Union[None, bool] = None) -> Tuple[tf.Tensor, tf.Tensor]:
        # Unpack inputs
        encoder_output, decoder_output = inputs

        # Attention Mechanism
        # Calculate attention scores
        # Expand dimensions to match the shapes for broadcasting
        encoder_output_expanded: tf.Tensor = tf.expand_dims(encoder_output,
                                                 1)  # Shape: (batch_size, 1, seq_len_encoder, units*2)
        decoder_output_expanded: tf.Tensor = tf.expand_dims(decoder_output,
                                                 2)  # Shape: (batch_size, seq_len_decoder, 1, units)

        # Compute the attention scores
        score: tf.Tensor = tf.nn.tanh(
            self.attention_dense1(encoder_output_expanded) + self.attention_dense2(decoder_output_expanded)
        )  # Shape: (batch_size, seq_len_decoder, seq_len_encoder, units)

        # Apply mask if available
        if mask is not None:
            # If mask is a list or tuple, both encoder and decoder mask have been passed.
            # Extract the encoder mask
            if isinstance(mask, (list, tuple)):
                encoder_mask: tf.Tensor = mask[0]
            else:
                encoder_mask = mask
            if encoder_mask is not None:
                # mask shape: (batch_size, seq_len_encoder)
                # Expand mask to match score dimensions
                encoder_mask = tf.cast(tf.expand_dims(encoder_mask, 1), dtype=score.dtype)  # (batch_size, 1, seq_len_encoder)
                encoder_mask = tf.expand_dims(encoder_mask, -1)  # (batch_size, 1, seq_len_encoder, 1)
                # Add a large negative value to masked positions to nullify their effect after softmax
                score += (1.0 - encoder_mask) * -1e9

        attention_weights: tf.Tensor = tf.nn.softmax(self.attention_v(score),
                                          axis=2)  # Shape: (batch_size, seq_len_decoder, seq_len_encoder, 1)

        # Compute the context vector
        context_vector: tf.Tensor = attention_weights * encoder_output_expanded  # Shape: (batch_size, seq_len_decoder, seq_len_encoder, units*2)
        context_vector: tf.Tensor = tf.reduce_sum(context_vector, axis=2)  # Shape: (batch_size, seq_len_decoder, units*2)

        return context_vector, attention_weights

    @staticmethod
    def compute_mask(inputs: List[tf.Tensor], mask: Optional[tf.Tensor] = None) -> None:
        # This layer does not propagate the mask further
        return None

    def get_config(self) -> dict:
        config = super(BahdanauAttention, self).get_config()
        config.update({
            'units': self.units,
            'attention_dense1': tf.keras.layers.serialize(self.attention_dense1),
            'attention_dense2': tf.keras.layers.serialize(self.attention_dense2),
            'attention_v': tf.keras.layers.serialize(self.attention_v),
        })
        return config

    @classmethod
    def from_config(cls, config: dict) -> 'BahdanauAttention':
        # Deserialize layers
        config['attention_dense1'] = tf.keras.layers.deserialize(config['attention_dense1'])
        config['attention_dense2'] = tf.keras.layers.deserialize(config['attention_dense2'])
        config['attention_v'] = tf.keras.layers.deserialize(config['attention_v'])
        return cls(**config)

Seq2Seq Model
import tensorflow as tf
from tensorflow.keras import Model
from tensorflow.keras.layers import Dense
from tensorflow.train import Checkpoint, CheckpointManager
from tensorflow.keras.callbacks import Callback
from typing import Optional, Any, Tuple

class RetrosynthesisSeq2SeqModel(Model):
    def __init__(self, input_vocab_size: int, output_vocab_size: int, encoder_embedding_dim: int,
                 decoder_embedding_dim: int, units: int, dropout_rate: float = 0.2, *args, **kwargs):
        super(RetrosynthesisSeq2SeqModel, self).__init__(*args, **kwargs)

        # Save the number of units (neurons)
        self.units: int = units

        # Encoder layer
        self.encoder: StackedBidirectionalLSTMEncoder = StackedBidirectionalLSTMEncoder(
            input_vocab_size, encoder_embedding_dim, units, dropout_rate
        )

        # Decoder layer
        self.decoder: StackedLSTMDecoder = StackedLSTMDecoder(
            output_vocab_size, decoder_embedding_dim, units, dropout_rate
        )

        # Save the vocabulary sizes
        self.input_vocab_size: int = input_vocab_size
        self.output_vocab_size: int = output_vocab_size

        # Mapping encoder final states to decoder initial states
        self.enc_state_h: Dense = Dense(units, name='enc_state_h')
        self.enc_state_c: Dense = Dense(units, name='enc_state_c')

        # Store the data processors (to be set externally)
        self.encoder_data_processor: Optional[Any] = None
        self.decoder_data_processor: Optional[Any] = None

        # Save the dropout rate
        self.dropout_rate: float = dropout_rate

    def build(self, input_shape):
        # Define the input shapes for encoder and decoder
        encoder_input_shape, decoder_input_shape = input_shape

        # Pass a dummy input through encoder and decoder to initialize weights
        encoder_dummy = tf.zeros(encoder_input_shape)
        decoder_dummy = tf.zeros(decoder_input_shape)

        # Forward pass to build the model
        self.call((encoder_dummy, decoder_dummy), training=False)

        # Mark the model as built
        super(RetrosynthesisSeq2SeqModel, self).build(input_shape)

    def call(self, inputs: Tuple[tf.Tensor, tf.Tensor], training: Optional[bool] = None) -> tf.Tensor:
        """"""
        Forward pass of the Seq2Seq model.

        Args:
            inputs (Tuple[tf.Tensor, tf.Tensor]): Tuple containing encoder and decoder inputs.
            training (Optional[bool], optional): Training flag. Defaults to None.

        Returns:
            tf.Tensor: The output predictions from the decoder.
        """"""
        # Extract encoder and decoder inputs
        encoder_input, decoder_input = inputs

        # Encoder
        encoder_output, state_h, state_c = self.encoder.call(encoder_input, training=training)

        # Map encoder final states to decoder initial states
        decoder_initial_state_h: tf.Tensor = self.enc_state_h(state_h)  # (batch_size, units)
        decoder_initial_state_c: tf.Tensor = self.enc_state_c(state_c)  # (batch_size, units)
        decoder_initial_state: Tuple[tf.Tensor, tf.Tensor] = (decoder_initial_state_h, decoder_initial_state_c)

        # Prepare decoder inputs as a tuple
        decoder_inputs: Tuple[tf.Tensor, Tuple[tf.Tensor, tf.Tensor], tf.Tensor] = (
            decoder_input,
            decoder_initial_state,
            encoder_output
        )

        # Extract encoder mask
        encoder_mask: Optional[tf.Tensor] = self.encoder.compute_mask(encoder_input)

        # Decoder
        output: tf.Tensor = self.decoder.call(
            decoder_inputs,
            training=training,
            mask=encoder_mask
        )

        return output

    def get_config(self) -> dict:
        config = super(RetrosynthesisSeq2SeqModel, self).get_config()
        config.update({
            'units': self.units,
            'input_vocab_size': self.input_vocab_size,
            'output_vocab_size': self.output_vocab_size,
            'encoder_embedding_dim': self.encoder.embedding.output_dim,
            'decoder_embedding_dim': self.decoder.embedding.output_dim,
            'dropout_rate': self.dropout_rate,
            'encoder': tf.keras.layers.serialize(self.encoder),
            'decoder': tf.keras.layers.serialize(self.decoder),
            'enc_state_h': tf.keras.layers.serialize(self.enc_state_h),
            'enc_state_c': tf.keras.layers.serialize(self.enc_state_c)
        })
        return config

    @classmethod
    def from_config(cls, config: dict) -> 'RetrosynthesisSeq2SeqModel':
        # Deserialize layers
        config['encoder'] = tf.keras.layers.deserialize(config['encoder'])
        config['decoder'] = tf.keras.layers.deserialize(config['decoder'])
        config['enc_state_h'] = tf.keras.layers.deserialize(config['enc_state_h'])
        config['enc_state_c'] = tf.keras.layers.deserialize(config['enc_state_c'])
        return cls(**config)

Minimal Reproducible Example Script
#!/usr/bin/env python3

import numpy as np
from tensorflow.keras.optimizers import Adam

input_vocab_size = 1000
output_vocab_size = 1000
encoder_embedding_dim = 32
decoder_embedding_dim = 64
units = 128
dropout_rate = 0.2

model = RetrosynthesisSeq2SeqModel(
    input_vocab_size=input_vocab_size,
    output_vocab_size=output_vocab_size,
    encoder_embedding_dim=encoder_embedding_dim,
    decoder_embedding_dim=decoder_embedding_dim,
    units=units,
    dropout_rate=dropout_rate
)

encoder_input_shape = (1, 20)  # (batch_size, sequence_length)
decoder_input_shape = (1, 20)  # (batch_size, sequence_length)

model.build([encoder_input_shape, decoder_input_shape])

sample_encoder_input = np.random.randint(0, input_vocab_size, size=(1, 20))
sample_decoder_input = np.random.randint(0, output_vocab_size, size=(1, 20))

learning_rate: float = 0.0001
optimizer: Adam = Adam(learning_rate=learning_rate, clipnorm=5.0)

model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])

output = model([sample_encoder_input, sample_decoder_input])
print(""Model output shape:"", output.shape)

model.save('minimal_seq2seq_model.keras')
print(""Model saved successfully."")

","I had this issue too, but I wasn't using custom layers. What solved it for me was creating a new conda environment and downgrading to TensorFlow 2.12.0 and h5py 3.6.0.
conda create -n tf_env python=3.10
conda activate tf_env
conda install h5py=3.6.0
conda install tensorflow[and-cuda]

You can specify the TensorFlow version too, but mine defaulted to downloading 2.12.0.
Hope this helps!
",tensorflow
how to save and reload a subclassed model in tf 260  python 397 wihtout performance drop,"Looks like the million dollars question. I have the model below built by sub classing Model in Keras.
Model trains fine and have good performance but I cannot find a way to save and restore the model without incurring a significant performance loss.
I track AUC on ROC curves for anomaly detection, and the ROC curve after loading the model is worse than before, using exactly the same validation data set.
I suspect the problem to come from the BatchNormalization, but I could be wrong.
I've tried several option:
This works but leads to performance drop.
model.save() / tf.keras.models.load()

This works but also lead to performance drop:
model.save_weights() / model.load_weights()

This does not work and I get the following error:
tf.saved_model.save() / tf.saved_model.load()

AttributeError: '_UserObject' object has no attribute 'predict'

This does not work either, as Subclassed model do not support json export:
model.to_json()

Here is the model:
class Deep_Seq2Seq_Detector(Model):
  def __init__(self, flight_len, param_len, hidden_state=16):
    super(Deep_Seq2Seq_Detector, self).__init__()
    self.input_dim = (None, flight_len, param_len)
    self._name_ = ""LSTM""
    self.units = hidden_state
    
    self.regularizer0 = tf.keras.Sequential([
        layers.BatchNormalization()
        ])
    
    self.encoder1 = layers.LSTM(self.units,
                  return_state=False,
                  return_sequences=True,
                  #activation=""tanh"",
                  name='encoder1',
                  input_shape=self.input_dim)#,
                  #kernel_regularizer= tf.keras.regularizers.l1(),
                  #)
    
    self.regularizer1 = tf.keras.Sequential([
        layers.BatchNormalization(),
        layers.Activation(""tanh"")
        ])
    
    self.encoder2 = layers.LSTM(self.units,
                  return_state=False,
                  return_sequences=True,
                  #activation=""tanh"",
                  name='encoder2')#,
                  #kernel_regularizer= tf.keras.regularizers.l1()
                  #) #                    input_shape=(None, self.input_dim[1],self.units),
    
    self.regularizer2 = tf.keras.Sequential([
        layers.BatchNormalization(),
        layers.Activation(""tanh"")
        ])
    
    self.encoder3 = layers.LSTM(self.units,
                  return_state=True,
                  return_sequences=False,
                  activation=""tanh"",
                  name='encoder3')#,
                  #kernel_regularizer= tf.keras.regularizers.l1(),
                  #) #                   input_shape=(None, self.input_dim[1],self.units),
    
    self.repeat = layers.RepeatVector(self.input_dim[1])
    
    self.decoder = layers.LSTM(self.units,
                  return_sequences=True,
                  activation=""tanh"",
                  name=""decoder"",
                  input_shape=(self.input_dim[1],self.units))
    
    self.dense = layers.TimeDistributed(layers.Dense(self.input_dim[2]))

  @tf.function 
  def call(self, x):
    
    # Encoder
    x0 = self.regularizer0(x)
    x1 = self.encoder1(x0)
    x11 = self.regularizer1(x1)
    
    x2 = self.encoder2(x11)
    x22 = self.regularizer2(x2)
    
    output, hs, cs = self.encoder3(x22)
    
    # see https://www.tensorflow.org/guide/keras/rnn 
    encoded_state = [hs, cs] 
    repeated_vec = self.repeat(output)
    
    # Decoder
    decoded = self.decoder(repeated_vec, initial_state=encoded_state)
    output_decoder = self.dense(decoded)

    return output_decoder

I've seen Git threads, but no straight answer:
https://github.com/keras-team/keras/issues/4875
Did anyone found a solution ? Do I have to use the Functional or Sequential API instead ?
","It seems the problem was coming from the Subclassing API.
I reconstructed the exact same model using the Functional API and now model.save / model.load yields similar results.
",tensorflow
error in prediction step after imported a keras model to tensorflow java,"my goal is to use a Keras model in a java program.
I export the keras model with model.export() and not model.save() so I get well a folder with the model in .pb format.
Then I used py .\saved_model_cli.py show -- dir '.' -all  to see the inputs and outputs to fill in the java code.
I get that :
MetaGraphDef with tag-set: 'serve' contains the following SignatureDefs:

signature_def['__saved_model_init_op']:
  The given SavedModel SignatureDef contains the following input(s):
  The given SavedModel SignatureDef contains the following output(s):
    outputs['__saved_model_init_op'] tensor_info:
        dtype: DT_INVALID
        shape: unknown_rank
        name: NoOp
  Method name is:

signature_def['serve']:
  The given SavedModel SignatureDef contains the following input(s):
    inputs['keras_tensor'] tensor_info:
        dtype: DT_FLOAT
        shape: (-1, 6)
        name: serve_keras_tensor:0
  The given SavedModel SignatureDef contains the following output(s):
    outputs['output_0'] tensor_info:
        dtype: DT_FLOAT
        shape: (-1, 1)
        name: StatefulPartitionedCall:0
  Method name is: tensorflow/serving/predict

signature_def['serving_default']:
  The given SavedModel SignatureDef contains the following input(s):
    inputs['keras_tensor'] tensor_info:
        dtype: DT_FLOAT
        shape: (-1, 6)
        name: serving_default_keras_tensor:0
  The given SavedModel SignatureDef contains the following output(s):
    outputs['output_0'] tensor_info:
        dtype: DT_FLOAT
        shape: (-1, 1)
        name: StatefulPartitionedCall_1:0
  Method name is: tensorflow/serving/predict
The MetaGraph with tag set ['serve'] contains the following ops: {'ReadVariableOp', 'Select', 'StatefulPartitionedCall', 'RestoreV2', 'NoOp', 'Identity', 'StaticRegexFullMatch', 'StringJoin', 'AssignVariableOp', 'SaveV2', 'MergeV2Checkpoints', 'VarIsInitializedOp', 'AddV2', 'VarHandleOp', 'DisableCopyOnRead', 'Pack', 'Placeholder', 'MatMul', 'Const', 'Relu', 'ShardedFilename'}

Concrete Functions:2024-11-12 16:47:24.597134: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.

  Function Name: 'serve'
    Option #1
      Callable with:
        Argument #1
          keras_tensor: TensorSpec(shape=(None, 6), dtype=tf.float32, name='keras_tensor')

Finally, the java code to import and make a prediction is :
public static void importKerasModel() {
        try (SavedModelBundle model = SavedModelBundle.load(""PATH\kerasModel"", ""serve"")) {
            float[] x = {0.48f, 0.48f, 0.48f, 0.48f, 0.48f, 0.48f};
            try (Tensor input = TFloat32.vectorOf(x);
                 Tensor output = model.session()
                         .runner()
                         .feed(""serve_keras_tensor"", input)
                         .fetch(""StatefulPartitionedCall"")
                         .run()
                         .get(0)) {

                float prediction = output.dataType().getNumber();
                System.out.println(""prediction = "" + prediction);
            }
        }
    }


But I get this error message :
2024-11-12 17:26:01.089591: I tensorflow/cc/saved_model/loader.cc:317] SavedModel load for tags { serve }; Status: success: OK. Took 61548 microseconds.
2024-11-12 17:26:01.317247: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: INVALID_ARGUMENT: In[0] is not a matrix
     [[{{node StatefulPartitionedCall/StatefulPartitionedCall/sequential_1/dense_1/Relu}}]]
Exception in thread ""main"" org.tensorflow.exceptions.TFInvalidArgumentException: In[0] is not a matrix
     [[{{node StatefulPartitionedCall/StatefulPartitionedCall/sequential_1/dense_1/Relu}}]]
    at org.tensorflow.internal.c_api.AbstractTF_Status.throwExceptionIfNotOK(AbstractTF_Status.java:76)
    at org.tensorflow.Session.run(Session.java:826)
    at org.tensorflow.Session$Runner.runHelper(Session.java:549)
    at org.tensorflow.Session$Runner.run(Session.java:476)
    at com.ptvgroup.platform.truckslogs.converter.HelloTensorFlow.importKerasModel(HelloTensorFlow.java:471)
    at com.ptvgroup.platform.truckslogs.converter.Main.main(Main.java:25)

Someone can help me ? What does it means ""In[0] is not a matrix"" ? It's because my dimensions/shapes of the inputs and outputs are (-1,6) and (-1,1) ?
","The error 'In[0] is not a matrix' comes from a wrong data type when I would like make a prediction. I create a tensor of vector instead a tensor of matrix. The exception message told well the value was not a matrix. The value concerned is the tensor of inputs.
public static void importKerasModel() {
        try (SavedModelBundle model = SavedModelBundle.load(""PATH"", ""serve"")) {
            float[] x = {200f,0f,1.5f,2f,2.5f,0f};
            FloatNdArray matrix = NdArrays.ofFloats(Shape.of(1, 6)); // my model have 6 features per observations
            matrix.set(NdArrays.vectorOf(x), 0);
            try (Tensor input = TFloat32.tensorOf(matrix);
                 TFloat32 output = (TFloat32) model.session()
                         .runner()
                         .feed(""serve_keras_tensor_272"", input)  ///## to know inputs and outputs  py .\saved_model_cli.py show --dir '.' --all
                         .fetch(""StatefulPartitionedCall"")
                         .run()
                         .get(0)) {
                float prediction =  output.getFloat();;
                System.out.println(""prediction = "" + prediction);
            }
        }
    }

This code fix the failed execution.
",tensorflow
tensorflow typeerror cannot unpack noniterable float object,"I am using tensorflow V2.2 and run into TyepError when I do model.evaluate. Can someone advise what the issues may be? A screenshot of the execution and error message is shown below.

","you need to define a metric when you compile the model model.compile('adam', 'binary_crossentropy', metrics='accuracy')
in this way during evaluation, loss and accuracy are returned
",tensorflow
does calling the modelfit method again reinitialize the already trained weights,"I am using Keras to train a network. Let's say that after 20 epochs I want to stop the training to check if everything is fine, then continue form the 21st epoch. Does calling the model.fit method for a second time reinitialize the already trained weights?
","No, it will use the preexisting weights your model had and perform updates on them. This means you can do consecutive calls to fit if you want to and manage it properly.
This is true also because in Keras you are also able to save a model (with the save and load_model methods), load it back, and call fit on it. For more info on that check this question.
Another option you got is to use the train_on_batch method instead:

train_on_batch(self, x, y, sample_weight=None, class_weight=None)
Runs a single gradient update on a single batch of data.

This way I think you may have more control in between the updates of you model, where you can check if everything is fine with the training, and then continue to the next gradient update.
",tensorflow
how to interpret increase in both loss and accuracy,"I have run deep learning models(CNN's) using tensorflow. Many times during the epoch, i have observed that both loss and accuracy have increased, or both have decreased. My understanding was that both are always inversely related. What could be scenario where both increase or decrease simultaneously.
","The loss decreases as the training process goes on, except for some fluctuation introduced by the mini-batch gradient descent and/or regularization techniques like dropout (that introduces random noise).
If the loss decreases, the training process is going well.
The (validation I suppose) accuracy, instead, it's a measure of how good the predictions of your model are.
If the model is learning, the accuracy increases. If the model is overfitting, instead, the accuracy stops to increase and can even start to decrease.
If the loss decreases and the accuracy decreases, your model is overfitting.
If the loss increases and the accuracy increase too is because your regularization techniques are working well and you're fighting the overfitting problem. This is true only if the loss, then, starts to decrease whilst the accuracy continues to increase.
Otherwise, if the loss keep growing your model is diverging and you should look for the cause (usually you're using a too high learning rate value).
",tensorflow
running modelfit multiple times without reinstantiating the model,"Background
I am watching a popular YouTube crash course on machine learning.
At 3:35:50, he mentions that the model is likely overfit, so fits it again with less epochs.
Since he didn't reinstantiate the model, isn't this equivalent to fitting the model with that same data, thereby continuing to overtrain it?
My Question
Assume you have a model created and data ready to go.
You run:
model.fit(train_images, train_labels, epochs=10)
model.fit(train_images, train_labels, epochs=8)

Is this equivalent to running:
model.fit(train_images, train_labels, epochs=18)

Or:
model.fit(train_images, train_labels, epochs=8)

If previously fitted data is overwritten, why does running model.fit a second time begin with the accuracy of the previous model?
In multiple other questions regarding saving and training models, the accepted solutions are to load the previously trained model, and run model.fit again. 
If this will overwrite the pre-existing weights, doesn't that defeat the purpose of saving the model in the first place? Wouldn't training the model for the first time on the new data be equivalent?
What is the appropriate way to train a model across multiple, similar datasets while retaining accuracy across all of the data?
","
Since he didn't reinstantiate the model, isn't this equivalent to
  fitting the model with that same data, thereby continuing to overtrain
  it?

You are correct! In order to check which number of epochs would do better in his example, he should have compiled the network again (that is, execute the above cell again).
Just remember that in general, whenever you instantiate a model again it most likely will start with completely new weights, totally different from past weights (unless you change this manually). So even though you keep the same amount of epochs, your final accuracy can change depending on the initial weights.
Are these two commands equivalent?
model.fit(train_images, train_labels, epochs=10)
model.fit(train_images, train_labels, epochs=8)

and
model.fit(train_images, train_labels, epochs=18)

No.
In the first case, you are training your network with some weights X going through all your training set 10 times, then you update your weights for some value y.
Then you will train your network again though all your training set 8 times but now you are using a network with weights X+y. 
For the second case, you will train your network through all your training data 18 times with the weights X. 
This is different!
",tensorflow
how to print the value of a tensor object in tensorflow,"I have been using the introductory example of matrix multiplication in TensorFlow.
matrix1 = tf.constant([[3., 3.]])
matrix2 = tf.constant([[2.],[2.]])
product = tf.matmul(matrix1, matrix2)

When I print the product, it is displaying it as a Tensor object:
<tensorflow.python.framework.ops.Tensor object at 0x10470fcd0>

But how do I know the value of product?
The following doesn't help:
print product
Tensor(""MatMul:0"", shape=TensorShape([Dimension(1), Dimension(1)]), dtype=float32)

I know that graphs run on Sessions, but isn't there any way I can check the output of a Tensor object without running the graph in a session?
","The easiest[A] way to evaluate the actual value of a Tensor object is to pass it to the Session.run() method, or call Tensor.eval() when you have a default session (i.e. in a with tf.Session(): block, or see below). In general[B], you cannot print the value of a tensor without running some code in a session.
If you are experimenting with the programming model, and want an easy way to evaluate tensors, the tf.InteractiveSession lets you open a session at the start of your program, and then use that session for all Tensor.eval() (and Operation.run()) calls. This can be easier in an interactive setting, such as the shell or an IPython notebook, when it's tedious to pass around a Session object everywhere. For example, the following works in a Jupyter notebook:
with tf.Session() as sess:  print(product.eval()) 

This might seem silly for such a small expression, but one of the key ideas in Tensorflow 1.x is deferred execution: it's very cheap to build a large and complex expression, and when you want to evaluate it, the back-end (to which you connect with a Session) is able to schedule its execution more efficiently (e.g. executing independent parts in parallel and using GPUs).

[A]: To print the value of a tensor without returning it to your Python program, you can use the tf.print() operator, as Andrzej suggests in another answer. According to the official documentation: 

To make sure the operator runs, users need to pass the produced op to tf.compat.v1.Session's run method, or to use the op as a control dependency for executed ops by specifying with tf.compat.v1.control_dependencies([print_op]), which is printed to standard output. 

Also note that:

In Jupyter notebooks and colabs, tf.print prints to the notebook cell outputs. It will not write to the notebook kernel's console logs.

[B]: You might be able to use the tf.get_static_value() function to get the constant value of the given tensor if its value is efficiently calculable.
",tensorflow
modulenotfounderror no module named 39keras_preprocessing39,"I'm trying to implement YOLO3 object detection using tensorflow 2.0 on google colab : https://github.com/theAIGuysCode/Object-Detection-API
And for this particular line:
!python3 /content/Object-Detection-API/load_weights.py

I'm facing this error:
ModuleNotFoundError: No module named 'keras_preprocessing'

I tried installing 'keras_preprocessing' using this command:
!conda install keras_preprocessing

and then I'm facing this error:
Collecting package metadata (repodata.json): done
Solving environment: failed with initial frozen solve. Retrying with flexible solve.

PackagesNotFoundError: The following packages are not available from current channels:

  - keras_preprocessing

Current channels:

  - https://repo.anaconda.com/pkgs/main/linux-64
  - https://repo.anaconda.com/pkgs/main/noarch
  - https://repo.anaconda.com/pkgs/r/linux-64
  - https://repo.anaconda.com/pkgs/r/noarch

To search for alternate channels that may provide the conda package you're
looking for, navigate to

    https://anaconda.org

and use the search bar at the top of the page.

Please help me solve the same!
Thank you!
","From the Anaconda repository of the Keras Preprocessing package, you can install it with
conda install -c conda-forge keras-preprocessing

that is, with an - instead of _ and with selecting the conda-forge channel.
",tensorflow
how to pass gpus all option to docker with go sdk,"I have seen how to do some basic commands such as running a container, pulling images, listing images, etc from the SDK examples.
I am working on a project where I need to use the GPU from within the container.
My system has GPU, I have installed the drivers, and I have also installed the nvidia-container-runtime.
If we remove Go SDK from the scene for a moment, I can run the following command to get the nvidia-smi output on my host system:
docker run -it --rm --gpus all nvidia/cuda:10.0-base nvidia-smi

I have to do this via the SDK. Here is the code to start with. This code prints ""hello world"". But in actual I will be running nvidia-smi command at that place:
package main

import (
    ""context""
    ""os""

    ""github.com/docker/docker/api/types""
    ""github.com/docker/docker/api/types/container""
    ""github.com/docker/docker/client""
    ""github.com/docker/docker/pkg/stdcopy""
)

func main() {
    ctx := context.Background()
    cli, err := client.NewClientWithOpts(client.FromEnv, client.WithAPIVersionNegotiation())
    if err != nil {
        panic(err)
    }

    RunContainer(ctx, cli)
}

func RunContainer(ctx context.Context, cli *client.Client) {
    reader, err := cli.ImagePull(ctx, ""nvidia/cuda:10.0-base"", types.ImagePullOptions{})
    if err != nil {
        panic(err)
    }

    defer reader.Close()
    // io.Copy(os.Stdout, reader)

    resp, err := cli.ContainerCreate(ctx, &container.Config{
        Image: ""nvidia/cuda:10.0-base"",
        Cmd:   []string{""echo"", ""hello world""},
        // Tty:   false,
    }, nil, nil, nil, """")

    if err != nil {
        panic(err)
    }

    if err := cli.ContainerStart(ctx, resp.ID, types.ContainerStartOptions{}); err != nil {
        panic(err)
    }

    statusCh, errCh := cli.ContainerWait(ctx, resp.ID, container.WaitConditionNotRunning)

    select {
    case err := <-errCh:
        if err != nil {
            panic(err)
        }
    case <-statusCh:
    }

    out, err := cli.ContainerLogs(ctx, resp.ID, types.ContainerLogsOptions{ShowStdout: true})
    if err != nil {
        panic(err)
    }

    stdcopy.StdCopy(os.Stdout, os.Stderr, out)
}

","see: https://github.com/docker/cli/blob/9ac8584acfd501c3f4da0e845e3a40ed15c85041/cli/command/container/opts.go#L594
import ""github.com/docker/cli/opts""

// ...

gpuOpts := opts.GpuOpts{}
gpuOpts.Set(""all"")

resp, err := cli.ContainerCreate(ctx, &container.Config{
    Image: ""nvidia/cuda:10.0-base"",
    Cmd:   []string{""echo"", ""hello world""},
    // Tty:   false,
}, &container.HostConfig{Resources: container.Resources{DeviceRequests: gpuOpts.Value()}}, nil, nil, """")

",tensorflow
training a cnn using imagedatagenerator and training fails after the 2nd epoch,"I was training a CNN using ImageDataGenerator and encountered this problem where after the Second Epoch an Attribute error is raised.
The model is as follows
Model
import tensorflow as tf
from tensorflow.keras.optimizers import RMSprop

def create_model():
  '''Creates a CNN with 4 convolutional layers'''
  model = tf.keras.models.Sequential([
      tf.keras.layers.Conv2D(32, (3,3), activation='relu', input_shape=(150, 150, 3)),
      tf.keras.layers.MaxPooling2D(2, 2),
      tf.keras.layers.Conv2D(64, (3,3), activation='relu'),
      tf.keras.layers.MaxPooling2D(2,2),
      tf.keras.layers.Conv2D(128, (3,3), activation='relu'),
      tf.keras.layers.MaxPooling2D(2,2),
      tf.keras.layers.Conv2D(128, (3,3), activation='relu'),
      tf.keras.layers.MaxPooling2D(2,2),
      tf.keras.layers.Flatten(),
      tf.keras.layers.Dense(512, activation='relu'),
      tf.keras.layers.Dense(1, activation='sigmoid')
  ])

  model.compile(loss='binary_crossentropy',
                optimizer=RMSprop(learning_rate=1e-4),
                metrics=['accuracy'])
  
  return model


from tensorflow.keras.preprocessing.image import ImageDataGenerator

train_datagen = ImageDataGenerator(rescale=1./255)
test_datagen = ImageDataGenerator(rescale=1./255)

train_generator = train_datagen.flow_from_directory(
        train_dir,  # This is the source directory for training images
        target_size=(150, 150),  # All images will be resized to 150x150
        batch_size=20,
        # Since we use binary_crossentropy loss, we need binary labels
        class_mode='binary')

validation_generator = test_datagen.flow_from_directory(
        validation_dir,
        target_size=(150, 150),
        batch_size=20,
        class_mode='binary',
        shuffle= False)


EPOCHS = 20

model = create_model()

history = model.fit(
      train_generator,
      steps_per_epoch=100,  # 2000 images = batch_size * steps
      epochs=EPOCHS,
      validation_data=validation_generator,
      validation_steps=50,  # 1000 images = batch_size * steps
      verbose=2)

Output
AttributeError                            Traceback (most recent call last)
Cell In[15], line 8
      5 model = create_model()
      7 # Train the model
----> 8 history = model.fit(
      9       train_generator,
     10       steps_per_epoch=100,  # 2000 images = batch_size * steps
     11       epochs=EPOCHS,
     12       validation_data=validation_generator,
     13       validation_steps=50,  # 1000 images = batch_size * steps
     14       verbose=2)

File ~\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\LocalCache\local-packages\Python311\site-packages\keras\src\utils\traceback_utils.py:122, in filter_traceback.<locals>.error_handler(*args, **kwargs)
    119     filtered_tb = _process_traceback_frames(e.__traceback__)
    120     # To get the full stack trace, call:
    121     # `keras.config.disable_traceback_filtering()`
--> 122     raise e.with_traceback(filtered_tb) from None
    123 finally:
    124     del filtered_tb

File ~\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\LocalCache\local-packages\Python311\site-packages\keras\src\backend\tensorflow\trainer.py:354, in TensorFlowTrainer.fit(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)
    333         self._eval_epoch_iterator = TFEpochIterator(
    334             x=val_x,
    335             y=val_y,
...
    355     }
    356     epoch_logs.update(val_logs)
    358 callbacks.on_epoch_end(epoch, epoch_logs)

AttributeError: 'NoneType' object has no attribute 'items'
Output is truncated. View as a scrollable element or open in a text editor. Adjust cell output settings...

I have tried the following debugging steps:

upgrading Tensorflow and Keras
trying a simpler Neural Network to see if that has the same issue but it worked fine.
Instead of passing validation_generator to the model.fit(), doing it manually using numpy but that also didn't work out since for it the accuracy and error on training data was coming 0, for the even epochs only.

Have also checked the validation data it is properly loaded.
Python Version: 3.11.9
Tensorflow Version: 2.17.0
Keras Version: 3.4.1
","I've successfully replicated your code using the  versions that you
specified, and it's functioning . The model is training  with defined
epochs. It looks like the problem might be related to how the image data generator is processing the data from the directory. Please check the provided path and verify that your main directory contains sub-folders, each representing a different class, matching the total number of classes in your dataset.
Please refer to this gist
",tensorflow
model with mnist gives poor accuracy and long training time when the data is loaded using tfdata,"I am new to machine learning and python, So sorry for a newbie question.
I am using MNIST data set to study a simple CNN model.
First, I saved MNIST data to png files.
import cv2
import os

from keras.datasets import mnist

(x_train, y_train), (x_test, y_test) = mnist.load_data()

img_root = './images'
train = os.path.join(img_root, 'train')
test = os.path.join(img_root, 'test')

if not os.path.exists(img_root):
    os.mkdir(img_root)

if not os.path.exists(train):
    os.mkdir(train)

if not os.path.exists(test):
    os.mkdir(test)

# Save Train images
for i in range(x_train.shape[0]):
    img_dir = os.path.join(train, str(y_train[i]))
    if not os.path.exists(img_dir):
        os.mkdir(img_dir)
    image_out = os.path.join(img_dir, str(i) + "".png"")
    cv2.imwrite(image_out, x_train[i])

# Save Test images
for i in range(x_test.shape[0]):
    img_dir = os.path.join(test, str(y_test[i]))
    if not os.path.exists(img_dir):
        os.mkdir(img_dir)
    image_out = os.path.join(img_dir, str(i) + "".png"")
    cv2.imwrite(image_out, x_test[i])

Then I loaded those images using three different methods and trained-tested the model.
Method 1 “cv_only”: load all images to numpy array using cv2.imread(). Do not use tf.data.
Results are

Epoch 48/50
469/469 [==============================] - 2s 5ms/step - loss: 0.0025 - accuracy: 0.9992 - val_loss: 0.0490 - val_accuracy: 0.9937


Epoch 49/50
469/469 [==============================] - 2s 5ms/step - loss: 0.0042 - accuracy: 0.9990 - val_loss: 0.0477 - val_accuracy: 0.9924


Epoch 50/50
469/469 [==============================] - 2s 5ms/step - loss: 0.0033 - accuracy: 0.9991 - val_loss: 0.0485 - val_accuracy: 0.9914


Learning time: 199[s]

Method 2 “tf_cv” : use tf.data.Dataset.from_tensor_slices(), map() and batch() methods to make data sets. Inside map() method, uses cv2.imread() to read images.
Results are

Epoch 48/50
469/469 [==============================] - 32s 68ms/step - loss: 5.8638e-07 - accuracy: 1.0000 - val_loss: 4.7863 - val_accuracy: 0.4507


Epoch 49/50
469/469 [==============================] - 32s 68ms/step - loss: 4.6416e-07 - accuracy: 1.0000 - val_loss: 4.8436 - val_accuracy: 0.4514


Epoch 50/50
469/469 [==============================] - 32s 69ms/step - loss: 3.6748e-07 - accuracy: 1.0000 - val_loss: 4.8742 - val_accuracy: 0.4517


Learning time: 1638[s]

Method 3 “tf_only : use tf.data.Dataset.from_tensor_slices(), map() and batch() methods to make data sets (Same as Method 2). But Inside map() method, uses tf.io.read_file() to read images.
Results are

Epoch 48/50
469/469 [==============================] - 16s 33ms/step - loss: 8.5148e-07 - accuracy: 1.0000 - val_loss: 5.4608 - val_accuracy: 0.4065


Epoch 49/50
469/469 [==============================] - 16s 34ms/step - loss: 6.7230e-07 - accuracy: 1.0000 - val_loss: 5.4721 - val_accuracy: 0.4085


Epoch 50/50
469/469 [==============================] - 15s 33ms/step - loss: 5.3065e-07 - accuracy: 1.0000 - val_loss: 5.4845 - val_accuracy: 0.4087


Learning time: 700[s]

What am I doing wrong in Method 2 and 3. Please Help.
This is the full code for the model I used.

import numpy as np
import cv2
import glob
import datetime

import tensorflow as tf
from keras.models import Sequential
from keras.layers import Conv2D, MaxPooling2D, BatchNormalization
from keras.layers import Dense, Flatten
from keras.utils import to_categorical

# Valid Options are ""cv_only"", ""tf_cv"" and ""tf_only""
data_load_method = ""tf_cv""

train_images = ""./images/train/""
test_images = ""./images/test/""

# network parameters
batch_size = 128

categories = [""0"", ""1"", ""2"", ""3"", ""4"", ""5"", ""6"", ""7"", ""8"", ""9""]
num_classes = len(categories)


def make_sample_imgs(files):
    global X, Y
    X = []
    Y = []
    for cat, fname in files:
        X.append(cv2.imread(fname, cv2.IMREAD_GRAYSCALE))
        Y.append(cat)
    return np.array(X), np.array(Y)


def make_sample(files):
    global X, Y
    X = []
    Y = []
    for cat, fname in files:
        X.append(fname)
        Y.append(cat)
    return np.array(X), np.array(Y)


def process_path_cv2(image_path, label):

    def load_image(path):
        image_gray = cv2.imread(path.decode(""utf-8""), cv2.IMREAD_GRAYSCALE)
        image_gray = image_gray.astype('float32')/255
        image_gray = tf.expand_dims(image_gray, axis=-1)
        return image_gray

    image = tf.numpy_function(load_image, [image_path], tf.float32)
    return image, label


def process_path_tf(image_path, label):
    image = tf.io.read_file(image_path)
    image = tf.image.decode_image(image, channels=1)
    image = tf.image.convert_image_dtype(image, tf.float32)
    return image, label


start_time = datetime.datetime.now()

train = []
test = []

for idx, cat in enumerate(categories):
    image_dir = train_images + cat
    files = glob.glob(image_dir + ""/*.png"")
    for f in files:
        train.append((idx, f))
    print(""Train "", image_dir, ""append "", len(files), ""files!"")

for idx, cat in enumerate(categories):
    image_dir = test_images + cat
    files = glob.glob(image_dir + ""/*.png"")
    for f in files:
        test.append((idx, f))
    print(""Test "", image_dir, ""append "", len(files), ""files!"")


if data_load_method == ""cv_only"":
    x_train, y_train = make_sample_imgs(train)
    x_test, y_test = make_sample_imgs(test)

    # convert to one-hot vector
    y_train = to_categorical(y_train)
    y_test = to_categorical(y_test)

    x_train = x_train.reshape(-1, 28, 28, 1)
    x_test = x_test.reshape(-1, 28, 28, 1)

    x_train = x_train.astype('float32') / 255
    x_test = x_test.astype('float32') / 255

elif data_load_method == ""tf_cv"" or data_load_method == ""tf_only"":
    x_train, y_train = make_sample(train)
    x_test, y_test = make_sample(test)

    # convert to one-hot vector
    y_train = to_categorical(y_train)
    y_test = to_categorical(y_test)

    train_datasets = tf.data.Dataset.from_tensor_slices((x_train, y_train))
    test_datasets = tf.data.Dataset.from_tensor_slices((x_test, y_test))

    if data_load_method == ""tf_cv"":
        train_datasets = train_datasets.map(process_path_cv2, num_parallel_calls=tf.data.AUTOTUNE)
        test_datasets = test_datasets.map(process_path_cv2, num_parallel_calls=tf.data.AUTOTUNE)

    else:
        train_datasets = train_datasets.map(process_path_tf, num_parallel_calls=tf.data.AUTOTUNE)
        test_datasets = test_datasets.map(process_path_tf, num_parallel_calls=tf.data.AUTOTUNE)

    # train_datasets = train_datasets.cache()
    # test_datasets = test_datasets.cache()

    train_datasets = train_datasets.batch(batch_size)
    test_datasets = test_datasets.batch(batch_size)
    train_datasets = train_datasets.prefetch(tf.data.AUTOTUNE)
    test_datasets = test_datasets.prefetch(tf.data.AUTOTUNE)

else:
    print(""Method not defined!"")
    exit()


model = Sequential()

model.add(Conv2D(filters=64, kernel_size=(3, 3), padding=""same"", activation=""relu"", input_shape=(28, 28, 1)))
model.add(Conv2D(filters=64, kernel_size=(3, 3), padding=""same"", strides=2, activation=""relu""))
model.add(BatchNormalization())

model.add(Conv2D(filters=128, kernel_size=(3, 3), padding=""same"", strides=2, activation=""relu""))
model.add(Conv2D(filters=128, kernel_size=(3, 3), padding=""same"", strides=2, activation=""relu""))
model.add(BatchNormalization())

model.add(Conv2D(filters=256, kernel_size=(3, 3), padding=""same"", strides=2, activation=""relu""))
model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(BatchNormalization())

model.add(Flatten())
model.add(Dense(512, activation=""relu""))

model.add(Dense(10, activation=""softmax""))

model.summary()

model.compile(loss='categorical_crossentropy',
              optimizer='adam',
              metrics=['accuracy'])

if data_load_method == ""cv_only"":
    model.fit(x_train, y_train, epochs=50, batch_size=batch_size, validation_data=(x_test, y_test))

elif data_load_method == ""tf_cv"" or data_load_method == ""tf_only"":
    model.fit(train_datasets, epochs=50, batch_size=batch_size, validation_data=test_datasets)

end_time = datetime.datetime.now()
time_diff = (end_time - start_time)
learning_time = time_diff.total_seconds()
print(f'Learning time: {int(learning_time)}[s]')

If I use

train_datasets = train_datasets.cache()


test_datasets = test_datasets.cache()

then time can be reduce to around 150 seconds, But the acurrcy also drops.
I am using python 3.10, keras 2.10.0, tensorflow-gpu 2.10.0
Sorry about the messy code. I am still learning coding.
","Solved by myself.
Summry is:

Changed tf.image.decode_image() to tf.image.decode_png(), becuase I am using png files.
Shuffle the data. train_datasets = train_datasets.shuffle(15000), test_datasets = test_datasets.shuffle(4000). In previous code, no shuffling.

Now all the three methods give almost same val_loss: and val_accuracy:.
New Code is as follows.
import numpy as np
import cv2
import glob
import datetime

import tensorflow as tf
from keras.models import Sequential
from keras.layers import Conv2D, MaxPooling2D, BatchNormalization
from keras.layers import Dense, Flatten
from keras.utils import to_categorical

# Valid Options are ""cv_only"", ""tf_cv"" and ""tf_only""
data_load_method = ""cv_only""

train_images = ""./images/train/""
test_images = ""./images/test/""

# network parameters
batch_size = 128

categories = [""0"", ""1"", ""2"", ""3"", ""4"", ""5"", ""6"", ""7"", ""8"", ""9""]
num_classes = len(categories)


def make_sample_imgs(files):
    global X, Y
    X = []
    Y = []
    for cat, fname in files:
        X.append(cv2.imread(fname, cv2.IMREAD_GRAYSCALE))
        Y.append(cat)
    return np.array(X), np.array(Y)


def make_sample(files):
    global X, Y
    X = []
    Y = []
    for cat, fname in files:
        X.append(fname)
        Y.append(cat)
    return np.array(X), np.array(Y)


def process_path_cv2(image_path, label):

    def load_image(path):
        image_gray = cv2.imread(path.decode(""utf-8""), cv2.IMREAD_GRAYSCALE)
        image_gray = image_gray.astype('float32')/255
        image_gray = tf.expand_dims(image_gray, axis=-1)
        return image_gray

    image = tf.numpy_function(load_image, [image_path], tf.float32)
    return image, label


def process_path_tf(image_path, label):
    image = tf.io.read_file(image_path)
    image = tf.image.decode_png(image, channels=1)  # 
    image = tf.image.resize(image, [28, 28])    # Must use this, increase the val_accuracy
    image = tf.image.convert_image_dtype(image, tf.float32)
    image /= 255.0
    return image, label


start_time = datetime.datetime.now()

train = []
test = []

for idx, cat in enumerate(categories):
    image_dir = train_images + cat
    files = glob.glob(image_dir + ""/*.png"")
    for f in files:
        train.append((idx, f))
    print(""Train "", image_dir, ""append "", len(files), ""files!"")

for idx, cat in enumerate(categories):
    image_dir = test_images + cat
    files = glob.glob(image_dir + ""/*.png"")
    for f in files:
        test.append((idx, f))
    print(""Test "", image_dir, ""append "", len(files), ""files!"")


if data_load_method == ""cv_only"":
    x_train, y_train = make_sample_imgs(train)
    x_test, y_test = make_sample_imgs(test)

    # convert to one-hot vector
    y_train = to_categorical(y_train)
    y_test = to_categorical(y_test)

    x_train = x_train.reshape(-1, 28, 28, 1)
    x_test = x_test.reshape(-1, 28, 28, 1)

    x_train = x_train.astype('float32') / 255
    x_test = x_test.astype('float32') / 255

elif data_load_method == ""tf_cv"" or data_load_method == ""tf_only"":
    x_train, y_train = make_sample(train)
    x_test, y_test = make_sample(test)

    # convert to one-hot vector
    y_train = to_categorical(y_train)
    y_test = to_categorical(y_test)

    train_datasets = tf.data.Dataset.from_tensor_slices((x_train, tf.cast(y_train, tf.int64)))
    test_datasets = tf.data.Dataset.from_tensor_slices((x_test, tf.cast(y_test, tf.int64)))

    if data_load_method == ""tf_cv"":
        train_datasets = train_datasets.map(process_path_cv2, num_parallel_calls=tf.data.AUTOTUNE)
        test_datasets = test_datasets.map(process_path_cv2, num_parallel_calls=tf.data.AUTOTUNE)

    else:
        train_datasets = train_datasets.map(process_path_tf, num_parallel_calls=tf.data.experimental.AUTOTUNE)
        test_datasets = test_datasets.map(process_path_tf, num_parallel_calls=tf.data.experimental.AUTOTUNE)

    train_datasets = train_datasets.cache()
    test_datasets = test_datasets.cache()
    train_datasets = train_datasets.shuffle(15000)
    test_datasets = test_datasets.shuffle(4000)
    train_datasets = train_datasets.batch(batch_size)
    test_datasets = test_datasets.batch(batch_size)
    test_datasets = test_datasets.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)

else:
    print(""Method not defined!"")
    exit()


model = Sequential()

model.add(Conv2D(filters=64, kernel_size=(3, 3), padding=""same"", activation=""relu"", input_shape=(28, 28, 1)))
model.add(Conv2D(filters=64, kernel_size=(3, 3), padding=""same"", strides=2, activation=""relu""))
model.add(BatchNormalization())

model.add(Conv2D(filters=128, kernel_size=(3, 3), padding=""same"", strides=2, activation=""relu""))
model.add(Conv2D(filters=128, kernel_size=(3, 3), padding=""same"", strides=2, activation=""relu""))
model.add(BatchNormalization())

model.add(Conv2D(filters=256, kernel_size=(3, 3), padding=""same"", strides=2, activation=""relu""))
model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(BatchNormalization())

model.add(Flatten())
model.add(Dense(512, activation=""relu""))

model.add(Dense(10, activation=""softmax""))

model.summary()

model.compile(loss='categorical_crossentropy',
              optimizer='adam',
              metrics=['accuracy'])

if data_load_method == ""cv_only"":
    model.fit(x_train, y_train, epochs=50, batch_size=batch_size, validation_data=(x_test, y_test))

elif data_load_method == ""tf_cv"" or data_load_method == ""tf_only"":
    model.fit(train_datasets, epochs=50, validation_data=test_datasets)

end_time = datetime.datetime.now()
time_diff = (end_time - start_time)
learning_time = time_diff.total_seconds()
print(f'Learning time: {int(learning_time)}[s]')

",tensorflow
transfer an lstm model from cpu to gpu,"I have a very simple LSTM model which I've built in tensorflow and it works on CPU. However, I want to use this model on GPU. For the pytorch, I've defined the device and etc, however for tensorflow, I don't have any idea why it can not work. Do you have any suggestion for me? Thanks
model = Sequential()
model.add(LSTM(64, activation='relu', input_shape=(X_train.shape[1], X_train.shape[2]), return_sequences=True))
model.add(LSTM(32, activation='relu', return_sequences=False))
model.add(Dropout(0.1))
model.add(Dense(Y_train.shape[1], kernel_regularizer='l2'))
callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=50)
opt = keras.optimizers.Adam(learning_rate=0.0008)
model.compile(optimizer=opt, loss='mse')
#model.summary()
history = model.fit(X_train, Y_train, epochs=2, batch_size=100, validation_data=(X_val, Y_val), callbacks=[callback],verbose=1, device).to(device)

","For tensorflow, the models run on GPU for computations by default. It is given on their official documentation.
Is there some kind of error that shows up when you run your model? Because this should work just fine when running on GPU as well instead of a CPU.
",tensorflow
rest api  quotwordquot eg quotpredictquot vs quotportquot eg quot1000quot what is this called,"Tensorflow REST URL endpoints look like this: http://host:port/v1/models/${MODEL_NAME}[/versions/${VERSION}|/labels/${LABEL}]:predict
Where the URL ends in "":predict"", rather than in "":1000"" a number/port like I would expect. Why does this work? Are there some other example URL endpoints that use this schema of a word rather than a number? I'm having a hard time googling for this concept so would love if stackoverflow can point me to a vocab term I'm missing.
","URLs don't end in a port. If there is a non-standard port, that port shows up right after the host (domain), but before the path.
The :predict part of your URL is simply part of the 'path' part of the URL. It doesn't have a special meaning fur URLs generally, but may have a special meaning for the specific API you're working with.
",tensorflow
tensorflow create protobuf for a tensor,"I want to have a Python script that converts Numpy arrays to TensorFlow Tensors in Protobuf Binary so later in C++ I can reload them. This can be done with a compute graph like this.
I found the following functions and features in the TensorFlow Python API.

Python Tensor Object
convert_to_tensor
Registration system for to_proto functions

C++ has a corresponding load operation
Can you give me an example of serializing a TF tensor to Protobuf binary and back?
","I'll post the answer as I figure it out, so perhaps someone can pitch in with the rest of the solution.
Python
Tensor -> Protobuf Binary
>>> import tensorflow as tf
>>> with tf.Graph().as_default():
...     s = tf.constant([1.2, 3.4, 5.6, 7.8])._op.node_def.attr['value'].SerializeToString()
...
>>> s
'B\x1a\x08\x01\x12\x04\x12\x02\x08\x04""\x10\x9a\x99\x99?\x9a\x99Y@33\xb3@\x9a\x99\xf9@'

Protobuf Binary -> Tensor
>>> import tensorflow as tf
>>> s = 'B\x1a\x08\x01\x12\x04\x12\x02\x08\x04""\x10\x9a\x99\x99?\x9a\x99Y@33\xb3@\x9a\x99\xf9@'
>>> with tf.Graph().as_default():
...     c = tf.constant(1)
...     c._op.node_def.attr['value'].ParseFromString(s)
...     c._op.node_def.attr['dtype'].type = c._op.node_def.attr['value'].tensor.dtype
...     print c.eval(session=tf.Session())
... 
28
[ 1.20000005  3.4000001   5.5999999   7.80000019]

Benchmarks
   Array Elements  from_string [us]  to_string [us]
0              10         10.273593        2.308139
1             100         10.450414        2.291126
2            1000         10.540897        2.359392
3           10000         12.175265        2.734819
4          100000         31.460438        7.349958


Benchmark script
import tensorflow as tf
import pandas as pd
import numpy as np
import timeit
import matplotlib.pyplot as plt

def to_string(shape):
    with tf.Graph().as_default():
        s = tf.constant(np.empty(shape))._op.node_def.attr['value'].SerializeToString()
    return s


def from_string(s):
    with tf.Graph().as_default():
        c = tf.constant(1)
        c._op.node_def.attr['value'].ParseFromString(s)
        c._op.node_def.attr['dtype'].type = c._op.node_def.attr['value'].tensor.dtype
        c.eval(session=tf.Session())

NUM_RUNS = 10000
MAX_POW = 6

print ""Collecting to_string stats""
to_string_results = np.array([[N, timeit.timeit('to_string((%d,))' % N,
                                                setup=""from __main__ import to_string"",
                                                number=NUM_RUNS)]
                              for N in 10**np.arange(1, MAX_POW)]).T

print ""Collecting from_string stats""
strings = {N:to_string((N,)) for N in 10**np.arange(1, MAX_POW)}
from_string_results = np.array([[N, timeit.timeit('from_string(strings[%d])' % N,
                                                  setup=""from __main__ import from_string, strings"",
                                                  number=NUM_RUNS)]
                                for N in 10**np.arange(1, MAX_POW)]).T

df = pd.DataFrame.from_dict({""Array Elements"": to_string_results[0],
                             ""to_string [us]"": to_string_results[1],
                             ""from_string [us]"": from_string_results[1]})

print df
df.to_csv('benchmark.csv')

plt.subplot(2, 1, 1)
plt.loglog(to_string_results[0], to_string_results[1])
plt.title('to_string')
plt.ylabel('microseconds')
plt.xlabel('Array size')
plt.subplot(2, 1, 2)
plt.loglog(from_string_results[0], from_string_results[1])
plt.title('from_string')
plt.ylabel('microseconds')
plt.xlabel('Array size')
plt.show()

C++
Working on it
",tensorflow
how to shift a tensor like pandasshift in tensorflow  keras without shift the last row to first row like tfroll,"I want to shift a tensor in a given axis. It's easy to do this in pandas or numpy. Like this:
import numpy as np
import pandas as pd

data = np.arange(0, 6).reshape(-1, 2)
pd.DataFrame(data).shift(1).fillna(0).values

Output is:

array([[0., 0.],
[0., 1.],
[2., 3.]])

But in tensorflow, the closest solution I found is tf.roll. But it shift the last row to the first row. (I don't want that). So I have to use something like

tf.roll + tf.slice(remove the last row) + tf.concat(add tf.zeros to the first row).

It's really ugly.
Is there a better way to handle shift in tensorflow or keras?
Thanks.
","I think I find a better way for this problem.
We could use tf.roll, then apply tf.math.multiply to set the first row to zeros.
Sample code is as follows:
Original tensor:
A = tf.cast(tf.reshape(tf.range(27), (-1, 3, 3)), dtype=tf.float32)
A

Output:
<tf.Tensor: id=117, shape=(3, 3, 3), dtype=float32, numpy=
array([[[ 0.,  1.,  2.],
        [ 3.,  4.,  5.],
        [ 6.,  7.,  8.]],

       [[ 9., 10., 11.],
        [12., 13., 14.],
        [15., 16., 17.]],

       [[18., 19., 20.],
        [21., 22., 23.],
        [24., 25., 26.]]], dtype=float32)>

Shift (like pd.shift):
B = tf.concat((tf.zeros((1, 3)), tf.ones((2, 3))), axis=0)
C = tf.expand_dims(B, axis=0)
tf.math.multiply(tf.roll(A, 1, axis=1), C)

Output:
<tf.Tensor: id=128, shape=(3, 3, 3), dtype=float32, numpy=
array([[[ 0.,  0.,  0.],
        [ 0.,  1.,  2.],
        [ 3.,  4.,  5.]],

       [[ 0.,  0.,  0.],
        [ 9., 10., 11.],
        [12., 13., 14.]],

       [[ 0.,  0.,  0.],
        [18., 19., 20.],
        [21., 22., 23.]]], dtype=float32)>

",tensorflow
why do we need a y variable in keras modelfit,"I am working with the hand written digits dataset. The data is loaded as follows:
(X_train, y_train), (X_test, y_test) = keras.datasets.mnist.load_data()

This is the code for a neural network created to classify the digits:
model = keras.Sequential([
    keras.layers.Dense(10, input_shape=(784,), activation='sigmoid')
])

model.compile(
    optimizer='adam', 
    loss = 'sparse_categorical_crossentropy',
    metrics = ['accuracy']
)
model.fit(X_train_flattened, y_train, epochs=5)

The question is, what is the function of y_train in model.fit(). this appears to be a classification problem and the network just needs the input(x_train_flattened) for it to be trained.
","X_train_flattened provides the images as input, y_train (the label telling the model which digit it is 0-9) tells the model what it should aim to predict for each image.
This is necessary in supervised machine learning (Supervised machine learning tutorial) for the model to learn what classification each image belongs to.
The loss function (sparse_categorical_crossentropy here) computes how far off the model's predictions are from the true labels (y_train). Without y_train, the model wouldn't have any basis for calculating this error and wouldn’t know how to improve.
During training, the model uses the error (or loss) calculated from comparing its predictions to y_train to update its parameters through backpropagation. Here is the original paper on backpropagation if it helps (George Hinton backpropagation) .
",tensorflow
python how can i reshape 3d images np array to 1d and then reshape them back correctly to 3d,"I have RGB images (32 x 32 x 3) saved as 3D numpy arrays which I use as input for my Neural Net (using tensorflow). In order to use them as an input I reshape them to a 1D np array (1 x 3072) using reshape(1,-1). When I finish training my Net I want to reshape the output back, but using reshape(32,32,3) doesn't seem to provide the desired outcome.
Is this the correct way to do it? How I can be sure that each datum will be back to the correct place?
","If you are looking to create a 1D array, use .reshape(-1), which will create a linear version of you array. If you the use .reshape(32,32,3), this will create an array of 32, 32-by-3, arrays, which is the original format described. Using '-1' creates a linear array of the same size as the number of elements in the combined, nested array.
",tensorflow
invalid input shape for validation data,"I'm using Tensorflow to develop a simple ML model in Python. The code is below:
import tensorflow as tf
import pandas as pd

# Load CSV Data
def load_data(filename):
    data = pd.read_csv(filename)
    X = data[['X0','X1','X2','X3']]
    Y = data[['Y0','Y1']]
    return tf.data.Dataset.from_tensor_slices((X.values, Y.values))

training_data = load_data(""binarydatatraining.csv"")
print(training_data)

# Build a simple neural network model
model = tf.keras.models.Sequential([
  tf.keras.layers.Dense(4, activation='relu'),
  tf.keras.layers.Dense(2)
])
# Compile the model
model.compile(optimizer='adam',
              loss='mean_squared_error')

# Load validation data
validation_data = load_data(""binarydatavalidation.csv"")
print(validation_data)

# Train the model
model.summary()
model.fit(training_data.batch(9), epochs=5)
model.summary()
model.fit(training_data.batch(9), epochs=1, validation_data = validation_data, validation_steps = 2)

Everything runs perfectly until I start to include the validation data, which has the same number of parameters as the training data. Then I get the error
ValueError: Exception encountered when calling Sequential.call().

[1mInvalid input shape for input Tensor(""sequential_1/Cast:0"", shape=(4,), dtype=float32). Expected shape (None, 4), but input has incompatible shape (4,)[0m

Arguments received by Sequential.call():
  • inputs=tf.Tensor(shape=(4,), dtype=int64)
  • training=False
  • mask=None

Printing the validation and training datasets shows that they have the same dimension, and running print(training_data) and print(validation_data) both give
<_TensorSliceDataset element_spec=(TensorSpec(shape=(4,), dtype=tf.int64, name=None), TensorSpec(shape=(2,), dtype=tf.int64, name=None))>

How do I correctly set up the validation data to run inline with the model.fit?
","The error suggests that the shape of the input data is not compatible with what the model expects. The model expects input data with a shape of (None, 4), but it's receiving input with the shape (4,). To fix this, add input layer to your model with the shape (4,) and batch the validation data using .batch() before passing it to model.fit() as below:
# Build a simple neural network model
model = tf.keras.models.Sequential([
    tf.keras.layers.Input(shape=(4,)),  # added Input layer 
    tf.keras.layers.Dense(4, activation='relu'),
    tf.keras.layers.Dense(2)
])
# Load and batch the training/validation data
training_data = load_data(""binarydatatraining.csv"").batch(9)
print(training_data)

validation_data = load_data(""binarydatavalidation.csv"").batch(9)
print(validation_data)

Please find the attached replicated gist for your reference. Thank you.
",tensorflow
what does keras do when the number of inputs to an lstm layer is greater than or less than the number of lstm cells in that layer,"Please see python code below, I put comments in the code where I felt emphasis on information is required.
import keras
import numpy

def build_model():
    model = keras.models.Sequential()
    model.add(keras.layers.LSTM(3, input_shape = (3, 1), activation = 'elu'))# Number of LSTM cells in this layer = 3.
    return model

def build_data():
    inputs = [1, 2, 3, 4, 5, 6, 7, 8, 9]
    outputs = [10, 11, 12, 13, 14, 15, 16, 17, 18]
    inputs = numpy.array(inputs)
    outputs = numpy.array(outputs)
    inputs = inputs.reshape(3, 3, 1)# Number of samples = 3, Number of input vectors in each sample  = 3, size of each input vector = 3.
    outputs = outputs.reshape(3, 3)# Number of target samples = 3, Number of outputs per target sample = 3.
    return inputs, outputs

def train():
    model = build_model()
    model.summary()
    model.compile(optimizer= 'adam', loss='mean_absolute_error', metrics=['accuracy'])
    x, y = build_data()
    model.fit(x, y, batch_size = 1, epochs = 4000)
    model.save(""LSTM_testModel"")

def apply():
    model = keras.models.load_model(""LSTM_testModel"")
    input = [[[7], [8], [9]]]
    input = numpy.array(input)
    print(model.predict(input))

def main():
    train()

main()

My understanding is that for each input sample there are 3 input vectors. Each input vector goes to an LSTM cell. i.e. For sample 1, input vector 1 goes to LSTM cell 1, input vector 2 goes to LSTM cell 2 and so on.
Looking at tutorials on the internet, I've seen that the number of LSTM cells is much greater than the number of input vectors e.g. 300 LSTM cells. 
So say for example I have 3 input vectors per sample what input goes to the 297 remaining LSTM cells?
I tried compiling the model to have 2 LSTM cells and it still accepted the 3 input vectors per sample, although I had to change the target outputs in the training data to accommodate for this(change the dimensions) . So what happened to the third input vector of each sample...is it ignored?

I believe the above image shows that each input vector (of an arbitrary scenario) is mapped to a specific RNN cell. I may be misinterpreting it. Above image taken from the following URL: http://karpathy.github.io/2015/05/21/rnn-effectiveness/
","I will try to answer some of your questions and then will consolidate the information provided in the comments for completeness, for the benefit of you as well as for the Community.
As mentioned by Matias in the comments, irrespective of whether the Number of Inputs are more than or less than the Number of Units/Neurons, it will be connected like a Fully Connected Network, as shown below.

To understand how RNN/LSTM work internally, let's assume we have 
Number of Input Features => 3 => F1, F2 and F3
Number of Timesteps => 2 => 0 and 1
Number of Hidden Layers => 1
Number of Neurons in each Hidden Layer => 5
Then what actually happens inside can be represented in the screenshots shown below:


You have also asked about words being assigned to LSTM Cell. Not sure which link you are referring to and whether it is correct or not but in simple terms (words in this screenshot actually will be replaced by Embedding Vectors), you can understand how LSTM handles the Text as shown in the screenshot below:

For more information, please refer Beautiful Explanation by OverLordGoldDragon and Daniel Moller.
Hope this helps. Happy Learning!
",tensorflow
how to use a different cnn without losing accuracy,"I have been given a task to implement a Convolutional neural network that can evaluate hand-written digits found in the MNIST dataset with the architecture of the network looking like this:

I have implemented a CNN that matches the architecture, unfortunately it only has about a 10% accuracy to it. I've looked online and tried other example CNNs to make sure if anything else causing the issue, however they seem to work fine and give me a ~99% accuracy. I've placed both CNNs in my code and made a boolean switch to show the difference between the two:
import tensorflow
from tensorflow.keras.datasets import mnist
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout, Flatten
from tensorflow.keras.layers import Conv2D, MaxPooling2D

batch_size = 128
num_classes = 10
epochs = 1
img_rows, img_cols = 28, 28


(x_train, y_train), (x_test, y_test) = mnist.load_data()

x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)
x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)
input_shape = (img_rows, img_cols, 1)

x_train = x_train.astype('float32')
x_test = x_test.astype('float32')
x_train /= 255
x_test /= 255
print('x_train shape:', x_train.shape)
print(x_train.shape[0], 'train samples')
print(x_test.shape[0], 'test samples')

y_train = tensorflow.keras.utils.to_categorical(y_train, num_classes)
y_test = tensorflow.keras.utils.to_categorical(y_test, num_classes)

exampleModel = False  # Use to toggle which CNN goes into the model

if exampleModel:  # An example CNN that I found for MNIST
    model = Sequential()
    model.add(Conv2D(32, kernel_size=(3, 3),
                     activation='relu',
                     input_shape=input_shape))
    model.add(Conv2D(64, (3, 3), activation='relu'))
    model.add(MaxPooling2D(pool_size=(2, 2)))
    model.add(Dropout(0.25))
    model.add(Flatten())
    model.add(Dense(128, activation='relu'))
    model.add(Dropout(0.5))
    model.add(Dense(num_classes, activation='softmax'))
else:  # The CNN I created
    input_layer = tensorflow.keras.layers.Input(shape=input_shape)
    conv1 = Conv2D(32, (1, 1), activation='relu')(input_layer)
    pool1 = MaxPooling2D(2, 2)(conv1)
    conv2_1 = Conv2D(64, (1, 1), activation='relu', padding='same')(pool1)
    pool2_1 = MaxPooling2D(2, 2)(conv2_1)
    drop2_1 = Dropout(0.5)(pool2_1)
    conv2_2 = Conv2D(64, (1, 1), activation='relu', padding='same')(pool1)
    pool2_2 = MaxPooling2D(2, 2)(conv2_2)
    drop2_2 = Dropout(0.5)(pool2_2)
    conv3_1 = Conv2D(256, (1, 1), activation='relu', padding='same')(drop2_1)
    conv3_2 = Conv2D(256, (1, 1), activation='relu', padding='same')(drop2_2)
    merged = tensorflow.keras.layers.concatenate([conv3_1, conv3_2], axis=-1)
    merged = Dropout(0.5)(merged)
    merged = Flatten()(merged)
    fc1 = Dense(1000, activation='relu')(merged)
    fc2 = Dense(500, activation='relu')(fc1)
    out = Dense(10)(fc2)
    model = tensorflow.keras.models.Model(input_layer, out)

model.compile(loss=tensorflow.keras.losses.categorical_crossentropy,
              optimizer=tensorflow.keras.optimizers.Adadelta(),
              metrics=['accuracy'])

model.fit(x_train, y_train,
          batch_size=batch_size,
          epochs=epochs,
          verbose=1,
          validation_data=(x_test, y_test))
score = model.evaluate(x_test, y_test, verbose=0)
print('Test loss:', score[0])
print('Test accuracy:', score[1])

In order to complete my task, I believe I have to convert my example CNN piece-by-piece into the required architecture. Although I have no idea how to do this,  they look completely different from each other (one is purely sequential, the other uses parallel layers and merging).
","You simply have to add an softmax activation to the last, out layer:
out = Dense(10, activation=""softmax"")(fc2)

Thus your model in completed form:
input_layer = tensorflow.keras.layers.Input(shape=input_shape)
conv1 = Conv2D(32, (1, 1), activation='relu')(input_layer)
pool1 = MaxPooling2D(2, 2)(conv1)
conv2_1 = Conv2D(64, (1, 1), activation='relu', padding='same')(pool1)
pool2_1 = MaxPooling2D(2, 2)(conv2_1)
drop2_1 = Dropout(0.5)(pool2_1)
conv2_2 = Conv2D(64, (1, 1), activation='relu', padding='same')(pool1)
pool2_2 = MaxPooling2D(2, 2)(conv2_2)
drop2_2 = Dropout(0.5)(pool2_2)
conv3_1 = Conv2D(256, (1, 1), activation='relu', padding='same')(drop2_1)
conv3_2 = Conv2D(256, (1, 1), activation='relu', padding='same')(drop2_2)
merged = tensorflow.keras.layers.concatenate([conv3_1, conv3_2], axis=-1)
merged = Dropout(0.5)(merged)
merged = Flatten()(merged)
fc1 = Dense(1000, activation='relu')(merged)
fc2 = Dense(500, activation='relu')(fc1)
out = Dense(10, activation=""softmax"")(fc2)

Out:
x_train shape: (60000, 28, 28, 1)
60000 train samples
10000 test samples
Train on 60000 samples, validate on 10000 samples
Epoch 1/1
60000/60000 [==============================] - 25s 416us/step - loss: 0.6394 - acc: 0.7858 - val_loss: 0.2956 - val_acc: 0.9047
Test loss: 0.29562548571825026
Test accuracy: 0.9047

",tensorflow
what is the difference between keras and tfkeras,"I'm learning TensorFlow and Keras. I'd like to try https://www.amazon.com/Deep-Learning-Python-Francois-Chollet/dp/1617294438/, and it seems to be written in Keras.
Would it be fairly straightforward to convert code to tf.keras?
I'm not more interested in the portability of the code, rather than the true difference between the two.
","At this point tensorflow has pretty much entirely adopted the keras API and for a good reason - it's simple, easy to use and easy to learn, whereas ""pure"" tensorflow comes with a lot of boilerplate code. And yes, you can use tf.keras without any issues, though you might have to re-work your imports in the code. For instance
from keras.layers.pooling import MaxPooling2D

Would turn into:
from tensorflow.keras.layers import MaxPooling2D

",tensorflow
tensorflow installation on windows dll load failed while importing _pywrap_tensorflow_internal,"I ran through the TensorFlow object detection installation on Windows.
https://medium.com/riow/tensorflow-object-detection-on-windows-ad10bfde217c
After successfully installing TensorFlow object detection,
I ran  following command to test, and receiving error below. How can this be fixed?
python object_detection/builders/model_builder_test.py


Error: Traceback (most recent call last):
File ""C:\TestImage\models\venv\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 64, in 
from tensorflow.python._pywrap_tensorflow_internal import *
ImportError: DLL load failed while importing pywrap_tensorflow_internal: The specified module could not be found.
Traceback (most recent call last):
File ""object_detection/builders/model_builder_test.py"", line 21, in 
from object_detection.builders import model_builder
File ""c:\testimage\models\research\object_detection\builders\model_builder.py"", line 23, in 
from object_detection.builders import anchor_generator_builder
File ""c:\testimage\models\research\object_detection\builders\anchor_generator_builder.py"", line 23, in 
from object_detection.anchor_generators import flexible_grid_anchor_generator
File ""c:\testimage\models\research\object_detection\anchor_generators\flexible_grid_anchor_generator.py"", line 17, in 
import tensorflow.compat.v1 as tf
File ""C:\TestImage\models\venv\lib\site-packages\tensorflow_init.py"", line 41, in 
from tensorflow.python.tools import module_util as module_util
File ""C:\TestImage\models\venv\lib\site-packages\tensorflow\python_init.py"", line 40, in 
from tensorflow.python.eager import context
File ""C:\TestImage\models\venv\lib\site-packages\tensorflow\python\eager\context.py"", line 35, in 
from tensorflow.python import pywrap_tfe
File ""C:\TestImage\models\venv\lib\site-packages\tensorflow\python\pywrap_tfe.py"", line 28, in 
from tensorflow.python import pywrap_tensorflow
File ""C:\TestImage\models\venv\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 83, in 
raise ImportError(msg)
ImportError: Traceback (most recent call last):
File ""C:\TestImage\models\venv\lib\site-packages\tensorflow\python\pywrap_tensorflow.py"", line 64, in 
from tensorflow.python._pywrap_tensorflow_internal import *
ImportError: DLL load failed while importing _pywrap_tensorflow_internal: The specified module could not be found.

Currently using Python 3.8 with latest tensorflow, with tensorflow-cpu
","tldr;
You need to install the C++ redistributable mentioned on the install page before installing TensorFlow so that TensorFlow can  use it to generate code at install time.
Details:
I'm not an expert but it sounds like this was key:
The pywrap_tensorflow_internal.py module you are missing appears to me to be a SWIG-generated python interface to a C++ library, or something of that nature. My guess is, that file gets generated when you install TensorFlow (it reminds me of how some Ruby gems have to compile C++ when you install them, for whatever that's worth). Since you don't have that generated file, my guess is that the C++ package mentioned on the TensorFlow install page is needed for that code generation, so you need to have the C++ package installed before installing TensorFlow.
Totally a guess, but perhaps you installed the C++ package after TF. In that case, you should be able to uninstall TF, install the C++ package, and then install TF again.
Maybe a lingering question is if there were any indicative error messages on installing TF about the missing C++ package and if not, maybe there should be.
",tensorflow
attributeerror the layer has never been called and thus has no defined output shape,"I am trying to define a model happyModel()
# GRADED FUNCTION: happyModel

def happyModel():
    """"""
    Implements the forward propagation for the binary classification model:
    ZEROPAD2D -> CONV2D -> BATCHNORM -> RELU -> MAXPOOL -> FLATTEN -> DENSE

Note that for simplicity and grading purposes, you'll hard-code all the values
such as the stride and kernel (filter) sizes. 
Normally, functions should take these values as function parameters.

Arguments:
None

Returns:
model -- TF Keras model (object containing the information for the entire training process) 
""""""
model = tf.keras.Sequential(
    [
        ## ZeroPadding2D with padding 3, input shape of 64 x 64 x 3
        tf.keras.layers.ZeroPadding2D(padding=(3,3), data_format=(64,64,3)),
    
        ## Conv2D with 32 7x7 filters and stride of 1            
        tf.keras.layers.Conv2D(32, (7, 7), strides = (1, 1), name = 'conv0'),
        
        ## BatchNormalization for axis 3
        
        tf.keras.layers.BatchNormalization(axis = 3, name = 'bn0'),
        
        ## ReLU            
        tf.keras.layers.Activation('relu'),
        
        ## Max Pooling 2D with default parameters            
        tf.keras.layers.MaxPooling2D((2, 2), name='max_pool0'),
    
        ## Flatten layer            
        tf.keras.layers.Flatten(),
    
        ## Dense layer with 1 unit for output & 'sigmoid' activation            
        tf.keras.layers.Dense(1, activation='sigmoid', name='fc'),
        
        # YOUR CODE STARTS HERE
        
        
        # YOUR CODE ENDS HERE
    ]
)

return model

and following code is for creating the object of this model defined above:
happy_model = happyModel()
# Print a summary for each layer
for layer in summary(happy_model):
    print(layer)
    
output = [['ZeroPadding2D', (None, 70, 70, 3), 0, ((3, 3), (3, 3))],
            ['Conv2D', (None, 64, 64, 32), 4736, 'valid', 'linear', 'GlorotUniform'],
            ['BatchNormalization', (None, 64, 64, 32), 128],
            ['ReLU', (None, 64, 64, 32), 0],
            ['MaxPooling2D', (None, 32, 32, 32), 0, (2, 2), (2, 2), 'valid'],
            ['Flatten', (None, 32768), 0],
            ['Dense', (None, 1), 32769, 'sigmoid']]
    
comparator(summary(happy_model), output)

I got following error:
---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
<ipython-input-67-f33284fd82fe> in <module>
      1 happy_model = happyModel()
      2 # Print a summary for each layer
----> 3 for layer in summary(happy_model):
      4     print(layer)
      5 

~/work/release/W1A2/test_utils.py in summary(model)
     30     result = []
     31     for layer in model.layers:
---> 32         descriptors = [layer.__class__.__name__, layer.output_shape, layer.count_params()]
     33         if (type(layer) == Conv2D):
     34             descriptors.append(layer.padding)

/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py in output_shape(self)
   2177     """"""
   2178     if not self._inbound_nodes:
-> 2179       raise AttributeError('The layer has never been called '
   2180                            'and thus has no defined output shape.')
   2181     all_output_shapes = set(

AttributeError: The layer has never been called and thus has no defined output shape.

I suspect my calling of ZeroPadding2D() is not right. The project seems to require the input shape of ZeroPadding2D() to be 64X64X3. I tried many formats but could not fix the problem. Anyone can give a pointer? Thanks a lot.
","In your model definition, there's an issue with the following layer:
tf.keras.layers.ZeroPadding2D(padding=(3,3), data_format=(64,64,3)),

First, you didn't define any input layer also, the data_format is a string, one of channels_last (default) or channels_first, source. The correct way to define the above model as follows:
def happyModel():
    model = tf.keras.Sequential(
        [
            ## ZeroPadding2D with padding 3, input shape of 64 x 64 x 3
            tf.keras.layers.ZeroPadding2D(padding=(3,3), 
                         input_shape=(64, 64, 3), data_format=""channels_last""),
           ....
           ....


happy_model = happyModel()
happy_model.summary()
Model: ""sequential_2""
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
zero_padding2d_4 (ZeroPaddin (None, 70, 70, 3)         0         
_________________________________________________________________
conv0 (Conv2D)               (None, 64, 64, 32)        4736      
_________________________________________________________________
bn0 (BatchNormalization)     (None, 64, 64, 32)        128       
_________________________________________________________________
activation_2 (Activation)    (None, 64, 64, 32)        0         
_________________________________________________________________
max_pool0 (MaxPooling2D)     (None, 32, 32, 32)        0         
_________________________________________________________________
flatten_16 (Flatten)         (None, 32768)             0         
_________________________________________________________________
fc (Dense)                   (None, 1)                 32769     
=================================================================
Total params: 37,633
Trainable params: 37,569
Non-trainable params: 64

",tensorflow
how to clean images to use with a mnist trained model,"I am creating a machine learning model for classifying images of numbers. I have trained the model using Tensorflow and Keras using the inbuilt tf.keras.datasets.mnist dataset. The model works quite well with the test images from the mnist dataset itself but I would like to feed it images of my own. The images that I am feeding this model is extracted from a Captcha so they will follow a similar pattern. I have included some examples of the images in this public google drive folder. When I feed these images, I noticed that the model is not very accurate and I have some guesses as to why.

The background of the image creates too much noise in the picture.
The number is not centered.
The image is not striclty in the color format of MNIST training set (Black background white text).

I wanted to ask how can I remove the background and centre it so that the noise in the image is reduced allowing for better classifications.
Here is the model I am using:
import tensorflow as tf
from tensorflow import keras

mnist = keras.datasets.mnist
(x_train, y_train), (x_test, y_test) = mnist.load_data()

class Stopper(keras.callbacks.Callback):
    def on_epoch_end(self, epoch, log={}):
        if log.get('acc') >= 0.99:
            self.model.stop_training = True
            print('\nReached 99% Accuracy. Stopping Training...')

model = keras.Sequential([
    keras.layers.Flatten(),
    keras.layers.Dense(1024, activation=tf.nn.relu),
    keras.layers.Dense(10, activation=tf.nn.softmax)])

model.compile(
    optimizer=tf.train.AdamOptimizer(),
    loss='sparse_categorical_crossentropy',
    metrics=['accuracy'])

x_train, x_test = x_train / 255, x_test / 255

model.fit(x_train, y_train, epochs=10, callbacks=[Stopper()])

And here is my method of importing the image into tensorflow:
from PIL import Image
img = Image.open(""image_file_path"").convert('L').resize((28, 28), Image.ANTIALIAS)
img = np.array(img)
model.predict(img[None,:,:])

I have also included some examples from the MNIST dataset here. I would like a script to convert my images as closely to the MNIST dataset format as possible. Also, since I would have to do this for an indefinite number of images, I would appreciate if you could provide a fully automated method for this conversion.
","You need to train with a dataset similar to the images you're testing. The MNIST data is hand-written numbers, which is not going to be similar to the computer generated fonts for Captcha data. 
What you need to do is gain a catalog of Captcha data similar to what you're predicting on (preferably from the same source you will be inputting to the final model). It's a painstaking task to capture the data, and you'll probably need around 300-400 images for each label before you start to get something useful.
A key note: your model will only ever be as good as the training data you supplied to the model. Trying to make a good model with bad training data is an effort in pure frustration
To address some of your thoughts:

[the model is not very accurate because] the background of the image creates too much noise in the picture.

This is true. If the image data has noise and the neural net was not trained using any noise in the images, then it will not recognize a strong pattern when it encounters this type of distortion. One possible way to combat this is to take clean images and progamatically add noise to the image (noise similar to what you see in the real Captcha) before sending it to be trained.

[the model is not very accurate because] The number is not centered.

Also true for the same reasons. If all the training data is centered, the model will be overtuned for this property and make incorrect guesses. Follow a similar pattern to the one above if you don't have the capacity to manually capture and catalog a good sampling of data.

[the model is not very accurate because] The image is not striclty in the color format of MNIST training set (Black background white text).

You can get around this by applying a binary threshold to the data before processing/ normalize the color input before training. Depending on the amount of noise in the captcha you may have better results allowing the number and noise to retain some of it's color information (still put in greyscale and normalize, just don't apply the threshold).

Additionally I'd recommend using a convolution net rather than the linear network as it is better at distinguishing 2D features like edges and corners. i.e. use keras.layers.Conv2D layers before flattening with keras.layers.Flatten
See the great example found here: Trains a simple convnet on the MNIST dataset.
model = tf.keras.models.Sequential(
    [
        tf.keras.layers.Conv2D(
            32,
            kernel_size=(3, 3),
            activation=tf.nn.relu,
            input_shape=input_shape,
        ),
        tf.keras.layers.Conv2D(64, (3, 3), activation=tf.nn.relu),
        tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),
        tf.keras.layers.Dropout(0.25),
        tf.keras.layers.Flatten(),
        tf.keras.layers.Dense(128, activation=tf.nn.relu),
        tf.keras.layers.Dropout(0.5),
        tf.keras.layers.Dense(
            num_classes, activation=tf.nn.softmax
        ),
    ]
)

I've used this setup for reading fonts in video gameplay footage, and with a test set of 10,000 images I'm achieving 99.98% accuracy, using a random sampling of half the dataset in training, and calculating accuracy using the total set.
",tensorflow
modelcheckpoint not saving the hdf5 file,"I am encountering a strange issue while building a speech-to-text model.
The model is trained and created, but when I try to save it as a hdf5 file, in order to reuse it for testing purposes, the operation simply does not work, and there are no errors popping up in the console…
Here is the full code:
import os
import librosa   
import IPython.display as ipd
import matplotlib.pyplot as plt
import numpy as np
from scipy.io import wavfile 
import warnings
from sklearn.preprocessing import LabelEncoder
from keras.utils import np_utils
from sklearn.model_selection import train_test_split
from keras.layers import Dense, Dropout, Flatten, Conv1D, Input, MaxPooling1D
from keras.models import Model
from keras.callbacks import EarlyStopping, ModelCheckpoint
from keras import backend as K
from keras.models import load_model
import random
    
warnings.filterwarnings(""ignore"")
    
samples, sample_rate = librosa.load(r'C:\Users\makra\OneDrive\Desktop\Conda\Speech-to-text\train\audio\yes\0a7c2a8d_nohash_0.wav', sr = 16000)
    
ipd.Audio(samples, rate=sample_rate)
samples = librosa.resample(samples, sample_rate, 8000)
ipd.Audio(samples, rate=8000)
    
labels=os.listdir(r'C:\Users\makra\OneDrive\Desktop\Conda\Speech-to-text\train\audio')
    
labels=[""yes"", ""no"", ""one"", ""two"", ""three"", ""four"", ""five"", ""six"", ""seven"", ""eight"", ""nine"", ""zero""]
    
all_wave = []
all_label = []
for label in labels:
    print(label)
    waves = [f for f in os.listdir(r'C:\Users\makra\OneDrive\Desktop\Conda\Speech-to-text\train\audio' + '/'+ label) if f.endswith('.wav')]
    for wav in waves:
        samples, sample_rate = librosa.load(r'C:\Users\makra\OneDrive\Desktop\Conda\Speech-to-text\train\audio' + '/' + label + '/' + wav, sr = 16000)
        samples = librosa.resample(samples, sample_rate, 8000)
        if(len(samples)== 8000) : 
            all_wave.append(samples)
            all_label.append(label)
                
le = LabelEncoder()
y=le.fit_transform(all_label)
classes= list(le.classes_)
    
y=np_utils.to_categorical(y, num_classes=len(labels))
all_wave = np.array(all_wave).reshape(-1,8000,1)
    
x_tr, x_val, y_tr, y_val = train_test_split(np.array(all_wave),np.array(y),stratify=y,test_size = 0.2,random_state=777,shuffle=True)
    
K.clear_session()
    
inputs = Input(shape=(8000,1))
    
#First Conv1D layer
conv = Conv1D(8,13, padding='valid', activation='relu', strides=1)(inputs)
conv = MaxPooling1D(3)(conv)
conv = Dropout(0.3)(conv)
    
#Second Conv1D layer
conv = Conv1D(16, 11, padding='valid', activation='relu', strides=1)(conv)
conv = MaxPooling1D(3)(conv)
conv = Dropout(0.3)(conv)
    
#Third Conv1D layer
conv = Conv1D(32, 9, padding='valid', activation='relu', strides=1)(conv)
conv = MaxPooling1D(3)(conv)
conv = Dropout(0.3)(conv)
    
#Fourth Conv1D layer
conv = Conv1D(64, 7, padding='valid', activation='relu', strides=1)(conv)
conv = MaxPooling1D(3)(conv)
conv = Dropout(0.3)(conv)
    
#Flatten layer
conv = Flatten()(conv)
    
#Dense Layer 1
conv = Dense(256, activation='relu')(conv)
conv = Dropout(0.3)(conv)
    
#Dense Layer 2
conv = Dense(128, activation='relu')(conv)
conv = Dropout(0.3)(conv)
    
outputs = Dense(len(labels), activation='softmax')(conv)
    
model = Model(inputs, outputs)
model.summary()    
    model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])
    
es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=10, min_delta=0.0001) 
mc = ModelCheckpoint('best_model.hdf5', monitor='val_acc', verbose=1, save_best_only=True, mode='max')
    
history=model.fit(x_tr, y_tr ,epochs=100, callbacks=[es,mc], batch_size=32, validation_data=(x_val,y_val))

Since no error is shown on the kernel, I am unable to report any.
","I can recommend using save_weights_only=True option in your ModelCheckpoint callback and use the API model.load_weights(checkpoint_path). This will load the latest model weights as shown here.This is just a temporary solution, I think your model accuracy is not increasing and that is the reason your save_best_only option fails to save any model as you have put a condition that accuracy should increase in your ModelCheckpoint callback.
",tensorflow
list operations of a tflite model,"I'm having trouble trying to list the operations of a TFLite model. I know operations can be listed given a frozen graph, but what about a TFLite .tflite model? Can operations be listed?
","As mentioned in the TensorFlow Lite docs, you need to use a tf.lite.Interpreter to parse a .tflite model.
# Load TFLite model and allocate tensors.
interpreter = tf.lite.Interpreter(model_path=""converted_model.tflite"")
interpreter.allocate_tensors()

Then use the get_tensor_details method to get the list of Tensors.
interpreter.get_tensor_details()

As per the docs,

Gets tensor details for every tensor with valid tensor details.
  Tensors where required information about the tensor is not found are not added to the list. This includes temporary tensors without a name.
Returns: A list of dictionaries containing tensor information.

",tensorflow
can39t load data from tensorflow data set,"class Jarvis(Model):
    def __init__(self):
        Model.__init__(self)
        self.model = Sequential()

        # Convulational layers\w MaxPooling
        self.model.add(Conv2D(64, (5, 5), activation=""relu""))
        self.model.add(MaxPooling2D((2, 2)))
        self.model.add(Conv2D(64, (5, 5), activation=""relu""))
        self.model.add(MaxPooling2D((2, 2)))

        # Flattening layers
        self.model.add(Flatten())

        # Dense layers
        self.model.add(Dense(1000))
        self.model.add(Dense(10, activation=""softmax""))

        # Compiling model
        self.model.compile(optimizer=""adam"",
                           loss=""categorical_crossentropy"",
                           metrics=[""accuracy""])

        self.model.fit(x=train_x, y=train_y,
                       epochs=8, batch_size=100)

I'm loading the data like this
(train_x, train_y), (test_x, test_y) = tfds.load(""glue"", split=""train"", data_dir=os.path.dirname(__file__))

","I would suggest you load your data using scikit-learn, as that is much better!
First load your data as a csv or excel file:
import pandas as pd
data = pd.read_csv('Example$Path$')

then you import train_test_split from scikitlearn:
from sklearn.model_selection import train_test_split
x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=101)

#X and y over here are the columns of the data. X is the training columns and y is the column you are trying to predict

",tensorflow
dimension problems error when checking input expected conv2d_1_input to have 4 dimensions but got array with shape 26 26 1,"I have a CNN that gets as input the following images converted by canny edge detection to a binary image.
And outputs one of three categories.
img = cv2.imread(path)
img = cv2.Canny(img, 33, 76)
img = np.resize(img, (26, 26, 1))
imgs.append(img)

As far I understood I have to convert it to a 3 dimensions (26,26,1) image so that the network can work with it. This is my network:
IMG_HEIGHT = 26
IMG_WIDTH = 26
no_Of_Filters=60
size_of_Filter=(5,5)
size_of_pool=(2,2)
no_Of_Nodes = 500
model_new = Sequential([
    Conv2D(no_Of_Filters, size_of_Filter, padding='same', activation='relu', input_shape=(IMG_HEIGHT, IMG_WIDTH , 1)),
    MaxPooling2D(pool_size=size_of_pool),
    Conv2D(no_Of_Filters, size_of_Filter, padding='same', activation='relu'),
    MaxPooling2D(pool_size=size_of_pool),
    Conv2D(64, size_of_Filter, padding='same', activation='relu'),
    MaxPooling2D(pool_size=size_of_pool),
    Flatten(),
    Dense(512, activation='relu'),
    Dense(3, activation='softmax')
])

Training works fine. After I trained and created a model I want to test images agains this network
test_image = cv2.Canny(test_image ,33,76)
test_image = np.resize(test_image, (26, 26, 1))
test_image = test_image [np.newaxis, ...]
prediction = model.predict(test_image)
print(prediction)

Now I get the error:
ValueError: Error when checking input: expected conv2d_1_input to have 4 dimensions, but got array with shape (26, 26, 1)

Why the trained model now wants a 4 dimensional input?
","You need to add a dimension to your array, because as the message says, keras expects a 4D input.
test_image = test_image[np.newaxis, ...]

keras works with shapes such as (1, 26, 26, 1), not (26, 26, 1). The added first dimensions is the batch size and keras needs it. 
",tensorflow
warningtensorflowlayers in a sequential model should only have a single input tensor,"I have copy past code from tensorflow website's introduction to autoencoder first examplefollowing code works with mnist fashion dataset but not mine.This gives me a very long warning.Please tell me what is worng with my dataset
the warning
screen short of same error
here x_train is my dataset:
tf.shape(x_train)

output <tf.Tensor: shape=(3,), dtype=int32, numpy=array([169,**  **28,  28])>

here x_train is the mnist dataset:
tf.shape(x_train)

output<tf.Tensor: shape=(3,), dtype=int32, numpy=array([60000,    28,    28])>

My whole code to make dataset:
dir_path='auto/ttt/'
data=[]
x_train=[]
for i in os.listdir(dir_path):
    img=image.load_img(dir_path+'//'+i,color_mode='grayscale',target_size=(28,28))  
    data=np.array(img)
    data=data/255.0
    x_train.append(data)

this is the warning:
WARNING:tensorflow:Layers in a Sequential model should only have a single input tensor. Received: inputs=(<tf.Tensor 'IteratorGetNext:0' shape=(None, 28) dtype=float32>, <tf.Tensor 'IteratorGetNext:1' shape=(None, 28) dtype=float32>, <tf.Tensor 'IteratorGetNext:2' shape=(None, 28) 
dtype=float32>, <tf.Tensor 'IteratorGetNext:3' shape=(None, 28) 
dtype=float32>, <tf.Tensor 'IteratorGetNext:4' shape=(None, 28) dtype=float32>, <tf.Tensor 'IteratorGetNext:5' shape=(None, 28) dtype=float32>, <tf.Tensor 'IteratorGetNext:6' shape=(None, 28) dtype=float32>, <tf.Tensor 'IteratorGetNext:7' shape=(None, 28) dtype=flo...

also this value error (same warning):
ValueError: Exception encountered when calling layer ""sequential_4"" (type Sequential).
        
Layer ""flatten_2"" expects 1 input(s), but it received 169 input tensors. Inputs received: [<tf.Tensor 'IteratorGetNext:0' shape=(None, 28) dtype=float32>, <tf.Tensor 'IteratorGetNext:1' shape=(None, 28) dtype=float32>, <tf.Tensor 'IteratorGetNext:2' shape=(None, 28) dtype=float32>, <tf.Tensor 'IteratorGetNext:3' shape=(None, 28) dtype=float32>, <tf.Tensor 'IteratorGetNext:4' shape=(None, 28) dtype=float32>, <tf.Tensor 'IteratorGetNext:5' shape=(None, 28) dtype=float32>, <tf.Tensor 'IteratorGetNext:6' shape=(None, 28) dtype=float32>, <tf.Tensor 'IteratorGetNext:7' shape=(None, 28) dtype=float32>, <tf.Tensor 'IteratorGetNext:8' shape=(None, 28) dtype=float3...

","The model.fit() is given a list of arrays as input. A list of arrays is generally passed to fit() when a model has multiple inputs. In this case, the fit() method is treating each array as an input, resulting in the error.
Please convert the data to a tensor as follows and try again.
x_train=tf.convert_to_tensor(x_train) 

Refer to the gist for complete code.
",tensorflow
input image is not compatible with tensorflow model input shape,"I was building a model and I wanted to test its performance, thus I imported a local file and load it and try to predict its label with the following code:
from tensorflow.preprocessing import image
# Other imports for tensorlfow etc.

#...

# Sample image
img_path = ""./Model/data/brain/train/Glioma/images/gg (2).jpg""
img = image.load_img(img_path,target_size=(256,256))
arr = image.img_to_array(img)
t_img = tf.convert_to_tensor(arr)
print(t_img.shape) # Returns (256,256,3)
# Client testing
client = Client(""brain"") # Custom Class. Contains model: Sequential (compiled and trained)
client.predict(img=t_img) # Calls self.model.predict(t_img)

However I get the following error:

Invalid input shape for input Tensor(""data:0"", shape=(32, 256, 3), dtype=float32). Expected shape (None, 256, 256, 3), but input has incompatible shape (32, 256, 3)

I have an input layer in the trained model which has input_shape=[256,256,3] (comes from image width, height, and rgb values)
Can you help me understand the issue and solve it?
","Dr. Snoopy already gave the answer in the comments, but for the sake of completeness a short solution copied from the TF load_image page:
image = keras.utils.load_img(image_path)
input_arr = keras.utils.img_to_array(image)
input_arr = np.array([input_arr])  # Convert single image to a batch.
predictions = model.predict(input_arr)

model.predict() expects batches of images. This solultion would transform your (256, 256, 3) shape to (1, 256, 256, 3). There are also other solutions, e.g. with tf.expand_dims(image, 0) if you rather want to work with tensors directly instead of arrays.
",tensorflow
unbalanced client size in federated learning,"I am applying federated learning on multiple files using Tensoflow Federated. The problem is, that the size of data (number of records) in each file is different.

Is it a problem in federated learning training to have different sizes for each client? if there is how can I overcome it?
Is there a way that I can see how each client performing while federated computation training?


","Is it a problem in federated learning training to have different sizes for each client? if there is how can I overcome it?
This depends on a variety of factors, a large one being the data distribution on the clients. For example, if each clients data looks very similar (e.g. effectively the same distribution, IID) it doesn't particularly matter which client is used.
If this isn't the case, a common technique is to limit the number of maximum steps a client takes on its dataset each round, to promote more equal participation in the training process. In TensorFlow and TFF this can be accomplished using tf.data.Dataset.take to restrict to a maximum number of iterations. In TFF this can be applied to every client using tff.simulation.datasets.ClientData.preprocess. This is discussed with examples in the tutorial Working with TFF's ClientData.
Is there a way that I can see how each client performing while federated computation training?
Clients can return individual metrics to report how they are performing, but this isn't done by default. In tff.learning.algorithms.build_weighted_fed_avg  the metrics_aggregator defaults to tff.learning.metrics.sum_then_finalize which in usually creates global averages of metrics. There isn't an out-of-the-box solution, but one could implement a ""finalize-then-sample"" that would likely meet this need. Re-using tff.aggregators.federated_sample and looking at the source code for sum_then_finalize as an example would be a good place to start.
",tensorflow
how should i determine the input size for layers following the initial layer in a cnn,"I am working on CS50AI unit 5, and this is the code from the number recognition part of the lecture. If I wanted to add another convolutional layer after the max pooling, how would I determine the input shape? Would it be IMG_WIDTH, IMG_HEIGHT, 3 or would I divide IMG_WIDTH and IMG_HEIGHT by 2 because of the max pooling?
Similarly, how is number of nodes in the first dense layer (128) determined? Is that an arbitrary number that I can decide or is it based on something else?
model = tf.keras.models.Sequential([
        tf.keras.layers.Conv2D(
            32, (5, 5), activation=""relu"", input_shape=(IMG_WIDTH, IMG_HEIGHT, 3)
        ),
        tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),
        tf.keras.layers.Flatten(),
        tf.keras.layers.Dense(128, activation=""relu""),
        tf.keras.layers.Dropout(0.5),
        tf.keras.layers.Dense(NUM_CATEGORIES, activation=""softmax"")
    ])

For reference
IMG_WIDTH = 30
IMG_HEIGHT = 30
NUM_CATEGORIES = 3

","Actually you don't need to determine image size for this specific code and keras will itself find out the input size based on input size of the whole network and previous layers.
But if you wish to know what is the input shape for the layer, after the first convolution layer the input shape will be (IMAGE_WIDTH - 4, IMAGE_HEIGHT - 4, 32) because you have 32 channels and used kernel size of 5. And after the pooling layer the height and width will be divided by two as you mentioned.
And the number of nodes in the dense layer can be determined arbitrarily.
",tensorflow
finding most similar sentences among all in python,"Suggestions / refer links /codes are appreciated.
I have a data which is having more than 1500 rows. Each row has a sentence. I am trying to find out the best method to find the most similar sentences among all.
What I have tried

I have tried K-mean algorithm which groups similar sentences in a cluster. But I found a drawback in which I have to pass K to create a cluster. It is hard to guess K. I tried elbo method to guess the clusters but grouping all together isn't sufficient. In this approach I am getting all the data grouped. I am looking for data which is similar above 0.90% data should be returned with ID.

I tried cosine similarity in which I used TfidfVectorizer to create matrix and then passed in cosine similarity. Even this approach didn't worked properly.


What I am looking for
I want an approach where I can pass a threshold example 0.90 data in all rows which are similar to each other above 0.90% should be returned as a result.
Data Sample
ID    |   DESCRIPTION
-----------------------------
10    | Cancel ASN WMS Cancel ASN   
11    | MAXPREDO Validation is corect
12    | Move to QC  
13    | Cancel ASN WMS Cancel ASN   
14    | MAXPREDO Validation is right
15    | Verify files are sent every hours for this interface from Optima
16    | MAXPREDO Validation are correct
17    | Move to QC  
18    | Verify files are not sent

Expected result
Above data which are similar upto 0.90% should get as a result with ID
ID    |   DESCRIPTION
-----------------------------
10    | Cancel ASN WMS Cancel ASN
13    | Cancel ASN WMS Cancel ASN
11    | MAXPREDO Validation is corect  # even spelling is not correct
14    | MAXPREDO Validation is right
16    | MAXPREDO Validation are correct
12    | Move to QC  
17    | Move to QC  

","Why did it not work for you with cosine similarity and the TFIDF-vectorizer?
I tried it and it works with this code:
import pandas as pd
import numpy as np

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity

df = pd.DataFrame(columns=[""ID"",""DESCRIPTION""], data=np.matrix([[10,""Cancel ASN WMS Cancel ASN""],
                                                                [11,""MAXPREDO Validation is corect""],
                                                                [12,""Move to QC""],
                                                                [13,""Cancel ASN WMS Cancel ASN""],
                                                                [14,""MAXPREDO Validation is right""],
                                                                [15,""Verify files are sent every hours for this interface from Optima""],
                                                                [16,""MAXPREDO Validation are correct""],
                                                                [17,""Move to QC""],
                                                                [18,""Verify files are not sent""]
                                                                ]))

corpus = list(df[""DESCRIPTION""].values)

vectorizer = TfidfVectorizer()
X = vectorizer.fit_transform(corpus)

threshold = 0.4

for x in range(0,X.shape[0]):
  for y in range(x,X.shape[0]):
    if(x!=y):
      if(cosine_similarity(X[x],X[y])>threshold):
        print(df[""ID""][x],"":"",corpus[x])
        print(df[""ID""][y],"":"",corpus[y])
        print(""Cosine similarity:"",cosine_similarity(X[x],X[y]))
        print()

The threshold can be adjusted as well, but will not yield the results you want with a threshold of 0.9.
The output for a threshold of 0.4 is:
10 : Cancel ASN WMS Cancel ASN
13 : Cancel ASN WMS Cancel ASN
Cosine similarity: [[1.]]

11 : MAXPREDO Validation is corect
14 : MAXPREDO Validation is right
Cosine similarity: [[0.64183024]]

12 : Move to QC
17 : Move to QC
Cosine similarity: [[1.]]

15 : Verify files are sent every hours for this interface from Optima
18 : Verify files are not sent
Cosine similarity: [[0.44897995]]

With a threshold of 0.39 all your expected sentences are features in the output, but an additional pair with the indices [15,18] can be found as well:
10 : Cancel ASN WMS Cancel ASN
13 : Cancel ASN WMS Cancel ASN
Cosine similarity: [[1.]]

11 : MAXPREDO Validation is corect
14 : MAXPREDO Validation is right
Cosine similarity: [[0.64183024]]

11 : MAXPREDO Validation is corect
16 : MAXPREDO Validation are correct
Cosine similarity: [[0.39895808]]

12 : Move to QC
17 : Move to QC
Cosine similarity: [[1.]]

14 : MAXPREDO Validation is right
16 : MAXPREDO Validation are correct
Cosine similarity: [[0.39895808]]

15 : Verify files are sent every hours for this interface from Optima
18 : Verify files are not sent
Cosine similarity: [[0.44897995]]

",tensorflow
valueerror out of range float values are not json compliant error on heroku and wsl but not on windows,"I'm trying to deploy a CNN model created using Tensorflow to Heroku with FastAPI. The app runs on Heroku but returns an error when trying to make model predictions. Running heroku logs --tail returns this:
2022-02-17T03:32:12.426547+00:00 app[web.1]: [2022-02-17 03:32:12 +0000] [10] [ERROR] Exception in ASGI application
2022-02-17T03:32:12.426549+00:00 app[web.1]: Traceback (most recent call last):
2022-02-17T03:32:12.426549+00:00 app[web.1]: File ""/app/.heroku/python/lib/python3.9/site-packages/uvicorn/protocols/http/h11_impl.py"", line 373, in run_asgi
2022-02-17T03:32:12.426550+00:00 app[web.1]: result = await app(self.scope, self.receive, self.send)
2022-02-17T03:32:12.426550+00:00 app[web.1]: File ""/app/.heroku/python/lib/python3.9/site-packages/uvicorn/middleware/proxy_headers.py"", line 75, in __call__
2022-02-17T03:32:12.426551+00:00 app[web.1]: return await self.app(scope, receive, send)
2022-02-17T03:32:12.426551+00:00 app[web.1]: File ""/app/.heroku/python/lib/python3.9/site-packages/fastapi/applications.py"", line 212, in __call__
2022-02-17T03:32:12.426552+00:00 app[web.1]: await super().__call__(scope, receive, send)
2022-02-17T03:32:12.426552+00:00 app[web.1]: File ""/app/.heroku/python/lib/python3.9/site-packages/starlette/applications.py"", line 112, in __call__
2022-02-17T03:32:12.426552+00:00 app[web.1]: await self.middleware_stack(scope, receive, send)
2022-02-17T03:32:12.426553+00:00 app[web.1]: File ""/app/.heroku/python/lib/python3.9/site-packages/starlette/middleware/errors.py"", line 181, in __call__
2022-02-17T03:32:12.426553+00:00 app[web.1]: raise exc
2022-02-17T03:32:12.426554+00:00 app[web.1]: File ""/app/.heroku/python/lib/python3.9/site-packages/starlette/middleware/errors.py"", line 159, in __call__
2022-02-17T03:32:12.426554+00:00 app[web.1]: await self.app(scope, receive, _send)
2022-02-17T03:32:12.426554+00:00 app[web.1]: File ""/app/.heroku/python/lib/python3.9/site-packages/starlette/exceptions.py"", line 82, in __call__
2022-02-17T03:32:12.426554+00:00 app[web.1]: raise exc
2022-02-17T03:32:12.426555+00:00 app[web.1]: File ""/app/.heroku/python/lib/python3.9/site-packages/starlette/exceptions.py"", line 71, in __call__
2022-02-17T03:32:12.426555+00:00 app[web.1]: await self.app(scope, receive, sender)
2022-02-17T03:32:12.426555+00:00 app[web.1]: File ""/app/.heroku/python/lib/python3.9/site-packages/starlette/routing.py"", line 656, in __call__
2022-02-17T03:32:12.426555+00:00 app[web.1]: await route.handle(scope, receive, send)
2022-02-17T03:32:12.426556+00:00 app[web.1]: File ""/app/.heroku/python/lib/python3.9/site-packages/starlette/routing.py"", line 259, in handle
2022-02-17T03:32:12.426556+00:00 app[web.1]: await self.app(scope, receive, send)
2022-02-17T03:32:12.426556+00:00 app[web.1]: File ""/app/.heroku/python/lib/python3.9/site-packages/starlette/routing.py"", line 61, in app
2022-02-17T03:32:12.426556+00:00 app[web.1]: response = await func(request)
2022-02-17T03:32:12.426557+00:00 app[web.1]: File ""/app/.heroku/python/lib/python3.9/site-packages/fastapi/routing.py"", line 250, in app
2022-02-17T03:32:12.426557+00:00 app[web.1]: response = actual_response_class(response_data, **response_args)
2022-02-17T03:32:12.426557+00:00 app[web.1]: File ""/app/.heroku/python/lib/python3.9/site-packages/starlette/responses.py"", line 49, in __init__
2022-02-17T03:32:12.426558+00:00 app[web.1]: self.body = self.render(content)
2022-02-17T03:32:12.426558+00:00 app[web.1]: File ""/app/.heroku/python/lib/python3.9/site-packages/starlette/responses.py"", line 174, in render
2022-02-17T03:32:12.426558+00:00 app[web.1]: return json.dumps(
2022-02-17T03:32:12.426559+00:00 app[web.1]: File ""/app/.heroku/python/lib/python3.9/json/__init__.py"", line 234, in dumps
2022-02-17T03:32:12.426559+00:00 app[web.1]: return cls(
2022-02-17T03:32:12.426559+00:00 app[web.1]: File ""/app/.heroku/python/lib/python3.9/json/encoder.py"", line 199, in encode
2022-02-17T03:32:12.426560+00:00 app[web.1]: chunks = self.iterencode(o, _one_shot=True)
2022-02-17T03:32:12.426560+00:00 app[web.1]: File ""/app/.heroku/python/lib/python3.9/json/encoder.py"", line 257, in iterencode
2022-02-17T03:32:12.426560+00:00 app[web.1]: return _iterencode(o, 0)
2022-02-17T03:32:12.426561+00:00 app[web.1]: ValueError: Out of range float values are not JSON compliant

Most notably on the last line, it says `ValueError: Out of range float values are not JSON compliant"". My procfile looks like this:
web: gunicorn -w 2 -k uvicorn.workers.UvicornWorker main:app

When trying to run the server on WSL using python -m uvicorn main:app, I get this error
ERROR:    Exception in ASGI application
Traceback (most recent call last):
  File ""/home/eruaro/.local/lib/python3.8/site-packages/uvicorn/protocols/http/httptools_impl.py"", line 375, in run_asgi
    result = await app(self.scope, self.receive, self.send)
  File ""/home/eruaro/.local/lib/python3.8/site-packages/uvicorn/middleware/proxy_headers.py"", line 75, in __call__
    return await self.app(scope, receive, send)
  File ""/home/eruaro/.local/lib/python3.8/site-packages/fastapi/applications.py"", line 212, in __call__
    await super().__call__(scope, receive, send)
  File ""/home/eruaro/.local/lib/python3.8/site-packages/starlette/applications.py"", line 112, in __call__
    await self.middleware_stack(scope, receive, send)
  File ""/home/eruaro/.local/lib/python3.8/site-packages/starlette/middleware/errors.py"", line 181, in __call__
    raise exc
  File ""/home/eruaro/.local/lib/python3.8/site-packages/starlette/middleware/errors.py"", line 159, in __call__
    await self.app(scope, receive, _send)
  File ""/home/eruaro/.local/lib/python3.8/site-packages/starlette/exceptions.py"", line 82, in __call__
    raise exc
  File ""/home/eruaro/.local/lib/python3.8/site-packages/starlette/exceptions.py"", line 71, in __call__
    await self.app(scope, receive, sender)
  File ""/home/eruaro/.local/lib/python3.8/site-packages/starlette/routing.py"", line 656, in __call__
    await route.handle(scope, receive, send)
  File ""/home/eruaro/.local/lib/python3.8/site-packages/starlette/routing.py"", line 259, in handle
    await self.app(scope, receive, send)
  File ""/home/eruaro/.local/lib/python3.8/site-packages/starlette/routing.py"", line 61, in app
    response = await func(request)
  File ""/home/eruaro/.local/lib/python3.8/site-packages/fastapi/routing.py"", line 250, in app
    response = actual_response_class(response_data, **response_args)
  File ""/home/eruaro/.local/lib/python3.8/site-packages/starlette/responses.py"", line 49, in __init__
    self.body = self.render(content)
  File ""/home/eruaro/.local/lib/python3.8/site-packages/starlette/responses.py"", line 174, in render
    return json.dumps(
  File ""/usr/lib/python3.8/json/__init__.py"", line 234, in dumps
    return cls(
  File ""/usr/lib/python3.8/json/encoder.py"", line 199, in encode
    chunks = self.iterencode(o, _one_shot=True)
  File ""/usr/lib/python3.8/json/encoder.py"", line 257, in iterencode
    return _iterencode(o, 0)
ValueError: Out of range float values are not JSON compliant

It's similar to the one I get on Heroku. However, on Windows, using the same command like the one I used on WSL, the app works. No error is returned and I'm able to make predictions on the server. How do I remove the JSON compliant error?
For reference, my code is in a singular main.py file:
from fastapi import FastAPI
from tensorflow.keras.models import load_model
from tensorflow.keras.utils import get_file 
from tensorflow.keras.utils import load_img 
from tensorflow.keras.utils import img_to_array
from tensorflow import expand_dims
from tensorflow.nn import softmax
from numpy import argmax
from numpy import max
from numpy import array

app = FastAPI()
model_dir = ""food-vision-model.h5""
model = load_model(model_dir)

class_predictions = array([
    'apple_pie',
    'baby_back_ribs',
    'baklava',
    'beef_carpaccio',
    'beef_tartare',
    'beet_salad',
    'beignets',
    'bibimbap',
    'bread_pudding',
    'breakfast_burrito',
    'bruschetta',
    'caesar_salad',
    'cannoli',
    'caprese_salad',
    'carrot_cake',
    'ceviche',
    'cheesecake',
    'cheese_plate',
    'chicken_curry',
    'chicken_quesadilla',
    'chicken_wings',
    'chocolate_cake',
    'chocolate_mousse',
    'churros',
    'clam_chowder',
    'club_sandwich',
    'crab_cakes',
    'creme_brulee',
    'croque_madame',
    'cup_cakes',
    'deviled_eggs',
    'donuts',
    'dumplings',
    'edamame',
    'eggs_benedict',
    'escargots',
    'falafel',
    'filet_mignon',
    'fish_and_chips',
    'foie_gras',
    'french_fries',
    'french_onion_soup',
    'french_toast',
    'fried_calamari',
    'fried_rice',
    'frozen_yogurt',
    'garlic_bread',
    'gnocchi',
    'greek_salad',
    'grilled_cheese_sandwich',
    'grilled_salmon',
    'guacamole',
    'gyoza',
    'hamburger',
    'hot_and_sour_soup',
    'hot_dog',
    'huevos_rancheros',
    'hummus',
    'ice_cream',
    'lasagna',
    'lobster_bisque',
    'lobster_roll_sandwich',
    'macaroni_and_cheese',
    'macarons',
    'miso_soup',
    'mussels',
    'nachos',
    'omelette',
    'onion_rings',
    'oysters',
    'pad_thai',
    'paella',
    'pancakes',
    'panna_cotta',
    'peking_duck',
    'pho',
    'pizza',
    'pork_chop',
    'poutine',
    'prime_rib',
    'pulled_pork_sandwich',
    'ramen',
    'ravioli',
    'red_velvet_cake',
    'risotto',
    'samosa',
    'sashimi',
    'scallops',
    'seaweed_salad',
    'shrimp_and_grits',
    'spaghetti_bolognese',
    'spaghetti_carbonara',
    'spring_rolls',
    'steak',
    'strawberry_shortcake',
    'sushi',
    'tacos',
    'takoyaki',
    'tiramisu',
    'tuna_tartare',
    'waffles'
])

@app.get(""/"")
async def root():
    return {""message"": ""Welcome to the Food Vision API!""}

@app.post(""/net/image/prediction/"")
async def get_net_image_prediction(image_link: str = """"):
    if image_link == """":
        return {""message"": ""No image link provided""}
    
    img_path = get_file(
        origin = image_link
    )
    img = load_img(
        img_path, 
        target_size = (224, 224)
    )

    img_array = img_to_array(img)
    img_array = expand_dims(img_array, 0)

    pred = model.predict(img_array)
    score = softmax(pred[0])

    class_prediction = class_predictions[argmax(score)]
    model_score = round(max(score) * 100, 2)

    return {
        ""model_prediction_class"": class_prediction,
        ""model_prediction_score"": model_score
    }

","I've solved the issue, the solution is to dockerize the whole application and then deploy it to Heroku. That way it can work when running on WSL (Linux), and by extension on Heroku as well (which uses Linux).
This thus requires editing the main.py file by a little bit:
from fastapi import FastAPI
from tensorflow.keras.models import load_model
from tensorflow.keras.utils import get_file 
from tensorflow.keras.utils import load_img 
from tensorflow.keras.utils import img_to_array
from tensorflow import expand_dims
from tensorflow.nn import softmax
from numpy import argmax
from numpy import max
from numpy import array
from json import dumps
from uvicorn import run
import os

app = FastAPI()
model_dir = ""food-vision-model.h5""
model = load_model(model_dir)

class_predictions = array([
    'apple_pie',
    'baby_back_ribs',
    'baklava',
    'beef_carpaccio',
    'beef_tartare',
    'beet_salad',
    'beignets',
    'bibimbap',
    'bread_pudding',
    'breakfast_burrito',
    'bruschetta',
    'caesar_salad',
    'cannoli',
    'caprese_salad',
    'carrot_cake',
    'ceviche',
    'cheesecake',
    'cheese_plate',
    'chicken_curry',
    'chicken_quesadilla',
    'chicken_wings',
    'chocolate_cake',
    'chocolate_mousse',
    'churros',
    'clam_chowder',
    'club_sandwich',
    'crab_cakes',
    'creme_brulee',
    'croque_madame',
    'cup_cakes',
    'deviled_eggs',
    'donuts',
    'dumplings',
    'edamame',
    'eggs_benedict',
    'escargots',
    'falafel',
    'filet_mignon',
    'fish_and_chips',
    'foie_gras',
    'french_fries',
    'french_onion_soup',
    'french_toast',
    'fried_calamari',
    'fried_rice',
    'frozen_yogurt',
    'garlic_bread',
    'gnocchi',
    'greek_salad',
    'grilled_cheese_sandwich',
    'grilled_salmon',
    'guacamole',
    'gyoza',
    'hamburger',
    'hot_and_sour_soup',
    'hot_dog',
    'huevos_rancheros',
    'hummus',
    'ice_cream',
    'lasagna',
    'lobster_bisque',
    'lobster_roll_sandwich',
    'macaroni_and_cheese',
    'macarons',
    'miso_soup',
    'mussels',
    'nachos',
    'omelette',
    'onion_rings',
    'oysters',
    'pad_thai',
    'paella',
    'pancakes',
    'panna_cotta',
    'peking_duck',
    'pho',
    'pizza',
    'pork_chop',
    'poutine',
    'prime_rib',
    'pulled_pork_sandwich',
    'ramen',
    'ravioli',
    'red_velvet_cake',
    'risotto',
    'samosa',
    'sashimi',
    'scallops',
    'seaweed_salad',
    'shrimp_and_grits',
    'spaghetti_bolognese',
    'spaghetti_carbonara',
    'spring_rolls',
    'steak',
    'strawberry_shortcake',
    'sushi',
    'tacos',
    'takoyaki',
    'tiramisu',
    'tuna_tartare',
    'waffles'
])

@app.get(""/"")
async def root():
    return {""message"": ""Welcome to the Food Vision API!""}

@app.post(""/net/image/prediction/"")
async def get_net_image_prediction(image_link: str = """"):
    if image_link == """":
        return {""message"": ""No image link provided""}
    
    img_path = get_file(
        origin = image_link
    )
    img = load_img(
        img_path, 
        target_size = (224, 224)
    )

    img_array = img_to_array(img)
    img_array = expand_dims(img_array, 0)

    pred = model.predict(img_array)
    score = softmax(pred[0])

    class_prediction = class_predictions[argmax(score)]
    model_score = round(max(score) * 100, 2)
    model_score = dumps(model_score.tolist())

    return {
        ""model_prediction_class"": class_prediction,
        ""model_prediction_score"": model_score
    }

if __name__ == ""__main__"":
    port = int(os.environ.get('PORT', 5000))
    run(app, host=""0.0.0.0"", port=port)

The main difference here is the use of the os.environ.get(). I found that just manually defining a port like port = 5000 returns an R10 (boot timeout) error on Heroku.
From here, I created a Dockerfile with the following contents:
FROM python:3.7.3-stretch

# Maintainer info
LABEL maintainer=""erickson_ruaroii@dlsu.edu.ph""

# Make working directories
RUN  mkdir -p  /my-directory/project
WORKDIR  /my-directory/project

# Upgrade pip with no cache
RUN pip install --no-cache-dir -U pip

# Copy application requirements file to the created working directory
COPY requirements.txt .

# Install application dependencies from the requirements file
RUN pip install -r requirements.txt

# Copy every file in the source folder to the created working directory
COPY  . .

# Run the python application
CMD [""python"", ""main.py""]

With requirements.txt having these:
fastapi==0.73.0
gunicorn==20.1.0
numpy==1.19.5
uvicorn==0.15.0
image==1.5.33
tensorflow-cpu==2.7.0

I used tensorflow-cpu to bypass the slug size and memory limit of Heroku on a free account.
From here I was then able to build the docker image and deploy it to heroku.
$ docker image build -t app-name .
$ heroku create app-name
$ heroku container:push web --app app-name
$ heroku container:release web --app food-vision-api

When running locally, I used the command below:
$ docker run -p 5000:5000 -d app-name

",tensorflow
tensorflow predict timeseries alignment in python,"Suppose I create a sequential input LSTM in Tensorflow along the lines of:
def Sequential_Input_LSTM(df, input_sequence):
    df_np = df.to_numpy()
    X = []
    y = []
    
    for i in range(len(df_np) - input_sequence):
        row = [a for a in df_np[i:i + input_sequence]]
        X.append(row)
        label = df_np[i + input_sequence]
        y.append(label)
        
    return np.array(X), np.array(y)

X, y = Sequential_Input_LSTM(df_data , 10) # pandas DataFrame df_data contains our data

In this example, I slice my data in X (input vector) and y (labels) in such a way that e.g. the first 10 values (sequence length) serve as X and the 11th value is the first y. Then, the window of 10 values is moved one step to the right (one timestep further) and we take again 10 values for X and the value after this second row as the next y, and so on.
Then suppose I take a part of X as my X_test, and use a LSTM model to make a time-series prediction, like predictions = model.predict(X_test).
When I actually tried this, and plotted the results from predict(X_test), it looks like the y array and the predictions results are synchronized without further adjustments. I expected that I would have to shift the prediction array manually 10 timesteps to the right when plotting it together with the labels, since I cannot explain where the first 10 timestamps of prediction come from.
Where do the predictions for the first 10 timesteps of X_test come from, seeing as the model has not received 10 input sequence values yet? Does Tensorflow use the last timesteps in X_test to create the predictions of the first 10 values, or are the predictions at the beginning just pure guesses?
","If I get it right, the problem is that the first 10 timesteps from X_test use the last 10 timesteps from X (or more precise, X_train) for the predicition. With big enough X_test, this does not make much difference, but is theoretically data leakage from the training set to the test set.
I demonstrate it with a small example (correct me if I'm wrong):
df_data = [0, 1, 2, .., 15]  # len 16
window_size = 3
X = [[0,1,2], [1,2,3], [2,3,4], ..., [12,13,14]]  # len 13
y = [3, 4, 5, .., 15]  # len 13
# split the data 10-3 for train-test
X_train = [[0,1,2], [1,2,3], [2,3,4], ..., [9,10,11]]
y_train = [3, 4, 5, .., 12]
X_test = [[10,11,12], [11,12,13], [12,13,14]]
y_test = [13, 14, 15]

The problem in this example is that 10 and 11 are both used in sequences for X_train and X_test. So you have to first split df_data into train/test (without shuffling) and then do the sequencing separately. With this, you'd lose the first n-th values for y in both train and test.

Edit: To clarify how the actual split would look like for a bit smaller X_train (the X_train from above is too big to leave predictions for X_test without leakage):
df_data = [0, 1, 2, .., 15]  # len 16
window_size = 3
train = [0, 1, 2, .., 10]
test = [11, 12, .., 15]
# make windows
X_train = [[0,1,2], [1,2,3], [2,3,4], ..., [7,8,9]]
y_train = [3, 4, 5, .., 10]
X_test = [[11,12,13], [12,13,14]]
y_test = [14,15]

Here, no values that are present in X_train or y_train are present in either X_test or y_test. The first 3 values in both train and test can not be predicted and are only used for windowing.
",tensorflow
image processing in tensor flow tfx pipelines,"I am trying to get a Tensorflow TFX pipeline up and running using the MNIST dataset.
# Imports
import pandas as pd
import numpy as np
from keras.datasets import mnist
import tensorflow as tf
from tfx import v1 as tfx
import os
from tfx.components import ImportExampleGen

from platform import python_version
python_version() #'3.8.8'


# Load the data - 60,000 training examples and 10,000 testing examples
(train_x, train_y), (test_x, test_y) = mnist.load_data()

Setup pipeline paths
_pipeline_root = './pipeline'
_data_root = './data'
if not os.path.isdir(_pipeline_root) and not os.path.isdir(_data_root):
    !mkdir {_pipeline_root}
    !mkdir {_data_root}

Write the data to TF.record format and save in eval and train dirs. NOTE that the MNIST data starts as a numpy array 28x28 and is converted to a bytestring to enable it to be encoded as part of the Tf.record.

def _bytes_feature(value):
    """"""Returns a bytes_list from a string / byte.""""""
    if isinstance(value, type(tf.constant(0))): # if value ist tensor
        value = value.numpy() # get value of tensor
    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))

def _int64_feature(value):
    """"""Returns an int64_list from a bool / enum / int / uint.""""""
    return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))



def serialize_array(array):
    array = tf.io.serialize_tensor(array)
    return array

def image_label_to_tf_train(image, label):

    image_shape = np.shape(image)
    #define the dictionary -- the structure -- of our single example
    data = {
        'height': _int64_feature(image_shape[0]),
        'width': _int64_feature(image_shape[1]),
        'raw_image' : _bytes_feature(serialize_array(image)),
        'label' : _int64_feature(label)
    }
    #create an Example, wrapping the single features
    return tf.train.Example(features=tf.train.Features(feature=data))

def write_images_to_tfr_short(images, labels, filename:str=""images"", folder = """"):
    if not os.path.isdir(folder):
        !mkdir {folder}
    filename= folder + ""/"" + filename+"".tfrecords""
    writer = tf.io.TFRecordWriter(filename) #create a writer that'll store our data to disk
    count = 0

    for index in range(len(images)):

        #get the data we want to write
        current_image = images[index]
        current_label = labels[index]

        out = image_label_to_tf_train(image=current_image, label=current_label)
        writer.write(out.SerializeToString())
        count += 1

    writer.close()
    print(f""Wrote {count} elements to TFRecord"")
    return count

The next stage is to call the transform component which uses the preprocessing_fn. This function should process all the data so for example divide the image array by 255 is a standard feature process. But the image is still as a bytestring and I can't for the life of me figure out how to turn it back into an array. The below is what I have tried.
def preprocessing_fn(inputs):
    """"""tf.transform's callback function for preprocessing inputs.
    Args:
        inputs: map from feature keys to raw not-yet-transformed features.
    Returns:
        Map from string feature key to transformed feature operations.
    """"""
    

    # Initialize outputs dictionary
    outputs = {}
    
    
    raw_image_dataset = inputs[_IMAGE_KEY]
    
    
    img = tf.io.decode_raw(raw_image_dataset, tf.int64)
    
    
    outputs[_IMAGE_KEY] = img
  
    
    
    outputs[_LABEL_KEY] = tf.cast(inputs[_LABEL_KEY], tf.int64)



    return outputs

I get the following error:
WARNING:root:This output type hint will be ignored and not used for type-checking purposes. Typically, output type hints for a PTransform are single (or nested) types wrapped by a PCollection, PDone, or None. Got: Tuple[Dict[str, Union[NoneType, _Dataset]], Union[Dict[str, Dict[str, PCollection]], NoneType], int] instead.
WARNING:root:This output type hint will be ignored and not used for type-checking purposes. Typically, output type hints for a PTransform are single (or nested) types wrapped by a PCollection, PDone, or None. Got: Tuple[Dict[str, Union[NoneType, _Dataset]], Union[Dict[str, Dict[str, PCollection]], NoneType], int] instead.
WARNING:root:Make sure that locally built Python SDK docker image has Python 3.8 interpreter.
INFO:tensorflow:Assets written to: ./pipeline/Transform/transform_graph/225/.temp_path/tftransform_tmp/26150ae80de847fab932efeb0f0c610f/assets
INFO:tensorflow:Assets written to: ./pipeline/Transform/transform_graph/225/.temp_path/tftransform_tmp/26150ae80de847fab932efeb0f0c610f/assets
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
/opt/conda/lib/python3.8/site-packages/apache_beam/runners/common.cpython-38-x86_64-linux-gnu.so in apache_beam.runners.common.DoFnRunner.process()

/opt/conda/lib/python3.8/site-packages/apache_beam/runners/common.cpython-38-x86_64-linux-gnu.so in apache_beam.runners.common.PerWindowInvoker.invoke_process()

/opt/conda/lib/python3.8/site-packages/apache_beam/runners/common.cpython-38-x86_64-linux-gnu.so in apache_beam.runners.common.PerWindowInvoker._invoke_process_per_window()

/opt/conda/lib/python3.8/site-packages/apache_beam/transforms/core.py in <lambda>(x, *args, **kwargs)
   1636   if fn_takes_side_inputs(fn):
-> 1637     wrapper = lambda x, *args, **kwargs: [fn(x, *args, **kwargs)]
   1638   else:

/opt/conda/lib/python3.8/site-packages/tensorflow_transform/beam/impl.py in _create_v2_saved_model(tensor_replacement_map, base_temp_dir, preprocessing_fn, input_signature, baseline_analyzers_fingerprint, output_keys_to_name_map)
    662   saved_model_dir = beam_common.get_unique_temp_path(base_temp_dir)
--> 663   impl_helper.trace_and_write_v2_saved_model(saved_model_dir, preprocessing_fn,
    664                                              input_signature, base_temp_dir,

/opt/conda/lib/python3.8/site-packages/tensorflow_transform/impl_helper.py in trace_and_write_v2_saved_model(saved_model_dir, preprocessing_fn, input_signature, base_temp_dir, baseline_analyzers_fingerprint, tensor_replacement_map, output_keys_to_name_map)
    893       analyzer_nodes.TENSOR_REPLACEMENTS):
--> 894     metadata = _trace_and_get_metadata(concrete_transform_fn, structured_inputs,
    895                                        preprocessing_fn, base_temp_dir,

/opt/conda/lib/python3.8/site-packages/tensorflow_transform/impl_helper.py in _trace_and_get_metadata(concrete_transform_fn, structured_inputs, preprocessing_fn, base_temp_dir, tensor_replacement_map)
    805   return dataset_metadata.DatasetMetadata(
--> 806       schema=schema_inference.infer_feature_schema_v2(
    807           concrete_transform_fn.structured_outputs,

/opt/conda/lib/python3.8/site-packages/tensorflow_transform/schema_inference.py in infer_feature_schema_v2(features, concrete_metadata_fn, evaluate_schema_overrides)
    255         metadata)
--> 256   return _infer_feature_schema_common(
    257       features,

/opt/conda/lib/python3.8/site-packages/tensorflow_transform/schema_inference.py in _infer_feature_schema_common(features, tensor_ranges, feature_annotations, global_annotations, is_evaluation_complete)
    300           min=min_value, max=max_value, is_categorical=True)
--> 301   feature_spec = _feature_spec_from_batched_tensors(features,
    302                                                     is_evaluation_complete)

/opt/conda/lib/python3.8/site-packages/tensorflow_transform/schema_inference.py in _feature_spec_from_batched_tensors(tensors, is_evaluation_complete)
    128           dim is None for dim in shape.as_list()[1:]):
--> 129         raise ValueError(
    130             'Feature {} ({}) had invalid shape {} for FixedLenFeature: apart '

ValueError: Feature raw_image (Tensor(""Identity_1:0"", shape=(None, 1, None), dtype=int64)) had invalid shape (None, 1, None) for FixedLenFeature: apart from the batch dimension, all dimensions must have known size

During handling of the above exception, another exception occurred:

ValueError                                Traceback (most recent call last)
<ipython-input-37-7beafa4fe436> in <module>
      3     schema=schema_gen.outputs['schema'],
      4     module_file=os.path.abspath(_mnist_transform_module))
----> 5 context.run(transform, enable_cache=False)

/opt/conda/lib/python3.8/site-packages/tfx/orchestration/experimental/interactive/interactive_context.py in run_if_ipython(*args, **kwargs)
     61       # __IPYTHON__ variable is set by IPython, see
     62       # https://ipython.org/ipython-doc/rel-0.10.2/html/interactive/reference.html#embedding-ipython.
---> 63       return fn(*args, **kwargs)
     64     else:
     65       absl.logging.warning(

/opt/conda/lib/python3.8/site-packages/tfx/orchestration/experimental/interactive/interactive_context.py in run(self, component, enable_cache, beam_pipeline_args)
    181         telemetry_utils.LABEL_TFX_RUNNER: runner_label,
    182     }):
--> 183       execution_id = launcher.launch().execution_id
    184 
    185     return execution_result.ExecutionResult(

/opt/conda/lib/python3.8/site-packages/tfx/orchestration/launcher/base_component_launcher.py in launch(self)
    198       # be immutable in this context.
    199       # output_dict can still be changed, specifically properties.
--> 200       self._run_executor(execution_decision.execution_id,
    201                          copy.deepcopy(execution_decision.input_dict),
    202                          execution_decision.output_dict,

/opt/conda/lib/python3.8/site-packages/tfx/orchestration/launcher/in_process_component_launcher.py in _run_executor(self, execution_id, input_dict, output_dict, exec_properties)
     71     # be immutable in this context.
     72     # output_dict can still be changed, specifically properties.
---> 73     executor.Do(
     74         copy.deepcopy(input_dict), output_dict, copy.deepcopy(exec_properties))

/opt/conda/lib/python3.8/site-packages/tfx/components/transform/executor.py in Do(self, input_dict, output_dict, exec_properties)
    581     # remove the `_pip_dependencies` attribute.
    582     with udf_utils.TempPipInstallContext(self._pip_dependencies):
--> 583       TransformProcessor().Transform(label_inputs, label_outputs, status_file)
    584     logging.debug('Cleaning up temp path %s on executor success', temp_path)
    585     io_utils.delete_dir(temp_path)

/opt/conda/lib/python3.8/site-packages/tfx/components/transform/executor.py in Transform(***failed resolving arguments***)
   1114     materialization_format = (
   1115         transform_paths_file_formats[-1] if materialize_output_paths else None)
-> 1116     self._RunBeamImpl(analyze_data_list, transform_data_list, preprocessing_fn,
   1117                       stats_options_updater_fn, force_tf_compat_v1,
   1118                       input_dataset_metadata, transform_output_path,

/opt/conda/lib/python3.8/site-packages/tfx/components/transform/executor.py in _RunBeamImpl(self, analyze_data_list, transform_data_list, preprocessing_fn, stats_options_updater_fn, force_tf_compat_v1, input_dataset_metadata, transform_output_path, raw_examples_data_format, temp_path, input_cache_dir, output_cache_dir, disable_statistics, per_set_stats_output_paths, materialization_format, analyze_paths_count, stats_output_paths, make_beam_pipeline_fn)
   1496             for dataset in transform_data_list:
   1497               infix = 'TransformIndex{}'.format(dataset.index)
-> 1498               (dataset.transformed
   1499                | 'EncodeAndSerialize[{}]'.format(infix) >> beam.ParDo(
   1500                    self._RecordBatchToExamplesFn(transformed_schema_proto))

/opt/conda/lib/python3.8/site-packages/apache_beam/pipeline.py in __exit__(self, exc_type, exc_val, exc_tb)
    594     try:
    595       if not exc_type:
--> 596         self.result = self.run()
    597         self.result.wait_until_finish()
    598     finally:

/opt/conda/lib/python3.8/site-packages/apache_beam/pipeline.py in run(self, test_runner_api)
    571         finally:
    572           shutil.rmtree(tmpdir)
--> 573       return self.runner.run_pipeline(self, self._options)
    574     finally:
    575       if not is_in_ipython():

/opt/conda/lib/python3.8/site-packages/apache_beam/runners/direct/direct_runner.py in run_pipeline(self, pipeline, options)
    129       runner = BundleBasedDirectRunner()
    130 
--> 131     return runner.run_pipeline(pipeline, options)
    132 
    133 

/opt/conda/lib/python3.8/site-packages/apache_beam/runners/portability/fn_api_runner/fn_runner.py in run_pipeline(self, pipeline, options)
    197         options.view_as(pipeline_options.ProfilingOptions))
    198 
--> 199     self._latest_run_result = self.run_via_runner_api(
    200         pipeline.to_runner_api(default_environment=self._default_environment))
    201     return self._latest_run_result

/opt/conda/lib/python3.8/site-packages/apache_beam/runners/portability/fn_api_runner/fn_runner.py in run_via_runner_api(self, pipeline_proto)
    208     # TODO(pabloem, BEAM-7514): Create a watermark manager (that has access to
    209     #   the teststream (if any), and all the stages).
--> 210     return self.run_stages(stage_context, stages)
    211 
    212   @contextlib.contextmanager

/opt/conda/lib/python3.8/site-packages/apache_beam/runners/portability/fn_api_runner/fn_runner.py in run_stages(self, stage_context, stages)
    393           )
    394 
--> 395           stage_results = self._run_stage(
    396               runner_execution_context, bundle_context_manager)
    397 

/opt/conda/lib/python3.8/site-packages/apache_beam/runners/portability/fn_api_runner/fn_runner.py in _run_stage(self, runner_execution_context, bundle_context_manager)
    658     while True:
    659       last_result, deferred_inputs, fired_timers, watermark_updates = (
--> 660           self._run_bundle(
    661               runner_execution_context,
    662               bundle_context_manager,

/opt/conda/lib/python3.8/site-packages/apache_beam/runners/portability/fn_api_runner/fn_runner.py in _run_bundle(self, runner_execution_context, bundle_context_manager, data_input, data_output, input_timers, expected_timer_output, bundle_manager)
    781         expected_timer_output)
    782 
--> 783     result, splits = bundle_manager.process_bundle(
    784         data_input, data_output, input_timers, expected_timer_output)
    785     # Now we collect all the deferred inputs remaining from bundle execution.

/opt/conda/lib/python3.8/site-packages/apache_beam/runners/portability/fn_api_runner/fn_runner.py in process_bundle(self, inputs, expected_outputs, fired_timers, expected_output_timers, dry_run)
   1092             process_bundle_descriptor.id,
   1093             cache_tokens=[next(self._cache_token_generator)]))
-> 1094     result_future = self._worker_handler.control_conn.push(process_bundle_req)
   1095 
   1096     split_results = []  # type: List[beam_fn_api_pb2.ProcessBundleSplitResponse]

/opt/conda/lib/python3.8/site-packages/apache_beam/runners/portability/fn_api_runner/worker_handlers.py in push(self, request)
    376       self._uid_counter += 1
    377       request.instruction_id = 'control_%s' % self._uid_counter
--> 378     response = self.worker.do_instruction(request)
    379     return ControlFuture(request.instruction_id, response)
    380 

/opt/conda/lib/python3.8/site-packages/apache_beam/runners/worker/sdk_worker.py in do_instruction(self, request)
    578     if request_type:
    579       # E.g. if register is set, this will call self.register(request.register))
--> 580       return getattr(self, request_type)(
    581           getattr(request, request_type), request.instruction_id)
    582     else:

/opt/conda/lib/python3.8/site-packages/apache_beam/runners/worker/sdk_worker.py in process_bundle(self, request, instruction_id)
    616         with self.maybe_profile(instruction_id):
    617           delayed_applications, requests_finalization = (
--> 618               bundle_processor.process_bundle(instruction_id))
    619           monitoring_infos = bundle_processor.monitoring_infos()
    620           monitoring_infos.extend(self.state_cache_metrics_fn())

/opt/conda/lib/python3.8/site-packages/apache_beam/runners/worker/bundle_processor.py in process_bundle(self, instruction_id)
    993                   element.timer_family_id, timer_data)
    994           elif isinstance(element, beam_fn_api_pb2.Elements.Data):
--> 995             input_op_by_transform_id[element.transform_id].process_encoded(
    996                 element.data)
    997 

/opt/conda/lib/python3.8/site-packages/apache_beam/runners/worker/bundle_processor.py in process_encoded(self, encoded_windowed_values)
    219       decoded_value = self.windowed_coder_impl.decode_from_stream(
    220           input_stream, True)
--> 221       self.output(decoded_value)
    222 
    223   def monitoring_infos(self, transform_id, tag_to_pcollection_id):

/opt/conda/lib/python3.8/site-packages/apache_beam/runners/worker/operations.cpython-38-x86_64-linux-gnu.so in apache_beam.runners.worker.operations.Operation.output()

/opt/conda/lib/python3.8/site-packages/apache_beam/runners/worker/operations.cpython-38-x86_64-linux-gnu.so in apache_beam.runners.worker.operations.Operation.output()

/opt/conda/lib/python3.8/site-packages/apache_beam/runners/worker/operations.cpython-38-x86_64-linux-gnu.so in apache_beam.runners.worker.operations.SingletonConsumerSet.receive()

/opt/conda/lib/python3.8/site-packages/apache_beam/runners/worker/operations.cpython-38-x86_64-linux-gnu.so in apache_beam.runners.worker.operations.DoOperation.process()

/opt/conda/lib/python3.8/site-packages/apache_beam/runners/worker/operations.cpython-38-x86_64-linux-gnu.so in apache_beam.runners.worker.operations.DoOperation.process()

/opt/conda/lib/python3.8/site-packages/apache_beam/runners/common.cpython-38-x86_64-linux-gnu.so in apache_beam.runners.common.DoFnRunner.process()

/opt/conda/lib/python3.8/site-packages/apache_beam/runners/common.cpython-38-x86_64-linux-gnu.so in apache_beam.runners.common.DoFnRunner._reraise_augmented()

/opt/conda/lib/python3.8/site-packages/apache_beam/runners/common.cpython-38-x86_64-linux-gnu.so in apache_beam.runners.common.DoFnRunner.process()

/opt/conda/lib/python3.8/site-packages/apache_beam/runners/common.cpython-38-x86_64-linux-gnu.so in apache_beam.runners.common.SimpleInvoker.invoke_process()

/opt/conda/lib/python3.8/site-packages/apache_beam/runners/common.cpython-38-x86_64-linux-gnu.so in apache_beam.runners.common._OutputProcessor.process_outputs()

/opt/conda/lib/python3.8/site-packages/apache_beam/runners/worker/operations.cpython-38-x86_64-linux-gnu.so in apache_beam.runners.worker.operations.SingletonConsumerSet.receive()

/opt/conda/lib/python3.8/site-packages/apache_beam/runners/worker/operations.cpython-38-x86_64-linux-gnu.so in apache_beam.runners.worker.operations.DoOperation.process()

/opt/conda/lib/python3.8/site-packages/apache_beam/runners/worker/operations.cpython-38-x86_64-linux-gnu.so in apache_beam.runners.worker.operations.DoOperation.process()

/opt/conda/lib/python3.8/site-packages/apache_beam/runners/common.cpython-38-x86_64-linux-gnu.so in apache_beam.runners.common.DoFnRunner.process()

/opt/conda/lib/python3.8/site-packages/apache_beam/runners/common.cpython-38-x86_64-linux-gnu.so in apache_beam.runners.common.DoFnRunner._reraise_augmented()

/opt/conda/lib/python3.8/site-packages/apache_beam/runners/common.cpython-38-x86_64-linux-gnu.so in apache_beam.runners.common.DoFnRunner.process()

/opt/conda/lib/python3.8/site-packages/apache_beam/runners/common.cpython-38-x86_64-linux-gnu.so in apache_beam.runners.common.SimpleInvoker.invoke_process()

/opt/conda/lib/python3.8/site-packages/apache_beam/runners/common.cpython-38-x86_64-linux-gnu.so in apache_beam.runners.common._OutputProcessor.process_outputs()

/opt/conda/lib/python3.8/site-packages/apache_beam/runners/worker/operations.cpython-38-x86_64-linux-gnu.so in apache_beam.runners.worker.operations.SingletonConsumerSet.receive()

/opt/conda/lib/python3.8/site-packages/apache_beam/runners/worker/operations.cpython-38-x86_64-linux-gnu.so in apache_beam.runners.worker.operations.DoOperation.process()

/opt/conda/lib/python3.8/site-packages/apache_beam/runners/worker/operations.cpython-38-x86_64-linux-gnu.so in apache_beam.runners.worker.operations.DoOperation.process()

/opt/conda/lib/python3.8/site-packages/apache_beam/runners/common.cpython-38-x86_64-linux-gnu.so in apache_beam.runners.common.DoFnRunner.process()

/opt/conda/lib/python3.8/site-packages/apache_beam/runners/common.cpython-38-x86_64-linux-gnu.so in apache_beam.runners.common.DoFnRunner._reraise_augmented()

/opt/conda/lib/python3.8/site-packages/apache_beam/runners/common.cpython-38-x86_64-linux-gnu.so in apache_beam.runners.common.DoFnRunner.process()

/opt/conda/lib/python3.8/site-packages/apache_beam/runners/common.cpython-38-x86_64-linux-gnu.so in apache_beam.runners.common.PerWindowInvoker.invoke_process()

/opt/conda/lib/python3.8/site-packages/apache_beam/runners/common.cpython-38-x86_64-linux-gnu.so in apache_beam.runners.common.PerWindowInvoker._invoke_process_per_window()

/opt/conda/lib/python3.8/site-packages/apache_beam/transforms/core.py in <lambda>(x, *args, **kwargs)
   1635   from apache_beam.transforms.util import fn_takes_side_inputs
   1636   if fn_takes_side_inputs(fn):
-> 1637     wrapper = lambda x, *args, **kwargs: [fn(x, *args, **kwargs)]
   1638   else:
   1639     wrapper = lambda x: [fn(x)]

/opt/conda/lib/python3.8/site-packages/tensorflow_transform/beam/impl.py in _create_v2_saved_model(tensor_replacement_map, base_temp_dir, preprocessing_fn, input_signature, baseline_analyzers_fingerprint, output_keys_to_name_map)
    661   """"""
    662   saved_model_dir = beam_common.get_unique_temp_path(base_temp_dir)
--> 663   impl_helper.trace_and_write_v2_saved_model(saved_model_dir, preprocessing_fn,
    664                                              input_signature, base_temp_dir,
    665                                              baseline_analyzers_fingerprint,

/opt/conda/lib/python3.8/site-packages/tensorflow_transform/impl_helper.py in trace_and_write_v2_saved_model(saved_model_dir, preprocessing_fn, input_signature, base_temp_dir, baseline_analyzers_fingerprint, tensor_replacement_map, output_keys_to_name_map)
    892   if not concrete_transform_fn.graph.get_collection(
    893       analyzer_nodes.TENSOR_REPLACEMENTS):
--> 894     metadata = _trace_and_get_metadata(concrete_transform_fn, structured_inputs,
    895                                        preprocessing_fn, base_temp_dir,
    896                                        tensor_replacement_map)

/opt/conda/lib/python3.8/site-packages/tensorflow_transform/impl_helper.py in _trace_and_get_metadata(concrete_transform_fn, structured_inputs, preprocessing_fn, base_temp_dir, tensor_replacement_map)
    804       evaluate_schema_overrides=True)
    805   return dataset_metadata.DatasetMetadata(
--> 806       schema=schema_inference.infer_feature_schema_v2(
    807           concrete_transform_fn.structured_outputs,
    808           concrete_metadata_fn,

/opt/conda/lib/python3.8/site-packages/tensorflow_transform/schema_inference.py in infer_feature_schema_v2(features, concrete_metadata_fn, evaluate_schema_overrides)
    254     tensor_annotations, global_annotations = _get_schema_annotations_v2(
    255         metadata)
--> 256   return _infer_feature_schema_common(
    257       features,
    258       tensor_ranges,

/opt/conda/lib/python3.8/site-packages/tensorflow_transform/schema_inference.py in _infer_feature_schema_common(features, tensor_ranges, feature_annotations, global_annotations, is_evaluation_complete)
    299       domains[name] = schema_pb2.IntDomain(
    300           min=min_value, max=max_value, is_categorical=True)
--> 301   feature_spec = _feature_spec_from_batched_tensors(features,
    302                                                     is_evaluation_complete)
    303 

/opt/conda/lib/python3.8/site-packages/tensorflow_transform/schema_inference.py in _feature_spec_from_batched_tensors(tensors, is_evaluation_complete)
    127       if is_evaluation_complete and any(
    128           dim is None for dim in shape.as_list()[1:]):
--> 129         raise ValueError(
    130             'Feature {} ({}) had invalid shape {} for FixedLenFeature: apart '
    131             'from the batch dimension, all dimensions must have known size'

ValueError: Feature raw_image (Tensor(""Identity_1:0"", shape=(None, 1, None), dtype=int64)) had invalid shape (None, 1, None) for FixedLenFeature: apart from the batch dimension, all dimensions must have known size [while running 'Analyze/CreateSavedModel[tf_v2_only]/CreateSavedModel']

I know the label feature is working as I can call the below code and get a print as so....
transform = tfx.components.Transform(
    examples=example_gen.outputs['examples'],
    schema=schema_gen.outputs['schema'],
    module_file=os.path.abspath(_mnist_transform_module))
context.run(transform, enable_cache=False)

# Get the URI of the output artifact representing the transformed examples
train_uri = os.path.join(transform.outputs['transformed_examples'].get()[0].uri, 'Split-train')

# Get the list of files in this directory (all compressed TFRecord files)
tfrecord_filenames = [os.path.join(train_uri, name)
                      for name in os.listdir(train_uri)]

# Create a `TFRecordDataset` to read these files
dataset = tf.data.TFRecordDataset(tfrecord_filenames, compression_type=""GZIP"")

# Decode the first record and print output
for tfrecord in dataset.take(1):
  serialized_example = tfrecord.numpy()
  example = tf.train.Example()
  example.ParseFromString(serialized_example)
  print(example)

IF I remove the lines:
img = tf.io.decode_raw(raw_image_dataset, tf.int64)
    
    
    outputs[_IMAGE_KEY] = img

I get printed
features {
  feature {
    key: ""label""
    value {
      int64_list {
        value: 5
      }
    }
  }
}

This shows what I am doing to the label feature is working but I really can't figure how to transform the image bytes. Part of the issue is I'm not completely sure what the format is as it's just a tensor which is pretty opaque. It seems given the label operation I'm operating on a column of data effectively but again, can't figure the correct operation or syntax
","For any future viewers this works
raw_image_dataset = tf.map_fn(fn = lambda x : tf.io.parse_tensor(x[0], tf.uint8, name=None), elems = raw_image_dataset, fn_output_signature = tf.TensorSpec((28,28),dtype=tf.uint8,    name=None), infer_shape = True)
    raw_image_dataset = tf.cast(raw_image_dataset, tf.int64)
    outputs[_IMAGE_KEY] = raw_image_dataset

",tensorflow
tensorflow errors cufft cudnn cublas and quotassertion 39__n lt thisgtsize39 failedquot,"I just started with TF and Keras and found out that I can't run these on my computer. I've notice the problem first in jupyter notebook, and than recreate it in a python file.
The code to reproduce (main.py):
import os
os.environ[""KERAS_BACKEND""] = ""tensorflow""
import keras
from keras import layers
from keras import ops


print(""pass"")    
model = keras.Sequential()
model.add(layers.Input(shape=(28,)))
print(""pass"")

The python main.py output:
2024-10-16 22:30:52.054073: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1729081852.071578   41750 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1729081852.076640   41750 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
pass
/usr/include/c++/14.1.1/bits/stl_vector.h:1130: constexpr std::vector<_Tp, _Alloc>::reference std::vector<_Tp, _Alloc>::operator[](size_type) [with _Tp = pybind11::object; _Alloc = std::allocator<pybind11::object>; reference = pybind11::object&; size_type = long unsigned int]: Assertion '__n < this->size()' failed.
fish: Job 1, 'python main.py' terminated by signal SIGABRT (Abort)

I using arch linux and installed python-tensorflow-opt-cuda package. I have a nvidia card and working cuda.
The nvcc --version output:
nvcc: NVIDIA (R) Cuda compiler driver
Copyright (c) 2005-2024 NVIDIA Corporation
Built on Thu_Sep_12_02:18:05_PDT_2024
Cuda compilation tools, release 12.6, V12.6.77
Build cuda_12.6.r12.6/compiler.34841621_0

The python -c ""import tensorflow as tf; print(tf.__version__)"" output:
2024-10-16 22:35:45.309836: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1729082145.328052   42393 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1729082145.333326   42393 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2.18.0-rc1

The pacman -Q cuda output:
cuda 12.6.2-1

The pacman -Q cudnn output:
cudnn 9.2.1.18-1

The pacman -Q nvidia output:
nvidia-dkms 560.35.03-14

nvidia-dkms 560.35.03-14
The pacman -Q blas output:
blas 3.12.0-5

The pacman -Q python-tensorflow-opt-cuda output:
python-tensorflow-opt-cuda 2.18rc1-2

The pacman -Q python output:
python 3.12.7-1

The pacman -Q python-numpy output:
python-numpy 2.1.2-1


The pacman -Q python-keras output:
python-keras 3.4.1-1


The sudo docker run --rm --runtime=nvidia --gpus all ubuntu nvidia-smi output:
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 560.35.03              Driver Version: 560.35.03      CUDA Version: 12.6     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA GeForce GTX 1650        Off |   00000000:01:00.0 Off |                  N/A |
| N/A   43C    P8              1W /   50W |       8MiB /   4096MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+

+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
+-----------------------------------------------------------------------------------------+

Kernel: 6.11.3-zen1-1-zen
CPU: AMD Ryzen 5 4600H with Radeon Graphics (12) @ 3.000GH
GPU: NVIDIA GeForce GTX 1650 Mobile / Max-Q

I also don't understand why tf is 2.18 version in archlinux package repo. Isn't it unstable for now?
I've tried to reinstall all of the packages, but didn't downgrade them yet. I also tried using docker version of tensorflow, and I still got the same errors.
","Found the problem.
It was an outdated python-optree package. Seems that 0.13.0 version doesn't have the issue.
The issue was:
>>> from optree import *
>>> tree_map(lambda x: x, ())
/usr/include/c++/14.1.1/bits/stl_vector.h:1130: constexpr std::vector<_Tp, _Alloc>::reference std::vector<_Tp, _Alloc>::operator[](size_type) [with _Tp = pybind11::object; _Alloc = std::allocator<pybind11::object>; reference = pybind11::object&; size_type = long unsigned int]: Assertion '__n < this->size()' failed.
fish: Job 1, 'python' terminated by signal SIGABRT (Abort)

Empty tuples or arrays raised the issue.
In version 0.13.0:
>>> from optree import *
>>> tree_map(lambda x: x, ())
()

It's not throwing anymore and everything works.
",tensorflow
why is numpy native on m1 max greatly slower than on old intel i5,"I just got my new MacBook Pro with M1 Max chip and am setting up Python. I've tried several combinational settings to test speed - now I'm quite confused. First put my questions here:

Why python run natively on M1 Max is greatly (~100%) slower than on my old MacBook Pro 2016 with Intel i5?
On M1 Max, why there isn't significant speed difference between native run (by miniforge) and run via Rosetta (by anaconda) - which is supposed to be slower ~20%?
On M1 Max and native run, why there isn't significant speed difference between conda installed Numpy and TensorFlow installed Numpy - which is supposed to be faster?
On M1 Max, why run in PyCharm IDE is constantly slower ~20% than run from terminal, which doesn't happen on my old Intel Mac.

Evidence supporting my questions is as follows:

Here are the settings I've tried:
1. Python installed by

Miniforge-arm64, so that python is natively run on M1 Max Chip. (Check from Activity Monitor, Kind of python process is Apple).
Anaconda. Then python is run via Rosseta. (Check from Activity Monitor, Kind of python process is Intel).

2. Numpy installed by

conda install numpy: numpy from original conda-forge channel, or pre-installed with anaconda.
Apple-TensorFlow: with python installed by miniforge, I directly install tensorflow, and numpy will also be installed. It's said that, numpy installed in this way is optimized for Apple M1 and will be faster. Here is the installation commands:

conda install -c apple tensorflow-deps
python -m pip install tensorflow-macos
python -m pip install tensorflow-metal

3. Run from

Terminal.
PyCharm (Apple Silicon version).


Here is the test code:
import time
import numpy as np
np.random.seed(42)
a = np.random.uniform(size=(300, 300))
runtimes = 10

timecosts = []
for _ in range(runtimes):
    s_time = time.time()
    for i in range(100):
        a += 1
        np.linalg.svd(a)
    timecosts.append(time.time() - s_time)

print(f'mean of {runtimes} runs: {np.mean(timecosts):.5f}s')

and here are the results:
+-----------------------------------+-----------------------+--------------------+
|   Python installed by (run on)→   | Miniforge (native M1) | Anaconda (Rosseta) |
+----------------------+------------+------------+----------+----------+---------+
| Numpy installed by ↓ | Run from → |  Terminal  |  PyCharm | Terminal | PyCharm |
+----------------------+------------+------------+----------+----------+---------+
|          Apple Tensorflow         |   4.19151  |  4.86248 |     /    |    /    |
+-----------------------------------+------------+----------+----------+---------+
|        conda install numpy        |   4.29386  |  4.98370 |  4.10029 | 4.99271 |
+-----------------------------------+------------+----------+----------+---------+

This is quite slow. For comparison,

run the same code on my old MacBook Pro 2016 with i5 chip - it costs 2.39917s.
another post (but not in English) reports that run with M1 chip (not Pro or Max), miniforge+conda_installed_numpy is 2.53214s, and miniforge+apple_tensorflow_numpy is 1.00613s.
you may also try on it your own.

Here is the CPU information details:

My old i5:

$ sysctl -a | grep -e brand_string -e cpu.core_count
machdep.cpu.brand_string: Intel(R) Core(TM) i5-6360U CPU @ 2.00GHz
machdep.cpu.core_count: 2


My new M1 Max:

% sysctl -a | grep -e brand_string -e cpu.core_count
machdep.cpu.brand_string: Apple M1 Max
machdep.cpu.core_count: 10


I follow instructions strictly from tutorials - but why would all these happen? Is it because of my installation flaws, or because of M1 Max chip? Since my work relies heavily on local runs, local speed is very important to me. Any suggestions to possible solution, or any data points on your own device would be greatly appreciated :)
","Update Mar 28 2022: Please see @AndrejHribernik's comment below.

How to install numpy on M1 Max, with the most accelerated performance (Apple's vecLib)? Here's the answer as of Dec 6 2021.

Steps
I. Install miniforge
So that your Python is run natively on arm64, not translated via Rosseta.

Download Miniforge3-MacOSX-arm64.sh, then
Run the script, then open another shell

$ bash Miniforge3-MacOSX-arm64.sh


Create an environment (here I use name np_veclib)

$ conda create -n np_veclib python=3.9
$ conda activate np_veclib

II. Install Numpy with BLAS interface specified as vecLib

To compile numpy, first need to install cython and pybind11:

$ conda install cython pybind11


Compile numpy by (Thanks @Marijn's answer) - don't use conda install!

$ pip install --no-binary :all: --no-use-pep517 numpy


An alternative of 2. is to build from source

$ git clone https://github.com/numpy/numpy
$ cd numpy
$ cp site.cfg.example site.cfg
$ nano site.cfg

Edit the copied site.cfg: add the following lines:
[accelerate]
libraries = Accelerate, vecLib

Then build and install:
$ NPY_LAPACK_ORDER=accelerate python setup.py build
$ python setup.py install


After either 2 or 3, now test whether numpy is using vecLib:

>>> import numpy
>>> numpy.show_config()

Then, info like /System/Library/Frameworks/vecLib.framework/Headers should be printed.
III. For further installing other packages using conda
Make conda recognize packages installed by pip
conda config --set pip_interop_enabled true

This must be done, otherwise if e.g. conda install pandas, then numpy will be in The following packages will be installed list and installed again. But the new installed one is from conda-forge channel and is slow.

Comparisons to other installations:
1. Competitors:
Except for the above optimal one, I also tried several other installations

A. np_default: conda create -n np_default python=3.9 numpy
B. np_openblas: conda create -n np_openblas python=3.9 numpy blas=*=*openblas*
C. np_netlib: conda create -n np_netlib python=3.9 numpy blas=*=*netlib*

The above ABC options are directly installed from conda-forge channel. numpy.show_config() will show identical results. To see the difference, examine by conda list - e.g. openblas packages are installed in B. Note that mkl or blis is not supported on arm64.

D. np_openblas_source: First install openblas by brew install openblas. Then add [openblas] path /opt/homebrew/opt/openblas to site.cfg and build Numpy from source.
M1 and i9–9880H in this post.
My old i5-6360U 2cores on MacBook Pro 2016 13in.

2. Benchmarks:
Here I use two benchmarks:

mysvd.py: My SVD decomposition

import time
import numpy as np
np.random.seed(42)
a = np.random.uniform(size=(300, 300))
runtimes = 10

timecosts = []
for _ in range(runtimes):
    s_time = time.time()
    for i in range(100):
        a += 1
        np.linalg.svd(a)
    timecosts.append(time.time() - s_time)

print(f'mean of {runtimes} runs: {np.mean(timecosts):.5f}s')


dario.py: A benchmark script by Dario Radečić at the post above.

3. Results:
+-------+-----------+------------+-------------+-----------+--------------------+----+----------+----------+
|  sec  | np_veclib | np_default | np_openblas | np_netlib | np_openblas_source | M1 | i9–9880H | i5-6360U |
+-------+-----------+------------+-------------+-----------+--------------------+----+----------+----------+
| mysvd |  1.02300  |   4.29386  |   4.13854   |  4.75812  |      12.57879      |  / |     /    |  2.39917 |
+-------+-----------+------------+-------------+-----------+--------------------+----+----------+----------+
| dario |     21    |     41     |      39     |    323    |         40         | 33 |    23    |    78    |
+-------+-----------+------------+-------------+-----------+--------------------+----+----------+----------+

",tensorflow
keras h5 model to tensorflowlite tflite model conversion,"I am trying to convert a .h5 keras model to a .tflite model. But the conversion results in core dumped error. Here's the script that I am running,
import tensorflow as tf
from keras.models import Sequential
from keras.layers import Dense

# Create a simple Keras model
model = Sequential([
    Dense(64, activation='relu', input_shape=(784,)),
    Dense(64, activation='relu'),
    Dense(10, activation='softmax')
])

# Compile the model
model.compile(optimizer='adam',
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])

# Convert the Keras model to TensorFlow Lite model
converter = tf.lite.TFLiteConverter.from_keras_model(model)
tflite_model = converter.convert()

# Save the TensorFlow Lite model to a file
with open('model.tflite', 'wb') as f:
    f.write(tflite_model)

print(""TensorFlow Lite model saved successfully!"")

I am getting this error, if I run the script
2024-04-01 12:27:04.793910: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.
loc(fused[""ReadVariableOp:"", callsite(""sequential_1/dense_1/Add/ReadVariableOp@__inference_serving_default_98""
callsite(""/home/spoon/Documents/GTSRB/lib/python3.9/site-packages/keras/src/ops/numpy.py"":311:1 at callsite(""/home/spoon/Documents/GTSRB/lib/python3.9/site-packages/keras/src/backend/tensorflow/sparse.py"":491:1 at callsite(""/home/spoon/Documents/GTSRB/lib/python3.9/site-packages/keras/src/backend/tensorflow/numpy.py"":35:1 at ""/home/spoon/Documents/GTSRB/lib/python3.9/site-packages/keras/src/backend/tensorflow/core.py"":64:1)))))))))))))))))))))))))))]): error: missing attribute 'value'
LLVM ERROR: Failed to infer result type(s).
Aborted (core dumped)

OS: Ubuntu 20.04.6 LTS
Python version: 3.9.18
pip freeze info:
keras==3.1.1
keras-core==0.1.7
keras-cv==0.8.2
tensorboard==2.16.2
tensorboard-data-server==0.7.2
tensorflow==2.16.1
tensorflow-datasets==4.9.3
tensorflow-io-gcs-filesystem==0.36.0
tensorflow-metadata==1.14.0

","This is a bug caused by inconsistent Keras versions.
TensorFlow versions ≥ 2.16 switched from Keras 2 to Keras 3, that is good since Keras 3 introduced many improvements with a significant performance uplift, but unfortunately they haven't properly ported all the TensorFlow modules to make them work as well with the latest Keras 3 module (and not even properly documented what this change has broken), so converter still expects a model generated by old Keras 2 module and crashes when a Keras 3 model is provided instead.
Hoping for a future update of the converter module, in meanwhile as workaround, you need to:
1) install tf_keras package (so that Keras 2 legacy mode is available) with:
pip install tf_keras

2) enable legacy mode adding in your code:
import os
os.environ[""TF_USE_LEGACY_KERAS""] = ""1""

placing it before
import tensorflow as tf

so that TensorFlow will be initialized with Keras 2 module.
Alternatively you can downgrade your TensorFlow to the latest Keras 2-based version with:
pip install tensorflow==2.15.0

",tensorflow
behavior of model saving when earlystopping is not triggered in keras,"I'm currently fine-tuning a network for a few epochs using the following code:
es= keras.callbacks.EarlyStopping(monitor='loss', min_delta=1e-4,patience=10, verbose=True, restore_best_weights=True)
history = model.fit(ft, steps_per_epoch=len(ft), callbacks=[es], epochs=150, verbose=verbose)
model.save('model_best.h5')

I understand that if there is no improvement in the loss (monitored by monitor='loss') for 10 epochs (determined by patience=10), the EarlyStopping callback is activated, and the model weights restored to the best encountered weights are saved.
My question is about the scenario where EarlyStopping does not trigger—say, if there isn't a sequence of 10 epochs without improvement, but the best performing epoch is not the last one. In this case, which weights are saved? Are they the weights from the last epoch or from the best epoch observed during training? Notably, when verbose=True, the EarlyStopping callback does not produce any output if it doesn't trigger, leaving me unsure about which model version is preserved. Can someone clarify what happens in this situation?
","By default, the model does not automatically save the weights of the best-
performing epoch. You need to explicitly configure this behavior using a
callback like modelcheckpoint in libraries like keras or TensorFlow.
I am attaching gist file for your reference
",tensorflow
how to access the weights of a layer in pretrained efficientnetb3 in torch,"It's not about loading the weights of a model. I am trying to see the weights of layers in loaded efficientnet-b3 model in the torch.
os.system('pip install efficientnet_pytorch')
from efficientnet_pytorch import EfficientNet
MODEL_NAME = 'efficientnet-b3'
effnet = EfficientNet.from_pretrained(MODEL_NAME) 
effnet.modules # this works, but only gives the module names
effnet.weights # doesn't work
effnet.layers # doesn't work
effnet.modules[1]  # doesn't work, second module is batch norm ._bc0

I would like some functionality to replicate the following TF code in torch, access weights of the first batch norm layer
from tensorflow.keras.applications import EfficientNetB3
base_model = EfficientNetB3(weights=""imagenet"")
base_model.trainable_variables[1].numpy() # indexing with 1 gives weights of conv layer

Output for the above code:
<tf.Variable 'stem_bn/gamma:0' shape=(40,) dtype=float32, numpy=
array([ 0.1913209 ,  2.7074034 ,  9.623442  ,  2.5562265 ,  3.127593  ,
        4.348222  ,  2.4381876 ,  3.4623973 ,  3.6115906 ,  4.1241236 ,
        2.18851   ,  8.9716835 ,  0.7232651 ,  0.6261555 ,  9.050293  ,
        7.9233327 ,  0.47725916,  3.4991856 ,  5.334402  ,  4.843143  ,
        1.4122163 ,  1.953061  ,  8.150878  ,  5.0044165 ,  2.3806598 ,
        4.2976685 ,  2.2239766 ,  0.551327  ,  7.799995  ,  3.3823645 ,
        1.8910869 ,  4.0793633 ,  0.73215246,  3.4526935 , 10.874565  ,
        2.0920732 ,  6.272054  ,  3.6823177 ,  4.2152214 ,  3.4319222 ],
      dtype=float32)>

","You can access the weights of a model by calling model.named_parameters().
In your case, for i in effnet.named_parameters(): print(i) returns a list of 2-tuples, where the first item in the tuple is the name of the parameter, and the second item is the actual parameter tensor.
Here is a sample extract so you can see what I mean:
[('_conv_head.weight', Parameter containing:
tensor([[[[ 0.0074]],

         [[-0.0904]],

         [[-0.0091]],

         ...,

         [[-0.0504]],

         [[ 0.1140]],

         [[-0.0710]]],


        [[[ 0.0635]],

         [[ 0.0290]],

         [[-0.0583]],

         ...,

         [[-0.0813]],

         [[-0.0230]],

         [[-0.1120]]],


        [[[ 0.0873]],

         [[-0.0174]],

         [[-0.0170]],

         ...,

         [[-0.0014]],

         [[ 0.0323]],

         [[ 0.0569]]],


        ...,


        [[[-0.0218]],

         [[ 0.0292]],

         [[ 0.0267]],

         ...,

         [[-0.0044]],

         [[-0.0463]],

         [[-0.0257]]],


        [[[-0.0209]],

         [[ 0.0279]],

         [[ 0.0094]],

         ...,

         [[-0.1759]],

         [[-0.0702]],

         [[-0.0902]]],


        [[[-0.0488]],

         [[ 0.0276]],

         [[-0.0174]],

         ...,

         [[-0.0391]],

         [[-0.0268]],

         [[-0.0205]]]], requires_grad=True)), 
('_bn1.weight', Parameter containing:
tensor([2.3115, 2.0343, 2.0015,  ..., 1.7868, 2.3552, 1.8885],
       requires_grad=True)), 
('_bn1.bias', Parameter containing:
tensor([-1.8066, -1.4178, -1.3111,  ..., -1.0494, -1.8520, -1.1798],
       requires_grad=True)), 
('_fc.weight', Parameter containing:
tensor([[-0.0157, -0.0483,  0.0139,  ..., -0.0201, -0.0092, -0.0752],
        [-0.0100, -0.0575,  0.0328,  ..., -0.0362, -0.0534, -0.0041],
        [-0.0083,  0.0142, -0.0006,  ...,  0.0151, -0.0033,  0.0500],
        ...,
        [-0.0840, -0.0217, -0.0354,  ..., -0.0620,  0.0143,  0.0786],
        [-0.0956, -0.0169,  0.0738,  ...,  0.1063, -0.0742,  0.0036],
        [ 0.0485,  0.0470,  0.1002,  ..., -0.0832,  0.1081,  0.0145]],
       requires_grad=True)), 
('_fc.bias', Parameter containing:
tensor([-1.8788e-04, -2.1204e-02, -2.2974e-02, -3.5067e-02, -4.2469e-02,
        -4.0472e-02, -3.3574e-02,  7.5904e-03, -1.3298e-02, -1.3364e-02,
        -4.7947e-02, -6.8513e-02, -5.1592e-02, -4.0660e-02, -2.1086e-02,
        -5.7097e-02, -8.5144e-02, -4.0252e-02,  1.5397e-02, -2.6116e-02,
        -7.2120e-02, -4.8167e-02, -4.5482e-02, -5.7588e-02, -6.5176e-02,
        -4.3350e-02, -6.1431e-03, -3.3575e-02, -1.6232e-02,  4.4864e-03,
        -7.7549e-02, -6.1085e-02, -3.7735e-02, -4.0341e-02,  1.7911e-03,
        -7.5653e-02,  3.0368e-02, -3.7621e-02, -1.5108e-02, -1.8987e-02,
        -7.0831e-02, -4.8989e-02, -4.6129e-02, -3.8295e-02, -2.3450e-02,
        -7.5764e-02, -9.7884e-03, -2.0963e-02, -5.0398e-02, -3.1158e-02,
        -4.3633e-02,  2.1600e-02,  2.2160e-02,  9.6163e-03, -3.5956e-02,
         2.3973e-03, -3.3229e-02, -5.7656e-02, -1.0077e-02, -1.7506e-02,
         3.1533e-02,  4.6831e-02, -1.2585e-02, -5.1598e-03,  2.8223e-02,
        -8.0142e-03, -3.1187e-03, -5.5687e-02, -1.6465e-02, -6.9663e-02,
        -1.0505e-02, -1.2698e-02, -4.9928e-02,  2.3078e-02, -3.1174e-02,
        -9.6878e-03,  3.8667e-02,  1.0131e-02,  2.0751e-02,  1.1730e-03,
        -2.4952e-02, -6.8574e-02, -2.2635e-02, -8.7558e-02, -7.1596e-02,
        -9.8193e-04, -3.9943e-02,  1.3720e-02, -1.0468e-02, -8.8728e-03,
        -6.1661e-02, -6.7819e-02, -4.3548e-02, -6.7823e-02, -1.1364e-01,
        -8.0226e-02, -3.1601e-02, -4.1108e-02, -1.0578e-01, -2.4910e-02,
        -3.6718e-02, -6.0854e-03, -1.7552e-02,  4.8444e-02, -1.0772e-01,
        -8.1519e-02,  1.1975e-02, -5.9317e-02, -4.8866e-02, -1.0751e-01,
        -4.0855e-02, -2.0209e-02,  2.0903e-02, -4.4489e-02,  4.7728e-03,
        -1.0581e-01, -6.0772e-02, -5.2845e-02,  5.1716e-02, -6.6787e-02,
        -6.2687e-02,  8.3072e-03, -7.7149e-03, -8.9555e-02, -4.8636e-02,
        -7.7438e-02, -2.0582e-02, -3.4283e-02, -6.6013e-02, -8.3553e-02,
        -1.7237e-02,  1.0309e-02, -1.3888e-02, -8.5651e-02, -1.0593e-02,
        -8.5910e-02, -4.4669e-02, -4.7769e-02, -5.6502e-02, -3.7197e-02,
        -5.9685e-02, -6.1591e-02, -3.5832e-02, -3.9733e-02, -2.6279e-03,
        -7.5230e-03, -4.8190e-02, -5.2629e-02,  1.3794e-03, -6.2900e-02,
         1.2298e-02,  3.3339e-02, -3.2597e-02,  2.7527e-02, -1.2776e-02,
         4.9703e-02,  1.7361e-02,  2.0815e-02, -3.7160e-02,  3.7085e-02,
        -3.6573e-02,  6.2998e-02,  6.3216e-02,  3.9644e-02,  2.1697e-02,
        -7.6096e-02, -3.1489e-02, -4.0166e-02,  3.0928e-02,  1.8492e-02,
         3.6115e-03,  6.6122e-02,  3.6215e-02,  1.1731e-02,  6.3609e-02,
        -5.1814e-02, -2.1708e-02,  2.3418e-02,  1.1752e-01, -5.7509e-05,
        -9.9666e-03,  3.7792e-02,  5.2852e-02,  2.7368e-02,  8.6873e-02,
         5.4690e-03,  1.4777e-02,  1.8022e-02,  1.4611e-02,  8.3892e-02,
        -2.4939e-02,  9.1274e-02,  6.7359e-04,  1.2745e-02, -3.5555e-02,
         1.3300e-01,  4.2316e-02,  3.8856e-02, -1.3746e-02,  7.2174e-02,
        -3.3013e-02,  2.3095e-02,  4.3574e-02,  9.0962e-02, -2.4382e-03,
         1.9608e-02, -9.1173e-03,  5.3899e-02,  4.8285e-02,  8.5972e-02,
        -1.0665e-02,  3.8411e-02,  3.8558e-03,  5.2799e-03,  1.4861e-02,
         3.9330e-02,  7.3648e-03,  8.2185e-02,  5.4206e-02,  5.7331e-03,
         6.0913e-02, -2.7097e-02,  9.5242e-03,  5.7349e-02, -8.2799e-03,
         3.3900e-02, -4.3817e-02,  7.2375e-03, -7.7864e-03,  6.6191e-03,
        -1.6398e-03,  3.7898e-03,  2.0504e-02, -2.7386e-02,  3.3018e-02,
         2.3809e-02,  6.7705e-02,  7.4084e-02,  4.5319e-02,  4.4979e-02,
         2.6763e-02,  2.5106e-02,  5.7135e-02,  4.2436e-02, -5.5549e-02,
         8.4225e-02, -1.8866e-02,  5.4292e-02,  7.3890e-02,  4.6438e-02,
         2.8466e-02,  9.7005e-02, -1.9982e-02,  5.4305e-02,  5.3359e-02,
         4.3933e-03,  1.2966e-02, -1.3056e-02,  2.8389e-02,  6.9528e-03,
        -9.5808e-03,  1.3317e-02, -5.5829e-02,  4.7666e-03,  3.1541e-02,
         2.3011e-02,  4.1077e-02,  5.9078e-02, -5.6244e-02, -1.7187e-02,
         4.5811e-02,  3.0973e-02,  1.8721e-02,  8.7556e-03, -1.3825e-02,
         1.6460e-03, -6.1395e-02, -1.6215e-02, -9.6606e-03, -3.0871e-02,
         2.9623e-02,  4.9153e-02,  4.9580e-02, -1.5432e-02,  4.9673e-02,
         3.5004e-02, -3.5870e-02, -4.6010e-02, -3.7892e-02, -5.9807e-03,
         7.9569e-03, -8.6313e-02, -2.0522e-02, -2.7805e-02, -4.1220e-02,
        -4.9287e-02, -9.1146e-03, -2.2379e-02, -3.7943e-02, -2.0446e-02,
        -6.7551e-02, -2.6903e-02,  3.5022e-03, -4.7364e-02, -1.0330e-01,
        -6.1197e-02, -1.9515e-02, -6.4015e-02, -3.7570e-02, -4.0806e-02,
        -1.7590e-02, -6.0645e-02, -3.7628e-02,  2.2510e-02,  6.6436e-02,
        -3.2913e-02, -7.2844e-02, -8.4280e-02, -9.0194e-04, -3.3313e-02,
        -5.0213e-02, -6.0932e-02, -6.3614e-02, -6.2003e-02, -1.8194e-02,
        -7.3311e-02, -6.6968e-02, -1.9100e-02, -6.7069e-02, -9.9181e-02,
        -2.4951e-02, -2.4315e-02, -2.8859e-02,  2.5487e-02,  1.7767e-03,
        -4.7624e-02, -1.6528e-02, -2.6242e-02, -2.0435e-02, -4.6075e-03,
         1.4161e-02, -1.7872e-02, -2.7038e-02, -2.0696e-02, -3.1628e-02,
         3.1667e-02,  1.2643e-02,  3.3634e-02, -2.2169e-02, -1.3101e-02,
        -2.6296e-02, -2.9903e-02, -5.1367e-02, -2.5846e-02,  1.1791e-02,
        -2.0295e-02,  2.2274e-02,  3.4522e-02,  9.1384e-02,  5.7635e-02,
        -3.7340e-02,  4.5483e-02,  9.2709e-03, -3.4176e-03, -1.8723e-02,
         4.7653e-02, -4.3498e-02,  1.5783e-02, -2.6597e-03,  4.8437e-02,
         5.4055e-03,  5.2496e-02, -7.5699e-02, -7.2362e-03, -2.4031e-02,
        -3.8313e-02, -5.7455e-02, -3.2797e-02, -1.0619e-02, -3.5333e-02,
         6.8303e-02,  4.2202e-02,  1.2961e-02, -1.0357e-02, -7.2061e-02,
         3.5756e-02,  1.0100e-02, -6.8939e-03, -3.2524e-03,  1.9981e-02,
        -7.9577e-02, -9.9632e-03, -6.4924e-02, -7.7214e-02, -4.2186e-03,
         7.7079e-03, -6.8280e-02, -9.0313e-02,  5.5780e-02,  4.0597e-02,
         1.5803e-02,  7.6331e-02,  8.0219e-02, -4.2492e-02, -5.0285e-02,
         2.8535e-02, -5.1758e-02, -5.5653e-02, -2.6481e-02,  8.4919e-02,
         3.3701e-02, -4.7973e-02,  4.3328e-02,  5.4467e-02, -3.5105e-02,
        -2.8679e-02,  3.2789e-02, -3.0762e-02, -5.7570e-03,  6.3135e-02,
         2.6109e-02,  4.5541e-02,  1.5574e-02,  3.4254e-02,  2.1206e-02,
         8.7765e-03, -1.5536e-02,  8.7542e-02,  2.8484e-02,  7.4923e-02,
         2.7793e-03,  3.0007e-02,  4.5130e-02,  3.4480e-02,  1.4038e-02,
         1.0104e-01,  8.9144e-03, -6.6594e-03,  2.3476e-02,  2.1639e-02,
         8.8072e-02,  3.4813e-02, -6.1209e-02, -1.9577e-02, -1.3342e-02,
         8.3109e-02, -5.9933e-02,  4.5758e-02,  8.2524e-02, -3.3844e-02,
        -2.8108e-02, -1.2548e-02,  2.2878e-03, -3.2685e-03,  4.4044e-02,
         8.5652e-02,  3.7278e-03,  7.4127e-02, -4.7680e-02, -1.0711e-02,
        -1.9743e-03,  2.8187e-02,  3.1251e-02,  1.3193e-01,  1.3656e-02,
        -8.9823e-03, -8.9561e-03, -3.4255e-02,  2.8345e-02,  3.1666e-02,
         5.3442e-02,  2.4717e-02, -1.8101e-02,  2.4845e-02, -1.5233e-02,
         1.0711e-01, -2.0983e-02, -4.5709e-02, -8.3504e-04, -2.5989e-02,
         1.4240e-02,  1.3953e-02, -1.6013e-02,  9.2497e-04, -3.4750e-02,
        -3.9424e-02,  7.6613e-02, -1.2504e-02,  1.2788e-01,  7.7914e-02,
         6.1930e-02,  1.4219e-03, -2.5443e-02,  6.1517e-03,  6.1918e-02,
        -2.7085e-02,  7.3593e-03, -7.3484e-02,  2.8937e-02,  9.8165e-02,
         2.0922e-03,  4.2529e-02, -1.1309e-02, -4.9490e-02,  1.6751e-02,
         1.0809e-02,  2.3618e-02,  2.0515e-02,  3.3854e-02, -1.1171e-01,
        -9.9522e-03, -2.2146e-02,  1.3169e-02,  1.1341e-01,  6.5784e-02,
         1.1520e-01, -1.2588e-02, -7.9052e-02, -1.4347e-02,  4.7605e-02,
         3.4523e-02, -4.4671e-02,  1.1775e-02,  1.0015e-01,  2.6564e-02,
        -1.3457e-02, -4.4883e-02,  3.8352e-02,  1.2466e-02, -1.1996e-03,
         1.0156e-01, -9.3025e-02,  1.9178e-02, -3.7339e-02, -3.1499e-02,
         1.3383e-02, -2.9213e-02, -4.8970e-02, -4.5686e-02,  2.8906e-02,
        -5.9064e-02, -1.5512e-03,  1.9657e-02,  7.8270e-03, -4.0602e-02,
         1.9702e-02,  4.0297e-02, -1.9938e-02, -1.2144e-02, -2.3456e-02,
        -5.3594e-02, -4.2403e-02,  7.4747e-02,  1.2299e-02, -2.0902e-02,
        -5.5567e-02,  1.4648e-02,  1.4435e-03,  1.0079e-01,  9.2539e-02,
        -1.0607e-02, -1.4108e-02,  9.1721e-03,  1.1072e-02,  2.9608e-02,
        -7.6078e-03,  2.5653e-02, -1.1136e-02,  7.2409e-02, -5.1752e-02,
         8.0248e-02, -6.1112e-02,  6.1196e-02, -2.2555e-02,  8.3575e-02,
        -3.1096e-02, -4.8845e-03,  7.4904e-02, -6.5950e-03,  1.9850e-02,
        -3.0121e-02,  3.6329e-02, -4.4467e-03,  8.3675e-02, -7.4437e-02,
        -4.0771e-02, -2.8450e-02,  8.7870e-02, -4.0946e-02, -3.8329e-03,
        -6.9315e-02, -1.0016e-02, -2.0778e-02,  8.0915e-02,  3.9063e-02,
        -1.5094e-02,  3.3627e-03, -2.3838e-02,  3.1394e-02,  1.2513e-02,
         1.3142e-01,  3.0603e-02, -2.2055e-03,  2.0502e-02,  1.0135e-01,
         7.1754e-02,  9.7944e-03,  3.7959e-02,  1.3989e-02,  1.7989e-02,
        -3.4136e-03,  3.9650e-02, -3.4606e-02, -2.8792e-03, -5.2186e-03,
        -3.3676e-02,  5.8599e-02,  6.2828e-02,  4.3374e-02,  2.6202e-02,
         9.4092e-02, -3.8893e-03,  9.6722e-02, -6.6398e-02, -5.4730e-02,
        -4.2413e-02,  2.9454e-02, -1.1035e-02, -1.3873e-02,  3.5274e-02,
        -5.2795e-03,  7.2810e-03,  5.6811e-03,  2.5880e-02, -1.6221e-02,
        -4.2152e-02,  2.0760e-02,  1.4120e-02,  7.2473e-03,  6.9235e-03,
        -6.1877e-02,  8.1591e-03,  5.1506e-02,  4.0437e-02,  1.4943e-02,
        -2.9214e-02,  7.7097e-02, -3.1952e-02,  3.0927e-03, -6.7025e-04,
         6.2347e-02,  8.4001e-03,  3.2072e-02,  5.1373e-02, -2.0174e-02,
         1.1651e-02,  1.1074e-02,  1.7667e-02, -8.6863e-03,  3.0662e-02,
        -1.7535e-02, -5.4594e-02, -1.0278e-02, -6.7767e-02,  1.8709e-02,
         4.3460e-02,  2.7637e-02,  5.1533e-02, -3.5134e-02,  3.0937e-02,
         1.0212e-02,  3.0756e-02,  6.7154e-02,  3.9204e-02,  6.0210e-02,
        -6.5048e-02,  7.2162e-02,  4.5481e-02,  4.0012e-02, -2.6958e-02,
         2.3580e-03,  1.9551e-02,  1.4279e-02,  6.8494e-02,  2.1821e-02,
        -3.8355e-02, -5.8506e-02, -7.3684e-02, -7.3094e-03, -4.3607e-02,
        -2.3467e-03, -8.3147e-03,  6.3290e-02, -1.1461e-02, -1.4600e-03,
         6.5386e-02,  9.3397e-03, -4.8493e-02, -2.3290e-02,  4.6487e-02,
         1.0229e-01,  4.5283e-03,  4.7067e-02,  1.0678e-02,  5.5262e-02,
         7.6263e-03, -3.7516e-02,  1.8642e-02,  1.2551e-02, -7.5747e-02,
        -6.9164e-03,  9.2134e-03, -7.1654e-02, -3.2144e-02,  4.4780e-02,
         1.6665e-02,  5.4213e-02, -5.7801e-02, -4.3401e-05,  9.2516e-02,
         2.4602e-02, -2.6873e-02,  6.9050e-02, -1.1062e-02, -6.5441e-02,
         3.2812e-02,  1.6676e-02, -3.6145e-02,  6.2055e-02, -5.5214e-02,
        -2.6646e-02,  1.5051e-01, -1.4609e-02,  7.3359e-02, -1.1975e-02,
         2.5550e-03,  2.8461e-02,  6.1058e-02,  1.5934e-02,  1.1280e-02,
        -5.7301e-02, -8.1453e-02, -1.3112e-02,  1.4247e-02,  2.6493e-02,
        -3.2159e-02, -1.2888e-02,  3.9029e-02, -3.8758e-02,  1.1523e-01,
         5.7670e-02, -3.0118e-02, -1.3478e-02,  5.0629e-02, -1.2562e-02,
        -3.1457e-02, -7.3343e-03, -4.2227e-02, -5.6207e-02, -2.7644e-02,
        -2.3450e-02,  3.6726e-02, -1.7531e-02,  5.1430e-02,  8.6157e-02,
         3.9421e-02, -3.5995e-02, -1.9596e-03,  1.4217e-02,  8.6251e-02,
         9.4631e-04, -8.1190e-02,  9.1071e-02,  2.4760e-02, -1.1475e-02,
         2.2392e-02,  5.3320e-02,  5.3901e-04, -4.8530e-02, -4.3654e-03,
        -5.6498e-02, -1.8796e-02,  7.8893e-02,  4.2227e-02,  5.3552e-02,
         7.8111e-02,  2.7515e-02, -1.6096e-02,  1.2153e-02,  6.6272e-03,
        -4.8895e-02,  5.0869e-02,  6.7843e-02,  6.1599e-02, -1.1996e-02,
        -9.4545e-03,  1.0977e-01,  5.2409e-02, -4.7280e-02,  3.6631e-02,
        -2.9781e-02, -4.2014e-03, -4.2217e-02, -1.1914e-02, -2.9351e-02,
         1.4702e-01, -2.4856e-02, -5.4605e-02,  9.2256e-02, -5.8185e-02,
        -4.9112e-03, -4.8391e-02,  9.6750e-03,  9.3729e-02, -7.0812e-02,
         1.3358e-02, -3.0346e-02, -1.2634e-02,  5.4181e-02,  3.5779e-03,
        -1.3641e-02,  1.8990e-02,  4.1806e-02,  1.4102e-02, -3.5755e-02,
        -9.7328e-03, -6.9314e-02, -1.1238e-02,  4.4573e-02, -5.3725e-02,
         1.4389e-04,  3.9408e-02, -4.2209e-02, -3.2018e-02,  8.1474e-02,
         9.0362e-03,  8.6104e-02,  1.3965e-01, -1.3486e-02,  3.1769e-02,
         3.9424e-02, -2.7504e-02,  4.4009e-02,  4.5092e-02,  3.0902e-02,
         9.5173e-02,  6.0828e-02,  1.8025e-03, -8.7061e-03,  6.5157e-03,
         3.7598e-02,  1.0603e-01,  1.6571e-01,  5.5066e-02, -7.3351e-03,
        -5.2073e-03, -6.4366e-02, -6.7670e-02,  5.9512e-02,  6.4599e-02,
        -5.4156e-02,  3.0373e-02, -6.4649e-03,  6.1524e-02, -6.0863e-02,
        -5.4839e-02,  3.2778e-02, -5.2311e-02, -6.3305e-02, -1.4740e-02,
         8.6265e-03, -1.4593e-02,  7.0843e-02,  9.6598e-03, -4.1514e-02,
         1.0386e-01,  9.7102e-02,  2.7971e-02, -8.3213e-03,  8.9160e-02,
         6.4613e-02,  7.2493e-03,  2.0423e-02, -1.7861e-02, -1.0312e-02,
        -2.7538e-02, -4.7007e-02, -5.1747e-02,  5.8215e-02,  6.6977e-02,
        -1.7237e-02, -3.6396e-02,  5.7006e-02,  7.6054e-03, -4.7352e-02,
        -9.1480e-03,  4.0137e-02,  6.6371e-03,  7.7091e-02,  5.0351e-02,
         6.8603e-04, -5.7972e-02,  1.2907e-02,  3.9684e-02, -2.7078e-02,
         5.0169e-02,  3.0860e-02,  5.1556e-02, -4.8597e-02, -2.1098e-02,
         5.4887e-02, -4.1379e-02,  2.0082e-02,  1.8863e-02, -1.6039e-02,
         4.4650e-02, -2.7449e-02,  2.2729e-02,  3.3439e-02,  6.7221e-02,
         3.5019e-02,  1.2706e-02, -3.3244e-02, -2.0954e-02, -7.2121e-02,
        -2.5076e-02, -9.3874e-02, -1.4208e-02, -4.0450e-02,  1.0401e-01,
        -2.2949e-02, -2.6151e-02,  8.9983e-03, -7.2041e-02, -2.8341e-03,
        -4.9221e-02, -4.2876e-02,  6.4064e-03, -2.5436e-02,  1.6128e-02,
        -4.1882e-02, -3.8972e-02, -4.6335e-02, -5.4710e-02, -1.8748e-04,
        -5.4616e-02, -5.6258e-02, -3.8972e-02, -2.6863e-02,  1.6232e-02,
         1.6155e-02, -1.8721e-02, -2.4802e-02, -1.3311e-02, -1.7580e-02,
        -4.3439e-02, -4.1209e-02, -2.6509e-02,  2.1775e-02, -1.2127e-02,
        -1.7708e-02, -1.5500e-02, -7.4598e-02, -5.0580e-02, -6.6671e-02,
        -4.6614e-02,  4.4161e-02,  1.9872e-02,  1.7880e-02, -7.2944e-03,
         9.7257e-03,  1.2171e-01,  2.0895e-03, -7.2846e-02, -9.7906e-03,
        -5.5273e-02, -1.8346e-03,  3.9121e-02,  1.7541e-02,  1.6898e-02,
         2.0039e-02, -8.8893e-03, -2.4714e-02, -3.4171e-02,  1.0218e-02,
         1.2038e-02, -5.3019e-02,  3.3046e-02, -3.6093e-02, -5.2178e-02,
        -4.7642e-02, -6.6923e-02, -2.2623e-02, -7.2356e-02, -1.1350e-01,
        -2.2522e-02, -1.8415e-02, -5.9977e-02, -2.2627e-02,  4.6349e-02],
       requires_grad=True))]

",tensorflow
is it possible to use face_landmarktflite from mediapipe to generate face mesh in android independently,"I'm working on a face tracking app (Android studio / Java) and I need to identify face landmarks. I'm interested using Mediapipe face mesh model. The problem is: I use Windows OS, and Mediapipe is not working on Windows OS.
I have very basic knowledge in Tensorflow, Can anybody explain to me how can i use Mediapipe's face_landmark.tflite model to detect faces in images and generate face mesh in Android studio with Java independently without the whole Mediapipe framework?
","You can try look into my notebook below for usage example in python. This only needs tflite model and does not require Mediapipe installation.
https://nbviewer.jupyter.org/github/quickgrid/CodeLab/blob/master/tensorflow/MediaPipe_Face_Mesh_TFLite_Python_Direct_Inference.ipynb
This is the output image,

This should give a starting point to use android tflite interpreter to get face landmarks and draw them. It will require a face detector such as blazeface to output the face bounding box first.
As I have not implemented this model in android yet I cannot say what else may be needed. Further details may be found in mediapipe face mesh codes. The notebook is based on this code,
MediaPipe TensorflowLite Iris Model
https://github.com/shortcipher3/stackoverflow/blob/master/mediapipe_iris_2d_landmarks.ipynb
Further references,
https://github.com/google/mediapipe/blob/master/mediapipe/modules/face_landmark/face_landmark.tflite
https://google.github.io/mediapipe/solutions/face_mesh
Model card with input, output details,
https://drive.google.com/file/d/1QvwWNfFoweGVjsXF3DXzcrCnz-mx-Lha/view
Alternate option
Android ML Kit also has face landmarks with very good documentation and code example.
https://developers.google.com/ml-kit/vision/face-detection
https://developers.google.com/android/reference/com/google/mlkit/vision/face/package-summary
",tensorflow
how can i increase my cnn model39s accuracy,"I built a cnn model that classifies facial moods as happy , sad, energetic and neutral faces. I used Vgg16 pre-trained model and freezed all layers. After 50 epoch of training my model's test accuracy is 0.65 validatation loss is about 0.8 .
My train data folder has 16000(4x4000)  , validation data folder has 2000(4x500) and Test data folder has 4000(4x1000) rgb images.

What is your suggestion to increase the model accuracy?

I have tried to do some prediction with my model , predicted class is always same. What can cause the problem?


What I Have Tried So Far:

Add dropout layer (0.5)
Add Dense (256, relu) before last layer
Shuff the train and validation datas.
Decrease the learning rate to 1e-5

But I could not the increase validation and test accuracy.
My Codes
train_src = ""/content/drive/MyDrive/Affectnet/train_class/""
val_src = ""/content/drive/MyDrive/Affectnet/val_class/""
test_src=""/content/drive/MyDrive/Affectnet/test_classs/""

train_datagen = tensorflow.keras.preprocessing.image.ImageDataGenerator(
      rescale=1./255, 
      shear_range=0.2,
      zoom_range=0.2,
      horizontal_flip=True,
    
      )

train_generator = train_datagen.flow_from_directory(
        train_src,
        target_size=(224,224 ),
        batch_size=32,
        class_mode='categorical',
        shuffle=True
        )

validation_datagen = tensorflow.keras.preprocessing.image.ImageDataGenerator(
        rescale=1./255
        )

validation_generator = validation_datagen.flow_from_directory(
        val_src,
        target_size=(224, 224),
        batch_size=32,
        class_mode='categorical',
        shuffle=True
        )
conv_base = tensorflow.keras.applications.VGG16(weights='imagenet',
                  include_top=False,
                  input_shape=(224, 224, 3)
                  )
for layer in conv_base.layers:
  layer.trainable = False

model = tensorflow.keras.models.Sequential()

# VGG16 is added as convolutional layer.
model.add(conv_base)

# Layers are converted from matrices to a vector.
model.add(tensorflow.keras.layers.Flatten())

# Our neural layer is added.
model.add(tensorflow.keras.layers.Dropout(0.5))
model.add(tensorflow.keras.layers.Dense(256, activation='relu'))

model.add(tensorflow.keras.layers.Dense(4, activation='softmax'))

model.compile(loss='categorical_crossentropy',
              optimizer=tensorflow.keras.optimizers.Adam(lr=1e-5),
              metrics=['acc'])
history = model.fit_generator(
      train_generator,
      epochs=50,
      steps_per_epoch=100,
      validation_data=validation_generator,
      validation_steps=5,
      workers=8
      )

Loss and accuracy
","Well a few things. For training set you say you have 16,0000 images. However with a batch size of 32 and steps_per_epoch= 100 then for any given epoch you are only training on 3,200 images. Similarly you have 2000 validation images, but with a batch size of 32 and validation_steps = 5 you are only validating on 5 X 32 = 160 images.
Now Vgg is an OK model but I don't use it because it is very large which increases the training time significantly and there are other models out there for transfer learning that are smaller and even more accurate. I suggest you try using EfficientNetB3. Use the code
conv_base = tensorflow.keras.applications.EfficientNetB3(weights='imagenet',
                  include_top=False,
                  input_shape=(224, 224, 3)
                  pooling='max'
                  )

with pooling='max' you can eliminate the Flatten layer. Also EfficientNet models expect pixels in the range 0 to 255 so remove the rescale=1/255 in your generators.
Next thing to do is to use an adjustable learning rate. This can be done using Keras callbacks. Documentation for that is here. You want to use the ReduceLROnPlateau callback. Documentation for that is here. Set it up to monitor validation loss. My suggested code for that is below
rlronp=tf.keras.callbacks.ReduceLROnPlateau(monitor=""val_loss"",factor=0.5,
                                            patience=1, verbose=1)

I also recommend you use the callback EarlyStopping. Documentation for that is here. . My recomended code for that is shown below
estop=tf.keras.callbacks.EarlyStopping( monitor=""val_loss"", patience=4, verbose=1,
                                        restore_best_weights=True)

Now in model.fit include
callbacks=[rlronp, estop]

set your learning rate to .001. Set epochs=50. The estop callback if tripped will return your model loaded with the weights from the epoch with the lowest validation loss. I notice you have the code
for layer in conv_base.layers:
  layer.trainable = False

I know the tutorials tell you to do that but I get better results leaving it trainable and I have done this on hundreds of models.
",tensorflow
using tfkerasmetricsr2score results in an error in tensorflow,"I'm making a regression model with Tensorflow, but when I use tf.keras.metrics.R2Score() as a metric, it fails with ValueError: Tensor conversion requested dtype int32 for Tensor with dtype float32: <tf.Tensor: shape=(), dtype=float32, numpy=0.0> after the first epoch. (But works fine up until then) However, if I use a different metric (tf.keras.metrics.RootMeanSquaredError(), it works fine.
import pandas as pd

weather_states = pd.read_sql(""SELECT stations.id, stations.capacity_kw, start, wind_speed_10m, wind_direction_10m, wind_speed_80m, wind_direction_80m, wind_speed_180m, wind_direction_180m FROM stations INNER JOIN weather_states ON stations.id = weather_states.station WHERE weather_states.source = 'openmeteo_forecast/history/best' AND stations.source = 'wind'"", db_client)
 
grid_states = pd.read_sql(""SELECT start, wind FROM grid_states"", db_client)
 
def create_x_y(df: tuple[Any, pd.DataFrame]):
    start = df[1][""start""].iloc[0]
    res = df[1].sort_values(""id"").drop([""id"", ""start""], axis=1)
    temp_wind = grid_states.loc[grid_states[""start""] == start][""wind""].to_list()
    wind_kw = temp_wind if len(temp_wind) >= 1 else None
    res_flat_df = pd.DataFrame(res.to_numpy().reshape((1, -1)))
    res_flat_df[""wind_kw""] = wind_kw
    return res_flat_df
 
data = pd.concat(map(create_x_y, weather_states.groupby(""start""))).dropna()
from sklearn.model_selection import train_test_split
 
 
data = data.astype(""float32"")
train, test = train, test = train_test_split(data.dropna(), test_size=0.2)
 
train_y = train.pop(""wind_kw"")
train_x = train
 
 
test_y = test.pop(""wind_kw"")
test_x = test
 
norm = tf.keras.layers.Normalization()
norm.adapt(train_x)
 
model = tf.keras.Sequential([
    norm,
    tf.keras.layers.Dense(16, activation=""linear""),
    tf.keras.layers.Dropout(0.3),
    tf.keras.layers.Dense(1, activation=""linear""),
])
 
 
model.compile(
    optimizer=tf.keras.optimizers.legacy.Adam(0.001),
    metrics=[tf.keras.metrics.R2Score(dtype=tf.float32)],
    loss=tf.keras.losses.MeanSquaredError(),
)
 
model.fit(train_x, train_y, epochs=7, batch_size=2)
 
tf.keras.models.save_model(model, 'wind.keras')

print(data.describe())
              0            1            2            3            4  ...          241          242          243          244      wind_kw
count    1896.0  1896.000000  1896.000000  1896.000000  1896.000000  ...  1896.000000  1896.000000  1896.000000  1896.000000  1896.000000
mean   144000.0     4.315717   189.610759     5.791377   193.830169  ...     3.881292   145.420359     4.572205   143.642405  1292.576958
std         0.0     2.482439   113.178764     2.926497   113.685887  ...     2.612259    93.293471     2.775681    94.721086   611.333721
min    144000.0     0.100000     1.000000     0.100000     1.000000  ...     0.100000     2.000000     0.000000     1.000000    34.263000
25%    144000.0     2.110000    88.000000     3.487500    90.000000  ...     1.900000    67.000000     2.500000    63.000000   793.109500
50%    144000.0     4.110000   199.000000     5.500000   231.000000  ...     3.075000   137.000000     3.940000   135.000000  1251.590000
75%    144000.0     6.220000   291.000000     7.882500   294.000000  ...     5.502500   205.000000     6.082500   205.000000  1761.926750
max    144000.0    11.670000   360.000000    15.210000   360.000000  ...    14.460000   360.000000    16.980000   360.000000  3008.125000

print(type(data))
#<class 'pandas.core.frame.DataFrame'>
print(data.dtypes)
#0          float32
#1          float32
#2          float32
#3          float32
#4          float32
#            ...   
#241        float32
#242        float32
#243        float32
#244        float32
#wind_kw    float32
#Length: 246, dtype: object
print(data.shape)
#(1896, 246)

I can't seem to find any information online about this error when using R2Score- any ideas as to what could be the issue?
","It is a bug of TensorFlow 2.13.
Use TensorFlow 2.15 or higher.
",tensorflow
the analogue of torchautograd in tensorflow,"I want to get the dradients of the model after its training. For exmaple, I have the input tensor X and the output y, that is y = model(x). So using pytorch I can calculate the dradient with the folowing command:
y = model(x)
dydx = torch.autograd.grad(Y, X, torch.ones_like(Y), create_graph=True)[0][:, 0]

I want to get the same value after training model with TensorFlow framework.
I tried:
y = model.predict_u(x)
dydx = tf.gradients(y, x)[0]

But I got dydx as NoneType. I tried to include the dydx in the model class and to get the gradient through the tf.Session but I had: ""ResourceExhaustedError: Graph execution error"".
I have worked with Pytorch framework and now I decide to try TensorFlow, but I have some difficulties.
","To calculate gradients in TensorFlow similar to how you did it in PyTorch, you'll need to use TensorFlow's automatic differentiation capabilities. However, there are a few key differences to keep in mind:

TensorFlow 2.x uses eager execution by default, which is more similar
to PyTorch's dynamic graph approach.
You'll need to use    tf.GradientTape to record operations for
automatic differentiation.

Here's how you can calculate gradients in TensorFlow 2.x, similar to your PyTorch example:
import tensorflow as tf

# Assuming x is your input tensor and model is your TensorFlow model
x = tf.Variable(x)  # Make sure x is a Variable or use tf.convert_to_tensor(x)

with tf.GradientTape() as tape:
    y = model(x)
    
dydx = tape.gradient(y, x)

More on the subject:
https://www.tensorflow.org/guide/autodiff
https://www.tensorflow.org/api_docs/python/tf/GradientTape
UPD: there is also may be a problem with x
dydx is None: This usually happens because TensorFlow doesn't know it needs to compute gradients with respect to x. By default, only tf.Variable objects are watched. By using tape.watch(x), you explicitly tell TensorFlow to track x.
import tensorflow as tf

# Assume x_value is your input data as a NumPy array or TensorFlow tensor
x_value = ...  # Your input data
x = tf.convert_to_tensor(x_value)  # Convert to a TensorFlow tensor if not already one

with tf.GradientTape() as tape:
    tape.watch(x)  # Ensure x is being tracked for gradient computation
    y = model(x)   # Forward pass through your model

# Compute the gradient of y with respect to x
dydx = tape.gradient(y, x)

",tensorflow
error  failed to create temp directory quotcusersuserappdatalocaltempcondaltrandomgtquot,"When I try to Activate ""conda activate tensorflow_cpu""
conda activate tensorflow_cpu

Error : Failed to create temp directory ""C:\Users\user\AppData\Local\Temp\conda-\""
","It is due to a bug from conda developers. The bug is the temp path is having names with spaces, so to overcome please reassign the Env Variables TEMP, TMP.
(for windows)

go to environment variables
In ""User Variables for "" section look for TEMP, TMP
double click on TMP and in ""variable value"", type ""C:\conda_tmp""
similarly do it for TEMP
close env variables section
Restart the anaconda prompt, the error should vanish

",tensorflow
why do i run out of memory when training with a large dataset but have no problems with a small dataset,"I'm trying to build a keypoint detection system using Keras. I've got a UNet like model, with a series of convolutions, batch normalization, and max pooling, followed by a symmetric series of up sampling, convolution, and batch normalization layers (and skip connections). When given 100 instances, I'm able to call model.fit() without a problem. However, if I leave the model the same but use 500 instances, Keras crashes with an OOM exception. Why does this happen, and is there anything I can do to fix it?
Here's (what I think is) the relevant part of the code where I call model.fit:
model = build_model(
    filters=50,
    filter_step=1,
    stages=5,
    stage_steps=1,
    initial_convolutions=0,
    stacks=1,
)

print(model.summary()) 

dataset = tf.data.Dataset.from_tensor_slices((X, y))
dataset = dataset.batch(1)

model.fit(
    dataset,
    epochs=2**7,
    callbacks=[
        EarlyStopping(monitor=""loss"", patience=5, min_delta=1e-7, start_from_epoch=10),
        LearningRateScheduler(step_decay)
    ],
)

X and y are Numpy arrays with the following shapes:

X: (100, 1024, 1024, 3)
y: (100, 1024, 1024)

100 here is the data set size. If I increase this to 500 (or more), I get the out-of-memory exception. It appears to me that Keras is perhaps trying to load the entire data set into memory, despite using from_tensor_slices and batch(1), so I'm clearly misunderstanding something.
","
When you use tf.data.Dataset.from_tensor_slices((X, y)), TensorFlow
attempts to create a dataset where each element is a pair (X[i], y[i]). If the dataset is too large, this can consume a significant
amount of memory, especially if X and y are large.

To address this memory issue, we can modify the data loading process using a generator to load the data in batches, during runtime.
You'll have to define a generator that yields batches of data (X_batch, y_batch) and then to create the dataset use:
tf.data.Dataset.from_generator
Full documentation here.
And an example could look like this:
import numpy as np
import tensorflow as tf

# Assume X and y are your data
X = np.random.rand(500, 1024, 1024, 3)
y = np.random.rand(500, 1024, 1024)

# Define a generator to yield batches of data
def data_generator(X, y, batch_size):
    num_samples = X.shape[0]
    for i in range(0, num_samples, batch_size):
        yield X[i:i+batch_size], y[i:i+batch_size]

# Parameters
batch_size = 16

# Create a generator
generator = data_generator(X, y, batch_size)

# Create a tf.data.Dataset using the generator
dataset = tf.data.Dataset.from_generator(
    lambda: generator,
    output_signature=(
        tf.TensorSpec(shape=(None, 1024, 1024, 3), dtype=tf.float32),
        tf.TensorSpec(shape=(None, 1024, 1024), dtype=tf.float32)
    )
)

# Model and training code would go here...

",tensorflow
how to calculate r2 in tensorflow,"I am trying to do regression in Tensorflow. I'm not positive I am calculating R^2 correctly as Tensorflow gives me a different answer than sklearn.metrics.r2_score Can someone please look at my below code and let me know if I implemented the pictured equation correctly. Thanks

total_error = tf.square(tf.sub(y, tf.reduce_mean(y)))
unexplained_error = tf.square(tf.sub(y, prediction))
R_squared = tf.reduce_mean(tf.sub(tf.div(unexplained_error, total_error), 1.0))
R = tf.mul(tf.sign(R_squared),tf.sqrt(tf.abs(R_squared)))

","What you are computing the ""R^2"" is

compared to the given expression, you are computing the mean at the wrong place. You should take the mean when computing the errors, before doing the division.
unexplained_error = tf.reduce_sum(tf.square(tf.sub(y, prediction)))
total_error = tf.reduce_sum(tf.square(tf.sub(y, tf.reduce_mean(y))))
R_squared = tf.sub(1, tf.div(unexplained_error, total_error))

",tensorflow
how to build a twobranch keras model with dense and conv2d layers,"This is a simple example that reproduces my issue in a network I am trying to deploy.
I have an image input layer (which I need to maintain), then a Dense layer, Conv2D layer and a dense layer.
The idea is that the inputs are 10x10 images and the labels are 10x10 images. Inspired by my code and this example.
import numpy as np
from keras.models import Model
from keras.layers import Input, Conv2D

#Building model
size=10
a = Input(shape=(size,size,1))
hidden = Dense(size)(a)
hidden = Conv2D(kernel_size = (3,3), filters = size*size, activation='relu', padding='same')(hidden)
outputs = Dense(size, activation='sigmoid')(hidden)

model = Model(inputs=a, outputs=outputs)
model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])

#Create random data and accounting for 1 channel of data
n_images=55
data = np.random.randint(0,2,(n_images,size,size,1))
labels = np.random.randint(0,2,(n_images,size,size,1))

#Fit model
model.fit(data, labels, verbose=1, batch_size=10, epochs=20)

print(model.summary())

I get the following error: ValueError: Error when checking target: expected dense_92 to have shape (10, 10, 10) but got array with shape (10, 10, 1)

I don't get an error if I change:
outputs = Dense(size, activation='sigmoid')(hidden)
with:
outputs = Dense(1, activation='sigmoid')(hidden)
No idea how Dense(1) is even valid and how it allows 10x10 output signal as model.summary() indicates:
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_26 (InputLayer)        (None, 10, 10, 1)         0         
_________________________________________________________________
dense_93 (Dense)             (None, 10, 10, 10)        20        
_________________________________________________________________
conv2d_9 (Conv2D)            (None, 10, 10, 100)       9100      
_________________________________________________________________
dense_94 (Dense)             (None, 10, 10, 1)         101       
=================================================================
Total params: 9,221
Trainable params: 9,221
Non-trainable params: 0
_________________________________________________________________
None

Edit (moved from old comments):


what I am trying to do isn't standard. I have set of images and for each image I want to find a binary image of the same size that if the
value of its pixel is 1 it means the feature exists in the input image

the insight wether a pixel has a feature should be taken both from local information (extracted by a convolution layers) and global
information extracted by Dense layers.



","Well, according to your comments:

what I am trying to do isn't standard. I have set of images and for
  each image I want to find a binary image of the same size that if the
  value of its pixel is 1 it means the feature exists in the input image
the insight wether a pixel has a feature should be taken both from
  local information (extracted by a convolution layers) and global
  information extracted by Dense layers.

I guess you are looking for creating a two branch model where one branch consists of convolution layers and another one is simply one or more dense layers on top of each other (although, I should mention that in my opinion one convolution network may achieve what you are looking for, because the combination of pooling and convolution layers and then maybe some up-sampling layers at the end somehow preserves both local and global information). To define such a model, you can use Keras functional API like this:
from keras import models
from keras import layers

input_image = layers.Input(shape=(10, 10, 1))

# branch one: dense layers
b1 = layers.Flatten()(input_image)
b1 = layers.Dense(64, activation='relu')(b1)
b1_out = layers.Dense(32, activation='relu')(b1)

# branch two: conv + pooling layers
b2 = layers.Conv2D(32, (3,3), activation='relu')(input_image)
b2 = layers.MaxPooling2D((2,2))(b2)
b2 = layers.Conv2D(64, (3,3), activation='relu')(b2)
b2_out = layers.MaxPooling2D((2,2))(b2)

# merge two branches
flattened_b2 = layers.Flatten()(b2_out)
merged = layers.concatenate([b1_out, flattened_b2])

# add a final dense layer
output = layers.Dense(10*10, activation='sigmoid')(merged)
output = layers.Reshape((10,10))(output)

# create the model
model = models.Model(input_image, output)

model.compile(optimizer='rmsprop', loss='binary_crossentropy')
model.summary()

Model summary:
__________________________________________________________________________________________________
Layer (type)                    Output Shape         Param #     Connected to                     
==================================================================================================
input_1 (InputLayer)            (None, 10, 10, 1)    0                                            
__________________________________________________________________________________________________
conv2d_1 (Conv2D)               (None, 8, 8, 32)     320         input_1[0][0]                    
__________________________________________________________________________________________________
max_pooling2d_1 (MaxPooling2D)  (None, 4, 4, 32)     0           conv2d_1[0][0]                   
__________________________________________________________________________________________________
flatten_1 (Flatten)             (None, 100)          0           input_1[0][0]                    
__________________________________________________________________________________________________
conv2d_2 (Conv2D)               (None, 2, 2, 64)     18496       max_pooling2d_1[0][0]            
__________________________________________________________________________________________________
dense_1 (Dense)                 (None, 64)           6464        flatten_1[0][0]                  
__________________________________________________________________________________________________
max_pooling2d_2 (MaxPooling2D)  (None, 1, 1, 64)     0           conv2d_2[0][0]                   
__________________________________________________________________________________________________
dense_2 (Dense)                 (None, 32)           2080        dense_1[0][0]                    
__________________________________________________________________________________________________
flatten_2 (Flatten)             (None, 64)           0           max_pooling2d_2[0][0]            
__________________________________________________________________________________________________
concatenate_1 (Concatenate)     (None, 96)           0           dense_2[0][0]                    
                                                                 flatten_2[0][0]                  
__________________________________________________________________________________________________
dense_3 (Dense)                 (None, 100)          9700        concatenate_1[0][0]              
__________________________________________________________________________________________________
reshape_1 (Reshape)             (None, 10, 10)       0           dense_3[0][0]                    
==================================================================================================
Total params: 37,060
Trainable params: 37,060
Non-trainable params: 0
__________________________________________________________________________________________________

Note that this is one way of achieving what you are looking for and it may or may not work for the specific problem and the data you are working on. You may modify this model (e.g. remove the pooling layers or add more dense layers) or completely use another architecture with different kind of layers (e.g. up-sampling, conv2dtrans) to reach a better accuracy. At the end, you must experiment to find the perfect solution.
Edit:
For completeness here is how to generate data and fitting the network:
n_images=10
data = np.random.randint(0,2,(n_images,size,size,1))
labels = np.random.randint(0,2,(n_images,size,size,1))
model.fit(data, labels, verbose=1, batch_size=32, epochs=20)

",tensorflow
invalidversionspecerror invalid version spec 27,"I first tried to install tensorflow=2.4.0 in Conda with the channel defaults, but it caused an error:
conda install tensorflow=2.4.0

Error:
Solving environment: failed
PackagesNotFoundError: The following packages are not available from current channels:
  - tensorflow=2.4.0

Current channels:

  - https://repo.continuum.io/pkgs/main/linux-64
  - https://repo.continuum.io/pkgs/main/noarch
  - https://repo.continuum.io/pkgs/free/linux-64
  - https://repo.continuum.io/pkgs/free/noarch
  - https://repo.continuum.io/pkgs/r/linux-64
  - https://repo.continuum.io/pkgs/r/noarch
  - https://repo.continuum.io/pkgs/pro/linux-64
  - https://repo.continuum.io/pkgs/pro/noarch

Then I'm tried using the Conda Forge channel:
conda install -c conda-forge tensorflow=2.4.0

Error:
Solving environment: failed
InvalidVersionSpecError: Invalid version spec: =2.7

How do I resolve this error?
","It looks like you are trying to install using an older version of conda.  There is a similar issue posted here: https://github.com/facebookresearch/vissl/issues/198
Try to update conda to 4.9.2 or higher using:
conda update conda -n base -y


If you don't have permission to update your conda version, you can always create a new environment with conda in it, and use that environment to install the package.
(base)$ conda create -n conda_env python conda==4.9.2
(base)$ conda activate conda_env
(conda_env)$ conda init --user
(conda_env)$ conda install tensorflow==2.4.0 -n my_other_env

You should check that your conda_env is picking up whichever config options you have as well.
",tensorflow
do i need to define metrics in modelcompile in order to use them later,"I am trying to train a model and after it is train, I want to see TP TN FP FN, recall, precision, and Sensitivity.
Question 1: Do I need to define all these metrics when I compile the model like this?
metrics = [CategoricalAccuracy(),Precision(),Recall(),TruePositives(),TrueNegatives(),FalseNegatives(),FalsePositives(),SensitivityAtSpecificity(0.5)]
model.compile(optimizer=Adam(learning_rate = 0.004), loss=CategoricalCrossentropy(from_logits = True), metrics=metrics)

What I want to do is after the model is trained, I want to evaluate it with these metrics and see how it did on the test set.
Question 2: If I run model.evaluate, are the metrics used in model.compile going to be used or can I define more metrics when I am doing evaluation?
For example I want to monitor accuracy during training and then Recall/Precision and so on when I evaluate.
","If you don't want to monitor Precision, Recall you don't have to put them on compile. You can simply use tf.keras.metrics.Precision() after getting predictions using model.predict. But if you want to use model.evaluate you need to put them on model.compile. This is because model.evaluate makes use only of the metrics mentioned when you compile the model, which are initiated when you call model.compile
",tensorflow
importerror cannot import name 39model_from_config39 from 39tensorflowkerasmodels39 for mac m1,"Trying to run a gym environment with RL agent on Mac M1.
import gym 
import numpy as np

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Flatten
from tensorflow.keras.optimizers import Adam

from rl.agents import DQNAgent 

While running this, it is giving error for the last line:
ImportError: cannot import name 'model_from_config' from 'tensorflow.keras.models' (/Users/username/miniforge3/envs/rl_gym/lib/python3.12/site-packages/keras/_tf_keras/keras/models/__init__.py). Did you mean: 'model_from_json'?

This is on Sonoma 14.4.1. Most of the other discussions (like this one link) suggested reinstalling tensorflow and other packages. I tried that but it did not work. The error still persists. This is inside a conda environment where I have keras-3.2.1, tensorflow-2.16.1 and keras-rl2-1.0.5. What am I dong wrong here?
","TensorFlow 2.16 uses Keras 3 by default, which has a slightly different API than Keras 2. See the release notes of TensorFlow 2.16:

Keras 3 will be the default Keras version for TensorFlow 2.16 onwards.
You may need to update your script to use Keras 3. Please refer to the
new Keras documentation for Keras 3 (https://keras.io/keras_3). Keras
2 will continue to be released alongside TensorFlow as tf_keras. To
continue using Keras 2 with TensorFlow 2.16+:

As a result, the tf.keras API also changed, and some functions are not available anymore. If you compare the documentation of the tf.keras.models module from TF 2.15 and the one from TF 2.16, you'll see that some serialization functions were removed.
The solution is to downgrade your version of TensorFlow to the 2.15 which is the latest compatible with the keras-rl library you're using.
",tensorflow
adding an attention block in deep neural network issue for regression problem,"I want to add an tf.keras.layers.MultiHeadAttention inside the two layers of neural network. However, I am getting IndexError:
The detailed code are as follow
x1 = Dense(58, activation='relu')(x1)
x1 = Dropout(0.1)(x1)
print(x1.shape)
attention = tf.keras.layers.MultiHeadAttention(num_heads=2, key_dim=58,
            dropout=0.1,output_shape=x1.shape)(x1,x1)

x1 = Dropout(0.2)(attention)
x1 = Dense(59, activation='relu')(x1)
output = Dense(1, activation='linear')(x1)` 
model = tf.keras.models.Model(inputs=input1, outputs=output)

In the above code I am getting following error
IndexError: Exception encountered when calling layer 'softmax' (type Softmax).

tuple index out of range

Call arguments received by layer 'softmax' (type Softmax):
  • inputs=tf.Tensor(shape=(None, 2), dtype=float32)
  • mask=None
Note that
`x1.shape`= (None, 58)

","The problem is solved now. MultiHeadAttention layer in TensorFlow expects a 3D input tensor. Therefor to introduce an attention block into normal neural network, there is need to set inputs and outputs of that block accordingly. So the updated code is as follow
    x1 = Dense(58, activation='relu')(x1)
    x1 = Dropout(0.1)(x1)
    x1 = tf.expand_dims(x1, axis=1) # here we need to expand dimension
    print(x1.shape)

    attention = tf.keras.layers.MultiHeadAttention(num_heads=3, key_dim=x1.shape[2], dropout=0.2)(x1, x1)
    x1 = Dropout(0.2)(attention)
    x1 = tf.keras.layers.LayerNormalization()(x1)
    x1 = tf.squeeze(x1, axis=1) # set dimension here again
    x1 = Dense(10, activation='relu')(x1)
    output = Dense(1, activation='linear')(x1)

",tensorflow
tensorflow how to convert meta data and index model files into one graphpb file,"In tensorflow the training from the scratch produced following 6 files:


events.out.tfevents.1503494436.06L7-BRM738 
model.ckpt-22480.meta 
checkpoint 
model.ckpt-22480.data-00000-of-00001 
model.ckpt-22480.index 
graph.pbtxt


I would like to convert them (or only the needed ones) into one file graph.pb to be able to transfer it to my Android application.
I tried the script freeze_graph.py but it requires as an input already the input.pb file which I do not have. (I have only these 6 files mentioned before). How to proceed to get this one freezed_graph.pb file? I saw several threads but none was working for me.
","You can use this simple script to do that. But you must specify the names of the output nodes.
import tensorflow as tf

meta_path = 'model.ckpt-22480.meta' # Your .meta file
output_node_names = ['output:0']    # Output nodes

with tf.Session() as sess:
    # Restore the graph
    saver = tf.train.import_meta_graph(meta_path)

    # Load weights
    saver.restore(sess,tf.train.latest_checkpoint('path/of/your/.meta/file'))

    # Freeze the graph
    frozen_graph_def = tf.graph_util.convert_variables_to_constants(
        sess,
        sess.graph_def,
        output_node_names)

    # Save the frozen graph
    with open('output_graph.pb', 'wb') as f:
      f.write(frozen_graph_def.SerializeToString())

If you don't know the name of the output node or nodes, there are two ways

You can explore the graph and find the name with Netron or with console summarize_graph utility. 
You can use all the nodes as output ones as shown below.

output_node_names = [n.name for n in tf.get_default_graph().as_graph_def().node]

(Note that you have to put this line just before convert_variables_to_constants call.)
But I think it's unusual situation, because if you don't know the output node, you cannot use the graph actually.
",tensorflow
tensorflow  invalidargumenterror shapes of all inputs must match,"I am currently following the TensorFlow guide/tutorial on seq2seq NMT models (https://www.tensorflow.org/text/tutorials/nmt_with_attention) using a Jupyter Notebook.
Upon running the following code,
# Setup the loop variables.
next_token, done, state = decoder.get_initial_state(ex_context)
tokens = []

for n in range(10):
  # Run one step.
  next_token, done, state = decoder.get_next_token(
      ex_context, next_token, done, state, temperature=1.0)
  # Add the token to the output.
  tokens.append(next_token)

# Stack all the tokens together.
tokens = tf.concat(tokens, axis=-1) # (batch, t)

# Convert the tokens back to a a string
result = decoder.tokens_to_text(tokens)
result[:3].numpy()

I receive an InvalidArgument Error as follows:
---------------------------------------------------------------------------
InvalidArgumentError                      Traceback (most recent call last)
Cell In[31], line 2
      1 # Setup the loop variables.
----> 2 next_token, done, state = decoder.get_initial_state(ex_context)
      3 tokens = []
      5 for n in range(10):
      6   # Run one step.

Cell In[28], line 8
      6 embedded = self.embedding(start_tokens)
      7 print(embedded)
----> 8 return start_tokens, done, self.rnn.get_initial_state(embedded)[0]

File ~/Library/Python/3.11/lib/python/site-packages/keras/src/layers/rnn/rnn.py:309, in RNN.get_initial_state(self, batch_size)
    307 get_initial_state_fn = getattr(self.cell, ""get_initial_state"", None)
    308 if get_initial_state_fn:
--> 309     init_state = get_initial_state_fn(batch_size=batch_size)
    310 else:
    311     return [
    312         ops.zeros((batch_size, d), dtype=self.cell.compute_dtype)
    313         for d in self.state_size
    314     ]

File ~/Library/Python/3.11/lib/python/site-packages/keras/src/layers/rnn/gru.py:326, in GRUCell.get_initial_state(self, batch_size)
    324 def get_initial_state(self, batch_size=None):
    325     return [
--> 326         ops.zeros((batch_size, self.state_size), dtype=self.compute_dtype)
    327     ]

File ~/Library/Python/3.11/lib/python/site-packages/keras/src/ops/numpy.py:5968, in zeros(shape, dtype)
   5957 @keras_export([""keras.ops.zeros"", ""keras.ops.numpy.zeros""])
   5958 def zeros(shape, dtype=None):
   5959     """"""Return a new tensor of given shape and type, filled with zeros.
   5960 
   5961     Args:
   (...)
   5966         Tensor of zeros with the given shape and dtype.
   5967     """"""
-> 5968     return backend.numpy.zeros(shape, dtype=dtype)

File ~/Library/Python/3.11/lib/python/site-packages/keras/src/backend/tensorflow/numpy.py:619, in zeros(shape, dtype)
--> 617 return tf.zeros(shape, dtype=dtype)

File ~/Library/Python/3.11/lib/python/site-packages/tensorflow/python/util/traceback_utils.py:153, in filter_traceback.<locals>.error_handler(*args, **kwargs)
    151 except Exception as e:
    152   filtered_tb = _process_traceback_frames(e.__traceback__)
--> 153   raise e.with_traceback(filtered_tb) from None
    154 finally:
    155   del filtered_tb

File ~/Library/Python/3.11/lib/python/site-packages/tensorflow/python/framework/ops.py:5983, in raise_from_not_ok_status(e, name)
   5981 def raise_from_not_ok_status(e, name) -> NoReturn:
   5982   e.message += ("" name: "" + str(name if name is not None else """"))
-> 5983   raise core._status_to_exception(e) from None

InvalidArgumentError: {{function_node __wrapped__Pack_N_2_device_/job:localhost/replica:0/task:0/device:CPU:0}} Shapes of all inputs must match: values[0].shape = [64,1,256] != values[1].shape = [] [Op:Pack] name: 

Any ideas? I'm pretty sure I'm following the guide to the letter.
","Turns out my only problem was versioning - use python 3.10 and tensorflow 2.11 and tensorflow-text 2.11. Creating a virtual environment with pyenv seemed to solve the problem. The wheels for the tensorflow and tensorflow-text packages can be found on PyPi, since pip didn't have those versions.
The other packages I used were:

einops==0.6.0
matplotlib==3.6.1
numpy==1.23.3

",tensorflow
valueerror unrecognized model in mrpc should have a model_type key in its configjson or contain one of the following strings in its name,"Goal: Amend this Notebook to work with Albert and Distilbert models
Kernel: conda_pytorch_p36. I did Restart & Run All, and refreshed file view in working directory.
Error occurs in Section 1.2, only for these 2 new models.
For filenames etc., I've created a variable used everywhere:
MODEL_NAME = 'albert-base-v2'  # 'distilbert-base-uncased', 'bert-base-uncased'

I replaced imports with:
from transformers import (AutoConfig, AutoModel, AutoTokenizer)
#from transformers import (BertConfig, BertForSequenceClassification, BertTokenizer,)

As suggested in Transformers Documentation - Auto Classes.

Instantiating one of AutoConfig, AutoModel, and AutoTokenizer will directly create a class of the relevant architecture.


Section 1.2:
# load model
model = AutoModel.from_pretrained(configs.output_dir)  # BertForSequenceClassification
model.to(configs.device)


# quantize model
quantized_model = torch.quantization.quantize_dynamic(
    model, {torch.nn.Linear}, dtype=torch.qint8
)

#print(quantized_model)

def print_size_of_model(model):
    torch.save(model.state_dict(), ""temp.p"")
    print('Size (MB):', os.path.getsize(""temp.p"")/(1024*1024))
    os.remove('temp.p')

print_size_of_model(model)
print_size_of_model(quantized_model)

Traceback:
ValueError: Unrecognized model in ./MRPC/. Should have a `model_type` key in its config.json, or contain one of the following strings in its name: imagegpt, qdqbert, vision-encoder-decoder, trocr, fnet, segformer, vision-text-dual-encoder, perceiver, gptj, layoutlmv2, beit, rembert, visual_bert, canine, roformer, clip, bigbird_pegasus, deit, luke, detr, gpt_neo, big_bird, speech_to_text_2, speech_to_text, vit, wav2vec2, m2m_100, convbert, led, blenderbot-small, retribert, ibert, mt5, t5, mobilebert, distilbert, albert, bert-generation, camembert, xlm-roberta, pegasus, marian, mbart, megatron-bert, mpnet, bart, blenderbot, reformer, longformer, roberta, deberta-v2, deberta, flaubert, fsmt, squeezebert, hubert, bert, openai-gpt, gpt2, transfo-xl, xlnet, xlm-prophetnet, prophetnet, xlm, ctrl, electra, speech-encoder-decoder, encoder-decoder, funnel, lxmert, dpr, layoutlm, rag, tapas, splinter, sew-d, sew, unispeech-sat, unispeech, wavlm

Please let me know if there's anything else I can add to post.
","Explanation:
When instantiating AutoModel, you must specify a model_type parameter in ./MRPC/config.json file (downloaded during Notebook runtime).
List of model_types can be found here.

Solution:
Code that appends model_type to config.json, in the same format:
import json

json_filename = './MRPC/config.json'

with open(json_filename) as json_file:
    json_decoded = json.load(json_file)

json_decoded['model_type'] = # !!

with open(json_filename, 'w') as json_file:
    json.dump(json_decoded, json_file, indent=2, separators=(',', ': '))

config.json:
{
  ""attention_probs_dropout_prob"": 0.1,
  ""finetuning_task"": ""mrpc"",
  ""hidden_act"": ""gelu"",
  ""hidden_dropout_prob"": 0.1,
  ""hidden_size"": 768,
  ""initializer_range"": 0.02,
  ""intermediate_size"": 3072,
  ""layer_norm_eps"": 1e-12,
  ""max_position_embeddings"": 512,
  ""num_attention_heads"": 12,
  ""num_hidden_layers"": 12,
  ""num_labels"": 2,
  ""output_attentions"": false,
  ""output_hidden_states"": false,
  ""pruned_heads"": {},
  ""torchscript"": false,
  ""type_vocab_size"": 2,
  ""vocab_size"": 30522,
  ""model_type"": ""albert""
}


",tensorflow
using tensorflow through keras  valueerror only input tensors may be passed as positional arguments,"I am trying to get keras_model_sequential to work in R:
library(tidyverse)
library(keras)

model <- keras_model_sequential(input_shape = c(8)) 
model %>% 
  layer_dense(units = 32) %>% 
  layer_activation('softmax') 

Defining model goes just fine. Running the last line, however, gives the following error.
Error in py_call_impl(callable, call_args$unnamed, call_args$named) : 
  ValueError: Only input tensors may be passed as positional arguments. The following argument value should be passed as a keyword argument: <Sequential name=sequential_5, built=False> (of type <class 'keras.src.models.sequential.Sequential'>)
Run `reticulate::py_last_error()` for details.

Following the advice in the last line I get
── R Traceback ─────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────
    ▆
 1. ├─model %>% layer_dense(units = 32) %>% ...
 2. ├─keras::layer_activation(., ""softmax"")
 3. │ └─keras::create_layer(...)
 4. └─keras::layer_dense(., units = 32)
 5.   └─keras::create_layer(...)
 6.     ├─keras:::compose_layer(object, layer)
 7.     └─keras:::compose_layer.default(object, layer)
 8.       └─reticulate (local) layer(object, ...)
 9.         └─reticulate:::py_call_impl(callable, call_args$unnamed, call_args$named)
See `reticulate::py_last_error()$r_trace$full_call` for more details.

I do not know what to make of this. I have tried to look online what the error means, but I am not getting problems that look similar, mostly a bunch of Python code.
In my efforts to solve this, I have updated my RStudio to 2024.09.0, Keras to 2.15.0 and R to 4.4.1 However, I am still getting exactly the same error.
How can I solve this error and successfully run keras_model_sequential?
","The error seems to be a bug with current release of keras package. Please remove it and try keras3 instead as suggested here
",tensorflow
how can i combine my three 2d tensors into a single 3d tensor in tensor flow,"Hello I am a newbie with the tensorflow and currently, I am working with colour Images and it's PCAS.
I have extracted PCAS in a form of ""Red"",""Green"" and ""Blue"" and also computed the weights which are associated with ""Red"",""Green"" and ""Blue"" components.
After doing the all the above stuff I want to combine all three 2D matrices into the single 3D matrix.
For a tensorflow it would be a 3D tensor.
def multi(h0,ppca,mu,i,scope=None):
with tf.variable_scope(scope or""multi""):
        return tf.matmul(ppca[:,:,0],h0[i,:,:,0]) + tf.reshape(mu[:,0],[4096,1]) , tf.matmul(ppca[:,:,1],h0[i,:,:,1]) + tf.reshape(mu[:,1],[4096,1]) ,tf.matmul(ppca[:,:,2],h0[i,:,:,2]) + tf.reshape(mu[:,2],[4096,1]) 

So from the above function, I will get all three different 2D tensors and  want to combine those 2D tensors to single 3D tensor which has dimensions [4096,1,3]  
How can I do that? 
any help is highly appreciated.  
","You need to concat them like this:
three_d_image = tf.concat(0, [[r], [g], [b]])

This tells tensorflow to concat them along the x dimension and treat each tensor as a matrix.
Doing the same without the additional brackets around the r,g,b tensors will try to concat them to one large 2D matrix
",tensorflow
valueerror in user code while using keras model in r,"I am trying to run 1 dimensional CNN in R using keras package. I am using the following code
library(MASS)
library(keras)

##Create some data
data(""Boston"")
data <- Boston
# create a list of 70% of the rows in the original dataset we can use for training
set.seed(123)
training <- sample(nrow(data), 0.7 * nrow(data))

dataTrain <- data[training,]
dataTest <- data[-training,]

dataTrain_y <- as.matrix(dataTrain$medv)
dataTrain_x <- as.matrix(subset(dataTrain, select = -c(medv)))

dataTest_y <- as.matrix(dataTest$medv)
dataTest_x <- as.matrix(subset(dataTest, select = -c(medv)))

#Reshaping the data for CNN
dataTrain_x <- array_reshape(dataTrain_x, c(ncol(dataTrain_x), nrow(dataTrain_x), 1))
dataTest_x <- array_reshape(dataTest_x, c(ncol(dataTest_x), nrow(dataTest_x), 1))

#CNN model
model <- keras_model_sequential() %>%
  layer_conv_1d(filters=32, kernel_size=4, activation=""relu"", 
                input_shape=c(ncol(dataTrain_x), nrow(dataTrain_x))) %>%
  layer_max_pooling_1d(pool_size=2) %>%
  layer_conv_1d(filters=64, kernel_size=2, activation=""relu"") %>%
  layer_max_pooling_1d(pool_size=2) %>%
  layer_dropout(rate=0.4) %>%
  layer_flatten() %>%
  layer_dense(units=100, activation=""relu"") %>%
  layer_dropout(rate=0.2) %>%
  layer_dense(units=1, activation=""linear"")

model %>% compile(
  loss = ""mse"",
  optimizer =  ""adam"", #'sgd' can also be used
  metrics = list(""mean_absolute_error"")
)

model %>% summary()

history <- model %>% fit(dataTrain_x, dataTrain_y, 
                         epochs = 100, batch_size = 50, 
                         #callbacks = callback_tensorboard(""logs/run_a""),
                         validation_split = 0.2)

But it returns me the following error
Error in py_call_impl(callable, dots$args, dots$keywords) : 
  ValueError: in user code:
    C:\Python37\lib\site-packages\tensorflow\python\keras\engine\training.py:805 train_function  *
        return step_function(self, iterator)
    C:\Python37\lib\site-packages\tensorflow\python\keras\engine\training.py:795 step_function  **
        outputs = model.distribute_strategy.run(run_step, args=(data,))
    C:\Python37\lib\site-packages\tensorflow\python\distribute\distribute_lib.py:1259 run
        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)
    C:\Python37\lib\site-packages\tensorflow\python\distribute\distribute_lib.py:2730 call_for_each_replica
        return self._call_for_each_replica(fn, args, kwargs)
    C:\Python37\lib\site-packages\tensorflow\python\distribute\distribute_lib.py:3417 _call_for_each_replica
        return fn(*args, **kwargs)
    C:\Python37\lib\site-packages\tensorflow\python\keras\engine\training.py:788 run_step  **
        outputs = model.train_step(data)
    C:\Python37\lib\site-pac 

Now how can I solve this error?
","The error message is truncated, so it's not much help, but it looks to me like you've got some minor typos. If you make the suggested changes (the ""###"" comments) the model appears to compile and train as expected, e.g.
library(MASS)
library(keras)

##Create some data
data(""Boston"")
data <- Boston
# create a list of 70% of the rows in the original dataset we can use for training
set.seed(123)
training <- sample(nrow(data), 0.7 * nrow(data))

dataTrain <- data[training,]
dataTest <- data[-training,]

dataTrain_y <- as.matrix(dataTrain$medv)
dataTrain_x <- as.matrix(subset(dataTrain, select = -c(medv)))

dataTest_y <- as.matrix(dataTest$medv)
dataTest_x <- as.matrix(subset(dataTest, select = -c(medv)))

#Reshaping the data for CNN
### These dimensions don't look correct; switch ncol() with nrow()
dataTrain_x <- array_reshape(dataTrain_x, c(nrow(dataTrain_x), ncol(dataTrain_x), 1))
dataTest_x <- array_reshape(dataTest_x, c(nrow(dataTest_x), ncol(dataTest_x), 1))

#CNN model
model <- keras_model_sequential() %>%
  layer_conv_1d(filters=32, kernel_size=4, activation=""relu"", 
                ### The input shape doesn't look correct; instead of 
                ### `c(ncol(dataTrain_x), nrow(dataTrain_x))` (354, 13)
                ### I believe you want `dim(dataTest_x)` (13, 1)
                input_shape=c(ncol(dataTrain_x), 1)) %>%
  layer_max_pooling_1d(pool_size=2) %>%
  layer_conv_1d(filters=64, kernel_size=2, activation=""relu"") %>%
  layer_max_pooling_1d(pool_size=2) %>%
  layer_dropout(rate=0.4) %>%
  layer_flatten() %>%
  layer_dense(units=100, activation=""relu"") %>%
  layer_dropout(rate=0.2) %>%
  layer_dense(units=1, activation=""linear"")

model %>% compile(
  loss = ""mse"",
  optimizer =  ""adam"", #'sgd' can also be used
  metrics = list(""mean_absolute_error"")
)

model %>% summary()
#> Model: ""sequential""
#> ________________________________________________________________________________
#> Layer (type)                        Output Shape                    Param #     
#> ================================================================================
#> conv1d_1 (Conv1D)                   (None, 10, 32)                  160         
#> ________________________________________________________________________________
#> max_pooling1d_1 (MaxPooling1D)      (None, 5, 32)                   0           
#> ________________________________________________________________________________
#> conv1d (Conv1D)                     (None, 4, 64)                   4160        
#> ________________________________________________________________________________
#> max_pooling1d (MaxPooling1D)        (None, 2, 64)                   0           
#> ________________________________________________________________________________
#> dropout_1 (Dropout)                 (None, 2, 64)                   0           
#> ________________________________________________________________________________
#> flatten (Flatten)                   (None, 128)                     0           
#> ________________________________________________________________________________
#> dense_1 (Dense)                     (None, 100)                     12900       
#> ________________________________________________________________________________
#> dropout (Dropout)                   (None, 100)                     0           
#> ________________________________________________________________________________
#> dense (Dense)                       (None, 1)                       101         
#> ================================================================================
#> Total params: 17,321
#> Trainable params: 17,321
#> Non-trainable params: 0
#> ________________________________________________________________________________

history <- model %>% fit(dataTrain_x, dataTrain_y, 
                         epochs = 100, batch_size = 50, 
                         #callbacks = callback_tensorboard(""logs/run_a""),
                         validation_split = 0.2)


Created on 2021-07-20 by the reprex package (v2.0.0)
",tensorflow
convert between nhwc and nchw in tensorflow,"What is the best way to convert a tensor from NHWC format to NCHW format, and vice versa?
Is there an op specifically that does this, or will I need to use some combination of the split/concat type operations?
","All you need to do is a permutation of the dimensions from NHWC to NCHW (or the contrary).
The meaning of each letter might help understand:

N: number of images in the batch
H: height of the image
W: width of the image
C: number of channels of the image (ex: 3 for RGB, 1 for grayscale...)


From NHWC to NCHW
The image shape is (N, H, W, C) and we want the output to have shape (N, C, H, W). Therefore we need to apply tf.transpose with a well chosen permutation perm.

The returned tensor's dimension i will correspond to the input dimension perm[i]

perm[0] = 0  # output dimension 0 will be 'N', which was dimension 0 in the input
perm[1] = 3  # output dimension 1 will be 'C', which was dimension 3 in the input
perm[2] = 1  # output dimension 2 will be 'H', which was dimension 1 in the input
perm[3] = 2  # output dimension 3 will be 'W', which was dimension 2 in the input

In practice:
images_nhwc = tf.placeholder(tf.float32, [None, 200, 300, 3])  # input batch
out = tf.transpose(images_nhwc, [0, 3, 1, 2])
print(out.get_shape())  # the shape of out is [None, 3, 200, 300]


From NCHW to NHWC
The image shape is (N, C, H, W) and we want the output to have shape (N, H, W, C). Therefore we need to apply tf.transpose with a well chosen permutation perm.

The returned tensor's dimension i will correspond to the input dimension perm[i]

perm[0] = 0  # output dimension 0 will be 'N', which was dimension 0 in the input
perm[1] = 2  # output dimension 1 will be 'H', which was dimension 2 in the input
perm[2] = 3  # output dimension 2 will be 'W', which was dimension 3 in the input
perm[3] = 1  # output dimension 3 will be 'C', which was dimension 1 in the input

In practice:
images_nchw = tf.placeholder(tf.float32, [None, 3, 200, 300])  # input batch
out = tf.transpose(images_nchw, [0, 2, 3, 1])
print(out.get_shape())  # the shape of out is [None, 200, 300, 3]

",tensorflow
how do i force clustering of data in a specific evident pattern,"I have a large set of 'Vehicle speed vs Engine RPM' values for a vehicle. I'm try to predict the time spent by the vehicle on each gear.
I ran K-Means clustering on the dataset and got the following result:

Clearly, my algorithm has failed to capture the evident pattern. I want to force K-Means (or any other clustering algorithm, for that matter) to cluster data along the six sloped lines. Snippet of relevant code:
import numpy as np
import pandas as pd
from matplotlib import pyplot as plt
from sklearn.cluster import KMeans

plt.rcParams['figure.figsize'] = (16, 9)
plt.style.use('ggplot')

# Importing the dataset
data = pd.read_csv('speedRpm.csv')
print(data.shape)
data.head()

# Getting the data points
f1 = data['rpm'].values
f2 = data['speed'].values
X = np.array(list(zip(f1, f2)))

# Number of clusters
k = 5

kmeans = KMeans(n_clusters=k)
# Fitting the input data
kmeans = kmeans.fit(X)
# Getting the cluster labels
labels = kmeans.predict(X)
# Centroid values
centroids = kmeans.cluster_centers_

labeled_array = {i: X[np.where(kmeans.labels_ == i)] for i in range(kmeans.n_clusters)}

colors = ['r', 'g', 'b', 'y', 'c']
fig, ax = plt.subplots()
for i in range(k):
        points = np.array([X[j] for j in range(len(X)) if kmeans.labels_[j] == i])
        ax.scatter(points[:, 0], points[:, 1], s=7, c=colors[i])
ax.scatter(centroids[:, 0], centroids[:, 1], marker='*', s=200, c='#050505')

plt.show()

How do I make sure the clustering algorithm captures the right pattern, even though it possibly isn't the most efficient?
Thanks!
EDIT:
Ran the same set of points using DBSCAN this time. After playing around with the eps and min_samples values for sometime, got the following result:
 
Although, still not perfect and way too many outliers, the algorithm is beginning to capture the linear trend.
Code:
import numpy as np
import pandas as pd
from matplotlib import pyplot as plt
from sklearn.cluster import KMeans
from sklearn.cluster import DBSCAN

plt.rcParams['figure.figsize'] = (16, 9)
plt.style.use('ggplot')

# Importing the dataset
data = pd.read_csv('speedRpm.csv')
print(data.shape)
data.head()

# Getting the values and plotting it
f1 = data['rpm'].values
f2 = data['speed'].values
X = np.array(list(zip(f1, f2)))

# DBSCAN

# Compute DBSCAN
db = DBSCAN(eps=1.1, min_samples=3).fit(X)
core_samples_mask = np.zeros_like(db.labels_, dtype=bool)
core_samples_mask[db.core_sample_indices_] = True
labels = db.labels_

# Number of clusters in labels, ignoring noise if present.
n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)
print ""Estimated Number of Clusters"", n_clusters_

# Black removed and is used for noise instead.
unique_labels = set(labels)
colors = [plt.cm.Spectral(each)
          for each in np.linspace(0, 1, len(unique_labels))]
for k, col in zip(unique_labels, colors):
    if k == -1:
        # Black used for noise.
        col = [0, 0, 0, 1]

    class_member_mask = (labels == k)

    xy = X[class_member_mask & core_samples_mask]
    plt.plot(xy[:, 0], xy[:, 1], 'o', markerfacecolor=tuple(col),
             markeredgecolor='k', markersize=14)

    xy = X[class_member_mask & ~core_samples_mask]
    plt.plot(xy[:, 0], xy[:, 1], 'o', markerfacecolor=tuple(col),
             markeredgecolor='k', markersize=6)

plt.title('Estimated number of clusters: %d' % n_clusters_)
plt.show()

"," High Level 
There are two major options here: 

Transform your data so that k-means-style clustering algorithms succeed
Pick a different algorithm

Minor option:

Tweak kmeans by forcing the initialization to be smarter


 Option 2 
Python has a good description of several clustering algorithms here . From the link, a (crudely cropped) helpful graphic:

This row looks similar to your dataset; have you tried a Gaussian mixture model? A GMM has few well known theoretical properties, but it works by assigning probabilities that points belong to each cluster center based on a posterior calculated from the data. You can often initialize it with kmeans, which Sklearn does for you.
Similarly, desnity-based clustering algorithms (DBSCAN, e.g.), seem like a logical choice. Your data has a nice segmentation of dense clusters, and this seems like a good topological property to be filtering for. In the image on the linked wikipedia page:

they offer the caption:

DBSCAN can find non-linearly separable clusters. This dataset cannot
  be adequately clustered with k-means

which seems to speak to your troubles.

 More on your troubles 
Kmeans is an extremely versatile algorithm, but it is not globally optimal and suffers from a lot of weak-points. Here is dense reading
In addition to problems like the mickey mouse problem, kmeans is often trying to minimize simple euclidean distance to the centroids. While this makes a lot of sense for a lot of problems, it doesn't make sense in yours, where the skew of the clusters means that isn't quite the correct measure. Notice that other algorithms like agglomerative/hierarchical clustering, shown above, that use similar measures, have similar trappings.
I haven't covered transforming your data or tweaking kmeans because the latter requires actually hacking into (or writing your own) clustering algorithm (I don't recommend for a simple exploratory problem given the coverage of sklearn and similar packages), where the former seems like a local solution sensitive to your exact data. ICA might be a decent start, but there's a lot of options for that task
",scikit-learn
sklearn pipeline  how to apply different transformations on different columns,"I have a dataset that has a mixture of text and numbers i.e. certain columns have text only and rest have integers (or floating point numbers).
I was wondering if it was possible to build a pipeline where I can for example call LabelEncoder() on the text features and MinMaxScaler() on the numbers columns. The examples I have seen on the web mostly point towards using LabelEncoder() on the entire dataset and not on select columns. Is this possible? If so any pointers would be greatly appreciated.
","The way I usually do it is with a FeatureUnion, using a FunctionTransformer to pull out the relevant columns. 
Important notes:

You have to define your functions with def since annoyingly you can't use lambda or partial in FunctionTransformer if you want to pickle your model
You need to initialize FunctionTransformer with validate=False

Something like this:
from sklearn.pipeline import make_union, make_pipeline
from sklearn.preprocessing import FunctionTransformer

def get_text_cols(df):
    return df[['name', 'fruit']]

def get_num_cols(df):
    return df[['height','age']]

vec = make_union(*[
    make_pipeline(FunctionTransformer(get_text_cols, validate=False), LabelEncoder()))),
    make_pipeline(FunctionTransformer(get_num_cols, validate=False), MinMaxScaler())))
])

",scikit-learn
how do i apply one hot encoding on a pandas dataframe with both categorical and numerical features,"Some features are numerical such as ""graduation rate from school"", while other features are categorical like the name of the school. I used a label encoder on the features that are categorical to transform them into integers.
I now have a dataframe with both floats and integers, representing numerical features and categorical features(transformed with label encoder) respectively.
I am unsure how to proceed with a learner, do I need to use one hot encoding? And if so, how can I do so? I cannot simply pass the dataframe to the sklearn OneHotEncoder since there are floats, according to my current understanding. Do I just apply the label encoder to all features to solve the issue?
Sample data from my dataframe. OPEID and opeid6 were transformed using a label encoder
","Just use the OneHotEncoder categorical_features argument to select with features are categorical:

categorical_features: “all” or array of indices or mask :
Specify what features are treated as categorical.

‘all’ (default): All features are treated as categorical.
array of indices: Array of categorical feature indices.
mask: Array of length n_features and with dtype=bool.
Non-categorical features are always stacked to the right of the matrix.


",scikit-learn
valueerror could not convert string to float 39n39,"I am working on a Machine Learning project that predicts the price electric cars on Jupyter Notebook.
I run this cell on Jupyter Notebook:
p = regressor.predict(df2)

I get this error:
ValueError                                Traceback (most recent call last)
~\AppData\Local\Temp\ipykernel_16424\818753220.py in <module>
----> 1 p = regressor.predict(df2)

~\.conda\envs\electricvehiclepriceprediction\lib\site-packages\sklearn\tree\_classes.py in predict(self, X, check_input)
    465         """"""
    466         check_is_fitted(self)
--> 467         X = self._validate_X_predict(X, check_input)
    468         proba = self.tree_.predict(X)
    469         n_samples = X.shape[0]

~\.conda\envs\electricvehiclepriceprediction\lib\site-packages\sklearn\tree\_classes.py in _validate_X_predict(self, X, check_input)
    431         """"""Validate the training data on predict (probabilities).""""""
    432         if check_input:
--> 433             X = self._validate_data(X, dtype=DTYPE, accept_sparse=""csr"", reset=False)
    434             if issparse(X) and (
    435                 X.indices.dtype != np.intc or X.indptr.dtype != np.intc

~\.conda\envs\electricvehiclepriceprediction\lib\site-packages\sklearn\base.py in _validate_data(self, X, y, reset, validate_separately, **check_params)
    564             raise ValueError(""Validation should be done on X, y or both."")
    565         elif not no_val_X and no_val_y:
--> 566             X = check_array(X, **check_params)
    567             out = X
    568         elif no_val_X and not no_val_y:

~\.conda\envs\electricvehiclepriceprediction\lib\site-packages\sklearn\utils\validation.py in check_array(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator)
    744                     array = array.astype(dtype, casting=""unsafe"", copy=False)
    745                 else:
--> 746                     array = np.asarray(array, order=order, dtype=dtype)
    747             except ComplexWarning as complex_warning:
    748                 raise ValueError(

~\.conda\envs\electricvehiclepriceprediction\lib\site-packages\pandas\core\generic.py in __array__(self, dtype)
   1991 
   1992     def __array__(self, dtype: NpDtype | None = None) -> np.ndarray:
-> 1993         return np.asarray(self._values, dtype=dtype)
   1994 
   1995     def __array_wrap__(

ValueError: could not convert string to float: 'N/'

What did I try?
I tried using the following code:
uv = np.nanpercentile(df2['Base MSRP'], [99])[0]*2
df2['Base MSRP'][(df2['Base MSRP']>uv)] = uv
le = preprocessing.LabelEncoder()
cols = ['County', 'City', 'State', 'ZIP Code', 'Model Year', 'Make', 'Model', 'Electric Vehicle Type', 'Clean Alternative Fuel Vehicle (CAFV) Eligibility']
for col in cols:
    le.fit(t[col])
    df2[col] = le.transform(df2[col]) 
    print(le.classes_)
regressor.fit(x, y)
p = regressor.predict(df2)

Here is the link to my notebook: https://github.com/SteveAustin583/electric-vehicle-price-prediction-revengers/blob/main/revengers.ipynb
How to fix this issue?
","I have found a solution. I have replaced ""N"" with ""-1"" by using this code:
df2 = df2[df2['Model Year'] != 'N/']  # Filter out rows where 'Model Year' is 'N/'

for col in cols:
    df2[col] = df2[col].replace('N/', -1)
    le.fit(df2[col])
    df2[col] = le.transform(df2[col]) 
    print(le.classes_)

",scikit-learn
skfolio how to set various parameters,"I have started using skfolio using a portfolio given in skfolio.datasets (sp500). My idea is to run a portfolio of equally weighted names using the sp500 as my universe. For this I do the following:
import skfolio
from skfolio.datasets import load_sp500_dataset
from skfolio.preprocessing import prices_to_returns

prices=load_sp500_dataset()
X=prices_to_returns(prices)
X_train, X_test = train_test_split(X, test_size=0.33, shuffle=False)

model = skfolio.optimization.naive.EqualWeighted()

model.fit(X_train)

portfolio = model.predict(X_test)

print(portfolio.annualized_sharpe_ratio)
print(portfolio.summary())

So here I am using the data from the SP500 index and apply an equally weighted portfolio model. It returns various statistics on the portfolio (Skew, Kurtosis,...) and I can plot the cumulative returns with
portfolio.plot_cumulative_returns()

I have a few questions/issues with the above as it seems a lot of parameters have been defined ""by default"". I believe they could all relate to some kind of set_parameters function (?). In particular:

the number of assets to be considered in my portfolio (from what I can tell the assets number is set to 20). How can I change that number?
I feel like the optimizer is rebalancing the portfolio to equal weights on a daily basis (?). How can I change the frequency of the rebalancing (eg say weekly, monthly, quarterly, yearly)?
Is there a way to set the period to run the portfolio? Currently it starts somewhere in 2012 (can this be modified to 2002 or anything else?)
the plot regarding the cumulative returns is about non-compounded returns. Is there a way to get the compounded returns instead?

","I’m one of the core developers of Skfolio.
Answer to Points 1 and 3
Your number of assets and start period are linked to your input X, which is your time series of asset returns.
In your example, you use the EqualWeighted allocation model. This model allocates a weight of 1/20 to the 20 assets from your input returns X. To change it, you need to modify your input by removing columns (assets) or rows (dates).
When using other models, such as MeanRisk, since these models support weight and cardinality constraints, you can control the number of invested assets with the model parameters: MeanRisk Cardinality Constraints.
Another option is through pre-selection transformers: Pre-Selection Transformers.

Answer to Point 2
To use compounded returns for your portfolio evaluation (e.g., plots, risk stats, etc.), you need to set compounded=True:
from skfolio.optimization import EqualWeighted
from skfolio.datasets import load_sp500_dataset
from skfolio.preprocessing import prices_to_returns

prices = load_sp500_dataset()
X = prices_to_returns(prices)

# Without compounded returns
model = EqualWeighted()
ptf = model.fit_predict(X)
ptf.cumulative_returns_df.tail()

# With compounded returns
model = EqualWeighted(portfolio_params=dict(compounded=True))
ptf = model.fit_predict(X)
ptf.cumulative_returns_df.tail()


Answer to Point 4
In terms of rebalancing, there are two different concepts that are sometimes both called “rebalancing”:

Model Re-Fitting
This changes the target weights, for example, re-fitting an MVO (Mean-Variance Optimization) model every month.
Model re-fittings are performed using cross-validators: Cross-Validators Example.

Rebalancing Towards Target Weights
This refers to performing rebalancing between two re-fittings to avoid weight drift as asset prices diverge.



The second type of rebalancing happens at the same frequency as your input data, so daily in your example.
The reason is that portfolio models and cross-validators in Skfolio are used for model validation, selection, and parameter tuning. Therefore, the model evaluation must be homogeneous to the optimization process. For example, if you call model.fit_predict(X) with a MeanRisk model that has a 10% volatility constraint, your predicted portfolio on the same training set X must also have a 10% volatility. Otherwise, you will create issues later in terms of model selection, evaluation, and comparison.
Mathematically, this means that in between re-fittings, portfolio returns are computed as the dot product of returns and weighs.
More details are available here: Portfolio Models.

Additional Note
If you need more granularity to model weight drifts between re-fittings, Skfolio can be used with a backtesting package.

Let me know if you have any other questions!
",scikit-learn
shortest syntax to use numpy 1darray as sklearn x,"I often have two numpy 1d arrays, x and y, and would like to perform some quick sklearn fitting + prediction using them.
 import numpy as np
 from sklearn import linear_model

 # This is an example for the 1d aspect - it's obtained from something else.
 x = np.array([1, 3, 2, ...]) 
 y = np.array([12, 32, 4, ...])

Now I'd like to do something like
 linear_model.LinearRegression().fit(x, y)...

The problem is that it expects an X which is a 2d column array. For this reason, I usually feed it 
 x.reshape((len(x), 1))

which I find cumbersome and hard to read. 
Is there some shorter way to transform a 1d array to a 2d column array (or, alternatively, get sklearn to accept 1d arrays)?
","You can slice your array, creating a newaxis:
x[:, None]

This:
>>> x = np.arange(5)
>>> x[:, None]
array([[0],
       [1],
       [2],
       [3],
       [4]])

Is equivalent to:
>>> x.reshape(len(x), 1)
array([[0],
       [1],
       [2],
       [3],
       [4]])

If you find it more readable, you can use a transposed matrix:
np.matrix(x).T

If you want an array:
np.matrix(x).T.A

",scikit-learn
notfittederror this decisiontreeclassifier instance is not fitted yet,"Am trying to run a decision tree based model. I tried the below:
X = df[['Quantity']]
y = df[['label']]
params = {'max_depth':[2,3,4], 'min_samples_split':[2,3,5,10]}
clf_dt = DecisionTreeClassifier()
clf = GridSearchCV(clf_dt, param_grid=params, scoring='f1')
clf.fit(X, y)
clf_dt = DecisionTreeClassifier(clf.best_params_)

And got the warning mentioned here
FutureWarning: Pass criterion={'max_depth': 2, 'min_samples_split': 2} as keyword args. From version 1.0 (renaming of 0.25) passing these as positional arguments will result in an error
  warnings.warn(f""Pass {args_msg} as keyword args. From version ""

Later, I tried running the below and got an error (but I already fit the model using .fit())
from sklearn import tree
tree.plot_tree(clf_dt, filled=True, feature_names = list(X.columns), class_names=['Iris-setosa', 'Iris-versicolor', 'Iris-virginica'])

NotFittedError: This DecisionTreeClassifier instance is not fitted yet. Call 
'fit' with appropriate arguments before using this estimator.

How can I fix this?
","If you go with best_params_, you'll have to refit the model with those parameters. Note that these should be unpacked when passed to the model:
clf_dt = DecisionTreeClassifier(**clf.best_params_)
clf_dt.fit(X, y)

However, you can also use the best_estimator_ attribute in order to access the best model directly:
clf_dt = clf.best_estimator_

",scikit-learn
dimensionality reduction  pca explanation,"I don't think I have a good understanding of PCA, can someone help with my confusion below please?
Take iris dataset as an example, I have 4 covariates, x1:sepal length; x2:sepal width; x3:petal length; x4:petal width. And the formula can be seen below, a1,a2,a3,a4 are the weightings for the covariates. And PCA will try to maximise the variance using different linear transformations. While also follows the rule of a1^2 + a2^2 + a3^2 + a4^2=1. I'm interested in knowing the value of a1,a2,a3,a4.
a1*x1 + a2*x2 + a3*x3 + a4*x4

I have below code on python which I think is correct?
# load libraries
from sklearn.datasets import load_iris
from sklearn.decomposition import PCA
import seaborn as sns
import pandas as pd
import numpy as np

iris = load_iris()
X = iris.data
df = pd.DataFrame(X,columns=iris.feature_names)

pca = decomposition.PCA(n_components = 4)
digits_pca_4 = pca.fit(X)
digits_pca_4.explained_variance_ratio_

And the result is
array([0.92461872, 0.05306648, 0.01710261, 0.00521218])

My question is:
Am I correct to assume that a1=sqrt(0.92), a2=sqrt(0.05), a3=sqrt(0.02), a4=sqrt(0.005)?
Second question:
And if I were to choose the linear combination of a1=a2=a3=a4=0.5, what's the variance of this compared to the variance from the PCA (I'm assuming it's less than the PCA result since PCA maximise the variance?)? How can I get the variance of when a1=a2=a3=a4=0.5 in python? And is the variance from PCA the code below?
pca.explained_variance_.sum()

","To answer directly your question: no, your initial interpretation is not correct.
Explanation
The actual projection done by PCA is a matrix multiplication Y = (X - u) W where u is the mean of X (u = X.mean(axis=0)) and W is the projection matrix found by PCA: a n x p orthonormal matrix, where n is the original data dimension and p the desired output dimensions. The expression you give (a1*x1 + a2*x2 + a3*x3 + a4*x4) does not mean anything with all values being scalars. At best, it could mean the calculation of a single component, using one column j of W below as the a_k: Y[i, j] == sum(W[k, j] * (X[i, k] - u[k]) for k in range(n)).
In any case, you can inspect all the variables of the result of pca = PCA.fit(...) with vars(pca). In particular, the projection matrix described above can be found as W = pca.components_.T. The following statements can be verified:
# projection
>>> u = pca.mean_
... W = pca.components_.T
... Y = (X - u).dot(W)
... np.allclose(Y, pca.transform(X))
True

>>> np.allclose(X.mean(axis=0), u)
True

# orthonormality
>>> np.allclose(W.T.dot(W), np.eye(W.shape[1]))
True

# explained variance is the sample variation (not population variance)
# of the projection (i.e. the variance along the proj axes)
>>> np.allclose(Y.var(axis=0, ddof=1), pca. explained_variance_)
True

Graphical demo
The simplest way to understand PCA is that it is purely a rotation in n-D (after mean removal) while retaining only the first p-dimensions. The rotation is such that your data's directions of largest variance become aligned with the natural axes in the projection.
Here is a little demo code to help you visualize what's going on. Please also read the Wikipedia page on PCA.
def pca_plot(V, W, idx, ax):
    # plot only first 2 dimensions of W along with axes W
    colors = ['k', 'r', 'b', 'g', 'c', 'm', 'y']
    u = V.mean(axis=0)  # n
    axes_lengths = 1.5*(V - u).dot(W).std(axis=0)
    axes = W * axes_lengths  # n x p
    axes = axes[:2].T  # p x 2
    ax.set_aspect('equal')
    ax.scatter(V[:, 0], V[:, 1], alpha=.2)
    ax.scatter(V[idx, 0], V[idx, 1], color='r')
    hlen = np.max(np.linalg.norm((V - u)[:, :2], axis=1)) / 25
    for k in range(axes.shape[0]):
        ax.arrow(*u[:2], *axes[k], head_width=hlen/2, head_length=hlen, fc=colors[k], ec=colors[k])

def pca_demo(X, p):
    n = X.shape[1]  # input dimension
    pca = PCA(n_components=p).fit(X)
    u = pca.mean_
    v = pca.explained_variance_
    W = pca.components_.T
    Y = pca.transform(X)
    assert np.allclose((X - u).dot(W), Y)
    
    # plot first 2D of both input space and output space
    # for visual identification: select a point that's as far as possible
    # in the direction of the diagonal of the axes cube, after normalization
    # Z: variance-1 projection
    Z = (X - u).dot(W/np.sqrt(v))
    idx = np.argmax(Z.sum(axis=1) / np.sqrt(np.linalg.norm(Z, axis=1)))

    fig, ax = plt.subplots(ncols=2, figsize=(12, 6))

    # input space
    pca_plot(X, W, idx, ax[0])
    ax[0].set_title('input data (first 2D)')

    # output space
    pca_plot(Y, np.eye(p), idx, ax[1])
    ax[1].set_title('projection (first 2D)')
    
    return Y, W, u, pca

Examples
Iris data
# to better understand the shape of W, we project onto
# a space of dimension p=3
X = load_iris().data
Y, W, u, pca = pca_demo(X, 3)


Note that the projection is really just (X - u) W:
>>> np.allclose((X - u).dot(W), Y)
True

Synthetic ellipsoid data
A = np.array([
    [20, 10, 7],
    [-1, 3, 7],
    [5, 1, 2],
])
X = np.random.normal(size=(1000, A.shape[0])).dot(A)
Y, W, u, pca = pca_demo(X, 3)


",scikit-learn
unable to run autosklearn on colab,"I am trying to run auto-sklearn on Google colab. I have successfully installed AutoSklearn but it gives me error while running following code:
Input:
import autosklearn.classfication

Output:
    ---------------------------------------------------------------------------
IncorrectPackageVersionError              Traceback (most recent call last)
<ipython-input-39-bb85952a9dd8> in <module>()
----> 1 import autosklearn.classification

2 frames
/usr/local/lib/python3.6/dist-packages/autosklearn/util/dependencies.py in _verify_package(name, operation, version)
     60     if not check:
     61         raise IncorrectPackageVersionError(name, installed_version, operation,
---> 62                                            required_version)
     63 
     64 

IncorrectPackageVersionError: found 'distributed' version 1.25.3 but requires distributed version >=2.2.0

","This might be the issue with the current version of auto sklearn , if you are ok with a older version then 0.10.0 works perfectly with colab
!pip install auto-sklearn==0.10.0

You might need additional commands
as shown in this answer
Reiterating the commands here for version 0.10.0
!sudo apt-get install build-essential swig 
!curl https://raw.githubusercontent.com/automl/auto-sklearn/master/requirements.txt | xargs -n 1 -L 1 pip install 
!pip install auto-sklearn==0.10.0

Colab might ask you to restart itself , after which the autosklearn import would work
",scikit-learn
plotting the roc curve for a multiclass problem,"I am trying to apply the idea of sklearn ROC extension to multiclass to my dataset. My per-class ROC curve looks find of a straight line each, unline the sklearn's example showing curve's fluctuating.
I give an MWE below to show what I mean:
# all imports
import numpy as np
import matplotlib.pyplot as plt
from itertools import cycle
from sklearn import svm, datasets
from sklearn.metrics import roc_curve, auc
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import label_binarize
from sklearn.datasets import  make_classification
from sklearn.ensemble import RandomForestClassifier
# dummy dataset
X, y = make_classification(10000, n_classes=5, n_informative=10, weights=[.04, .4, .12, .5, .04])
train, test, ytrain, ytest = train_test_split(X, y, test_size=.3, random_state=42)

# random forest model
model = RandomForestClassifier()
model.fit(train, ytrain)
yhat = model.predict(test)

The following function then plots the ROC curve:
def plot_roc_curve(y_test, y_pred):
  
  n_classes = len(np.unique(y_test))
  y_test = label_binarize(y_test, classes=np.arange(n_classes))
  y_pred = label_binarize(y_pred, classes=np.arange(n_classes))

  # Compute ROC curve and ROC area for each class
  fpr = dict()
  tpr = dict()
  roc_auc = dict()
  for i in range(n_classes):
    fpr[i], tpr[i], _ = roc_curve(y_test[:, i], y_pred[:, i])
    roc_auc[i] = auc(fpr[i], tpr[i])
  
  # Compute micro-average ROC curve and ROC area
  fpr[""micro""], tpr[""micro""], _ = roc_curve(y_test.ravel(), y_pred.ravel())
  roc_auc[""micro""] = auc(fpr[""micro""], tpr[""micro""])

  # First aggregate all false positive rates
  all_fpr = np.unique(np.concatenate([fpr[i] for i in range(n_classes)]))

  # Then interpolate all ROC curves at this points
  mean_tpr = np.zeros_like(all_fpr)
  for i in range(n_classes):
    mean_tpr += np.interp(all_fpr, fpr[i], tpr[i])

  # Finally average it and compute AUC
  mean_tpr /= n_classes

  fpr[""macro""] = all_fpr
  tpr[""macro""] = mean_tpr
  roc_auc[""macro""] = auc(fpr[""macro""], tpr[""macro""])

  # Plot all ROC curves
  #plt.figure(figsize=(10,5))
  plt.figure(dpi=600)
  lw = 2
  plt.plot(fpr[""micro""], tpr[""micro""],
    label=""micro-average ROC curve (area = {0:0.2f})"".format(roc_auc[""micro""]),
    color=""deeppink"", linestyle="":"", linewidth=4,)

  plt.plot(fpr[""macro""], tpr[""macro""],
    label=""macro-average ROC curve (area = {0:0.2f})"".format(roc_auc[""macro""]),
    color=""navy"", linestyle="":"", linewidth=4,)

  colors = cycle([""aqua"", ""darkorange"", ""darkgreen"", ""yellow"", ""blue""])
  for i, color in zip(range(n_classes), colors):
    plt.plot(fpr[i], tpr[i], color=color, lw=lw,
        label=""ROC curve of class {0} (area = {1:0.2f})"".format(i, roc_auc[i]),)

  plt.plot([0, 1], [0, 1], ""k--"", lw=lw)
  plt.xlim([0.0, 1.0])
  plt.ylim([0.0, 1.05])
  plt.xlabel(""False Positive Rate"")
  plt.ylabel(""True Positive Rate"")
  plt.title(""Receiver Operating Characteristic (ROC) curve"")
  plt.legend()

Output:
plot_roc_curve(ytest, yhat)


Kind of straight line bending once. I would like to see the model performance at different thresholds, not just one, a figure similar to sklearn's illustration for 3-classes shown below:

","
Point is that you're using predict() rather than predict_proba()/decision_function() to define your y_hat. This means - considering that the threshold vector is defined by the number of distinct values in y_hat (see here for reference), that you'll have few thresholds per class only on which tpr and fpr are computed (which in turn implies that your curves are evaluated at few points only).

Indeed, consider what the doc says to pass to y_scores in roc_curve(), either prob estimates or decision values. In the example from sklearn, decision values are used to compute the scores. Given that you're considering a RandomForestClassifier(), considering probability estimates in your y_hat should be the way to go.

What's the point then of label-binarizing the output? The standard definition for ROC is in terms of binary classification. To pass to a multiclass problem, you have to convert your problem into binary by using OneVsAll approach, so that you'll have n_class number of ROC curves. (Observe, indeed, that as SVC() handles multiclass problems in a OvO fashion by default, in the example they had to force to use OvA by applying OneVsRestClassifier constructor; with a RandomForestClassifier you don't have such problem as that's inherently multiclass, see here for reference). In these terms, once you switch to predict_proba() you'll see there's no much sense in label binarizing predictions.
 # all imports
 import numpy as np
 import matplotlib.pyplot as plt
 from itertools import cycle
 from sklearn import svm, datasets
 from sklearn.metrics import roc_curve, auc
 from sklearn.model_selection import train_test_split
 from sklearn.preprocessing import label_binarize
 from sklearn.datasets import  make_classification
 from sklearn.ensemble import RandomForestClassifier
 # dummy dataset
 X, y = make_classification(10000, n_classes=5, n_informative=10, weights=[.04, .4, .12, .5, .04])
 train, test, ytrain, ytest = train_test_split(X, y, test_size=.3, random_state=42)

 # random forest model
 model = RandomForestClassifier()
 model.fit(train, ytrain)
 yhat = model.predict_proba(test)

 def plot_roc_curve(y_test, y_pred):
     n_classes = len(np.unique(y_test))
     y_test = label_binarize(y_test, classes=np.arange(n_classes))

     # Compute ROC curve and ROC area for each class
     fpr = dict()
     tpr = dict()
     roc_auc = dict()
     thresholds = dict()
     for i in range(n_classes):
       fpr[i], tpr[i], thresholds[i] = roc_curve(y_test[:, i], y_pred[:, i], drop_intermediate=False)
     roc_auc[i] = auc(fpr[i], tpr[i])

     # Compute micro-average ROC curve and ROC area
     fpr[""micro""], tpr[""micro""], _ = roc_curve(y_test.ravel(), y_pred.ravel())
     roc_auc[""micro""] = auc(fpr[""micro""], tpr[""micro""])

     # First aggregate all false positive rates
     all_fpr = np.unique(np.concatenate([fpr[i] for i in range(n_classes)]))

     # Then interpolate all ROC curves at this points
     mean_tpr = np.zeros_like(all_fpr)
     for i in range(n_classes):
       mean_tpr += np.interp(all_fpr, fpr[i], tpr[i])

     # Finally average it and compute AUC
     mean_tpr /= n_classes

     fpr[""macro""] = all_fpr
     tpr[""macro""] = mean_tpr
     roc_auc[""macro""] = auc(fpr[""macro""], tpr[""macro""])

     # Plot all ROC curves
     #plt.figure(figsize=(10,5))
     plt.figure(dpi=600)
     lw = 2
     plt.plot(fpr[""micro""], tpr[""micro""],
     label=""micro-average ROC curve (area = {0:0.2f})"".format(roc_auc[""micro""]),
     color=""deeppink"", linestyle="":"", linewidth=4,)

     plt.plot(fpr[""macro""], tpr[""macro""],
     label=""macro-average ROC curve (area = {0:0.2f})"".format(roc_auc[""macro""]),
     color=""navy"", linestyle="":"", linewidth=4,)

     colors = cycle([""aqua"", ""darkorange"", ""darkgreen"", ""yellow"", ""blue""])
     for i, color in zip(range(n_classes), colors):
       plt.plot(fpr[i], tpr[i], color=color, lw=lw,
       label=""ROC curve of class {0} (area = {1:0.2f})"".format(i, roc_auc[i]),)

     plt.plot([0, 1], [0, 1], ""k--"", lw=lw)
     plt.xlim([0.0, 1.0])
     plt.ylim([0.0, 1.05])
     plt.xlabel(""False Positive Rate"")
     plt.ylabel(""True Positive Rate"")
     plt.title(""Receiver Operating Characteristic (ROC) curve"")
     plt.legend()



Eventually, consider that roc_curve() has also a drop_intermediate parameter meant for dropping suboptimal thresholds (it might be useful to know).
",scikit-learn
sklearn mutual_info_classif returns different values depending on feature order,"I noticed behavior in the sklearn mutual_info_classif function that is inconsistent with what I  expect in the mutual information objective.
Given a set of columns ['A', 'B', 'C'] and a dependent variable y, the mutual information computed could be between all features and y (a single scalar) or a single feature and y (list of scalars). Based on this, I'm not sure what values sklearn is returning to me because the values change with the ordering of the features and it changes with the number of features inputted.

The mutual information value (between a feature and the dependent variable) given by sklearn changes with the ordering of the columns.
For example, the following queries give different outputs.

feature_scores = mutual_info_classif(X[['A', 'B', 'C']], y, random_state=0)
feature_scores

array([0.        , 0.13, 0.045])

feature_scores = mutual_info_classif(X[['A', 'C', 'B']], y, random_state=0)
feature_scores

array([0.        , 0.017, 0.14]


Another unexpected behavior is that changing the set of features changes the mutual information value.

feature_scores = mutual_info_classif(X[['A', 'B']], y, random_state=0)
feature_scores

array([0.        , 0.14])

feature_scores = mutual_info_classif(X[['A', 'B', 'C']], y, random_state=0)
feature_scores

array([0.        , 0.13, 0.045])

Can anyone explain this behavior to me and why this is correct?
","It's likely that mutual_info_classif uses a single random number generator, which means that changing the amount and order of data to process changes the way random values are applied to calculating the mutual information.
To avoid this, you can just calculate MI for each column one at a time.
",scikit-learn
how to show loss values during training in scikitlearn,"I want check my loss values during the training time so I can observe the loss at each iteration. So far I haven't found an easy way for scikit learn to give me a history of loss values, nor did I find a functionality already within scikit to plot the loss for me.
If there was no way to plot this, it'd be great if I could simply fetch the final loss values at the end of classifier.fit.
Note: I am aware of the fact that some solutions are closed form. I'm using several classifiers which do not have analytical solutions, such as logistic regression and svm.
Does anyone have any suggestions? 
","So I couldn't find very good documentation on directly fetching the loss values per iteration, but I hope this will help someone in the future:
old_stdout = sys.stdout
sys.stdout = mystdout = StringIO()
clf = SGDClassifier(**kwargs, verbose=1)
clf.fit(X_tr, y_tr)
sys.stdout = old_stdout
loss_history = mystdout.getvalue()
loss_list = []
for line in loss_history.split('\n'):
    if(len(line.split(""loss: "")) == 1):
        continue
    loss_list.append(float(line.split(""loss: "")[-1]))
plt.figure()
plt.plot(np.arange(len(loss_list)), loss_list)
plt.savefig(""warmstart_plots/pure_SGD:""+str(kwargs)+"".png"")
plt.xlabel(""Time in epochs"")
plt.ylabel(""Loss"")
plt.close()

This code will take a normal SGDClassifier(just about any linear classifier), and intercept the verbose=1 flag, and will then split to get the loss from the verbose printing. Obviously this is slower but will give us the loss and print it.
",scikit-learn
discrepancy between sklearn39s r2_score and pytorch39s mseloss,"I am not sure if I am missing something very basic but I have started to notice some slight discrepancies between R2 scores returned by sklearn's r2_score() function and R2 scores calculated from PyTorch's MSELoss() (with the additional help of statistics.variance()).
The R2 score returned by the sklearn method is consistently (slightly) lower than the one returned via MSELoss().
Here is some basic code to reproduce the difference:
from sklearn.metrics import r2_score
from torch.nn import MSELoss
import statistics 
import random
import torch
import numpy as np 

actuals = random.sample(range(1, 50), 40)

preds = []

for value in actuals:
    pred = value * 0.70
    preds.append(pred)

loss = MSELoss()

mse = loss(torch.tensor(preds), torch.tensor(actuals))

r2 = 1 - mse / statistics.variance(actuals)

score = r2_score(actuals, preds)

print(f'R2 Score using (PyTorch) MSELoss: {r2}')
print(f'R2 Score using (sklearn) r2_score: {score}')

Example output:
R2 Score using (PyTorch) MSELoss: 0.6261289715766907
R2 Score using (sklearn) r2_score: 0.6165425269729996

I figured this could be related to the fact that MSELoss() takes tensors as input (but sklearn doesn't) but I don't really know why or how.
Versions:

PyTorch == 2.1.0
scikit-learn == 1.4.2
Python == 3.9.18

","This is due to Bessel's correction. You can achieve the same results using pvariance instead of variance.
from sklearn.metrics import r2_score
from torch.nn import MSELoss
import statistics 
import random
import torch
import numpy as np 

actuals = random.sample(range(1, 50), 40)

preds = []

for value in actuals:
    pred = value * 0.70
    preds.append(pred)

loss = MSELoss()

mse = loss(torch.tensor(preds), torch.tensor(actuals))

r2_sample = 1 - mse / statistics.variance(actuals)
r2_population = 1 - mse / statistics.pvariance(actuals)

score = r2_score(actuals, preds)

print(f'R2 Score using (PyTorch) MSELoss (sample): {r2_sample}')
print(f'R2 Score using (PyTorch) MSELoss (population): {r2_population}')
print(f'R2 Score using (sklearn) r2_score: {score}')

Output:
> R2 Score using (PyTorch) MSELoss (sample): 0.6582530736923218
> R2 Score using (PyTorch) MSELoss (population): 0.6494903564453125
> R2 Score using (sklearn) r2_score: 0.6494903644913157

",scikit-learn
using scikitlearn39s mlpclassifier in adaboostclassifier,"For a binary classification problem I want to use the MLPClassifier as the base estimator in the AdaBoostClassifier. However, this does not work because MLPClassifier does not implement sample_weight, which is required for AdaBoostClassifier (see here). Before that, I tried using a Keras model and the KerasClassifier within AdaBoostClassifier but that did also not work as mentioned here . 
A way, which is proposed by User V1nc3nt is to build an own MLPclassifier in TensorFlow and take into account the sample_weight.
User V1nc3nt shared large parts of his code but since I have only limited experience with Tensorflow, I am not able to fill in the missing parts. Hence, I was wondering if anyone has found a working solution for building Adaboost ensembles from MLPs or can help me out in completing the solution proposed by V1nc3nt.
Thank you very much in advance!
","Based on the references, which you had mentioned, I have modified MLPClassifier to accommodate sample_weights.
Try this!
from sklearn.neural_network import MLPClassifier
from sklearn.datasets import load_iris
from sklearn.ensemble import AdaBoostClassifier
import numpy as np

class customMLPClassifer(MLPClassifier):
    def resample_with_replacement(self, X_train, y_train, sample_weight):

        # normalize sample_weights if not already
        sample_weight = sample_weight / sample_weight.sum(dtype=np.float64)

        X_train_resampled = np.zeros((len(X_train), len(X_train[0])), dtype=np.float32)
        y_train_resampled = np.zeros((len(y_train)), dtype=int)
        for i in range(len(X_train)):
            # draw a number from 0 to len(X_train)-1
            draw = np.random.choice(np.arange(len(X_train)), p=sample_weight)

            # place the X and y at the drawn number into the resampled X and y
            X_train_resampled[i] = X_train[draw]
            y_train_resampled[i] = y_train[draw]

        return X_train_resampled, y_train_resampled


    def fit(self, X, y, sample_weight=None):
        if sample_weight is not None:
            X, y = self.resample_with_replacement(X, y, sample_weight)
        
        return self._fit(X, y, incremental=(self.warm_start and
                                            hasattr(self, ""classes_"")))


X, y = load_iris(return_X_y=True)
adabooster = AdaBoostClassifier(base_estimator=customMLPClassifer())

adabooster.fit(X, y)

",scikit-learn
sklearn multiclass problem and reporting sensitivity and specificity,"I have a three-class problem and I'm able to report precision and recall for each class with the below code:
from sklearn.metrics import classification_report
print(classification_report(y_test, y_pred))

which gives me the precision and recall nicely for each of the 3 classes in a table format.
My question is how can I now get sensitivity and specificity for each of the 3 classes? I looked at sklearn.metrics and I didn't find anything for reporting sensitivity and specificity.
","If we check the help page for classification report:

Note that in binary classification, recall of the positive class is
also known as “sensitivity”; recall of the negative class is
“specificity”.

So we can convert the pred into a binary for every class, and then use the recall results from precision_recall_fscore_support.
Using an example:
from sklearn.metrics import classification_report
y_true = [0, 1, 2, 2, 2]
y_pred = [0, 0, 2, 2, 1]
target_names = ['class 0', 'class 1', 'class 2']
print(classification_report(y_true, y_pred, target_names=target_names))

Looks like:
              precision    recall  f1-score   support

     class 0       0.50      1.00      0.67         1
     class 1       0.00      0.00      0.00         1
     class 2       1.00      0.67      0.80         3

    accuracy                           0.60         5
   macro avg       0.50      0.56      0.49         5
weighted avg       0.70      0.60      0.61         5

Using sklearn:
from sklearn.metrics import precision_recall_fscore_support
res = []
for l in [0,1,2]:
    prec,recall,_,_ = precision_recall_fscore_support(np.array(y_true)==l,
                                                      np.array(y_pred)==l,
                                                      pos_label=True,average=None)
    res.append([l,recall[0],recall[1]])

put the results into a dataframe:
pd.DataFrame(res,columns = ['class','sensitivity','specificity'])

    class   sensitivity specificity
0   0   0.75    1.000000
1   1   0.75    0.000000
2   2   1.00    0.666667

",scikit-learn
sklearn builtin function for hard margin svm,"I know that there is a built-in function for soft margin SVM as follows.
from sklearn.svm import SVC
clf = SVC(C=1, kernel = 'linear')
clf.fit(X, y)

But for Hard margin SVM, we need C=0, right? But the code report error ValueError: C <= 0 when I let C=0.
","There is no hard-margin SVM in scikit-learn, as it is not very useful model. Numercically you can get very close to it by just setting C=1e10, but it might lead to convergence issues, as in dual formualtion of SVM C is an upper bound on the lagrange multipliers.
",scikit-learn
is there a way to load a saved sklearn vectorstore using langchain,"I created and saved a vectorstore using langchain_community.vectorstores SKLearnVectorStore and I can't load it.
I created and saved vectorstore as below:
from langchain_community.vectorstores import SKLearnVectorStore

vectorstore = SKLearnVectorStore.from_texts(
    texts=doc_splits,
    embedding=OllamaEmbeddings(model=""Gemma-2:9b""),
    persist_path=""assets/vectorstore"",
)
vectorstore.persist()

I want to use this vectorstore in another file how do I use it?
","Save the Vectors
vectorstore = SKLearnVectorStore.from_documents(
        documents=doc_splits,
        persist_path=PERSIST_PATH,
        embedding=OllamaEmbeddings(model=""Gemma-2:9b""),
        serializer=""parquet"",
    )

vectorstore.persist()

Load the Saved parquet file
vectorstore = SKLearnVectorStore(
            persist_path=PERSIST_PATH,
            embedding=OllamaEmbeddings(model=""Gemma-2:9b""),
            serializer=""parquet""
        )
docs = vectorstore.similarity_search(query)

*Note: PERSIST_PATH is the path where you would like to save the file and load it.
Refer: https://python.langchain.com/docs/integrations/vectorstores/sklearn/*
",scikit-learn
how can i call scikitlearn classifiers from java,"I have a classifier that I trained using Python's scikit-learn. How can I use the classifier from a Java program? Can I use Jython? Is there some way to save the classifier in Python and load it in Java? Is there some other way to use it?
","You cannot use jython as scikit-learn heavily relies on numpy and scipy that have many compiled C and Fortran extensions hence cannot work in jython.
The easiest ways to use scikit-learn in a java environment would be to:

expose the classifier as a HTTP / Json service, for instance using a microframework such as flask or bottle or cornice and call it from java using an HTTP client library
write a commandline wrapper application in python that reads data on stdin and output predictions on stdout using some format such as CSV or JSON (or some lower level binary representation) and call the python program from java for instance using Apache Commons Exec.
make the python program output the raw numerical parameters learnt at fit time (typically as an array of floating point values) and reimplement the predict function in java (this is typically easy for predictive linear models where the prediction is often just a thresholded dot product). 

The last approach will be a lot more work if you need to re-implement feature extraction in Java as well.
Finally you can use a Java library such as Weka or Mahout that implement the algorithms you need instead of trying to use scikit-learn from Java.
",scikit-learn
download sklearn datasets behind a proxy,"I installed sklearn in my enviorment and running it now on jupyter notebook on windows.
How can I avoid the error:

URLError: urlopen error [Errno 11004] getaddrinfo failed

I am running the following code:
import sklearn
import sklearn.ensemble
import sklearn.metrics
from sklearn.datasets import fetch_20newsgroups
categories = ['alt.atheism', 'soc.religion.christian']
newsgroups_train = fetch_20newsgroups(subset='train', categories=categories)

which gives the error with line 5:

----> 3 newsgroups_train = fetch_20newsgroups(subset='train', categories=categories)

I am behind a proxy on my working computer, is there any option to avoid this error and to be able to use the sample datasets?
","According to source code, scikit-learn will download the file from:
https://ndownloader.figshare.com/files/5975967

I am assuming that you cannot reach this location from behind the proxy.
Can you access the dataset by some other means? If yes, then you can download it manually and then run the following script on it:
and keep it at the location:
~/scikit_learn_data/

Here ~ refers to the user home folder. You can use the following code to know the default location of that folder according to your system.
from sklearn.datasets import get_data_home
print(get_data_home())

Update: Once done, use the following script to make it in a form in which scikit-learn keeps its caches
import codecs, pickle, tarfile, shutil
from sklearn.datasets import load_files

data_folder = '~/scikit_learn_data/'
target_folder = data_folder+'20news_home/'

tarfile.open(data_folder+'20newsbydate.tar.gz', ""r:gz"").extractall(path=target_folder)
cache = dict(train=load_files(target_folder+'20news-bydate-train', encoding='latin1'),
             test=load_files(target_folder+'20news-bydate-test', encoding='latin1'))

compressed_content = codecs.encode(pickle.dumps(cache), 'zlib_codec')

with open(data_folder+'20news-bydate_py3.pkz', 'wb') as f:
    f.write(compressed_content)

shutil.rmtree(target_folder)

Scikit-learn will always check if the dataset exists locally before attempting to download from internet. For that it will check the above location.
After that you can run the import normally.
",scikit-learn
what is the best way to fit a quadratic polynomial to pdimensional data and compute its gradient and hessian matrix,"I have been trying to use the scikit-learn library to solve this problem.  Roughly:
from sklearn.preprocessing import PolynomialFeatures
from sklearn.linear_model import LinearRegression

# Make or load an n x p data matrix X and n x 1 array y of the corresponding
# function values.

poly = PolynomialFeatures(degree=2)
Xp = poly.fit_transform(X)
model = LinearRegression()
model.fit(Xp, y)

# Approximate the derivatives of the gradient and Hessian using the relevant
# finite-difference equations and model.predict.

As the above illustrates, sklearn makes the design choice to separate polynomial regression into PolynomialFeatures and LinearRegression rather than combine these into a single function.  This separation has conceptual advantages but also a major drawback:  it effectively prevents model from offering the methods gradient and hessian, and model would be significantly more useful if it did.
My current work-around uses finite-difference equations and model.predict to approximate the elements of the gradient and Hessian (as described here).  But I don't love this approach — it is sensitive to floating-point error and the ""exact"" information needed to build the gradient and Hessian is already contained in model.coef_.
Is there any more elegant or accurate method to fit a p-dimensional polynomial and find its gradient and Hessian within Python? I would be fine with one that uses a different library.
","To compute the gradient or the Hessian of a polynomial, one needs to know exponents of variables in each monomial and the corresponding monomial coefficients. The first piece of this information is provided by poly.powers_, the second by model.coef_:
from sklearn.preprocessing import PolynomialFeatures
from sklearn.linear_model import LinearRegression
import numpy as np

np.set_printoptions(precision=2, suppress=True)

X = np.arange(6).reshape(3, 2)
y = np.arange(3)
poly = PolynomialFeatures(degree=2)
Xp = poly.fit_transform(X)
model = LinearRegression()
model.fit(Xp, y)

print(""Exponents:"")
print(poly.powers_.T)
print(""Coefficients:"")
print(model.coef_)

This gives:
Exponents:
[[0 1 0 2 1 0]
 [0 0 1 0 1 2]]
Coefficients:
[ 0.    0.13  0.13 -0.12 -0.    0.13]

The following function can be then used to compute the gradient at a point given by an array x:
def gradient(x, powers, coeffs):
    x = np.array(x)
    gp = np.maximum(0, powers[:, np.newaxis] - np.eye(powers.shape[1], dtype=int))
    gp = gp.transpose(1, 2, 0)
    gc = coeffs * powers.T
    return (((x[:, np.newaxis] ** gp).prod(axis=1)) * gc).sum(axis=1)

For example, we can use it to compute the gradient at the point [0, 1]:
print(gradient([0, 1],  poly.powers_, model.coef_))

This gives:
[0.13 0.38]

The Hessian at a given point can be computed in a similar way.
",scikit-learn
ignore nan to calculate mean_absolute_error,"I'm trying to calculate MAE (Mean absolute error).
In my original DataFrame, I have 1826 rows and 3 columns. I'm using columns 2 and 3 to calculate MAE.
But, in column 2, I have some NaN values.
When I used:
from sklearn.metrics import mean_absolute_error

and selected these columns, it gave me an error: ""Input contains NaN'.
As an example, I'm trying to do something like this:
from sklearn.metrics import mean_absolute_error
y_true = [3, -0.5, 2, 7, 10]
y_pred = [2.5, np.NaN, 2, 8, np.NaN]
mean_absolute_error(y_true, y_pred)

Is it possible to skip or ignore the rows with NaN?
UPDATE
I was analyzing with my advisor teacher, and we decided that the best is to drop all these NaN values.
","If you want to ignore the NaNs, build a mask a perform boolean indexing:
from sklearn.metrics import mean_absolute_error
import numpy as np

y_true = np.array([3, -0.5, 2, 7, 10])
y_pred = np.array([2.5, np.NaN, 2, 8, np.NaN])
m = ~np.isnan(y_pred)

mean_absolute_error(y_true[m], y_pred[m])

Output: 0.5
",scikit-learn
how to create a scaler applying log transformation and minmaxscaler in sklearn,"I want to apply log() to my DataFrame and MinMaxScaler() together.
I want the output to be a pandas DataFrame() with indexes and columns from the original data.
I want to use the parameters used to fit_transform() to inverse_transform() resulting in a new data frame. So, it needs to be constructed inside the FunctionTransformer.
What I tried:
from sklearn.preprocessing import FunctionTransformer
from sklearn.preprocessing import MinMaxScaler
import pandas as pd
import numpy as np

# Initialize MinMaxScaler with range (0, 1)
scaler_logMinMax = MinMaxScaler(feature_range=(0, 1))

# Log transformation function
def log_and_scale(X, scaler=scaler_logMinMax, shift=1e-9):
    X_log = np.log(X + shift)  # Apply log transformation with a small shift
    return pd.DataFrame(scaler.fit_transform(X_log))  # Scale the log-transformed data


# Inverse transformation: first unscale, then inverse log transform
def inv_log_and_scale(X, scaler=scaler_logMinMax, shift=1e-9):
    X_unscaled = scaler.inverse_transform(X)  # Inverse scaling
    return np.exp(X_unscaled) - shift  # Inverse of log transformation

# Create FunctionTransformer for the log and scale transformation
log_and_scale_transformer = FunctionTransformer(func=log_and_scale, inverse_func=inv_log_and_scale, validate=True)
df_subset = pd.DataFrame(
    {
        1: [135.2342984, 83.17136704, 23.41329775, 3.574450787],
        2: [59.31328422, 18.15285711, 11.1736562, 4.788951527],
        3: [45.0087282, 4.094515245, 106.536704, 527.0962651],
    }
)
df_subset.columns = [1, 2, 3]
df_subset.index = [""201001"", ""201002"", ""201003"", ""201004""]
df_subset.index.name = ""Date""
df_subset.columns.name = ""id""
cols_to_apply_scaler = [1, 2]
df_subset

id  1           2           3
Date            
201001  135.234298  59.313284   45.008728
201002  83.171367   18.152857   4.094515
201003  23.413298   11.173656   106.536704
201004  3.574451    4.788952    527.096265

# Transforming
df_subset[cols_to_apply_scaler] = pd.DataFrame(log_and_scale_transformer.fit_transform(df_subset[cols_to_apply_scaler]))
df_subset

id  1   2   3
Date            
201001  NaN NaN 45.008728
201002  NaN NaN 4.094515
201003  NaN NaN 106.536704
201004  NaN NaN 527.096265
# The way that I expect to apply the inverse transformer.
# df_subset[cols_to_apply_scaler] = log_and_scale_transformer.inverse_transform(df_subset[cols_to_apply_scaler])

Questions:

The pd.DataFrame(log_and_scale_transformer.fit_transform(df_subset[cols_to_apply_scaler])) works, but it can't assign to the original DataFrame because the name of columns change. How to fix it?
How the values of scaler_logMinMax from fit_transform() were carried through the inverse_transform?

I also tried log_and_scale_transformer = log_and_scale_transformer.set_output(transform=""pandas"") after creating the dataframe, but it did not work.
I need to filter the columns before applying the function.
I also want to stick with FunctionTransformer because I use other transformers with the same structure. For ex:
# Define the inverse transformation function with a shift
def inv_y(X, shift=0.5):
    return 1 / (X + shift)

# Define the inverse inverse transformation to revert to original values
def inv_inv_y(X, shift=0.5):
    return (1 - X * shift) / X

# Create the FunctionTransformer
inverse_transformer = FunctionTransformer(func=inv_y, inverse_func=inv_inv_y, validate=False, check_inverse=True)

In summary, I cannot apply a function and a scaler together.

With a different simple example, it works:
# DataFrame Example
X = np.array([[0, 1, 2], [2, 3, 4], [5, 7, 9]])
cols = [""A"", ""B"", ""C""]
cols_to_apply_scaler = cols[:-1]
X = pd.DataFrame(X, columns=cols, index=[0,1,2])
X

    A   B   C
0   0   1   2
1   2   3   4
2   5   7   9

# Transforming
X[cols_to_apply_scaler] = pd.DataFrame(log_and_scale_transformer.fit_transform(X[cols_to_apply_scaler]))

    A           B           C
0   0.000000    0.000000    2
1   0.958971    0.564575    4
2   1.000000    1.000000    9

/home/guilherme/anaconda3/envs/time_series/lib/python3.11/site-packages/sklearn/base.py:493: UserWarning: X does not have valid feature names, but FunctionTransformer was fitted with feature names
  warnings.warn(

# Inverse
X[cols_to_apply_scaler] = log_and_scale_transformer.inverse_transform(X[cols_to_apply_scaler])
X

    A               B   C
0   6.203855e-25    1.0 2
1   2.000000e+00    3.0 4
2   5.000000e+00    7.0 9

But I did not understand the warning. Can I fix it?
","Concerning your first question, to preserve the index and columns work with Dataframes.
Concerning your second question, the values used in fit_transform() are carried through to inverse_transform() because the state of the scaler is stored internally within the object instance.
Full example based on you OP:
from sklearn.preprocessing import FunctionTransformer, MinMaxScaler
import pandas as pd, numpy as np


scaler_logMinMax = MinMaxScaler(feature_range=(0, 1))

# Log transformation function
def log_and_scale(X, scaler=scaler_logMinMax, shift=1e-9):
    X_log = np.log(X + shift)
    scaled = scaler.fit_transform(X_log)
    return pd.DataFrame(scaled, index=X.index, columns=X.columns)


def inv_log_and_scale(X, scaler=scaler_logMinMax, shift=1e-9):
    unscaled = scaler.inverse_transform(X)
    return pd.DataFrame(np.exp(unscaled) - shift, index=X.index, columns=X.columns)


log_and_scale_transformer = FunctionTransformer(
    func=log_and_scale, 
    inverse_func=inv_log_and_scale, 
    validate=False  # Allow pandas
)


df_subset = pd.DataFrame(
    {
        1: [135.2342984, 83.17136704, 23.41329775, 3.574450787],
        2: [59.31328422, 18.15285711, 11.1736562, 4.788951527],
        3: [45.0087282, 4.094515245, 106.536704, 527.0962651],
    },
    index=[""201001"", ""201002"", ""201003"", ""201004""]
)
df_subset.columns = [1, 2, 3]
df_subset.index.name = ""Date""
df_subset.columns.name = ""id""
cols_to_apply_scaler = [1, 2]

# fit and transform
df_subset[cols_to_apply_scaler] = log_and_scale_transformer.fit_transform(df_subset[cols_to_apply_scaler])


print(""Transformed DataFrame:"")
print(df_subset)

# inverse transform the same columns
df_subset[cols_to_apply_scaler] = log_and_scale_transformer.inverse_transform(df_subset[cols_to_apply_scaler])


print(""\nInverse Transformed DataFrame:"")
print(df_subset)

This prints
id           1         2           3
Date                                
201001  1.000000  1.000000   45.008728
201002  0.894048  0.789684    4.094515
201003  0.574869  0.649927  106.536704
201004  0.000000  0.000000  527.096265

and
id            1          2           3
Date                                 
201001  135.234298  59.313284   45.008728
201002   83.171367  18.152857    4.094515
201003   23.413298  11.173656  106.536704
201004    3.574451   4.788952  527.096265

",scikit-learn
problem in installing and using scikitlearn in python,"I want to run the following code in jupyter-notebook
from sklearn.linear_model import SGDRegressor
before that, I have used the following lines in windows cmd:
python -m venv sklearn-env
sklearn-env\Scripts\activate
pip install -U scikit-learn

But still when I run the code, I get the error as:
No module named 'sklearn'
Any idea what is the problem?
","I added the .venv kernel. Using the following codes
pip install virtualenv

and then
pip install -U scikit-learn scipy matplotlib

and the problem was solved.
",scikit-learn
scikitlearn model giving 39localoutlierfactor39 object has no attribute 39predict39 error,"I'm new to machine learning world and I have built and trained a ml model using ScikitLearn library.It works perfectly well in the Jupyter notebook but when I deployed this model to Google Cloud ML and try to serve it using a Python script, it throws an error.
Here's a snippet from my model code:

Updated:

from sklearn.metrics import classification_report, accuracy_score
from sklearn.ensemble import IsolationForest
from sklearn.neighbors import LocalOutlierFactor

# define a random state
state = 1

classifiers = {
    ""Isolation Forest"": IsolationForest(max_samples=len(X),
                                       contamination=outlier_fraction,
                                       random_state=state),
    # ""Local Outlier Factor"": LocalOutlierFactor(
    # n_neighbors = 20,
    # contamination = outlier_fraction)
}

import pickle
# fit the model
n_outliers = len(Fraud)

for i, (clf_name, clf) in enumerate(classifiers.items()):

    # fit te data and tag outliers
    if clf_name == ""Local Outlier Factor"":
        y_pred = clf.fit_predict(X)
        print(""LOF executed"")
        scores_pred = clf.negative_outlier_factor_
        # Export the classifier to a file
        with open('model.pkl', 'wb') as model_file:
            pickle.dump(clf, model_file)
    else:
        clf.fit(X)
        scores_pred = clf.decision_function(X)
        y_pred = clf.predict(X)
        print(""IF executed"")
        # Export the classifier to a file
        with open('model.pkl', 'wb') as model_file:
            pickle.dump(clf, model_file)
    # Reshape the prediction values to 0 for valid and 1 for fraudulent
    y_pred[y_pred == 1] = 0
    y_pred[y_pred == -1] = 1

    n_errors = (y_pred != Y).sum()

# run classification metrics 
print('{}:{}'.format(clf_name, n_errors))
print(accuracy_score(Y, y_pred ))
print(classification_report(Y, y_pred ))

and here's the output in the Jupyter Notebook:

Isolation Forest:7
0.93
               precision    recall  f1-score   support


         0       0.97      0.96      0.96        94
         1       0.43      0.50      0.46         6

  avg / total    0.94      0.93      0.93       100


I have deployed this model to Google  Cloud ML-Engine and then try to serve it using the following python script:
import os
from googleapiclient import discovery
from oauth2client.service_account import ServiceAccountCredentials
credentials = ServiceAccountCredentials.from_json_keyfile_name('Machine Learning 001-dafe42dfb46f.json')

PROJECT_ID = ""machine-learning-001-201312""
VERSION_NAME = ""v1""
MODEL_NAME = ""mlfd""
service = discovery.build('ml', 'v1', credentials=credentials)
name = 'projects/{}/models/{}'.format(PROJECT_ID, MODEL_NAME)
name += '/versions/{}'.format(VERSION_NAME)

data = [[265580, 7, 68728, 8.36, 4.76, 84.12, 79.36, 3346, 1, 11.99, 1.14,655012, 0.65, 258374, 0, 84.12] ]

response = service.projects().predict(
    name=name,
    body={'instances': data}
).execute()

if 'error' in response:
  print (response['error'])
else:
  online_results = response['predictions']
  print(online_results)

Here is the output of this script:

Prediction failed: Exception during sklearn prediction: 'LocalOutlierFactor' object has no attribute 'predict'

","LocalOutlierFactor does not have a predict method, but only a private _predict method.  Here is the justification from the source.
def _predict(self, X=None):
    """"""Predict the labels (1 inlier, -1 outlier) of X according to LOF.
    If X is None, returns the same as fit_predict(X_train).
    This method allows to generalize prediction to new observations (not
    in the training set). As LOF originally does not deal with new data,
    this method is kept private.

https://github.com/scikit-learn/scikit-learn/blob/a24c8b46/sklearn/neighbors/lof.py#L200
",scikit-learn
implementing bag of words in scikitlearn,"from sklearn.feature_extraction.text import CountVectorizer
import pandas as pd
headers = ['label', 'sms_message']
df = pd.read_csv ('spam.csv', names = headers)
df ['label'] = df['label'].map({'ham': 0, 'spam': 1})
print (df.head(7))
print (df.shape)
count_vector = CountVectorizer()
#count_vector.fit(df)
y = count_vector.fit_transform(df)
count_vector.get_feature_names()
doc_array = y.toarray()
print (doc_array)
frequency_matrix = pd.DataFrame(doc_array, columns = count_vector.get_feature_names())
frequency_matrix

Sample data and output:
   label                                        sms_message
0      0  Go until jurong point, crazy.. Available only ...
1      0                      Ok lar... Joking wif u oni...
2      1  Free entry in 2 a wkly comp to win FA Cup fina...
3      0  U dun say so early hor... U c already then say...

(5573, 2)
[[1 0]
 [0 1]]

label   sms_message
0   1   0
1   0   1

My Question:
My csv file is basically many rows of sms messages.
I cannot understand why I am getting only output for the column labels and not for the entire rows of sms texts.
Thank you for any help.
","Pass only the sms_message column to count vectorizer as shown below.
import numpy as np
import pandas as pd
from sklearn.feature_extraction.text import CountVectorizer

docs = ['Tea is an aromatic beverage..',
        'After water, it is the most widely consumed drink in the world',
        'There are many different types of tea.',
        'Tea has a stimulating effect in humans.',
        'Tea originated in Southwest China during the Shang dynasty'] 

df = pd.DataFrame({'sms_message': docs, 'label': np.random.choice([0, 1], size=5)})

cv = CountVectorizer()
counts = cv.fit_transform(df['sms_message'])

df_counts = pd.DataFrame(counts.toarray(), columns=cv.get_feature_names_out())
df_counts['label'] = df['label']

Output:
df_counts

Out[26]: 
   after  an  are  aromatic  beverage  ...  types  water  widely  world  label
0      0   1    0         1         1  ...      0      0       0      0      1
1      1   0    0         0         0  ...      0      1       1      1      0
2      0   0    1         0         0  ...      1      0       0      0      1
3      0   0    0         0         0  ...      0      0       0      0      1
4      0   0    0         0         0  ...      0      0       0      0      0

[5 rows x 32 columns]

",scikit-learn
gridsearchcv  xgboost  early stopping,"i am trying to do hyperparemeter search with using scikit-learn's GridSearchCV on XGBoost. During gridsearch i'd like it to early stop, since it reduce search time drastically and (expecting to) have better results on my prediction/regression task. I am using XGBoost via its Scikit-Learn API.
    model = xgb.XGBRegressor()
    GridSearchCV(model, paramGrid, verbose=verbose ,fit_params={'early_stopping_rounds':42}, cv=TimeSeriesSplit(n_splits=cv).get_n_splits([trainX, trainY]), n_jobs=n_jobs, iid=iid).fit(trainX,trainY)

I tried to give early stopping parameters with using fit_params, but then it throws this error which is basically because of lack of validation set which is required for early stopping:
/opt/anaconda/anaconda3/lib/python3.5/site-packages/xgboost/callback.py in callback(env=XGBoostCallbackEnv(model=<xgboost.core.Booster o...teration=4000, rank=0, evaluation_result_list=[]))
    187         else:
    188             assert env.cvfolds is not None
    189 
    190     def callback(env):
    191         """"""internal function""""""
--> 192         score = env.evaluation_result_list[-1][1]
        score = undefined
        env.evaluation_result_list = []
    193         if len(state) == 0:
    194             init(env)
    195         best_score = state['best_score']
    196         best_iteration = state['best_iteration']

How can i apply GridSearch on XGBoost with using early_stopping_rounds?
note: model is working without gridsearch, also GridSearch works without 'fit_params={'early_stopping_rounds':42}
","An update to @glao's answer and a response to @Vasim's comment/question, as of sklearn 0.21.3 (note that fit_params has been moved out of the instantiation of GridSearchCV and been moved into the fit() method; also, the import specifically pulls in the sklearn wrapper module from xgboost):
import xgboost.sklearn as xgb
from sklearn.model_selection import GridSearchCV
from sklearn.model_selection import TimeSeriesSplit

cv = 2

trainX= [[1], [2], [3], [4], [5]]
trainY = [1, 2, 3, 4, 5]

# these are the evaluation sets
testX = trainX 
testY = trainY

paramGrid = {""subsample"" : [0.5, 0.8]}

fit_params={""early_stopping_rounds"":42, 
            ""eval_metric"" : ""mae"", 
            ""eval_set"" : [[testX, testY]]}

model = xgb.XGBRegressor()

gridsearch = GridSearchCV(model, paramGrid, verbose=1,             
         cv=TimeSeriesSplit(n_splits=cv).get_n_splits([trainX, trainY]))

gridsearch.fit(trainX, trainY, **fit_params)

",scikit-learn
columns are missing after imputing and creating dummy variables how should i fix this,"In short: My columns are different between train set and test set after imputing.
Code of making train, test dataset
random_state_value = 0

#Define target
X = data.drop(columns = 'income', axis=1)
y = data['income']

#Split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state = random_state_value)

#Impute missing data
imputer_cat = SimpleImputer(strategy = 'most_frequent')
imputer_num = SimpleImputer(strategy = 'median')

X_train[['workclass', 'occupation', 'native-country']] = imputer_cat.fit_transform(X_train[['workclass', 'occupation', 'native-country']])
X_train[['age']] = imputer_num.fit_transform(X_train[['age']])

X_test[['workclass', 'occupation', 'native-country']] = imputer_cat.fit_transform(X_test[['workclass', 'occupation', 'native-country']])
X_test[['age']] = imputer_num.fit_transform(X_test[['age']])

#Create dummy vars
X_train = pd.get_dummies(X_train, columns=['workclass', 'education', 'marital-status', 
                                     'occupation', 'relationship', 'race', 'gender', 'native-country'], drop_first = True)
X_test = pd.get_dummies(X_test, columns=['workclass', 'education', 'marital-status', 
                                     'occupation', 'relationship', 'race', 'gender', 'native-country'], drop_first = True)

y_train = pd.get_dummies(y_train, columns='income', drop_first = True)
y_test = pd.get_dummies(y_test, columns='income', drop_first = True)

y_test = y_test.values.ravel()
y_train = y_train.values.ravel()

I had categorical variables which had missing values. This is what I have done.
1.
split the data into train, test set
2.
impute each value in train and test set
3.
make dummy variables for categorical variables
But then some columns have disappeared and the length of X_test and X_train is different.
length not matching
lost columns
temp_test = X_test.columns.sort_values()
temp_train = X_train.columns.sort_values()

[col for col in temp_train if col not in temp_test]

These are the columns.
Why does this happen? And how can I fix this problem?
","When encoding categorical variables, you need to be careful, if you decide to use pd.get_dummies. After you use it for your training data, it's not trivial to keep the same encoding it came up with on your test data. That is, if on your training data you had categorical values in a gender column, such as [""female"", ""male""], and it encoded it as [0, 1] respectively, there is no guarantee that it will do the same if you run it on your test data with the same categorical values.
On the other hand, if you have categorical values that only appear on your training set, pd.get_dummies will naturally only encode those and create the respective new columns, that is, [""gender_male"", ""gender_female""]. If coincidentally after making your train/test split, the training set only has the ""male"" values, then your current code will create ""gender_male"" for that DataFrame and the testing set will have a ""gender_female"" column. Hence, both having different columns. Note that I purposely avoided the drop_first=True conversation to make my point, but you might consider doing that, as discussed heavily in this StackOverflow post.
This post: Keep same dummy variable in training and testing data also goes over this topic in detail.
The following example aims to exemplify this with some made-up data (since we don't have access to yours):
import numpy as np
import pandas as pd
from sklearn.impute import SimpleImputer
from sklearn.model_selection import train_test_split

# Generate sample data
np.random.seed(42)
num_rows = 1000

data = pd.DataFrame(
    {
        ""occupation"": np.random.choice(
            [""Tech-support"", ""Priv-house-serv"", ""Protective-serv"", ""Armed-Forces""], num_rows
        ),
        ""race"": np.random.choice([""White"", ""Asian-Pac-Islander"", ""Amer-Indian-Eskimo"", ""Other"", ""Black""], num_rows),
        ""gender"": np.random.choice([""Male"", ""Female""], num_rows),
        ""native-country"": np.random.choice([""United-States"", ""Cambodia"", ""England"", ""Puerto-Rico""], num_rows),
        ""age"": np.random.randint(18, 81, num_rows),
        ""income"": np.where(np.random.rand(num_rows) < 0.25, "">50K"", ""<=50K""),
    }
)

# Split the data into train and test sets
X = data.drop(columns=""income"")
y = data[""income""]
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

# Simulate a scenario in which ""Other"" is coincidentally missing from 
# the test set
X_test[X_test[""race""] == ""Other""] = np.nan

# Create dummy variables on the train and test sets separately
X_train = pd.get_dummies(
    X_train,
    columns=[""occupation"", ""race"", ""gender"", ""native-country""],
    drop_first=False,
)
X_test = pd.get_dummies(
    X_test,
    columns=[""occupation"", ""race"", ""gender"", ""native-country""],
    drop_first=False,
)

print(""X_train columns:"", X_train.columns)
print(""X_test columns:"", X_test.columns)

# Test if the columns are the same
assert X_train.columns.equals(X_test.columns)  # This will fail!

What you should do instead is to use pd.get_dummies on your whole dataset first and then train/test split!
This should avoid any columns being different issues. That is,
# Generate sample data
data = ...

# Split the data into train and test sets
X = data.drop(columns=""income"")
y = data[""income""]
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

# Simulate a scenario in which ""Other"" is coincidentally missing from the test set
X_test[X_test[""race""] == ""Other""] = np.nan

# Combine the train and test sets before creating dummy variables
# (Note that we can just do this on X first, then train/test split, but it helps me
#  make my point regarding categorical values missing in X_test)
X_all = pd.concat([X_train, X_test], ignore_index=True)
X_all = pd.get_dummies(
    X_all,
    columns=[""occupation"", ""race"", ""gender"", ""native-country""],
    drop_first=False,
)

# Split the data back into train and test sets
X_train, X_test, y_train, y_test = train_test_split(
    X_all, y, test_size=0.2, random_state=42
)

print(""X_train columns:"", X_train.columns)
print(""X_test columns:"", X_test.columns)

# Test if the columns are the same
assert X_train.columns.equals(X_test.columns)  # This won't fail!

Finally, I recommend using scikit-learn's OneHotEncoder and your code would look like something like this with the added benefit of saving how exactly it did the encoding to the categorical values:
# Simulate a scenario in which ""Other"" is coincidentally missing from the test set
# This time doing it on the original dataset, X
X[X[""race""] == ""Other""] = np.nan
original_columns = X.columns

# Create dummy variables using OneHotEncoder
encoder = OneHotEncoder(handle_unknown=""ignore"", sparse_output=False)
X = encoder.fit_transform(X)

# Convert the encoded data to DataFrames
X = pd.DataFrame(X, columns=encoder.get_feature_names_out(original_columns))

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

print(""X_train columns:"", X_train_encoded.columns)
print(""X_test columns:"", X_test_encoded.columns)

# Test if the columns are the same
# This also won't fail!
assert X_train_encoded.columns.equals(X_test_encoded.columns)

You can read up more about it here.
",scikit-learn
how to get scikitlearn to ensure that all prediction outputs should sum to 100,"I have a 'MultiOutputRegressor' which is based on a 'LinearRegression' regressor.
I am using it to predict three outputs per row of X_data (like a classifier) which represent the percentage likelihood of three outcomes.
The regressor is fitted against y_data where the three labels sum correctly to 100%.
Obviously the regressor doesn't really know that it's three prediction outputs should sum, it just knows roughly what values they should be.
Is there a way that I can tell the regressor explicitly that one of the rules is that all three prediction outputs should together sum to 100%?
","Shortly, no. Regressors cannot know this. Your problem is a multi-class classification problem. You need to use classifier for your problem. Classifier model predicts probabilites of three labels. And sum of them will be 1 (100%).
https://scikit-learn.org/stable/modules/generated/sklearn.multioutput.MultiOutputClassifier.html
",scikit-learn
implementing gridsearchcv and pipelines to perform hyperparameters tuning for knn algorithm,"I have been reading about perfroming Hyperparameters Tuning for KNN Algorthim, and understood that the best practice of implementing it is to make sure that for each fold, my dataset should be normalized and oversamplmed using a pipeline (To avoid data leakage and overfitting).
What I'm trying to do is that I'm trying to identify the best number of neighbors (n_neighbors) possible that gives me the best accuracy in training. In the code I have set the number of neighbors to be a list range (1,50), and the number of iterations cv=10.
My code below:
# dataset reading & preprocessing libraries
import pandas as pd
import numpy as np
from sklearn.preprocessing import MinMaxScaler

#oversmapling
from imblearn.over_sampling import SMOTE

#KNN Model related Libraries
import cuml 
from imblearn.pipeline import Pipeline
from sklearn.model_selection import train_test_split, GridSearchCV
from cuml.neighbors import KNeighborsClassifier

#loading the dataset
df = pd.read_csv(""/content/drive/MyDrive/Colab Notebooks/dataset/IanDataset.csv"")

#filling missing values with zeros
df = df.fillna(0)

#replace the data in from being objects to integers
df[""command response""].replace({""b'0'"": ""0"", ""b'1'"": ""1""}, inplace=True)
df[""binary result""].replace({""b'0'"": ""0"", ""b'1'"": ""1""}, inplace=True)

#change the datatype of some features to be able to be used later 
df[""command response""] = pd.to_numeric(df[""command response""]).astype(float)
df[""binary result""] = pd.to_numeric(df[""binary result""]).astype(int)

# dataset splitting
X = df.iloc[:, 0:17]
y_bin = df.iloc[:, 17]

# spliting the dataset into train and test for binary classification
X_train, X_test, y_bin_train, y_bin_test = train_test_split(X, y_bin, random_state=0, test_size=0.2)

#making pipleline that normalize, oversample and use classifier before GridSearchCV
pipe = Pipeline([
        ('normalization', MinMaxScaler()),
        ('oversampling', SMOTE()),
        ('classifier', KNeighborsClassifier(metric='eculidean', output='input'))
])

#Using GridSearchCV
neighbors = list(range(1,50))
parameters = {
    'classifier__n_neighbors': neighbors 
}

grid_search = GridSearchCV(pipe, parameters, cv=10)
grid_search.fit(X_train, y_bin_train)

print(""Best Accuracy: {}"" .format(grid_search.best_score_))
print(""Best num of neighbors: {}"" .format(grid_search.best_estimator_.get_params()['n_neighbors']))

At step grid_search.fit(X_train, y_bin_train), the program is repeating the error that i'm getting is :
/usr/local/lib/python3.7/site-packages/sklearn/model_selection/_validation.py:619: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: 
Traceback (most recent call last):
  File ""/usr/local/lib/python3.7/site-packages/sklearn/model_selection/_validation.py"", line 598, in _fit_and_score
    estimator.fit(X_train, y_train, **fit_params)
  File ""/usr/local/lib/python3.7/dist-packages/imblearn/pipeline.py"", line 266, in fit
    self._final_estimator.fit(Xt, yt, **fit_params_last_step)
  File ""/usr/local/lib/python3.7/site-packages/cuml/internals/api_decorators.py"", line 409, in inner_with_setters
    return func(*args, **kwargs)
  File ""cuml/neighbors/kneighbors_classifier.pyx"", line 176, in cuml.neighbors.kneighbors_classifier.KNeighborsClassifier.fit
  File ""/usr/local/lib/python3.7/site-packages/cuml/internals/api_decorators.py"", line 409, in inner_with_setters
    return func(*args, **kwargs)
  File ""cuml/neighbors/nearest_neighbors.pyx"", line 397, in cuml.neighbors.nearest_neighbors.NearestNeighbors.fit
ValueError: Metric  is not valid. Use sorted(cuml.neighbors.VALID_METRICSeculidean[brute]) to get valid options.

I'm not sure from which side is this error coming from, is it because I'm importing KNN Algorthim from cuML Library instead of sklearn ? Or is there something wrong wtih my Pipeline and GridSearchCV implementation?
","This error indicates you've passed an invalid value for the metric parameter (in both scikit-learn and cuML). You've misspelled ""euclidean"".
import cuml
from sklearn import datasets
​
from sklearn.preprocessing import MinMaxScaler
​
from imblearn.over_sampling import SMOTE
​
from imblearn.pipeline import Pipeline
from sklearn.model_selection import train_test_split, GridSearchCV
from cuml.neighbors import KNeighborsClassifier
​
X, y = datasets.make_classification(
    n_samples=100
)
​
pipe = Pipeline([
        ('normalization', MinMaxScaler()),
        ('oversampling', SMOTE()),
        ('classifier', KNeighborsClassifier(metric='euclidean', output='input'))
])
​
parameters = {
    'classifier__n_neighbors': [1,3,6] 
}
​
grid_search = GridSearchCV(pipe, parameters, cv=2)
grid_search.fit(X, y)
GridSearchCV(cv=2,
             estimator=Pipeline(steps=[('normalization', MinMaxScaler()),
                                       ('oversampling', SMOTE()),
                                       ('classifier', KNeighborsClassifier())]),
             param_grid={'classifier__n_neighbors': [1, 3, 6]})

",scikit-learn
how do i set random_state correctly so that my results are always the same,"If I have for example this snippet of code:
knn = KNeighborsClassifier()
grid_search_knn = GridSearchCV(
    estimator=knn,
    n_jobs=-1)

Do I have to set it like this:
knn = KNeighborsClassifier(random_state=42)

grid_search_knn = GridSearchCV(
    estimator=knn,
    n_jobs=-1
)

Or do I have to set it like this?
knn = KNeighborsClassifier(random_state=42)

grid_search_knn = GridSearchCV(
    estimator=knn,
    random_state=42,
    n_jobs=-1
)

what is the correct why? And what if I use randomisedsearch instead of gridsearch?
","In this case, setting the random_state depends on the specific algorithm you’re using, rather than on the GridSearchCV or RandomizedSearchCV class.
For KNeighborsClassifier, adding random_state is actually unnecessary because this classifier is a deterministic algorithm, meaning it doesn’t rely on randomness to make predictions. Therefore, it won’t be affected by a random_state parameter. As a result:

For KNeighborsClassifier: You don’t need to set random_state at all in either the classifier or in the GridSearchCV/RandomizedSearchCV.

For Randomized Algorithms: If you’re using an algorithm that involves randomness, like a decision tree or a random forest, you can set the random_state in the estimator (like RandomForestClassifier(random_state=42)). You don’t need to set random_state in GridSearchCV, as it only influences the cross-validation process, which is deterministic.


In summary:

For KNeighborsClassifier: No random_state is needed.
For Randomized algorithms: Set random_state in the estimator, not in GridSearchCV/RandomizedSearchCV.
For RandomizedSearchCV: You might set random_state there if the search itself is randomized and you want reproducibility.

",scikit-learn
scikitlearn classifiers and regressors caching training data,"I have some 22,000 rows of training data.
I use train_test_split to get training and testing data.
I run fitting and then get some idea of how well fitting went using various methods or estimation.
I want to have the fitted model go back over the 22,000 rows and predict against them as if it had never seen them before.
However when I do this the regressors or classifiers get every single row 100% correct, which cannot be right given that largely the best I can expect is 75% etc.
Do the estimators have some sort of learning data cache? How can I delete the cache but keep the trained model?
","
go back over the 22,000 rows and predict against them as if it had never seen them before

This is not possible. It HAS seen them during training and was optimized to best fit to the data presented.
There is no magical cache, but the model's learned parameters are deduced from what it saw during training. Worst case is, you have an overfitted model that gets 100% accuracy on training data without any generalisation because it had enough parameters or your dataset too little variation and your model just learned to reproduce your training data exactly.
See scikits example page about the same topic which also has a plot for demonstration:

note how the model fitted to the data points in the last panel has acurately learned to represent the training data, but is far away from a meaningful representation of the actual underlying true model
",scikit-learn
scikit train_test_split by an index,"I have a pandas dataframe indexed by date. Let's assume it from Jan-1 to Jan-30. I want to split this dataset into X_train, X_test, y_train, y_test but I don't want to mix the dates so I want the train and test samples to be divided by a certain date (or index). I'm trying
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0) 
But when I check the values, I see the dates are mixed. I want to split my data as:
Jan-1 to Jan-24 to train and Jan-25 to Jan-30 to test (as test_size is 0.2, that makes 24 to train and 6 to test)
How can I do this?
","you should use
X_train, X_test, y_train, y_test = train_test_split(X,Y, shuffle=False, test_size=0.2, stratify=None)

don't use random_state=None it will take numpy.random
in here its mentioned that use shuffle=False along with stratify=None
",scikit-learn
why is my data not getting properly concatenated,"I split the data using train_test_split after preprocessing:
from sklearn.model_selection import train_test_split
X_train,X_test,y_train,y_test= train_test_split(X,y,test_size=0.2,random_state=42)

Then did robust scaling separately for the numerical columns in test and train:
from sklearn.preprocessing import RobustScaler
robust = RobustScaler()
X_train_ = robust.fit_transform(X_train[numeric_columns])
X_test_ = robust.transform(X_test[numeric_columns])
X_train_sc_num=pd.DataFrame(X_train_,columns=[numeric_columns])
X_test_sc_num=pd.DataFrame(X_test_,columns=[numeric_columns])

Then did concatenation:
X_train_scaled=pd.concat([X_train_sc_num,X_train[categoric_columns]],axis=1)
X_test_scaled=pd.concat([X_test_sc_num,X_test[categoric_columns]],axis=1)

but the shape got broken and so many 'nan' values added in the categorical columns of the output data.
The sahpe was (466,17)+(466,11),
it should be (466,28),
but it became (560,28).
How can I solve this issue?
I want to do Robust scale my data after train_test_split, but without touching my OHE columns.
","Your issue might be arising from a couple of things.

First of all, you're using columns=[numeric_columns], which would treat the list as a single column name. Instead, it should be just columns=numeric_columns.
When creating the scaled DataFrames, you weren't preserving the index from the original data. To do this, you simply add an additional parameter index=X_train.index (or X_test, depending on the case) to the pd.DataFrame() initialization.

Here is a reproducible example using this example data illustrating the steps you'd need to follow with your data:
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import RobustScaler

# Create example data
np.random.seed(42)
n_samples = 466

numerical_data = {""temperature"": np.random.normal(14, 3, n_samples), ""moisture"": np.random.normal(96, 2, n_samples)}
categorical_data = {""color"": np.random.choice([""green"", ""yellow"", ""purple""], size=n_samples, p=[0.8, 0.1, 0.1])}

# Create DataFrame
df = pd.DataFrame({**numerical_data, **categorical_data})

# Define numeric and categorical columns
numerical_columns = numerical_data.keys()
categorical_columns = categorical_data.keys()

# One-hot encode categorical columns
df_encoded = pd.get_dummies(df, columns=categorical_columns)

# Split features and target (creating dummy target for example)
y = np.random.randint(0, 2, n_samples)
X = df_encoded

# Split the data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Get the one-hot encoded column names
# In general, these will be different than categorical_columns, since you're doing OHE
categorical_columns_encoded = [col for col in X_train.columns if col not in numerical_columns]

# Initialize RobustScaler
robust = RobustScaler()

# Scale only numeric columns
X_train_scaled_numeric = robust.fit_transform(X_train[numerical_columns])
X_test_scaled_numeric = robust.transform(X_test[numerical_columns])

# Create DataFrames with correct column names for scaled numeric data
X_train_scaled_numeric_df = pd.DataFrame(
    X_train_scaled_numeric,
    columns=numerical_columns,
    index=X_train.index,  # Preserve the index
)

X_test_scaled_numeric_df = pd.DataFrame(
    X_test_scaled_numeric,
    columns=numerical_columns,
    index=X_test.index,  # Preserve the index
)

# Concatenate with categorical columns
X_train_scaled = pd.concat([X_train_scaled_numeric_df, X_train[categorical_columns_encoded]], axis=1)
X_test_scaled = pd.concat([X_test_scaled_numeric_df, X_test[categorical_columns_encoded]], axis=1)

# Verify the shapes
print(""Original shapes:"")
print(f""X_train: {X_train.shape}"")
print(f""X_test: {X_test.shape}"")
print(""\nScaled shapes:"")
print(""X_train_scaled: {X_train_scaled.shape} = {X_train_scaled_numeric.shape} + {X_train[categorical_columns_encoded].shape}"")
print(f""X_test_scaled: {X_test_scaled.shape} = {X_test_scaled_numeric.shape} + {X_test[categorical_columns_encoded].shape}"")

# Verify no NaN values
print(""\nNaN check:"")
print(""NaN in X_train_scaled:"", X_train_scaled.isna().sum().sum())
print(""NaN in X_test_scaled:"", X_test_scaled.isna().sum().sum())

That would print:
Original shapes:
X_train: (372, 5)
X_test: (94, 5)

Scaled shapes:
X_train_scaled: (372, 5) = (372, 2) + (372, 3)
X_test_scaled: (94, 5) = (94, 2) + (94, 3)

",scikit-learn
how to apply the exported sklearn trained tree to the test data,"from sklearn.tree import DecisionTreeRegressor, export_text

cols_X = ['f1', 'f2']
df_train = pd.DataFrame([[1, 3, 4], [2, 5, 1], [7, 8, 7]], columns=['f1', 'f2', 'label'])
df_test = pd.DataFrame([[2, 5], [3, 1]], columns=cols_X)

tree = DecisionTreeRegressor()
tree.fit(df_train[['f1', 'f2']], df_train['label'])

file = open(path + ""myTree.txt"", ""w"") 
file.write(export_text(tree, feature_names=cols_X)) 
file.write(""\n"") 
file.close()

input_tree = pd.read_csv(path + ""myTree.txt"") #not sure if should read as csv

A sklearn regression tree has been trained and exported as a txt file. Then how do I import it and apply onto the test data to make a prediction as .predict()? Since I am totally unfamiliar with the data structure in txt file, even not sure if i should read it as 'txt'.
","You are missing something important here.
The exported tree text when the function export_text was used to create your .txt is for interpretability purposes, not for reloading the model for prediction.
It prints the decision tree rules. This is neither the model nor its parameters. It is a visual way of what the model is doing internally. You cannot load or make a model using this txt file.
Re-build your model and save it as pickle.
Example:
# training the model on training set 
model_clf = KNeighborsClassifier(n_neighbors=3) 
model_clf.fit(X_train, y_train) 
  
# Saving classifier using pickle 
pickle.dump(model_clf, open(""model_clf_pickle"", 'wb')) 
  
# load classifier using pickle 
my_model_clf = pickle.load(open(""model_clf_pickle"", 'rb')) 
result_score = my_model_clf.score(X_test,y_test) 

",scikit-learn
disable internal scikit input validation checks,"Under the hood Scikit-Learn does a lot of input validation checks defined here such as checking for (X,y) shapes, expected scalars/arrays, ensuring estimator is already fitted, etc. When designing models these checks are extremely useful to catch early bugs, but these become runtime overheads in large mature pipelines once in production.
Is there currently any way, maybe via a global setting, passing in keyword arguments, or similar to disable these internal checks within pipelines, estimators & transformers?
","From this scikit discussion, turns out we can set global settings via:
import sklearn
sklearn.set_config(
   assume_finite=True,  # disable validation
)

",scikit-learn
how do i onehotencode a single column in a dataframe,"I have a dataframe called ""vehicles"" with 8 columns. 7 are numerical but the column named 'Car_name'  which is index 1 in the dataframe and  is categorical. i need to encode it
i tried this code and wont work
ohe = OneHotEncoding(categorical_features = [1])

vehicles_enc = ohe.fit_transform(vehicles).toarray()

TypeError: OneHotEncoder.__init__() got an unexpected keyword argument 'categorical_features'

this  however works perfectly in a youtube vid i used.
","It seems you are using a newer version of scikit-learn, which is most likely to be different from the video you are watching. The categorical_features argument is not valid for OneHotEncoder.
You can try the ColumnTransformer or directly specify which columns to encode.. something like this:
from sklearn.preprocessing import OneHotEncoder
from sklearn.compose import ColumnTransformer

ohe = OneHotEncoder()

column_transformer = ColumnTransformer(
    transformers=[
        ('ohe', ohe, [1])  # Index of 'Car_name' column
    ],
    remainder='passthrough'  # Keep the other columns as they are
)

vehicles_enc = column_transformer.fit_transform(vehicles).toarray()

And as an extra recommendation, always check the versions of the libraries that are used in tutorials, and make sure to check the official documentation of the libraries you are using:

https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html
https://scikit-learn.org/stable/modules/generated/sklearn.compose.ColumnTransformer.html

",scikit-learn
sklearn get distance from point to nearest cluster,"I'm using clustering algorithms like DBSCAN.
It returns a 'cluster' called -1 which are points that are not part of any cluster. For these points I want to determine the distance from it to the nearest cluster to get something like a metric for how abnormal this point is. Is this possible? Or are there any alternatives for this kind of metric?
","The answer will depend on the linkage strategy you choose. I'll give the example of single linkage.
First, you can construct the distance matrix of your data.
from sklearn.metrics.pairwise import pairwise_distances
dist_matrix = pairwise_distances(X)

Then, you'll extract the nearest cluster:
for point in unclustered_points:
    distances = []
    for cluster in clusters:
        distance = dist_matrix[point, cluster].min()  # Single linkage
        distances.append(distance)
    print(""The cluster for {} is {}"".format(point, cluster)

EDIT: This works, but it's O(n^2) as noted by Anony-Mousse. Considering core points is a better idea because it cuts down on your work. In addition, it is somewhat similar to centroid linkage.
",scikit-learn
how to make predictions with scikit39s surprise,"I'm having some trouble understanding the Surprise workflow. I have a file for training (which I seek to split into training and validation), and a file for testing data. I'm having trouble understanding the difference between a Surprise Dataset and Trainset
# Import data
data_dir = 'DIRECTORY_NAME'
reader = Reader(rating_scale=(1, 5))

# Create pandas dataframes
train_valid_df = pd.read_csv(os.path.join(data_dir, 'TRAINING_FILENAME.csv'))
train_df, valid_df = train_test_split(train_valid_df, test_size=0.2)
test_df = pd.read_csv(os.path.join(data_dir, 'TEST_FILENAME.csv'))

# Create surprise Dataset objects
train_valid_Dataset = Dataset.load_from_df(train_valid_df[['user_id', 'item_id', 'rating']], reader)
train_Dataset = Dataset.load_from_df(train_df[['user_id', 'item_id', 'rating']], reader)
valid_Dataset = Dataset.load_from_df(valid_df[['user_id', 'item_id', 'rating']], reader)
test_Dataset = Dataset.load_from_df(test_df[['user_id', 'item_id', 'rating']], reader)

# Create surprise Trainset object (and testset object?)
train_Trainset = train_data.build_full_trainset()
valid_Testset = trainset.build_anti_testset()

Then, I create my predictor:
algo = KNNBaseline(k=60, min_k=2, sim_options={'name': 'msd', 'user_based': True})

Now, if I want to cross validate I would do
cross_v = cross_validate(algo, all_data, measures=['mae'], cv=10, verbose=True)

Which trains the model (?), but if I wanted to use my fixed validation set, what would I do? This:?
algo.fit(train_Trainset)

After doing this, I tried to get some predictions:
predictions = algo.test(valid_Testset)
print(predictions[0])

With this being the result

But when I try to predict using item and user id numbers, it says such a prediction is impossible:
print(algo.predict('13', '194'))
print(algo.predict('260', '338'))
print(algo.predict('924', '559'))

Yielding:

The first user/item pair is from the training antiset, the second from the validation set, and the third from the training set. I don't know why this is behaving like this, and I've found the documentation confusing at times. Similarly, many tutorials online seem to be training on pandas dataframes which I get errors thrown for. Can anybody clarify what the workflow of surprise actually looks like? How do I train and then make predictions on a testing set?
","Since you have separate train and test, we create something similar to your data:
from surprise import Dataset, KNNBaseline, Reader
import pandas as pd
import numpy as np
from surprise.model_selection import cross_validate
reader = Reader(rating_scale=(1, 5))

train_df = pd.DataFrame({'user_id':np.random.choice(['1','2','3','4'],100),
                         'item_id':np.random.choice(['101','102','103','104'],100),
                         'rating':np.random.uniform(1,5,100)})

valid_df = pd.DataFrame({'user_id':np.random.choice(['1','2','3','4'],100),
                         'item_id':np.random.choice(['101','102','103','104'],100),
                         'rating':np.random.uniform(1,5,100)})

Then we need to convert the training data to a surprise.trainset , similar to what you have done:
train_Dataset = Dataset.load_from_df(train_df[['user_id', 'item_id', 'rating']], reader)
valid_Dataset = Dataset.load_from_df(valid_df[['user_id', 'item_id', 'rating']], reader)

train_Dataset = train_Dataset.build_full_trainset()

For fitting, you only need the train_Dataset, for the cross-validation, I am not sure what you are trying to do and i see it's out of the scope of question for prediction, so we fit:
algo = KNNBaseline(k=60, min_k=2, sim_options={'name': 'msd', 'user_based': True})
algo.fit(train_Dataset)

To predict, you need to provide the input as a list or array, which has the same shape as your input, so for example, if we want to provide the test Dataset, it will be:
testset = [valid_Dataset.df.loc[i].to_list() for i in range(len(valid_Dataset.df))]
algo.test(testset)[:2] 

[Prediction(uid='2', iid='103', r_ui=3.0224818872683845, est=2.8486558674146125, details={'actual_k': 25, 'was_impossible': False}),
 Prediction(uid='2', iid='103', r_ui=4.609064535195377, est=2.8486558674146125, details={'actual_k': 25, 'was_impossible': False})]

If you wanna test one or two values, it will be:
algo.test([['1','101',None]])

",scikit-learn
how to compute precision recall accuracy and f1score for the multiclass case with scikit learn,"I'm working in a sentiment analysis problem the data looks like this:
label instances
    5    1190
    4     838
    3     239
    1     204
    2     127

So my data is unbalanced since 1190 instances are labeled with 5. For the classification Im using scikit's SVC. The problem is I do not know how to balance my data in the right way in order to compute accurately the precision, recall, accuracy and f1-score for the multiclass case. So I tried the following approaches:
First:
wclf = SVC(kernel='linear', C= 1, class_weight={1: 10})
wclf.fit(X, y)
weighted_prediction = wclf.predict(X_test)

print 'Accuracy:', accuracy_score(y_test, weighted_prediction)
print 'F1 score:', f1_score(y_test, weighted_prediction,average='weighted')
print 'Recall:', recall_score(y_test, weighted_prediction,
                              average='weighted')
print 'Precision:', precision_score(y_test, weighted_prediction,
                                    average='weighted')
print '\n clasification report:\n', classification_report(y_test, weighted_prediction)
print '\n confussion matrix:\n',confusion_matrix(y_test, weighted_prediction)

Second:
auto_wclf = SVC(kernel='linear', C= 1, class_weight='auto')
auto_wclf.fit(X, y)
auto_weighted_prediction = auto_wclf.predict(X_test)

print 'Accuracy:', accuracy_score(y_test, auto_weighted_prediction)

print 'F1 score:', f1_score(y_test, auto_weighted_prediction,
                            average='weighted')

print 'Recall:', recall_score(y_test, auto_weighted_prediction,
                              average='weighted')

print 'Precision:', precision_score(y_test, auto_weighted_prediction,
                                    average='weighted')

print '\n clasification report:\n', classification_report(y_test,auto_weighted_prediction)

print '\n confussion matrix:\n',confusion_matrix(y_test, auto_weighted_prediction)

Third:
clf = SVC(kernel='linear', C= 1)
clf.fit(X, y)
prediction = clf.predict(X_test)


from sklearn.metrics import precision_score, \
    recall_score, confusion_matrix, classification_report, \
    accuracy_score, f1_score

print 'Accuracy:', accuracy_score(y_test, prediction)
print 'F1 score:', f1_score(y_test, prediction)
print 'Recall:', recall_score(y_test, prediction)
print 'Precision:', precision_score(y_test, prediction)
print '\n clasification report:\n', classification_report(y_test,prediction)
print '\n confussion matrix:\n',confusion_matrix(y_test, prediction)


F1 score:/usr/local/lib/python2.7/site-packages/sklearn/metrics/classification.py:676: DeprecationWarning: The default `weighted` averaging is deprecated, and from version 0.18, use of precision, recall or F-score with multiclass or multilabel data or pos_label=None will result in an exception. Please set an explicit value for `average`, one of (None, 'micro', 'macro', 'weighted', 'samples'). In cross validation use, for instance, scoring=""f1_weighted"" instead of scoring=""f1"".
  sample_weight=sample_weight)
/usr/local/lib/python2.7/site-packages/sklearn/metrics/classification.py:1172: DeprecationWarning: The default `weighted` averaging is deprecated, and from version 0.18, use of precision, recall or F-score with multiclass or multilabel data or pos_label=None will result in an exception. Please set an explicit value for `average`, one of (None, 'micro', 'macro', 'weighted', 'samples'). In cross validation use, for instance, scoring=""f1_weighted"" instead of scoring=""f1"".
  sample_weight=sample_weight)
/usr/local/lib/python2.7/site-packages/sklearn/metrics/classification.py:1082: DeprecationWarning: The default `weighted` averaging is deprecated, and from version 0.18, use of precision, recall or F-score with multiclass or multilabel data or pos_label=None will result in an exception. Please set an explicit value for `average`, one of (None, 'micro', 'macro', 'weighted', 'samples'). In cross validation use, for instance, scoring=""f1_weighted"" instead of scoring=""f1"".
  sample_weight=sample_weight)
 0.930416613529

However, Im getting warnings like this:
/usr/local/lib/python2.7/site-packages/sklearn/metrics/classification.py:1172:
DeprecationWarning: The default `weighted` averaging is deprecated,
and from version 0.18, use of precision, recall or F-score with 
multiclass or multilabel data or pos_label=None will result in an 
exception. Please set an explicit value for `average`, one of (None, 
'micro', 'macro', 'weighted', 'samples'). In cross validation use, for 
instance, scoring=""f1_weighted"" instead of scoring=""f1""

How can I deal correctly with my unbalanced data in order to compute in the right way classifier's metrics?
","I think there is a lot of confusion about which weights are used for what. I am not sure I know precisely what bothers you so I am going to cover different topics, bear with me ;).
Class weights
The weights from the class_weight parameter are used to train the classifier.
They are not used in the calculation of any of the metrics you are using: with different class weights, the numbers will be different simply because the classifier is different.
Basically in every scikit-learn classifier, the class weights are used to tell your model how important a class is. That means that during the training, the classifier will make extra efforts to classify properly the classes with high weights.
How they do that is algorithm-specific. If you want details about how it works for SVC and the doc does not make sense to you, feel free to mention it.
The metrics
Once you have a classifier, you want to know how well it is performing.
Here you can use the metrics you mentioned: accuracy, recall_score, f1_score...
Usually when the class distribution is unbalanced, accuracy is considered a poor choice as it gives high scores to models which just predict the most frequent class.
I will not detail all these metrics but note that, with the exception of accuracy, they are naturally applied at the class level: as you can see in this print of a classification report they are defined for each class. They rely on concepts such as true positives or false negative that require defining which class is the positive one.
             precision    recall  f1-score   support

          0       0.65      1.00      0.79        17
          1       0.57      0.75      0.65        16
          2       0.33      0.06      0.10        17
avg / total       0.52      0.60      0.51        50

The warning
F1 score:/usr/local/lib/python2.7/site-packages/sklearn/metrics/classification.py:676: DeprecationWarning: The 
default `weighted` averaging is deprecated, and from version 0.18, 
use of precision, recall or F-score with multiclass or multilabel data  
or pos_label=None will result in an exception. Please set an explicit 
value for `average`, one of (None, 'micro', 'macro', 'weighted', 
'samples'). In cross validation use, for instance, 
scoring=""f1_weighted"" instead of scoring=""f1"".

You get this warning because you are using the f1-score, recall and precision without defining how they should be computed!
The question could be rephrased: from the above classification report, how do you output one global number for the f1-score?
You could:

Take the average of the f1-score for each class: that's the avg / total result above. It's also called macro averaging.
Compute the f1-score using the global count of true positives / false negatives, etc. (you sum the number of true positives / false negatives for each class). Aka micro averaging.
Compute a weighted average of the f1-score. Using 'weighted' in scikit-learn will weigh the f1-score by the support of the class: the more elements a class has, the more important the f1-score for this class in the computation.

These are 3 of the options in scikit-learn, the warning is there to say you have to pick one. So you have to specify an average argument for the score method.
Which one you choose is up to how you want to measure the performance of the classifier: for instance macro-averaging does not take class imbalance into account and the f1-score of class 1 will be just as important as the f1-score of class 5. If you use weighted averaging however you'll get more importance for the class 5.
The whole argument specification in these metrics is not super-clear in scikit-learn right now, it will get better in version 0.18 according to the docs. They are removing some non-obvious standard behavior and they are issuing warnings so that developers notice it.
Computing scores
Last thing I want to mention (feel free to skip it if you're aware of it) is that scores are only meaningful if they are computed on data that the classifier has never seen.
This is extremely important as any score you get on data that was used in fitting the classifier is completely irrelevant.
Here's a way to do it using StratifiedShuffleSplit, which gives you a random splits of your data (after shuffling) that preserve the label distribution.
from sklearn.datasets import make_classification
from sklearn.cross_validation import StratifiedShuffleSplit
from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report, confusion_matrix

# We use a utility to generate artificial classification data.
X, y = make_classification(n_samples=100, n_informative=10, n_classes=3)
sss = StratifiedShuffleSplit(y, n_iter=1, test_size=0.5, random_state=0)
for train_idx, test_idx in sss:
    X_train, X_test, y_train, y_test = X[train_idx], X[test_idx], y[train_idx], y[test_idx]
    svc.fit(X_train, y_train)
    y_pred = svc.predict(X_test)
    print(f1_score(y_test, y_pred, average=""macro""))
    print(precision_score(y_test, y_pred, average=""macro""))
    print(recall_score(y_test, y_pred, average=""macro""))

",scikit-learn
how to perform standardization and normalization on features from different feature engineering process,"I'm working with a dataset where each sample contains both numeric and text data. Therefore multiple methods are employed to build the training feature matrix from the dataset. For each sample in the dataset, I construct a vector representation from 3 parts.

Doc2Vec vector representation for paragraph text: I use the gensim implemetation of paragraph vector to encode the text into a 100-D vetors of floats between [-5, 5]

One-hot encoded vector for text label: Each sample in the dataset has zero or more text label, I aggregate out all of the unique labels used in the dataset and encode it into a binary array containing only 0 and 1. For example, if the complete set of labels is [Python, Java, JavaScript, C++] and a sample contains labels Python and Java, the resulted vector will be [1, 1, 0, 0].

Numeric data & categorical data:

Numeric data fields are built into the feature vector as is
Categorical data are mapped to integers and built into the feature vector



The resulted feature matrix looks something like below
[
  [-1.02, 1.33, 2.35, -0.48, ... -4.11, 1, 0, 1, 1, 0, 0, ..., 1, 0, 235, 11.5, 333],
  [-0.22, 3.03, 1.95, -0.48, ... -4.11, 0, 1, 1, 1, 0, 0, ..., 0, 0, 233, 22, 333],
  [-2.07, -1.33, -2.35, -0.48, ... -4.11, 1, 1, 0, 1, 1, 0, ..., 1, 1, 102, 13, 333],
  [-4.32, 4.33, 1.75, -0.48, ... -4.11, 0, 0, 0, 1, 0, 1, ..., 1, 0, 98, 8, 333],
]

Should I apply any standardization or normalization on the dataset? If so, should I do it before or after concatenating different parts of feature?
I'm using scikit-learn and the major algorithm I using will be Gradient Boosting.
","Yes, you need to process features separately: you should apply standardization or normalization only on the original numerical features, you shouldn't do it for doc2vec, OHE or encoded categorical features.
",scikit-learn
getting error attributeerror 39bool39 object has no attribute 39transpose39 when attempting to fit machine learning model,"I am trying to create a machine learning model to predict who would survive on the Titanic. Everytime I try to fit my model, I get this error :
    coordinates = np.where(mask.transpose())[::-1]
AttributeError: 'bool' object has no attribute 'transpose'

The code I am running is the following :

from xgboost import XGBClassifier
from sklearn.preprocessing import OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
from sklearn.feature_selection import SelectFromModel
from itertools import combinations
import pandas as pd 
import numpy as np

#read in data
training_data = pd.read_csv('train.csv')
testing_data = pd.read_csv('test.csv')




#seperate X and Y
X_train_full = training_data.copy()
y = X_train_full.Survived
X_train_full.drop(['Survived'], axis=1, inplace=True)

y_test = testing_data

#get all str columns
cat_columns1 = [cname for cname in X_train_full.columns if
                    X_train_full[cname].dtype == ""object""]

interactions = pd.DataFrame(index= X_train_full)

#create new features
for combination in combinations(cat_columns1,2):
    imputer = SimpleImputer(strategy='constant')

    new_col_name = '_'.join(combination)
    col1 = X_train_full[combination[0]]
    col2 = X_train_full[combination[1]]
    col1 = np.array(col1).reshape(-1,1)
    col2 = np.array(col2).reshape(-1,1)
    col1 = imputer.fit_transform(col1)
    col2 = imputer.fit_transform(col2)


    new_vals = col1 + '_' + col2
    OneHot = OneHotEncoder()




    interactions[new_col_name] = OneHot.fit_transform(new_vals)
 

interactions = interactions.reset_index(drop = True)


#create new dataframe with new features included
new_df = X_train_full.join(interactions)
 

#do the same for the test file
interactions2 = pd.DataFrame(index= y_test)
for combination in combinations(cat_columns1,2):
    imputer = SimpleImputer(strategy='constant')

    new_col_name = '_'.join(combination)
    col1 = y_test[combination[0]]
    col2 = y_test[combination[1]]
    col1 = np.array(col1).reshape(-1,1)
    col2 = np.array(col2).reshape(-1,1)
    col1 = imputer.fit_transform(col1)
    col2 = imputer.fit_transform(col2)


    new_vals = col1 + '_' + col2

    OneHot = OneHotEncoder()




    interactions2[new_col_name] = OneHot.fit_transform(new_vals)


    interactions2[new_col_name] = new_vals
 

interactions2 = interactions2.reset_index(drop = True)
y_test = y_test.join(interactions2)


#get names of cat columns (with new features added)
cat_columns = [cname for cname in new_df.columns if
                    new_df[cname].dtype == ""object""]

# Select numerical columns
num_columns = [cname for cname in new_df.columns if 
                new_df[cname].dtype in ['int64', 'float64']]



#set up pipeline
numerical_transformer = SimpleImputer(strategy = 'constant')


categorical_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='constant')),
    ('onehot', OneHotEncoder(handle_unknown='ignore'))
])


preprocessor = ColumnTransformer(
    transformers=[
        ('num', numerical_transformer, num_columns),
        ('cat', categorical_transformer, cat_columns)
    ])
model = XGBClassifier()

my_pipeline = Pipeline(steps=[('preprocessor', preprocessor),
                              ('model', model)
                             ])
#fit model
my_pipeline.fit(new_df,y)


The csv files I am reading are available from Kaggle at this link :
https://www.kaggle.com/c/titanic/data
I cannot figure out what is causing this problem. Any help would be much appreciated.
","This probably happens because your data contains pd.NA values. pd.NA was introduced in pandas 1.0.0, but is still marked as experimental.
SimpleImputer will ultimately run data == np.nan, which would usually return a numpy array. In stead, it is returning a single boolean scalar when data contains pd.NA values.
An example:
import pandas as pd
import numpy as np

test_pd_na = pd.DataFrame({""A"": [1, 2, 3, pd.NA]})
test_np_nan = pd.DataFrame({""A"": [1, 2, 3, np.nan]})

test_np_nan.to_numpy() == np.nan:
> array([[False],
       [False],
       [False],
       [False]])

test_pd_na.to_numpy() == np.nan

> False

The solution would be to convert all pd.NA values to np.nan before running SimpleImputer. You can use .replace({pd.NA: np.nan})on your data frames for this purpose. The downside is obviously that you loose the benefits pd.NA brings, such as integer columns with missing data, in stead of those columns being converted to float columns.
",scikit-learn
how is the hidden layer size determined for mlpregressor in scikitlearn,"Lets say I'm creating a neural net using the following code:
from sklearn.neural_network import MLPRegressor

model = MLPRegressor(
  hidden_layer_sizes=(100,),
  activation='identity'
)
model.fit(X_train, y_train)

For the hidden_layer_sizes, I simply set it to the default. However, I don't really understand how it works. What is the number of hidden layers in my definition? Is it 100?
","From the docs:

hidden_layer_sizes : tuple, length = n_layers - 2, default (100,)
The ith element represents the number of neurons in the ith hidden layer.

It is length = n_layers - 2, because the number of your hidden layers is the total number of layers n_layers minus 1 for your input layer, minus 1 for your output layer.
In your (default) case of (100,), it means one hidden layer of 100 units (neurons).
For 3 hidden layers of, say, 100, 50, and 25 units respectively, it would be
hidden_layer_sizes = (100, 50, 25)

See the example in the docs (it is for MLPClassifier, but the logic is identical).
",scikit-learn
grid search over multiple classifiers,"Is there  a better  inbuilt way to do grid search and test multiple models in a single pipeline? Of course the parameters of the models would be different, which made is complicated for me to figure this out. Here is what I did:
from sklearn.pipeline import Pipeline
from sklearn.ensemble import RandomForestClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC
from sklearn.naive_bayes import MultinomialNB
from sklearn.grid_search import GridSearchCV


def grid_search():
    pipeline1 = Pipeline((
    ('clf', RandomForestClassifier()),
    ('vec2', TfidfTransformer())
    ))

    pipeline2 = Pipeline((
    ('clf', KNeighborsClassifier()),
    ))

    pipeline3 = Pipeline((
    ('clf', SVC()),
    ))

    pipeline4 = Pipeline((
    ('clf', MultinomialNB()),
    ))
    
    parameters1 = {
    'clf__n_estimators': [10, 20, 30],
    'clf__criterion': ['gini', 'entropy'],
    'clf__max_features': [5, 10, 15],
    'clf__max_depth': ['auto', 'log2', 'sqrt', None]
    }

    parameters2 = {
    'clf__n_neighbors': [3, 7, 10],
    'clf__weights': ['uniform', 'distance']
    }

    parameters3 = {
    'clf__C': [0.01, 0.1, 1.0],
    'clf__kernel': ['rbf', 'poly'],
    'clf__gamma': [0.01, 0.1, 1.0],

    }
    parameters4 = {
    'clf__alpha': [0.01, 0.1, 1.0]
    }

    pars = [parameters1, parameters2, parameters3, parameters4]
    pips = [pipeline1, pipeline2, pipeline3, pipeline4]
    
    print ""starting Gridsearch""
    for i in range(len(pars)):
        gs = GridSearchCV(pips[i], pars[i], verbose=2, refit=False, n_jobs=-1)
        gs = gs.fit(X_train, y_train)
        print ""finished Gridsearch""
        print gs.best_score_

However, this approach is still giving the best model within each classifier, and not comparing between classifiers.
","Instead of using Grid Search for hyperparameter selection, you can use the 'hyperopt' library.
Please have a look at section 2.2 of this page. In the above case, you can use an hp.choice expression to select among the various pipelines and then define the parameter expressions for each one separately.
In your objective function, you need to have a check depending on the pipeline chosen and return the CV score for the selected pipeline and parameters (possibly via cross_val_score).
The trials object at the end of the execution, will indicate the best pipeline and parameters overall.
",scikit-learn
how to predict time series in scikitlearn,"Scikit-learn utilizes a very convenient approach based on fit and predict methods. I have time-series data in the format suited for fit and predict.
For example I have the following Xs:
[[1.0, 2.3, 4.5], [6.7, 2.7, 1.2], ..., [3.2, 4.7, 1.1]]

and the corresponding ys:
[[1.0], [2.3], ..., [7.7]]

These data have the following meaning. The values stored in ys form a time series. The values in Xs are corresponding time dependent ""factors"" that are known to have some influence on the values in ys (for example: temperature, humidity and atmospheric pressure).
Now, of course, I can use fit(Xs,ys). But then I get a model in which future values in ys depend only on factors and do not dependend on the previous Y values (at least directly) and this is a limitation of the model. I would like to have a model in which Y_n depends also on Y_{n-1} and Y_{n-2} and so on. For example I might want to use an exponential moving average as a model. What is the most elegant way to do it in scikit-learn
ADDED
As it has been mentioned in the comments, I can extend Xs by adding ys. But this way has some limitations. For example, if I add the last 5 values of y as 5 new columns to X, the information about time ordering of ys is lost. For example, there is no indication in X that values in the 5th column follows value in the 4th column and so on. As a model, I might want to have a linear fit of the last five ys and use the found linear function to make a prediction. But if I have 5 values in 5 columns it is not so trivial.
ADDED 2
To make my problem even more clear, I would like to give one concrete example. I would like to have a ""linear"" model in which y_n = c + k1*x1 + k2*x2 + k3*x3 + k4*EMOV_n, where EMOV_n is just an exponential moving average. How, can I implement this simple model in scikit-learn?
","This might be what you're looking for, with regard to the exponentially weighted moving average:
import pandas, numpy
ewma = pandas.stats.moments.ewma
EMOV_n = ewma( ys, com=2 )

Here, com is a parameter that you can read about here. Then you can combine EMOV_n to Xs, using something like:
Xs = numpy.vstack((Xs,EMOV_n))

And then you can look at various linear models, here, and do something like:
from sklearn import linear_model
clf = linear_model.LinearRegression()
clf.fit ( Xs, ys )
print clf.coef_

",scikit-learn
forecasting with time series in python,"I want to predict the next values of a variable Y (c_start) when X (day) represent the time.
As you can see in the picture, i have values for the attribute ""c_start"" and I would like to predict the next ""c_start"" values for the next 7 days(for example).
May someone help me?


","To examine the ARMA model in a sample group:
import pandas as pd
from pandas.tseries.offsets import *
import numpy as np
import matplotlib.pyplot as plt
import statsmodels.api as sm

csv_file = '/home/Jian/Downloads/analyse vtr.csv'
df = pd.read_csv(csv_file, index_col=[0], sep='\t')
grouped = df.groupby('adserver_id')
group = list(grouped)[0][1]

ts_data = pd.TimeSeries(group.c_start.values, index=pd.to_datetime(group.day))
# positive-valued process, looks non-stationary
# simple way is to do a log transform
fig, axes = plt.subplots(figsize=(10,8), nrows=3)
ts_data.plot(ax=axes[0])

ts_log_data = np.log(ts_data)
ts_log_data.plot(ax=axes[1], style='b-', label='actual')

# in-sample fit
# ===================================
model = sm.tsa.ARMA(ts_log_data, order=(1,1)).fit()
print(model.params)

y_pred = model.predict(ts_log_data.index[0].isoformat(), ts_log_data.index[-1].isoformat())
y_pred.plot(ax=axes[1], style='r--', label='in-sample fit')

y_resid = model.resid
y_resid.plot(ax=axes[2])

# out-sample predict
# ===================================
start_date = ts_log_data.index[-1] + Day(1)
end_date = ts_log_data.index[-1] + Day(7)

y_forecast = model.predict(start_date.isoformat(), end_date.isoformat())

print(y_forecast)


2015-07-11    7.5526
2015-07-12    7.4584
2015-07-13    7.3830
2015-07-14    7.3224
2015-07-15    7.2739
2015-07-16    7.2349
2015-07-17    7.2037
Freq: D, dtype: float64


# NOTE: this step introduces bias, it is used here just for simplicity
# E[exp(x)] != exp[E[x]]
print(np.exp(y_forecast))

2015-07-11    1905.6328
2015-07-12    1734.4442
2015-07-13    1608.3362
2015-07-14    1513.8595
2015-07-15    1442.1183
2015-07-16    1387.0486
2015-07-17    1344.4080
Freq: D, dtype: float64


To run the ARMA model for each subgroup (really time consuming):
import pandas as pd
from pandas.tseries.offsets import *
import numpy as np
import matplotlib.pyplot as plt
import statsmodels.api as sm

csv_file = '/home/Jian/Downloads/analyse vtr.csv'
df = pd.read_csv(csv_file, index_col=[0], sep='\t')
grouped = df.groupby('adserver_id')


def forecast_func(group):
    ts_log_data = np.log(pd.TimeSeries(group.c_start.values, index=pd.to_datetime(group.day)))
    # for some group, it raise convergence issue
    try:
        model = sm.tsa.ARMA(ts_log_data, order=(1,1)).fit()
        start_date = ts_log_data.index[-1] + Day(1)
        end_date = ts_log_data.index[-1] + Day(7)
        y_forecast = model.predict(start_date.isoformat(), end_date.isoformat())
        return pd.Series(np.exp(y_forecast).values, np.arange(1, 8))
    except Exception:
        pass


result = df.groupby('adserver_id').apply(forecast_func)

Alternative models: for fast computation, consider exponential smoothing; Also, I see the data looks like a positive-valued process with a time-varying Possion distribution, might consider state-space model using pymc module.
",scikit-learn
train machine learning model with scikit learn for timeseries prediction,"I need to train a model with scikit-learn to predict possible time for less  people in a room.
Here is how my dataset looks like:
Time                          PeopleCount
---------------------------------------------
2019-12-29 12:40:10               50
2019-12-29 12:42:10               30
2019-12-29 12:44:10               10
2019-12-29 12:46:10               10
2019-12-29 12:48:10               80
and so on...

This data will be available for 30 days.
Once the model is trained, I will query the model to get the possible time when there will be fewer people in the room between 10.AM and 8.PM. I expect the machine learning model to respond back with the 30-minute accuracy, ie. ""3.00 PM to 3.30PM""
What algorithm can I use for this problem and how can I achieve the goal? Or are there any other Python libraries than SciKit-Learn which can be used for this purpose?
","First of all, time-series prediction is on the base of theory that current value more or less depend on the past ones. For instance, 80 of people count as of 2019-12-29 12:48:10 has to be strongly influenced on the people count at the time of 12:46:10, 12:44:20 or previous ones, correlating with past values. If not, you would be better off using the other algorithm for prediction. 
While the scikit package contains a various modules as the machine learning algorithm, most of them specialize in the classification algorithm. I think the classification algorithm certainly satisfy your demand if your date is not identified as the type of time series. Actually, scikit also has some regression module, even though I think that seem not to be well suitable for prediction of time series data.
In the case of prediction of time series data, RNN or LSTM algorithm (Deep Learning) has been widely utilized, but scikit does not provide the build-in algorithm of it. So, you might be better off studying Tensorflow or Pytorch framework which are common tools to be enable you to build the RNN or LSTM model.     
",scikit-learn
advice on how to predict future time series data,"I have time series data for different countries and factors, e.g. birth rate for ""Afghanistan"" for years from 1972 up until 2007 (source).
Goal:
Predict e.g. birth rate for 2008 and 2012
I am familiar with linear regressions, but need some help on how to work with time series data and predict future values.
Can you point me to examples or share code snippets?
","Take a look at the statsmodels Time Series Analysis module. Time series models are often based around autocorrelation, and the module has the standard univariate (for individual time series) AR(p) and MA(p) models, as well as the combined version ARIMA that allows for unit roots. You'll also find multivariate (for various interrelated time series) VAR models.
And here's a time series tutorial for statistical analysis and forecasting using pandas and statsmodels.
",scikit-learn
precision calculation warning when using gridsearchcv for logistic regression,"I am trying to run GridSearchCV with the LogisticRegression estimator and record the model accuracy, precision, recall, f1 metrics.
However, I get the following error on the precision metric:
Precision is ill-defined and being set to 0.0 due to no predicted samples. 
Use `zero_division` parameter to control this behavior

I understand why I am getting the error as there are no predictions with output value equal to 1 in the Kfold split. However I don't understand how I can specific set ""zero_divison"" as 1 in GridSearchCV (logistic_reg variable).
Original code
logistic_reg = GridSearchCV(estimator=LogisticRegression(penalty=""l1"", random_state=42, max_iter=10000), param_grid={
        ""C"": [1e-4, 5e-4, 1e-3, 5e-3, 1e-2, 5e-2, 1e-1, 5e-1, 1, 5, 10, 20],
        ""solver"": [""liblinear"", ""saga""]
        }, scoring=[""accuracy"", ""precision"", ""recall"", ""f1""], cv=StratifiedKFold(n_splits=10), refit=""accuracy"")
    
logistic_reg_X_train = self.X_train.copy()
logistic_reg_X_train.drop(self.columns_removed, axis=1, inplace=True)
    
logistic_reg.fit(logistic_reg_X_train, self.y_train)
logistic_reg_results = pd.DataFrame(logistic_reg.cv_results_)

I tried changing ""precision"" to precision_score(zero_division=1) but this gives me another error (missing 2 required positional arguments: 'y_true' and 'y_pred'). Again I understand this but the 2 missing parameters are not defined before applying the fit method.
How can I specify the 1zero_division parameter to the precision score metric?
Edit
What I don't understand is that I stratified the y data in my train_test_split method and used the StratifedKFold in the GridSearchCV. My understanding from this is that the train/test data will have the same split proportion of y values and the same should happen during cross validation. This means that in the gridsearchcv samples, the data should have y values of both 0 and 1 and thus precision cannot equal 0 (model will be able to calculate TP and FP as the sample test data contains samples where y is equal to 1). I'm not sure where to go from here.
","From reading further into this issue, my understanding is that the error is occurring because not all the labels in my y_test are appearing in my y_pred. This is not the case for my data.
I used the comment from G.Anderson to remove the warning (but it doesn't answer my question)

Created new custom_scorer object

Created customer_scoring dictionary

Updated GridSearchCV scoring and refit parameters
from sklearn.metrics import precision_score, make_scorer

precision_scorer = make_scorer(precision_score, zero_division=0)

custom_scoring = {""accuracy"": ""accuracy"", ""precision"": precision_scorer, ""recall"": ""recall"", ""f1"": ""f1""}

logistic_reg = GridSearchCV(estimator=LogisticRegression(penalty=""l1"", random_state=42, max_iter=10000), param_grid={
      ""C"": [1e-4, 5e-4, 1e-3, 5e-3, 1e-2, 5e-2, 1e-1, 5e-1, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 20],
      ""solver"": [""liblinear"", ""saga""]
      }, scoring=custom_scoring, cv=StratifiedKFold(n_splits=10), refit=""accuracy"")



Edit - Answer to Question Above
I used GridSearchCV to find the best hyperparameters for the model. To view the model metrics for each split, I create a StratifedKFold estimator with the best hyperparameters and then did cross validation on its own. This gave me no precision warning messages. I have no idea why GridSearchCV is giving me a warning but atleast this way works!!!
Note: I get the same results from the method below and GridSearchCV in the question above.
skf = StratifiedKFold(n_splits=10)
logistic_reg_class_skf = LogisticRegression(penalty=""l1"", max_iter=10000, random_state=42, C=5, solver=""liblinear"")
    
logistic_reg_class_score = []
                    
for train, test in skf.split(logistic_reg_class_X_train, self.y_train):
        
    logistic_reg_class_skf_X_train = logistic_reg_class_X_train.iloc[train]
    logistic_reg_class_skf_X_test = logistic_reg_class_X_train.iloc[test]
    logistic_reg_class_skf_y_train = self.y_train.iloc[train]
    logistic_reg_class_skf_y_test = self.y_train.iloc[test]
        
    logistic_reg_class_skf.fit(logistic_reg_class_skf_X_train, logistic_reg_class_skf_y_train)
    logistic_reg_skf_y_pred = logistic_reg_class_skf.predict(logistic_reg_class_skf_X_test)
        
    skf_accuracy_score = metrics.accuracy_score(logistic_reg_class_skf_y_test, logistic_reg_skf_y_pred)
    skf_precision_score = metrics.precision_score(logistic_reg_class_skf_y_test, logistic_reg_skf_y_pred)
    skf_recall_score = metrics.recall_score(logistic_reg_class_skf_y_test, logistic_reg_skf_y_pred)
    skf_f1_score = metrics.f1_score(logistic_reg_class_skf_y_test, logistic_reg_skf_y_pred)

    logistic_reg_class_score.append([skf_accuracy_score, skf_precision_score, skf_recall_score, skf_f1_score])

    classification_results = pd.DataFrame({""Algorithm"": [""Logistic Reg Train""], ""Accuracy"": [0.0], ""Precision"": [0.0],
                                            ""Recall"": [0.0], ""F1 Score"": [0.0]})
    
    for i in range (0, 10):
        classification_results.loc[i] = [""Logistic Reg Train"", logistic_reg_class_score[i][0], logistic_reg_class_score[i][1],
                                         logistic_reg_class_score[2][0], logistic_reg_class_score[3][0]]

",scikit-learn
how to preserve data types when working with pandas and sklearn transformers,"While working with a large sklearn Pipeline (fit using a DataFrame) I ran into an error that lead back to a wrong data type of my input. The problem occurred on an a single observation coming from an API that is supposed to interface the model in production. Missing information in a single line makes pandas (obviously) incapable of inferring the correct dtype but I thought that my fit transformers will handle the conversions. Apparently, I am mistaken.
import pandas as pd
from sklearn.impute import SimpleImputer

X_tr = pd.DataFrame({""A"": [5, 8, None, 4], ""B"": [3, None, 9.9, 12]})
print(X_tr.dtypes)  

#>> A    float64
#>> B    float64

x = pd.DataFrame({""A"": [10.1], ""B"": [None]})
print(x.dtypes)

#>> A    float64
#>> B    object

The above shows clearly that pandas infers the float64 types for column A and B in the training dataset, however (again obviously) for the single observation it doesn't know the dtype for the column B so it assigns object. No issue so far. But let's imagine a SimpleImputer somewhere within a Pipeline to replace the missing values:
imputer = SimpleImputer(
    fill_value=0, strategy=""constant"", missing_values=pd.NA
).set_output(transform=""pandas"")

X_tr_im = imputer.fit_transform(X_tr)  # training
print(X_tr_im.dtypes)

#>> A    float64
#>> B    float64

x_im = imputer.transform(x)
print(x_im.dtypes)

#>> A    object
#>> B    object

The imputer does replace the None values with zeros in all cases, however, two things happened that I did not expect:

Column B was NOT converted to the dtype that it was fit on
Column A was converted to the unwanted dtype of object

This creates two unwanted non-numeric data types that lead to errors further down the pipeline. Even if it is not the task of transformers to preserve dtypes, in my case it would still be very helpful.
Am I doing something fundamentally wrong? Are there any solutions available?
","The issue you are running into is how pandas handles None in a column.  If the column has other float or integer values, the None is coerced to a numpy.nan which is an instance of float.  This coercion maintains the column's type as a numeric column.
However, if no other values are present in the column, just None values, pandas does NOT try to coerce the column to float, and instead keeps it as a column of Python object types, which is what you are seeing when you try to impute x.
To ensure your datafame's columns are converted to a numeric type before being passed through the rest of the pipeline, you can use a sklearn.preprocessing.FunctionTransformer before the imputer, and use a function that forces the date type to np.float64 before any other computation happens.
import pandas as pd
from sklearn.impute import SimpleImputer
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import FunctionTransformer

def df_to_float(x):
    return x.astype(np.float64)

x_tr = pd.DataFrame({""A"": [5, 8, None, 4], ""B"": [3, None, 9.9, 12]})
x = pd.DataFrame({""A"": [10.1], ""B"": [None]})

float_xform = FunctionTransformer(df_to_float)

imputer = SimpleImputer(
    fill_value=0, 
    strategy=""constant"",
    missing_values=pd.NA
).set_output(transform=""pandas"")

pipe = Pipeline([('float-transform', float_xform), ('impute-NA', imputer)])

print(pipe.fit_transform(x_tr).dtypes)
# A    float64
# B    float64

print(pipe.transform(x).dtypes)
# A    float64
# B    float64

",scikit-learn
what is the difference between transformer and estimator in sklearn,"I saw both transformer and estimator were mentioned in the sklearn documentation. 
Is there any difference between these two words?
","The basic difference is that a:

Transformer transforms the input data (X) in some ways. 
Estimator predicts a new value (or values) (y) by using the input data (X). 

Both the Transformer and Estimator should have a fit() method which can be used to train them (they learn some characteristics of the data). The signature is:
fit(X, y)

fit() does not return any value, just stores the learnt data inside the object.
Here X represents the samples (feature vectors) and y is the target vector (which may have single or multiple values per corresponding sample in X). Note that y can be optional in some transformers where its not needed, but its mandatory for most estimators (supervised estimators). Look at StandardScaler for example. It needs the initial data X for finding the mean and std of the data (it learns the characteristics of X, y is not needed). 
Each Transformer should have a transform(X, y) function which like fit() takes the input X and returns a new transformed version of X (which generally should have same number samples but may or may not have same features).
On the other hand, Estimator should have a predict(X) method which should output the predicted value of y from the given X.
There will be some classes in scikit-learn which implement both transform() and predict(), like KMeans, in that case carefully reading the documentation should solve your doubts.
",scikit-learn
inconsistent numbers of samples error in multiclass  multilabel machine learning model,"I have a OneVsOne model running fine with textual feature and target fields. To progress to a multi-class model (i.e. with multiple textual feature fields), I believe OneVsRest with Logistic Regression is suitable.
However, when I use the following pipeline:
model = Pipeline([
    ('vect', CountVectorizer()),
    ('tfidfT', TfidfTransformer()),
    ('clf', OneVsRestClassifier(LogisticRegression(), n_jobs = 1))
    ])

I am getting the following error when trying to run a OneVsRest classifier with Logistic Regression:
ValueError: Found input variables with inconsistent numbers of samples: [3, 224]

The feature fields are in a pandas dataframe of 224 rows and the target field is a pandas series of length 224. There are no nulls in the data.
Here is the full traceback:
ValueError                                Traceback (most recent call last)
File c:\Users\u363028\OneDrive - IBERDROLA S.A\Tools\sklearn\defect_autocategorisation_main9.py:127
    119 model = Pipeline([
    120     ('vect', CountVectorizer()),
    121     ('tfidfT', TfidfTransformer()),
    122     ('clf', OneVsRestClassifier(LogisticRegression(), n_jobs = 1))
    123     ])
    124 #model = OneVsRestClassifier(LogisticRegression())
    125 
    126 # Initialize the classifier
--> 127 model.fit(X,y)
    128 predicted = model.predict(X_test)
    129 #predicted = model.predict(X_test, Y_test)
    130    
    131 # creating a confusion matrix  
   (...)
    140 
    141 # Generate classification report

File c:\Users\u363028\OneDrive - IBERDROLA S.A\Tools\sklearn\sk_learn\Lib\site-packages\sklearn\base.py:1473, in _fit_context.<locals>.decorator.<locals>.wrapper(estimator, *args, **kwargs)
   1466     estimator._validate_params()
   1468 with config_context(
   1469     skip_parameter_validation=(
   1470         prefer_skip_nested_validation or global_skip_validation
   1471     )
   1472 ):
-> 1473     return fit_method(estimator, *args, **kwargs)

File c:\Users\u363028\OneDrive - IBERDROLA S.A\Tools\sklearn\sk_learn\Lib\site-packages\sklearn\pipeline.py:473, in Pipeline.fit(self, X, y, **params)
    471     if self._final_estimator != ""passthrough"":
    472         last_step_params = routed_params[self.steps[-1][0]]
--> 473         self._final_estimator.fit(Xt, y, **last_step_params[""fit""])
    475 return self

File c:\Users\u363028\OneDrive - IBERDROLA S.A\Tools\sklearn\sk_learn\Lib\site-packages\sklearn\base.py:1473, in _fit_context.<locals>.decorator.<locals>.wrapper(estimator, *args, **kwargs)
   1466     estimator._validate_params()
   1468 with config_context(
   1469     skip_parameter_validation=(
   1470         prefer_skip_nested_validation or global_skip_validation
   1471     )
   1472 ):
-> 1473     return fit_method(estimator, *args, **kwargs)

File c:\Users\u363028\OneDrive - IBERDROLA S.A\Tools\sklearn\sk_learn\Lib\site-packages\sklearn\multiclass.py:370, in OneVsRestClassifier.fit(self, X, y, **fit_params)
    366 columns = (col.toarray().ravel() for col in Y.T)
    367 # In cases where individual estimators are very fast to train setting
    368 # n_jobs > 1 in can results in slower performance due to the overhead
    369 # of spawning threads.  See joblib issue #112.
--> 370 self.estimators_ = Parallel(n_jobs=self.n_jobs, verbose=self.verbose)(
    371     delayed(_fit_binary)(
    372         self.estimator,
    373         X,
    374         column,
    375         fit_params=routed_params.estimator.fit,
    376         classes=[
    377             ""not %s"" % self.label_binarizer_.classes_[i],
    378             self.label_binarizer_.classes_[i],
    379         ],
    380     )
    381     for i, column in enumerate(columns)
    382 )
    384 if hasattr(self.estimators_[0], ""n_features_in_""):
    385     self.n_features_in_ = self.estimators_[0].n_features_in_

File c:\Users\u363028\OneDrive - IBERDROLA S.A\Tools\sklearn\sk_learn\Lib\site-packages\sklearn\utils\parallel.py:74, in Parallel.__call__(self, iterable)
     69 config = get_config()
     70 iterable_with_config = (
     71     (_with_config(delayed_func, config), args, kwargs)
     72     for delayed_func, args, kwargs in iterable
     73 )
---> 74 return super().__call__(iterable_with_config)

File c:\Users\u363028\OneDrive - IBERDROLA S.A\Tools\sklearn\sk_learn\Lib\site-packages\joblib\parallel.py:1918, in Parallel.__call__(self, iterable)
   1916     output = self._get_sequential_output(iterable)
   1917     next(output)
-> 1918     return output if self.return_generator else list(output)
   1920 # Let's create an ID that uniquely identifies the current call. If the
   1921 # call is interrupted early and that the same instance is immediately
   1922 # re-used, this id will be used to prevent workers that were
   1923 # concurrently finalizing a task from the previous call to run the
   1924 # callback.
   1925 with self._lock:

File c:\Users\u363028\OneDrive - IBERDROLA S.A\Tools\sklearn\sk_learn\Lib\site-packages\joblib\parallel.py:1847, in Parallel._get_sequential_output(self, iterable)
   1845 self.n_dispatched_batches += 1
   1846 self.n_dispatched_tasks += 1
-> 1847 res = func(*args, **kwargs)
   1848 self.n_completed_tasks += 1
   1849 self.print_progress()

File c:\Users\u363028\OneDrive - IBERDROLA S.A\Tools\sklearn\sk_learn\Lib\site-packages\sklearn\utils\parallel.py:136, in _FuncWrapper.__call__(self, *args, **kwargs)
    134     config = {}
    135 with config_context(**config):
--> 136     return self.function(*args, **kwargs)

File c:\Users\u363028\OneDrive - IBERDROLA S.A\Tools\sklearn\sk_learn\Lib\site-packages\sklearn\multiclass.py:93, in _fit_binary(estimator, X, y, fit_params, classes)
     91 else:
     92     estimator = clone(estimator)
---> 93     estimator.fit(X, y, **fit_params)
     94 return estimator

File c:\Users\u363028\OneDrive - IBERDROLA S.A\Tools\sklearn\sk_learn\Lib\site-packages\sklearn\base.py:1473, in _fit_context.<locals>.decorator.<locals>.wrapper(estimator, *args, **kwargs)
   1466     estimator._validate_params()
   1468 with config_context(
   1469     skip_parameter_validation=(
   1470         prefer_skip_nested_validation or global_skip_validation
   1471     )
   1472 ):
-> 1473     return fit_method(estimator, *args, **kwargs)

File c:\Users\u363028\OneDrive - IBERDROLA S.A\Tools\sklearn\sk_learn\Lib\site-packages\sklearn\linear_model\_logistic.py:1223, in LogisticRegression.fit(self, X, y, sample_weight)
   1220 else:
   1221     _dtype = [np.float64, np.float32]
-> 1223 X, y = self._validate_data(
   1224     X,
   1225     y,
   1226     accept_sparse=""csr"",
   1227     dtype=_dtype,
   1228     order=""C"",
   1229     accept_large_sparse=solver not in [""liblinear"", ""sag"", ""saga""],
   1230 )
   1231 check_classification_targets(y)
   1232 self.classes_ = np.unique(y)

File c:\Users\u363028\OneDrive - IBERDROLA S.A\Tools\sklearn\sk_learn\Lib\site-packages\sklearn\base.py:650, in BaseEstimator._validate_data(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)
    648         y = check_array(y, input_name=""y"", **check_y_params)
    649     else:
--> 650         X, y = check_X_y(X, y, **check_params)
    651     out = X, y
    653 if not no_val_X and check_params.get(""ensure_2d"", True):

File c:\Users\u363028\OneDrive - IBERDROLA S.A\Tools\sklearn\sk_learn\Lib\site-packages\sklearn\utils\validation.py:1320, in check_X_y(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)
   1301 X = check_array(
   1302     X,
   1303     accept_sparse=accept_sparse,
   (...)
   1315     input_name=""X"",
   1316 )
   1318 y = _check_y(y, multi_output=multi_output, y_numeric=y_numeric, estimator=estimator)
-> 1320 check_consistent_length(X, y)
   1322 return X, y

File c:\Users\u363028\OneDrive - IBERDROLA S.A\Tools\sklearn\sk_learn\Lib\site-packages\sklearn\utils\validation.py:457, in check_consistent_length(*arrays)
    455 uniques = np.unique(lengths)
    456 if len(uniques) > 1:
--> 457     raise ValueError(
    458         ""Found input variables with inconsistent numbers of samples: %r""
    459         % [int(l) for l in lengths]
    460     )

ValueError: Found input variables with inconsistent numbers of samples: [3, 224]

There is this similar Stack Overflow question:
ValueError: Number of features of the model must match the input
but neither the suggestions in this nor in any of the few other similar questions work for me.
Although my data is textual, for info the above pipeline causes the same error when using the Iris dataset but it completes successfully when just running the classifier (i.e. omitting the vectorizer and transformer). However, just running the classifier on my textual data doesn't work, giving the following expected error:
ValueError: could not convert string to float: 'Jacket'

I am aware of OneHot encoding but this 'inconsistent numbers of samples' problem seems irrespective of any encoding issue and I would like to solve this before tackling any other issues.
Edit 22/10/24:
Here is a Minimal Reproducible Example geared to use the iris dataset:
import pandas as pd
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.feature_extraction.text import TfidfTransformer
from sklearn.pipeline import Pipeline
from sklearn.model_selection import train_test_split
from sklearn.multiclass import OneVsRestClassifier
from sklearn.linear_model import LogisticRegression

# Read in the dataset to train the model
training_data = pd.read_csv('iris_dataset.csv')   
print(training_data) 

# Load feature data
X = training_data[['sepal.length', 'sepal.width','petal.length','petal.width']]
    
# Load target data
y = training_data['variety']

# Split training data into training and test portions
X_train, X_test, y_train, y_test \
    = train_test_split(X, y, test_size=0.5, random_state=42)
    
# Create the pipeline composed of vectoriser, transformer and classifier
model = Pipeline([
    ('vect', CountVectorizer()),
    ('tfidfT', TfidfTransformer()),
    ('clf', OneVsRestClassifier(LogisticRegression(), n_jobs = 1))
    ])

# Initialize the classifier
model.fit(X,y)
predicted = model.predict(X_test)

Edit 23/10/24: Here is the MRE again, self-contained with textual data inputs:
import pandas as pd
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.feature_extraction.text import TfidfTransformer
from sklearn.pipeline import Pipeline
from sklearn.model_selection import train_test_split
from sklearn.multiclass import OneVsRestClassifier
from sklearn.linear_model import LogisticRegression

# Read in the dataset to train the model
training_data = pd.read_csv('iris_dataset.csv')   
print(training_data)

training_data = pd.DataFrame({
                'Location': ['Structure', 'Stucture', 'Structure', 'Access systems'],\
                'Component': ['Mid bay brace12', 'Mid bay brace10', 'Mid bay brace07', 'First stage ladder'],\
                'Defect Description': ['Surface corrosion', 'Coating delamination with minor surface corrosion', 'Corrosion', 'Entangled rope'],\
                'Failure Mode': ['Corrosion', 'Corrosion','Corrosion', 'Debris']
                })


# Load feature data
X = training_data[['Location', 'Component','Defect Description']]
    
# Load target data
y = training_data['Failure Mode']

# Split training data into training and test portions
X_train, X_test, y_train, y_test \
    = train_test_split(X, y, test_size=0.5, random_state=42)
    
# Create the pipeline composed of vectoriser, transformer and classifier
model = Pipeline([
    ('vect', CountVectorizer()),
    ('tfidfT', TfidfTransformer()),
    ('clf', OneVsRestClassifier(LogisticRegression(), n_jobs = 1))
    ])

# Initialize the classifier
model.fit(X,y)
predicted = model.predict(X_test)

","I'll expand this later, but the main solution is to use a ColumnTransformer to run separate text transformers per column (I've also consolidated CountVectorizer+TfidfTransformer=TfidfVectorizer):
preproc = ColumnTransformer([
    (col+""_tfidf"", TfidfVectorizer(), col)
    for col in X.columns
])

model = Pipeline([
    (""preproc"", preproc),
    ('clf', LogisticRegression()),
])

model.fit(X,y)

",scikit-learn
zerodivisionerror when using sklearn39s baggingclassifier with gridsearchcv,"I'm trying to improve a perfectly working Bernoulli Naive Bayes model with bagging.
But when I try to cross-validate the BaggingClassifier, I get a very unexpected ZeroDivisionError coming from parallel.py.
I've tried to change all the parameters I know, rebooted python but nothing worked.
Here is a reproducible example with a binary-modified iris dataset:
#%% run
import numpy as np

from sklearn.model_selection import GridSearchCV
from sklearn.ensemble import BaggingClassifier
from sklearn.naive_bayes import BernoulliNB
from sklearn.datasets import load_iris


data = load_iris()
data.targetbin = (data.target!=0).astype(""int"")




param_grid2={'max_samples' : np.linspace(0.5,1.0,3),
            'base_estimator__alpha':np.linspace(0.1,1,3),
            'base_estimator__binarize':[*np.linspace(0.0,1,3)],
            'base_estimator__fit_prior':[True,False]}


param_grid2={'max_samples' :[0.7]}


clf = GridSearchCV(
        BaggingClassifier(
                BernoulliNB(),
                n_estimators = 10, max_features = 0.5),
        param_grid2,
        scoring = ""accuracy"",
        verbose=-1)


clf.fit(data.data, data.targetbin)

And here is the stacktrace of my error:
[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers. Traceback (most recent call last):

  File ""<ipython-input-1-dc4eaed2671b>"", line 33, in <module>
    clf.fit(data.data, data.targetbin)

  File ""C:\Users\Dan\Anaconda3\lib\site-packages\sklearn\model_selection\_search.py"", line 722, in fit
    self._run_search(evaluate_candidates)

  File ""C:\Users\Dan\Anaconda3\lib\site-packages\sklearn\model_selection\_search.py"", line 1191, in _run_search
    evaluate_candidates(ParameterGrid(self.param_grid))

  File ""C:\Users\Dan\Anaconda3\lib\site-packages\sklearn\model_selection\_search.py"", line 711, in evaluate_candidates
    cv.split(X, y, groups)))

  File ""C:\Users\Dan\Anaconda3\lib\site-packages\sklearn\externals\joblib\parallel.py"", line 917, in __call__
    if self.dispatch_one_batch(iterator):

  File ""C:\Users\Dan\Anaconda3\lib\site-packages\sklearn\externals\joblib\parallel.py"", line 759, in dispatch_one_batch
    self._dispatch(tasks)

  File ""C:\Users\Dan\Anaconda3\lib\site-packages\sklearn\externals\joblib\parallel.py"", line 716, in _dispatch
    job = self._backend.apply_async(batch, callback=cb)

  File ""C:\Users\Dan\Anaconda3\lib\site-packages\sklearn\externals\joblib\_parallel_backends.py"", line 184, in apply_async
    callback(result)

  File ""C:\Users\Dan\Anaconda3\lib\site-packages\sklearn\externals\joblib\parallel.py"", line 306, in __call__
    self.parallel.print_progress()

  File ""C:\Users\Dan\Anaconda3\lib\site-packages\sklearn\externals\joblib\parallel.py"", line 806, in print_progress
    if (is_last_item or cursor % frequency):

ZeroDivisionError: integer division or modulo by zero

What am I doing wrong?
","I tried to debug the lib and found self.verbose for sklearn/externals/joblib/parallel.py is -1, however it's supposed to be at least 0 by default. So I think it's a bug.
",scikit-learn
attributeerror and typeerror using customtransformers,"I am building a model using customized transformers (KeyError: ""None of [Index([('A','B','C')] , dtype='object')] are in the [columns]).
When I run the below code, I get an error because of .fit:
---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
<ipython-input-165-289e1d466eb9> in <module>
     10 
     11 # fit on the complete pipeline
---> 12 training = full_pipeline.fit(X, y)
     13 
     14 # metrics

~/opt/anaconda3/lib/python3.8/site-packages/sklearn/pipeline.py in fit(self, X, y, **fit_params)
    339         """"""
    340         fit_params_steps = self._check_fit_params(**fit_params)
--> 341         Xt = self._fit(X, y, **fit_params_steps)
    342         with _print_elapsed_time('Pipeline',
    343                                  self._log_message(len(self.steps) - 1)):

~/opt/anaconda3/lib/python3.8/site-packages/sklearn/pipeline.py in _fit(self, X, y, **fit_params_steps)
    301                 cloned_transformer = clone(transformer)
    302             # Fit or load from cache the current transformer
--> 303             X, fitted_transformer = fit_transform_one_cached(
    304                 cloned_transformer, X, y, None,
    305                 message_clsname='Pipeline',

~/opt/anaconda3/lib/python3.8/site-packages/joblib/memory.py in __call__(self, *args, **kwargs)
    350 
    351     def __call__(self, *args, **kwargs):
--> 352         return self.func(*args, **kwargs)
    353 
    354     def call_and_shelve(self, *args, **kwargs):

~/opt/anaconda3/lib/python3.8/site-packages/sklearn/pipeline.py in _fit_transform_one(transformer, X, y, weight, message_clsname, message, **fit_params)
    752     with _print_elapsed_time(message_clsname, message):
    753         if hasattr(transformer, 'fit_transform'):
--> 754             res = transformer.fit_transform(X, y, **fit_params)
    755         else:
    756             res = transformer.fit(X, y, **fit_params).transform(X)

~/opt/anaconda3/lib/python3.8/site-packages/sklearn/pipeline.py in fit_transform(self, X, y, **fit_params)
    385             fit_params_last_step = fit_params_steps[self.steps[-1][0]]
    386             if hasattr(last_step, 'fit_transform'):
--> 387                 return last_step.fit_transform(Xt, y, **fit_params_last_step)
    388             else:
    389                 return last_step.fit(Xt, y,

~/opt/anaconda3/lib/python3.8/site-packages/sklearn/pipeline.py in fit_transform(self, X, y, **fit_params)
    978             sum of n_components (output dimension) over transformers.
    979         """"""
--> 980         results = self._parallel_func(X, y, fit_params, _fit_transform_one)
    981         if not results:
    982             # All transformers are None

~/opt/anaconda3/lib/python3.8/site-packages/sklearn/pipeline.py in _parallel_func(self, X, y, fit_params, func)
   1000         transformers = list(self._iter())
   1001 
-> 1002         return Parallel(n_jobs=self.n_jobs)(delayed(func)(
   1003             transformer, X, y, weight,
   1004             message_clsname='FeatureUnion',

~/opt/anaconda3/lib/python3.8/site-packages/joblib/parallel.py in __call__(self, iterable)
   1042                 self._iterating = self._original_iterator is not None
   1043 
-> 1044             while self.dispatch_one_batch(iterator):
   1045                 pass
   1046 

~/opt/anaconda3/lib/python3.8/site-packages/joblib/parallel.py in dispatch_one_batch(self, iterator)
    857                 return False
    858             else:
--> 859                 self._dispatch(tasks)
    860                 return True
    861 

~/opt/anaconda3/lib/python3.8/site-packages/joblib/parallel.py in _dispatch(self, batch)
    775         with self._lock:
    776             job_idx = len(self._jobs)
--> 777             job = self._backend.apply_async(batch, callback=cb)
    778             # A job can complete so quickly than its callback is
    779             # called before we get here, causing self._jobs to

~/opt/anaconda3/lib/python3.8/site-packages/joblib/_parallel_backends.py in apply_async(self, func, callback)
    206     def apply_async(self, func, callback=None):
    207         """"""Schedule a func to be run""""""
--> 208         result = ImmediateResult(func)
    209         if callback:
    210             callback(result)

~/opt/anaconda3/lib/python3.8/site-packages/joblib/_parallel_backends.py in __init__(self, batch)
    570         # Don't delay the application, to avoid keeping the input
    571         # arguments in memory
--> 572         self.results = batch()
    573 
    574     def get(self):

~/opt/anaconda3/lib/python3.8/site-packages/joblib/parallel.py in __call__(self)
    260         # change the default number of processes to -1
    261         with parallel_backend(self._backend, n_jobs=self._n_jobs):
--> 262             return [func(*args, **kwargs)
    263                     for func, args, kwargs in self.items]
    264 

~/opt/anaconda3/lib/python3.8/site-packages/joblib/parallel.py in <listcomp>(.0)
    260         # change the default number of processes to -1
    261         with parallel_backend(self._backend, n_jobs=self._n_jobs):
--> 262             return [func(*args, **kwargs)
    263                     for func, args, kwargs in self.items]
    264 

~/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/fixes.py in __call__(self, *args, **kwargs)
    220     def __call__(self, *args, **kwargs):
    221         with config_context(**self.config):
--> 222             return self.function(*args, **kwargs)

~/opt/anaconda3/lib/python3.8/site-packages/sklearn/pipeline.py in _fit_transform_one(transformer, X, y, weight, message_clsname, message, **fit_params)
    752     with _print_elapsed_time(message_clsname, message):
    753         if hasattr(transformer, 'fit_transform'):
--> 754             res = transformer.fit_transform(X, y, **fit_params)
    755         else:
    756             res = transformer.fit(X, y, **fit_params).transform(X)

~/opt/anaconda3/lib/python3.8/site-packages/sklearn/pipeline.py in fit_transform(self, X, y, **fit_params)
    385             fit_params_last_step = fit_params_steps[self.steps[-1][0]]
    386             if hasattr(last_step, 'fit_transform'):
--> 387                 return last_step.fit_transform(Xt, y, **fit_params_last_step)
    388             else:
    389                 return last_step.fit(Xt, y,

~/opt/anaconda3/lib/python3.8/site-packages/sklearn/feature_extraction/text.py in fit_transform(self, raw_documents, y)
   1200         max_features = self.max_features
   1201 
-> 1202         vocabulary, X = self._count_vocab(raw_documents,
   1203                                           self.fixed_vocabulary_)
   1204 

~/opt/anaconda3/lib/python3.8/site-packages/sklearn/feature_extraction/text.py in _count_vocab(self, raw_documents, fixed_vocab)
   1112         for doc in raw_documents:
   1113             feature_counter = {}
-> 1114             for feature in analyze(doc):
   1115                 try:
   1116                     feature_idx = vocabulary[feature]

~/opt/anaconda3/lib/python3.8/site-packages/sklearn/feature_extraction/text.py in _analyze(doc, analyzer, tokenizer, ngrams, preprocessor, decoder, stop_words)
    102     else:
    103         if preprocessor is not None:
--> 104             doc = preprocessor(doc)
    105         if tokenizer is not None:
    106             doc = tokenizer(doc)

~/opt/anaconda3/lib/python3.8/site-packages/sklearn/feature_extraction/text.py in _preprocess(doc, accent_function, lower)
     67     """"""
     68     if lower:
---> 69         doc = doc.lower()
     70     if accent_function is not None:
     71         doc = accent_function(doc)

AttributeError: 'numpy.ndarray' object has no attribute 'lower'

The code is
# MODEL
from sklearn import tree

# Decision Tree
decision_tree = tree.DecisionTreeClassifier()
# define full pipeline --> preprocessing + model
full_pipeline = Pipeline(steps=[
    ('preprocess_pipeline', preprocess_pipeline),
    ('model', decision_tree)])

# fit on the complete pipeline
training = full_pipeline.fit(X, y) # <- this step returns the error

I have also tried with .fit_transform but I get the same error.
I read this: AttributeError: 'numpy.ndarray' object has no attribute 'lower' fitting logistic model data but it seems that  I am not passing X or y in the Decision tree like in that example, but maybe I am wrong.
Adding
# Defining the steps in the text pipeline
text_pipeline = Pipeline(steps=[
    ('text_transformer', TextTransformer()),
    ('cv', CountVectorizer(analyzer='word', ngram_range=(2, 2), lowercase=False))])

I get this new error:
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
<ipython-input-159-289e1d466eb9> in <module>
     10 
     11 # fit on the complete pipeline
---> 12 training = full_pipeline.fit(X, y)
     13 
     14 # metrics

~/opt/anaconda3/lib/python3.8/site-packages/sklearn/pipeline.py in fit(self, X, y, **fit_params)
    339         """"""
    340         fit_params_steps = self._check_fit_params(**fit_params)
--> 341         Xt = self._fit(X, y, **fit_params_steps)
    342         with _print_elapsed_time('Pipeline',
    343                                  self._log_message(len(self.steps) - 1)):

~/opt/anaconda3/lib/python3.8/site-packages/sklearn/pipeline.py in _fit(self, X, y, **fit_params_steps)
    301                 cloned_transformer = clone(transformer)
    302             # Fit or load from cache the current transformer
--> 303             X, fitted_transformer = fit_transform_one_cached(
    304                 cloned_transformer, X, y, None,
    305                 message_clsname='Pipeline',

~/opt/anaconda3/lib/python3.8/site-packages/joblib/memory.py in __call__(self, *args, **kwargs)
    350 
    351     def __call__(self, *args, **kwargs):
--> 352         return self.func(*args, **kwargs)
    353 
    354     def call_and_shelve(self, *args, **kwargs):

~/opt/anaconda3/lib/python3.8/site-packages/sklearn/pipeline.py in _fit_transform_one(transformer, X, y, weight, message_clsname, message, **fit_params)
    752     with _print_elapsed_time(message_clsname, message):
    753         if hasattr(transformer, 'fit_transform'):
--> 754             res = transformer.fit_transform(X, y, **fit_params)
    755         else:
    756             res = transformer.fit(X, y, **fit_params).transform(X)

~/opt/anaconda3/lib/python3.8/site-packages/sklearn/pipeline.py in fit_transform(self, X, y, **fit_params)
    385             fit_params_last_step = fit_params_steps[self.steps[-1][0]]
    386             if hasattr(last_step, 'fit_transform'):
--> 387                 return last_step.fit_transform(Xt, y, **fit_params_last_step)
    388             else:
    389                 return last_step.fit(Xt, y,

~/opt/anaconda3/lib/python3.8/site-packages/sklearn/pipeline.py in fit_transform(self, X, y, **fit_params)
    978             sum of n_components (output dimension) over transformers.
    979         """"""
--> 980         results = self._parallel_func(X, y, fit_params, _fit_transform_one)
    981         if not results:
    982             # All transformers are None

~/opt/anaconda3/lib/python3.8/site-packages/sklearn/pipeline.py in _parallel_func(self, X, y, fit_params, func)
   1000         transformers = list(self._iter())
   1001 
-> 1002         return Parallel(n_jobs=self.n_jobs)(delayed(func)(
   1003             transformer, X, y, weight,
   1004             message_clsname='FeatureUnion',

~/opt/anaconda3/lib/python3.8/site-packages/joblib/parallel.py in __call__(self, iterable)
   1042                 self._iterating = self._original_iterator is not None
   1043 
-> 1044             while self.dispatch_one_batch(iterator):
   1045                 pass
   1046 

~/opt/anaconda3/lib/python3.8/site-packages/joblib/parallel.py in dispatch_one_batch(self, iterator)
    857                 return False
    858             else:
--> 859                 self._dispatch(tasks)
    860                 return True
    861 

~/opt/anaconda3/lib/python3.8/site-packages/joblib/parallel.py in _dispatch(self, batch)
    775         with self._lock:
    776             job_idx = len(self._jobs)
--> 777             job = self._backend.apply_async(batch, callback=cb)
    778             # A job can complete so quickly than its callback is
    779             # called before we get here, causing self._jobs to

~/opt/anaconda3/lib/python3.8/site-packages/joblib/_parallel_backends.py in apply_async(self, func, callback)
    206     def apply_async(self, func, callback=None):
    207         """"""Schedule a func to be run""""""
--> 208         result = ImmediateResult(func)
    209         if callback:
    210             callback(result)

~/opt/anaconda3/lib/python3.8/site-packages/joblib/_parallel_backends.py in __init__(self, batch)
    570         # Don't delay the application, to avoid keeping the input
    571         # arguments in memory
--> 572         self.results = batch()
    573 
    574     def get(self):

~/opt/anaconda3/lib/python3.8/site-packages/joblib/parallel.py in __call__(self)
    260         # change the default number of processes to -1
    261         with parallel_backend(self._backend, n_jobs=self._n_jobs):
--> 262             return [func(*args, **kwargs)
    263                     for func, args, kwargs in self.items]
    264 

~/opt/anaconda3/lib/python3.8/site-packages/joblib/parallel.py in <listcomp>(.0)
    260         # change the default number of processes to -1
    261         with parallel_backend(self._backend, n_jobs=self._n_jobs):
--> 262             return [func(*args, **kwargs)
    263                     for func, args, kwargs in self.items]
    264 

~/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/fixes.py in __call__(self, *args, **kwargs)
    220     def __call__(self, *args, **kwargs):
    221         with config_context(**self.config):
--> 222             return self.function(*args, **kwargs)

~/opt/anaconda3/lib/python3.8/site-packages/sklearn/pipeline.py in _fit_transform_one(transformer, X, y, weight, message_clsname, message, **fit_params)
    752     with _print_elapsed_time(message_clsname, message):
    753         if hasattr(transformer, 'fit_transform'):
--> 754             res = transformer.fit_transform(X, y, **fit_params)
    755         else:
    756             res = transformer.fit(X, y, **fit_params).transform(X)

~/opt/anaconda3/lib/python3.8/site-packages/sklearn/pipeline.py in fit_transform(self, X, y, **fit_params)
    385             fit_params_last_step = fit_params_steps[self.steps[-1][0]]
    386             if hasattr(last_step, 'fit_transform'):
--> 387                 return last_step.fit_transform(Xt, y, **fit_params_last_step)
    388             else:
    389                 return last_step.fit(Xt, y,

~/opt/anaconda3/lib/python3.8/site-packages/sklearn/feature_extraction/text.py in fit_transform(self, raw_documents, y)
   1200         max_features = self.max_features
   1201 
-> 1202         vocabulary, X = self._count_vocab(raw_documents,
   1203                                           self.fixed_vocabulary_)
   1204 

~/opt/anaconda3/lib/python3.8/site-packages/sklearn/feature_extraction/text.py in _count_vocab(self, raw_documents, fixed_vocab)
   1112         for doc in raw_documents:
   1113             feature_counter = {}
-> 1114             for feature in analyze(doc):
   1115                 try:
   1116                     feature_idx = vocabulary[feature]

~/opt/anaconda3/lib/python3.8/site-packages/sklearn/feature_extraction/text.py in _analyze(doc, analyzer, tokenizer, ngrams, preprocessor, decoder, stop_words)
    104             doc = preprocessor(doc)
    105         if tokenizer is not None:
--> 106             doc = tokenizer(doc)
    107         if ngrams is not None:
    108             if stop_words is not None:

TypeError: cannot use a string pattern on a bytes-like object

If I remove text_pipeline, the error does not occur, so it seems that something is going wrong because of the way to use countVectorizer.
An example of text is
an example
example number 1
this is another small example

I have other columns that are numerical and categorical.
Have you experienced a similar issue? If yes, how did you handle it?
","A common error in text transformers of sklearn involves the shape of the data: unlike most other sklearn preprocessors, text transformers generally expect a one-dimensional input, and python's duck-typing causes weird errors from both arrays and strings being iterables.
Your TextTransformer.transform returns X[['Tweet']], which is 2-dimensional, and will cause problems with the subsequent CountVectorizer.  (Converting to a numpy array with .values doesn't change the dimensionality problem, but there's also no compelling reason to do that conversion.)  Returning X['Tweet'] instead should cure that problem.
",scikit-learn
how to check whether an sklearn estimator is a scaler,"I'm writing a function that needs to determine whether an object passed to it is an imputer (can check with isinstance(obj, _BaseImputer)), a scaler, or something else.
While all imputers have a common base class that identifies them as imputers, scalers do not. I found that all scalers in sklearn.preprocessing._data inherit (OneToOneFeatureMixin, TransformerMixin, BaseEstimator), so I could check if they are instances of all of them. However that could generate false positives (not sure which other object may inherit the same base classes). It doesn't feel very clean or pythonic either.
I was also thinking of checking whether the object has the .inverse_transform() method. However, not only scalers have that, a SimpleImputer (and maybe other objects) have also.
How can I easily check if my object is a scaler?
","Unfortunately, the cleanest way to do this is to check each scaler type individually, any other check will potentially let through non-scaler objects as well. Nevertheless, I'll offer some ""hack-jobs"" too.
The most failsafe solution is to import your scalers and then check if your object is any of these scalers or not.
from sklearn.preprocessing import MinMaxScaler, RobustScaler # ... other scalers your code base uses

SCALER_TYPES = [MinMaxScaler, RobustScaler] # Extend list if needed

if any([isinstance(YourObject, scaler_type) for scaler_type in SCALER_TYPES]):
    # Do something
    pass
else:
    # Do something else
    pass

Now, if you want something that catches them all without listing all the scalers you use in your code, you could rely on private properties of the scaler objects. These are private for a good reason though, and are subject to change without notice even between patch versions, so nothing at all guarantees that your code will work if you update sklearn to a new version. You could rely on the string representation (__repr__) of the object to check if it contains Scaler. This is how you can do it:
if 'Scaler' in str(YourObject):
    # Do something
    pass
else:
    # Do something else
    pass

or
if 'Scaler' in YourObject.__repr__():
    # Do something
    pass
else:
    # Do something else
    pass

This will let through anything that has Scaler in its string representation though, so you are definitely better off with being explicit and defining your list of scalers.
",scikit-learn
how to pass parameters to this sklearn cox model in a pipeline,"If I run the following Python code it works well:
target = 'churn'
tranOH = ColumnTransformer([ ('one', OneHotEncoder(drop='first', dtype='int'), 
make_column_selector(dtype_include='category', pattern=f""^(?!{target}).*"")   
) ], remainder='passthrough')


dftrain2 = tranOH.fit_transform(dftrain)
cph = CoxPHFitter(penalizer=0.1)
cph.fit(dftrain2, 'months', 'churn')

But if I try to do it with a Pipeline I get an error:
mcox = Pipeline(steps=[
(""onehot"", tranOH),
('modelo', CoxPHFitter(penalizer=0.1)) 
])

mcox.fit(dftrain, modelo__duration_col=""months"", modelo__event_col='churn')

It says:
TypeError                                 Traceback (most recent call last)
Cell In[88], line 6
      1 mcox = Pipeline(steps=[
      2     (""onehot"", tranOH),
      3     ('modelo', CoxPHFitter(penalizer=0.1)) 
      4     ])
----> 6 mcox.fit(dftrain, modelo__duration_col=""months"", modelo__event_col=target)

File ~\AppData\Roaming\Python\Python310\site-packages\sklearn\base.py:1473, in _fit_context.<locals>.decorator.<locals>.wrapper(estimator, *args, **kwargs)
   1466     estimator._validate_params()
   1468 with config_context(
   1469     skip_parameter_validation=(
   1470         prefer_skip_nested_validation or global_skip_validation
   1471     )
   1472 ):
-> 1473     return fit_method(estimator, *args, **kwargs)

File ~\AppData\Roaming\Python\Python310\site-packages\sklearn\pipeline.py:473, in Pipeline.fit(self, X, y, **params)
    471     if self._final_estimator != ""passthrough"":
    472         last_step_params = routed_params[self.steps[-1][0]]
--> 473         self._final_estimator.fit(Xt, y, **last_step_params[""fit""])
    475 return self

File ~\AppData\Roaming\Python\Python310\site-packages\lifelines\utils\__init__.py:56, in CensoringType.right_censoring.<locals>.f(model, *args, **kwargs)
     53 @wraps(function)
     54 def f(model, *args, **kwargs):
     55     cls.set_censoring_type(model, cls.RIGHT)
---> 56     return function(model, *args, **kwargs)

TypeError: CoxPHFitter.fit() got multiple values for argument 'duration_col'

tranOH is a Columntransformer that onehot encodes all categorical columns except 'churn'.
I have also tried using col=""months"" and event_col=target directly inside CoxPHFitter() but I get the same error.
Later I want to use it to perform a GridSearchCV to finetune the penalizer parameter, optimizing the accuracy score to predict churn at a given time=""months"".
I don't have the same problem with other models, for example if I replace CoxPHFitter with LogisticRegression it works well.
","CoxPHFitter doesn't abide by the sklearn API: its fit method takes df followed by other arguments (duration_col first), not X then y followed by other arguments.
This is mostly OK, because you can just pass the whole frame (including the target and duration columns) as X, and sklearn pipelines will just take that as an unsupervised setting, i.e. the default value y=None.  But, then we get to fitting the last step of the pipeline, and this gets called:
self._final_estimator.fit(Xt, y, **last_step_params[""fit""])

(source).  y here will still be None, but since CoxPHFitter takes its second argument as being duration_col, this means gets set to both None (via the y argument here) and then ""months"" (your kwarg parameter).
I don't think there's an easy way to fix this.  lifelines provides an sklearn_wrapper, but that seems to have its own issues, being slated for removal in version 0.28.  I would just keep the model separate from the preprocessing pipeline.
",scikit-learn
gaussian process binary classification why is the variance with gpy much smaller than with scikitlearn,"I'm learning about binary classification with Gaussian Processes and I am comparing GPy with scikit-learn on a toy 1D problem inspired by Martin Krasser's blog post. Both implementations (GPy and scikit-learn) seem to use a similar setup with a RBF kernel. After optimizing the kernel hyperparameters, the lenghtscales are similar, but the variances differ by a lot. The GPy kernel variance appears to be too small.
How can I modify my GPy implentation and obtain a result similar to those with scikit-learn? I suspect it has to do with the internal implentation of each algorithm, but I can't tell what's causing this drastic difference. I explain further below why I believe my GPy implementation needs to be fixed.
Implementation details: Python 3.9 with GPy 1.13.2 and scikit-learn 1.5.1. Reproducible example:
import numpy as np
from scipy.stats import bernoulli
from scipy.special import expit as sigmoid

##############################
# Part 1: toy dataset creation
##############################

np.random.seed(0)
X = np.arange(0, 5, 0.05).reshape(-1, 1)
X_test = np.arange(-2, 7, 0.1).reshape(-1, 1)

a = np.sin(X * np.pi * 0.5) * 2  # latent function
t = bernoulli.rvs(sigmoid(a))    # Bernoulli training data (0s and 1s)

#####################################
# Part 2: scikit-learn implementation
#####################################

from sklearn.gaussian_process import GaussianProcessClassifier
from sklearn.gaussian_process.kernels import ConstantKernel, RBF

rbf = ConstantKernel(1.0, constant_value_bounds=(1e-3, 10)) \
        * RBF(length_scale=1.0, length_scale_bounds=(1e-3, 10))
gpc = GaussianProcessClassifier(
    kernel=rbf,
    optimizer='fmin_l_bfgs_b',
    n_restarts_optimizer=10)

gpc.fit(X_scaled, t.ravel())

print(gpc.kernel_)
# 1.5**2 * RBF(length_scale=0.858)

############################
# Part 3: GPy implementation
############################

import GPy

kern = GPy.kern.RBF(
    input_dim=1,
    variance=1.,
    lengthscale=1.)
kern.lengthscale.unconstrain()
kern.variance.unconstrain()
kern.lengthscale.constrain_bounded(1e-3, 10)
kern.variance.constrain_bounded(1e-3, 10)

m = GPy.core.GP(
    X=X,Y=t, kernel=kern, 
    inference_method=GPy.inference.latent_function_inference.laplace.Laplace(),    
    likelihood=GPy.likelihoods.Bernoulli())

m.optimize_restarts(
    num_restarts=10, optimizer='lbfgs',
    verbose=True, robust=True)

print(m.kern)
#  rbf.         |               value  |  constraints  |  priors
#  variance     |  0.8067562453940487  |  0.001,10.0   |        
#  lengthscale  |  0.8365668826459536  |  0.001,10.0   |

The lenghtscale values are roughly similar (0.858 vs 0.836), but the variance values are very different (1.5**2 = 2.25 for scikit-learn and only 0.806 for GPy).
The reason why I believe my GPy implementation needs adjustments is that the true latent function (see ""a"" in part 1 of code above) does not closely match the predicted one even with +/- 2 standard deviation bounds. On the other hand, the scikit-learn implementation matches it reasonably well (it it possible to retrieve the latent function mean and std. deviation with scikit-learn as shown here).

 Left: predicted probabilities are similar with both models (makes sense since they share similar lenghtscale values). Right: the predicted latent function of GPy does not fit the true latent function as well as the scikit-learn model does. 
What I've tried so far, without significant change in the results:

Input feature (X) normalization
Use GPy.inference.latent_function_inference.expectation_propagation.EP() as the GPy inference method instead of the Laplace one
Add a WhiteKernel component to the scikit-learn implementation, as suggested here

","It turns out GPy's Bernoulli likelihood class maps the latent predictions to a probability using the standard normal CDF (source code shows the default is the probit link function). The dataset in my example instead uses the sigmoid function, meaning that a logit link function would instead be needed. Even though it's not implemented in the GPy library, we can create our own link function object and pass it to the GPy model when initializing it. Here's an updated version of my code (see changes in Part 3):
import numpy as np
from scipy.stats import bernoulli
from scipy.special import expit as sigmoid

##############################
# Part 1: toy dataset creation
##############################

np.random.seed(0)
X = np.arange(0, 5, 0.05).reshape(-1, 1)
X_test = np.arange(-2, 7, 0.1).reshape(-1, 1)

a = np.sin(X * np.pi * 0.5) * 2  # latent function
t = bernoulli.rvs(sigmoid(a))    # Bernoulli training data (0s and 1s)

#####################################
# Part 2: scikit-learn implementation
#####################################

from sklearn.gaussian_process import GaussianProcessClassifier
from sklearn.gaussian_process.kernels import ConstantKernel, RBF

rbf = ConstantKernel(1.0, constant_value_bounds=(1e-3, 10)) \
        * RBF(length_scale=1.0, length_scale_bounds=(1e-3, 10))
gpc = GaussianProcessClassifier(
    kernel=rbf,
    optimizer='fmin_l_bfgs_b',
    n_restarts_optimizer=10)

gpc.fit(X_scaled, t.ravel())

print(gpc.kernel_)
# 1.95**2 * RBF(length_scale=0.898)

############################
# Part 3: GPy implementation
############################

import GPy
from GPy.likelihoods.link_functions import GPTransformation

kern = GPy.kern.RBF(
    input_dim=1,
    variance=1.,
    lengthscale=1.)
kern.lengthscale.unconstrain()
kern.variance.unconstrain()
kern.lengthscale.constrain_bounded(1e-3, 10)
kern.variance.constrain_bounded(1e-3, 10)

# Create custom link function
class LogitLink(GPTransformation):
    def transf(self, f):
        """"""Sigmoid function""""""
        return 1 / (1 + np.exp(-f))

    def dtransf_df(self, f):
        """"""First derivative of sigmoid""""""
        p = self.transf(f)
        return p * (1 - p)

    def d2transf_df2(self, f):
        """"""Second derivative of sigmoid""""""
        p = self.transf(f)
        return p * (1 - p) * (1 - 2 * p)

    def d3transf_df3(self, f):
        """"""Third derivative of sigmoid""""""
        p = self.transf(f)
        return p * (1 - p) * (1 - 6 * p + 6 * p**2)

logit_link = LogitLink()

m = GPy.core.GP(
    X=X,Y=t, kernel=kern, 
    inference_method=GPy.inference.latent_function_inference.laplace.Laplace(),    
    likelihood=GPy.likelihoods.Bernoulli(gp_link=logit_link))

m.optimize_restarts(
    num_restarts=10, optimizer='lbfgs',
    verbose=True, robust=True)

print(m.kern)
#  rbf.         |               value  |  constraints  |  priors
#  variance     |  3.7922168280178328  |  0.001,10.0   |        
#  lengthscale  |  0.8977846144774547  |  0.001,10.0   |

The scikit-learn implementation (Part 2 of the code) uses the sigmoid mapping (logit link function), which is why the results with it made more sense. Now, both implementation (scikit-learn and GPy) return the same lengthscale and variance values after optimization.
",scikit-learn
plot a decision tree from histgratientboostingclassifier,"I have a HistGradientBoostingClassifier model and I want to plot one or more of its decision trees, nevertheless I can't manage to find a native function to do it, I can access the Tree predictor objects and thus it's nodes, but in order to plot it into the sklearn.tree.plot_tree function it needs to be a DecisionTree type object
I tried this:
from sklearn.tree import plot_tree

plot_tree(RF_90._predictors[0][0])

getting this error:

InvalidParameterError: The 'decision_tree' parameter of plot_tree must
be an instance of 'sklearn.tree._classes.DecisionTreeClassifier' or an
instance of 'sklearn.tree._classes.DecisionTreeRegressor'. Got
<sklearn.ensemble._hist_gradient_boosting.predictor.TreePredictor
object at 0x7f676ebf0310> instead.

Note: RF_90 is the HistGradientBoostingClassifier fitted model
","In order to visualize trees generated by HistGradientboostingClassifier this function worked for me:
def visualize_tree(tree, feature_names, class_names): 
dot = graphviz.Digraph() 
def add_nodes_edges(dot, nodes, node_id):
    node = nodes[node_id]
    if node['is_leaf']: 
        value = node['value'] 
        dot.node(str(node_id), f""Predict: {value}"") 
    else: 
        feature = feature_names[node['feature_idx']] 
        threshold = node['bin_threshold'] 
        dot.node(str(node_id), f""{feature} <= {threshold:.2f}"") 
        left_child = node['left'] 
        right_child = node['right']
        dot.edge(str(node_id), str(left_child), ""True"") 
        dot.edge(str(node_id), str(right_child), ""False"") 
        add_nodes_edges(dot, nodes, left_child) 
        add_nodes_edges(dot, nodes, right_child) 
nodes = tree.__getstate__()['nodes'] 
add_nodes_edges(dot, nodes, 0) 
return dot 

# Create and visualize the tree 
dot = visualize_tree(single_tree, RF_90.feature_names_in_, 1) 
dot.render(""hist_gb_tree"") # Save to file 
dot #view from jupyter

where RF_90 is the fitted model and single_tree is:
single_tree = trees_per_iteration[iteration][class_index]

where iteration = 0 and class_index = 0
",scikit-learn
adaboostclassifier with different base learners,"I am trying to use AdaBoostClassifier with a base learner other than DecisionTree. I have tried SVM and KNeighborsClassifier but I get errors. What are the classifiers that can be used with AdaBoostClassifier?
","Ok, we have a systematic method to find out all the base learners supported by AdaBoostClassifier. Compatible base learner's fit method needs to support sample_weight, which can be obtained by running following code:
import inspect
from sklearn.utils.testing import all_estimators
for name, clf in all_estimators(type_filter='classifier'):
    if 'sample_weight' in inspect.getargspec(clf().fit)[0]:
       print name

This results in following output:
AdaBoostClassifier,
BernoulliNB,
DecisionTreeClassifier,
ExtraTreeClassifier,
ExtraTreesClassifier,
MultinomialNB,
NuSVC,
Perceptron,
RandomForestClassifier,
RidgeClassifierCV,
SGDClassifier,
SVC.

If the classifier doesn't implement predict_proba, you will have to set AdaBoostClassifier parameter algorithm = 'SAMME'.
",scikit-learn
how to run model on new data that requires pdget_dummies,"I have a model that runs the following:
import pandas as pd
import numpy as np

# initialize list of lists 
data = [['tom', 10,1,'a'], ['tom', 15,5,'a'], ['tom', 14,1,'a'], ['tom', 15,4,'b'], ['tom', 18,1,'b'], ['tom', 15,6,'a'], ['tom', 17,3,'a']
       , ['tom', 14,7,'b'], ['tom',16 ,6,'a'], ['tom', 22,2,'a'],['matt', 10,1,'c'], ['matt', 15,5,'b'], ['matt', 14,1,'b'], ['matt', 15,4,'a'], ['matt', 18,1,'a'], ['matt', 15,6,'a'], ['matt', 17,3,'a']
       , ['matt', 14,7,'c'], ['matt',16 ,6,'b'], ['matt', 10,2,'b']]

# Create the pandas DataFrame 
df = pd.DataFrame(data, columns = ['Name', 'Attempts','Score','Category']) 

print(df.head(2))
  Name  Attempts  Score Category
0  tom        10      1        a
1  tom        15      5        a

Then I have created a dummy df to use in the model using the following code:
from sklearn.linear_model import LogisticRegression

df_dum = pd.get_dummies(df)
print(df_dum.head(2))
  Attempts  Score  Name_matt  Name_tom  Category_a  Category_b  Category_c
0        10      1          0         1           1           0           0
1        15      5          0         1           1           0           0

Then I have created the following model:
#Model

X = df_dum.drop(('Score'),axis=1)
y = df_dum['Score'].values

#Training Size
train_size = int(X.shape[0]*.7)
X_train = X[:train_size]
X_test = X[train_size:]
y_train = y[:train_size]
y_test = y[train_size:]


#Fit Model
model = LogisticRegression(max_iter=1000)
model.fit(X_train,y_train)


#Send predictions back to dataframe
Z = model.predict(X_test)
zz = model.predict_proba(X_test)

df.loc[train_size:,'predictions']=Z
dfpredictions = df.dropna(subset=['predictions'])

print(dfpredictions)
    Name  Attempts  Score Category  predictions
14  matt        18      1        a          1.0
15  matt        15      6        a          1.0
16  matt        17      3        a          1.0
17  matt        14      7        c          1.0
18  matt        16      6        b          1.0
19  matt        10      2        b          1.0

Now I have new data which i would like to predict:
newdata = [['tom', 10,'a'], ['tom', 15,'a'], ['tom', 14,'a']]

newdf = pd.DataFrame(newdata, columns = ['Name', 'Attempts','Category']) 

print(newdf)

 Name  Attempts Category
0  tom        10        a
1  tom        15        a
2  tom        14        a

Then create dummies and run prediction
newpredict = pd.get_dummies(newdf)

predict = model.predict(newpredict)

Output:
ValueError: X has 3 features per sample; expecting 6

Which makes sense because there are no categories b and c and no name called matt.
My question is how is the best way to set this model up given my new data wont always have the full set of columns used in the original data. Each day i have new data so I'm not quite sure of the most efficient and error free way.
This is an example data - my dataset has 2000 columns when running pd.get_dummies.
","Let me explain Nicolas and BlueSkyz's recommendation a bit more in detail.
pd.get_dummies is useful when you are sure that there will not be any new categories for a specific categorical variable in production/new data set, e.g. Gender, Products, etc. based on your Company or Database's internal data classification/consistency rules.
However, for the majority of machine learning tasks where you can expect to have new categories in the future which were not used in model training, sklearn.OneHotEncoder should be the standard choice. handle_unknown parameter of sklearn.OneHotEncoder can be set to 'ignore' to do just that: ignore new categories when applying the encoder in future. From the documentation:

Whether to raise an error or ignore if an unknown categorical feature is present during transform (default is to raise). When this parameter is set to ‘ignore’ and an unknown category is encountered during transform, the resulting one-hot encoded columns for this feature will be all zeros. In the inverse transform, an unknown category will be denoted as None

The full flow based on LabelEncoding and OneHotEncoding for your example is as below:
# Create a categorical boolean mask
categorical_feature_mask = df.dtypes == object
# Filter out the categorical columns into a list for easy reference later on in case you have more than a couple categorical columns
categorical_cols = df.columns[categorical_feature_mask].tolist()
 
# Instantiate the OneHotEncoder Object
from sklearn.preprocessing import OneHotEncoder
ohe = OneHotEncoder(handle_unknown='ignore', sparse = False)
# Apply ohe on data
ohe.fit(df[categorical_cols])
cat_ohe = ohe.transform(df[categorical_cols])

#Create a Pandas DataFrame of the hot encoded column
ohe_df = pd.DataFrame(cat_ohe, columns = ohe.get_feature_names(input_features = categorical_cols))
#concat with original data and drop original columns
df_ohe = pd.concat([df, ohe_df], axis=1).drop(columns = categorical_cols, axis=1)

# The following code is for your newdf after training and testing on original df
# Apply ohe on newdf
cat_ohe_new = ohe.transform(newdf[categorical_cols])
#Create a Pandas DataFrame of the hot encoded column
ohe_df_new = pd.DataFrame(cat_ohe_new, columns = ohe.get_feature_names(input_features = categorical_cols))
#concat with original data and drop original columns
df_ohe_new = pd.concat([newdf, ohe_df_new], axis=1).drop(columns = categorical_cols, axis=1)

# predict on df_ohe_new
predict = model.predict(df_ohe_new)

Output (that you can assign back to newdf):
array([1, 1, 1])

However, if you really want to use pd.get_dummies only, then the following can work as well:
newpredict = newpredict.reindex(labels = df_dum.columns, axis = 1, fill_value = 0).drop(columns = ['Score'])
predict = model.predict(newpredict)

The above code snippet will make sure that you have the same columns in your new dummies df (newpredict) as the original df_dum (with 0 values) and drop the 'Score' column. Output here is same as above. This code will ensure that any categorical values present in the new data set but now in the original trained data will be removed while keeping the order of the columns same as that in the original df.
Keep in mind that pd.get_dummies is usually much faster to execute than sklearn.OneHotEncoder.
",scikit-learn
keep tfidf result for predicting new content,"I am using sklearn on Python to do some clustering. I've trained 200,000 data, and code below works well.
corpus = open(""token_from_xml.txt"")
vectorizer = CountVectorizer(decode_error=""replace"")
transformer = TfidfTransformer()
tfidf = transformer.fit_transform(vectorizer.fit_transform(corpus))
km = KMeans(30)
kmresult = km.fit(tfidf).predict(tfidf)

But when I have new testing content, I'd like to cluster it to existed clusters I'd trained. So I'm wondering how to save IDF result, so that I can do TFIDF for the new testing content and make sure the result for new testing content have same array length.
Thanks in advance.
UPDATE
I may need to save ""transformer"" or ""tfidf"" variable to file(txt or others), if one of them contains the trained IDF result.
UPDATE
For example. I have the training data:
[""a"", ""b"", ""c""]
[""a"", ""b"", ""d""]

And do TFIDF, the result will contains 4 features(a,b,c,d)
When I TEST:
[""a"", ""c"", ""d""]

to see which cluster(already made by k-means) it belongs to. TFIDF will only give the result with 3 features(a,c,d), so the clustering in k-means will fall. (If I test [""a"", ""b"", ""e""], there may have other problems.)
So how to store the features list for testing data (even more, store it in file)?
","I successfully saved the feature list by saving vectorizer.vocabulary_, and reuse by CountVectorizer(decode_error=""replace"",vocabulary=vectorizer.vocabulary_)
Codes below:
corpus = np.array([""aaa bbb ccc"", ""aaa bbb ddd""])
vectorizer = CountVectorizer(decode_error=""replace"")
vec_train = vectorizer.fit_transform(corpus)
#Save vectorizer.vocabulary_
pickle.dump(vectorizer.vocabulary_,open(""feature.pkl"",""wb""))

#Load it later
transformer = TfidfTransformer()
loaded_vec = CountVectorizer(decode_error=""replace"",vocabulary=pickle.load(open(""feature.pkl"", ""rb"")))
tfidf = transformer.fit_transform(loaded_vec.fit_transform(np.array([""aaa ccc eee""])))

That works. tfidf will have same feature length as trained data.
",scikit-learn
png conversion into scikit learn digit format,"I would need some help for converting a PNG RGBA into the correct format for digit recognition in Scikit learn. This is my code
image = Image.open(image_path)
print (image.size)
print (image.mode)
print (image.format)


image = image.resize((8, 8)).convert(""L"")
print (image.size)
print (image.mode)
print (image.format)


data = np.array(image)

data = data / 16.0
data = data.reshape(1, -1)
print (data.shape)
print (data)

print (model.predict(data))

The convert method always converts to an array of the correct size with all 0s.
Some articles online suggested to use convert(""LA"") because of the RGBA format, but if I do so the size of the resulting array is wrong and I do not understand how to convert it to a correct 8x8.
This is the link to the test image (png file, zipped to avoid any alterations).
","That's because your image is solid black in the RGB channels and the digit 7 that you are looking for is contained only in the A channel (A=alpha or transparency).
If I split your image into its constituent R, G, B, A channels and put a yellow border around each then ""squidge"" them together across the page left-to-right with ImageMagick:
magick YOURIMAGE -separate -bordercolor yellow -border 3 +append result.png


Hopefully you can see R (on left) is empty, G (second box from left) is empty, B (third box from left) is empty and your 7 is in the rightmost A channel.

So, in Python, basically you need to just use the A channel:
from PIL import Image

# Load image
RGBA = Image.open('YOURIMAGE')

# Extract just A channel as single channel L-mode image
A = RGBA.getchannel('A')

",scikit-learn
how to apply model trained with pca and random forest to test data,"In solving one of the machine learning problem, I am implementing PCA on training data and and then applying .transform on train data using sklearn. After observing the variances, I retain only those columns from the transformed data whose variance is large. Then I am training the model using RandomForestClassifier. Now, I am confused: how to apply that trained model on the test data as the number of columns of test data and the retained transformed data (on which random forest is applied) is different?
","Here is a way of doing it if this is what you seek... ideally u should use the same number of principle components in test as well as train... otherwise defeats the purpose of a hold-out set.
pca = PCA(n_components=20)
train_features = pca.fit_transform(train_data)

rfr = sklearn.RandomForestClassifier(n_estimators = 100, n_jobs = 1, 
                                         random_state = 2016, verbose = 1,
                                         class_weight='balanced',oob_score=True)

rfr.fit(train_features)

test_features = pca.transform(test_data)
rfr.predict(test_features)

",scikit-learn
how to extract the decision rules from scikitlearn decisiontree,"Can I extract the underlying decision-rules (or 'decision paths') from a trained tree in a decision tree as a textual list?
Something like:
if A>0.4 then if B<0.2 then if C>0.8 then class='X'

","I believe that this answer is more correct than the other answers here:
from sklearn.tree import _tree

def tree_to_code(tree, feature_names):
    tree_ = tree.tree_
    feature_name = [
        feature_names[i] if i != _tree.TREE_UNDEFINED else ""undefined!""
        for i in tree_.feature
    ]
    print ""def tree({}):"".format("", "".join(feature_names))

    def recurse(node, depth):
        indent = ""  "" * depth
        if tree_.feature[node] != _tree.TREE_UNDEFINED:
            name = feature_name[node]
            threshold = tree_.threshold[node]
            print ""{}if {} <= {}:"".format(indent, name, threshold)
            recurse(tree_.children_left[node], depth + 1)
            print ""{}else:  # if {} > {}"".format(indent, name, threshold)
            recurse(tree_.children_right[node], depth + 1)
        else:
            print ""{}return {}"".format(indent, tree_.value[node])

    recurse(0, 1)

This prints out a valid Python function. Here's an example output for a tree that is trying to return its input, a number between 0 and 10.
def tree(f0):
  if f0 <= 6.0:
    if f0 <= 1.5:
      return [[ 0.]]
    else:  # if f0 > 1.5
      if f0 <= 4.5:
        if f0 <= 3.5:
          return [[ 3.]]
        else:  # if f0 > 3.5
          return [[ 4.]]
      else:  # if f0 > 4.5
        return [[ 5.]]
  else:  # if f0 > 6.0
    if f0 <= 8.5:
      if f0 <= 7.5:
        return [[ 7.]]
      else:  # if f0 > 7.5
        return [[ 8.]]
    else:  # if f0 > 8.5
      return [[ 9.]]

Here are some stumbling blocks that I see in other answers:

Using tree_.threshold == -2 to decide whether a node is a leaf isn't a good idea. What if it's a real decision node with a threshold of -2? Instead, you should look at tree.feature or tree.children_*.
The line features = [feature_names[i] for i in tree_.feature] crashes with my version of sklearn, because some values of tree.tree_.feature are -2 (specifically for leaf nodes).
There is no need to have multiple if statements in the recursive function, just one is fine.

",scikit-learn
scikitlearn svm digit recognition,"I want to make a program to recognize the digit in an image. I follow the tutorial in scikit learn .
I can train and fit the svm classifier like the following.
First, I import the libraries and dataset
from sklearn import datasets, svm, metrics

digits = datasets.load_digits()
n_samples = len(digits.images)
data = digits.images.reshape((n_samples, -1))

Second, I create the SVM model and train it with the dataset.
classifier = svm.SVC(gamma = 0.001)
classifier.fit(data[:n_samples], digits.target[:n_samples])

And then, I try to read my own image and use the function predict() to recognize the digit.
Here is my image:

I reshape the image into (8, 8) and then convert it to a 1D array.
img = misc.imread(""w1.jpg"")
img = misc.imresize(img, (8, 8))
img = img[:, :, 0]

Finally, when I print out the prediction, it returns [1]
predicted = classifier.predict(img.reshape((1,img.shape[0]*img.shape[1] )))
print predicted

Whatever I user others images, it still returns [1]


When I print out the ""default"" dataset of number ""9"", it looks like:
My image number ""9"" :

You can see the non-zero number is quite large for my image.
I dont know why. I am looking for help to solve my problem.
","My best bet would be that there is a problem with your data types and array shapes.
It looks like you are training on numpy arrays that are of the type np.float64 (or possibly np.float32 on 32 bit systems, I don't remember) and where each image has the shape (64,).
Meanwhile your input image for prediction, after the resizing operation in your code, is of type uint8 and shape (1, 64).
I would first try changing the shape of your input image since dtype conversions often just work as you would expect. So change this line:
predicted = classifier.predict(img.reshape((1,img.shape[0]*img.shape[1] )))
to this:
predicted = classifier.predict(img.reshape(img.shape[0]*img.shape[1]))
If that doesn't fix it, you can always try recasting the data type as well with 
img = img.astype(digits.images.dtype).
I hope that helps. Debugging by proxy is a lot harder than actually sitting in front of your computer :)
Edit: According to the SciPy documentation, the training data contains integer values from 0 to 16. The values in your input image should be scaled to fit the same interval. (http://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_digits.html#sklearn.datasets.load_digits)
",scikit-learn
xgboost xgbclassifier defaults in python,"I am attempting to use XGBoosts classifier to classify some binary data. When I do the simplest thing and just use the defaults (as follows)
clf = xgb.XGBClassifier()
metLearn=CalibratedClassifierCV(clf, method='isotonic', cv=2)
metLearn.fit(train, trainTarget)
testPredictions = metLearn.predict(test)

I get reasonably good classification results.
My next step was to try tuning my parameters. Guessing from the parameters guide at...
https://github.com/dmlc/xgboost/blob/master/doc/parameter.rst
I wanted to start from the default and work from there...
# setup parameters for xgboost
param = {}
param['booster'] = 'gbtree'
param['objective'] = 'binary:logistic'
param[""eval_metric""] = ""error""
param['eta'] = 0.3
param['gamma'] = 0
param['max_depth'] = 6
param['min_child_weight']=1
param['max_delta_step'] = 0
param['subsample']= 1
param['colsample_bytree']=1
param['silent'] = 1
param['seed'] = 0
param['base_score'] = 0.5

clf = xgb.XGBClassifier(params)
metLearn=CalibratedClassifierCV(clf, method='isotonic', cv=2)
metLearn.fit(train, trainTarget)
testPredictions = metLearn.predict(test)

The result is everything being predicted to be one of the conditions and not the other.
curiously if I set
params={}

which I expected to give me the same defaults as not feeding any parameters, I get the same thing happening
So does anyone know what the defaults for XGBclassifier is? so that I can start tuning?
","That isn't how you set parameters in xgboost. You would either want to pass your param grid into your training function, such as xgboost's train or sklearn's GridSearchCV, or you would want to use your XGBClassifier's set_params method. Another thing to note is that if you're using xgboost's wrapper to sklearn (ie: the XGBClassifier() or XGBRegressor() classes) then the paramater names used are the same ones used in sklearn's own GBM class (ex: eta --> learning_rate). I'm not seeing where the exact documentation for the sklearn wrapper is hidden, but the code for those classes is here: https://github.com/dmlc/xgboost/blob/master/python-package/xgboost/sklearn.py
For your reference here is how you would set the model object parameters directly.
>>> grid = {'max_depth':10}
>>> 
>>> clf = XGBClassifier()
>>> clf.max_depth
3
>>> clf.set_params(**grid)
XGBClassifier(base_score=0.5, colsample_bylevel=1, colsample_bytree=1,
       gamma=0, learning_rate=0.1, max_delta_step=0, max_depth=10,
       min_child_weight=1, missing=None, n_estimators=100, nthread=-1,
       objective='binary:logistic', reg_alpha=0, reg_lambda=1,
       scale_pos_weight=1, seed=0, silent=True, subsample=1)
>>> clf.max_depth
10

EDIT:
I suppose you can set parameters on model creation, it just isn't super typical to do so since most people grid search in some means. However if you do so you would need to either list them as full params or use **kwargs. For example:
>>> XGBClassifier(max_depth=10)
XGBClassifier(base_score=0.5, colsample_bylevel=1, colsample_bytree=1,
       gamma=0, learning_rate=0.1, max_delta_step=0, max_depth=10,
       min_child_weight=1, missing=None, n_estimators=100, nthread=-1,
       objective='binary:logistic', reg_alpha=0, reg_lambda=1,
       scale_pos_weight=1, seed=0, silent=True, subsample=1)
>>> XGBClassifier(**grid)
XGBClassifier(base_score=0.5, colsample_bylevel=1, colsample_bytree=1,
       gamma=0, learning_rate=0.1, max_delta_step=0, max_depth=10,
       min_child_weight=1, missing=None, n_estimators=100, nthread=-1,
       objective='binary:logistic', reg_alpha=0, reg_lambda=1,
       scale_pos_weight=1, seed=0, silent=True, subsample=1)

Using a dictionary as input without **kwargs will set that parameter to literally be your dictionary:
>>> XGBClassifier(grid)
XGBClassifier(base_score=0.5, colsample_bylevel=1, colsample_bytree=1,
       gamma=0, learning_rate=0.1, max_delta_step=0,
       max_depth={'max_depth': 10}, min_child_weight=1, missing=None,
       n_estimators=100, nthread=-1, objective='binary:logistic',
       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=0, silent=True,
       subsample=1)

",scikit-learn
how do i pass sklearns train_test_split actual dataseries and not single values as input argument,"I want to train an LSTM-based RNN model for binary classification and for that I wanted to use tensorflow keras model with LSTM layers. In order to do so, I need testing input and output as well as validation input and output, which I wanted to generate with sklearns train_test_split.
def prepare_data(self, satellites):
        """"""
        Prepare time-series data for RNN.
        """"""
        feature_sequences = []
        labels = []
        
        for sat in satellites:
            if sat.manoeuvrability is not None:
                # Stack the orbital parameters as time-series features (epochs will be the time dimension)
                features = np.column_stack((
                    sat.apoapses,
                    sat.periapses,
                    sat.inclinations,
                    sat.mean_motions,
                    sat.eccentricities,
                    sat.semimajor_axes,
                    sat.orbital_energy
                ))
                feature_sequences.append(features)
                labels.append(sat.manoeuvrability)
        
        X = np.array(feature_sequences, dtype=object)
        y = np.array(labels)
        
        return train_test_split(X, y, test_size=0.2, random_state=42)

train_test_split returns me a None-Value. Removing the dtype=object cast in the argument leads me to an

ValueError: setting an array element with a sequence. The requested
array has an inhomogeneous shape after 1 dimensions. The detected
shape was (73,) + inhomogeneous part.

How do I properly form my features vector for sklearns train_test_split if I want to pass literal timeseries as arguments? The time-dependence is important in my case, so I really cant work around with manually breaking down time series to the average or something
","I simplified your code to this:
def prepare_data():
        feature_sequences = []
        labels = []
        
        for i in range(10):

            features = np.column_stack((2*i*5, ""hello""))  # wrong?
            # features = (2*i*5, ""hello"")  # correct

            feature_sequences.append(features)
            labels.append(i)

        X = np.array(feature_sequences, dtype=object)
        y = np.array(labels)
        
        return train_test_split(X, y, test_size=0.2, random_state=42)

The returned split for the features in a 3D array which it shouldn't be. Just replace the line with column_stack and the resulting split looks better.
",scikit-learn
decision tree with a probability target,"I'm currently working on a model to predict a probability of fatality once a person is infected with the Corona virus.
I'm using a Dutch dataset with categorical variables: date of infection, fatality or cured, gender, age-group etc.
It was suggested to use a decision tree, which I've already built.
Since I'm new to decision trees I would like some assistance.
I would like to have the prediction (target variable) expressed in a probability (%), not in a binary output.
How can I achieve this?
Also I want to play around with samples by inputting the data myself and see what the outcome is.
For instance: let's take someone who is 40, male etc. and calculate what its survival chance is.
How can I achieve this?
I've attached the code below:
from pandas import read_csv
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier 
import pandas as pd
import random as rnd

filename = '/Users/sef/Downloads/pima-indians-diabetes.csv'
names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']
dataframe = read_csv(filename, names=names)
array = dataframe.values
X = array[:,0:8]
Y = array[:,8]

X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state=1234)

model = DecisionTreeClassifier()

model.fit(X_train, Y_train)

DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,
            max_features=None, max_leaf_nodes=None,
            min_impurity_decrease=0.0, min_impurity_split=None,
            min_samples_leaf=1, min_samples_split=2,
            min_weight_fraction_leaf=0.0, presort=False, random_state=None,
            splitter='best')

rnd.seed(123458)
X_new = X[rnd.randrange(X.shape[0])]
X_new = X_new.reshape(1,8)
YHat = model.predict_proba(X_new)


df = pd.DataFrame(X_new, columns = names[:-1])
df[""predicted""] = YHat
print(df)


","you can use the method ""predict_proba"" of the DecisionTreeClassifier to compute the probabilities instead of the binary classification values.
In order to test individual data, that you can create by hand, you have to create an array of the shape of your X_test data (just that it only has one entry). Then you can use that with model.predict(array) or model.predict_proba(array).
By the way, your tree is currently not useful for retrieving probabilities. There is an article that explains the problem very well: https://web.archive.org/web/20210507022823/https://rpmcruz.github.io/machine%20learning/2018/02/09/probabilities-trees.html
So you can fix your code by defining the max_depths of your tree:
from pandas import read_csv
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier 
import pandas as pd
import random as rnd

filename = 'pima-indians-diabetes.csv'
names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']
dataframe = read_csv(filename, names=names)
array = dataframe.values
X = array[:,0:8]
Y = array[:,8]

X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state=1234)

model = DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=1,
            max_features=None, max_leaf_nodes=None,
            min_impurity_decrease=0.0, min_impurity_split=None,
            min_samples_leaf=1, min_samples_split=2,
            min_weight_fraction_leaf=0.0, presort=False, random_state=None,
            splitter='best')

model.fit(X_train, Y_train)

rnd.seed(123458)
X_new = X[rnd.randrange(X.shape[0])]
X_new = X_new.reshape(1,8)
YHat = model.predict_proba(X_new)


df = pd.DataFrame(X_new, columns = names[:-1])
df[""predicted""] = list(YHat)
print(df)

",scikit-learn
sklearn stackingclassifier very slow and inconsistent cpu usage,"I've been trying out the StackingClassifier and StackingRegressor from sklearn recently but I've noticed it's always very slow and uses my cpu inefficiently. Let's say (just for the sake of this example) that I want to use StackingClassifier to stack a random forest and lightgbm, also using lightgbm as the final classifier. In this case I would expect the time it takes to run the StackingClassifier to be roughly equal to the time it takes to run an individual random forest + time to run 2 individual lightgbm + some small margin (so basically the sum of the parts + the time to train the StackingClassifier itself + small margin), however in practice it seems to take several times as long. Example:
import numpy as np
from sklearn.datasets import load_iris
from sklearn.ensemble import RandomForestClassifier
from sklearn.ensemble import StackingClassifier
import lightgbm as ltb
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import StratifiedKFold

X,y = load_iris(return_X_y=True)
cv = StratifiedKFold(n_splits=10)
lgbm = ltb.LGBMClassifier(n_jobs=4)
rf = RandomForestClassifier()

First just LightGBM, going by wall time this takes about 140ms on my computer:
%%time
scores = cross_val_score(lgbm, X, y, scoring='accuracy', cv=cv, n_jobs=4, error_score='raise')
np.mean(scores)

And just a random forest, this takes about 220ms for me:
%%time
scores = cross_val_score(rf, X, y, scoring='accuracy', cv=cv, n_jobs=-1, error_score='raise')
np.mean(scores)

And now a StackingClassifier that combines these two. Since it is basically running the above two blocks of code + another round of lightgbm, I would expect it to take roughly 250+120+120=490ms, but instead it takes about 3000ms, over 6x as long:
%%time
estimators = [
     ('rf', rf),
     ('lgbm,', lgbm)
     ]

clf = StackingClassifier(
    estimators=estimators, final_estimator=lgbm, passthrough=True)

scores = cross_val_score(clf, X, y, scoring='accuracy', cv=cv, n_jobs=4, error_score='raise')
np.mean(scores)    

I've also noticed (when running this exact same code on a bigger dataset so it takes long enough for me to be able to monitor my cpu usage) that the cpu usage with the StackingClassifier is all over the place.
For example, cpu usage running the individual lightgbm:
cpu usage running the individual lightgbm
(basically consistently 100%, so using cpu efficiently)
cpu usage running lightgbm as stackingclassifier
(all over the place, usually nowehere near 100%)
Am I doing something wrong that is causing StackingClassifier to be this much slower than the sum of the parts?
","The Sklearn's StackingClassifier does not implement a Cross validation for the estimators, only for the final meta-estimators. Indeed, it implements a cross_val_predict that is set to 5 by default.StackingClassifier Documentation
In your code you call the StackingClassifier inside the 'cross_val_score'. It is basically executing the cross_val_predict of the StackingClassifier as many times as the cross_val_score's outer cv that you set.
This may explain the wall time difference.
",scikit-learn
predict with sklearn logisticregression and randomforest models always predict minority class 1,"I'm building a Logistic Regression model to predict if a transaction is valid (1) or not (0) with a dataset of just 150 observations. My data is distributed as follows between the two classes:

106 observations are 0 (not valid)
44 observations are 1 (valid)

I am using two predictors (both numerical). Despite the data being mostly 0's, my classifier only predicts 1's for every transaction in my test set even though most of them should be 0. The classifier never outputs a 0 for any observation.
Here is my entire code:
# Logistic Regression
import numpy as np
import pandas as pd
from pandas import Series, DataFrame

import scipy
from scipy.stats import spearmanr
from pylab import rcParams
import seaborn as sb
import matplotlib.pyplot as plt
import sklearn
from sklearn.preprocessing import scale
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn import metrics
from sklearn import preprocessing

address = ""dummy_csv-150.csv""
trades = pd.read_csv(address)
trades.columns=['location','app','el','rp','rule1','rule2','rule3','validity','transactions']
trades.head()

trade_data = trades.ix[:,(1,8)].values
trade_data_names = ['app','transactions']

# set dependent/response variable
y = trades.ix[:,7].values

# center around the data mean
X= scale(trade_data)

LogReg = LogisticRegression()

LogReg.fit(X,y)
print(LogReg.score(X,y))

y_pred = LogReg.predict(X)

from sklearn.metrics import classification_report

print(classification_report(y,y_pred)) 

log_prediction = LogReg.predict_log_proba(
    [
       [2, 14],[3,1], [1, 503],[1, 122],[1, 101],[1, 610],[1, 2120],[3, 85],[3, 91],[2, 167],[2, 553],[2, 144]
    ])
prediction = LogReg.predict([[2, 14],[3,1], [1, 503],[1, 122],[1, 101],[1, 610],[1, 2120],[3, 85],[3, 91],[2, 167],[2, 553],[2, 144]])

My model is defined as:
LogReg = LogisticRegression()  
LogReg.fit(X,y)

where X looks like this :
X = array([[1, 345],
       [1, 222],
       [1, 500],
       [2, 120]]....)

and Y is just 0 or 1 for each observation.
Normalized X that gets passed to the model is this:
[[-1.67177659  0.14396503]
 [-1.67177659 -0.14538932]
 [-1.67177659  0.50859856]
 [-1.67177659 -0.3853417 ]
 [-1.67177659 -0.43239119]
 [-1.67177659  0.743846  ]
 [-1.67177659  4.32195953]
 [ 0.95657805 -0.46062089]
 [ 0.95657805 -0.45591594]
 [ 0.95657805 -0.37828428]
 [ 0.95657805 -0.52884264]
 [ 0.95657805 -0.20420118]
 [ 0.95657805 -0.63705646]
 [ 0.95657805 -0.65587626]
 [ 0.95657805 -0.66763863]
 [-0.35759927 -0.25125067]
 [-0.35759927  0.60975496]
 [-0.35759927 -0.33358727]
 [-0.35759927 -0.20420118]
 [-0.35759927  1.37195666]
 [-0.35759927  0.27805607]
 [-0.35759927  0.09456307]
 [-0.35759927  0.03810368]
 [-0.35759927 -0.41121892]
 [-0.35759927 -0.64411389]
 [-0.35759927 -0.69586832]
 [ 0.95657805 -0.57353966]
 [ 0.95657805 -0.57353966]
 [ 0.95657805 -0.53825254]
 [ 0.95657805 -0.53354759]
 [ 0.95657805 -0.52413769]
 [ 0.95657805 -0.57589213]
 [ 0.95657805  0.03810368]
 [ 0.95657805 -0.66293368]
 [ 0.95657805  2.86107294]
 [-1.67177659  0.14396503]
 [-1.67177659 -0.14538932]
 [-1.67177659  0.50859856]
 [-1.67177659 -0.3853417 ]
 [-1.67177659 -0.43239119]
 [-1.67177659  0.743846  ]
 [-1.67177659  4.32195953]
 [ 0.95657805 -0.46062089]
 [ 0.95657805 -0.45591594]
 [ 0.95657805 -0.37828428]
 [ 0.95657805 -0.52884264]
 [ 0.95657805 -0.20420118]
 [ 0.95657805 -0.63705646]
 [ 0.95657805 -0.65587626]
 [ 0.95657805 -0.66763863]
 [-0.35759927 -0.25125067]
 [-0.35759927  0.60975496]
 [-0.35759927 -0.33358727]
 [-0.35759927 -0.20420118]
 [-0.35759927  1.37195666]
 [-0.35759927  0.27805607]
 [-0.35759927  0.09456307]
 [-0.35759927  0.03810368]
 [-0.35759927 -0.41121892]
 [-0.35759927 -0.64411389]
 [-0.35759927 -0.69586832]
 [ 0.95657805 -0.57353966]
 [ 0.95657805 -0.57353966]
 [ 0.95657805 -0.53825254]
 [ 0.95657805 -0.53354759]
 [ 0.95657805 -0.52413769]
 [ 0.95657805 -0.57589213]
 [ 0.95657805  0.03810368]
 [ 0.95657805 -0.66293368]
 [ 0.95657805  2.86107294]
 [-1.67177659  0.14396503]
 [-1.67177659 -0.14538932]
 [-1.67177659  0.50859856]
 [-1.67177659 -0.3853417 ]
 [-1.67177659 -0.43239119]
 [-1.67177659  0.743846  ]
 [-1.67177659  4.32195953]
 [ 0.95657805 -0.46062089]
 [ 0.95657805 -0.45591594]
 [ 0.95657805 -0.37828428]
 [ 0.95657805 -0.52884264]
 [ 0.95657805 -0.20420118]
 [ 0.95657805 -0.63705646]
 [ 0.95657805 -0.65587626]
 [ 0.95657805 -0.66763863]
 [-0.35759927 -0.25125067]
 [-0.35759927  0.60975496]
 [-0.35759927 -0.33358727]
 [-0.35759927 -0.20420118]
 [-0.35759927  1.37195666]
 [-0.35759927  0.27805607]
 [-0.35759927  0.09456307]
 [-0.35759927  0.03810368]
 [-0.35759927 -0.41121892]
 [-0.35759927 -0.64411389]
 [-0.35759927 -0.69586832]
 [ 0.95657805 -0.57353966]
 [ 0.95657805 -0.57353966]
 [ 0.95657805 -0.53825254]
 [ 0.95657805 -0.53354759]
 [ 0.95657805 -0.52413769]
 [ 0.95657805 -0.57589213]
 [ 0.95657805  0.03810368]
 [ 0.95657805 -0.66293368]
 [ 0.95657805  2.86107294]
 [-1.67177659  0.14396503]
 [-1.67177659 -0.14538932]
 [-1.67177659  0.50859856]
 [-1.67177659 -0.3853417 ]
 [-1.67177659 -0.43239119]
 [-1.67177659  0.743846  ]
 [-1.67177659  4.32195953]
 [ 0.95657805 -0.46062089]
 [ 0.95657805 -0.45591594]
 [ 0.95657805 -0.37828428]
 [ 0.95657805 -0.52884264]
 [ 0.95657805 -0.20420118]
 [ 0.95657805 -0.63705646]
 [ 0.95657805 -0.65587626]
 [ 0.95657805 -0.66763863]
 [-0.35759927 -0.25125067]
 [-0.35759927  0.60975496]
 [-0.35759927 -0.33358727]
 [-0.35759927 -0.20420118]
 [-0.35759927  1.37195666]
 [-0.35759927  0.27805607]
 [-0.35759927  0.09456307]
 [-0.35759927  0.03810368]
 [-0.35759927 -0.41121892]
 [-0.35759927 -0.64411389]
 [-0.35759927 -0.69586832]
 [ 0.95657805 -0.57353966]
 [ 0.95657805 -0.57353966]
 [ 0.95657805 -0.53825254]
 [ 0.95657805 -0.53354759]
 [ 0.95657805 -0.52413769]
 [ 0.95657805 -0.57589213]
 [ 0.95657805  0.03810368]
 [ 0.95657805 -0.66293368]
 [ 0.95657805  2.86107294]
 [-0.35759927  0.60975496]
 [-0.35759927 -0.33358727]
 [-0.35759927 -0.20420118]
 [-0.35759927  1.37195666]
 [-0.35759927  0.27805607]
 [-0.35759927  0.09456307]
 [-0.35759927  0.03810368]]

and Y is:
[0 0 0 0 0 0 1 1 0 0 0 1 1 1 1 0 0 0 0 1 0 0 0 0 1 1 0 0 0 0 0 0 1 1 1 0 0
 0 0 0 0 1 1 0 0 0 1 1 1 1 0 0 0 0 1 0 0 0 0 1 1 0 0 0 0 0 0 1 1 1 0 0 0 0
 0 0 1 1 0 0 0 1 1 1 1 0 0 0 0 1 0 0 0 0 1 1 0 0 0 0 0 0 1 1 1 0 0 0 0 0 0
 1 1 0 0 0 1 1 1 1 0 0 0 0 1 0 0 0 0 1 1 0 0 0 0 0 0 1 1 1 0 0 0 1 0 0 0]

The model metrics are:
             precision    recall  f1-score   support

          0       0.78      1.00      0.88        98
          1       1.00      0.43      0.60        49

avg / total       0.85      0.81      0.78       147

with a Score of 0.80
When I run model.predict_log_proba(test_data) I get probability intervals that look like this:
array([[ -1.10164032e+01,  -1.64301095e-05],
       [ -2.06326947e+00,  -1.35863187e-01],
       [            -inf,   0.00000000e+00],
       [            -inf,   0.00000000e+00],
       [            -inf,   0.00000000e+00],
       [            -inf,   0.00000000e+00],
       [            -inf,   0.00000000e+00],
       [            -inf,   0.00000000e+00],
       [            -inf,   0.00000000e+00],
       [            -inf,   0.00000000e+00],
       [            -inf,   0.00000000e+00],
       [            -inf,   0.00000000e+00]])

My test set is and all but 2 should be 0 but they're all classified as 1. This happens for every test set, even ones that have values the model trained on.
[2, 14],[3,1], [1, 503],[1, 122],[1, 101],[1, 610],[1, 2120],[3, 85],[3, 91],[2, 167],[2, 553],[2, 144]

I found a similar question here: https://stats.stackexchange.com/questions/168929/logistic-regression-is-predicting-all-1-and-no-0 but in this question, the problem seemed to be that the data was mostly 1's so it made sense the model would ouput 1s. My case is the opposite because the train data is mostly 0's but for some reason my model always outputs 1's for everything even though 1's are relatively few. I also tried a Random Forest Classifier to see if the model was wrong but the same thing happened. Perhaps it is my data but I don't know what's wrong with it since it meets all assumptions.
What could be wrong? The data meets all assumptions for the logistic model (both predictors are independent, output is binary, no missing data points).
","You are not scaling your test data.  You are correct to be scaling your train data when you do this:
X= scale(trade_data)

After you train your model you do not do the same with the test data:
log_prediction = LogReg.predict_log_proba(
[
   [2, 14],[3,1], [1, 503],[1, 122],[1, 101],[1, 610],[1, 2120],[3, 85],[3, 91],[2, 167],[2, 553],[2, 144]
])

The coefficients for your model were built expecting normalized inputs.  Your test data is not normalized.  Any positive coefficient of the model will be multiplied by a huge number since your data isn't scaled likely driving your predicted values to all be 1.
A general rule is whatever transformations you do on your training set should likewise be done on your testing set.  You should also apply the same transformation on your training set with your test set.  Instead of:
X = scale(trade_data)

You should create a scaler from your training data like this:
scaler = StandardScaler().fit(trade_date)
X = scaler.transform(trade_data)

Then later apply that scaler to your test data:
scaled_test = scaler.transform(test_x)

",scikit-learn
prediction using sklearn39s randomforestregressor,"Here's what my data looks like...
date,locale,category,site,alexa_rank,sessions,user_logins
20170110,US,1,google,1,500,5000
20170110,EU,1,google,2,400,2000
20170111,US,2,facebook,2,400,2000

... and so on. This is just a toy dataset I came up with, but which resembles the original data.
I'm trying to build a model to predict how many user logins and sessions a particular site would have, using sklearn's RandomForestRegressor.
I do the usual stuff, encoding categories to labels and I've trained my model on the first eight months of the year and now I'd like to predict logins and sessions for the ninth month. I've created one model trained on logins and another one trained on sessions.
My test dataset is of the same form:
date,locale,category,site,alexa_rank,sessions,user_logins
20170910,US,1,google,1,500,5000
20170910,EU,1,google,2,400,2000
20170911,US,2,facebook,2,400,2000

Ideally I'd like to pass in the test dataset without the columns I need predicted, but RandomForestRegressor complains about the dimensions being different between the training and test set.
When I pass the test dataset in its current form, the model predicts the exact values in the sessions and user_logins columns in most cases and values with tiny variations otherwise.
I zeroed out the  sessions and user_logins columns in the test data and passed it to the model but the model predicted nearly all zeroes.

Is my workflow correct? Am I using RandomForestRegressor correctly?
How am I getting so close to the actual values when my test dataset does contain actual values? Are the actual values in the test data being used in the prediction?
If the model works correctly, shouldn't I be getting the same values predicted if I zero out the columns I'm looking to predict (sessions and user_logins)?

","You shouldn't pass the column you want to predict in the test data. You workflow is not correct.
If X is the set of columns of the information you have,
if y is the set of columns of the information you want to predict,
then you should pass (X_train, y_train) during the training (using the method fit), and (X_test, ) only during the testing (using the method predict). You will obtain y_pred that you can compare with y_test if you have it.
In your example, if you want to predict user_logins:
X_train = array([['20170110', 'US', '1', 'google', '1', '500'],
                 ['20170110', 'EU', '1', 'google', '2', '400'],
                 ['20170111', 'US', '2', 'facebook', '2', '400']],
                dtype='<U21')
y_train = array(['5000', '2000', '2000'], dtype='<U21')

X_test = array([['20170112', 'EU', '2', 'google', '1', '500'],
                ['20170113', 'US', '1', 'facebook', '2', '400'],
                ['2017014', 'US', '2', 'google', '1', '500']],
               dtype='<U21')

estimator = RandomForestRegressor().fit(X_train, y_train)
y_pred = estimator.predict(X_test)

Take a look at the documentation for more examples, or at the tutorials.
",scikit-learn
how do i install sklearn module properly,"I'm trying to install sklearn module using pip command but after the installation is completed , all I can see is this folder
C:\Users\Aditi\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.8_qbz5n2kfra8p0\LocalCache\local-packages\Python38\site-packages\sklearn-0.0-py3.8.egg-info 

In my directory and even the error says module name sklearn not found.
I've tried reinstalling it many times but still I'm not able to see the main sklearn folder in the above directory.Only 1 folder is installed i.e sklearn-0.0-py3.8.egg-info .Can anyone please help?
","Try to install using command pip install scikit-learn.
If it still not work for you, you can update the numpy or reinstall the numpy.
You can check here all the help related to installation and verifying the installation of scikit-learn
",scikit-learn
sklearn plot confusion matrix with labels,"I want to plot a confusion matrix to visualize the classifer's performance, but it shows only the numbers of the labels, not the labels themselves:
from sklearn.metrics import confusion_matrix
import pylab as pl
y_test=['business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business']

pred=array(['health', 'business', 'business', 'business', 'business',
       'business', 'health', 'health', 'business', 'business', 'business',
       'business', 'business', 'business', 'business', 'business',
       'health', 'health', 'business', 'health'], 
      dtype='|S8')

cm = confusion_matrix(y_test, pred)
pl.matshow(cm)
pl.title('Confusion matrix of the classifier')
pl.colorbar()
pl.show()

How can I add the labels (health, business..etc) to the confusion matrix?
","As hinted in this question, you have to ""open"" the lower-level artist API, by storing the figure and axis objects passed by the matplotlib functions you call (the fig, ax and cax variables below). You can then replace the default x- and y-axis ticks using set_xticklabels/set_yticklabels:
from sklearn.metrics import confusion_matrix

labels = ['business', 'health']
cm = confusion_matrix(y_test, pred, labels)
print(cm)
fig = plt.figure()
ax = fig.add_subplot(111)
cax = ax.matshow(cm)
plt.title('Confusion matrix of the classifier')
fig.colorbar(cax)
ax.set_xticklabels([''] + labels)
ax.set_yticklabels([''] + labels)
plt.xlabel('Predicted')
plt.ylabel('True')
plt.show()

Note that I passed the labels list to the confusion_matrix function to make sure it's properly sorted, matching the ticks.
This results in the following figure:

",scikit-learn
sklearn can39t find lapack in new conda environment,"Question:
Why can't sklearn detect its liblapack dependency and how do I fix this?
Background:
I the following conda environment (torch-nightly) with scikit-learn installed (note that the liblapack dependency also exists):
(torch-nightly) au@a2 ~ % conda list |grep ""scikit-learn""
scikit-learn              1.1.1            py38hc377ac9_0  
(torch-nightly) au@a2 ~ % conda list |grep ""liblapack""
liblapack                 3.9.0           15_osxarm64_openblas    conda-forge

Problem:
I get the following error when attempting to import sklearn:
(torch-nightly) au@a2 ~ % python -c ""import sklearn""                                                              
Traceback (most recent call last):
  File ""<string>"", line 1, in <module>
  File ""/Users/au/miniconda3/envs/torch-nightly/lib/python3.8/site-packages/sklearn/__init__.py"", line 82, in <module>
    from .base import clone
  File ""/Users/au/miniconda3/envs/torch-nightly/lib/python3.8/site-packages/sklearn/base.py"", line 17, in <module>
    from .utils import _IS_32BIT
  File ""/Users/au/miniconda3/envs/torch-nightly/lib/python3.8/site-packages/sklearn/utils/__init__.py"", line 22, in <module>
    from scipy.sparse import issparse
  File ""/Users/au/miniconda3/envs/torch-nightly/lib/python3.8/site-packages/scipy/sparse/__init__.py"", line 283, in <module>
    from . import csgraph
  File ""/Users/au/miniconda3/envs/torch-nightly/lib/python3.8/site-packages/scipy/sparse/csgraph/__init__.py"", line 182, in <module>
    from ._laplacian import laplacian
  File ""/Users/au/miniconda3/envs/torch-nightly/lib/python3.8/site-packages/scipy/sparse/csgraph/_laplacian.py"", line 7, in <module>
    from scipy.sparse.linalg import LinearOperator
  File ""/Users/au/miniconda3/envs/torch-nightly/lib/python3.8/site-packages/scipy/sparse/linalg/__init__.py"", line 120, in <module>
    from ._isolve import *
  File ""/Users/au/miniconda3/envs/torch-nightly/lib/python3.8/site-packages/scipy/sparse/linalg/_isolve/__init__.py"", line 4, in <module>
    from .iterative import *
  File ""/Users/au/miniconda3/envs/torch-nightly/lib/python3.8/site-packages/scipy/sparse/linalg/_isolve/iterative.py"", line 9, in <module>
    from . import _iterative
ImportError: dlopen(/Users/au/miniconda3/envs/torch-nightly/lib/python3.8/site-packages/scipy/sparse/linalg/_isolve/_iterative.cpython-38-darwin.so, 0x0002): Library not loaded: '@rpath/liblapack.3.dylib'
  Referenced from: '/Users/au/miniconda3/envs/torch-nightly/lib/python3.8/site-packages/scipy/sparse/linalg/_isolve/_iterative.cpython-38-darwin.so'
  Reason: tried: '/Users/au/miniconda3/envs/torch-nightly/lib/python3.8/site-packages/scipy/sparse/linalg/_isolve/../../../../../../liblapack.3.dylib' (no such file), '/Users/au/miniconda3/envs/torch-nightly/lib/python3.8/site-packages/scipy/sparse/linalg/_isolve/../../../../../../liblapack.3.dylib' (no such file), '/Users/au/miniconda3/envs/torch-nightly/bin/../lib/liblapack.3.dylib' (no such file), '/Users/au/miniconda3/envs/torch-nightly/bin/../lib/liblapack.3.dylib' (no such file), '/usr/local/lib/liblapack.3.dylib' (no such file), '/usr/lib/liblapack.3.dylib' (no such file)

Research on Problem:
I confirmed that the ""missing"" library (liblapack.3.dylib) actually exists at the location shown in the error:
(torch-nightly) au@a2 ~ % ls miniconda3/envs/torch-nightly/lib/liblapack.3.dylib
miniconda3/envs/torch-nightly/lib/liblapack.3.dylib

","NOTE: This answer is for macOS users only, and involves copying dilyb(s) that may not be compatible all versions of sklearn.
I highly recommend trying to re-install the packages first.
If you continue to fail and have reached a point where you are willing to try anything, the following steps may resolve the issue.
I was able to resolve this by manually copying over a copy of the necessary dylib(s) out of a homebrew installation.
First: I ran brew install lapack
[NOTE: For me, this installed version (3.11), you may end up with a different version after running the command]
Then I identified all the relevant files in of /opt/homebrew/Cellar/lapack/3.11/lib/*:
liblapacke.dylib
libblas.3.11.0.dylib
libblas.3.dylib
libblas.dylib
liblapack.3.11.0.dylib
liblapack.3.dylib
liblapack.dylib
liblapacke.3.11.0.dylib
liblapacke.3.dylib

Identify a search path that I all my python packages would search.
I'm using miniconda for my python environment management, for me it was: ~/miniconda3/envs/maxEnt/lib/.
Step 1:
Delete all existing files with a matching name in this lib folder.
Step 2:
Copy the files referenced above.
Step 3:
Update all the symlinks (aliases) in these files [.dylib points to 3.11.0.dylib; .3.dylib points to 3.11.0.dylib].
",scikit-learn
can i set a random state for the complement naive bayes,"I note I can set the random state in random forest classifier, but not in Complement Naive Bayes. I get error
TypeError: __init__() got an unexpected keyword argument 'random_state'

I assume this is because naive_bayes is computing the probability of each class?
from sklearn.naive_bayes import ComplementNB
model= ComplementNB(random_state=0)

","That's because the model doesn't do anything needed to be randomly initialized or generated. see the source code for more details.
Be in case you want to reset the random seed in that line from your own side, you can do something like this.
import numpy as np
import random

def set_random_seed(seed=0):
  np.random.seed(seed)
  random.seed(seed)

Then you can reset the random seed right before calling your model
from sklearn.naive_bayes import ComplementNB

set_random_seed(0)
model= ComplementNB()

Read more about the mechanism of Complement Naive Bayes.
",scikit-learn
understanding scikit learn import variants,"Scikit learn import statements in their tutorials are on the form
from sklearn.decomposition import PCA

Another versions that works is
import sklearn.decomposition
pca = sklearn.decomposition.PCA(n_components = 2)

However
import sklearn
pca = sklearn.decomposition.PCA(n_components = 2)

does not, and complains
AttributeError: module 'sklearn' has no attribute 'decomposition'

Why is this, and how can I predict which ones will work and not so i don't have to test around? If the understanding and predictiveness extends to python packages in general that would be the best.
","sklearn doesn't automatically import its submodules. If you want to use sklearn.<SUBMODULE>, then you will need to import it explicitly e.g. import sklearn.<SUBMODULE>. Then you can use it without any further imports like result = sklearn.<SUBMODULE>.function(...).
Large packages often behave this way where they don't automatically import all the submodules.
Memory and load-time efficiency become worse if the submodules are automatically loaded; by specifying the submodule explicitly it saves on memory consumption and minimises the start-up time. I think namespace cluttering is another consideration, where explicit imports reduce the chance of naming conflicts and help maintain clarity about the specific functionality being used.
",scikit-learn
trying to find optimal price point in a data set,"I have a data set that looks like this.
Year    Quarter Quantity    Price
2000    1   23  142
2000    2   23  144
2000    3   23  147
2000    4   23  151
2001    1   22  160
2001    2   22  183
2001    3   22  186
2001    4   22  186
2002    1   21  212
2002    2   19  232
2002    3   19  223
2002    4   19  224
2003    1   19  231
2003    2   19  228
2003    3   19  238
2003    4   19  238
2004    1   19  234
2004    2   19  231
2004    3   20  239
2004    4   19  235
2005    1   19  233
2005    2   19  243
2005    3   20  244
2005    4   19  233
2006    1   19  234
2006    2   19  241

I am trying to figure out a pricing strategy to maximize revenue (optimal price) using Python.  I found a few example online, but didn't know how to adapt them to my specific scenario.  
This one looks good, but I'm not sure how to modify it to fit my data set.
https://www.datascience.com/resources/notebooks/python-dynamic-pricing
This one looks goo too, but again, I'm not sure how to make it work for me.
https://cscherrer.github.io/post/max-profit/
If someone here knows how to modify the sample code at one of those sites to fit the data I'm working with, I'd really like to see it.  Or, if you have a link to another site that answers my question, please do share it.  Thanks!
","Fundamentally I don't think there's a enough data here to be able to implement a pricing strategy based on pure statistics. The differences in quantity are barely outside of the standard deviation (std 1.6, mean 20.2). However theoretically what you want to do is:

convert the data to a more easily importable formate such as csv

Year,Quarter,Quantity,Price
2000,1,23,142
2000,2,23,144
2000,3,23,147
2000,4,23,151
2001,1,22,160
2001,2,22,183
2001,3,22,186
2001,4,22,186
2002,1,21,212
2002,2,19,232
2002,3,19,223
2002,4,19,224
2003,1,19,231
2003,2,19,228
2003,3,19,238
2003,4,19,238
2004,1,19,234
2004,2,19,231
2004,3,20,239
2004,4,19,235
2005,1,19,233
2005,2,19,243
2005,3,20,244
2005,4,19,233
2006,1,19,234
2006,2,19,241


load in the data prices = pd.read_csv(""price_data.csv"")
make a graph to visually show change in price with respect to quantity sns.scatterplot(x=prices[""Price""], y=prices[""Quantity""])

add columns for change in quantity and change in price

prices[""% Change in Quantity""] = prices[""Quantity""].pct_change()
prices[""% Change in Price""] = prices[""Price""].pct_change()


calculate the price elasticity prices[""Price Elasticity""] = prices[""% Change in Quantity""] / prices[""% Change in Price""]
Graph price elasticity vs the price


from this data you could then fit a model (depending on the complexity some order of polynomial makes sense), and use this to figure out at what point the price elasticity becomes too high and you wouldn't be making enough sales. But that's highly subjective and based more on business needs than anything else.
",scikit-learn
how to apply labelencoder to a polars dataframe column,"I'm trying to use scikit-learn's LabelEncoder with a Polars DataFrame to encode a categorical column. I am using the following code.
import polars as pl

from sklearn.preprocessing import LabelEncoder

df = pl.DataFrame({
    ""Color"" : [""red"",""white"",""blue""]
})

enc = LabelEncoder()

However, an error is raised.

ValueError: y should be a 1d array, got an array of shape () instead.

Next, I tried converting the column to a NumPy.
df.with_columns(
    enc.fit_transform(pl.col(""Color"").to_numpy()) 
)

Now, a different error is raised.

AttributeError: 'Expr' object has no attribute 'to_numpy'

Note. I found that .cast(pl.Categorical).to_physical() could be used to obtain the desired result. Still, I'd prefer using something like transform() on my test dataset.
df.with_columns(
    pl.col(""Color"").cast(pl.Categorical).to_physical().alias(""Color_encoded"")
)

","For such a call to an external API taking an entire sequence of values, such as enc.fit_transform, pl.Expr.map_batches could be used.
df.with_columns(
    pl.col(""Color"").map_batches(enc.fit_transform)
)

shape: (3, 1)
┌───────┐
│ Color │
│ ---   │
│ i64   │
╞═══════╡
│ 1     │
│ 2     │
│ 0     │
└───────┘

Note. It would be nice if enc.set_output(""polars"") (as outlined in this answer) was available for the LabelEncoder. However, this is not implemented.

You already shared an approach to label encoding a column using polars' native expression API. A cleaner way could rely on dense ranking as follows.
df.with_columns(
    pl.col(""Color"").rank(""dense"") - 1
)

Subtraction is used only to obtain an output with lowest label being 0.
",scikit-learn
how to make featureunion return dataframe,"So I currently have a Pipeline that has a lot of customer transformers:
p = Pipeline([
(""GetTimeFromDate"",TimeTransformer(""Date"")), #Custom Transformer that adds [""time""] column
(""GetZipFromAddress"",ZipTransformer(""Address"")), #Custom Transformer that adds [""zip""] column
(""GroupByTimeandZip"",GroupByTransformer([""time"",""zip""]) #Custom Transformer that adds onehot columns
])

Each transformer takes in a pandas dataframe and returns the same dataframe with one or more new columns.  It actually works quite well, but how can I run the ""GetTimeFromDate"" and the ""GetZipFromAddress"" steps in parallel?  
I would like to use FeatureUnion:
f = FeatureUnion([  
(""GetTimeFromDate"",TimeTransformer(""Date"")), #Custom Transformer that adds [""time""] column
(""GetZipFromAddress"",ZipTransformer(""Address"")), #Custom Transformer that adds [""zip""] column])
])

p = Pipeline([
(""FeatureUnionStep"",f),
(""GroupByTimeandZip"",GroupByTransformer([""time"",""zip""]) #Custom Transformer that adds onehot columns
])

But the problem is that FeatureUnion returns a numpy.ndarray, but the ""GroupByTimeandZip"" step needs a dataframe.  
Is there a way I can get FeatureUnion to return a pandas dataframe?
","For a FeatureUnion to output a DataFrame you can use the PandasFeatureUnion from this blog post. Also see the gist.
",scikit-learn
facing valueerror target is multiclass but average39binary39,"I'm trying to use Naive Bayes algorithm for my dataset. I'm able to find out the accuracy but trying to find out precision and recall for the same. But, it is throwing the following error:
ValueError: Target is multiclass but average='binary'. Please choose another average setting.

Can anyone please suggest me how to proceed with it. I have tried using average ='micro' in the precision and the recall scores. It worked without any errors but it is giving the same score for accuracy, precision, recall.
My dataset:
train_data.csv:
review,label
Colors & clarity is superb,positive
Sadly the picture is not nearly as clear or bright as my 40 inch Samsung,negative

test_data.csv:
review,label
The picture is clear and beautiful,positive
Picture is not clear,negative

My code:
import pandas as pd
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import precision_score
from sklearn.metrics import recall_score
from sklearn.metrics import confusion_matrix

X_train, y_train = pd.read_csv('train_data.csv')
X_test, y_test = pd.read_csv('test_data.csv')

vec = CountVectorizer() 
X_train_transformed = vec.fit_transform(X_train) 
X_test_transformed = vec.transform(X_test)

clf = MultinomialNB()
clf.fit(X_train_transformed, y_train)

score = clf.score(X_test_transformed, y_test)

y_pred = clf.predict(X_test_transformed)
cm = confusion_matrix(y_test, y_pred)

precision = precision_score(y_test, y_pred, pos_label='positive')
recall = recall_score(y_test, y_pred, pos_label='positive')

","You need to add the 'average' param. According to the documentation:

average : string, [None, ‘binary’ (default), ‘micro’, ‘macro’,
  ‘samples’, ‘weighted’]
This parameter is required for multiclass/multilabel targets. If None, the 
      scores for each class are returned. Otherwise, this
      determines the type of averaging performed on the data:

Do this:
print(""Precision Score : "",precision_score(y_test, y_pred, 
                                           pos_label='positive'
                                           average='micro'))
print(""Recall Score : "",recall_score(y_test, y_pred, 
                                           pos_label='positive'
                                           average='micro'))

Replace 'micro' with any one of the above options except 'binary'. Also, in the multiclass setting, there is no need to provide the 'pos_label' as it will be anyways ignored.
Update for comment:
Yes, they can be equal. Its given in the user guide here:

Note that for “micro”-averaging in a multiclass setting with all
  labels included will produce equal precision, recall and F, while
  “weighted” averaging may produce an F-score that is not between
  precision and recall.

",scikit-learn
use scikitlearn to classify into multiple categories,"I'm trying to use one of scikit-learn's supervised learning methods to classify pieces of text into one or more categories. The predict function of all the algorithms I tried just returns one match.
For example I have a piece of text:
""Theaters in New York compared to those in London""

And I have trained the algorithm to pick a place for every text snippet I feed it.
In the above example I would want it to return New York and London, but it only returns New York.
Is it possible to use scikit-learn to return multiple results? Or even return the label with the next highest probability? 
Thanks for your help.
---Update 
I tried using OneVsRestClassifier but I still only get one option back  per piece of text. Below is the sample code I am using
y_train = ('New York','London')


train_set = (""new york nyc big apple"", ""london uk great britain"")
vocab = {'new york' :0,'nyc':1,'big apple':2,'london' : 3, 'uk': 4, 'great britain' : 5}
count = CountVectorizer(analyzer=WordNGramAnalyzer(min_n=1, max_n=2),vocabulary=vocab)
test_set = ('nice day in nyc','london town','hello welcome to the big apple. enjoy it here and london too')

X_vectorized = count.transform(train_set).todense()
smatrix2  = count.transform(test_set).todense()


base_clf = MultinomialNB(alpha=1)

clf = OneVsRestClassifier(base_clf).fit(X_vectorized, y_train)
Y_pred = clf.predict(smatrix2)
print Y_pred

Result: ['New York' 'London' 'London']
","What you want is called multi-label classification. Scikits-learn can do that. See here: http://scikit-learn.org/dev/modules/multiclass.html.
I'm not sure what's going wrong in your example, my version of sklearn apparently doesn't have WordNGramAnalyzer. Perhaps it's a question of using more training examples or trying a different classifier? Though note that the multi-label classifier expects the target to be a list of tuples/lists of labels.
The following works for me:
import numpy as np
from sklearn.pipeline import Pipeline
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.svm import LinearSVC
from sklearn.feature_extraction.text import TfidfTransformer
from sklearn.multiclass import OneVsRestClassifier

X_train = np.array([""new york is a hell of a town"",
                    ""new york was originally dutch"",
                    ""the big apple is great"",
                    ""new york is also called the big apple"",
                    ""nyc is nice"",
                    ""people abbreviate new york city as nyc"",
                    ""the capital of great britain is london"",
                    ""london is in the uk"",
                    ""london is in england"",
                    ""london is in great britain"",
                    ""it rains a lot in london"",
                    ""london hosts the british museum"",
                    ""new york is great and so is london"",
                    ""i like london better than new york""])
y_train = [[0],[0],[0],[0],[0],[0],[1],[1],[1],[1],[1],[1],[0,1],[0,1]]
X_test = np.array(['nice day in nyc',
                   'welcome to london',
                   'hello welcome to new york. enjoy it here and london too'])   
target_names = ['New York', 'London']

classifier = Pipeline([
    ('vectorizer', CountVectorizer(min_n=1,max_n=2)),
    ('tfidf', TfidfTransformer()),
    ('clf', OneVsRestClassifier(LinearSVC()))])
classifier.fit(X_train, y_train)
predicted = classifier.predict(X_test)
for item, labels in zip(X_test, predicted):
    print '%s => %s' % (item, ', '.join(target_names[x] for x in labels))

For me, this produces the output:
nice day in nyc => New York
welcome to london => London
hello welcome to new york. enjoy it here and london too => New York, London

",scikit-learn
using hyphendash in python repository name and package name,"I am trying to make my git repository pip-installable. In preparation for that I am restructuring the repo to follow the right conventions. My understanding from looking at other repositories is that I should put all my source code in a package that has the same name as the repository name. E.g. if my repository is called myrepo, then the source code would all go into a package also called myrepo.
My repository has a hyphen in it for readability: e.g. my-repo. So if I wanted to make a package for it with the same name, it would have a hyphen in it as well. In this tutorial it says ""don't use hyphens"" for python package names. However I've seen well-established packages such as scikit-learn that have hyphens in their name. One thing that I have noticed though is that in the scikit-learn repo, the package name is not the same as the repo name and is instead called sklearn.
I think my discussion above boils down to the following questions:

When packaging a repo, what is the relationship between the repository's name and the package's name? Is there anything to beware of when having names that don't match?
Is it okay to have hyphens in package names? What about in repository names?
If the package name for scikit-learn is sklearn, then how come when I install it I do pip install scikit-learn instead of pip install sklearn?

","To answer your 1st point let me rephrase my answer to a different question.
The biggest source of misunderstanding is that the word ""package"" is heavily overloaded. There are 4 different names in the game — the name of the repository, the name of the directory being used for development (the one that contains setup.py), the name of the directory containing __init__.py and other importable modules, the name of distribution at PyPI. Quite often these 4 are the same or similar but that's not required.
The names of the repository and development directory can be any, their names don't play any role. Of course it's convenient to name them properly but that's only convenience.
The name of the directory with Python files name the package to be imported. Once the package is named for import the name usually stuck and cannot be changed.
The name of the distribution gives one a page at PyPI and the name of distribution files (source distribution, eggs, wheels). It's the name one puts in setup(name='distribution') call.
Let me show detailed real example. I've been maintaining a templating library called CheetahTemplate. I develop it in the development directory called cheetah3/. The distribution at PyPI is called Cheetah3; this is the name I put into setup(name='Cheetah3'). The top-level module is Cheetah hence one does import Cheetah.Template or from Cheetah import Template; that means that I have a directory cheetah3/Cheetah/.
The answer to 2 is: you can have dashes in repository names and PyPI distribution names but not in package (directories with __init__.py files) names and module (.py files) names because you cannot write in Python import xy-zzy, that would be subtraction and SyntaxError.
Point 3: The site and the repository names are scikit-learn, as well as the distribution name, but the importable package (the top-level directory with __init__.py) is sklearn.
PEP 8 has nothing to do with the question as it doesn't talk about distribution, only about importable packages and modules.
",scikit-learn
cubic spline regression with sklearn,"I want to regress very accurately a target which depends non-linearly from a single variable x. In my sklearn pipeline, I use:
pipe = Pipeline([('poly', PolynomialFeatures(3, include_bias=False)), \
                     ('regr', ElasticNet(random_state=0))])

This appears to give similar results to np.polyfit(x, y, 3) in terms of accuracy. However, I can basically go to machine precision by using cubic splines. See figure below where I show the data and various fits, along with the residual errors. [Note: the example below has 50 samples. I have 2000 samples in reality]
I have two questions:

Any clue why polyfit or polyfeat + Elasticnet aren't able to reach the same level of accuracy?
Any example of target regression with cubic splines in scikit-learn?

    import pandas as pd
    import numpy as np
    import matplotlib.pyplot as plt
    from scipy.interpolate import interp1d
    %matplotlib inline
    
    data = pd.read_csv('example.txt') # added to this post below
    
    p = np.polyfit(data['x'], data['y'], 3)
    data['polyfit'] = np.poly1d(p)(data['x'])
    f = interp1d(data['x'], data['y'], kind='cubic')
    data['spline'] = f(data['x'])
    
    fig, axes = plt.subplots(nrows=3, sharex=True)
    
    axes[0].plot(data['x'], data['polyfit'],'.', label='polyfit')
    axes[0].plot(data['x'], data['spline'],'.', label='spline')
    axes[0].plot(data['x'], data['y'],'.', label='true')
    axes[0].legend()
    
    axes[1].plot(data['x'], data['polyfit']-data['y'],'.', label='error polyfit')
    axes[1].legend()
    
    axes[2].plot(data['x'], data['spline']-data['y'],'.', label='error spline')
    axes[2].legend()
    
    plt.show()


Here are the data:
example.txt:
    ,x,y
257,6.26028462060192,-1233.748349982897
944,4.557099191827032,928.1430280794456
1560,6.765081341690966,-1807.9090703632864
504,4.0015671921214775,1683.311523022658
1499,3.0496689401255783,3055.291788377236
1247,5.608726443314061,-441.9226126757499
1856,4.6124942196224845,845.129184983355
1495,1.273838638033053,5479.078773760113
1052,5.353775782315625,-115.14032709875217
247,2.6495185259267076,3656.7467318648232
1841,9.73337795053495,-4884.806993807511
1574,1.1772247845133335,5544.080005636716
1116,5.698561786140379,-555.3435567718
1489,4.184371293153768,1427.6922357286753
603,1.568868565047676,5179.156099377357
358,4.534081088923849,960.3983442022857
774,9.304809492028289,-4468.215701489676
1525,9.17423541311121,-4340.565494266174
1159,6.705834877066449,-1750.189447626367
1959,3.0431599461645207,3065.358649171256
1086,1.3861557136230234,5378.274828554064
81,4.728366950632029,682.7245723055514
1791,6.954198834068505,-2027.0414501796324
234,2.8672306789699844,3330.7282514295102
1850,2.0086469278742363,4603.0931759401155
1531,9.843164998128215,-4973.735518791005
903,1.534448692052103,5220.331847067942
1258,7.243723209152924,-2354.629822080041
645,2.3302780902754514,4128.077572586273
1425,3.295574067849755,2694.766296765896
311,2.3225198086033756,4152.206609684557
219,8.479436097125713,-3665.2515034579396
1917,7.1524135031820135,-2253.3455629418195
1412,6.79800860136838,-1861.3756670478142
705,1.9001265482939966,4756.283634364785
663,3.441268690856777,2489.7632239249424
1871,6.473544271091015,-1480.6593600880415
1897,8.217615163361007,-3386.5427698021977
558,6.609652057181176,-1634.1672307700298
553,5.679571371137544,-524.352981663938
1847,6.487178186324092,-1500.1891501936236
752,9.377368455681758,-4548.188126821915
1469,8.586759667609758,-3771.691600599668
1794,6.649801445466815,-1674.4870918398076
968,1.6226439291315056,5117.8804886837
108,3.0077346937655647,3118.0786841570025
96,6.278616413290749,-1245.4758811316083
994,7.631678455127069,-2767.3224262153176
871,2.6696610777085863,3630.02481913033
1405,9.209358577104299,-4368.622350004463

","The former two methods fit a single cubic equation to your data, but (as the name implies) interp1d interpolates the data with cubic splines: that is, there is a cubic curve for each consecutive pair of points, and so you are guaranteed a perfect fit (up to computational precision).
",scikit-learn
how to resolve quotcannot import name 39_missingvalues39 from 39sklearnutils_param_validation39quot issue when trying to import imblearn,"I am trying to import imblearn into my python notebook after installing the required modules. However, I am getting the following error:

Additional info: I am using a virtual environment in Visual Studio Code.
I've made sure that venv was selected as interpreter and as the notebook kernel. I've reloaded the window and restarted the kernel several times. I have also uninstalled and installed imbalanced-learn and scikit-learn several times, with and without ""--upgrade"". I'm still getting the same error.

Edit: Full traceback of error
{
    ""name"": ""ImportError"",
    ""message"": ""cannot import name '_MissingValues' from 'sklearn.utils._param_validation' (c:\\Users\\wen\\OneDrive\\Desktop\\Colab_Notebooks\\.venv\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py)"",
    ""stack"": ""\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)\nCell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mimblearn\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[39m# Data Processing\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpandas\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mpd\u001b[39;00m\n\nFile \u001b[1;32mc:\\Users\\wen\\OneDrive\\Desktop\\Colab_Notebooks\\.venv\\Lib\\site-packages\\imblearn\\__init__.py:52\u001b[0m\n\u001b[0;32m     48\u001b[0m     sys\u001b[39m.\u001b[39mstderr\u001b[39m.\u001b[39mwrite(\u001b[39m\""\u001b[39m\u001b[39mPartial import of imblearn during the build process.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\""\u001b[39m)\n\u001b[0;32m     49\u001b[0m     \u001b[39m# We are not importing the rest of scikit-learn during the build\u001b[39;00m\n\u001b[0;32m     50\u001b[0m     \u001b[39m# process, as it may not be compiled yet\u001b[39;00m\n\u001b[0;32m     51\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 52\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[0;32m     53\u001b[0m         combine,\n\u001b[0;32m     54\u001b[0m         ensemble,\n\u001b[0;32m     55\u001b[0m         exceptions,\n\u001b[0;32m     56\u001b[0m         metrics,\n\u001b[0;32m     57\u001b[0m         over_sampling,\n\u001b[0;32m     58\u001b[0m         pipeline,\n\u001b[0;32m     59\u001b[0m         tensorflow,\n\u001b[0;32m     60\u001b[0m         under_sampling,\n\u001b[0;32m     61\u001b[0m         utils,\n\u001b[0;32m     62\u001b[0m     )\n\u001b[0;32m     63\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_version\u001b[39;00m \u001b[39mimport\u001b[39;00m __version__\n\u001b[0;32m     64\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mbase\u001b[39;00m \u001b[39mimport\u001b[39;00m FunctionSampler\n\nFile \u001b[1;32mc:\\Users\\wen\\OneDrive\\Desktop\\Colab_Notebooks\\.venv\\Lib\\site-packages\\imblearn\\combine\\__init__.py:5\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m\""\""\""The :mod:`imblearn.combine` provides methods which combine\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[39mover-sampling and under-sampling.\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[39m\""\""\""\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_smote_enn\u001b[39;00m \u001b[39mimport\u001b[39;00m SMOTEENN\n\u001b[0;32m      6\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_smote_tomek\u001b[39;00m \u001b[39mimport\u001b[39;00m SMOTETomek\n\u001b[0;32m      8\u001b[0m __all__ \u001b[39m=\u001b[39m [\u001b[39m\""\u001b[39m\u001b[39mSMOTEENN\u001b[39m\u001b[39m\""\u001b[39m, \u001b[39m\""\u001b[39m\u001b[39mSMOTETomek\u001b[39m\u001b[39m\""\u001b[39m]\n\nFile \u001b[1;32mc:\\Users\\wen\\OneDrive\\Desktop\\Colab_Notebooks\\.venv\\Lib\\site-packages\\imblearn\\combine\\_smote_enn.py:12\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mbase\u001b[39;00m \u001b[39mimport\u001b[39;00m clone\n\u001b[0;32m     10\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m \u001b[39mimport\u001b[39;00m check_X_y\n\u001b[1;32m---> 12\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mbase\u001b[39;00m \u001b[39mimport\u001b[39;00m BaseSampler\n\u001b[0;32m     13\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mover_sampling\u001b[39;00m \u001b[39mimport\u001b[39;00m SMOTE\n\u001b[0;32m     14\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mover_sampling\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mbase\u001b[39;00m \u001b[39mimport\u001b[39;00m BaseOverSampler\n\nFile \u001b[1;32mc:\\Users\\wen\\OneDrive\\Desktop\\Colab_Notebooks\\.venv\\Lib\\site-packages\\imblearn\\base.py:21\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmulticlass\u001b[39;00m \u001b[39mimport\u001b[39;00m check_classification_targets\n\u001b[0;32m     20\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m \u001b[39mimport\u001b[39;00m check_sampling_strategy, check_target_type\n\u001b[1;32m---> 21\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_param_validation\u001b[39;00m \u001b[39mimport\u001b[39;00m validate_parameter_constraints\n\u001b[0;32m     22\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_validation\u001b[39;00m \u001b[39mimport\u001b[39;00m ArraysTransformer\n\u001b[0;32m     25\u001b[0m \u001b[39mclass\u001b[39;00m \u001b[39mSamplerMixin\u001b[39;00m(BaseEstimator, metaclass\u001b[39m=\u001b[39mABCMeta):\n\nFile \u001b[1;32mc:\\Users\\wen\\OneDrive\\Desktop\\Colab_Notebooks\\.venv\\Lib\\site-packages\\imblearn\\utils\\_param_validation.py:908\u001b[0m\n\u001b[0;32m    906\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_param_validation\u001b[39;00m \u001b[39mimport\u001b[39;00m generate_valid_param  \u001b[39m# noqa\u001b[39;00m\n\u001b[0;32m    907\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_param_validation\u001b[39;00m \u001b[39mimport\u001b[39;00m validate_parameter_constraints  \u001b[39m# noqa\u001b[39;00m\n\u001b[1;32m--> 908\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_param_validation\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[0;32m    909\u001b[0m     HasMethods,\n\u001b[0;32m    910\u001b[0m     Hidden,\n\u001b[0;32m    911\u001b[0m     Interval,\n\u001b[0;32m    912\u001b[0m     Options,\n\u001b[0;32m    913\u001b[0m     StrOptions,\n\u001b[0;32m    914\u001b[0m     _ArrayLikes,\n\u001b[0;32m    915\u001b[0m     _Booleans,\n\u001b[0;32m    916\u001b[0m     _Callables,\n\u001b[0;32m    917\u001b[0m     _CVObjects,\n\u001b[0;32m    918\u001b[0m     _InstancesOf,\n\u001b[0;32m    919\u001b[0m     _IterablesNotString,\n\u001b[0;32m    920\u001b[0m     _MissingValues,\n\u001b[0;32m    921\u001b[0m     _NoneConstraint,\n\u001b[0;32m    922\u001b[0m     _PandasNAConstraint,\n\u001b[0;32m    923\u001b[0m     _RandomStates,\n\u001b[0;32m    924\u001b[0m     _SparseMatrices,\n\u001b[0;32m    925\u001b[0m     _VerboseHelper,\n\u001b[0;32m    926\u001b[0m     make_constraint,\n\u001b[0;32m    927\u001b[0m     validate_params,\n\u001b[0;32m    928\u001b[0m )\n\n\u001b[1;31mImportError\u001b[0m: cannot import name '_MissingValues' from 'sklearn.utils._param_validation' (c:\\Users\\wen\\OneDrive\\Desktop\\Colab_Notebooks\\.venv\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py)""
} 


The versions of the modules are as follows:
scikit-learn 1.3.0
imblearn 0.0
imbalanced-learn 0.10.1
","I was having the same issue, downgrading to scikit-learn 1.2.2 fixed it for me
",scikit-learn
snowflake snowpark quotimport sklearnquot results in quotno module foundquot,"I'm using a python worksheet in Snowflake.
When I got to information_schema.packages, scikit-learn is clearly there and installed.
But when I reference 'sklearn' (don't really understand where this full-name vs. shortened name comes from, but sklearn is obviously the reference to scikit-learn package). It says module not found.
My understanding is that it's pre-installed with Snowpark. So what's the deal? What am I doing wrong?
","For Python Worksheet UI - Package Picker:


For Snowflake Notebooks:

Import package from Anaconda

or

environment.yml


",scikit-learn
partialdependencedisplayfrom_estimator plots having lines with 0 values,"Need to evaluate the two way interaction between two variables after regressor model.
Used PartialDependenceDisplay.from_estimator to plot but the contour  lines inside the plot all have value 0.Not sure what might cause this.
Checked the data and model and there  are no problems while loading the model and data.
Checked the other two variable combinations and they have same issue.
from sklearn.inspection import partial_dependence, PartialDependenceDisplay
model = load_model(model_path)
model_features = model.feature_name_

fig, ax = plt.subplots(figsize=(10,5))
X = training_data[model_features]
PartialDependenceDisplay.from_estimator(model, X, features=[('temperature',  'speed')], ax=ax, n_jobs=-1, grid_resolution=20)


","Most probably your contour values are all < 0.005.
Contour labels are formatted as ""%2.2f"" and there appears to be no documented way of changing this format. The only workaround I could think of is to retrieve the labels and their values and replace the label texts:
import matplotlib.pyplot as plt
from matplotlib.text import Text
import numpy as np
from sklearn.datasets import make_friedman1
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.inspection import PartialDependenceDisplay

X, y = make_friedman1()
clf = GradientBoostingRegressor(n_estimators=10).fit(X, y)

pdd = PartialDependenceDisplay.from_estimator(clf, X, [0, (0, 1)])

for c in pdd.axes_[0][1].get_children():
  if isinstance(c, Text):
    try:
       label_value = float(c.get_text())
    except ValueError:
       continue
    idx = np.argmin(abs(pdd.contours_[0][1].levels - label_value))
    c.set_text(f'{pdd.contours_[0][1].levels[idx]:g}')


Update 1
The above method doesn't work if all existing labels are identical. A somewhat unreliable quick and dirty workaround would be to rely on the fact that the label texts are added to the Axes in ascending order. The first and last level are not labelled. This leads to the following example:
import matplotlib.pyplot as plt
from matplotlib.text import Text
from sklearn.datasets import make_friedman1
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.inspection import PartialDependenceDisplay

X, y = make_friedman1(random_state=42)
clf = GradientBoostingRegressor(n_estimators=10).fit(X, y)

pdd = PartialDependenceDisplay.from_estimator(clf, X, [0, (0, 1)])

i = 1
for c in pdd.axes_[0][1].get_children():
  if isinstance(c, Text) and c.get_text():
    c.set_text(f'{pdd.contours_[0][1].levels[i]:g}')
    i += 1


Update 2
Another (reliable but still hacky) possibility is to overwrite the clabel function used by Scikit with your own version that uses an appropriate format specification. In order to get hold of this function you'll have to provide your own Axes instance to PartialDependenceDisplay.from_estimator:
import matplotlib.pyplot as plt
from sklearn.datasets import make_friedman1
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.inspection import PartialDependenceDisplay

fig, axes = plt.subplots(ncols=2)

original_clabel = axes[1].clabel
def new_clabel(CS, **kwargs):
  del kwargs['fmt']
  return original_clabel(CS, fmt='%2.5f', **kwargs)
axes[1].clabel = new_clabel

X, y = make_friedman1(random_state=42)
clf = GradientBoostingRegressor(n_estimators=10).fit(X, y)

pdd = PartialDependenceDisplay.from_estimator(clf, X, [0, (0, 1)], ax=axes)


",scikit-learn
valueerror with scikitlearn,"link to car data.csv
My code:
import pandas as pd
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
car_data = pd.read_csv('car_data.csv')

# Create X
X = car_data.drop('Buy Rate', axis=1)

# Create Y
y = car_data['Buy Rate']

clf = RandomForestClassifier()
clf.get_params()

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)
clf.fit(X_train, y_train)

After the line with clf.fit, this error pops up:
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
/tmp/ipykernel_51905/2395142735.py in ?()
----> 1 clf.fit(X_train, y_train)

~/Desktop/ml-course/env/lib/python3.10/site-packages/sklearn/base.py in ?(estimator, *args, **kwargs)
   1147                 skip_parameter_validation=(
   1148                     prefer_skip_nested_validation or global_skip_validation
   1149                 )
   1150             ):
-> 1151                 return fit_method(estimator, *args, **kwargs)

~/Desktop/ml-course/env/lib/python3.10/site-packages/sklearn/ensemble/_forest.py in ?(self, X, y, sample_weight)
    344         """"""
    345         # Validate or convert input data
    346         if issparse(y):
    347             raise ValueError(""sparse multilabel-indicator for y is not supported."")
--> 348         X, y = self._validate_data(
    349             X, y, multi_output=True, accept_sparse=""csc"", dtype=DTYPE
    350         )
    351         if sample_weight is not None:

~/Desktop/ml-course/env/lib/python3.10/site-packages/sklearn/base.py in ?(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)
    617                 if ""estimator"" not in check_y_params:
    618                     check_y_params = {**default_check_params, **check_y_params}
    619                 y = check_array(y, input_name=""y"", **check_y_params)
    620             else:
--> 621                 X, y = check_X_y(X, y, **check_params)
    622             out = X, y
    623 
    624         if not no_val_X and check_params.get(""ensure_2d"", True):

~/Desktop/ml-course/env/lib/python3.10/site-packages/sklearn/utils/validation.py in ?(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)
   1143         raise ValueError(
   1144             f""{estimator_name} requires y to be passed, but the target y is None""
   1145         )
   1146 
-> 1147     X = check_array(
   1148         X,
   1149         accept_sparse=accept_sparse,
   1150         accept_large_sparse=accept_large_sparse,

~/Desktop/ml-course/env/lib/python3.10/site-packages/sklearn/utils/validation.py in ?(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)
    914                         )
    915                     array = xp.astype(array, dtype, copy=False)
    916                 else:
    917                     array = _asarray_with_order(array, order=order, dtype=dtype, xp=xp)
--> 918             except ComplexWarning as complex_warning:
    919                 raise ValueError(
    920                     ""Complex data not supported\n{}\n"".format(array)
    921                 ) from complex_warning

~/Desktop/ml-course/env/lib/python3.10/site-packages/sklearn/utils/_array_api.py in ?(array, dtype, order, copy, xp)
    376         # Use NumPy API to support order
    377         if copy is True:
    378             array = numpy.array(array, order=order, dtype=dtype)
    379         else:
--> 380             array = numpy.asarray(array, order=order, dtype=dtype)
    381 
    382         # At this point array is a NumPy ndarray. We convert it to an array
    383         # container that is consistent with the input's namespace.

~/Desktop/ml-course/env/lib/python3.10/site-packages/pandas/core/generic.py in ?(self, dtype)
   2082     def __array__(self, dtype: npt.DTypeLike | None = None) -> np.ndarray:
   2083         values = self._values
-> 2084         arr = np.asarray(values, dtype=dtype)
   2085         if (
   2086             astype_is_view(values.dtype, arr.dtype)
   2087             and using_copy_on_write()

ValueError: could not convert string to float: 'Hyundai'

I have viewed similar questions that have been posted here, but none help.
","The error arises because your features(i.e.: Make and Color) in X is categorical, if you encode them into numerical variables using Label Encoder:
from sklearn.preprocessing import LabelEncoder
le = LabelEncoder()
car_data['Make'] = le.fit_transform(car_data['Make'])
car_data['Color'] = le.fit_transform(car_data['Color'])

then your result will be like this if car_data.head(2) is executed:




Make
Year
Price
Mileage
Color
Buy Rate




0
30
2018
20000
50000
1
0.80


1
13
2019
25000
40000
4
0.70



This solves your problem!
But since your target variable(i.e.: Buy Rate) is continuous, you will get an error while training your data with RandomForestClassifier,
So in order to classify, first you need to bin your target variable.
num_bins = 3
bin_boundaries = [0, 0.5, 0.75, 1]
car_data['Buy Rate'] = pd.cut(car_data['Buy Rate'], bins=num_bins, labels=False)
car_data['Buy Rate'] = car_data['Buy Rate'].map({0: 'Low', 1: 'Medium', 2: 'High'})

Result:
0     Medium
1     Medium
2        Low
.     
.
.
32      High

After binning you can train your data with RandomForestClassifier.
import pandas as pd
from sklearn.model_selection import train_test_split
car_data = pd.read_csv('car_data.csv')

from sklearn.preprocessing import LabelEncoder
le = LabelEncoder()
car_data['Make'] = le.fit_transform(car_data['Make'])
car_data['Color'] = le.fit_transform(car_data['Color'])

num_bins = 3
bin_boundaries = [0, 0.5, 0.75, 1] 
car_data['Buy Rate'] = pd.cut(car_data['Buy Rate'], bins=num_bins, labels=False)
car_data['Buy Rate'] = car_data['Buy Rate'].map({0: 'Low', 1: 'Medium', 2: 'High'})

# Create X
X = car_data.drop('Buy Rate', axis=1)
# Create Y
y = car_data['Buy Rate'] # target variable

from sklearn.ensemble import RandomForestClassifier
clf = RandomForestClassifier()
clf.get_params()
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)
clf.fit(X_train, y_train)

Finally, this is how your code should be modified.
",scikit-learn
classifierchain with random forest why is npnan not supported even though base estimator handles it,"I'm working on a multilabel classification problem using the ClassifierChain approach with RandomForestClassifier as the base estimator. I've encountered an issue where my input matrix X contains np.nan values. When using RandomForestClassifier alone, it handles NaN values without any problem, as it natively supports missing values via its internal tree splitting mechanism.
This is confusing to me because the base estimator (RandomForestClassifier) does handle NaN values correctly. I don't understand why ClassifierChain, which is just a wrapper, raises this error when the underlying classifier doesn't have an issue with NaNs.
When I train a simple RandomClassifier it does handle np.nan:
from sklearn.ensemble import RandomForestClassifier
import numpy as np

X = np.array([np.nan, -1, np.nan, 1]).reshape(-1, 1)
y_single_label = [0, 0, 1, 1]

tree = RandomForestClassifier(random_state=0)
tree.fit(X, y_single_label)
X_test = np.array([np.nan]).reshape(-1, 1)
tree.predict(X_test)

Even when I use MultiOutputClassifier instead of ClassifierChain (which doesn't model dependencies between labels), the training proceeds without any errors, even with NaNs in the input - as expected.
import numpy as np
from sklearn.ensemble import RandomForestClassifier
from sklearn.multioutput import ClassifierChain , MultiOutputClassifier

X = np.array([np.nan, -1, np.nan, 1]).reshape(-1, 1)

# Two label columns for multilabel classification
y = np.array([[0, 1], [0, 0], [1, 0], [1, 1]])

# Base classifier
base_clf = RandomForestClassifier()

# MultiOutputClassifier (Binary Relevance) with the base classifier
clf_BR = MultiOutputClassifier(base_clf)

# Fitting the model
clf_BR.fit(X, y)

However, when I switch to the ClassifierChain approach:
# Classifier chain with the base classifier
clf_chain = ClassifierChain(base_clf)

# Fitting the model
clf_chain.fit(X, y)

I get the following error during hyperparameter tuning:

Trial 0 failed with parameters: {'n_estimators': 30, 'max_depth': 16, 'max_samples': 0.4497444900238575, 'max_features': 550, 'order_type': 'random'} because of the following error: ValueError('Input X contains NaN.\nClassifierChain does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values')

Since it's important for us to keep the missing values as they are and not impute or drop them, I'm wondering if there's a way to make ClassifierChain work with missing values. Is there any workaround or something I'm missing here?
Here are my environment details:

Python version: 3.12.5 (packaged by conda-forge)
scikit-learn version: 1.5.1

","Yes this seems to be how they have written this.
If you see the full stack trace, you will see that this code gets called eventually where the check for Nan does happen.
One workaround is to impute it but also create a feature called is_nan so that model knows when it's actually missing and if your model is complex enough it could learn to ignore the imputed value when the is_nan feature is true.
I agree that the class should have supported Nans, so it might be worth filing a bug request.
",scikit-learn
image not segmenting properly using dbscan,"I am trying to use DBSCAN from scikitlearn to segment an image based on color.  The results I'm getting are . As you can see there are 3 clusters.  My goal is to separate the buoys in the picture into different clusters. But obviously they are showing up as the same cluster. I've tried a wide range of eps values and min_samples but those two things always cluster together. My code is:
img= cv2.imread(""buoy1.jpg) 
labimg = cv2.cvtColor(img, cv2.COLOR_BGR2LAB)

n = 0
while(n<4):
    labimg = cv2.pyrDown(labimg)
    n = n+1

feature_image=np.reshape(labimg, [-1, 3])
rows, cols, chs = labimg.shape

db = DBSCAN(eps=5, min_samples=50, metric = 'euclidean',algorithm ='auto')
db.fit(feature_image)
labels = db.labels_

plt.figure(2)
plt.subplot(2, 1, 1)
plt.imshow(img)
plt.axis('off')
plt.subplot(2, 1, 2)
plt.imshow(np.reshape(labels, [rows, cols]))
plt.axis('off')
plt.show()

I assume this is taking the euclidean distance and since its in lab space euclidean distance would be different between different colors. If anyone can give me guidance on this I'd really appreciate it.
Update: 
The below answer works. Since DBSCAN requires an array with no more then 2 dimensions I concatenated the columns to the original image and reshaped to produce a n x 5 matrix where n is the x dimension times the y dimension.  This seems to work for me.
indices = np.dstack(np.indices(img.shape[:2]))
xycolors = np.concatenate((img, indices), axis=-1) 
np.reshape(xycolors, [-1,5])

","You need to use both color and position.
Right now, you are using colors only.
",scikit-learn
machine learning not predicting correct results,"I am working on creating a simple Python machine learning script which will predict if loan will be approved or not based on below parameters
business experience: should be greater than 7
year of founded: should be after 2015
loan: no previous or current loan

If above conditions matches, then only loan will be approved. This dataset can be downloaded from this link:
https://drive.google.com/file/d/1QtJ3EED7KDqJDrSHxHB6g9kc5YAfTlmF/view?usp=sharing
For above data, I have below script
from sklearn.linear_model import LogisticRegression
import pandas as pd
import numpy as np

data = pd.read_csv(""test2.csv"")
data.head()

X = data[[""Business Exp"", ""Year of Founded"", ""Previous/Current Loan""]]
Y = data[""OUTPUT""]

clf = LogisticRegression()
clf.fit(X, Y)

test_x2 = np.array([[9, 2017, 0]])
Y_pred = clf.predict(test_x2)
print(Y_pred)

I am passing the test data in test_x2. Test data is if business exp is 9, year of founded is 2017 and no current/previous loan, so that means loan will be provided. So it should predict and the result should be 1 but it shows 0. Is there any issue with the code or with the dataset. As I am beginner in machine learning and still learning it so I have created this custom dataset for my own understanding.
","You should use StandardScaler() within a pipeline
from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import make_pipeline
import pandas as pd
import numpy as np

data = pd.read_csv(""test2.csv"")
data.head()

X = data[[""Business Exp"", ""Year of Founded"", ""Previous/Current Loan""]]
Y = data[""OUTPUT""]

clf = make_pipeline(StandardScaler(), LogisticRegression())
clf.fit(X, Y)

test_x2 = np.array([[9, 2017, 0]])
Y_pred = clf.predict(test_x2)

print(""prediction = "", Y_pred.item())
prediction =  1
print(""score = "", clf.score(X, Y))
score =  0.95535

",scikit-learn
issue setting up scikeras model,"I have an existing setup using scikit-learn, but am looking into expanding into deep learning with Keras. I am also using Dask, which recommends using SciKeras.
The way the SciKeras KerasClassifier is currently setup, seems to fit as expected (from the verbose output), but the model seems to have learned nothing at all. I have followed the SciKeras docs here, but I might have overlooked something.

With a Scikit-Learn RF Classifier the kappa score is about 0.44, with
Keras it is about 0.55, and with SciKeras it is 0.0 (clearly an
issue). In the 2. Following SciKeras docs to use Keras where
is the implementation error that prevents a similar result compared to
the one achieved using the 3. Exclusively using Keras below?

Below I have listed the current scikit-learn implementation with RF (as expected output), the output with SciKeras (as actual output), and the output using Keras exclusively (as expected result)
1. Current output using scikit-learn random forest:
def default_classifier():
    return RandomForestClassifier(oob_score=True, n_jobs=-1)

... ### Preprocessing stuff...

X_train, X_test, y_train, y_test = splits

# Define the Pipeline    
## Classification    
model = default_classifier()
model.fit(X_train, y_train)

## Evaluation Metrics
from sklearn.model_selection import cross_val_score
score = cross_val_score(model, X_test, y_test, scoring='accuracy', cv=5, n_jobs=-1, error_score='raise')
print('Mean: %.3f (Std: %.3f)' % (np.mean(score), np.std(score)))

# Verbose with results...
columns, report, true_matrix, pred_matrix = cl.classification_metrics(model, splits, score)

Respective sklearn output:
Test Size:  0.2
Split Shapes:   [(79997, 96), (20000, 96), (79997, 12), (20000, 12)]
Mean: 0.374 (Std: 0.006)
Overall: 0.510  Kappa: 0.441
Weighted F1-Score: 0.539

2. Following SciKeras docs to use Keras:
from tensorflow import keras
from scikeras.wrappers import KerasClassifier
from sklearn.model_selection import train_test_split
import numpy as np

def fcn_model(hidden_layer_dim, meta):
    # note that meta is a special argument that will be
    # handed a dict containing input metadata
    n_features_in_ = meta[""n_features_in_""]
    X_shape_ = meta[""X_shape_""]
    n_classes_ = meta[""n_classes_""]
    
    model = keras.models.Sequential()
    model.add(keras.layers.Dense(n_features_in_, input_shape=X_shape_[1:]))
    model.add(keras.layers.Activation(""relu""))
    model.add(keras.layers.Dense(hidden_layer_dim))
    model.add(keras.layers.Activation(""relu""))
    model.add(keras.layers.Dense(n_classes_))
    model.add(keras.layers.Activation(""softmax""))
    return model

def get_model_fcn(modelargs={}):
    return KerasClassifier(fcn_model, 
                           hidden_layer_dim=128, 
                           epochs=10,
                           optimizer='adam',
                           loss='categorical_crossentropy',
                           metrics=['accuracy'],
                           fit__use_multiprocessing=True,
                           **modelargs)

... ### Preprocessing stuff...

X_train, X_test, y_train, y_test = splits

# Define the Pipeline    
## Classification    
model = get_model_fcn()
model.fit(X_train, y_train)

## Evaluation Metrics
from sklearn.model_selection import cross_val_score
score = cross_val_score(model, X_test, y_test, scoring='accuracy', cv=5, n_jobs=-1, error_score='raise')
print('Mean: %.3f (Std: %.3f)' % (np.mean(score), np.std(score)))

columns, report, true_matrix, pred_matrix = cl.classification_metrics(model, splits, score)

Respective scikeras output (result not very good):
Test Size:  0.2
Split Shapes:   [(79997, 96), (20000, 96), (79997, 12), (20000, 12)]
Epoch 1/10
2500/2500 [==============================] - 4s 1ms/step - loss: 1.6750 - accuracy: 0.3762
Epoch 2/10
2500/2500 [==============================] - 3s 1ms/step - loss: 1.3132 - accuracy: 0.5021
Epoch 3/10
2500/2500 [==============================] - 3s 1ms/step - loss: 1.2295 - accuracy: 0.5371
Epoch 4/10
2500/2500 [==============================] - 3s 1ms/step - loss: 1.1651 - accuracy: 0.5599
Epoch 5/10
2500/2500 [==============================] - 3s 1ms/step - loss: 1.1178 - accuracy: 0.5806
Epoch 6/10
2500/2500 [==============================] - 3s 1ms/step - loss: 1.0889 - accuracy: 0.5935
Epoch 7/10
2500/2500 [==============================] - 3s 1ms/step - loss: 1.0845 - accuracy: 0.5922
Epoch 8/10
2500/2500 [==============================] - 3s 1ms/step - loss: 1.0548 - accuracy: 0.6043
Epoch 9/10
2500/2500 [==============================] - 3s 1ms/step - loss: 1.0415 - accuracy: 0.6117
Epoch 10/10
2500/2500 [==============================] - 3s 1ms/step - loss: 1.0316 - accuracy: 0.6172
Mean: 0.000 (Std: 0.000)
625/625 [==============================] - 0s 700us/step # Here it is running model.predict(X_test)
Overall: 0.130  Kappa: 0.000
Weighted F1-Score: 0.030

3. Exclusively using Keras:
# meta copies what SciKeras passes to the Keras model
meta = {
    #'classes_': array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11]), 
    #'target_type_': 'multilabel-indicator', 
    'y_dtype_': np.dtype('uint8'), 
    'y_ndim_': 2, 
    'X_dtype_': np.dtype('float32'), 
    'X_shape_': (79997, 96), 
    'n_features_in_': 96, 
    #'target_encoder_': ClassifierLabelEncoder(loss='categorical_crossentropy'), 
    'n_classes_': 12, 
    'n_outputs_': 1, 
    'n_outputs_expected_': 1, 
    #'feature_encoder_': FunctionTransformer()
}

def fcn_model(hidden_layer_dim, meta):
    # note that meta is a special argument that will be
    # handed a dict containing input metadata
    n_features_in_ = meta[""n_features_in_""]
    X_shape_ = meta[""X_shape_""]
    n_classes_ = meta[""n_classes_""]
    
    model = keras.models.Sequential()
    model.add(keras.layers.Dense(n_features_in_, input_shape=X_shape_[1:]))
    model.add(keras.layers.Activation(""relu""))
    model.add(keras.layers.Dense(hidden_layer_dim))
    model.add(keras.layers.Activation(""relu""))
    model.add(keras.layers.Dense(n_classes_))
    model.add(keras.layers.Activation(""softmax""))
    return model

def get_model_fcn(modelargs={}):
    model = fcn_model(128, meta)
    model.compile(optimizer='adam', 
              loss='categorical_crossentropy', 
              metrics=['accuracy'])
    
    return model

... ### Preprocessing stuff...

X_train, X_test, y_train, y_test = splits

# Define the Pipeline    
## Classification    
model = get_model_fcn()
model.fit(X_train, y_train, epochs=10)

## Evaluation Metrics
#from sklearn.model_selection import cross_val_score
#score = cross_val_score(model, X_test, y_test, scoring='accuracy', cv=5, n_jobs=-1, #error_score='raise')
#print('Mean: %.3f (Std: %.3f)' % (np.mean(score), np.std(score)))

columns, report, true_matrix, pred_matrix = cl.classification_metrics(model, splits, score)

Expected output from using Keras:
Test Size:  0.2
Split Shapes:   [(79997, 96), (20000, 96), (79997, 12), (20000, 12)]
Epoch 1/10
2500/2500 [==============================] - 3s 1ms/step - loss: 1.6941 - accuracy: 0.3730
Epoch 2/10
2500/2500 [==============================] - 3s 1ms/step - loss: 1.3193 - accuracy: 0.5002
Epoch 3/10
2500/2500 [==============================] - 3s 1ms/step - loss: 1.2206 - accuracy: 0.5399
Epoch 4/10
2500/2500 [==============================] - 3s 1ms/step - loss: 1.1585 - accuracy: 0.5613
Epoch 5/10
2500/2500 [==============================] - 3s 1ms/step - loss: 1.1221 - accuracy: 0.5758
Epoch 6/10
2500/2500 [==============================] - 3s 1ms/step - loss: 1.0923 - accuracy: 0.5928
Epoch 7/10
2500/2500 [==============================] - 3s 1ms/step - loss: 1.0682 - accuracy: 0.5984
Epoch 8/10
2500/2500 [==============================] - 3s 1ms/step - loss: 1.0611 - accuracy: 0.6046
Epoch 9/10
2500/2500 [==============================] - 3s 1ms/step - loss: 1.0445 - accuracy: 0.6138
Epoch 10/10
2500/2500 [==============================] - 3s 1ms/step - loss: 1.0236 - accuracy: 0.6186
Overall: 0.601  Kappa: 0.548
Weighted F1-Score: 0.600

","Apparently it was a bug with how it handled multi-class one-hot encoded targets, issue handled here
",scikit-learn
is it possible to transform a target variable using ravel or to_numpy in a sklearn pipeline,"I am using RStudio and tidymodels in an R markdown document. I would like to incorporate some models from scikit-learn. Getting data from the R code chunks to the Python code chunk works well, but when I train and test a model using the following code:
from sklearn.pipeline import Pipeline
from sklearn.linear_model import LogisticRegression

log_reg_pipe = Pipeline([
  ('Logistic Regression', LogisticRegression())
])

log_reg_pipe.fit(X_train, y_train).score(X_val, y_val)

I get the error
DataConversionWarning: A column-vector y was passed when a 1d array was expected. 
Please change the shape of y to (n_samples, ), for example using ravel().

I can solve it by training the data using y_train['clinical_course'].to_numpy(), but I would ideally like this to be done directly in the pipeline. Is this possible?
Note that the code above is just a simple example to show my problem. In this case X_train has four columns and y_train has one.
As described above I tried to use .to_numpy(), but I would like a solution that does all the transformations within the pipeline.
","I don't think this is possible: sklearn pipelines don't support transforming the target variable. See https://stackoverflow.com/a/62826301/10495893 for some notes about that.
(There is TransformedTargetRegressor, but that's for e.g. log-transforming the target before fitting a regressor. I don't think there's a way to hack it to working with a classifier.)
IMO, since throughout much of sklearn y is taken to be 1D, that should happen outside pipelines. You probably don't need to_numpy, just slicing to a pandas Series should be enough, and could be done sooner in your workflow, e.g. y = df['clinical_course'].
",scikit-learn
difference between scikitlearn and sklearn now deprecated,"On OS X 10.11.6 and python 2.7.10 I need to import from sklearn  manifold. 
I have numpy 1.8 Orc1, scipy .13 Ob1 and scikit-learn 0.17.1 installed.
I used pip to install sklearn(0.0), but when I try to import from sklearn manifold I get the following: 

Traceback (most recent call last):   File """", line 1, in
     File
  ""/Library/Python/2.7/site-packages/sklearn/init.py"", line 57, in
       from .base import clone   File
  ""/Library/Python/2.7/site-packages/sklearn/base.py"", line 11, in
       from .utils.fixes import signature   File
  ""/Library/Python/2.7/site-packages/sklearn/utils/init.py"", line
  10, in      from .murmurhash import murmurhash3_32   File
  ""numpy.pxd"", line 155, in init sklearn.utils.murmurhash
  (sklearn/utils/murmurhash.c:5029) ValueError: numpy.dtype has the
  wrong size, try recompiling.

What is the difference between scikit-learn and sklearn?  Also, 
I cant import scikit-learn because of a syntax error
","You might need to reinstall numpy. It doesn't seem to have been installed correctly.
sklearn is how you type the scikit-learn name in python (only the latter should be installed, the former is now deprecated).
Also, try running the standard tests in scikit-learn and check the output. You will have detailed error information there.

As a side note, do you have nosetests installed? Try: nosetests -v sklearn. You type this in bash, not in the python interpreter.
",scikit-learn
isolation forest sklearn for 1d array or list and how to tune hyper parameters,"Is there a way to implement sklearn isolation forest for a 1D array or list? All the examples I came across are for data of 2 Dimension or more.
I have right now developed a model with three features and the example code snipped is mentioned below:
# dataframe of three columns
df_data = datafr[['col_A', 'col_B', 'col_C']]
w_train = page_data[:700]
w_test = page_data[700:-2]

from sklearn.ensemble import IsolationForest
# fit the model
clf = IsolationForest(max_samples='auto')
clf.fit(w_train)

#testing it using test set
y_pred_test = clf.predict(w_test)

The reference I mainly relied upon: IsolationForest example | scikit-learn
The df_data is a data frame with three columns. I am actually looking to find outlier in 1 Dimension or list data.
The other question is how to tune an isolation forest model? One of the ways is to increase the contamination value to reduce the false positives. But how to use the other parameters like n_estimators, max_samples, max_features, versbose, etc.
","It won't make sense to apply Isolation forest to 1D array or list. This is because in that case it would simply be a one to one mapping from feature to target. 
You can read the official documentation to get a better idea of the different parameters helps

contamination


The amount of contamination of the data set, i.e. the proportion of outliers in the data set. Used when fitting to define the threshold on the decision function.

Try experimenting with different values in range [0,0.5] to see which one gives the best results

max_features 


The number of features to draw from X to train each base estimator.

Try values like 5,6,10, etc. any int of your choice and validate it with the final test data

n_estimators try multiple values like 10,20,50, etc. to see which works best.

You can also use GridSearchCV to automate this process of parameter estimation. 
Just try experimenting with different values using gridSearchCV and see which one gives the best results.
Try this
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import f1_score, make_scorer

my_scoring_func = make_scorer(f1_score)
parameters = {'n_estimators':[10,30,50,80], 'max_features':[0.1, 0.2, 0.3,0.4], 'contamination' : [0.1, 0.2, 0.3]}
iso_for =  IsolationForest(max_samples='auto')
clf = GridSearchCV(iso_for, parameters,  scoring=my_scoring_func)

Then use clf to fit the data. Although note that GridSearchCV requires bot x and y (i.e. train data and labels) for the fit method.
Note :You can read this blog post for further reference if you wish to use GridSearchCv with Isolation forest, else you can manually try with different values and plot graphs to see the results.
",scikit-learn
can i force sklearn to use float32 instead of float64,"I am building a product recommender that will use the description of products to find similar products and recommend them. I am using CountVectorizer over the description to find semantically similar descriptions, rank them and suggest those similar.
The problem comes when calculating the cosine similarity matrix. My initial dataframe has 47,046 rows so Im coming up with RAM issues both on my local pc and in my Colab notebook.
Checking the count matrix that CountVectorizer I see that it outputs it as int64:
<47046x3607 sparse matrix of type '<class 'numpy.int64'>'
    with 699336 stored elements in Compressed Sparse Row format>

There is no issue in casting it to int32 with : count_matrix = count_matrix.astype(np.int32) but still when running the cosinesimilarity  from sklearn it outputs float64 instead of float32 (I confirmed this by testing with a smaller dataset that can be processed fine).
Is there any way to force the use of float32? Or a way to actually solve the high RAM usage with matrices altogether?
","
Is there any way to force the use of float32?

You could cast the input sparse matrix to float32. In my testing, this causes the output array to be float32.
Here's a test program I wrote.
import scipy
import sklearn.metrics
import numpy as np

rvs = scipy.stats.randint(low=0, high=10)

A = scipy.sparse.random(47046, 3607, density=0.0005, data_rvs=rvs.rvs, dtype=np.int64)
print(""starting dtype"", A.dtype)
print(""output dtype"", sklearn.metrics.pairwise.cosine_similarity(A, A).dtype)
A = A.astype(np.float32)
print(""starting dtype"", A.dtype)
print(""output dtype"", sklearn.metrics.pairwise.cosine_similarity(A, A).dtype)

Output:
starting dtype int64
output dtype float64
starting dtype float32
output dtype float32

",scikit-learn
loading a pipeline with a densearray conversion step,"I trained and saved the following model using joblib:
def to_dense(x):
    return np.asarray(x.todense())

to_dense_array = FunctionTransformer(to_dense, accept_sparse=True)

model = make_pipeline(
    TfidfVectorizer(),
    to_dense_array,
    HistGradientBoostingClassifier()
)

est = model.fit(texts, y)
save_path = os.path.join(os.getcwd(), ""VAT_estimator.pkl"")
joblib.dump(est, save_path)


Model works fine, accuracy is good and no message is issued during the saving in joblib.
Now, I try to reload the model from joblib using the following code:
import joblib
# Load the saved model
estimator_file = ""VAT_estimator.pkl""
model = joblib.load(estimator_file)

I then get the following error messge:
AttributeError: Can't get attribute 'to_dense' on <module '__main__'>

I can't avoid the conversion step to a dense array in the pipeline.
I tried to insert the conversion step back into the model after the import, but, at prediction time, I get the message that FunctionTransformer is not callable.
I can't see any way out.
","The issue arises because the FunctionTransformer in your pipeline uses a custom function to_dense defined in the __main__ scope, and when you reload the model with joblib, it doesn't know how to find to_dense since it's not in the same scope.
To solve this, you need to ensure that the function is defined in the same module (file) when you load the model or provide joblib with a way to find the custom function.
there are several options to solve this issues :
One of them is : to define the Function Outside of main and Save/Load the Model
example :
First, we will recreate what you did mainly :

we will define the to_dense function at the top level of your script or in a separate module
Then we are going to save it

import os
import numpy as np
import joblib
from sklearn.preprocessing import FunctionTransformer
from sklearn.pipeline import make_pipeline
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.ensemble import HistGradientBoostingClassifier

# first: Define the `to_dense` function at the top level
def to_dense(x):
    return np.asarray(x.todense())

# Create a transformer using the `to_dense` function
to_dense_array = FunctionTransformer(to_dense, accept_sparse=True)

# second : Define the model pipeline
model = make_pipeline(
    TfidfVectorizer(),
    to_dense_array,
    HistGradientBoostingClassifier()
)

# do your stuff there 
# est = model.fit(texts, y)

# third: Save the model using joblib
save_path = os.path.join(os.getcwd(), ""VAT_estimator.pkl"")
joblib.dump(model, save_path)


Now we are going to load the model :
PS : ensure that to_dense is defined or imported when you load the model :
import joblib
import numpy as np
from sklearn.preprocessing import FunctionTransformer

# define the `to_dense` function again or import it from your module
def to_dense(x):
    return np.asarray(x.todense())

# load the saved model
estimator_file = ""VAT_estimator.pkl""
model = joblib.load(estimator_file)

# Now, you can use `model` to make predictions or further train the model good luck.
# Example: model.predict(new_texts)


By defining to_dense at the top level of your script and ensuring it's present when you load the model, joblib will correctly locate the function, and the pipeline should work without any issues.
This complete workflow should save and load your model without encountering the AttributeError.
",scikit-learn
simulating scikitlearn39s onehotencoder mixed data type error,"My focus is on the caveat mentioned in OneHotEncoder's list parameter doc.

list : categories[i] holds the categories expected in the ith column.
The passed categories should not mix strings and numeric values within
a single feature, and should be sorted in case of numeric values.

I'm trying two different approaches. The first one has a hard-coded data frame.
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import OneHotEncoder
import pandas as pd

X = pd.DataFrame(
    {'city': ['London', 'London', 'Paris', 'NewYork'],
     'country': ['UK', 0.2, 'FR', 'US'],
     'user_rating': [4, 5, 4, 3]}
)
categorical_features = ['city', 'country']
one_hot = OneHotEncoder()
transformer = ColumnTransformer([(""one_hot"", one_hot, categorical_features)], remainder=""passthrough"")
transformed_X = transformer.fit_transform(X)

This code throws a TypeError at transformed_X = transformer.fit_transform(X).
I wanted to try the same, but this time reading data from a CSV file instead of hard-coding it.
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import OneHotEncoder
import pandas as pd

X = pd.read_csv('data.csv', header=0)
categorical_features = ['city', 'country']
one_hot = OneHotEncoder()
transformer = ColumnTransformer([(""one_hot"", one_hot, categorical_features)], remainder=""passthrough"")
transformed_X = transformer.fit_transform(X)

The csv file looks like this.
city,country,user_rating
London,UK,4
London,0.2,5
Paris,FR,4
NewYork,US,3

However, this code does not throw any errors. I can see four different encodings when I print the transformed_X. It seems like scikit-learn treated 0.2 as a string instead of a float.
Can the mixed data type error be simulated when reading CSV files? Or it is not possible, because pandas infer the column type when reading the data, so the entire column gets the type object unlike the time of hard-coding data.
","Option 1 (pd.to_numeric)
After reading the data with pd.read_csv, use pd.to_numeric:
import pandas as pd
from io import StringIO

s = """"""city,country,user_rating
London,UK,4
London,0.2,5
Paris,FR,4
NewYork,US,3
""""""

X = pd.read_csv(StringIO(s), header=0)
X[categorical_features] = X[categorical_features].apply(
    lambda x: pd.to_numeric(x, errors='coerce').fillna(x)
        )

Output:
X['country'].tolist()

['UK', 0.2, 'FR', 'US']

Here, we attempt to convert all values to numeric data types; where it fails, we get NaN values, which we fill again with the original values.

Option 2 (converters parameter)
With pd.read_csv, you can pass a custom function to the converters parameter:
def convert(val):
    try:
        return float(val)
    except ValueError:
        return val

categorical_features = ['city', 'country']
converters = {feature: convert for feature in categorical_features}
X = pd.read_csv(StringIO(s), header=0, converters=converters)

# same result

Here, we define a function convert that attempts to convert each value in a column to float; when it fails, it returns the original value. We use a dictionary comprehension to map the categorical columns to this function:
converters = {feature: convert for feature in categorical_features}
converters

{'city': <function __main__.convert(val)>,
 'country': <function __main__.convert(val)>}

Via converters=converters, the function gets applied to the applicable columns from the source.

Note that option 1 will be much faster on a sizeable dataset, as pd.to_numeric is vectorized, meaning its logic is applied to an entire column at once, while converters will apply a function (like convert) to each value in a column individually.
Reproducing the error:
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import OneHotEncoder

one_hot = OneHotEncoder()
transformer = ColumnTransformer([(""one_hot"", one_hot, categorical_features)], 
                                remainder=""passthrough"")
transformed_X = transformer.fit_transform(X)

Result:
TypeError: Encoders require their input to be uniformly strings or numbers. Got ['float', 'str']

",scikit-learn
runtimewarning numpydtype size changed may indicate binary incompatibility,"I have this error for trying to load a saved SVM model. I have tried uninstalling sklearn, NumPy and SciPy, reinstalling the latest versions all-together again (using pip). I am still getting this error. Why?
In [1]: import sklearn; print sklearn.__version__
0.18.1
In [3]: import numpy; print numpy.__version__
1.11.2
In [5]: import scipy; print scipy.__version__
0.18.1
In [7]: import pandas; print pandas.__version__
0.19.1

In [10]: clf = joblib.load('model/trained_model.pkl')
---------------------------------------------------------------------------
RuntimeWarning                            Traceback (most recent call last)
<ipython-input-10-5e5db1331757> in <module>()
----> 1 clf = joblib.load('sentiment_classification/model/trained_model.pkl')

/usr/local/lib/python2.7/dist-packages/sklearn/externals/joblib/numpy_pickle.pyc in load(filename, mmap_mode)
    573                     return load_compatibility(fobj)
    574
--> 575                 obj = _unpickle(fobj, filename, mmap_mode)
    576
    577     return obj

/usr/local/lib/python2.7/dist-packages/sklearn/externals/joblib/numpy_pickle.pyc in _unpickle(fobj, filename, mmap_mode)
    505     obj = None
    506     try:
--> 507         obj = unpickler.load()
    508         if unpickler.compat_mode:
    509             warnings.warn(""The file '%s' has been generated with a ""

/usr/lib/python2.7/pickle.pyc in load(self)
    862             while 1:
    863                 key = read(1)
--> 864                 dispatch[key](self)
    865         except _Stop, stopinst:
    866             return stopinst.value

/usr/lib/python2.7/pickle.pyc in load_global(self)
   1094         module = self.readline()[:-1]
   1095         name = self.readline()[:-1]
-> 1096         klass = self.find_class(module, name)
   1097         self.append(klass)
   1098     dispatch[GLOBAL] = load_global

/usr/lib/python2.7/pickle.pyc in find_class(self, module, name)
   1128     def find_class(self, module, name):
   1129         # Subclasses may override this
-> 1130         __import__(module)
   1131         mod = sys.modules[module]
   1132         klass = getattr(mod, name)

/usr/local/lib/python2.7/dist-packages/sklearn/svm/__init__.py in <module>()
     11 # License: BSD 3 clause (C) INRIA 2010
     12
---> 13 from .classes import SVC, NuSVC, SVR, NuSVR, OneClassSVM, LinearSVC, \
     14         LinearSVR
     15 from .bounds import l1_min_c

/usr/local/lib/python2.7/dist-packages/sklearn/svm/classes.py in <module>()
      2 import numpy as np
      3
----> 4 from .base import _fit_liblinear, BaseSVC, BaseLibSVM
      5 from ..base import BaseEstimator, RegressorMixin
      6 from ..linear_model.base import LinearClassifierMixin, SparseCoefMixin, \

/usr/local/lib/python2.7/dist-packages/sklearn/svm/base.py in <module>()
      6 from abc import ABCMeta, abstractmethod
      7
----> 8 from . import libsvm, liblinear
      9 from . import libsvm_sparse
     10 from ..base import BaseEstimator, ClassifierMixin

__init__.pxd in init sklearn.svm.libsvm (sklearn/svm/libsvm.c:10207)()

RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 80

UPDATE: OK, by following here, and
pip uninstall -y scipy scikit-learn
pip install --no-binary scipy scikit-learn

The error has now gone, though I still have no idea why it occurred in the first place...
","According to MAINT: silence Cython warnings about changes dtype/ufunc size. - numpy/numpy:

These warnings are visible whenever you import scipy (or another
  package) that was compiled against an older numpy than is installed.

and the checks are inserted by Cython (hence are present in any module compiled with it).
Long story short, these warnings should be benign in the particular case of numpy, and these messages are filtered out since numpy 1.8 (the branch this commit went onto). While scikit-learn 0.18.1 is compiled against numpy 1.6.1.
To filter these warnings yourself, you can do the same as the patch does:
import warnings
warnings.filterwarnings(""ignore"", message=""numpy.dtype size changed"")
warnings.filterwarnings(""ignore"", message=""numpy.ufunc size changed"")

Of course, you can just recompile all affected modules from source against your local numpy with pip install --no-binary :all:¹ instead if you have the balls tools for that.

Longer story: the patch's proponent claims there should be no risk specifically with numpy, and 3rd-party packages are intentionally built against older versions:

[Rebuilding everything against current numpy is] not a feasible
  solution, and certainly shouldn't be necessary. Scipy (as many other
  packages) is compatible with a number of versions of numpy. So when we
  distribute scipy binaries, we build them against the lowest supported
  numpy version (1.5.1 as of now) and they work with 1.6.x, 1.7.x and
  numpy master as well.
The real correct would be for Cython only to issue warnings when the
  size of dtypes/ufuncs has changes in a way that breaks the ABI, and be
  silent otherwise.

As a result, Cython's devs agreed to trust the numpy team with maintaining binary compatibility by hand, so we can probably expect that using versions with breaking ABI changes would yield a specially-crafted exception or some other explicit show-stopper.

¹The previously available --no-use-wheel option has been removed since pip 10.0.0.
",scikit-learn
is there a way to use scipy39s differential evolution or another library to minimize one of a multioutput regressor39s outs and bound the others,"I am using a simple RandomForestRegressor from sklearn to predict 3 outputs, and it works well. The problem is that I now need to optimize one of the outputs to be as low as possible, while ensuring the other two predictions don't escape a couple of bounds in the process. I know I could pass the prediction function to something like differential_evolution to minimize the output I need, but my understanding is that you can only enforce bounds on the inputs and not other outputs.
Is there a way to do this that you know of? Am I perhaps approaching this problem incorrectly?
Minimizing the output I wanted results in the other outputs escaping the boundaries I need to set. I'm hoping there is a more intelligent way to find the global minimum other than testing a huge range of inputs, and then finding the minimum among the ""allowed"" predictions where the other outputs are within the bounds.
","Use something like the following to supply to differential_evolution. Turn the polishing option off.
def my_objective(x):
    y = RandomForestRegressorFun(x)
    if np.logical_or(y[1:] > 2.5, y[1:] < 0).any():
        return np.inf

    return y[0]

",scikit-learn
importing a model with scikit learn on vertex,"I'm trying to import a model from my local but everytime I get the same error from gcp logs. The framework is scikit-learn
AttributeError: Can't get attribute 'preprocess_text' on <module 'model_server' from '/usr/app/model_server.py'> 
The code snippet with this problem is
complaints_clf_pipeline = Pipeline(
    [
        (""preprocess"", text.TfidfVectorizer(preprocessor=utils.preprocess_text, ngram_range=(1, 2))),
        (""clf"", naive_bayes.MultinomialNB(alpha=0.3)),
    ]
)

this
preprocess_text 

comes from the cell above, but I keep receiving this issue with model_server which is not present on my code.
Can someone help?
I tried to refactor the code but got the same error, tried to undo this pipeline structure but then I got another error while trying to consult the model by API.
","GCP is trying to load the model, but it can't find the preprocess_text function because it's not included in the serialized model.
Save the scikit-learn pipeline, functions like preprocess_text are not automatically saved with the model. To ensure that GCP knows where to find this function, you can either:
Define preprocess_text inside the same script where you're loading the model, or
Package utils as part of your deployment (including it in your GCP deployment files) so that the preprocess_text function is available in the same environment.
import pickle
from sklearn.pipeline import Pipeline
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.naive_bayes import MultinomialNB

class CustomTextClassifier:
    def __init__(self):
        self.pipeline = Pipeline(
            [
                (""preprocess"", TfidfVectorizer(preprocessor=self.preprocess_text, ngram_range=(1, 2))),
                (""clf"", MultinomialNB(alpha=0.3)),
            ]
        )

    def preprocess_text(self, text):
        
        return text.lower() 

    def train(self, X, y):
        self.pipeline.fit(X, y)

    def predict(self, X):
        return self.pipeline.predict(X)


model = CustomTextClassifier()
# train model with your data...
with open('model.pkl', 'wb') as f:
    pickle.dump(model, f)

",scikit-learn
all intermediate steps should be transformers and implement fit and transform,"I am implementing a pipeline using important features selection and then using the same features to train my random forest classifier. Following is my code.
m = ExtraTreesClassifier(n_estimators = 10)
m.fit(train_cv_x,train_cv_y)
sel = SelectFromModel(m, prefit=True)
X_new = sel.transform(train_cv_x)
clf = RandomForestClassifier(5000)

model = Pipeline([('m', m),('sel', sel),('X_new', X_new),('clf', clf),])
params = {'clf__max_features': ['auto', 'sqrt', 'log2']}

gs = GridSearchCV(model, params)
gs.fit(train_cv_x,train_cv_y)

So X_new are the new features selected via SelectFromModel and sel.transform. Then I want to train my RF using the new features selected.
I am getting the following error:
All intermediate steps should be transformers and implement fit and transform, 
ExtraTreesClassifier ...

","Like the traceback says: each step in your pipeline needs to have a fit() and transform() method (except the last, which just needs fit().  This is because a pipeline chains together transformations of your data at each step.
sel.transform(train_cv_x) is not an estimator and doesn't meet this criterion.
In fact, it looks like based on what you're trying to do, you can leave this step out.  Internally, ('sel', sel) already does this transformation--that's why it's included in the pipeline.
Secondly, ExtraTreesClassifier (the first step in your pipeline), doesn't have a transform() method, either.  You can verify that here, in the class docstring.  Supervised learning models aren't made for transforming data; they're made for fitting on it and predicting based off that.
What type of classes are able to do transformations?

Ones that scale your data.  See preprocessing and normalization.
Ones that transform your data (in some other way than the above).  Decomposition and other unsupervised learning methods do this.

Without reading between the lines too much about what you're trying to do here, this would work for you:

First split x and y using train_test_split.  The test dataset produced by this is held out for final testing, and the train dataset within GridSearchCV's cross-validation will be further broken out into smaller train and validation sets.
Build a pipeline that satisfies what your traceback is trying to tell you.
Pass that pipeline to GridSearchCV, .fit() that grid search on X_train/y_train, then .score() it on X_test/y_test.

Roughly, that would look like this:
from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.33, random_state=444)

sel = SelectFromModel(ExtraTreesClassifier(n_estimators=10, random_state=444), 
                      threshold='mean')
clf = RandomForestClassifier(n_estimators=5000, random_state=444)

model = Pipeline([('sel', sel), ('clf', clf)])
params = {'clf__max_features': ['auto', 'sqrt', 'log2']}

gs = GridSearchCV(model, params)
gs.fit(X_train, y_train)

# How well do your hyperparameter optimizations generalize
# to unseen test data?
gs.score(X_test, y_test)

Two examples for further reading:

Pipelining: chaining a PCA and a logistic regression
Sample pipeline for text feature extraction and evaluation

",scikit-learn
use sklearn transformers on list of columns and preserve the input columns,"Using sklearn transformers, I'd like to be able to apply transformations to a list of columns and have the transformer create new columns with the transformed values rather than apply them to the existing ones. Is this possible? The transformer also needs to slot into a a Pipeline.
My goal is to compare the original columns and transformed columns. A wrapper class around the transformer could work, but I wonder if there's an easier way? Thank you.
","The easiest way to do this would be to use a function with an argument that accepts a list of the features you would like to transform. From there you have 2 options:

This is the method you requested. Since the function transforms only those features that you requested, you can replace the values of the features you would like to have transformed within the function.

This is what I would recommend. Create a copy of the original dataframe and ""paste"" all the transformed features into it. You can then print the two dataframes in different cells (I'm assuming you're using jupyter notebooks) to compare the differences.


This would be the function to use:
    def transform_data(scaler, df, feats_to_transform):
        features = scaled_df[feats_to_transform]
        transformed_feats = scaler.fit_transform(features.values) # The transformers take only 2d arrays
                      ​
        return transformed_feats 

Method 1:
df  = pd.read_csv('path/to/csv')
scaler = StandardScaler() # from sklearn.preprocessing import StandardScaler

feats_to_transform = ['feat1, feat2, feat3'] 
transformed_feats = transform_data(scaler, df, feats_to_transform)

df[feats_to_transform] = transformed_feats

Method 2:
df  = pd.read_csv('path/to/csv')
scaled_df = df.copy(deep=True) # Using deep copy prevents alteration of original data
scaler = StandardScaler() # from sklearn.preprocessing import StandardScaler

feats_to_transform = ['feat1, feat2, feat3'] 
transformed_feats = transform_data(scaler, df, feats_to_transform)

scaled_df [feats_to_transform] = transformed_feats

# now compare in different cells
df.head()
scaled_df.head()

",scikit-learn
what does fit_transform do in the context of scikit learn pca,"I don't understand what fit_transform does compared to fit in the context of Scikit Learn and PCA.

https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html#

PCA takes some data and attempts to measure a set of eigenvectors, where each vector is orthogonal to all others and aligned in the direction of maximum remaining variance.
Put another way, the first eigenvector found is oriented along the axis of maximal data variance.
What transformation does fit_transform do, and what interpretation does it have in the context of PCA?
In other words, what transformation is being done by the transform step?
","In simple terms:

fit(X): Calculates the eigenvectors of the covariance matrix of X using eigendecomposition. You can retrieve the eigenvectors after you have fit the PCA (pca.fit(X)) via pca.components_.
transform(X): Converts the input data from the input vector space to the PCA vector space. That is - the vector space defined by the eigenvectors obtained from the PCA algorithm. The transformed data are commonly referred to as the principal components (PCs).
fit_transform(X): Combines both steps - first finding the eigenvectors and then projecting the data onto them.

In practice, Scikit-learn’s PCA implementation uses Singular Value Decomposition (SVD) on X, which gives you both the eigenvectors and principal components in one step during fit(). However, if you have new data to project into the principal component space, you’ll need the transform() method to do that projection.
Note on Scikit-learn's terminology: eigenvectors = components_
",scikit-learn
apply scaler on each id on polars dataframe,"I have a dataset with multiple columns and an ID column. Each ID can have different magnitudes and varying sizes across these columns. I want to normalize the columns for each ID separately.
import polars as pl
from sklearn.preprocessing import MinMaxScaler

scaler = MinMaxScaler()

df = pl.DataFrame(
{    ""ID"" : [1,1,2,2,3,3],
    ""Values"" : [1,2,3,4,5,6]}
)

If i do this, its using the scaler of the entire dataframe, and i would like to use scaler() for each ID.
I tried this:
(
    df
    .with_columns(
        Value_scaled = scaler.fit_transform(df.select(pl.col(""Value""))).over(""ID""),
    )
)

But : AttributeError: 'numpy.ndarray' object has no attribute 'over'
And i also tried using a group_by()
(
    df
    .group_by(
        pl.col(""ID"")
    ).agg(
        scaler.fit_transform(pl.col(""Value"")).alias(""Value_scaled"")
    )
)

And i get :
TypeError: float() argument must be a string or a real number, not 'Expr'
","Following the definition outlined in the documentation, the functionality of the MinMaxScaler can be implemented easily using polars' native expression API.
def min_max_scaler(x: str | pl.Expr) -> pl.Expr:
    if isinstance(x, str):
        x = pl.col(x)
    return (x - x.min()) / (x.max() - x.min())

Then, it is compatible with polars' window functions, such as pl.Expr.over, to apply the scaling separately for each ID.
df.with_columns(min_max_scaler(""Values"").over(""ID""))

shape: (6, 2)
┌─────┬────────┐
│ ID  ┆ Values │
│ --- ┆ ---    │
│ i64 ┆ f64    │
╞═════╪════════╡
│ 1   ┆ 0.0    │
│ 1   ┆ 1.0    │
│ 2   ┆ 0.0    │
│ 2   ┆ 1.0    │
│ 3   ┆ 0.0    │
│ 3   ┆ 1.0    │
└─────┴────────┘

",scikit-learn
calculate the number of points in a given radius by x and y coordinates,"I have a python code where, when given a list of points (X and Y coordinate pairs) in a dataframe and a specified circle radius I would like the code to go through each point, treat it as the the center of the circle, and find how many other points are inside the circle selection.
Looking around Ive seen people suggest a similar problem using longitudes and latitudes can be solved with BallTree from SciKit-Learn so I've tried that but I'm not getting the answers I'm expecting. My code is below:
import numpy as np
import pandas as pd
from sklearn.neighbors import BallTree

df = pd.DataFrame({'id':list('abcde'),'X': [10, 1000, 1010, 5000, 5100],'Y': [10, 1000, 1010, 5000, 5100]})

radius = int(input('Enter the selection radius of the circle:'))

coords = df[[""X"",""Y""]]

tree = BallTree(coords, metric='haversine')
answers = tree.query_radius(coords, r=radius, count_only=True)

print(answers)

For example when i do radius = 100 i get an answer of [1 1 1 1 1] which is not correct. Any ideas how to get this to work in the simplest way in python?
","The issue you're encountering is because the BallTree is being used with the haversine metric, which is designed for computing distances on a spherical surface, typically used with longitude and latitude coordinates. However, in your case, you're working with Cartesian coordinates (X, Y), where the Euclidean distance should be used instead.
You should replace this line:
tree = BallTree(coords, metric='haversine')

With:
tree = BallTree(coords, metric='euclidean')

",scikit-learn
saving model hyperparameters as dictionary jsondecoderjsondecodeerror expecting value line 1 column 1 char 0,"I am tuning some sklearn and tensorflow models, I get the hyperparameters as a dictionary, e.g., {'l1': 0.001, 'lr':0.001} and I save them in a larger dictionary, within my tuning function:
def optunize_hyperparameters(X_tr: Union[List[np.ndarray], pd.DataFrame, np.ndarray], y_tr: Union[dict, pd.DataFrame, np.ndarray],
                            Objective: Callable, builder_func: Callable, model_name: str, fit_params: dict, log_path: str, n_trials: int=10, **kwargs) -> dict:


    """"""
    Optimizes a model's hyperparameters with grid search. Loads existing ones if they exist.

    Parameters
    ----------
    X: 
        Training features.
    y: 
        Training targets
    Objective:
        Optuna objective callable class.
    model_name:
        In the format of ""<model name>_qx"" for x in {5,25,50,75,95}.
    hp_grid:
        Parameter grid for grid search
    fit_params:
        Parameters to pass to model.fit()
    log_path:
        Path to hyperparameter log file, e.g., ""tuning_log.txt""

    Returns
    -------
    best_hps:
        A dictionary that can be passed as **kwargs to builder_func.
    """"""
    
    # Check if log exists, create log file if not
    if not os.path.exists(log_path):
        with open(log_path, 'w') as f:
            log = {}
            json.dump(log, f)

    # Load log
    try:
        with open(log_path, 'r') as f:
            log = json.load(f)
            print(""Successfully loaded existing hyperparameters."")
    except OSError as e: 
            print(e)

    # Look for existing hps, optimize otherwise
    try:
        best_hps = log[model_name]
        print(""Existing hyperparameters found, loading..."")
    
    except:

        print(""No existing hyperparameters found, optimizing hyperparameters..."")
        study = optuna.create_study(
            sampler=optuna.samplers.RandomSampler(),
            pruner=optuna.pruners.SuccessiveHalvingPruner(),
            direction='maximize'
        )

        study.optimize(
            Objective(
                X_tr, y_tr,
                builder_func=builder_func,
                fit_params=fit_params,
                **kwargs
            ),
            n_trials=n_trials,
            n_jobs=-1
        )
        
        best_hps = study.best_params
        
        # Add hps to log and save it
        log[model_name] = best_hps

        with open(log_path, 'w') as f:
            json.dump(log,  f)
        
    return best_hps

I am submitting several jobs in parallel (32 or so) on Compute Canada using slurm. I seem to get json.decoder.JSONDecodeError: Expecting value: line 1 column 1 (char 0) randomly, it is not consistent for any particular job or any particular model. I cannot replicate this issue locally so I don't even know how to debug it. Changing the file from .json to .txt seemed to help a little. In the past, I have also gotten the decode error for other lines in the file, so it may not necessarily be an issue with the first character, though the first character issue does seem to be the most common. I have looked at tuning_log.txt and it seems fine:
{""MT1_2012"": {""l1"": 0.01, ""lr"": 0.01}, ""MT3_1997"": {""l1"": 1e-06, ""lr"": 0.01}, ...}

I have looked through other posts on SO about this issue, but they all pertain to fetching web data and the issues do not seem to be the same as mine (which is seemingly random and inconsistent).
","The reason this was happening is because I was running multiple jobs in parallel that were trying to open and write to the same file. I solved the issue by using file locking. I modified my function as so (I need to update my doc string)
def optunize_hyperparameters(X_tr: Union[List[np.ndarray], pd.DataFrame, np.ndarray], y_tr: Union[dict, pd.DataFrame, np.ndarray],
                            Objective: Callable, builder_func: Callable, model_name: str, fit_params: dict, log_path: str, n_trials: int=10, **kwargs) -> dict:


    """"""
    Optimizes a model's hyperparameters with grid search. Loads existing ones if they exist.

    Parameters
    ----------
    X: 
        Training features.
    y: 
        Training targets
    Objective:
        Optuna objective callable class.
    model_name:
        In the format of ""<model name>_qx"" for x in {5,25,50,75,95}.
    hp_grid:
        Parameter grid for grid search
    fit_params:
        Parameters to pass to model.fit()
    log_path:
        Path to hyperparameter log file, e.g., ""tuning_log.txt""

    Returns
    -------
    best_hps:
        A dictionary that can be passed as **kwargs to builder_func.
    """"""
    
    # Check if log exists, create log file if not
    if not os.path.exists(log_path):
        with open(log_path, 'w') as f:
            portalocker.lock(f, portalocker.LOCK_EX)
            log = {}
            json.dump(log, f)
            portalocker.unlock(f)

    # Load log
    try:
        with open(log_path, 'r+') as f:
            portalocker.lock(f, portalocker.LOCK_EX)
            log = json.load(f)
            # Perform read/write operations
            portalocker.unlock(f)
    except json.JSONDecodeError as e:
        raise e
    

    # Look for existing hps, optimize otherwise
    try:
        best_hps = log[model_name]
        print(""Existing hyperparameters found, loading..."")
    
    except:

        print(""No existing hyperparameters found, optimizing hyperparameters..."")
        study = optuna.create_study(
            sampler=optuna.samplers.RandomSampler(),
            pruner=optuna.pruners.SuccessiveHalvingPruner(),
            direction='maximize'
        )

        study.optimize(
            Objective(
                X_tr, y_tr,
                builder_func=builder_func,
                fit_params=fit_params,
                **kwargs
            ),
            n_trials=n_trials,
            n_jobs=-1
        )
        
        best_hps = study.best_params
        
        # Add hps to log and save it
        log[model_name] = best_hps

        with open(log_path, 'r+') as f:
            portalocker.lock(f, portalocker.LOCK_EX)
            log[model_name] = best_hps
            f.seek(0)
            json.dump(log, f)
            f.truncate()
            portalocker.unlock(f)
            
    return best_hps

",scikit-learn
keep training pytorch model on new data,"I'm working on a text classification task and have decided to use a PyTorch model for this purpose. The process mainly involves the following steps:

Load and process the text.
Use a TF-IDF Vectorizer.
Build the neural network and save the TF-IDF Vectorizer and model to predict new data.

However, every day I need to classify new comments and correct any wrong classifications.
Currently, my approach is to add the new comments with the correct classification to the dataset and retrain the entire model. This process is time-consuming, and the new comments can be lost during validation. I would like to create a new dataset with the newly classified texts and continue training over this new data (the new comments are classified manually, so each label is correct).
Using GPT and some online code, i write the desired process, however, im not sure if its working as expected, or im making some silly mistakes that should not happen.
So the mains questions are:

How could i check if the propossed way to solve this problem work as i expect?
What can i do with the vectorizer when it face new tokens, can i just do a .fit_transform() or i would loose the original vectorizer?

Here its the full training process:
import torch
from torch import nn
from torch.utils.data import Dataset, DataLoader, random_split
from sklearn.preprocessing import LabelEncoder
import polars as pl
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
import joblib

set1 = (
    pl
    .read_csv(
        ""set1.txt"",
        separator="";"",
        has_header=False,
        new_columns=[""text"",""label""]
    )
)

# since the dateset its unbalanced, im going to force to have more balance

fear_df = set1.filter(pl.col(""label"") == ""fear"")
joy_df = set1.filter(pl.col(""label"") == ""joy"").sample(n=2500)
sadness_df = set1.filter(pl.col(""label"") == ""sadness"").sample(n=2500)
anger_df = set1.filter(pl.col(""label"") == ""anger"")

train_df = pl.concat([fear_df,joy_df,sadness_df,anger_df])

""""""
The text its already clean, so im going to change the labels to numeric
and then split it on train, test ,val
""""""

label_mapping = {
    ""anger"": 0,
    ""fear"": 1,
    ""joy"": 2,
    ""sadness"": 3
}

train_mapped = (
    train_df
    .with_columns(
        pl.col(""label"").replace_strict(label_mapping, default=""other"").cast(pl.Int16)
    )
   
)

train_set, pre_Test = train_test_split(train_mapped,
                                    test_size=0.4,
                                    random_state=42,
                                    stratify=train_mapped[""label""])

test_set, val_set = train_test_split(pre_Test,
                                    test_size=0.5,
                                    random_state=42,
                                    stratify=pre_Test[""label""]) 

# Vectorize text data using TF-IDF
vectorizer = TfidfVectorizer(max_features=30000, ngram_range=(1, 2))

X_train_tfidf = vectorizer.fit_transform(train_set['text']).toarray()
X_val_tfidf = vectorizer.transform(val_set['text']).toarray()
X_test_tfidf = vectorizer.transform(test_set['text']).toarray()

y_train = train_set['label']
y_val = val_set['label']
y_test = test_set['label']

class TextDataset(Dataset):
    def __init__(self, texts, labels):
        self.texts = texts
        self.labels = labels
    
    def __len__(self):
        return len(self.texts)
    
    def __getitem__(self, idx):
        text = self.texts[idx]
        label = self.labels[idx]
        return text, label
    
train_dataset = TextDataset(X_train_tfidf, y_train)
val_dataset = TextDataset(X_val_tfidf, y_val)
test_dataset = TextDataset(X_test_tfidf, y_test)

batch_size = 32
train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=batch_size)
test_loader = DataLoader(test_dataset, batch_size=batch_size)

class TextClassificationModel(nn.Module):
    def __init__(self, input_dim, num_classes):
        super(TextClassificationModel, self).__init__()
        self.fc1 = nn.Linear(input_dim, 64)
        self.dropout1 = nn.Dropout(0.5)
        self.fc2 = nn.Linear(64, 32)
        self.dropout2 = nn.Dropout(0.5)
        self.fc3 = nn.Linear(32, num_classes)

    def forward(self, x):
        x = torch.relu(self.fc1(x))
        x = self.dropout1(x)
        x = torch.relu(self.fc2(x))
        x = self.dropout2(x)
        x = torch.softmax(self.fc3(x), dim=1)
        return x
    
input_dim = X_train_tfidf.shape[1]
model = TextClassificationModel(input_dim, 4)

# Define loss and optimizer
criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.Adamax(model.parameters())

# Training loop
num_epochs = 17
best_val_acc = 0.0
best_model_path = ""modelbest.pth""

for epoch in range(num_epochs):
    model.train()
    for texts, labels in train_loader:
        texts, labels = texts.float(), labels.long()
        outputs = model(texts)
        loss = criterion(outputs, labels)
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

    # Validation
    model.eval()
    correct, total = 0, 0
    with torch.no_grad():
        for texts, labels in val_loader:
            texts, labels = texts.float(), labels.long()
            outputs = model(texts)
            _, predicted = torch.max(outputs.data, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()
    val_acc = correct / total
    if val_acc > best_val_acc:
        best_val_acc = val_acc
        torch.save(model.state_dict(), best_model_path)

    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}, Val Acc: {val_acc:.4f}')

# Load the best model
model.load_state_dict(torch.load(best_model_path))

# Load the best model
model.load_state_dict(torch.load(best_model_path))

# Test the model
model.eval()
correct, total = 0, 0
with torch.no_grad():
    for texts, labels in test_loader:
        texts, labels = texts.float(), labels.long()
        outputs = model(texts)
        _, predicted = torch.max(outputs.data, 1)
        total += labels.size(0)
        correct += (predicted == labels).sum().item()
test_acc = correct / total
print(f'Test Acc: {test_acc:.3f}')


# Save the TF-IDF vectorizer
vectorizer_path = ""tfidf_vectorizer.pkl""
joblib.dump(vectorizer, vectorizer_path)

# Save the PyTorch model
model_path = ""text_classification_model.pth""
torch.save(model.state_dict(), model_path)


Proposed code:
import torch
import joblib
import polars as pl
from sklearn.model_selection import train_test_split
from torch import nn
from torch.utils.data import Dataset, DataLoader

# Load the saved TF-IDF vectorizer
vectorizer_path = ""tfidf_vectorizer.pkl""
vectorizer = joblib.load(vectorizer_path)

input_dim = len(vectorizer.get_feature_names_out())

class TextClassificationModel(nn.Module):
    def __init__(self, input_dim, num_classes):
        super(TextClassificationModel, self).__init__()
        self.fc1 = nn.Linear(input_dim, 64)
        self.dropout1 = nn.Dropout(0.5)
        self.fc2 = nn.Linear(64, 32)
        self.dropout2 = nn.Dropout(0.5)
        self.fc3 = nn.Linear(32, num_classes)

    def forward(self, x):
        x = torch.relu(self.fc1(x))
        x = self.dropout1(x)
        x = torch.relu(self.fc2(x))
        x = self.dropout2(x)
        x = torch.softmax(self.fc3(x), dim=1)
        return x
    
# Load the saved PyTorch model
model_path = ""text_classification_model.pth""
model = TextClassificationModel(input_dim, 4)
model.load_state_dict(torch.load(model_path))

# Map labels to numeric values
label_mapping = {""anger"": 0, ""fear"": 1, ""joy"": 2, ""sadness"": 3}
sentiments = [""fear"",""joy"",""sadness"",""anger""]

new_data = (
    pl
    .read_csv(
        ""set2.txt"",
        separator="";"",
        has_header=False,
        new_columns=[""text"",""label""]
    )
    .filter(pl.col(""label"").is_in(sentiments))
    .with_columns(
        pl.col(""label"").replace_strict(label_mapping, default=""other"").cast(pl.Int16)
    )
    
)
# Vectorize the new text data using the loaded TF-IDF vectorizer
X_new = vectorizer.transform(new_data['text']).toarray()
y_new = new_data['label']

class TextDataset(Dataset):
    def __init__(self, texts, labels):
        self.texts = texts
        self.labels = labels
    
    def __len__(self):
        return len(self.texts)
    
    def __getitem__(self, idx):
        text = self.texts[idx]
        label = self.labels[idx]
        return text, label

batch_size = 10
   
# Create DataLoader for the new training data
new_train_dataset = TextDataset(X_new, y_new)
new_train_loader = DataLoader(new_train_dataset, batch_size=batch_size, shuffle=True)

# Define loss and optimizer
criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.Adamax(model.parameters())

num_epochs = 5
new_best_model_path = ""modelbest.pth""
for epoch in range(num_epochs):
    model.train()
    for texts, labels in new_train_loader:
        texts, labels = texts.float(), labels.long()
        outputs = model(texts)
        loss = criterion(outputs, labels)
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        torch.save(model.state_dict(), new_best_model_path)
        
print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')

# Save the PyTorch model
new_best_model_path = ""new_moedl.pth""
torch.save(model.state_dict(), new_best_model_path)

The dataset can be found here
","use  pre-trained word embeddings like BertForSequenceClassification.  These embeddings can handle unseen tokens more gracefully since they map words to continuous vectors based on semantic meaning, reducing the impact of unseen words.
Model Training with BERT
import torch
from torch import nn, optim
from torch.utils.data import DataLoader, Dataset
from transformers import BertTokenizer, BertModel, BertForSequenceClassification
from transformers import Trainer, TrainingArguments
from sklearn.model_selection import train_test_split
import polars as pl

# Load and prepare data
set1 = pl.read_csv(""set1.txt"", separator="";"", has_header=False, new_columns=[""text"", ""label""])

# Balance dataset
fear_df = set1.filter(pl.col(""label"") == ""fear"")
joy_df = set1.filter(pl.col(""label"") == ""joy"").sample(n=2500)
sadness_df = set1.filter(pl.col(""label"") == ""sadness"").sample(n=2500)
anger_df = set1.filter(pl.col(""label"") == ""anger"")
train_df = pl.concat([fear_df, joy_df, sadness_df, anger_df])

label_mapping = {""anger"": 0, ""fear"": 1, ""joy"": 2, ""sadness"": 3}
train_df = train_df.with_columns(pl.col(""label"").replace_strict(label_mapping, default=""other"").cast(pl.Int16))

# Split dataset
train_set, test_val_set = train_test_split(train_df, test_size=0.4, random_state=42, stratify=train_df[""label""])
test_set, val_set = train_test_split(test_val_set, test_size=0.5, random_state=42, stratify=test_val_set[""label""])

# Dataset class
class TextDataset(Dataset):
    def __init__(self, texts, labels, tokenizer, max_length=128):
        self.texts = texts
        self.labels = labels
        self.tokenizer = tokenizer
        self.max_length = max_length

    def __len__(self):
        return len(self.texts)

    def __getitem__(self, idx):
        text = self.texts[idx]
        label = self.labels[idx]
        encoding = self.tokenizer.encode_plus(
            text,
            add_special_tokens=True,
            max_length=self.max_length,
            padding='max_length',
            truncation=True,
            return_tensors='pt'
        )
        return {
            'input_ids': encoding['input_ids'].flatten(),
            'attention_mask': encoding['attention_mask'].flatten(),
            'labels': torch.tensor(label, dtype=torch.long)
        }

# Initialize tokenizer and datasets
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
train_dataset = TextDataset(train_set['text'], train_set['label'], tokenizer)
val_dataset = TextDataset(val_set['text'], val_set['label'], tokenizer)
test_dataset = TextDataset(test_set['text'], test_set['label'], tokenizer)

# Initialize BERT model for classification
model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=4)

# Training arguments
training_args = TrainingArguments(
    output_dir='./results',
    num_train_epochs=3,
    per_device_train_batch_size=16,
    per_device_eval_batch_size=16,
    evaluation_strategy='epoch',
    save_strategy='epoch',
    logging_dir='./logs',
    learning_rate=2e-5,
    load_best_model_at_end=True
)

# Define Trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=val_dataset
)

# Train model
trainer.train()

# Evaluate model
results = trainer.evaluate(test_dataset)
print(f""Test Accuracy: {results['eval_accuracy']:.4f}"")

# Save the model and tokenizer
model.save_pretrained(""saved_model"")
tokenizer.save_pretrained(""saved_tokenizer"")

Incremental training with least effort
# Load the saved model and tokenizer
model = BertForSequenceClassification.from_pretrained(""saved_model"")
tokenizer = BertTokenizer.from_pretrained(""saved_tokenizer"")

# Load new data
new_data = (
    pl.read_csv(""set2.txt"", separator="";"", has_header=False, new_columns=[""text"", ""label""])
    .filter(pl.col(""label"").is_in([""fear"", ""joy"", ""sadness"", ""anger""]))
    .with_columns(pl.col(""label"").replace_strict(label_mapping, default=""other"").cast(pl.Int16))
)

# Create new dataset
new_dataset = TextDataset(new_data['text'], new_data['label'], tokenizer)

# Update training arguments for incremental training
new_training_args = TrainingArguments(
    output_dir='./results_incremental',
    num_train_epochs=2,  # Fewer epochs since it's incremental
    per_device_train_batch_size=16,
    evaluation_strategy='epoch',
    logging_dir='./logs_incremental',
    learning_rate=2e-5,
    load_best_model_at_end=True
)

# Define new trainer
new_trainer = Trainer(
    model=model,
    args=new_training_args,
    train_dataset=new_dataset,
    eval_dataset=val_dataset  # Validate on previous validation set
)

# Train on new data
new_trainer.train()

# Evaluate after retraining
new_results = new_trainer.evaluate(test_dataset)
print(f""Test Accuracy After Incremental Training: {new_results['eval_accuracy']:.4f}"")

# Save the updated model
model.save_pretrained(""saved_model_incremental"")

",scikit-learn
installing an old version of scikitlearn,"Problem Statment
I'm trying to run some old python code that requires scikit-learn 18.0 but the current version I have installed is 0.22 and so I'm getting a warning/invalid data when I run the code.

What I've Tried
I tried installing the specific version both in the terminal: python -m pip install scikit-learn==0.18 and in conda and none of that has worked. All I can install is v 0.22. Help? Thanks.

Error In Terminal

ERROR: Failed building wheel for scikit-learn
  Running setup.py clean for scikit-learn
  Failed to build scikit-learn
  Installing collected packages: scikit-learn
  Found existing installation: scikit-learn 0.22.1
  Uninstalling scikit-learn-0.22.1:
  Successfully uninstalled scikit-learn-0.22.1
  Running setup.py install for scikit-learn ... error
  ERROR: Command errored out with exit status 1:

Error through conda environment:

PackagesNotFoundError: The following packages are not available from
  current channels:
  - scikit-learn==0.18   this was after creating and activating the new environment

","Tackling your issues one at a time:

python -m pip install scikit-learn==0.18 fails

This is probably due to the fact that scikit-learn==0.18, if you check on pypi only has whl files for python 3.5 and 2.7 for windows, therefore pip downloads the source distribution and then fails in compiling it, probably because it doesn't work with newer python versions

The following packages are not available from current channels

This happens, because scikit-learn==18.0 simply does not exist in the default conda channels. On my win64 machine, the oldesst version that I can install is 19.0 (You can check by typing conda search scikit-learn in the cmd), so unfortunately no way to install using the default conda channels. There is a channel called free (found through the anaconda website) that has scikit-learn 18.0, so you could install with:
conda install -c free scikit-learn 

To also make sure that the python version is compatible, I would just create a fitting environment:
conda create -n py35 -c free scikit-learn=0.18.0

",scikit-learn
convergencewarning lbfgs failed to converge status1 stop total no of iterations reached limit,"I have a dataset consisting of both numeric and categorical data and I want to predict adverse outcomes for patients based on their medical characteristics. I defined a prediction pipeline for my dataset like so:
X = dataset.drop(columns=['target'])
y = dataset['target']

# define categorical and numeric transformers
numeric_transformer = Pipeline(steps=[
    ('knnImputer', KNNImputer(n_neighbors=2, weights=""uniform"")),
    ('scaler', StandardScaler())])

categorical_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),
    ('onehot', OneHotEncoder(handle_unknown='ignore'))])

#  dispatch object columns to the categorical_transformer and remaining columns to numerical_transformer
preprocessor = ColumnTransformer(transformers=[
    ('num', numeric_transformer, selector(dtype_exclude=""object"")),
    ('cat', categorical_transformer, selector(dtype_include=""object""))
])

# Append classifier to preprocessing pipeline.
# Now we have a full prediction pipeline.
clf = Pipeline(steps=[('preprocessor', preprocessor),
                      ('classifier', LogisticRegression())])

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

clf.fit(X_train, y_train)
print(""model score: %.3f"" % clf.score(X_test, y_test))

However, when running this code, I get the following warning message:
ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.
Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)

    model score: 0.988

Can someone explain to me what this warning means? I am new to machine learning so am a little lost as to what I can do to improve the prediction model. As you can see from the numeric_transformer, I scaled the data through standardisation. I am also confused as to how the model score is quite high and whether this is a good or bad thing.
","The warning means what it mainly says: Suggestions to try to make the solver (the algorithm) converges.

lbfgs stand for: ""Limited-memory Broyden–Fletcher–Goldfarb–Shanno Algorithm"". It is one of the solvers' algorithms provided by Scikit-Learn Library.
The term limited-memory simply means it stores only a few vectors that represent the gradients approximation implicitly.
It has better convergence on relatively small datasets.

But what is algorithm convergence?
In simple words. If the error of solving is ranging within very small range (i.e., it is almost not changing), then that means the algorithm reached the solution (not necessary to be the best solution as it might be stuck at what so-called ""local Optima"").
On the other hand, if the error is varying noticeably (even if the error is relatively small [like in your case the score was good], but rather the differences between the errors per iteration is greater than some tolerance) then we say the algorithm did not converge.
Now, you need to know that Scikit-Learn API sometimes provides the user the option to specify the maximum number of iterations the algorithm should take while it's searching for the solution in an iterative manner:
LogisticRegression(... solver='lbfgs', max_iter=100 ...)

As you can see, the default solver in LogisticRegression is 'lbfgs' and the maximum number of iterations is 100 by default.
Final words, please, however, note that increasing the maximum number of iterations does not necessarily guarantee convergence, but it certainly helps!

Update:
Based on your comment below, some tips to try (out of many) that might help the algorithm to converge are:

Increase the number of iterations: As in this answer;
Try a different optimizer: Look here;
Scale your data: Look here;
Add engineered features: Look here;
Data pre-processing: Look here - use case and here;
Add more data: Look here.

",scikit-learn
module 39sklearn39 has no attribute 39model_selection39,"Why does the code below return the error
module 'sklearn' has no attribute 'model_selection'

import sklearn
xtrain, xtest, ytrain, ytest = sklearn.model_selection.train_test_split(x,y, test_size = 0.2)

but this one works fine:
from sklearn.model_selection import train_test_split
xtrain, xtest, ytrain, ytest = train_test_split(x,y, test_size = 0.2)

note that I did declare x and y as arrays.
","In Python, when you import a module using a statement like
 import sklearn

you're essentially importing only the top-level module, and you do not automatically gain access to all the submodules or functions contained within that module. That's why you often need to import specific submodules like
from sklearn.model_selection import train_test_split

",scikit-learn
figure out which words a naive bayes classificator uses for deciding,"I'm doing text classification with Naive Bayes in Python and want to figure out which words are used for deciding to what class a text belongs.
I have found this answer https://stackoverflow.com/a/62097661/3992979, but it doesn't help me as my vectorizer doesn't have a get_feature_names() method and my Naive Bayes classifier no coef_ attribute.
df_train is a data frame with manually labelled training data df_test is a data frame with unlabelled data NB should classify. There are two classes only, ""terror"" 1 for text about terrorism attacks and ""terror"" 0 for text without that topic.
### Create ""Bag of Words""
vec = CountVectorizer(
    ngram_range=(1, 3)
)

x_train = vec.fit_transform(df_train.clean_text)
x_test = vec.transform(df_test.clean_text)

y_train = df_train.terror
y_test = df_test.terror

### Train and evaluate the model (Naive Bayes classification)
nb = MultinomialNB()
nb.fit(x_train, y_train)

preds = nb.predict(x_test)

","I figured it out with trial-and-error:
### Get the words that trigger the AI detection
features_log_prob = nb.feature_log_prob_
feature_names = vec.get_feature_names_out()

def show_top100(classifier, vectorizer, categories):
  feature_names = vectorizer.get_feature_names_out()
  for i, category in enumerate(categories):
    top100 = np.argsort(classifier.feature_log_prob_[i])[-100:]
    print(""%s: %s"" % (category, "" "".join(feature_names[top100])))

show_top100(nb, vec, nb.classes_)

",scikit-learn
can39t install imports in vscode using 39pip install39 and extra python extensions,"I am trying to make a machine learning algorithm that when you enter a math equation you say if the answer is right or not. But when I run it the terminal says...
File ""******** - **Vs Code\Python\math.py"", line 2, in <module>
    from sklearn.feature_extraction.text import CountVectorizer
ModuleNotFoundError: No module named 'sklearn'

This is not just the sklearn but also sympy. And when I do the pip install sklearn this error pops up
pip : The term 'pip' is not recognized as the name of a cmdlet, function, script file, or operable program. Check the spelling of the name, or if a path was included, verify that the path is correct and try again. At line:1 char:1 + pip install sympy sklearn + ~~~ + CategoryInfo : ObjectNotFound: (pip:String) [], CommandNotFoundException + FullyQualifiedErrorId : CommandNotFoundException
This is my code. The bolded code is where the errors are.
import os
import pickle
from ***sklearn.feature_extraction.text*** import CountVectorizer
from ***sklearn.naive_bayes*** import MultinomialNB
from ***sympy*** import solve, symbols

DATA_FILE = ""equations_data.pkl""

# Function to convert equations into features
def equation_to_features(equations):
    vectorizer = CountVectorizer()
    X = vectorizer.fit_transform(equations)
    return X

# Function to train the machine learning model
def train_model(equations, labels):
    X = equation_to_features(equations)
    clf = MultinomialNB()
    clf.fit(X, labels)
    return clf

# Function to predict solutions for equations
def predict_solution(equations, clf):
    X_test = equation_to_features(equations)
    predictions = clf.predict(X_test)
    return predictions

# Function to solve equations using SymPy
def solve_equations(equations):
    solutions = []
    x = symbols('x')
    for eq in equations:
        try:
            solution = solve(eq, x)
            solutions.append(solution[0] if solution else None)
        except:
            solutions.append(None)
    return solutions

# Function to load data from file
def load_data():
    if os.path.exists(DATA_FILE):
        with open(DATA_FILE, ""rb"") as f:
            return pickle.load(f)
    return [], []

# Function to save data to file
def save_data(equations, labels):
    with open(DATA_FILE, ""wb"") as f:
        pickle.dump((equations, labels), f)

# Main function
def main():
    equations, solutions = load_data()

    while True:
        # Ask for an equation
        equation = input(""Enter an equation (e.g., '2*x + 3 = 7') or type 'exit' to quit: "")
        if equation.lower() == 'exit':
            break
        
        # Add equation to the list
        equations.append(equation)

        # Ask if the solution provided by the model is correct
        answer = input(""Is the solution correct? (y/n): "")
        if answer.lower() == 'y':
            solutions.append(1)
        else:
            solutions.append(0)

        # Save data to file
        save_data(equations, solutions)

    # Train the model
    clf = train_model(equations, solutions)

    while True:
        # Ask for an equation
        equation = input(""Enter an equation (e.g., '2*x + 3 = 7'): "")
        if equation.lower() == 'exit':
            break
        
        # Predict solution
        predicted_solution = predict_solution([equation], clf)[0]

        # Solve the equation using SymPy
        sym_solution = solve_equations([equation])[0]

        if sym_solution is not None:
            print(""SymPy solution:"", sym_solution)

        if predicted_solution is not None:
            print(""Predicted solution:"", predicted_solution)

        if predicted_solution is not None and sym_solution is not None:
            answer = input(""Is the solution correct? (y/n): "")
            if answer.lower() == 'y':
                print(""Great!"")
                # Add equation and correctness to the training data
                equations.append(equation)
                solutions.append(1)
                clf = train_model(equations, solutions)
                # Save data to file
                save_data(equations, solutions)
            else:
                print(""Oops! Let me learn from that."")
                # Add equation and correctness to the training data
                equations.append(equation)
                solutions.append(0)
                clf = train_model(equations, solutions)
                # Save data to file
                save_data(equations, solutions)
        else:
            print(""Couldn't solve the equation."")

if __name__ == ""__main__"":
    main()

I tried installing extra extensions in VsCode but it didn't work. I tried looking at other qustions in stack overflow but none of them answer the question I have. Please help.
","You need to install the external packages like sympy, sklearn, etc as they are not included with a vanilla Python installation. In your terminal, run:
pip install sympy sklearn # include any other packages you need

",scikit-learn
installing old version of scikitlearn modulenotfounderror no module named 39numpy39,"I have an old Python project that uses scikit-learn version 0.22.2.post1. Unfortunately I am unable to update to a newer version of scikit-learn as the training data has long been lost, and I understand that the version of scikit-learn is tied to the model (stored as a .pkl file).
The project uses Python 3.8 and works fine with this version, but I am trying to upgrade it to use Python 3.9.19. I have managed to do this in my local dev environment, but when I try to do so in my Azure Devops pipeline, I get the following error after the command pip install --target=""./.python_packages/lib/site-packages"" -r ./requirements.txt is run:
Building wheels for collected packages: scikit-learn
  Building wheel for scikit-learn (pyproject.toml): started
  Building wheel for scikit-learn (pyproject.toml): finished with status 'error'
  error: subprocess-exited-with-error
  
  × Building wheel for scikit-learn (pyproject.toml) did not run successfully.
  │ exit code: 1
  ╰─> [32 lines of output]
      <string>:12: DeprecationWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html
      Partial import of sklearn during the build process.
      Traceback (most recent call last):
        File ""<string>"", line 195, in check_package_status
        File ""/opt/hostedtoolcache/Python/3.9.19/x64/lib/python3.9/importlib/__init__.py"", line 127, in import_module
          return _bootstrap._gcd_import(name[level:], package, level)
        File ""<frozen importlib._bootstrap>"", line 1030, in _gcd_import
        File ""<frozen importlib._bootstrap>"", line 1007, in _find_and_load
        File ""<frozen importlib._bootstrap>"", line 984, in _find_and_load_unlocked
      ModuleNotFoundError: No module named 'numpy'

The fact that I can do the upgrade locally (same OS, same version of Python, same version of PIP) gives me hope that this is a fixable problem. When I run the same command locally, PIP outputs:
Installing collected packages: azure-functions, numpy, cython, pandas, nltk, flask, xgboost, scikit-learn, spacy
Successfully installed azure-functions-1.20.0 cython-0.29.36 flask-3.0.3 nltk-3.9.1 numpy-1.19.5 pandas-1.4.4 scikit-learn-0.22.2.post1 spacy-3.7.6 xgboost-1.1.1

So the big difference is that the pipeline attempts to build a wheel, while locally it does not. Perhaps I can work around this problem by getting the pipeline to not build a wheel? I have tried using --no-cache-dir or --no-binary=""scikit-learn"" when calling pip but unfortunately it still attempts to build a wheel and therefore still fails. I have also tried doing pip install numpy==1.19.5 immediately before the existing call to pip, in the hope that scikit-learn would then find numpy, but I still get the same error. I've also tried to install numpy and scikit-learn together in a separate call to pip (pip install --no-cache-dir --no-binary=""scikit-learn"" --target=""./.python_packages/lib/site-packages"" numpy==1.19.5 scikit-learn==0.22.2.post1) but again, same error. In case it matters, my requirements.txt file looks like this:
numpy<1.20.0
azure-functions
cython==0.29.36
scikit-learn==0.22.2.post1
pandas>=0.25.1
spacy==3.7.6
(a few other libraries that I don't think are relevant to the problem)

Is there any way to force pip to not build a wheel, or otherwise to fix this error where scikit-learn can't find numpy?
","I think I've found a solution. Similar to the answer https://stackoverflow.com/a/66603865/68846, I was able to work around the error by running python -m pip install --upgrade pip setuptools wheel scipy==1.10.1 cython==0.29.36 numpy==1.19.5 before proceeding with pip install -r requirements.txt as before.
",scikit-learn
onehotencoding with missing categories,"I have a dataset with a category column. In order to use linear regression, I 1-hot encode this column. 
My set has 10 columns, including the category column. After dropping that column and appending the 1-hot encoded matrix, I end up with 14 columns (10 - 1 + 5).
So I train (fit) my LinearRegression model with a matrix of shape (n, 14).
After training it, I want to test it on a subset of the training set, so I take only the 5 first and put them through the same pipeline. But these 5 first only contain 3 of the categories. So after going through the pipeline, I'm only left with a matrix of shape (n, 13) because it's missing 2 categories. 
How can I force the 1-hot encoder to use the 5 categories ?
I'm using LabelBinarizer from sklearn.
","The error is to ""put the test data through the same pipeline"". Basically i was doing:
data_prepared = full_pipeline.fit_transform(train_set)

lin_reg = LinearRegression()
lin_reg.fit(data_prepared, labels)

some_data = train_set.iloc[:5]
some_data_prepared = full_pipeline.fit_transform(some_data)

lin_reg.predict(some_data_prepared)
# => error because mismatching shapes

The problematic line is:
some_data_prepared = full_pipeline.fit_transform(some_data)

By doing fit_transform, I'll fit the LabelBinarizer to a set containing only 3 labels. Instead I should do:
some_data_prepared = full_pipeline.transform(some_data)

This way I'm using the pipeline fitted by the full set (train_set) and transform it in the same way.
Thanks @Vivek Kumar
",scikit-learn
convert pandas dataframe of objects to a dataframe of vectors,"I have a Pandas dataframe (over 1k of rows). There are numbers, objects, strings, and Boolean values in my dataframe. I want to convert each 'cell' of the dataframe to a vector, and work with the resulting vectors. I then plan to compare each row of vectors for similarities.
For example, My data is:
Col 0,Col 1,Col 2,Col 3,Col 4,Col 5,Col 6,Col 7,Col 8,Col 9,Col 10

12,65e1e35b7fe333,harry Joe,1,FALSE,swe,1,142.158.0.2,10.10.0.2,text1,0
13,65e1e35b7fe599,allen,1,FALSE,swe,1,142.158.0.20,10.10.0.20,text2,0
14,65e1e35b7fe165,carter,1,FALSE,swe,1,142.158.0.21,10.10.0.21,text3,0

I want to end up with a dataframe of vectors that looks like:
Col 0,Col 1,Col 2,Col 3,Col 4,Col 5,Col 6,Col 7,Col 8,Col 9,Col 10

Vect1,Vect2,Vect3,Vect4,Vect5,Vect6,Vect7,Vect8,Vect9,Vect10,Vect11

Vect12,Vect 13,Vect 14,Vect4,Vect5,Vect6,Vect7,Vect 15,Vect 16,Vect 17,Vect11

Vect18,Vect 19,Vect 20,Vect4,Vect5,Vect6,Vect7,Vect 21,Vect 22,Vect 23,Vect11

Is there a good way to do this in Python w SciKit maybe?
I have tried:
import pandas as pd 
from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer

df=mydata

vectorizer = TfidfVectorizer()

# Transform the data to feature vectors
X = vectorizer.fit_transform(df)
X = pd.DataFrame(_X.todense(), index=df.index, columns=vectorizer.vocabulary_)
X.head()

# Labels
y = df['label']

What I got was:
TypeError                                 Traceback (most recent call last)
Cell In[11], line 14
     11 vectorizer = TfidfVectorizer()
     13 # Transform the text data to feature vectors
---> 14 X = vectorizer.fit_transform(df)
     16 X = pd.DataFrame(_X.todense(), index=df.index, columns=vectorizer.vocabulary_)
     17 X.head()

File /anaconda/envs/xxx_py38/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:2079, in TfidfVectorizer.fit_transform(self, raw_documents, y)
   2072 self._check_params()
   2073 self._tfidf = TfidfTransformer(
   2074     norm=self.norm,
   2075     use_idf=self.use_idf,
   2076     smooth_idf=self.smooth_idf,
   2077     sublinear_tf=self.sublinear_tf,
   2078 )
-> 2079 X = super().fit_transform(raw_documents)
   2080 self._tfidf.fit(X)
   2081 # X is already a transformed view of raw_documents so
   2082 # we set copy to False

File /anaconda/envs/xxx_py38/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:1338, in CountVectorizer.fit_transform(self, raw_documents, y)
   1330             warnings.warn(
   1331                 ""Upper case characters found in""
   1332                 "" vocabulary while 'lowercase'""
   1333                 "" is True. These entries will not""
   1334                 "" be matched with any documents""
   1335             )
   1336             break
-> 1338 vocabulary, X = self._count_vocab(raw_documents, self.fixed_vocabulary_)
   1340 if self.binary:
   1341     X.data.fill(1)

File /anaconda/envs/xxx_py38/lib/python3.9/site-packages/sklearn/feature_extraction/text.py:1207, in CountVectorizer._count_vocab(self, raw_documents, fixed_vocab)
   1205 values = _make_int_array()
   1206 indptr.append(0)
-> 1207 for doc in raw_documents:
   1208     feature_counter = {}
   1209     for feature in analyze(doc):

TypeError: 'Data' object is not iterable

","Go through the code. I think , This is what you want.
import pandas as pd
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline


df = pd.DataFrame({
    'Col 0': [10, 100, 1000],
    'Col 1': ['65e1e35b7fe333', '65e1e35b7fe599', '65e1e35b7fe165'],
    'Col 2': [True, False, False],
    'Col 3': ['Protijayi', 'Roopa', 'Gina'],

})
print(df)
'''
  Col 0           Col 1  Col 2      Col 3
0     10  65e1e35b7fe333   True  Protijayi
1    100  65e1e35b7fe599  False      Roopa
2   1000  65e1e35b7fe165  False       Gina
'''
# Define transformations for different types of data
preprocessor = ColumnTransformer(
    transformers=[
        ('num', StandardScaler(), ['Col 0']),  # For numerical columns
        ('cat', OneHotEncoder(), ['Col 1']),  # For categorical columns
        ('bool', 'passthrough', ['Col 2']),  # For Boolean columns (already 0 or 1)
        ('text', TfidfVectorizer(), 'Col 3')  # For text data
    ],
    remainder='drop'  # Drop untransformed columns
)

# Apply transformations
X = preprocessor.fit_transform(df)

# Convert the resulting matrix to a DataFrame for easy inspection
X_df = pd.DataFrame(X.toarray() if hasattr(X, 'toarray') else X)

print(X_df)
'''
         0    1    2    3    4    5    6    7
0 -0.805387  0.0  1.0  0.0  1.0  0.0  1.0  0.0
1 -0.604040  0.0  0.0  1.0  0.0  0.0  0.0  1.0
2  1.409428  1.0  0.0  0.0  0.0  1.0  0.0  0.0
'''

Method 2 :
import pandas as pd
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.decomposition import PCA
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.preprocessing import OneHotEncoder

#DataFrame with 20 rows.df.shape = (20, 4)
df = pd.DataFrame({
    'Col_0': [10, 100, 1000, 500, 200, 50, 600, 300, 800, 400, 150, 700, 900, 100,
               350, 250, 50, 450, 750, 850],
    'Col_1': [
        '65e1e35b7fe333', '65e1e35b7fe599', '65e1e35b7fe165', '65e1e35b7fe444',
        '65e1e35b7fe555', '65e1e35b7fe666', '65e1e35b7fe777', '65e1e35b7fe888',
        '65e1e35b7fe999', '65e1e35b7fea11', '65e1e35b7fea22', '65e1e35b7fea33',
        '65e1e35b7fea44', '65e1e35b7fea55', '65e1e35b7fea66', '65e1e35b7fea77',
        '65e1e35b7fea88', '65e1e35b7fea99', '65e1e35b7fea10', '65e1e35b7fea20'
    ],
    'Col_2': [
        True, False, False, True, False, True, False, True,
        False, True, False, True, False, True, False, True,
        True, False, True, False
    ],
    'Col_3': [
        'Protijayi', 'Roopa', 'Gina', 'Anna', 'John', 'Bob', 'Alice', 'Zara',
        'Tom', 'Jerry', 'Donald', 'Mickey', 'HeMan', 'Soudipta', 'Phantom', 'YogiBear',
        'Thundarr', 'GummyBears', 'Tabu', 'Stanley'
    ],
})
print(df.shape)#(20, 4)
print(df)
# Label Encoding for categorical data
label_encoder = LabelEncoder()
df['Col_1'] = label_encoder.fit_transform(df['Col_1'])

# Pipeline for text transformation and dimensionality reduction
text_pipeline = Pipeline([
    ('tfidf', TfidfVectorizer(max_features=50)),  # Adjust max_features if needed
    ('pca', PCA(n_components=16))  # Reduce to 16 components
])

# Preprocessor with ColumnTransformer
preprocessor = ColumnTransformer(
    transformers=[
        ('num', StandardScaler(), ['Col_0']),
        ('cat', 'passthrough', ['Col_1']),  # Already label encoded
        ('bool', 'passthrough', ['Col_2']),
        ('text', text_pipeline, 'Col_3')
    ]
)

# Apply transformations
X = preprocessor.fit_transform(df)

# Create the appropriate number of column names
columns = ['Col_0_scaled', 'Col_1_encoded', 'Col_2'] + [f'Col_3_pca_{i+1}' for i in range(16)]

# Convert the resulting matrix to a DataFrame
X_df = pd.DataFrame(X, columns=columns)

print(X_df.to_string())
'''
   Col_0_scaled  Col_1_encoded  Col_2  Col_3_pca_1  Col_3_pca_2  Col_3_pca_3  Col_3_pca_4  Col_3_pca_5  Col_3_pca_6  Col_3_pca_7  Col_3_pca_8  Col_3_pca_9  Col_3_pca_10  Col_3_pca_11  Col_3_pca_12  Col_3_pca_13  Col_3_pca_14  Col_3_pca_15  Col_3_pca_16
0      -1.334401            1.0    1.0    -0.073105    -0.146901     0.067336     0.125044     0.090760     0.009809     0.305958     0.139551     0.156774      0.527018      0.060816     -0.203797     -0.374745     -0.141668     -0.421094      0.188830
1      -1.045361            4.0    0.0     0.115282    -0.015415    -0.150784    -0.360184    -0.080453     0.019688    -0.031309     0.255278     0.108915     -0.311483     -0.167477      0.008409     -0.300929     -0.220409      0.084341     -0.017631
2       1.845039            0.0    0.0     0.160889    -0.020035     0.330186     0.127126     0.273287    -0.071395    -0.319679     0.219527    -0.279286     -0.004423     -0.239776     -0.416249      0.067402      0.081037     -0.241028     -0.213331
3       0.239261            2.0    1.0     0.554679     0.130482     0.081359     0.118546     0.214038    -0.363213     0.385183    -0.148279    -0.027544     -0.001442     -0.244361      0.424055      0.026407     -0.058623      0.014459      0.048063
4      -0.724206            3.0    0.0     0.307266     0.050949     0.310680    -0.297831    -0.383361     0.129143    -0.158281    -0.122024     0.075167      0.182113      0.224166      0.004674      0.072569      0.380125     -0.010579      0.439877
5      -1.205939            5.0    1.0    -0.113145     0.538810    -0.043185    -0.150704     0.057516     0.046128     0.343291     0.039415    -0.118799     -0.155313      0.381745     -0.250619      0.343292     -0.315427     -0.122863      0.068318
6       0.560416            6.0    0.0    -0.204690     0.181405     0.543654     0.340886    -0.275855    -0.037939    -0.050138    -0.239225    -0.052624     -0.239889      0.117819      0.052465     -0.373745     -0.149760      0.156353     -0.179371
7      -0.403050            7.0    1.0    -0.178138    -0.281203    -0.081624     0.090432     0.090038    -0.427198    -0.084569    -0.062534     0.569291     -0.134243      0.239196     -0.085040      0.016309     -0.078131      0.113432      0.097394
8       1.202728            8.0    0.0     0.071775    -0.180170     0.087681    -0.318030    -0.142572     0.139888    -0.008269     0.168043     0.011826     -0.299417     -0.171258      0.052936     -0.101520     -0.211515     -0.154248      0.049600
9      -0.081895           10.0    1.0    -0.353030     0.077459    -0.044642     0.225100     0.078206     0.115056    -0.117477    -0.203697    -0.136310      0.074003     -0.313551      0.184199      0.171788     -0.074015      0.060184      0.428194
10     -0.884784           12.0    0.0    -0.183930     0.011794     0.095495     0.182962    -0.045277     0.198398     0.415487     0.507330     0.169621     -0.066478     -0.165655     -0.000376      0.133879      0.459423      0.324964     -0.064214
11      0.881572           13.0    1.0    -0.195712     0.218775    -0.177670    -0.200639    -0.208203    -0.025052     0.042678    -0.401426     0.275074      0.075878     -0.359991     -0.039838      0.072103      0.257290     -0.342703     -0.426637
12      1.523883           14.0    0.0     0.128607     0.480509    -0.385878     0.091855     0.056152    -0.134789    -0.352402     0.208674    -0.017009      0.200666      0.185478      0.041582     -0.337446      0.178371      0.190868     -0.087371
13     -1.045361           15.0    1.0     0.166597    -0.089110    -0.109782     0.104984     0.528854     0.529754    -0.056245    -0.326489     0.153271     -0.298661      0.157056     -0.061705     -0.097035      0.178674     -0.027044      0.043495
14     -0.242472           16.0    0.0    -0.085249     0.037130     0.142847    -0.251928     0.195658     0.027961    -0.248846    -0.007671     0.135104      0.343121     -0.248098     -0.064966      0.228748     -0.321116      0.422975      0.018823
15     -0.563628           17.0    1.0    -0.190203    -0.053749    -0.160313     0.149640    -0.063512    -0.206393    -0.264280     0.238996    -0.146314     -0.244733      0.020880      0.278250      0.181423      0.106724     -0.400457      0.237057
16     -1.205939           18.0    1.0    -0.083474    -0.194554     0.005645    -0.045870     0.003321     0.321345    -0.048387     0.106808    -0.108053      0.278396      0.292693      0.555157      0.063186     -0.166453     -0.074729     -0.392409
17      0.078683           19.0    0.0     0.138741    -0.283597     0.057301    -0.004913    -0.023166    -0.249826    -0.017610    -0.051725    -0.066780      0.012460      0.282140     -0.096769      0.381560      0.099963      0.068412     -0.286344
18      1.042150            9.0    1.0     0.283151    -0.208560    -0.415019     0.392379    -0.470813     0.187894     0.039962    -0.102994    -0.140439      0.044455     -0.102141     -0.281541      0.093164     -0.231794      0.139206     -0.001186
19      1.363305           11.0    0.0    -0.266310    -0.254019    -0.153286    -0.318855     0.105382    -0.209260     0.224933    -0.217558    -0.561887      0.017975      0.050319     -0.100827     -0.266410      0.227304      0.219550      0.048843

'''

Undertsanding ColumnTransformer :
ColumnTransformer applies its transformers in parallel.
When using a ColumnTransformer, if you pass the same column to multiple transformers,
that column will appear multiple times in the output, once for each transformation applied to it.
 from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import StandardScaler, OneHotEncoder
import pandas as pd

data = pd.DataFrame({
    'numeric_feature': [1, 2, 3],
    'categorical_feature': ['A', 'B', 'A']
})
print(data)
'''
   numeric_feature categorical_feature
0                1                   A
1                2                   B
2                3                   A
'''
ct = ColumnTransformer(
transformers = [
('num_identity','passthrough',['numeric_feature']),
#passing numeric_feature again
('num',StandardScaler(),['numeric_feature']),
('cat',OneHotEncoder(),['categorical_feature'])] )
transformedData = ct.fit_transform(data)
df_transformed = pd.DataFrame(
transformedData,columns = ['original_numeric','scaled_numeric','catA','catB'])
print(df_transformed)
'''
   original_numeric  scaled_numeric  catA  catB
0               1.0       -1.224745   1.0   0.0
1               2.0        0.000000   0.0   1.0
2               3.0        1.224745   1.0   0.0
'''

method 3 :
import pandas as pd
from sklearn.compose import ColumnTransformer
from sklearn.decomposition import IncrementalPCA
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.feature_extraction.text import TfidfVectorizer
from scipy.sparse import hstack, csr_matrix
import scipy.sparse

df = pd.DataFrame({
    'num_col1': [10, 20, 30, 40, 50],  # Numeric column to be standardized and reduced
    'lookupTable': [1, 2, 3, 4, 5],    # Lookup table (should not be standardized)
    'cat_col': ['R', 'A', 'S', 'R', 'S'],  # Categorical column
    'text_col': ['R world', 'S fun', 'Data science', 'Text data', 'Preprocessing text']  # Text column
})
def selectNumCols(df1):
    aa = df1.select_dtypes(include=['int64']).columns.difference(['lookupTable'])
    return aa 

num_cols = selectNumCols(df)

n_components = min(2,len(num_cols))

if len(num_cols) > 1 :
    num_pipeline = Pipeline([
    ('scaler',StandardScaler()),
    ('IPCA',IncrementalPCA(n_components = min(2,len(num_cols))))
        
    ])
else:
    num_pipeline = Pipeline([('scaler',StandardScaler())])  

# Define the column transformer
preprocessor = ColumnTransformer(
    transformers=[
        ('num', num_pipeline, num_cols),  # Apply the num_pipeline to numeric columns
        ('cat', OneHotEncoder(), ['cat_col']),   # Apply OneHotEncoder to the categorical column
        ('text', TfidfVectorizer(), 'text_col'),  # Apply TfidfVectorizer to the text column
        ('lookup', 'passthrough', ['lookupTable'])  # Pass through the lookup table column
    ]
    #sparse_threshold=0.3  # Keep sparse matrices when the data is more than 30% sparse
)

X = preprocessor.fit_transform(df)

num_cols = [f'num_ipca{i}' for i in range(n_components)] if len(num_cols) > 1 else list(num_cols)

oneHotColNames = preprocessor.named_transformers_['cat'].get_feature_names_out()
print(oneHotColNames)

tfidColNames = preprocessor.named_transformers_['text'].get_feature_names_out()
print(tfidColNames)

cols =  ['lookupTable'] + num_cols + list(oneHotColNames) + list(tfidColNames) 
print(cols)
X_df = pd.DataFrame(X,columns = cols)
print(X_df.to_string())
'''
   lookupTable  num_col1  cat_col_A  cat_col_R  cat_col_S  data       fun  preprocessing   science  text  world
0    -1.414214       0.0        1.0        0.0   0.000000   0.0  0.000000       0.000000  0.000000   1.0    1.0
1    -0.707107       1.0        0.0        0.0   0.000000   1.0  0.000000       0.000000  0.000000   0.0    2.0
2     0.000000       0.0        0.0        1.0   0.627914   0.0  0.000000       0.778283  0.000000   0.0    3.0
3     0.707107       0.0        1.0        0.0   0.707107   0.0  0.000000       0.000000  0.707107   0.0    4.0
4     1.414214       0.0        0.0        1.0   0.000000   0.0  0.778283       0.000000  0.627914   0.0    5.0

'''

method 5 :
import pandas as pd
from sklearn.compose import ColumnTransformer
from sklearn.decomposition import IncrementalPCA
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.feature_extraction.text import TfidfVectorizer
from scipy.sparse import hstack, csr_matrix
from scipy.sparse import csr_matrix
from Only_Testing1.test1 import aa

df = pd.DataFrame({
    'num_col1': [10, 20, 30, 40, 50],  # Numeric column to be standardized and reduced
    'lookupTable': [1, 2, 3, 4, 5],    # Lookup table (should not be standardized)
    'cat_col': ['R', 'A', 'S', 'R', 'S'],  # Categorical column
    'text_col': ['R world', 'S fun', 'Data science', 'Text data', 'Preprocessing text']  # Text column
})
# Custom function to select numeric columns except 'lookupTable'
def select_numeric_columns(X):
    return X.select_dtypes(include=['int64']).columns.difference(['lookupTable'])

# Determine the number of numeric columns after excluding 'lookupTable'
numeric_columns = select_numeric_columns(df)
n_numeric_features = len(numeric_columns)

# Set n_components to the minimum of 2 or the number of numeric features
n_components = min(2, n_numeric_features)

# Conditionally include IncrementalPCA in the pipeline
if n_numeric_features > 1:
    num_pipeline = Pipeline([
        ('scaler', StandardScaler()),  # Standardize numeric data
        ('ipca', IncrementalPCA(n_components=n_components))   # Apply IncrementalPCA to reduce to n_components
    ])
else:
    num_pipeline = Pipeline([
        ('scaler', StandardScaler())  # Standardize numeric data only (no PCA)
    ])

# Define the column transformer
preprocessor = ColumnTransformer(
    transformers=[
        ('num', num_pipeline, numeric_columns),  # Apply the num_pipeline to numeric columns
        ('cat', OneHotEncoder(sparse_output=True), ['cat_col']),   # Apply OneHotEncoder to the categorical column
        ('text', TfidfVectorizer(), 'text_col'),  # Apply TfidfVectorizer to the text column
        ('lookup', 'passthrough', ['lookupTable'])  # Pass through the lookup table column
    ],
    sparse_threshold=0.3  # Keep sparse matrices when the data is more than 30% sparse
)

# Apply transformations
X = preprocessor.fit_transform(df)

# Convert dense matrix to sparse if it's not already sparse
if not isinstance(X, csr_matrix):
    X = csr_matrix(X)


# Automatically generate column names from transformers
num_cols = [f'num_ipca_{i+1}' for i in range(n_components)] if n_numeric_features > 1 else list(numeric_columns)
onehot_col_names = preprocessor.named_transformers_['cat'].get_feature_names_out(['cat_col'])
tfidf_feature_names = preprocessor.named_transformers_['text'].get_feature_names_out()

# Combine all the column names
cols = num_cols + list(onehot_col_names) + list(tfidf_feature_names) + ['lookupTable']

# Convert the resulting sparse matrix to a sparse DataFrame
X_df = pd.DataFrame.sparse.from_spmatrix(X, columns=cols)

print(X_df.to_string())
'''
   num_col1  cat_col_A  cat_col_R  cat_col_S      data  fun  preprocessing   science      text  world  lookupTable
0 -1.414214          0        1.0          0         0    0              0         0         0    1.0          1.0
1 -0.707107        1.0          0          0         0  1.0              0         0         0      0          2.0
2         0          0          0        1.0  0.627914    0              0  0.778283         0      0          3.0
3  0.707107          0        1.0          0  0.707107    0              0         0  0.707107      0          4.0
4  1.414214          0          0        1.0         0    0       0.778283         0  0.627914      0          5.0

'''

",scikit-learn
accessing the values used to impute and normalize new data based upon scikitlearn columntransformer,"Using scikit-learn I'm building machine learning models on a training set, and then evaluating them on a test set. On the train set I perform data imputation and scaling with the ColumnTransformer, then build a logistic regression model using Kfold CV, and the final model is used to predict the values on the test set. The final model is also using its results from ColumnTransformer to impute the missing values on the test set. For example min-max scalar would be taking the min and max values from the train set and would use those values when scaling the test set. How can I see these scaling values that are derived from the the train set and then used to predict on the test set? I can't find anything on the scikit-learn documentation about it. Here is the code I'm using:
from sklearn.linear_model import SGDClassifier
from sklearn.model_selection import RepeatedStratifiedKFold
from sklearn.model_selection import GridSearchCV
from sklearn.compose import ColumnTransformer
from sklearn.impute import SimpleImputer
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import MinMaxScaler, OneHotEncoder

def preprocessClassifierLR(categorical_vars, numeric_vars):###categorical_vars and numeric_vars are lists defining the column names of the categorical and numeric variables present in X


    categorical_pipeline = Pipeline(steps=[('mode', SimpleImputer(missing_values=np.nan, strategy=""most_frequent"")),
                                           (""one_hot_encode"", OneHotEncoder(handle_unknown='ignore'))])

    numeric_pipeline = Pipeline(steps=[('numeric', SimpleImputer(strategy=""median"")),
                                       (""scaling"", MinMaxScaler())])

    col_transform = ColumnTransformer(transformers=[(""cats"", categorical_pipeline, categorical_vars),
                                                    (""nums"", numeric_pipeline, numeric_vars)])

    lr = SGDClassifier(loss='log_loss', penalty='elasticnet')
    model_pipeline = Pipeline(steps=[('preprocess', col_transform),
                                     ('classifier', lr)])


    random_grid_lr = {'classifier__alpha': [1e-1, 0.2, 0.5],
                      'classifier__l1_ratio': [1e-3, 0.5]}

    kfold = RepeatedStratifiedKFold(n_splits=10, n_repeats=10, random_state=47)

    param_search = GridSearchCV(model_pipeline, random_grid_lr, scoring='roc_auc', cv=kfold, refit=True)

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30)

param_search = preprocessClassifierLR(categorical_vars, numeric_vars)
train_mod = param_search.fit(X_train, y_train)
print(""Mod AUC:"", train_mod.best_score_)

test_preds = train_mod.predict_proba(X_)[:,1]

I can't provide the real data, but X is a dataframe with the independent variables and y is the binary outcome variable. train_mod is a pipeline which contains the columntransformer and SGDclassifier steps. I can easily get similar parameter information from the classifier such as the optimal lambda and alpha values by running: train_mod.best_params_, but I cannot figure out the stats used for the column transformer such as 1) the modes used for the simple imputer for the categorical features, 2) the median values used for the simple imputer for the numeric features, and 3) the min and max values used for the scaling of the numeric features. How to access this information?
I assumed that train_mod.best_estimator_['preprocess'].transformers_ would contain this information, in a similar way to how train_mod.best_params_ gives me the alpha and lambda values derived from the model training that are then applied to the test set.
","Pipelines, ColumnTransformers, GridSearch, and others all have attributes (and sometimes a custom __getitem__ to access these like dictionaries) exposing their component parts, and similarly each of the transformers has fitted statistics as attributes, so it's just a matter of chaining these all together, e.g.:
(
    train_mod  # is a grid search, has the next attribute
    .best_estimator_ # is a pipeline, has steps accessible by getitem
    ['preprocess'] # is a columntransformer
    .named_transformers_ # fitted transformers, accessed by getitem
    ['cats']  # pipeline
    ['mode']  # simpleimputer
    .statistics_  # the computed modes, per column seen by this simpleimputer.
)

",scikit-learn
valueerror a given column is not a column of the dataframe,"Everyone
I am trying to create a pipeline using the scikit-learn.
Basically, I have a jupyter-notebook that loading data using pandas, split dataset to train and test the model.
My problem occur in the line: clf.fit(X_train, y_train) you can see the whole code on my github repo jupyter-notebook
log error:
----------------------------------------------------------------------
KeyError                             Traceback (most recent call last)
~/anaconda3/lib/python3.7/site-packages/pandas/core/indexes/base.py in get_loc(self, key, method, tolerance)
   2656             try:
-> 2657                 return self._engine.get_loc(key)
   2658             except KeyError:

pandas/_libs/index.pyx in pandas._libs.index.IndexEngine.get_loc()

pandas/_libs/index.pyx in pandas._libs.index.IndexEngine.get_loc()

pandas/_libs/hashtable_class_helper.pxi in pandas._libs.hashtable.PyObjectHashTable.get_item()

pandas/_libs/hashtable_class_helper.pxi in pandas._libs.hashtable.PyObjectHashTable.get_item()

KeyError: 'survived'

During handling of the above exception, another exception occurred:

KeyError                             Traceback (most recent call last)
~/anaconda3/lib/python3.7/site-packages/sklearn/utils/__init__.py in _get_column_indices(X, key)
    446             for col in columns:
--> 447                 col_idx = all_columns.get_loc(col)
    448                 if not isinstance(col_idx, numbers.Integral):

~/anaconda3/lib/python3.7/site-packages/pandas/core/indexes/base.py in get_loc(self, key, method, tolerance)
   2658             except KeyError:
-> 2659                 return self._engine.get_loc(self._maybe_cast_indexer(key))
   2660         indexer = self.get_indexer([key], method=method, tolerance=tolerance)

pandas/_libs/index.pyx in pandas._libs.index.IndexEngine.get_loc()

pandas/_libs/index.pyx in pandas._libs.index.IndexEngine.get_loc()

pandas/_libs/hashtable_class_helper.pxi in pandas._libs.hashtable.PyObjectHashTable.get_item()

pandas/_libs/hashtable_class_helper.pxi in pandas._libs.hashtable.PyObjectHashTable.get_item()

KeyError: 'survived'

The above exception was the direct cause of the following exception:

ValueError                           Traceback (most recent call last)
<ipython-input-16-17661ab0f723> in <module>
----> 1 clf.fit(X_train, y_train)
      2 print(""model score: %.3f"" % clf.score(X_test, y_test))

~/anaconda3/lib/python3.7/site-packages/sklearn/pipeline.py in fit(self, X, y, **fit_params)
    328         """"""
    329         fit_params_steps = self._check_fit_params(**fit_params)
--> 330         Xt = self._fit(X, y, **fit_params_steps)
    331         with _print_elapsed_time('Pipeline',
    332                                  self._log_message(len(self.steps) - 1)):

~/anaconda3/lib/python3.7/site-packages/sklearn/pipeline.py in _fit(self, X, y, **fit_params_steps)
    294                 message_clsname='Pipeline',
    295                 message=self._log_message(step_idx),
--> 296                 **fit_params_steps[name])
    297             # Replace the transformer of the step with the fitted
    298             # transformer. This is necessary when loading the transformer

~/anaconda3/lib/python3.7/site-packages/joblib/memory.py in __call__(self, *args, **kwargs)
    350 
    351     def __call__(self, *args, **kwargs):
--> 352         return self.func(*args, **kwargs)
    353 
    354     def call_and_shelve(self, *args, **kwargs):

~/anaconda3/lib/python3.7/site-packages/sklearn/pipeline.py in _fit_transform_one(transformer, X, y, weight, message_clsname, message, **fit_params)
    738     with _print_elapsed_time(message_clsname, message):
    739         if hasattr(transformer, 'fit_transform'):
--> 740             res = transformer.fit_transform(X, y, **fit_params)
    741         else:
    742             res = transformer.fit(X, y, **fit_params).transform(X)

~/anaconda3/lib/python3.7/site-packages/sklearn/compose/_column_transformer.py in fit_transform(self, X, y)
    527         self._validate_transformers()
    528         self._validate_column_callables(X)
--> 529         self._validate_remainder(X)
    530 
    531         result = self._fit_transform(X, y, _fit_transform_one)

~/anaconda3/lib/python3.7/site-packages/sklearn/compose/_column_transformer.py in _validate_remainder(self, X)
    325         cols = []
    326         for columns in self._columns:
--> 327             cols.extend(_get_column_indices(X, columns))
    328 
    329         remaining_idx = sorted(set(range(self._n_features)) - set(cols))

~/anaconda3/lib/python3.7/site-packages/sklearn/utils/__init__.py in _get_column_indices(X, key)
    454             raise ValueError(
    455                 ""A given column is not a column of the dataframe""
--> 456             ) from e
    457 
    458         return column_indices

ValueError: A given column is not a column of the dataframe

I checked if the columns exist before of pass the dataframe
to split in train and test.
Someone have some idea of how to solve this issue?
Thanks in advance!
Cheers
","The error comes from the fact that you from the very beginning drop the column survived when defining X. You only checked its presence in y_train.
Simply replace
X= df.drop('survived', axis=1)

by
X= df 

and your
clf.fit(X_train, y_train)
print(""model score: %.3f"" % clf.score(X_test, y_test))

returns
model score: 1.000

",scikit-learn
scikitlearn dbscan memory usage,"UPDATED: In the end, the solution I opted to use for clustering my large dataset was one suggested by Anony-Mousse below. That is, using ELKI's DBSCAN implimentation to do my clustering rather than scikit-learn's. It can be run from the command line and with proper indexing, performs this task within a few hours. Use the GUI and small sample datasets to work out the options you want to use and then go to town. Worth looking into. Anywho, read on for a description of my original problem and some interesting discussion.
I have a dataset with ~2.5 million samples, each with 35 features (floating point values) that I'm trying to cluster. I've been trying to do this with scikit-learn's implementation of DBSCAN, using the Manhattan distance metric and a value of epsilon estimated from some small random samples drawn from the data. So far, so good. (here is the snippet, for reference)
db = DBSCAN(eps=40, min_samples=10, metric='cityblock').fit(mydata)

My issue at the moment is that I easily run out of memory. (I'm currently working on a machine with 16 GB of RAM)
My question is, is DBSCAN calculating the pairwise distance matrix on the fly as it runs, and that's what's gobbling up my memory? (2.5 million ^ 2) * 8 bytes is obviously stupidly large, I would understand that. Should I not be using the fit() method? And more generally, is there a way around this issue, or am I generally barking up the wrong tree here?
Apologies if the answer winds up being obvious. I've been puzzling over this for a few days. Thanks!
Addendum: Also if anyone could explain the difference between fit(X) and fit_predict(X) to me more explicitly I'd also appreciate that--I'm afraid I just don't quite get it.
Addendum #2: To be sure, I just tried this on a machine with ~550 GB of RAM and it still blew up, so I feel like DBSCAN is likely trying to make a pairwise distance matrix or something I clearly don't want it to do. I guess now the big question is how to stop that behavior, or find other methods that might suit my needs more. Thanks for bearing with me here. 
Addendum #3(!): I forgot to attach the traceback, here it is,
Traceback (most recent call last):
  File ""tDBSCAN.py"", line 34, in <module>
    db = DBSCAN(eps=float(sys.argv[2]), min_samples=10, metric='cityblock').fit(mydata)
  File ""/home/jtownsend/.local/lib/python2.6/site-packages/sklearn/base.py"", line 329, in fit_predict
    self.fit(X)
  File ""/home/jtownsend/.local/lib/python2.6/site-packages/sklearn/cluster/dbscan_.py"", line 186, in fit
    **self.get_params())
  File ""/home/jtownsend/.local/lib/python2.6/site-packages/sklearn/cluster/dbscan_.py"", line 69, in dbscan
    D = pairwise_distances(X, metric=metric)
  File ""/home/jtownsend/.local/lib/python2.6/site-packages/sklearn/metrics/pairwise.py"", line 651, in pairwise_distances
    return func(X, Y, **kwds)
  File ""/home/jtownsend/.local/lib/python2.6/site-packages/sklearn/metrics/pairwise.py"", line 237, in manhattan_distances
    D = np.abs(X[:, np.newaxis, :] - Y[np.newaxis, :, :])
MemoryError

","The problem apparently is a non-standard DBSCAN implementation in scikit-learn.
DBSCAN does not need a distance matrix. The algorithm was designed around using a database that can accelerate a regionQuery function, and return the neighbors within the query radius efficiently (a spatial index should support such queries in O(log n)).
The implementation in scikit however, apparently, computes the full O(n^2) distance matrix, which comes at a cost both memory-wise and runtime-wise.
So I see two choices:

You may want to try the DBSCAN implementation in ELKI instead, which when used with an R*-tree index usually is substantially faster than a naive implementation.
Otherwise, you may want to reimplement DBSCAN, as the implementation in scikit apparently isn't too good. Don't be scared of that: DBSCAN is really simple to implement yourself. The trickiest part of a good DBSCAN implementation is actually the regionQuery function. If you can get this query fast, DBSCAN will be fast. And you can actually reuse this function for other algorithms, too.

Update: by now, sklearn no longer computes a distance matrix and can, e.g., use a kd-tree index. However, because of ""vectorization"" it will still precompute the neighbors of every point, so the memory usage of sklearn for large epsilon is O(n²), whereas to my understanding the version in ELKI will only use O(n) memory. So if you run out of memory, choose a smaller epsilon and/or try ELKI.
",scikit-learn
sklearnmetricsaccuracy_score is very slow,"I need to measure accuracy of my model's prediction for binary classification (0 and 1 outputs). I am testing my model with many different values of threshold, and my testing dataset is quite big (50-100 million of examples), so I need a fast way to compute model's accuracy.
I was optimizing my code and noticed that the standard function for computing accuracy is ~50 times slower than the direct computation.
Minimal example:
from sklearn.metrics import accuracy_score
import numpy as np
import timeit
a=np.random.randint(0,2,1000000)
b=np.random.randint(0,2,1000000)
%timeit accuracy_score(a,b)
# 46.7 ms ± 390 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)

%timeit (a==b).sum()/a.size
# 713 µs ± 7.22 µs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)

Am I missing something? It looks like accuracy_score is a standard way to measure accuracy. Why is it so slow? No C optimization under the hood?
","There are two reasons why this happens. Part of the answer is that accuracy_score() is doing more validation to assure that the answer it's computing makes sense. Another part, however, is that some of this validation doesn't seem to have been implemented as efficiently as it could have been.
To explain why I think this, I'm going to show some profiling data. I obtained this data using line profiler, and the following Jupyter command:
from sklearn.metrics import accuracy_score
from sklearn.metrics._classification import _check_targets
from sklearn.utils.multiclass import type_of_target
%load_ext line_profiler
%lprun -f accuracy_score.__wrapped__ -f _check_targets -f type_of_target [accuracy_score(a,b) for i in range(10)]

Some notes about this:

The -f option controls which functions are getting traced. I tracing accuracy_score.__wrapped__ because accuracy_score() has a decorator. The other functions are things that accuracy_score() calls; we'll get into that.
[accuracy_score(a,b) for i in range(10)] is the code we're running.

Here's the result of this for accuracy_score:
Timer unit: 1e-09 s

Total time: 0.813048 s
File: /home/jupyter-njodell/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py
Function: accuracy_score at line 137

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
   146                                           def accuracy_score(y_true, y_pred, *, normalize=True, sample_weight=None):
   ...
   211                                           
   212                                               # Compute accuracy for each possible representation
   213        10  791633889.0    8e+07     97.4      y_type, y_true, y_pred = _check_targets(y_true, y_pred)
   214        10     856305.0  85630.5      0.1      check_consistent_length(y_true, y_pred, sample_weight)
   215        10      11260.0   1126.0      0.0      if y_type.startswith(""multilabel""):
   216                                                   differing_labels = count_nonzero(y_true - y_pred, axis=1)
   217                                                   score = differing_labels == 0
   218                                               else:
   219        10   14279630.0    1e+06      1.8          score = y_true == y_pred
   220                                           
   221        10    6267160.0 626716.0      0.8      return _weighted_sum(score, sample_weight, normalize)

Notice that 97% of its time is spent inside _check_targets(). Let's trace that function:
Timer unit: 1e-09 s

Total time: 0.791422 s
File: /home/jupyter-njodell/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py
Function: _check_targets at line 58

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
    58                                           def _check_targets(y_true, y_pred):
    59                                               """"""Check that y_true and y_pred belong to the same classification task.
    60                                           
    61                                               This converts multiclass or binary types to a common shape, and raises a
    62                                               ValueError for a mix of multilabel and multiclass targets, a mix of
    63                                               multilabel formats, for the presence of continuous-valued or multioutput
    64                                               targets, or for targets of different lengths.
    65                                           
    66                                               Column vectors are squeezed to 1d, while multilabel formats are returned
    67                                               as CSR sparse label indicators.
    ...
    84                                               """"""
    85        10     586514.0  58651.4      0.1      check_consistent_length(y_true, y_pred)
    86        10  192463822.0    2e+07     24.3      type_true = type_of_target(y_true, input_name=""y_true"")
    87        10  172566835.0    2e+07     21.8      type_pred = type_of_target(y_pred, input_name=""y_pred"")
    88                                           
    89        10      11888.0   1188.8      0.0      y_type = {type_true, type_pred}
    90        10      15326.0   1532.6      0.0      if y_type == {""binary"", ""multiclass""}:
    91                                                   y_type = {""multiclass""}
    92                                           
    93        10      11207.0   1120.7      0.0      if len(y_type) > 1:
    94                                                   raise ValueError(
    95                                                       ""Classification metrics can't handle a mix of {0} and {1} targets"".format(
    96                                                           type_true, type_pred
    97                                                       )
    98                                                   )
    99                                           
   100                                               # We can't have more than one value on y_type => The set is no more needed
   101        10      18352.0   1835.2      0.0      y_type = y_type.pop()
   102                                           
   103                                               # No metrics support ""multiclass-multioutput"" format
   104        10       7047.0    704.7      0.0      if y_type not in [""binary"", ""multiclass"", ""multilabel-indicator""]:
   105                                                   raise ValueError(""{0} is not supported"".format(y_type))
   106                                           
   107        10       4171.0    417.1      0.0      if y_type in [""binary"", ""multiclass""]:
   108        10      91096.0   9109.6      0.0          xp, _ = get_namespace(y_true, y_pred)
   109        10     930291.0  93029.1      0.1          y_true = column_or_1d(y_true)
   110        10     498877.0  49887.7      0.1          y_pred = column_or_1d(y_pred)
   111        10       4883.0    488.3      0.0          if y_type == ""binary"":
   112        10       2488.0    248.8      0.0              try:
   113        10  424147403.0    4e+07     53.6                  unique_values = _union1d(y_true, y_pred, xp)
   114                                                       except TypeError as e:
   115                                                           # We expect y_true and y_pred to be of the same data type.
   116                                                           # If `y_true` was provided to the classifier as strings,
   117                                                           # `y_pred` given by the classifier will also be encoded with
   118                                                           # strings. So we raise a meaningful error
   119                                                           raise TypeError(
   120                                                               ""Labels in y_true and y_pred should be of the same type. ""
   121                                                               f""Got y_true={xp.unique(y_true)} and ""
   122                                                               f""y_pred={xp.unique(y_pred)}. Make sure that the ""
   123                                                               ""predictions provided by the classifier coincides with ""
   124                                                               ""the true labels.""
   125                                                           ) from e
   126        10      24858.0   2485.8      0.0              if unique_values.shape[0] > 2:
   127                                                           y_type = ""multiclass""
   128                                           
   129        10      31701.0   3170.1      0.0      if y_type.startswith(""multilabel""):
   130                                                   y_true = csr_matrix(y_true)
   131                                                   y_pred = csr_matrix(y_pred)
   132                                                   y_type = ""multilabel-indicator""
   133                                           
   134        10       4790.0    479.0      0.0      return y_type, y_true, y_pred

There are two major uses of time in this function:

type_of_target() is called for both y_true and y_pred. This is 24.3% + 21.8% = 46.1% of time used.
_union1d() is called to get the number of distinct classes across both y_true and y_pred. This is 53.6% of time used.

What are both of these functions doing?
In type_of_target(), most time is spent on this line:
Total time: 0.364502 s
File: /home/jupyter-njodell/.local/lib/python3.10/site-packages/sklearn/utils/multiclass.py
Function: type_of_target at line 228

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
   228                                           def type_of_target(y, input_name=""""):
   ...
   [... snip many lines of code ...]
   ...
   395        20  361088394.0    2e+07     99.1      if xp.unique_values(y).shape[0] > 2 or (y.ndim == 2 and len(first_row) > 1):
   396                                                   # [1, 2, 3] or [[1., 2., 3]] or [[1, 2]]
   397                                                   return ""multiclass"" + suffix

_union1d is taking the union of two NumPy arrays. It calls np.union1d, which is implemented like so:
def union1d(ar1, ar2):
    return unique(np.concatenate((ar1, ar2), axis=None))

In summary, for binary classification, accuracy_score spends 96% of its compute time computing one of three things:

np.unique(y_true)
np.unique(y_pred)
np.unique(np.concatenate([y_true, y_pred]))

However, the third thing can be computed from the first two.
Specifically, these three operations can be re-written as:
y_true_unique = np.unique(y_true)
y_pred_unique = np.unique(y_pred)
combined_unqiue = np.unique(np.concatenate([y_true_unique, y_pred_unique]))

In the case where the number of distinct classes is much smaller than the number of training examples (which I would think would apply to most machine learning problems) the second form would be about 50% faster, as np.concatenate([y_true_unique, y_pred_unique]) would be much smaller than np.concatenate([y_true, y_pred]).
Actually making this optimization and submitting a scikit-learn PR is left as an exercise to the reader. :)
",scikit-learn
how to displayview sklearnutilsbunch data set,"I am going through a tutorial that uses sklearn.utils.Bunch as a data set:
cal_housing = fetch_california_housing()
I'm running this on a Databricks notebook.
I've read through the documentation that I can find like
https://scikit-learn.org/stable/modules/generated/sklearn.utils.Bunch.html and search engines aren't yielding anything useful.
but how can I see/view what's in this data set?
","If I understood correctly, you can convert it to pandas dataframe:
df = california_housing.fetch_california_housing()
calf_hous_df = pd.DataFrame(data= df.data, columns=df.feature_names)    
calf_hous_df.sample(4)


Moreover, you can see attributes:
df.keys()
dict_keys(['data', 'target', 'feature_names', 'DESCR'])

",scikit-learn
how to give column names after onehot encoding with sklearn,"Here is my question, I hope someone can help me to figure it out..
To explain, there are more than 10 categorical columns in my data set and each of them has 200-300 categories. I want to convert them into binary values. For that I used first label encoder to convert string categories into numbers. The Label Encoder code and the output is shown below.

After Label Encoder, I used One Hot Encoder From scikit-learn again and it is worked. BUT THE PROBLEM IS, I need column names after one hot encoder. For example, column A with categorical values before encoding. A = [1,2,3,4,..]
It should be like that after encoding,

A-1, A-2, A-3

Anyone know how to assign column names to (old column names -value name or number) after one hot encoding. Here is my one hot encoding and it's output;

I need columns with name because I trained an ANN, but every time data comes up I cannot convert all past data again and again. So, I want to add just new ones every time. Thank anyway..
","You can get the column names using  .get_feature_names() attribute.
>>> ohenc.get_feature_names()
>>> x_cat_df.columns = ohenc.get_feature_names()

Detailed example is here.
Update
from Version  1.0, use get_feature_names_out
",scikit-learn
sagemaker failed to extract model data archive targz for container when deploying,"I am trying in Amazon Sagemaker to deploy an existing Scikit-Learn model. So a model that wasn't trained on SageMaker, but locally on my machine.
On my local (windows) machine I've saved my model as model.joblib and tarred the model to model.tar.gz.
Next, I've uploaded this model to my S3 bucket ('my_bucket') in the following path s3://my_bucket/models/model.tar.gz. I can see the tar file in S3.
But when I'm trying to deploy the model, it keeps giving the error message ""Failed to extract model data archive"".
The .tar.gz is generated on my local machine by running 'tar -czf model.tar.gz model.joblib' in a powershell command window.
The code for uploading to S3
import boto3
s3 = boto3.client(""s3"", 
              region_name='eu-central-1', 
              aws_access_key_id=AWS_KEY_ID, 
              aws_secret_access_key=AWS_SECRET)
s3.upload_file(Filename='model.tar.gz', Bucket=my_bucket, Key='models/model.tar.gz')

The code for creating the estimator and deploying:
import boto3
from sagemaker.sklearn.estimator import SKLearnModel

...

model_data = 's3://my_bucket/models/model.tar.gz'
sklearn_model = SKLearnModel(model_data=model_data,
                             role=role,
                             entry_point=""my-script.py"",
                             framework_version=""0.23-1"")
predictor = sklearn_model.deploy(instance_type=""ml.t2.medium"", initial_instance_count=1)                             

The error message:

error message: UnexpectedStatusException: Error hosting endpoint
sagemaker-scikit-learn-2021-01-24-17-24-42-204: Failed. Reason: Failed
to extract model data archive for container ""container_1"" from URL
""s3://my_bucket/models/model.tar.gz"". Please ensure that the object
located at the URL is a valid tar.gz archive

Is there a way to see why the archive is invalid?
","I had a similar issue as well, along with a similar fix to Bas (per comment above).
I was finding I wasn't necessarily having issues with the .tar.gz step, this command does work fine:
tar -czf <filename> ./<directory-with-files>
but rather with the uploading step.
Manually uploading to S3 should take care of this, however, if you're doing this step programmatically, you might need to double check the steps taken. Bas appears to have had filename issues, mine were around using boto properly. Here's some code that works (Python only here, but watch for similar issues with other libraries):
bucket = 'bucket-name'
key = 'directory-inside-bucket'
file = 'the file name of the .tar.gz'

s3_client = boto3.client('s3')
s3_client.upload_file(file, bucket, key)

Docs: https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/s3.html#S3.Client.upload_file
",scikit-learn
what is quotsupportquot in classification_report within sklearn,"I have been wrote a code and the result was a report which you can seen blow. the code is about the number of people who survived or died in titanic.
my question is what is ""Support"" in this report?
              precision    recall  f1-score   support

           0       0.78      0.87      0.82       154
           1       0.79      0.67      0.72       114

    accuracy                           0.78       268
   macro avg       0.79      0.77      0.77       268
weighted avg       0.78      0.78      0.78       268

Now I found an explanation about ""Support"" on the internet which is:
""Support is the number of actual occurrences of the class in the dataset. It doesn’t vary between models, it just diagnoses the performance evaluation process.""
I didn't understood what is ""actual occurrences"" means as well.
I would be grateful if someone could explain these definitions to me with an example.
","support is how many samples are in each class. In your case, 154 samples are in class 0, and 114 samples are in class 1. The total number of samples is 268.
It uses the ground truth labels, which represent the actual class of each sample.
You might be interested in seeing how these values can be manually calculated - see https://stackoverflow.com/a/76789551/21896093
",scikit-learn
set parameters for classifier and use it without fitting,"I'm using python and scikit-learn to do some classification.
Is it possible to reuse the parameters, learned by classifier?
For example:
from sklearn.svm import SVC

cl = SVC(...)    # create svm classifier with some hyperparameters
cl.fit(X_train, y_train)
params = cl.get_params()

Let's store this params somewhere as a dictionary of strings or even write to file a json. Assume, we want later to use this trained classifier to make some predictions on some data. Try to restore it:
params = ...  # retrieve these parameters stored somewhere as a dictionary
data = ...    # the data, we want make predictions on
cl = SVC(...)
cl.set_params(**params)
predictions = cl.predict(data)

If I do it this way, I get the NonFittedError and the following stacktrace:
File ""C:\Users\viacheslav\Python\Python36-32\lib\site-packages\sklearn\svm\base.py"", line 548, in predict
    y = super(BaseSVC, self).predict(X)
  File ""C:\Users\viacheslav\Python\Python36-32\lib\site-packages\sklearn\svm\base.py"", line 308, in predict
    X = self._validate_for_predict(X)
  File ""C:\Users\viacheslav\Python\Python36-32\lib\site-packages\sklearn\svm\base.py"", line 437, in _validate_for_predict
    check_is_fitted(self, 'support_')
  File ""C:\Users\viacheslav\Python\Python36-32\lib\site-packages\sklearn\utils\validation.py"", line 768, in check_is_fitted
    raise NotFittedError(msg % {'name': type(estimator).__name__})
sklearn.exceptions.NotFittedError: This SVC instance is not fitted yet. Call 'fit' with appropriate arguments before using this method.

Is it possible to set parameters to classifier and make predictions without fitting? How do I do that?
","Please read about model persistence in SKLearn:
from sklearn.externals import joblib
joblib.dump(clf, 'filename.pkl') 

and later on:
clf = joblib.load('filename.pkl')

",scikit-learn
what is the rationale behind transformedtargetregressor always cloning the given regressor and how to prevent this behavior,"The docs of sklearn.compose.TransformedTargetRegressor state that:

regressor object, default=None
Regressor object such as derived from RegressorMixin. This regressor will automatically be cloned each time prior to fitting. If regressor is None, LinearRegression is created and used.

What is the rationale behind cloning the given regressor each time prior to fitting? Why would this be useful?
This behavior prevents, for example, the following code from working:
import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestRegressor
from sklearn.pipeline import Pipeline
from sklearn.compose import TransformedTargetRegressor


X = np.random.default_rng(seed=1).normal(size=(100,3))
y = np.random.default_rng(seed=1).normal(size=100)

model = RandomForestRegressor()
pipeline = Pipeline(
    steps=[
        ('normalize', StandardScaler()),
        ('model', model),
    ],
)
tt = TransformedTargetRegressor(regressor=pipeline, transformer=StandardScaler())
tt.fit(X, y)

print(model.feature_importances_)

It results in:
Traceback (most recent call last):
  File ""/tmp/test.py"", line 21, in <module>
    print(model.feature_importances_)
[...]
sklearn.exceptions.NotFittedError: This RandomForestRegressor instance is not fitted yet. Call 'fit' with appropriate arguments before using this estimator.

which is not surprising given that the model object is cloned by the TransformedTargetRegressor.
So, is there a way to prevent this cloning behavior and make the above code work?
","All sklearn meta-estimators (except Pipeline) clone their base estimators; I can't confidently answer why the developers chose that paradigm.
But the fitted base estimators are always made available in new attributes: instead of
model.feature_importances_

use
tt.regressor_['model'].feature_importances_

",scikit-learn
npload fails with valueerror cannot reshape array of size 838715 into shape 838710,"I'm trying to save the scaling parameters of a dataset into a .npy file on the disk, so I avoid having to recalculate them every time I re-run the code.
For now, I'm using MaxAbsScaler() from sklearn and I save the scaler's max_abs_ property to a .npy file as well as the list of remaining files to process into another .npy file, so I can continue running the code from the last saved state.
The files I'm processing are represented by a series of FFT amplitude features which I want to scale in [-1,1] and for that I need to apply partial_fit() because their total size exceeds my RAM.
Moreover, I do not use a .reshape in my code at all.
The problem is: Sometimes (not always) when I run the code that computes a scaler, I would a get an error like the one suggested in the title.
I tried to output the length of the remaining list of files before saving it to the disk to see what was happening, however, there was seemingly no discrepancy, even though I would have expected that to happen.
I then tried to convert my list into a np array, but I kept facing the same error.
Can't really share much of the relevant logic behind, since it's private, but long story short it's something like:
remaining_paths = np.load('remaining_files.npy', allow_pickle=True)
list_of_paths = remaining_paths
for path in list_of_paths:
    data = np.load(path,allow_pickle=True)
    scaler.partial_fit(data)
    remaining_paths.remove(path)
    np.save(scaler.max_abs_,'max_abs.npy',allow_pickle=True)
    remaining_paths.remove(path)
    np.save(remaining_paths ,'remaining_files.npy',allow_pickle=True)

","I had the same issue myself and np.load is sometimes funny when it comes to clarity of errors. In my case, it was because the file got corrupted/the process was interrupted while still writing to the file.
The short answer is:
Try to have a backup file or something similar: one input file and one output file. After the output file was written, overwrite the input file with its contents - maybe do this at every 5 steps or so.
remaining_paths = np.load('remaining_files.npy', allow_pickle=True)
list_of_paths = remaining_paths
ctr = 0
steps_to_save = 5 
for path in list_of_paths:
    data = np.load(path,allow_pickle=True)
    scaler.partial_fit(data)
    remaining_paths.remove(path)
    np.save(scaler.max_abs_,'max_abs.npy',allow_pickle=True)
    remaining_paths.remove(path)
    np.save(remaining_paths ,'remaining_files_out.npy',allow_pickle=True)
    if i % steps_to_save == 0: 
            shutil.copyfile('remaining_files_out.npy','remaining_files_in.npy')
    ctr+=1  

You can refer to shutil documentation or to this answer on the different type of file copy/moving/overwriting options.
The long answer is:
If you dig deeper into the code of np.load, it will try to reshape the input array to the shape specified in the header dictionary of the .npy file.
If you want a hotfix - this may or may not work, depending on where the file writer stopped: use a hex editor like this and change whatever value is in the file with the other value from your error. In my case, it was 839915 to 839910, as in the image:

Again, this only works if you're lucky enough that the rest of the file was correctly written and only the shape is wrong, but I would not advise it - who knows in what other ways can the data get corrupted and still be readable?
Please refer to the npy format documentation for more details on how npy files are built/structured.
https://numpy.org/devdocs/reference/generated/numpy.lib.format.html
",scikit-learn
how to extract the most important features from a ml model using shap  why are all my column names empty,"I want to find the most important features in my model using shap.
I have this code:
from sklearn.model_selection import train_test_split
from sklearn.datasets import load_breast_cancer
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import KFold
import shap
import pandas as pd
import numpy as np


#loading and preparing the data
iris = load_breast_cancer()
X = iris.data
y = iris.target
columns = iris.feature_names
#if you don't shuffle you wont need to keep track of test_index, but I think 
#it is always good practice to shuffle your data
kf = KFold(n_splits=2,shuffle=True)

list_shap_values = list()
list_test_sets = list()
for train_index, test_index in kf.split(X):
    X_train, X_test = X[train_index], X[test_index]
    y_train, y_test = y[train_index], y[test_index]
    X_train = pd.DataFrame(X_train,columns=columns)
    X_test = pd.DataFrame(X_test,columns=columns)

    #training model
    clf = RandomForestClassifier(random_state=0)
    clf.fit(X_train, y_train)

    #explaining model
    explainer = shap.TreeExplainer(clf)
    shap_values = explainer.shap_values(X_test)
    #for each iteration we save the test_set index and the shap_values
    list_shap_values.append(shap_values)
    list_test_sets.append(test_index)


#combining results from all iterations
test_set = list_test_sets[0]
shap_values = np.array(list_shap_values[0])
for i in range(1,len(list_test_sets)):
    test_set = np.concatenate((test_set,list_test_sets[i]),axis=0)
    shap_values = np.concatenate((shap_values,np.array(list_shap_values[i])),axis=1)
#bringing back variable names    
X_test = pd.DataFrame(X[test_set],columns=columns)

#creating explanation plot for the whole experiment, the first dimension from shap_values indicate the class we are predicting (0=0, 1=1)
#shap.summary_plot(shap_values[1], X_test)

shap_sum = np.abs(shap_values).mean(axis=0)
#columns = full_X_train.columns
X_test = pd.DataFrame(X[test_set],columns=columns)
importance_df = pd.DataFrame([X_test.columns.tolist(),shap_sum.tolist()]).T
importance_df.columns = ['column_name','shap_importance']
importance_df = importance_df.sort_values('shap_importance',ascending=False)
print(importance_df)

The output is:
390        None  [0.07973283098297632, 0.012745693741197047, 0....
477        None  [0.07639585953247056, 0.012705549054148915, 0....
542        None  [0.07263038600009886, 0.004509187889530952, 0....
359        None  [0.07006782821092902, 0.008022265024270826, 0....
292        None  [0.06501143916982145, 0.014648801487419996, 0....
..          ...                                                ...
129        None  [0.001207252383050206, 0.005154096692481416, 0...
68         None  [0.000537261423323933, 0.000554437257101772, 0...
229        None  [0.00046312350178067416, 0.0171676941721087, 0...
94         None  [0.00016002701188627102, 0.015384623641506117,...
97         None  [0.0001434577248065334, 0.01162161896706629, 0...

This isn't correct, the column names are all None, and it's not clear to me what the shap values are (I was expecting one number for each column ranked from most important features at the top of what's being printed - not a list).
I was hoping for something more like:
Column      Shap value
Age         0.3 
Gender      0.2

Could someone show me where I went wrong, and how to list the important features for my model using this method?
","If you check:
shap_values.shape


(2, 569, 30)

and
X_test.shape


(569, 30)

you'll find out a surprising coincidence on the last 2 axes, which happened not by chance:

The first axis in shap values is for classes
Second stands for the number of datapoints in the dataset you iterated over
Third one is for the actual shap values.

Then, asking the question ""what, on average, are the most influential features judged by Shapley contributions"" you'll get:
sv = np.abs(shap_values[1,:,:]).mean(0)
cols = X_test.columns
importance_df = pd.DataFrame({
    ""column_name"": cols,
    ""shap_values"": sv
})
#expected result
importance_df.sort_values(""shap_values"", ascending=False)


    column_name shap_values
22  worst perimeter 0.078686
27  worst concave points    0.059845
20  worst radius    0.051551
23  worst area  0.044879
7   mean concave points 0.042114
6   mean concavity  0.033258
2   mean perimeter  0.032346
...

Same as:
shap.plots.bar(shap.Explanation(shap_values[1], feature_names=X_test.columns))


",scikit-learn
how fix quotfound array with dim 4quoterror when using ml algorthims to classify image,"I have a simple ML classification problem. I have 8 folder each one represent class so I have first load these images from folders and assign labels and then save it as csv file (code in below)
def load_images_from_folder(root_folder):
 image_paths = []
 images = []
 labels = []
    for label in os.listdir(root_folder):
        label_path = os.path.join(root_folder, label)
        if os.path.isdir(label_path):
            for filename in os.listdir(label_path):
                img_path = os.path.join(label_path, filename)
                if os.path.isfile(img_path) and (filename.endswith("".jpg""):
                img = Image.open(img_path)
                img = img.resize((128, 128))
                img_array = np.array(img)
                image_paths.append(img_path)
                images.append(img_array)
                labels.append(label)
 return image_paths, images, labels
if __name__ == ""__main__"":
root_folder_path = ""./Datasets_1""
image_paths, images, labels = load_images_from_folder(root_folder_path)

I then convert images and labels to DataFrame and load it
data = {""Images"": image_paths, ""Labels"": labels}
df = pd.DataFrame(data)
df.to_csv(""original_data.csv"", index=False)
csv_file = ""original_data.csv""
df = pd.read_csv(csv_file)

I'm also add a new column 'Encoded_Labels' to the DataFrame with the encoded labels and convert 'Encoded_Labels' column to integers
df['Encoded_Labels'] = encoded_labels
df['Encoded_Labels'] = df['Encoded_Labels'].astype(int)

Finally I have split the dataset into training and testing sets and preprocess images for training
train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)
def load_and_preprocess_images(file_paths, target_size=(128, 128)):
    images = []
    for file_path in file_paths:
        img = Image.open(file_path)
        img = img.resize(target_size)
        img_array = np.array(img) / 255.0  # Normalize pixel values
        images.append(img_array)
    return np.array(images)

X_train = load_and_preprocess_images(train_df['Images'].values)
y_train = train_df['Encoded_Labels'].values
X_test = load_and_preprocess_images(test_df['Images'].values)
y_test = test_df['Encoded_Labels'].values**your text**

And the output shape of X_train is
(20624, 128, 128, 3)

For this point I have no problem and I can use it with DL models with no problem but when try to use ML models such as KNN, SVM, DT, etc. For examples codes in below
from sklearn.svm import SVC
svc = SVC(kernel='linear',gamma='auto')
svc.fit(X_train, y_train)`

or
knn_clf = KNeighborsClassifier()
knn_clf.fit(X_train, y_train)
y_pred = knn_clf.predict(X_test)
accuracy = metrics.accuracy_score(y_test, y_pred)
print(""Accuracy of KNN Classifier : %.2f"" % (accuracy*100))

I got this error
ValueError: Found array with dim 4. SVC expected <= 2.

How to fix this error?
","In the example using sklearn.svm.SVC.fit(), the input is expected to be of shape (n_samples, n_features) (thus being 2-dimensional).
In your case, each sample would be an image. To make your code technically work, you would thus need to flatten your X_train input and make each ""raw"" pixel value a feature,
X_train_flat = X_train.reshape(X_train.shape[0], -1)

which, in your example, would produce a (20624, 49152)-shaped array (as 128·128·3=49152), where each row is a flattened version of the corresponding image.
What is often done instead of using the ""raw"" pixels as an input to SVM and similar classifiers, however, is using a set of ""features"" extracted from the images, to reduce the dimensionality of the data (i.e., in your example, using a (20624, d)-shaped array instead, where d<49152). This could be HOG features, for example, or the result of any other dimensionality reduction technique (it could even be the output of a neural network) – you might want to also have a look at this related question and its answers.
",scikit-learn
fastapi quotdetailquotquotmethod not allowedquot,"I am using FAST API for my ML Model.
I have a pipeline.
lr_tfidf = Pipeline([('vect', tfidf),
                     ('clf', LogisticRegression(penalty='l2'))])



Now In Fast API, when I want to predict, and display result as API, my code is
app = FastAPI()


@app.post('/predict')
def predict_species(data: str):
    data = np.array([data])

    prob = lr_tfidf.predict_proba(data).max()
    pred = lr_tfidf.predict(data)
    return {'Probability': f'{prob}', 
            'Predictions':f'{pred}'}

I copied it from a tutorial. When I test it on GUI by FASTAPI, it works good as shown in Image, i.e it shows probability and predictions.

When I go to request URL, as provided by the GUI, which is http://127.0.0.1:8000/predict?data=hello (test data is hello) It gives me error.
{""detail"":""Method Not Allowed""}

On my Terminal, the error message is
INFO:     127.0.0.1:42568 - ""GET /predict?data=hello HTTP/1.1"" 405 Method Not Allowed

","The method of the endpoint is defined as POST (@app.post('/predict')). When you call the URL from your browser, the HTTP Method is GET.
A simply solution is to change the endpoints method to GET via @app.get.
But this will most likely violates how REST-API endpoints should be named and when to use what HTTP method. A good starting point is https://restfulapi.net/resource-naming/.
Or maybe you are implementing an RPC (remote procedure call)? Than it can be different as well.
",scikit-learn
sklearn kmeans is not working as i only get 39nonetype39 object has no attribute 39split39 on nonempty array,"I don't know what is wrong but suddenly KMeans from sklearn is not working anymore and I don't know what I am doing wrong. Has anyone encountered this problem yet or knows how I can fix it?
from sklearn.cluster import KMeans

kmeanModel = KMeans(n_clusters=k, random_state=0)
kmeanModel.fit(allLocations)

allLocations looks like this:
array([[12.40236   , 51.38086   ],
       [12.40999   , 51.38494   ],
       [12.40599   , 51.37284   ],
       [12.28692   , 51.32039   ],
       [12.41349   , 51.34443   ], ...])

and allLocations.dtype gives dtype('float64').
The scikit-learn version is 1.0.2 and the NumPy version is 1.22.2 and I am using Jupyter Notebook.
The Error says:
'NoneType' object has no attribute 'split'
The whole Error looks like this:
AttributeError                            Traceback (most recent call last)
<ipython-input-30-db8e8220c8b9> in <module>
     12 for k in K:
     13     kmeanModel = KMeans(n_clusters=k, random_state=0)
---> 14     kmeanModel.fit(allLocations)
     15     distortions.append(kmeanModel.inertia_)
     16 #Plotting the distortions
    
~\anaconda3\lib\site-packages\sklearn\cluster\_kmeans.py in fit(self, X, y, sample_weight)
   1169         if self._algorithm == ""full"":
   1170             kmeans_single = _kmeans_single_lloyd
-> 1171             self._check_mkl_vcomp(X, X.shape[0])
   1172         else:
   1173             kmeans_single = _kmeans_single_elkan
    
~\anaconda3\lib\site-packages\sklearn\cluster\_kmeans.py in _check_mkl_vcomp(self, X, n_samples)
   1026         active_threads = int(np.ceil(n_samples / CHUNK_SIZE))
   1027         if active_threads < self._n_threads:
-> 1028             modules = threadpool_info()
   1029             has_vcomp = ""vcomp"" in [module[""prefix""] for module in modules]
   1030             has_mkl = (""mkl"", ""intel"") in [
    
~\anaconda3\lib\site-packages\sklearn\utils\fixes.py in threadpool_info()
    323         return controller.info()
    324     else:
--> 325         return threadpoolctl.threadpool_info()
    326 
    327 
    
~\anaconda3\lib\site-packages\threadpoolctl.py in threadpool_info()
    122     In addition, each module may contain internal_api specific entries.
    123     """"""
--> 124     return _ThreadpoolInfo(user_api=_ALL_USER_APIS).todicts()
    125 
    126 
    
~\anaconda3\lib\site-packages\threadpoolctl.py in __init__(self, user_api, prefixes, modules)
    338 
    339             self.modules = []
--> 340             self._load_modules()
    341             self._warn_if_incompatible_openmp()
    342         else:
    
~\anaconda3\lib\site-packages\threadpoolctl.py in _load_modules(self)
    371             self._find_modules_with_dyld()
    372         elif sys.platform == ""win32"":
--> 373             self._find_modules_with_enum_process_module_ex()
    374         else:
    375             self._find_modules_with_dl_iterate_phdr()
    
~\anaconda3\lib\site-packages\threadpoolctl.py in _find_modules_with_enum_process_module_ex(self)
    483 
    484                 # Store the module if it is supported and selected
--> 485                 self._make_module_from_path(filepath)
    486         finally:
    487             kernel_32.CloseHandle(h_process)
    
~\anaconda3\lib\site-packages\threadpoolctl.py in _make_module_from_path(self, filepath)
    513             if prefix in self.prefixes or user_api in self.user_api:
    514                 module_class = globals()[module_class]
--> 515                 module = module_class(filepath, prefix, user_api, internal_api)
    516                 self.modules.append(module)
    517 
    
~\anaconda3\lib\site-packages\threadpoolctl.py in __init__(self, filepath, prefix, user_api, internal_api)
    604         self.internal_api = internal_api
    605         self._dynlib = ctypes.CDLL(filepath, mode=_RTLD_NOLOAD)
--> 606         self.version = self.get_version()
    607         self.num_threads = self.get_num_threads()
    608         self._get_extra_info()
    
~\anaconda3\lib\site-packages\threadpoolctl.py in get_version(self)
    644                              lambda: None)
    645         get_config.restype = ctypes.c_char_p
--> 646         config = get_config().split()
    647         if config[0] == b""OpenBLAS"":
    648             return config[1].decode(""utf-8"")
    
AttributeError: 'NoneType' object has no attribute 'split'

","Downgrading numpy to 1.21.4 made it work again
",scikit-learn
39exact39 object has no attribute 39shap_values39,"import shap
import pandas as pd
from sklearn.datasets import fetch_california_housing
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor

dataset = fetch_california_housing(as_frame=True)
X = dataset[""data""]
y = dataset[""target""]
X_train, X_test, y_train, y_test = train_test_split(X, y,  test_size=0.2)

model = RandomForestRegressor()

model.fit(X_train, y_train)

explainer = shap.Explainer(model.predict, X_test)
shap_values = explainer(X_test)
shap_values = explainer.shap_values(X)

AttributeError: 'Exact' object has no attribute 'shap_values'

In order to know the SHAP values, even though I entered the above code, the above error occurs at the last line.
When I look at the example codes, it seems to work without any problem, but the SHAP library version is updated and the problem seems to have occurred.
What code can I use instead of .shap_values?
","If you check out explainer object:
dir(explainer)


['__call__',
 '__class__',
 '__delattr__',
 '__dict__',
 '__dir__',
 '__doc__',
 '__eq__',
 '__format__',
 '__ge__',
 '__getattribute__',
 '__gt__',
 '__hash__',
 '__init__',
 '__init_subclass__',
 '__le__',
 '__lt__',
 '__module__',
 '__ne__',
 '__new__',
 '__reduce__',
 '__reduce_ex__',
 '__repr__',
 '__setattr__',
 '__sizeof__',
 '__str__',
 '__subclasshook__',
 '__weakref__',
 '_cached_gray_codes',
 '_compute_main_effects',
 '_gray_code_cache',
 '_instantiated_load',
 'explain_row',
 'feature_names',
 'linearize_link',
 'link',
 'load',
 'masker',
 'model',
 'output_names',
 'save',
 'supports_model_with_masker']

you'll find out the object doesn't have shap_values method or attribute. Hence your error.
However, you already have shap_values object with everything you need:
shap_values


.values =
array([[-3.35735356e-01, -7.13048075e-02,  1.72043493e-02, ...,
         4.34819130e-01, -7.32259144e-01,  1.33657838e-01],
       [ 6.52537977e-01,  1.13007470e-01,  9.42159765e-03, ...,
         2.27540128e-01,  2.63606641e-01,  3.19787969e-02],
       [-5.21078654e-01,  1.27421958e-02, -6.97000846e-02, ...,
         6.47177548e-05,  7.54791715e-02,  1.12118219e-01],
       ...,
       [ 7.92578890e-02, -5.45558945e-03,  1.97615869e-02, ...,
        -2.96984576e-01,  1.09652257e-01, -6.13352645e-01],
       [-9.01863457e-02, -4.94473246e-02,  1.27294579e-01, ...,
         2.32800011e-02, -7.88081199e-01,  1.23530401e-01],
       [ 2.62943083e-01, -2.16743690e-02,  4.10964201e-02, ...,
        -3.23427676e-01,  3.72933299e-01, -6.50382792e-02]])

.base_values =
array([2.0767511, 2.0767511, 2.0767511, ..., 2.0767511, 2.0767511,
       2.0767511])

.data =
array([[   3.089     ,   18.        ,    3.99712644, ...,    1.61091954,
          38.58      , -121.41      ],
       [   5.1718    ,   31.        ,    5.67641682, ...,    2.48446069,
          34.16      , -118.23      ],
       [   2.6893    ,   26.        ,    5.35526316, ...,    2.59962406,
          34.45      , -119.29      ],
       ...,
       [   4.3676    ,   28.        ,    6.39285714, ...,    3.1047619 ,
          34.18      , -117.3       ],
       [   3.9722    ,   17.        ,   20.90078329, ...,    2.60835509,
          38.93      , -120.        ],
       [   5.0091    ,   33.        ,    5.88377193, ...,    3.33333333,
          33.77      , -118.02      ]])

",scikit-learn
shap partial dependence plot misalignment with traintest split in linear regression,"I'm experiencing an issue with SHAP's partial dependence plot when using a train-test split for a linear regression model in Python. When I calculate SHAP values and plot the partial dependence for the first observation in my test set, the alignment of the data point and the baseline seems off.
Here's a simplified version of my code:
import shap
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
import pandas as pd
import matplotlib.pyplot as plt
import requests

def load_data() -> pd.DataFrame:
    """"""
    Loads and returns the dataset from the given URL as a Pandas DataFrame.

    Returns:
        pd.DataFrame: The loaded dataset.
    """"""
    url = ""https://archive.ics.uci.edu/static/public/165/concrete+compressive+strength.zip""

    r = requests.get(url)

    if r.ok:
        with zipfile.ZipFile(BytesIO(r.content)) as thezip:
            with thezip.open(""Concrete_Data.xls"") as thefile:
                return pd.read_excel(thefile, header=0)
    else:
        raise Exception(""Something went wrong."")

df = load_data()

df = df.rename(
    columns={
        'Cement (component 1)(kg in a m^3 mixture)':'cement',
        'Blast Furnace Slag (component 2)(kg in a m^3 mixture)':'blast',
        'Fly Ash (component 3)(kg in a m^3 mixture)':'ash',
        'Water  (component 4)(kg in a m^3 mixture)':'water',
        'Superplasticizer (component 5)(kg in a m^3 mixture)':'superplasticizer',
        'Coarse Aggregate  (component 6)(kg in a m^3 mixture)':'coarse',
        'Fine Aggregate (component 7)(kg in a m^3 mixture)':'fine',
        'Age (day)':'age',
        'Concrete compressive strength(MPa, megapascals) ': 'strength'
    }
)
df = df.drop_duplicates()
X = df.drop(['strength'], axis=1) 
y = df['strength']

# Split the data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Train the model
model = LinearRegression()
model.fit(X_train, y_train)

# Initialize SHAP explainer and calculate values for the test set
explainer = shap.Explainer(model.predict, X_train)
shap_values = explainer(X_test)

# Plot partial dependence for the first test observation
idx = 0
shap.partial_dependence_plot(
    ""cement"", model.predict, X_test,
    model_expected_value=True, feature_expected_value=True, ice=False,
    shap_values=shap_values[idx:idx+1,:]
)

# Save the plot
plt.tight_layout()
plt.savefig('shap_dependence_plot.png', dpi=300)

However, when I generate the plot, the data point (black dot) does not align with the expected value line (blue line) for the feature of interest. It seems to be shifted along the y-axis. Here's the output plot for reference:

The plot seems correct when I initialize the SHAP explainer with the entire dataset X instead of just X_train:
explainer = shap.Explainer(linreg, X)
shap_values = explainer(X_test)

idx = 0
shap.partial_dependence_plot(
    ""cement"", model.predict, X_test,
    model_expected_value=True, feature_expected_value=True, ice=False,
    shap_values=shap_values[idx:idx+1,:]
)

Result:

Can someone explain why this misalignment occurs and how to correct the partial dependence plot when using a train-test split?
Any insights or suggestions would be greatly appreciated!
","Starting from where you define shap_values, it would make sense to be consistent with what you use as a background dataset and data to be explained (hence my comment above):
# Initialize SHAP explainer and calculate values for the test set
explainer = shap.Explainer(model.predict, X_test)
shap_values = explainer(X_test)

# Plot partial dependence for the first test observation
idx = 0
shap.partial_dependence_plot(
    ""cement"", model.predict, X_test,
    model_expected_value=True, feature_expected_value=True, ice=False,
    shap_values=shap_values[idx:idx+1,:]
)


or if you wish
# Initialize SHAP explainer and calculate values for the train set
explainer = shap.Explainer(model.predict, X_train)
shap_values = explainer(X_train)

# Plot partial dependence for the first train observation
idx = 0
shap.partial_dependence_plot(
    ""cement"", model.predict, X_train,
    model_expected_value=True, feature_expected_value=True, ice=False,
    shap_values=shap_values[idx:idx+1,:]
)


",scikit-learn
why does garchsvm output identical predictions for conditional volatility,"I'm using the SVR-GARCH model to predict conditional volatility, as described in the book Machine Learning for Financial Risk Management with Python: Algorithms for Modeling Risk by Abdullah Karasan.
I’ve encountered an issue where my code sometimes produces the same repeated value for the conditional volatility across the entire forecasting horizon. I understand that the initial parameter values are randomized, but I am confused about why, in most cases, the prediction results in a constant value throughout the forecast period.
import yfinance as yf
from datetime import datetime, timedelta
import pandas as pd
import numpy as np
from sklearn.svm import SVR
from sklearn.model_selection import RandomizedSearchCV
from scipy.stats import uniform as sp_rand
from sklearn.preprocessing import StandardScaler

# Select assets
stock_name = ['AAPL']
end_date = datetime.today()
start_date = end_date - timedelta(days = 365 * 25)

# Download the prices
prices = yf.download(
    stock_name,
    start = start_date,
    end = end_date,
    interval = '1d',
)['Adj Close']
prices = prices.dropna()
stock_name = ['Apple']
prices = prices.rename(stock_name[0], inplace = True)

# Log returns
returns = np.log(np.array(prices)[1:] / np.array(prices)[:-1])

# Forecasting horizon
H = 146

returns_series = pd.Series(returns)
realized_vol = returns_series.rolling(5).std()
realized_vol = pd.DataFrame(realized_vol)
realized_vol.reset_index(drop=True, inplace=True)

returns_svm = pd.DataFrame(returns ** 2)

X = pd.concat([realized_vol, returns_svm], axis=1, ignore_index=True)
X = X[4:].copy()
X = X.reset_index()
X.drop('index', axis=1, inplace=True)

realized_vol = realized_vol.dropna().reset_index()
realized_vol.drop('index', axis=1, inplace=True)

conditional_volatility = pd.DataFrame(index=prices.index[-H:], columns=['SVM Linear','SVM RBF','SVM Poly'])

para_grid = {'gamma': sp_rand(0.1, 1), 'C': sp_rand(0.1, 10), 'epsilon': sp_rand(0.1, 1)}

svr_lin = SVR(kernel='linear')
clf = RandomizedSearchCV(svr_lin, para_grid)
clf.fit(X[:-H], realized_vol.iloc[1:-(H-1)].values.reshape(-1,))
predict_svr_lin = clf.predict(X[-H:])
conditional_volatility['SVM Linear'] = predict_svr_lin

svr_rbf = SVR(kernel='rbf')
clf = RandomizedSearchCV(svr_rbf, para_grid)
clf.fit(X[:-H], realized_vol.iloc[1:-(H-1)].values.reshape(-1,))
predict_svr_rbf = clf.predict(X[-H:])
conditional_volatility['SVM RBF'] = predict_svr_rbf

svr_poly = SVR(kernel='poly')
clf = RandomizedSearchCV(svr_poly, para_grid)
clf.fit(X[:-H], realized_vol.iloc[1:-(H-1)].values.reshape(-1,))
predict_svr_poly = clf.predict(X[-H:])
conditional_volatility['SVM Poly'] = predict_svr_poly

print(conditional_volatility)

Output:
[*********************100%%**********************]  1 of 1 completed
            SVM Linear   SVM RBF  SVM Poly
Date                                      
2024-01-09    0.168156  0.168156  0.138204
2024-01-10    0.168156  0.168156  0.138204
2024-01-11    0.168156  0.168156  0.138204
2024-01-12    0.168156  0.168156  0.138204
2024-01-16    0.168156  0.168156  0.138204
...                ...       ...       ...
2024-08-01    0.168156  0.168156  0.138204
2024-08-02    0.168156  0.168156  0.138204
2024-08-05    0.168156  0.168156  0.138204
2024-08-06    0.168156  0.168156  0.138204
2024-08-07    0.168156  0.168156  0.138204

[146 rows x 3 columns]

Why this might be happening and how to address it?
","the issue is with your epsilon values. According to the SVR manual:

epsilon - Epsilon in the epsilon-SVR model. It specifies the
epsilon-tube within which no penalty is associated in the training
loss function with points predicted within a distance epsilon from the
actual value. Must be non-negative.

Thus with high epsilon (epsilon much greater than volatility) you won't give any penalties for bad predictions.
Let's check how your data is distributed:
print(np.quantile(realized_vol, q = [0,0.01,0.05,0.5,0.95, 0.99,1]))
# [0.00075101 0.00375092 0.0058641  0.04780242 0.06916338 0.33556116]

Basically in the beginning of the dataset volatility is very low and it is much greater in later periods.
Anyway, if you allow epsilon to be close to 1 (as in your code) you don't give penalty to any of the volatility points, because they all far below 1.
The fix is to change parameter grid to smth like this:
para_grid = {'gamma': sp_rand(0.1, 1), 
             'C': sp_rand(0.1, 10), 
             'epsilon': sp_rand(0.001, 0.01)
            }

or to multiply volatility by a high constant (after all volatility is a percentage):
para_grid = {'gamma': sp_rand(0.1, 1), 
             'C': sp_rand(0.1, 10), 
             'epsilon': sp_rand(0.1, 1)
             }

realized_vol = realized_vol * 100

",scikit-learn
how to train random forest classifier with large dataset to avoid memory errors in python,"I have a dataset that is 30 million rows. I have two columns: one that contains a 1 or 0 label and the other column has a list of 1280 features for each row (181 GB total). All I want to do is plug this dataset into a Random Forest algorithm, but the memory runs out and it crashes (I've tried using memory of 400 GB, but it still crashes).
After loading the dataset, I had to manipulate it a bit since it is in the Hugging Face arrow format: https://huggingface.co/docs/datasets/en/about_arrow (I suspect this is taking up a lot of RAM).
I am aware I could do some dimensionality reduction to my dataset, but is there any changes I should make to my code to reduce RAM usage?
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, roc_auc_score, roc_curve, auc
from datasets import load_dataset, Dataset

# Load dataset
df = Dataset.from_file(""data.arrow"")
df = pd.DataFrame(df)
X = df['embeddings'].to_numpy() # Convert Series to NumPy array
X = np.array(X.tolist()) # Convert list of arrays to a 2D NumPy array
X = X.reshape(X.shape[0], -1) # Flatten the 3D array into a 2D array
y = df['labels']

# Split the data into training and testing sets (80% train, 20% test)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Initialize the random forest classifier
rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)

# Train the classifier
rf_classifier.fit(X_train, y_train)

# Make predictions on the test set
y_pred = rf_classifier.predict(X_test)

# Evaluate the classifier

# Calculate accuracy
accuracy = accuracy_score(y_test, y_pred)
print(""Accuracy:"", accuracy)

# Calculate AUC score
auc_score = roc_auc_score(y_test, y_pred)
print(""AUC Score:"", auc_score)

with open(""metrics.txt"", ""w"") as f:
    f.write(""Accuracy: "" + str(accuracy) + ""\n"")
    f.write(""AUC Score: "" + str(auc_score))
    
# Make predictions on the test set
y_pred_proba = rf_classifier.predict_proba(X_test)[:, 1]

# Calculate ROC curve
fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba)

# Plot ROC curve
plt.figure()
plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % auc(fpr, tpr))
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve')
plt.legend(loc=""lower right"")

# Save ROC curve plot to an image file
plt.savefig('roc_curve.png')

# Close plot to free memory

","Few ideas:
Ensemble of models
Train N models (you must chose N, depending on RAM usage), each on only separate part of train data.
Then make fusion of models, use predict_proba(x) method for each model to make an inference and calculate average predictions.
This may have better/worse/same accuracy than single model, if N is not very large it should not have big impact.
Fork of scikit learn
Fork scikit learn and substitute every loop over x input train data with custom loop that loads the data from disk instead from RAM.
This is hard or very hard, long approach and I am not sure what problems you will face on the way.
In terms of difficulty worse would be only to write RF from the scratch.
Other ideas

RAM usage can be lowered by decreasing max_depth, n_estimators, max_features, etc.. Note those will affect your model accuracy (maybe in positive way! But to know this you would have to compare results...)
convert data to float32 (data.astype(np.float32)), or maybe even int16 if properly scaled+tranformed?
remove duplicates or close duplicates. Unlikely with so many features, but maybe you will find this useful. If there are samples that are very close to each other (distance metric I leave to you), calculate mean of those samples and replace them with this mean. Also give this sample sample_weight = number_of_averaged_samples
remove low importance features - train a model on random part of dataset and look at the model feature_importances. Then disregard them while loading dataset next time.

",scikit-learn
removing bigrams after tokenization for tfidfvectorizer,"I'm attempting to remove bi-grams that are created by TfidfVectorizer.  I'm using text.TfidfVectorizer so that I can use my own preprocessor function.
Test strings and preprocessor function:
doc2 = ['this is a test past performance here is another that has aa aa adding builing cat dog horse hurricane', 
        'another that has aa aa and start date and hurricane hitting south carolina']

def remove_bigrams(doc):
    gram_2 = ['past performance', 'start date', 'aa aa']
    res = []
    for record in doc:
        the_string = record
        for phrase in gram_2:
            the_string = the_string.replace(phrase, """")
        res.append(the_string)
    return res

remove_bigrams(doc2)

My TfidfVectorizer instantiation and fit_transform:
from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS as stop_words
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.feature_extraction import text

custom_stop_words = [i for i in stop_words]

vec = text.TfidfVectorizer(stop_words=custom_stop_words,
                           analyzer='word',
                           ngram_range=(2, 2),
                           preprocessor=remove_bigrams,
                          )

features = vec.fit_transform(doc2)

Here is my error:
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
Input In [49], in <cell line: 5>()
      3 #t3_cv = CountVectorizer(t2, stop_words = stop_words)
      4 vec = text.TfidfVectorizer(stop_words=custom_stop_words, analyzer='word', ngram_range = (2,2), preprocessor = remove_bigrams)
----> 5 features = vec.fit_transform(doc2)

File c:\Development_Solutions\Sandbox\SBVE\lib\site-packages\sklearn\feature_extraction\text.py:2079, in TfidfVectorizer.fit_transform(self, raw_documents, y)
   2072 self._check_params()
   2073 self._tfidf = TfidfTransformer(
   2074     norm=self.norm,
   2075     use_idf=self.use_idf,
   2076     smooth_idf=self.smooth_idf,
   2077     sublinear_tf=self.sublinear_tf,
   2078 )
-> 2079 X = super().fit_transform(raw_documents)
   2080 self._tfidf.fit(X)
   2081 # X is already a transformed view of raw_documents so
   2082 # we set copy to False

File c:\Development_Solutions\Sandbox\SBVE\lib\site-packages\sklearn\feature_extraction\text.py:1338, in CountVectorizer.fit_transform(self, raw_documents, y)
   1330             warnings.warn(
   1331                 ""Upper case characters found in""
   1332                 "" vocabulary while 'lowercase'""
   1333                 "" is True. These entries will not""
   1334                 "" be matched with any documents""
   1335             )
   1336             break
-> 1338 vocabulary, X = self._count_vocab(raw_documents, self.fixed_vocabulary_)
   1340 if self.binary:
   1341     X.data.fill(1)

File c:\Development_Solutions\Sandbox\SBVE\lib\site-packages\sklearn\feature_extraction\text.py:1209, in CountVectorizer._count_vocab(self, raw_documents, fixed_vocab)
   1207 for doc in raw_documents:
   1208     feature_counter = {}
-> 1209     for feature in analyze(doc):
   1210         try:
   1211             feature_idx = vocabulary[feature]

File c:\Development_Solutions\Sandbox\SBVE\lib\site-packages\sklearn\feature_extraction\text.py:113, in _analyze(doc, analyzer, tokenizer, ngrams, preprocessor, decoder, stop_words)
    111     doc = preprocessor(doc)
    112 if tokenizer is not None:
--> 113     doc = tokenizer(doc)
    114 if ngrams is not None:
    115     if stop_words is not None:

TypeError: expected string or bytes-like object

How to resolve it?
","The preprocessor should handle documents, not the whole corpus. (The clues are the ""expected string"" in the error, and the fact that the TfidfVectorizer docs refer to ""the preprocessing (string transformation) stage"". The docs could definitely be clearer.)
This should fix it:
def remove_bigrams(doc: str) -> str:
    """"""Remove certain bi-grams from a document.""""""
    gram_2 = ['past performance', 'start date', 'aa aa']
    for phrase in gram_2:
        doc = doc.replace(phrase, """")
    return doc

",scikit-learn
difference between numpylinalglstsq and sklearnlinear_modellinearregression,"As I understand, numpy.linalg.lstsq and sklearn.linear_model.LinearRegression both look for solutions x of the linear system Ax = y, that minimise the resdidual sum ||Ax - y||.
But they don't give the same result:
from sklearn import linear_model
import numpy as np

A = np.array([[1, 0], [0, 1]])
b = np.array([1, 0])
x , _, _, _ = np.linalg.lstsq(A,b)
x

Out[1]: array([ 1.,  0.])

clf = linear_model.LinearRegression()
clf.fit(A, b)                              
coef = clf.coef_
coef

Out[2]: array([ 0.5, -0.5])

What am I overlooking?
","Both of them are implemented by LPACK gelsd.
The difference is that linear_model.LinearRegression will do data pre-process (default) as below for input X (your A). But np.linalg.lstsq don't. You can refer to the source code of LinearRegression for more details about the data pre-process.
X = (X - X_offset) / X_scale

If you don't want the data pre-process, you should set fit_intercept=False.
Briefly speaking, if you normalize your input before linear regression, you will get the same result by both linear_model.LinearRegression and np.linalg.lstsq as below.
# Normalization/Scaling
from sklearn.preprocessing import StandardScaler
A = np.array([[1, 0], [0, 1]])
X_scaler = StandardScaler()
A = X_scaler.fit_transform(A)

Now A is array([[ 1., -1.],[-1.,  1.]])
from sklearn import linear_model
import numpy as np

b = np.array([1, 0])
x , _, _, _ = np.linalg.lstsq(A,b)
x
Out[1]: array([ 0.25, -0.25])

clf = linear_model.LinearRegression()
clf.fit(A, b)                              
coef = clf.coef_
coef

Out[2]: array([ 0.25, -0.25])

",scikit-learn
roc curve without a model estimator,"I have created an AI Tool which extracts content from an image and then reviews that content for completeness and accuracy. I am trying to evaluate the performance of this tool and am gathering metrics to do so.
I have a resulting table that looks similar to the below; the true values come from manual ground truth reviews of documents and the predicted values are the actual output of the tool.
ID | true | predicted |
-----------------------
1  |  0   |      1    |
2  |  0   |      0    |
3  |  1   |      1    |
4  |  1   |      0    |

I have been able to use the true and predicted columns to obtain various metrics using the code below:
def calculate_metrics(df, true, predicted):
    accuracy = accuracy_score(df[true], df[predicted])
    precision = precision_score(df[true], df[predicted])
    recall = recall_score(df[true], df[predicted])
    f1 = f1_score(df[true], df[predicted])
    roc_auc = roc_auc_score(df[true], df[predicted])
    return print(f""accuracy: {accuracy}\nprecision: {precision}\nrecall: {recall}\nf1: {f1}\nroc_auc: {roc_auc}"")

Additionally, I would like to plot a ROC curve. I am able to obtain the roc_auc score and assumed I could plot from there, but I'm having a hard time wrapping my head around how exactly to do so. It looks like I need a model estimator to determine probabilities and then from there I can create the plot but I'm unclear how to do so with the data I've obtained.
Is it even possible to create a ROC curve using the results I have and, if so, how do I do so?
","
Is it even possible to create a ROC curve using the results I have and, if so, how do I do so?

No. You need to have the model produce the probability that an observation falls into a class, rather than the predicted class. The point of an ROC curve is that it allows you to visualize how different thresholds for probability of determining that the data falls into either class affects the true positive rate and false positive rate.
However, this data is after thresholding, which means that you can't visualize the effect of different thresholds. You need your model to output a probability rather than the predicted class in order to perform this visualization.
",scikit-learn
valueerror the number of classes has to be greater than one python,"When passing x,y in fit, I am getting the following error:
Traceback (most recent call last):

File ""C:/Classify/classifier.py"", line 95, in 
train_avg, test_avg, cms = train_model(X, y, ""ceps"", plot=True)
  File ""C:/Classify/classifier.py"", line 47, in train_model
clf.fit(X_train, y_train)   File ""C:\Python27\lib\site-packages\sklearn\svm\base.py"", line 676, in fit
  raise ValueError(""The number of classes has to be greater than"" ValueError: The number of classes has to be greater than one.

Below is my code:
def train_model(X, Y, name, plot=False):
""""""
    train_model(vector, vector, name[, plot=False])

    Trains and saves model to disk.
""""""
labels = np.unique(Y)

cv = ShuffleSplit(n=len(X), n_iter=1, test_size=0.3, indices=True, random_state=0)

train_errors = []
test_errors = []

scores = []
pr_scores = defaultdict(list)
precisions, recalls, thresholds = defaultdict(list), defaultdict(list), defaultdict(list)

roc_scores = defaultdict(list)
tprs = defaultdict(list)
fprs = defaultdict(list)

clfs = []  # for the median

cms = []

for train, test in cv:
    X_train, y_train = X[train], Y[train]
    X_test, y_test = X[test], Y[test]

    clf = LogisticRegression()
    clf.fit(X_train, y_train)
    clfs.append(clf)

","You probably have only one unique class label in the training set present. As the error messages noted, you need to have at least two unique classes in the dataset. E.g., you can run np.unique(y) to see what the unique class labels in your dataset are.
",scikit-learn
sklearn logistic regression gives biased results,"I am running around 1000 similar logistic regressions, with the same covariates but slightly different data and response variables. All of my response variables have a sparse successes (p(success) < .05 usually).
I run LR as follows: I have a matrix of called “success_fail” that has for each setting (row of the design matrix) the number of success and number of fail. I run LR as:
skdesign = np.vstack((design,design))
sklabel = np.hstack((np.ones(success_fail.shape[0]), 
                     np.zeros(success_fail.shape[0])))
skweight = np.hstack((success_fail['success'], success_fail['fail']))
logregN = linear_model.LogisticRegression(C=1, 
                                solver= 'lbfgs',fit_intercept=False)   
logregN.fit(skdesign, sklabel, sample_weight=skweight)

(sklearn version 0.18)
I noticed that with the regularized regression, the results are consistently biased to predict more ""successes"" than is observed in the training data.  When I relax the regularization, this  bias goes away.  The bias observed is unacceptable for my use case, but the more-regularized model does seem a bit better. 
Below, I plot the results for the 1000 different regressions for 2 different values of C:

I looked at the parameter estimates for one of these regressions: below each point is one parameter. It seems like the intercept (the point on the bottom left) is too high for the C=1 model. 
Why is this happening? How can I fix it?  Can I make sklearn regularize the intercept less?
","Thanks to  the lovely folks at the sklearn mailing list I found out the answer. As you can see in the Question I made a design matrix (including intercept), and then fit the model with fit_intercept = False set.  This resulted in regularization of the intercept.  Very stupid on my part!  All I needed to do was remove the intercept from the design and remove fit_intercept = False.
",scikit-learn
value error y contains previously unseen labels 8 9,"I have a dataset, that I use label_encoding for data preprocessing before feed into the model.
I first label_encoding all of my data, then split the data into training and testing.
And I want to use ""label_encoder.inverse_transform"" to reverse the code, but it has this error message:
y contains previously unseen labels: [8 9]

The error message was clear, but that's the part that I am confused, because I label-encoded all of the data before splitting.
I've checked the other post: Getting ValueError: y contains new labels when using scikit learn's LabelEncoder, but that post was about the author had new dataset as testing data, mine on the otherhand, I did label_encoding the whole dataset and then split. So the encoder should have already seen all of my data, including x_test.
from sklearn.preprocessing import LabelEncoder
label_encoder = LabelEncoder()
category =[]
data_encoded = data_cleaned
for i in data_cleaned.columns:
    if data_cleaned[i].dtype == 'category':
        category.append(i)
        data_encoded[i] = label_encoder.fit_transform(data_cleaned[i])

Split the training and testing data
target_variable = ['target']
data_encoded = data_encoded.dropna(subset=target_variable)
x = data_encoded.drop(target_variable, axis=1)
y = data_encoded[target_variable]

And try to reverse the encoded label
//only want to reverse the category column
for i in category:
    X_test[i] = label_encoder.inverse_transform(X_test[i])
    print(X_test[i])

If anyone could point some direction that would be great!
","The issue is that in your loop, the label_encoder keep getting fit_transform to a new column, in which it relearns the new encoding. So when you do inverse_transform, the label_encoder only has the information from the last column it encoded. You will need to create a LE for each column and can use it to inverse_transform for each column.
Here is what I would suggest you to do:
# List of Label Encoder for each categorical feature
le = [LabelEncoder() for i in range(len(category))]
data_encoded = data_cleaned
for i in range(len(category)): 
    feat = cat_feat[i]
    data_encoded[feat] = le[i].fit_transform(data_cleaned[feat])

# After splitting:
for i in range(len(category)): 
    feat = category[i]
    X_test[feat] = le[i].inverse_transform(X_test[feat])
    print(X_test[feat])

",scikit-learn
difference between normalizer and minmaxscaler,"I'm trying to understand the effects of applying the Normalizer or applying MinMaxScaler or applying both in my data. I've read the docs in SKlearn, and saw some examples of use. I understand that MinMaxScaler is important (is important to scale the features), but what about Normalizer?
It keeps unclear to me the practical result of using the Normamlizer in my data.
MinMaxScaler is applied column-wise, Normalizer is apllied row-wise. What does it implies? Should I use the Normalizer or just use the MinMaxScale or should use then both?
","As you have said,

MinMaxScaler is applied column-wise, Normalizer is applied row-wise.

Do not confuse Normalizer with MinMaxScaler. The Normalizer class from Sklearn normalizes samples individually to unit norm. It is not column based but a row-based normalization technique. In other words, the range will be determined either by rows or columns.
So, remember that we scale features not records, because we want features to have the same scale, so the model to be trained will not give different weights to different features based on their range. If we scale the records, this will give each record its own scale, which is not what we need.
So, if features are represented by rows, then you should use the Normalizer. But in most cases, features are represented by columns, so you should use one of the scalers from Sklearn depending on the case:

MinMaxScaler transforms features by scaling each feature to a given range. It scales and translates each feature individually such that it is in the given range on the training set, e.g. between zero and one.
The transformation is given by:
X_std = (X - X.min(axis=0)) / (X.max(axis=0) - X.min(axis=0))

X_scaled = X_std * (max - min) + min

where min, max = feature_range.
This transformation is often used as an alternative to zero mean, unit variance scaling.

StandardScaler standardizes features by removing the mean and scaling to unit variance. The standard score of a sample x is calculated as:
z = (x - u) / s. Use this if the data distribution is normal.

RobustScaler is robust to outliers. It removes the median and scales the data according to IQR (Interquartile Range). The IQR is the range between the 25th quantile and the 75th quantile.


",scikit-learn
target scaling using gridsearchcv,"For hyperparameter tuning, I use the function GridSearchCV from the Python package sklearn. Some of the models that I test require feature scaling (e.g. Support Vector Regression - SVR). Recently, in the Udemy course Machine Learning A-Z™: Hands-On Python & R In Data Science, the instructors mentioned that for SVR, the target should also be scaled (if it is not binary). Bearing this in mind, I wonder whether the target is also scaled in each iteration of the cross-validation procedure performed by GridSearchCV or if only the features are scaled. Please see the code below, which illustrates the normal procedure that I use for hyperparameter tuning for estimators that require scaling for the training sets:
from sklearn.pipeline import make_pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import GridSearchCV
from sklearn.svm import SVR
    
def SVRegressor(**kwargs):
   '''contruct a pipeline to perform SVR regression'''
   return make_pipeline(StandardScaler(), SVR(**kwargs))

params = {'svr__kernel': [""poly"", ""rbf""]}
grid_search = GridSearchCV(SVRegressor(), params)
grid_search.fit(X, y)

I know that I could simply scale X and y a priori and drop the StandardScaler from the pipeline. However, I want to implement this approach in a code pipeline where multiple models are tested, in which, some require scaling and others do not. That is why I want to know how GridSearchCV handles scaling under the hood.
","No it doesn't scale the target, if you look at make_pipeline, it simply passes the X and y argument into your transformer, and StandardScaler() does nothing to your y:
def _fit_transform_one(transformer,
                       X,
                       y,
                       weight,
                       message_clsname='',
                       message=None,
                       **fit_params):
    """"""
    Fits ``transformer`` to ``X`` and ``y``. The transformed result is returned
    with the fitted transformer. If ``weight`` is not ``None``, the result will
    be multiplied by ``weight``.
    """"""
    with _print_elapsed_time(message_clsname, message):
        if hasattr(transformer, 'fit_transform'):
            res = transformer.fit_transform(X, y, **fit_params)
        else:
            res = transformer.fit(X, y, **fit_params).transform(X)

    if weight is None:
        return res, transformer
    return res * weight, transformer

You can try this on StandardScaler() and you can see it does not do anything with y:
np.random.seed(111)
X = np.random.normal(5,2,(100,3))
y = np.random.normal(5,2,100)

res = StandardScaler().fit_transform(X=X,y=y)
res.shape
(100, 3)

res.mean(axis=0)
array([1.01030295e-15, 4.39648318e-16, 8.91509089e-16])

res.std(axis=0)
array([1., 1., 1.])

You can also check the result of your gridsearchcv:
SVRegressor = make_pipeline(StandardScaler(), SVR())
params = {'svr__kernel': [""poly"", ""rbf""]}
grid_search = GridSearchCV(SVRegressor, params,
scoring='neg_mean_absolute_error')

On unscaled y, you will see that on the unscaled data, your negative mean absolute error is around the same scale as your standard deviation (I used 2 in my example):
grid_search.fit(X, y)

grid_search.cv_results_['mean_test_score']
array([-2.01029707, -1.88779205])

On scaled y, our standard deviation would be 1, and you can see the error is around -1,:
y_scaled = StandardScaler().fit_transform(y.reshape(-1,1)).ravel()
grid_search.fit(X, y_scaled)

grid_search.cv_results_['mean_test_score']
array([-1.00585999, -0.88330208])

",scikit-learn
performing voting for classification tasks,"I am wondering is it possible to do voting for classification tasks. I have seen plenty of blogs explaining how to use voting for regression purposes.As given below.
# initializing all the model objects with default parameters
model_1 = LinearRegression()
model_2 = xgb.XGBRegressor()
model_3 = RandomForestRegressor()
 
# training all the model on the training dataset
model_1.fit(X_train, y_target)
model_2.fit(X_train, y_target)
model_3.fit(X_train, y_target)
 
# predicting the output on the validation dataset
pred_1 = model_1.predict(X_test)
pred_2 = model_2.predict(X_test)
pred_3 = model_3.predict(X_test)
 
# final prediction after averaging on the prediction of all 3 models
pred_final = (pred_1+pred_2+pred_3)/3.0
 
# printing the mean squared error between real value and predicted value
print(mean_squared_error(y_test, pred_final))

","That can be done.
# initializing all the model objects with default parameters

model_1= svm.SVC(kernel='rbf')
model_2 = XGBClassifier()
model_3 = RandomForestClassifier()
 
# Making the final model using voting classifier
final_model = VotingClassifier(estimators=[('svc', model_1), ('xgb', model_2), ('rf', model_3)], voting='hard')

# applying 10 fold cross validation
scores = cross_val_score(final_model, X_all, y, cv=10, scoring='accuracy')
print(scores) 
print('Model accuracy score : {0:0.4f}'.format(scores.mean()))

You can add more machine learning models than three if necessary
Here note that I have applied cross validation and got the accuracy
",scikit-learn
python3 skglm  39poisson39 object has no attribute 39get_lipschitz39,"Im working with count data and want to fit a poisson regression with a L1 norm. I have the following code which throws the error and is reproducable:
import numpy as np
import skglm
import sklearn

X = np.random.rand(100, 10)  
y = np.random.poisson(lam=2.0, size=100)  + 1 #If poisson == 0 at any point it throws an error for count data, so add 1.

# Split the data into training and test sets
X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(X, y, test_size=0.2, random_state=42)

# Create the Poisson regression model with L1 regularization
poisson_model = skglm.GeneralizedLinearEstimator(datafit=skglm.datafits.Poisson(), penalty=skglm.penalties.L1(alpha=1.0))

# Fit the model to the training data
#The error is thrown on this line.
poisson_model.fit(X_train, y_train)

# Predict on the test set
y_pred = poisson_model.predict(X_test)

# Evaluate the model
mse = sklearn.metrics.mean_squared_error(y_test, y_pred)
print(f'Mean Squared Error: {mse}')

The error I get running on skglm 0.3.1 (most recent version I think):
Cell In[58], line 15
     12 poisson_model = skglm.GeneralizedLinearEstimator(datafit=skglm.datafits.Poisson(), penalty=skglm.penalties.L1(alpha=1.0))
     14 # Fit the model to the training data
---> 15 poisson_model.fit(X_train, y_train)
     17 # Step 5: Make predictions
     18 # Predict on the test set
     19 y_pred = poisson_model.predict(X_test)

File c:...\Python\Python311\Lib\site-packages\skglm\estimators.py:252, in GeneralizedLinearEstimator.fit(self, X, y)
    249 self.datafit = self.datafit if self.datafit else Quadratic()
    250 self.solver = self.solver if self.solver else AndersonCD()
--> 252 return _glm_fit(X, y, self, self.datafit, self.penalty, self.solver)

File c:...\Python\Python311\Lib\site-packages\skglm\estimators.py:137, in _glm_fit(X, y, model, datafit, penalty, solver)
    132     if len(penalty.weights) != n_features:
    133         raise ValueError(
    134             ""The size of the WeightedL1 penalty weights should be n_features, ""
    135             ""expected %i, got %i."" % (X_.shape[1], len(penalty.weights)))
--> 137 coefs, p_obj, kkt = solver.solve(X_, y, datafit_jit, penalty_jit, w, Xw)
    138 model.coef_, model.stop_crit_ = coefs[:n_features], kkt
    139 if y.ndim == 1:

File c:...\Python\Python311\Lib\site-packages\skglm\solvers\anderson_cd.py:83, in AndersonCD.solve(self, X, y, datafit, penalty, w_init, Xw_init)
     81 else:
     82     datafit.initialize(X, y)
---> 83     lipschitz = datafit.get_lipschitz(X, y)
     85 if len(w) != n_features + self.fit_intercept:
     86     if self.fit_intercept:

AttributeError: 'Poisson' object has no attribute 'get_lipschitz'

I changed the file location names since it has my full name but otherwise it is copy pasted.
This thread on sklearn says that it should be possible to perform a poisson regression with an L1 norm, but I can't figure out what I'm doing wrong.
Any help is greatly appreciated, thank you!
","You need to use a different solver than the default (AndersonCD), use the ProxNewton-solver.
from skglm.solvers import ProxNewton

estimator = GeneralizedLinearEstimator(
    datafit=Poisson(),
    penalty=L1(alpha=1.),
    solver=ProxNewton(),
)
estimator.fit(X, y)

",scikit-learn
how can i link the records in the training dataset to the corresponding model predictions,"Using scikit-learn, I've set up a regression model to predict customers' maximum spend per transaction. The dataset I'm using looks a bit like this; the target column is maximum spend per transaction during the previous year:
customer_number | metric_1 | metric_2 | target
----------------|----------|----------|-------
111             | A        | X        | 15
222             | A        | Y        | 20
333             | B        | Y        | 30

I split the dataset into training & testing sets, one-hot encode the features, train the model, and make some test predictions:
target = pd.DataFrame(dataset, columns = [""target""])
features = dataset.drop(""target"", axis = 1)
train_features, test_features, train_target, test_target = train_test_split(features, target, test_size = 0.25)

train_features = pd.get_dummies(train_features)
test_features = pd.get_dummies(test_features)

model = RandomForestRegressor()
model.fit(X = train_features, y = train_target)

test_prediction = model.predict(X = test_features)

I can output various measures of the model's accuracy (mean average error, mean squared error etc) using the relevant functions in scikit-learn. However, I'd like to be able to tell which customers' predictions are the most inaccurate. So I want to be able to create a dataframe which looks like this:
customer_number | target | prediction | error
----------------|--------|----------- |------
111             | 15     | 17         | 2
222             | 20     | 19         | 1
333             | 30     | 50         | 20

I can use this to investigate if there is any correlation between the features and the model making inaccurate predictions. In this example, I can see that customer 333 has the biggest error by far, so I could potentially infer that customers with metric_1 = B end up with less accurate predictions.
I think I can calculate errors like this (please correct me if I'm wrong on this), but I don't know how to tie them back to customer number.
error = abs(test_target - test_prediction) 

How can I get the desired result?
","The error you are computing is the absolute error. When averaged it gives the Mean Absolute Error which is commonly used to evaluate regression models. You can read about the choice of an error metric here.
This error vector is the length of your test dataset and its elements are in the same order as your records. Many people assign them back into the dataframe. Then, if you leave customer number in there, everything should line up.
Starting with the DataFrame df and using idiomatic names for things:
df_train, df_test = train_test_split(df)

y_train, y_test = df_train[""target""], df_test[""target""]

X_train = df_train.drop([""customer_number"", ""target""], axis=1)
X_test = df_test.drop([""customer_number"", ""target""], axis=1)

X_train = pd.get_dummies(X_train)
X_test = pd.get_dummies(X_test)

model = RandomForestRegressor()
model.fit(X_train, y_train)

df_test[""prediction""] = model.predict(X_test)
df_test[""error""] = abs(df_test[""target""] - df_test[""prediction""])

",scikit-learn
choose naive bayes model for continous feature multiple labels,"Imagining I have a data set, whose feature values are continuous, and there are more than two possible labels (eg: rain, sunny, windy etc), which naive bayes model should I implement in sklearn?
I am thinking about Gaussian or Multinomial. However, multinomial works for discrete features, and I tried gaussian, but it turns out that the accuracy of the prediction is like random selecting.
","Naive Bayes Classification (NBC) works with discrete values.
That means you have to discretize all features which are continuous. For more details, this could help
Anyways, multinominal is correct because you have more than one label. But you should also keep in mind that you have to one-hot encode your labels (OneHotEncoder in sklearn).
",scikit-learn
confusion matrix on images in cnn keras,"I have trained my model(multiclass classification) of CNN using keras and now I want to evaluate the model on my test set of images.
What are the possible options for evaluating my model apart from the accuracy, precision and recall? I know how to get the precision and recall from a custom script. But I cannot find a way to get the confusion matrix for my 12 classes of images. 
Scikit-learn shows a way, but not for images.
I am using model.fit_generator ()
Is there a way to create confusion matrix for all my classes or finding classification confidence on my classes? I am using Google Colab, though I can download the model and run it locally.
Any help would be appreciated.
Code:
train_data_path = 'dataset_cfps/train'
validation_data_path = 'dataset_cfps/validation'

#Parametres
img_width, img_height = 224, 224

vggface = VGGFace(model='resnet50', include_top=False, input_shape=(img_width, img_height, 3))

#vgg_model = VGGFace(include_top=False, input_shape=(224, 224, 3))

last_layer = vggface.get_layer('avg_pool').output
x = Flatten(name='flatten')(last_layer)
xx = Dense(256, activation = 'sigmoid')(x)
x1 = BatchNormalization()(xx)
x2 = Dropout(0.3)(x1)
y = Dense(256, activation = 'sigmoid')(x2)
yy = BatchNormalization()(y)
y1 = Dropout(0.6)(yy)
x3 = Dense(12, activation='sigmoid', name='classifier')(y1)

custom_vgg_model = Model(vggface.input, x3)


# Create the model
model = models.Sequential()

# Add the convolutional base model
model.add(custom_vgg_model)

model.summary()
#model = load_model('facenet_resnet_lr3_SGD_sameas1.h5')

def recall(y_true, y_pred):
     true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))
     possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))
     recall = true_positives / (possible_positives + K.epsilon())
     return recall

def precision(y_true, y_pred):
    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))
    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))
    precision = true_positives / (predicted_positives + K.epsilon())
    return precision

train_datagen = ImageDataGenerator(
      rescale=1./255,
      rotation_range=20,
      width_shift_range=0.2,
      height_shift_range=0.2,
      horizontal_flip=True,
      fill_mode='nearest')


validation_datagen = ImageDataGenerator(rescale=1./255)

# Change the batchsize according to your system RAM
train_batchsize = 32
val_batchsize = 32

train_generator = train_datagen.flow_from_directory(
        train_data_path,
        target_size=(img_width, img_height),
        batch_size=train_batchsize,
        class_mode='categorical')

validation_generator = validation_datagen.flow_from_directory(
        validation_data_path,
        target_size=(img_width, img_height),
        batch_size=val_batchsize,
        class_mode='categorical',
        shuffle=True)

# Compile the model
model.compile(loss='categorical_crossentropy',
              optimizer=optimizers.SGD(lr=1e-3),
              metrics=['acc', recall, precision])
# Train the model
history = model.fit_generator(
      train_generator,
      steps_per_epoch=train_generator.samples/train_generator.batch_size ,
      epochs=100,
      validation_data=validation_generator,
      validation_steps=validation_generator.samples/validation_generator.batch_size,
      verbose=1)

# Save the model
model.save('facenet_resnet_lr3_SGD_new_FC.h5')

","Here's how to get the confusion matrix(or maybe statistics using scikit-learn) for all classes:
1.Predict classes
test_generator = ImageDataGenerator()
test_data_generator = test_generator.flow_from_directory(
    test_data_path, # Put your path here
     target_size=(img_width, img_height),
    batch_size=32,
    shuffle=False)
test_steps_per_epoch = numpy.math.ceil(test_data_generator.samples / test_data_generator.batch_size)

predictions = model.predict_generator(test_data_generator, steps=test_steps_per_epoch)
# Get most likely class
predicted_classes = numpy.argmax(predictions, axis=1)

2.Get ground-truth classes and class-labels
true_classes = test_data_generator.classes
class_labels = list(test_data_generator.class_indices.keys())   

3. Use scikit-learn to get statistics
report = metrics.classification_report(true_classes, predicted_classes, target_names=class_labels)
print(report)    

You can read more here
EDIT:
If the above does not work, have a look at this video Create confusion matrix for predictions from Keras model. Probably look through the comments if you have an issue.
Or Make predictions with a Keras CNN Image Classifier
",scikit-learn
mixing categorial and continuous data in naive bayes classifier using scikitlearn,"I'm using scikit-learn in Python to develop a classification algorithm to predict the gender of certain customers. Amongst others, I want to use the Naive Bayes classifier but my problem is that I have a mix of categorical data (ex: ""Registered online"", ""Accepts email notifications"" etc) and continuous data (ex: ""Age"", ""Length of membership"" etc). I haven't used scikit much before but I suppose that that Gaussian Naive Bayes is suitable for continuous data and that Bernoulli Naive Bayes can be used for categorical data. However, since I want to have both categorical and continuous data in my model, I don't really know how to handle this. Any ideas would be much appreciated!
","You have at least two options:

Transform all your data into a categorical representation by computing percentiles for each continuous variables and then binning the continuous variables using the percentiles as bin boundaries. For instance for the height of a person create the following bins: ""very small"", ""small"", ""regular"", ""big"", ""very big"" ensuring that each bin contains approximately 20% of the population of your training set. We don't have any utility to perform this automatically in scikit-learn but it should not be too complicated to do it yourself. Then fit a unique multinomial NB on those categorical representation of your data.
Independently fit a gaussian NB model on the continuous part of the data and a multinomial NB model on the categorical part. Then transform all the dataset by taking the class assignment probabilities (with predict_proba method) as new features: np.hstack((multinomial_probas, gaussian_probas)) and then refit a new model (e.g. a new gaussian NB) on the new features.

",scikit-learn
understanding sklearn39s knnimputer,"I was going through its documentation and it says

Each sample’s missing values are imputed using the mean value from
n_neighbors nearest neighbors found in the training set. Two samples
are close if the features that neither are missing are close.

Now, playing around with a toy dataset, i.e.
>>>X = [[1, 2, nan], [3, 4, 3], [nan, 6, 5], [8, 8, 7]]
>>>X

   [[ 1.,  2., nan],
    [ 3.,  4.,  3.],
    [nan,  6.,  5.],
    [ 8.,  8.,  7.]]

And we make a KNNImputer as follows:
imputer = KNNImputer(n_neighbors=2)

The question is, how does it fill the nans while having nans in 2 of the columns? For example, if it is to fill the nan in the 3rd column of the 1st row, how will it choose which features are the closest since one of the rows has nan in the first column as well? When I do imputer.fit_transform(X) it gives me
array([[1. , 2. , 4. ],
       [3. , 4. , 3. ],
       [5.5, 6. , 5. ],
       [8. , 8. , 7. ]])

which means for filling out the nan in row 1, the nearest neighbors were the second and the third row. How did it calculate the euclidean distance between the first and the third row?
","
How does it fill the NaNs using rows that also have NaNs?

This doesn't seem to be mentioned in the docs. But by digging a bit into the source code, it appears that for each column being imputed, all donors at a smaller distance are considered, even if they have missing values. The way this is handled is by setting to 0 the missing values in a weight matrix, which is obtained according to the used distance, see _get_weights.
The relevant code is in _calc_impute, where after finding a distance matrix for all potential donors, and then the above mentioned matrix of weights, it is imputed as:
# fill nans with zeros
if weight_matrix is not None:
    weight_matrix[np.isnan(weight_matrix)] = 0.0

Where all potential donors are considered if they have at least one non-nan distance with the reciever
dist_pot_donors : ndarray of shape (n_receivers, n_potential_donors)
    Distance matrix between the receivers and potential donors from
    training set. There must be at least one non-nan distance between
    a receiver and a potential donor.

We could check this with a toy example; in the following matrix, when inputting the missing value in [nan,  7.,  4.,  5.], the last row (which also contains two NaNs) is chosen (note that I've set n_neighbors=1). This is because the distance wrt the last row is 0, as the distance corresponding to the NaN values has been set to 0. So by just having a minimal difference with rows 2 and 3, the last row is chosen since it is seen as being equal:
X = np.array([[np.nan,7,4,5],[2,8,4,5],[3,7,4,6],[1,np.nan,np.nan,5]])

print(X)
array([[nan,  7.,  4.,  5.],
       [ 2.,  8.,  4.,  5.],
       [ 3.,  7.,  4.,  6.],
       [ 1., nan, nan,  5.]])

from sklearn.impute import KNNImputer
imputer = KNNImputer(n_neighbors=1)

imputer.fit_transform(X)
array([[1., 7., 4., 5.],
       [2., 8., 4., 5.],
       [3., 7., 4., 6.],
       [1., 7., 4., 5.]])

",scikit-learn
attributeerror 39decisiontreeclassifier39 object has no attribute 39feature_names_in_39,"I want to implement a decision tree for a dataset, and I am just a beginner in this field. But after I run the function, I get the error:

AttributeError: 'DecisionTreeClassifier' object has no attribute
'feature_names_in_'

Although based on this link, this attribute can be called over DecisionTreeClassifier() objects.
And here is my function, and the packages which I have installed:
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score
from sklearn.metrics import confusion_matrix
from sklearn.tree import export_graphviz 
from IPython.display import Image  
from sklearn import tree
import gdown
from graphviz import Source
from sklearn.tree import export_graphviz

def decision_tree(data):
  X = data.drop(['VendorID', 'VendorID_zscore', 'VendorID_boxwhiskerscore', 'VendorID_normalized',
                 'VendorID_zscore_normalized', 'VendorID_boxwhiskerscore_normalized', 'cluster'], axis=1)
  y = data['cluster']

  X_train, X_test, y_train, y_test =  train_test_split(X, y, test_size = 0.25, random_state= 0)

  sc_X = StandardScaler()
  X_train = sc_X.fit_transform(X_train)
  X_test = sc_X.transform(X_test)

  classifier = DecisionTreeClassifier()
  classifier.fit(X_train, y_train)

  #  Prediction
  y_pred = classifier.predict(X_test) #Accuracy
  print('Accuracy Score:', accuracy_score(y_test,y_pred))

  #  Confusion Matrix
  cm = confusion_matrix(y_test, y_pred)
  print('Confusion Matrix: ', cm)

  # visualization
  export_graphviz(
        classifier,
        out_file=""tree.dot"",
        feature_names = classifier.feature_names_in_,
        class_names=['cluster'],
        rounded=True,
        filled=True
    )

Here is the whole error in my Jupiter notebook terminal:
<ipython-input-19-51196bcefa11> in decision_tree(data)
     25         classifier,
     26         out_file=""tree.dot"",
---> 27         feature_names = classifier.feature_names_in_,
     28         class_names=['cluster'],
     29         rounded=True,

AttributeError: 'DecisionTreeClassifier' object has no attribute 'feature_names_in_'

Edit:
I have tried the plot_tree library, and I can save the tree in a file and render it in the terminal. Here is my new code:
def decision_tree(data):
  X = data.drop(['VendorID', 'VendorID_zscore', 'VendorID_boxwhiskerscore', 'VendorID_normalized',
                 'VendorID_zscore_normalized', 'VendorID_boxwhiskerscore_normalized', 'cluster'], axis=1)
  y = data['cluster']

  X_train, X_test, y_train, y_test =  train_test_split(X, y, test_size = 0.25, random_state= 0)

  sc_X = StandardScaler()
  X_train = sc_X.fit_transform(X_train)
  X_test = sc_X.transform(X_test)

  plt.figure(dpi=1200, figsize=(8, 6))
  classifier = DecisionTreeClassifier().fit(X_train, y_train)
  plot_tree(classifier, filled=True, max_depth=4)
  plt.title(""Decision tree trained on all the NYC Taxi Trips features"")
  plt.savefig('decision_tree.png', dpi=1200)
  plt.show()
  #  Prediction
  y_pred = classifier.predict(X_test) #Accuracy
  print('Accuracy Score:', accuracy_score(y_test,y_pred))

  #  Confusion Matrix
  cm = confusion_matrix(y_test, y_pred)
  print('Confusion Matrix: ', cm)

","I solved the problem with some little changes.
def decision_tree(data):

  X =data.drop(['VendorID', 'VendorID_zscore', 'VendorID_boxwhiskerscore', 'VendorID_normalized',
                 'VendorID_zscore_normalized', 'VendorID_boxwhiskerscore_normalized', 'cluster'], axis=1)
  
  y = data['cluster']

  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20)

  # fill NaN annd infinite
  X_train = X_train.replace((np.inf, -np.inf, np.nan), 0).reset_index(drop=True)
  X_test = X_test.replace((np.inf, -np.inf, np.nan), 0).reset_index(drop=True)

  classifier = DecisionTreeClassifier(max_depth=5)
  classifier.fit(X_train, y_train)

  y_pred = classifier.predict(X_test)

  # validation
  print(confusion_matrix(y_test, y_pred))
  print(classification_report(y_test, y_pred))

  # visualization
  export_graphviz(
        classifier,
        out_file=""tree.dot"",
        feature_names = classifier.feature_names_in_,
        class_names=[""0"", ""1"", ""2"", ""3""],
        rounded=True,
        filled=True
    )
  
  view = Source.from_file(""tree.dot"")
  view.render('tree', format='jpg',view=True)
  view.view()

",scikit-learn
sklearn standardscaler doesn39t allow direct transform we need to fit_transform,"What is the difference between fit_transform and transform?
Why doesn't transform directly works?
from sklearn.preprocessing import StandardScaler

 X_scaler = StandardScaler()
 X_train = X_scaler.fit_transform(X_train)
 X_test = X_scaler.transform(X_test)

If directly transformed it gives the below error

NotFittedError: This StandardScaler instance is not fitted yet. Call
  'fit' with appropriate arguments before using this method.

","StandardScaler, as per documentation:

Standardize features by removing the mean and scaling to unit variance

So it needs to somehow first know about the mean and variance of your data.
So fit() or fit_transform() is needed so that StandardScaler can go through all of your data to find the mean and variance. Those can be accessed 
by attributes:

mean_ : The mean value for each feature (column) in the training set.
var_ : The variance for each feature in the training set. 

Note that those will be calculated separately for each column in the data. 
In transform(), it will just use those mean and variance values to scale the data.
Now you might say that why just it don't calculate those attributes during transform(). This is done so that the test data is scaled in the same way as a training data is scaled (from fit_transform()). If you calculate mean and variance of data in each call to transform(), then all passed data will have different scale, which is not what you want.
This is true for all scikit transformers. 
1) fit() - Will only go through the data and save all needed attributes of data
2) transform() -  Use the saved attributes from fit() to change the data
3) fit_transform() - Utility function to fit() and then transform() the same data.
Usually you would call fit_transform() on training data, and only transform() on test data.
",scikit-learn
how to cluster parts of a mask in an image using python,"I need to split a mask in such a way that if there is an inconsistency within the mask, it gets separated. For example, if I draw a mask on a cat, I want the wide part (the body) to be one mask and the narrow part (the tail) to be another.
Currently, I have a continuous mask that includes both the cat's body and its tail. I want to separate this into two distinct masks. How can I achieve this using Python?
original mask

desired mask

I looked into using methods described in this, which focuses on polygon partitioning and separating contours into multiple triangles. However, this approach does not suit my needs, as I want to split the mask based on size and shape rather than creating multiple triangular partitions.
","You can use Convexity defects to identify the points to ""cut"" between.
It is done like this here for example.
import cv2
import matplotlib.pyplot as plt
import numpy as np

def split_mask(mask):
    _, thresh = cv2.threshold(mask , 120,255, cv2.THRESH_BINARY)
    contours, _ = cv2.findContours(thresh, 2, 1)

    for contour in contours:
        if  cv2.contourArea(contour) > 20:
            hull = cv2.convexHull(contour, returnPoints = False)
            defects = cv2.convexityDefects(contour, hull)
            if defects is None:
                continue
            
            # Gather all defect points to filter them.
            dd = [e[0][3]/256 for e in defects]
            points = []
            for i in range(len(dd)):
                _,_,f,_ = defects[i,0]
                if dd[i] > 1.0 and dd[i]/np.max(dd) > 0.2:
                    points.append(f)

            # If there is defect points, erase the mask closest points.
            if len(points) >= 2:
                for i, f1 in enumerate(points):
                    p1 = tuple(contour[f1][0])
                    nearest = min((tuple(contour[f2][0]) for j, f2 in enumerate(points) if i != j),
                                  key=lambda p2: (p1[0] - p2[0])**2 + (p1[1] - p2[1])**2)
                    cv2.line(thresh, p1, nearest, 0, 20)
    return thresh    

if __name__==""__main__"":
    mask = cv2.imread(""<path-to-your-image>"", cv2.IMREAD_GRAYSCALE)
    mask_splitted = split_mask(mask)
    plt.imshow(mask_splitted)
    plt.show()

This yield the following on your image:

",scikit-learn
kerasregressor coefficient of determination r2 score,"I'm building a small neural net in Keras meant for a regression task, and I want to use the same accuracy metric as the scikit-learn RandomForestRegressor:

The coefficient R^2 is defined as (1 - u/v), where u is the regression sum of squares ((y_true - y_pred) ** 2).sum() and v is the residual sum of squares ((y_true - y_true.mean()) ** 2).sum().

It's a handy metric because it shows values up to 1.0 (similar to percent accuracy in classification). Is my usage of Keras backend correct for the accuracy metric I want?
def create_model():
    model = Sequential()
    model.add(Dense(10, input_dim=X.shape[1], activation=""relu""))
    model.add(Dense(10, activation=""relu""))
    model.add(Dense(1))

    # Compile model
    model.compile(loss=""mean_squared_error"", optimizer=""adam"", metrics=[det_coeff])
    return model

# Is this computing the right thing?
def det_coeff(y_true, y_pred):
    u = K.sum(K.square(y_true - y_pred))
    v = K.sum(K.square(y_true - K.mean(y_true)))
    return K.ones_like(v) - (u / v)

This appears to work in that nothing errors and the metric is increasing towards 1 over time, but I want to be sure I implemented the metric correctly.
","you can check this post out. I tested the following code and it works ok for your purpose.
from keras import backend as K

def coeff_determination(y_true, y_pred):
    SS_res =  K.sum(K.square( y_true-y_pred ))
    SS_tot = K.sum(K.square( y_true - K.mean(y_true) ) )
    return ( 1 - SS_res/(SS_tot + K.epsilon()) )

",scikit-learn
sklearn pipelines  gridsearchcv  xgboost  learning curve,"I am new to sklearn & XGBoost.
I would like to use GridSearchCV to tune a XGBoost classifier. One of the checks that I would like to do is the graphical analysis of the loss from train and test. So far I have created the following code:
# Create a new instance of the classifier
xgbr =  xgb.XGBClassifier()
# Create a new pipeline with preprocessing steps and model (imballanced learn)
pipeline  = imb_pipeline([
                          ('preprocess', preprocess), # Encode and transform categorical variables
                          ('re-sample', samplers[0]), # re-samples data to ballanced state
                          ('scale', scalers[0]), # scales the data
                          ('model', xgbr), # models
                          ])

# Create parameter values for gridsearch - carefull, ""model__"" prepended defined in pipeline
params = { 
    'model__max_depth': [3, 4, 5, 6, 8, 10, 12, 15],
    'model__learning_rate': [0.001, 0.01, 0.1, 0.20, 0.25, 0.30],
    ""model__gamma"":[0, 0.25, 0.5, 0.75,1],
    'model__n_estimators': [100, 500, 1000],
    ""model__subsample"":[0.9],
    ""model__colsample_bytree"":[0.5],
    ""model__early_stopping_rounds"": [10], 
    ""model__random_state"": [random_state], 
    ""model__eval_metric"" : [""error""], 
    ""model__eval_set"" : [[(X_train, Y_train), (X_test,Y_test)]]
}

# Use GridSearchCV for all combinations
grid = GridSearchCV(
    estimator = pipeline,
    param_grid = params,
    scoring = 'roc_auc',
    n_jobs = -1,
    cv = 5,
    verbose = 3,
)

# Model fitting
grid.fit(X_train, Y_train)

I have create in params a key-value pair for eval_metric and eval_set:
My question is now, how to access those values and plot a curve of train and test loss (sorry I cannot post a figure here).
Another question: Are the values hand-over by eval_set also piped by the pipeline or do I have to create a separate pipeline for those?
I am using xgb.__version == 0.90, sklearn.__version__ == 1.0.2, python == 3.7.13 @ (google colab)
","I think you are misunderstanding how the grid search is coupled to the cross-validation. Here your training set will be partitioned in 5 (cv = 5) almost even chunks, for each value of the hyperparameter grid it will train on 4 and predict (+ eval) on on the last one, iterating on all possible splits. This will give a cv estimate of the error. But this is all happening inside your training data. After picking the hyperparameters giving the best (=smallest) error, you want to evaluate this model (trained on the entire training set) on the evaluation data (held out and totally new data to the model and hyperparameters. This will give you the a reliable estimate of the generalization error of your model.
Also if you use
    ""model__eval_set"" : [[(X_train, Y_train), (X_test,Y_test)]]

the grid will assume that this is a binary parameter to optimize on, and you do not want that.
All in all you are looking for something of the like:

params = { 
    'model__max_depth': [3, 4, 5, 6, 8, 10, 12, 15],
    'model__learning_rate': [0.001, 0.01, 0.1, 0.20, 0.25, 0.30],
    ""model__gamma"":[0, 0.25, 0.5, 0.75,1],
    'model__n_estimators': [100, 500, 1000],
    ""model__subsample"":[0.9],
    ""model__colsample_bytree"":[0.5],
    ""model__early_stopping_rounds"": [10], 
    ""model__random_state"": [random_state], 
}

grid = GridSearchCV(
    estimator = pipeline,
    param_grid = params,
    scoring = 'roc_auc',
    n_jobs = -1,
    cv = 5,
    verbose = 3,
)

# Model fitting
grid = grid.fit(X_train, Y_train, eval_set=[(X_test, Y_test)])

eval_auc = sklearn.metrics.roc_auc_score( Y_test, grid.best_estimator_.predict_proba(X_test)[:,1])

",scikit-learn
display all parameters of an estimator including defaults,"I am using Watson Studio and using a markdown notebook.  In the notebook, I write the code:
from sklearn.tree import DecisionTreeClassifier
Tree_Loan= DecisionTreeClassifier(criterion=""entropy"", max_depth = 4)
Tree_Loan

and it displays
DecisionTreeClassifier(criterion='entropy', max_depth=4)

However, it should display something in the form of (this is from a different lab I've done using Skills Network Labs):
DecisionTreeClassifier(class_weight=None, criterion='entropy', max_depth=4,
            max_features=None, max_leaf_nodes=None,
            min_impurity_decrease=0.0, min_impurity_split=None,
            min_samples_leaf=1, min_samples_split=2,
            min_weight_fraction_leaf=0.0, presort=False, random_state=None,
            splitter='best')

The best I can tell is that it is not importing the decision tree classifier.  I have the same problem with svm from sklearn.  Other functions in scikit-learn like train test split and k nearest neighbors work fine.  A classmate says the rest of my code is correct and there is no reason for the error.  What might be causing it?
","It is importing the DecisionTreeClassifier, no problem there. But by default, sklearn prints only the parameters that were given to estimator with non-default values.
For the ""full"" output, you can set the configuration of print_changed_only to False via sklearn.set_config like so:
# Before
>>> tree
DecisionTreeClassifier(criterion='entropy', max_depth=4)

# After
>>> import sklearn
>>> sklearn.set_config(print_changed_only=False)
>>> tree
DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='entropy',
                       max_depth=4, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_impurity_split=None,
                       min_samples_leaf=1, min_samples_split=2,
                       min_weight_fraction_leaf=0.0, presort='deprecated',
                       random_state=None, splitter='best')

If you want to do this temporarily, there is a context manager variant:
>>> with sklearn.config_context(print_changed_only=False):
...     print(tree)
...
DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='entropy',
                       max_depth=4, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       random_state=None, splitter='best')
>>> tree
DecisionTreeClassifier(criterion='entropy', max_depth=4)

",scikit-learn
gridsearchcv not choosing the best hyperparameters for xgboost,"I am developing a regression model with xgboost. Since xgboost has multiple hyperparameters, I have added the cross validation logic with GridSearchCV(). As a trial, I set max_depth: [2,3]. My python code is as below.
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import make_scorer
from sklearn.metrics import mean_squared_error
​
xgb_reg = xgb.XGBRegressor()
​
# Obtain the best hyper parameter
scorer=make_scorer(mean_squared_error, False)
params = {'max_depth': [2,3], 
          'eta': [0.1], 
          'colsample_bytree': [1.0],
          'colsample_bylevel': [0.3],
          'subsample': [0.9],
          'gamma': [0],
          'lambda': [1],
          'alpha':[0],
          'min_child_weight':[1]
         }
grid_xgb_reg=GridSearchCV(xgb_reg,
                          param_grid=params,
                          scoring=scorer,
                          cv=5,
                          n_jobs=-1)
​
grid_xgb_reg.fit(X_train, y_train)
y_pred = grid_xgb_reg.predict(X_test)
y_train_pred = grid_xgb_reg.predict(X_train)

## Evaluate model
from sklearn.metrics import mean_squared_error
from sklearn.metrics import r2_score
​
print('RMSE  train: %.3f,  test: %.3f' %(np.sqrt(mean_squared_error(y_train, y_train_pred)),np.sqrt(mean_squared_error(y_test, y_pred))))
print('R^2   train: %.3f,  test: %.3f' %(r2_score(y_train, y_train_pred),r2_score(y_test, y_pred)))

The problem is the GridSearchCV does not seem to choose the best hyperparameters. In my case, when I set max_depth as [2,3], The result is as follows. In the following case, GridSearchCV chose max_depth:2 as the best hyper params.
#  The result when max_depth is 2
RMSE  train: 11.861,  test: 15.113
R^2   train: 0.817,  test: 0.601

However, if I updated max_depth to [3](by getting rid of 2), the test score is better than the previous value as follows.
#  The result when max_depth is 3
RMSE  train: 9.951,  test: 14.752
R^2   train: 0.871,  test: 0.620

Question
My understanding is that even if I set max_depth as [2,3], the GridSearchCV method SHOULD choose the max_depth:3 as the best hyperparameters since max_depth:3 can return the better score in terms of RSME or R^2 than max_depth:2. Could anyone tell me why my code cannot choose the best hyperparameters when I set max_depth as [2,3]?
","If you run a second experiment with max_depth:2, then the results are not comparable to the first experiment with max_depth:[2,3] even for the run with max_depth:2, since there are sources of randomness in your code which you do not explicitly control, i.e. your code is not reproducible.
The first source of randomness is the CV folds; in order to ensure that the experiments will be run on identical splits of the data, you should define your GridSearchCV as follows:
from sklearn.model_selection import KFold

seed_cv = 123 # any random value here

kf = KFold(n_splits=5, random_state=seed_cv)

grid_xgb_reg=GridSearchCV(xgb_reg,
                          param_grid=params,
                          scoring=scorer,
                          cv=kf,   # <- change here
                          n_jobs=-1)

The second source of randomness is the XGBRegressor itself, which also includes a random_state argument (see the docs); you should change it to:
seed_xgb = 456 # any random value here (can even be the same with seed_cv)
xgb_reg = xgb.XGBRegressor(random_state=seed_xgb)

But even with these arrangements, while your data splits will now be identical, the regression models built will not be necessarily so in the general case; here, if you keep the experiments like that, i.e. first with  max_depth:[2,3] and then with max_depth:2, the results will be identical indeed; but if you change it to, say,  first with  max_depth:[2,3] and then with max_depth:3, they will not, since in the first experiment, the run with max_depth:3 will start with a different state of the random number generator (i.e. the one after the run with max_depth:2 has finished).
There are limits to how identical you can make different runs under such conditions; for an example of a very subtle difference that nevertheless destroys the exact reproducibility between two experiments, see my answer in Why does the importance parameter influence performance of Random Forest in R?
",scikit-learn
obtain eigen values and vectors from sklearn pca,"How I can get the the eigen values and eigen vectors of the PCA application? 
from sklearn.decomposition import PCA
clf=PCA(0.98,whiten=True)      #converse 98% variance
X_train=clf.fit_transform(X_train)
X_test=clf.transform(X_test)

I can't find it in docs.
1.I am ""not"" able to comprehend the different results here.
Edit:
def pca_code(data):
    #raw_implementation
    var_per=.98
    data-=np.mean(data, axis=0)
    data/=np.std(data, axis=0)
    cov_mat=np.cov(data, rowvar=False)
    evals, evecs = np.linalg.eigh(cov_mat)
    idx = np.argsort(evals)[::-1]
    evecs = evecs[:,idx]
    evals = evals[idx]
    variance_retained=np.cumsum(evals)/np.sum(evals)
    index=np.argmax(variance_retained>=var_per)
    evecs = evecs[:,:index+1]
    reduced_data=np.dot(evecs.T, data.T).T
    print(evals)
    print(""_""*30)
    print(evecs)
    print(""_""*30)
    #using scipy package
    clf=PCA(var_per)
    X_train=data.T
    X_train=clf.fit_transform(X_train)
    print(clf.explained_variance_)
    print(""_""*30)
    print(clf.components_)
    print(""__""*30)


I wish to obtain all the eigenvalues and eigenvectors instead of just the reduced set with the convergence condition.

","Your implementation
You are computing the eigenvectors of the correlation matrix, that is the covariance matrix of the normalized variables.
data/=np.std(data, axis=0) is not part of the classic PCA, we only center the variables.
So the sklearn PCA does not feature scale the data beforehand.
Apart from that you are on the right track, if we abstract the fact that the code you provided did not run ;).
You only got confused with the row/column layouts. Honestly I think it's much easier to start with X = data.T and work only with X from there on.  I added your code 'fixed' at the end of the post.
Getting the eigenvalues
You already noted that you can get the eigenvectors using clf.components_.
So you have the principal components. They are eigenvectors of the covariance matrix 𝑋ᵀ𝑋.
A way to retrieve the eigenvalues from there is to apply this matrix to each principal components and project the results onto the component.
Let v_1  be the first principal component and lambda_1 the associated eigenvalue. We have:
 and thus:
 since . (x, y)  the scalar product of vectors x and y.
Back in Python you can do:
n_samples = X.shape[0]
# We center the data and compute the sample covariance matrix.
X -= np.mean(X, axis=0)
cov_matrix = np.dot(X.T, X) / n_samples
for eigenvector in pca.components_:
    print(np.dot(eigenvector.T, np.dot(cov_matrix, eigenvector)))

And you get the eigenvalue associated with the eigenvector.
Well, in my tests it turned out not to work with the couple last eigenvalues but I'd attribute that to my absence of skills in numerical stability.
Now that's not the best way to get the eigenvalues but it's nice to know where they come from.
The eigenvalues represent the variance in the direction of the eigenvector. So you can get them through the pca.explained_variance_ attribute:
eigenvalues = pca.explained_variance_

Here is a reproducible example that prints the eigenvalues you get with each method:
import numpy as np
from sklearn.decomposition import PCA
from sklearn.datasets import make_classification


X, y = make_classification(n_samples=1000)
n_samples = X.shape[0]

pca = PCA()
X_transformed = pca.fit_transform(X)

# We center the data and compute the sample covariance matrix.
X_centered = X - np.mean(X, axis=0)
cov_matrix = np.dot(X_centered.T, X_centered) / n_samples
eigenvalues = pca.explained_variance_
for eigenvalue, eigenvector in zip(eigenvalues, pca.components_):    
    print(np.dot(eigenvector.T, np.dot(cov_matrix, eigenvector)))
    print(eigenvalue)

Your original code, fixed
If you run it you'll see the values are consistent. They're not exactly equal because numpy and scikit-learn are not using the same algorithm here.
The main thing was that you were using correlation matrix instead of covariance, as mentioned above. Also you were getting the transposed eigenvectors from numpy which made it very confusing.
import numpy as np
from scipy.stats.mstats import zscore
from sklearn.decomposition import PCA

def pca_code(data):
    #raw_implementation
    var_per=.98
    data-=np.mean(data, axis=0)
    # data/=np.std(data, axis=0)
    cov_mat=np.cov(data, rowvar=False)
    evals, evecs = np.linalg.eigh(cov_mat)
    idx = np.argsort(evals)[::-1]
    evecs = evecs[:,idx]
    evals = evals[idx]
    variance_retained=np.cumsum(evals)/np.sum(evals)
    index=np.argmax(variance_retained>=var_per)
    evecs = evecs[:,:index+1]
    reduced_data=np.dot(evecs.T, data.T).T
    print(""evals"", evals)
    print(""_""*30)
    print(evecs.T[1, :])
    print(""_""*30)
    #using scipy package
    clf=PCA(var_per)
    X_train=data
    X_train=clf.fit_transform(X_train)
    print(clf.explained_variance_)
    print(""_""*30)
    print(clf.components_[1,:])
    print(""__""*30)

",scikit-learn
how to use sklearn when target variable is a proportion,"There are standard ways of predicting proportions such as logistic regression (without thresholding) and beta regression. There have already been discussions about this:
http://scikit-learn-general.narkive.com/4dSCktaM/using-logistic-regression-on-a-continuous-target-variable
http://scikit-learn-general.narkive.com/lLVQGzyl/beta-regression
I cannot tell if there exists a work-around within the sklearn framework.
","There exists a workaround, but it is not intrinsically within the sklearn framework.
If you have a proportional target variable (value range 0-1) you run into two basic difficulties with scikit-learn:

Classifiers (such as logistic regression) deal with class labels as target variables only. As a workaround you could simply threshold your probabilities to 0/1 and interpret them as class labels, but you would lose a lot of information.
Regression models (such as linear regression) do not restrict the target variable. You can train them on proportional data, but there is no guarantee that the output on unseen data will be restricted to the 0/1 range. However, in this situation, there is a powerful work-around (below).

There are different ways to mathematically formulate logistic regression. One of them is the generalized linear model, which basically defines the logistic regression as a normal linear regression on logit-transformed probabilities. Normally, this approach requires sophisticated mathematical optimization because the probabilities are unknown and need to be estimated along with the regression coefficients.
In your case, however, the probabilities are known. This means you can simply transform them with y = log(p / (1 - p)). Now they cover the full range from -oo to oo and can serve as the target variable for a LinearRegression model [*]. Of course, the model output then needs to be transformed again to result in probabilities p = 1 / (exp(-y) + 1).
import numpy as np
from sklearn.linear_model import LinearRegression


class LogitRegression(LinearRegression):

    def fit(self, x, p):
        p = np.asarray(p)
        y = np.log(p / (1 - p))
        return super().fit(x, y)

    def predict(self, x):
        y = super().predict(x)
        return 1 / (np.exp(-y) + 1)


if __name__ == '__main__':
    # generate example data
    np.random.seed(42)
    n = 100
    x = np.random.randn(n).reshape(-1, 1)
    noise = 0.1 * np.random.randn(n).reshape(-1, 1)
    p = np.tanh(x + noise) / 2 + 0.5

    model = LogitRegression()
    model.fit(x, p)

    print(model.predict([[-10], [0.0], [1]]))
    # [[  2.06115362e-09]
    #  [  5.00000000e-01]
    #  [  8.80797078e-01]]


There are also numerous other alternatives. Some non-linear regression models can work naturally in the 0-1 range. For example Random Forest Regressors will never exceed the target variables' range they were trained with. Simply put probabilities in and you will get probabilities out. Neural networks with appropriate output activation functions (tanh, I guess) will also work well with probabilities, but if you want to use those there are more specialized libraries than sklearn.

[*] You could in fact plug in any linear regression model which can make the method more powerful, but then it no longer is exactly equivalent to logistic regression.
",scikit-learn
leave one out encoding on test set with transform,"Context: When preprocessing a data set using sklearn, you use fit_transform on the training set and transform on the test set, to avoid data leakage. Using leave one out (LOO) encoding, you need the target variable value to calculate the encoded value of a feature value. When using the LOO encoder in a pipeline, you can apply it to the training set using the fit_transform function, which accepts the features (X) and the target values (y).
How do I calculate the LOO encodings for the test set with the same pipeline, knowing that transform does not accept the target variable values as an argument? I'm quite confused about this. The transform function indeed transforms the columns but without considering the value of the target, since it doesn't have that information.
","You shouldn't need the target variable of the test set while applying leave-one-out (or any other) encoding. Even if you somehow managed to pass it when you do your offline evaluations on the test set, how will you actually apply
it during inference time? During inference time when your model is serving traffic from real users, obviously the true label wouldn't be available. And you should always compute your test metrics such that they are representative of what happens in the real world. So conceptually, it seems wrong to use the test labels to do feature encoding.
I looked up the source code of leave-one-out encoding in the category_encoders package and it's apparent that they find the mean target without leaving the current example out when the target variable is not supplied
# Replace level with its mean target; if level occurs only once, use global mean
level_means = (colmap['sum'] / colmap['count']).where(level_notunique, self._mean)

So if I would just use the encoder like this
import category_encoders as ce
from sklearn.model_selection import train_test_split
import pandas as pd

dataframe = pd.DataFrame({
    'f1': ['P', 'Q', 'P', 'Q', 'P', 'P', 'Q', 'Q'],
    'f2': ['M', 'N', 'M', 'N', 'M', 'N', 'M', 'N'],
    'f3': ['A', 'B', 'C', 'C', 'C', 'C', 'A', 'C'],
    'y': [1, 0, 1, 0, 1, 1, 0, 0]
})

train_data, test_data = train_test_split(dataframe, test_size=0.2)

encoder = ce.LeaveOneOutEncoder(cols=['f1', 'f2', 'f3'])

encoded_train = encoder.fit_transform(train_data, train_data['y'])
encoded_test = encoder.transform(test_data)

",scikit-learn
getting counts in multilabelbinarizer,"How can I get counts of items in MultiLabelBinarizer?
import pandas as pd
from sklearn.preprocessing import MultiLabelBinarizer
mlb = MultiLabelBinarizer()

pd.DataFrame(mlb.fit_transform([(1,1,2), (3,3,2,5)]),columns=mlb.classes_)

Out[0]: 
   1  2  3  5
0  1  1  0  0
1  0  1  1  1

Instead of this, I want to get
Out[0]: 
   1  2  3  5
0  2  1  0  0
1  0  1  2  1

As 1 is repeated 2 times in row 1 and 3 is repeated 2 times in row 2
","from collections import Counter

data = [(1,1,2), (3,3,2,5)]
pd.DataFrame([Counter(x) for x in data]).fillna(0)

Output:
    1       2   3       5
0   2.0     1   0.0     0.0
1   0.0     1   2.0     1.0

",scikit-learn
what is the default accuracy scoring in cross_val_score in sklearn,"I have a regression model made using random-Forest. I made pipelines using scikit-learn to process data and now have used RandomForestRegressor to predict.
I want to get the accuracy of model. because of the problem of over-fitting I decided to use the cross_val_score function to get rid of that.
from sklearn.ensemble import RandomForestRegressor
forest_reg = make_pipeline(preprocessing,
RandomForestRegressor(random_state=1))
acc = cross_val_score(forest_reg, data, labels,cv=10)

then, I used this to get the accuracy:
print(acc.mean(),acc.std())

It gives me around 0.84 and 0.06.
I understand the standard deviation part but how is the first one calculated? Is 0.84 good? Is there a better scoring way to get accuracy?
","Firt, keep in mind that accuracy is typically used for classification tasks. Not for regression.
The documentation says:

scores:  ndarray of float of shape=(len(list(cv)),)
Array of scores of the estimator for each run of the cross validation.

You have set the cv parameter to 10. It means that acc is an array of 10 scores.
But here you don't have the accuracy of each run. Instead you have the coefficient of determination of the random forest prediction:
Again the cross_val_score documentation says:

scoring: str or callable, default=None
A str (see model evaluation
documentation) or a scorer callable object / function with signature
scorer(estimator, X, y) which should return only a single value.
Similar to cross_validate but only a single metric is permitted.
If None, the estimator’s default scorer (if available) is used.

And the default scorer of RandomForestRegressor is R²:

score(X, y, sample_weight=None)
Return the coefficient of determination of the prediction.
The coefficient of determination   is defined as    , where   is the
residual sum of squares ((y_true - y_pred)** 2).sum() and   is the
total sum of squares ((y_true - y_true.mean()) ** 2).sum(). The best
possible score is 1.0 and it can be negative (because the model can be
arbitrarily worse). A constant model that always predicts the expected
value of y, disregarding the input features, would get a   score of
0.0.

Based on your question, you have an R² value with a mean of 0.86 and a standard deviation of 0.06 over 10 runs. I cannot determine if this is good or not; only you can decide if it is acceptable.
",scikit-learn
importerror cannnot import name 39imputer39 from 39sklearnpreprocessing39,"Trying to import Imputer from sklearn.preprocessing,
from sklearn.preprocessing import Imputer

Shows
ImportError: cannot import name 'Imputer' from 'sklearn.preprocessing' (/home/codeknight13/anaconda3/lib/python3.7/site-packages/sklearn/preprocessing/__init_\_.py)""

","from sklearn.preprocessing import Imputer was deprecated with scikit-learn v0.20.4 and removed as of v0.22.2. See the sklean changelog.
from sklearn.impute import SimpleImputer
import numpy as np

imputer = SimpleImputer(missing_values=np.nan, strategy='mean')

pip install scikit-learn==0.20.4 or conda install scikit-learn=0.20.4 are not a good options because scikit-learn==0.20.4 is more than 3 years out of date.
",scikit-learn
scikitlearn clustering predictx vs fit_predictx,"In scikit-learn, some clustering algorithms have both predict(X) and fit_predict(X) methods, like KMeans and MeanShift, while others only have the latter, like SpectralClustering. According to the doc:
fit_predict(X[, y]):    Performs clustering on X and returns cluster labels.
predict(X): Predict the closest cluster each sample in X belongs to.

I don't really understand the difference between the two, they seem equivalent to me.
","In order to use the 'predict' you must use the 'fit' method first. So using 'fit()' and then 'predict()' is definitely the same as using 'fit_predict()'. However, one could benefit from using only 'fit()' in such cases where you need to know the initialization parameters of your models rather than if you use 'fit_predict()', where you will just be obtained the labeling results of running your model on the data.
",scikit-learn
knnimputer drops columns despite of numeric datatypes and right shape,"I am using KNNImputer to impute np.nan values in several pd.DataFrame. I checked that all the datatypes of each one of the dataframes are numeric. However, KNNImputer drops some columns in some dataframes:
>>>input_df.shape   
(816, 216) 

>>> input_df.dtypes.value_count()
float64    216
dtype: int64

>>output_df.shape 
(816, 27)

I used the following KNNImputer configuration
imputer = KNNImputer(n_neighbors=1, 
                     weights=""uniform"",
                     add_indicator=False)

output_df = imputer.fit_transform(input_df)

I would like to know why it is happening since each one of the dataframes have np.nan values. By the way, the parameter n_neighbors=1 should not have any impact in the outcome since I am replacing missing values with the values of the closest neighbor.
","I think in your data there could be some columns where there are only np.nan or empty features for all rows that can cause KNNImputer to drop that column in the output
>>> import numpy as np
>>> import pandas as pd
>>> from sklearn.impute import KNNImputer
>>> 
>>> imputer = KNNImputer(n_neighbors=1, 
...                      weights=""uniform"",
...                      add_indicator=False)
>>> 
>>> df = pd.DataFrame([[1.69, 2.69, np.nan], [3.69, 4.69, 3.69, np.nan], [np.nan, 6.69, 5.69, np.nan], [8.69, 8.69, 7.69, np.nan]])
>>> print(df)
      0     1     2   3
0  1.69  2.69   NaN NaN
1  3.69  4.69  3.69 NaN
2   NaN  6.69  5.69 NaN
3  8.69  8.69  7.69 NaN
>>> print(df.shape)
(4, 4)
>>> print(df.dtypes.value_counts())
float64    4
Name: count, dtype: int64
>>> 
>>> output_df = imputer.fit_transform(df)
>>> print(output_df.shape)
(4, 3)

I think you can avoid this by setting keep_empty_features  param to True instead of default False to avoid removing columns
>>> import numpy as np
>>> import pandas as pd
>>> from sklearn.impute import KNNImputer
>>> 
>>> imputer = KNNImputer(n_neighbors=1, 
...                      weights=""uniform"",
...                      keep_empty_features=True,
...                      add_indicator=False)
>>> 
>>> df = pd.DataFrame([[1.69, 2.69, np.nan], [3.69, 4.69, 3.69, np.nan], [np.nan, 6.69, 5.69, np.nan], [8.69, 8.69, 7.69, np.nan]])
>>> print(df)
      0     1     2   3
0  1.69  2.69   NaN NaN
1  3.69  4.69  3.69 NaN
2   NaN  6.69  5.69 NaN
3  8.69  8.69  7.69 NaN
>>> print(df.shape)
(4, 4)
>>> print(df.dtypes.value_counts())
float64    4
Name: count, dtype: int64
>>> 
>>> output_df = imputer.fit_transform(df)
>>> print(output_df.shape)
(4, 4)

",scikit-learn
sklearn classifier39s predict_proba doesn39t sum to 1,"I have a classifier (in this case, it is the sklearn.MLPClassifier), with which I'm trying to perform classification into one of 18 classes.
The class is thus multi-class, not multi-label. I'm trying to predict only a single class.
I have my training data:  X X.shape = (103393, 300)  and Y Y.shape = (103393, 18), where the target Y is a one-hot encoded vetctor, denoting the target class.

EDIT in response to @Dr. Snoopy: I do not supply any labels -- I simply pass the 18-dimensional vector with the corret class' index corresponding to the 1 in the vector, and all others being 0 (One hot encoded vector).
To prove that the vectors are correctly 1-hot encoded, I can run

import pandas as pd
pd.DataFrame(Y.sum(axis=1)).value_counts()


This returns 103393 counts of 1. Vectors are correctly 1-hot encoded, even upon examination.

When I fit the model, and return the class probability for all classes, the probability vector does not sum up to 1. Why might that be?
Here is an example of how I run the fitting:
from sklearn.neural_network import MLPClassifier

X_train, Y_train, X_test, Y_test = get_data()

model = MLPClassifier(max_iter=10000)
model.fit(X_train,Y_train)
probability_vector = model.predict_proba(X_test[0, :].respahe(1,-1))

Some of the time, the outputs are pretty close to 1. I suspect the error is probably due to rounding.
In other cases, the outputs sum to ~0.5 or less. Example output:
probability_vector = list(model.predict_proba(X_test[301,:].reshape(1,-1))[0])
print(probability_vector)
>>> [1.7591416e-06,
 3.148203e-05,
 3.9732524e-05,
 0.3810972,
 0.059248358,
 0.00032832936,
 8.5996935e-06,
 9.0914684e-05,
 9.377927e-07,
 0.0007674346,
 1.5543707e-06,
 0.0008467222,
 0.009655427,
 2.5728454e-05,
 1.07812774e-07,
 0.00022920035,
 0.00050288404,
 0.013878004]

len(probability_vecto)

>>> 18

sum(probability_vector)
>>> 0.46675437349917814



Why might this be happening? Is my model initialized incorrectly?

Note: A couple of possible reasons for the error & my comments on them:

Class imbalance: The classes in the dataset are indeed, imbalanced. However, the non-1 summation problem is happening in well represented classes too, not just the underrepresented ones. Could this be a consequence of a model, which is not expressive enough?

Model uncertainty: ""The model may not have a high level of confidence in its predictions for every input. "" Is that all it is?



","Do not one-hot encode your labels Y. If your labels have multiple dimensions it will do multi-label classification.
Just pass it as it is, the MLP classifier will do the encoding for you using LabelBinarizer and then it will apply the softmax function correctly. You find some more explanation in the docs.

You can check this for example by accessing model.out_activation_ or LabelBinarizer().fit(Y).y_type_ which should be ""softmax/multiclass"", but here it will be ""logistic/multilabel-indicator""
What you get at the moment are the logistic outputs of the individual classes.
",scikit-learn
how to normalize a confusion matrix,"I calculated a confusion matrix for my classifier using confusion_matrix() from scikit-learn. The diagonal elements of the confusion matrix represent the number of points for which the predicted label is equal to the true label, while off-diagonal elements are those that are mislabeled by the classifier.
I would like to normalize my confusion matrix so that it contains only numbers between 0 and 1. I would like to read the percentage of correctly classified samples from the matrix.
I found several methods how to normalize a matrix (row and column normalization) but I don't know much about maths and am not sure if this is the correct approach.
","I'm assuming that M[i,j] stands for Element of real class i was classified as j. If its the other way around you are going to need to transpose everything I say. I'm also going to use the following matrix for concrete examples:
1 2 3
4 5 6
7 8 9

There are essentially two things you can do:
Finding how each class has been classified
The first thing you can ask is what percentage of elements of real class i here classified as each class. To do so, we take a row fixing the i and divide each element by the sum of the elements in the row. In our example, objects from class 2 are classified as class 1 4 times, are classified correctly as class 2 5 times and are classified as class 3 6 times. To find the percentages we just divide everything by the sum 4 + 5 + 6 = 15
4/15 of the class 2 objects are classified as class 1
5/15 of the class 2 objects are classified as class 2
6/15 of the class 2 objects are classified as class 3

Finding what classes are responsible for each classification
The second thing you can do is to look at each result from your classifier and ask how many of those results originate from each real class. Its going to be similar to the other case but with columns instead of rows. In our example, our classifier returns ""1"" 1 time when the original class is 1, 4 times when the original class is 2 and 7 times when the original class is 3. To find the percentages we divide by the sum 1 + 4 + 7 = 12
1/12 of the objects classified as class 1 were from class 1
4/12 of the objects classified as class 1 were from class 2
7/12 of the objects classified as class 1 were from class 3

--
Of course, both the methods I gave only apply to single row column at a time and I'm not sure if it would be a good idea to actually modify your confusion matrix in this form. However, this should give the percentages you are looking for.
",scikit-learn
pca in python reproducing pcafit_transform results using pcafit,"I have a data frame called data_principal_components with dimensions (306x21154), so 306 observations and 21154 features. Using PCA, I want to project the data into 10 dimensions.
As far as I understand, the following code does this. The resulting matrix (projected) has a dimension of (306x10).
import numpy as np
import pandas as pd
from sklearn.decomposition import PCA

# Sample data:
# Define the dimensions of the DataFrame
num_rows = 306
num_cols = 21154

# Generate random numbers from a normal distribution
data = np.random.randn(num_rows, num_cols)

# Create a DataFrame from the random data
data_principal_components = pd.DataFrame(data)

pca = PCA(10)  
projected = pca.fit_transform(data_principal_components)
```

To better understand how the code works, I wanted to reproduce the result of pca.fit_transform() manually.
Based on my research, I found the following steps:
pc_components = pca.components_  # This gives the eigenvectors
pc_components = pc_components.transpose()  # Transposes the eigenvectors, so it has the dimensions (21154x10)
eigenvalues = pca.explained_variance_  # These are the eigenvalues with dimensions (1x10)

Now, as I understand, one can calculate the loadings using the following code based on the formula $\text{loadings} = \text{eigenvectors} \times \sqrt{\text{eigenvalues}}$ :
# Create an empty DataFrame
df = pd.DataFrame()

# Iterate over eigenvalues
for i in range(len(eigenvalues)):
    result = np.dot(pc_components[:, i], np.sqrt(eigenvalues[i]))
    df[f'Result_{i+1}'] = result  # Assign result as a new column in the DataFrame
    
loadings = df
```

After obtaining the loadings with dimensions (21154x10), I wanted to use them to obtain the projected values with $ \text{Actual values} \times \text{loadings}$ resulting in dimensions (306x21154) $\times$ (21154x10) = (306x10):
test = np.dot(data_principal_components, loadings)

However, when I compare test to projected, the values differ substantially. Where am I wrong?
EDIT
I found this way to extract the loadings. However, I still want to derive them semi-manually, can somone help?:
pca = PCA(10)  # project from 64 to 2 dimensions
projected = pca.fit_transform(data_principal_components)

loadings = pd.DataFrame(pca.components_.T, columns=['PC1', 'PC2','PC3', 'PC4','PC5', 'PC6','PC7', 'PC8','PC9', 'PC10'], index=data_principal_components.columns)
loadings

","You ask for two different things here:

I wanted to reproduce the result of pca.fit_transform()

When you ask about the result of pca.fit_transform(), you're asking about the principal component (PC) scores.

I found this way to extract the loadings. However, I still want to
derive them semi-manually, can somone help?

Here you ask about the loadings.
In a nutshell, using singular value decomposition, you can decompose your data matrix as X = USV.T where

U -> normalized PC scores
S -> singular values which are directly linked to the eigenvalues
V -> eigenvectors (often called loadings)

So this allows you to answer your second question: You obtain the loadings by applying a SVD to your data matrix:
data = data - data.mean(0)  # don't forget to center your data
U, S, VT = np.linalg.svd(data)
loadings = VT.T  # your loadings

Now, the result of pca.fit_transform() is just the projection of your centered data onto these loadings:
PC_scores = xr.dot(data, loadings[:, :10)  # use the first 10 components only

",scikit-learn
how to add type hints to scikitlearn functions,"I have the following simple function:
def f1(y_true, y_pred):
    return {""f1"": 100 * sklearn.metrics.f1_score(y_true, y_pred)}

According to the scikit-learn documentation, the arguments to f1_score can have the following types:

y_true: 1d array-like, or label indicator array / sparse matrix
y_pred: 1d array-like, or label indicator array / sparse matrix

and the output is of type:

float or array of float, shape = [n_unique_labels]

How do I add type hints to this function so that mypy doesn't complain?
I tried variations of the following:
Array1D = NewType('Array1D', Union[np.ndarray, List[np.float64]])

def f1(y_true: Union[List[float], Array1D], y_pred: Union[List[float], Array1D]) -> Dict[str, Union[List[float], Array1D]]:
    return {""f1"": 100 * sklearn.metrics.f1_score(y_true, y_pred)}

but that gave errors.
","This is the approach I use to avoid similar mypy issues. It takes advantage of numpy typing introduced in 1.20. The ArrayLike type covers List[float], so no need to worry about covering it explicitly.
Running mypy v0.971 with numpy v1.23.1 on this shows no issues.
from typing import List, Dict
import numpy as np
import numpy.typing as npt
import sklearn.metrics


def f1(y_true: npt.ArrayLike, y_pred: npt.ArrayLike) -> Dict[str, npt.ArrayLike]:
    return {""f1"": 100 * sklearn.metrics.f1_score(y_true, y_pred)}

y_true_list: List[float] = [1, 0, 1, 0]
y_pred_list: List[float] = [1, 0, 1, 1]
y_true_np: npt.ArrayLike = np.array(y_true_list)
y_pred_np: npt.ArrayLike = np.array(y_pred_list)

assert f1(y_true_list, y_pred_list) == f1(y_true_np, y_pred_np)

",scikit-learn
python 3104 scikitlearn import hangs when executing via cpp,"Python 3.10.4 is embedded into cpp application.
I'm trying to import sklearn library which is installed at custom location using pip --target.
sklearn custom path (--target path) is appended to sys.path.
Below is a function from the script which just prints the version information.
Execution using Command Line works well as shown below.
python3.10 -c 'from try_sklearn import *; createandload()'

Output
[INFO ] [try_sklearn.py:23] 3.10.4 (main, Aug  4 2023, 01:24:50) [GCC 4.8.5 20150623 (Red Hat 4.8.5-16)]
[INFO ] [try_sklearn.py:24] sklearn /users/xxxx/temp/python/scikit-learn/sklearn/__init__.py Version = 1.5.1

The same script when called using CPP, hangs at
import sklearn

Other libraries like pandas, numpy etc works without any issues.
","https://github.com/scipy/scipy/issues/21189
Looks like Scipy and Numpy do not support Embedded python
",scikit-learn
scikitlearn39s feature_names_in method,"A number of scikit-learn's classes have a feature_names_in method, which would be a real time saver if I could understand it better.  Specifically, assume your X is a nested list of strings [['A', 'B', 'C'], ['A', 'B', 'D']] and your y is a list of labels ['Blue', 'Green'].  Now, assume you are doing feature selection using, for example, the selectKbest class in scikit.  Assume you choose the chi2 univariate approach and ask for the top 2 features (i.e., k=2) and you get your k_best_object.  Now, that k_best_object has a method associated with it called feature_names_in which would be really helpful if it returned the ""names"" of the top 2 features.  The problem is that the documentation says that this method is only available when the features are entirely strings.  That would be fine, except for the fact that I haven't been able to get selectKbest (or other scikit classes) to work on strings. Instead, I have only been able to get them to work by converting the X values into a numpy array of floats using TFIDVectorizer (either count or TF-IDF).  So, my question is... how would this method ever be used?  If it's only viable when all X input values are strings, but the only X it will take is floats, then how does this method ever apply?

To illustrate with code, if you try this:
X_t = [['Land','Building','Cat'],['Land','Building','Dog']]
y_t = ['Blue', 'Green']
chi_select_object_test = SelectKBest(chi2, k=100)
chi_select_object_test.fit(X_t,y_t)

It won't work because the data consists of strings, not numbers. You get this error: ValueError: dtype='numeric' is not compatible with arrays of bytes/strings.Convert your data to numeric values explicitly instead.
But, if you convert X_t to numbers using, for example, TFID Vectorizer(), the class will work:

X_t = ['Land Building Camp','Land Building Dog']
tfidvectorizer_t = TfidfVectorizer(analyzer='word',stop_words= 'english')
X_t = tfidvectorizer_t.fit_transform(X_t)
y_t = ['Blue', 'Green']
chi_select_object_test = SelectKBest(chi2, k=1)
chi_select_object_test.fit(X_t,y_t)

But, then, when you try and access the feature names attribute:
chi_select_object_test.feature_names_in_

You receive the error that: 'SelectKBest' object has no attribute 'feature_names_in_'
","I believe what you want to do is pass a pandas.Dataframe to SelectKBest. The Dataframe includes column names that then become the feature names. In the end you can get the best according to the metric you passed by using get_feature_names_out.
In my silly example I generate a Dataframe with 3 random columns and tell SelectKBest that I want to predict if the 3rd column is bigger than 0.5. Obviously we expect it to give us the 3rd column then.
import pandas as pd
import numpy as np
from sklearn.feature_selection import SelectKBest, chi2

df = pd.DataFrame(np.random.random((100,3)), columns=['a', 'b','c'])

selector = SelectKBest(chi2, k=1)
selector.fit(df, df['c'] > 0.5)

selector.get_feature_names_out()

and indeed it returns
array(['c'], dtype=object)

Finally feature_names_in is now set to array(['a', 'b', 'c'], dtype=object)  since those are the names of the features we put into the feature selector.
",scikit-learn
nearest neighbor for list of arrays,"`I have a list of arrays like this (in x, y coordinates):
coordinates= array([[ 300, 2300],
      [ 670, 2360],
      [ 400, 2300]]), array([[1500, 1960],
      [1620, 2200],
      [1505, 1975]]), array([[ 980, 1965],
      [1060, 2240],
      [1100, 2250],
      [ 980, 1975]]), array([[ 565, 1940],
      [ 680, 2180],
      [ 570, 1945]])]

I want the arrays to be sorted by the nearest neighbor. So, the last coordinate of the first array should be close to the first coordinate of the next array and so forth.
I extracted all of the first and last coordinates and put them in two lists using python numpy. Then tried to use sklearn.neighbors NearestNeighbors, but it didn't work.`
expected output:
coordinates= array([[ 300, 2300],
      [ 670, 2360],
      [ 400, 2300]]), array([[ 565, 1940],
      [ 680, 2180],
      [ 570, 1945]]), array([[ 980, 1965],
      [1060, 2240],
      [1100, 2250],
      [ 980, 1975]]), array([[1500, 1960],
      [1620, 2200],
      [1505, 1975]])]

","This looks like a traveling_salesman_problem. Compute the pairwise distance between all nodes, make the self distance NaN/Inf, then get the shortest path:
from numpy import array
import networkx as nx

coordinates= [array([[ 300, 2300],
                     [ 670, 2360],
                     [ 400, 2300]]),
              array([[1500, 1960],
                     [1620, 2200],
                     [1505, 1975]]),
              array([[ 980, 1965],
                     [1060, 2240],
                     [1100, 2250],
                     [ 980, 1975]]),
              array([[ 565, 1940],
                     [ 680, 2180],
                     [ 570, 1945]])]

# extract starts and ends
starts = np.vstack([a[0]  for a in coordinates])
ends   = np.vstack([a[-1] for a in coordinates])

# compute pairwise distances, except self
dist = np.sqrt(((starts[:, None] - ends)**2).sum(-1))
np.fill_diagonal(dist, np.nan)

# build the graph
G = nx.from_numpy_array(np.round(dist, 1), create_using=nx.DiGraph)
G.remove_edges_from(nx.selfloop_edges(G))

# find shortest path (NB. cycle could be False)
path = nx.approximation.traveling_salesman_problem(G, cycle=True)
# [1, 0, 3, 2, 1]

out = [coordinates[i] for i in path[1:]]

If you don't necessarily want a cycle, use cycle=False and out = [coordinates[i] for i in path].
Output:
[array([[ 300, 2300],
        [ 670, 2360],
        [ 400, 2300]]),
 array([[ 565, 1940],
        [ 680, 2180],
        [ 570, 1945]]),
 array([[ 980, 1965],
        [1060, 2240],
        [1100, 2250],
        [ 980, 1975]]),
 array([[1500, 1960],
        [1620, 2200],
        [1505, 1975]])]

Intermediate dist:
array([[          nan, 1248.05849222,  753.67433285,  446.01008957],
       [1151.34703717,           nan,  520.21630117,  930.12095988],
       [ 669.79474468,  525.09522946,           nan,  410.48751504],
       [ 396.01136347,  940.65137006,  416.47328846,           nan]])

Graph (with distances as edge labels and the shortest path in red):

",scikit-learn
sklearn importing standard scaler,"I imported standardscaler for a code:
from sklearn.preprocessing import standardscaler

It shows like
cannot import name 'standardscaler' from 'sklearn.preprocessing'

How to solve the error?
","Names in Python are case-sensitive.
Try:
from sklearn.preprocessing import StandardScaler

",scikit-learn
use and save plots created by scikit learn roccurvedisplay with matplotlib39s functions,"I have created a plot with scikit's RocCurveDisplay class and I cannot figure out how to save this plot. I cannot find a way to use matplotlib's functions like savefig() somehow. I would also like to use the plots of RocCurveDisplay in matplotlib's subplots(). Is this possible? Could someone help, and if possible elaborate a little?
","The RocCurveDisplay's plot uses a set of subplots (1x1) if no axes is provided.
If you need more than one, you can simply build your own and pass each ax to the plot :
import matplotlib.pyplot as plt

fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(7, 5))

# used inputs from the docs (Visualizations with Display Objects)

roc_display.plot(ax=ax1) # RocCurveDisplay's plot
pr_display.plot(ax=ax2) # the other plots (eventually)

# to save an image for example
plt.savefig(""display_object_skl.png"", dpi=300, bbox_inches=""tight"")

plt.show()

",scikit-learn
sklearn roc auc score  valueerror y should be a 1d array got an array of shape 15 2 instead,"I have this dataset with target LULUS, it's an imbalance dataset. I'm trying to print roc auc score if I could for each fold of my data but in every fold somehow it's always raise error saying ValueError: y should be a 1d array, got an array of shape (15, 2) instead.. I'm kind of confused which part I did wrong because I do it exactly like in the documentation. And in several fold, I get it that It won't print the score if there's only one label but then it will return the second type of error about 1d array.
merged_df = pd.read_csv(r'C:\...\merged.csv')

num_columns = merged_df.select_dtypes(include=['float64']).columns
cat_columns = merged_df.select_dtypes(include=['object']).drop(['TARGET','NAMA'], axis=1).columns

numeric_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='mean')),
    ('scaler', StandardScaler())])

categorical_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='most_frequent')),
    ('label', OneHotEncoder(handle_unknown='ignore'))])

preprocessor = ColumnTransformer(
    transformers=[
        ('num', numeric_transformer, num_columns),
        ('cat', categorical_transformer, cat_columns)])

X = merged_df.drop(['TARGET','Unnamed: 0'],1)
y = merged_df['TARGET']

X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2)

X_train = X_train.drop(['NIM', 'NAMA'],1)
X_test = X_test.drop(['NIM', 'NAMA'],1)

rf = Pipeline(steps=[('preprocessor', preprocessor),
                     ('classifier',tree.DecisionTreeClassifier(class_weight='balanced', criterion='entropy'))])

rf.fit(X_train, y_train)

pred = rf.predict(X_test)

y_proba = rf.predict_proba(X_test)

from sklearn.model_selection import KFold

kf = KFold(n_splits=10)

for train, test in kf.split(X):
    X_train, X_test = X.loc[train], X.loc[test]
    y_train, y_test = y.loc[train], y.loc[test]
    model = rf.fit(X_train, y_train)
    y_proba = model.predict_proba(X_test)
    try:
        print(roc_auc_score(y_test, y_proba,average='weighted', multi_class='ovr'))
    except ValueError:
        pass

See my data in spreadsheet
","Your output from model.predict_proba() is a matrix with 2 columns, one for each class. To calculate roc, you need to provide the probability of the positive class:
Using an example dataset:
from sklearn.datasets import make_classification
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import roc_auc_score
from sklearn.model_selection import train_test_split

X, y = make_classification(n_classes=2)
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.33, random_state=42)
rf = RandomForestClassifier()
model = rf.fit(X_train, y_train)
y_proba = model.predict_proba(X_test)

It looks like this:
array([[0.69, 0.31],
       [0.13, 0.87],
       [0.94, 0.06],
       [0.94, 0.06],
       [0.07, 0.93]])

Then do:
roc_auc_score(y_test, y_proba[:,1])

",scikit-learn
how to optimise hyperparameterss for randomforestclassifier in python for large datasets,"
I'm just working on this problem where I thought RandomForestClassifier from scikit-learn would be better solution for a large dataset. Only after trying with it for this, I found it to be not accurate. The model is either overfitting or underperforming, and sometimes the training time goes on forever.

500000 samples and 50 features. My goal si to classify data into 3 categories.
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.metrics import accuracy_score

# Load dataset
X = ...  # Features
y = ...  # Labels

# Split the dataset
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Initialize the classifier
rf = RandomForestClassifier(random_state=42)

# Define the parameter grid
param_grid = {
    'n_estimators': [100, 200, 300],
    'max_depth': [10, 20, 30],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4],
    'bootstrap': [True, False]
}

# Grid search
grid_search = GridSearchCV(estimator=rf, param_grid=param_grid, cv=3, n_jobs=-1, verbose=2)
grid_search.fit(X_train, y_train)

# Best parameters and model
best_params = grid_search.best_params_
best_rf = grid_search.best_estimator_

# Predictions and accuracy
y_pred = best_rf.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)

print(f""Best Parameters: {best_params}"")
print(f""Accuracy: {accuracy}"")

I tried many methods like manual hyperparameter tuning, grid search for a systematic approach and randomised search giving me inconsistent results.
I want help in improving all these areas. Thanks in advance.
","To optimise for large datasets, you can use techniques like Bayesian optimization or some evolutionary algorithms.
This can help you explore parameter space without exhaustive grid search.
Use just the best frameworks like Dask or Spark to handle such large datasets and accelerate training times efficiently.
",scikit-learn
sklearnfeature_selectionmutual_info_regression not found,"I have been trying to utilise mutual_info_regression method from sklearn, I have updated sklearn to latest build which is 0.24.1 and when I checked the source code inside my conda env path there is folder and files for feature_selection.mutual_info_regression, but when I try to import it in my Jupiter notebook it throws this error ImportError: cannot import name 'mutual_info_regression' from 'sklearn.model_selection' (/opt/anaconda3/envs/<my_env>/lib/python3.8/site-packages/sklearn/model_selection/__init__.py)
I tried restarting kernel as well, but it is still not working, has anyone else faced this issue? Im using macOS 11.2.1 and conda 4.8.3 with Python3
","I found the solution,
I just had to restart my terminal and then it started working for some reason.
I hope this helps anyone facing such problem in future
Thanks SO!
",scikit-learn
what is the difference between pipeline and make_pipeline in scikitlearn,"I got this from the sklearn webpage:

Pipeline: Pipeline of transforms with a final estimator

Make_pipeline: Construct a Pipeline from the given estimators. This is a shorthand for the Pipeline constructor.


But I still do not understand when I have to use each one. Can anyone give me an example?
","The only difference is that make_pipeline generates names for steps automatically.
Step names are needed e.g. if you want to use a pipeline with model selection utilities (e.g. GridSearchCV). With grid search you need to specify parameters for various steps of a pipeline:
pipe = Pipeline([('vec', CountVectorizer()), ('clf', LogisticRegression()])
param_grid = [{'clf__C': [1, 10, 100, 1000]}
gs = GridSearchCV(pipe, param_grid)
gs.fit(X, y)

compare it with make_pipeline:
pipe = make_pipeline(CountVectorizer(), LogisticRegression())     
param_grid = [{'logisticregression__C': [1, 10, 100, 1000]}
gs = GridSearchCV(pipe, param_grid)
gs.fit(X, y)

So, with Pipeline:

names are explicit, you don't have to figure them out if you need them;
name doesn't change if you change estimator/transformer used in a step, e.g. if you replace LogisticRegression() with LinearSVC() you can still use clf__C.

make_pipeline:

shorter and arguably more readable notation;
names are auto-generated using a straightforward rule (lowercase name of an estimator).

When to use them is up to you :) I prefer make_pipeline for quick experiments and Pipeline for more stable code; a rule of thumb: IPython Notebook -> make_pipeline; Python module in a larger project -> Pipeline. But it is certainly not a big deal to use make_pipeline in a module or Pipeline in a short script or a notebook. 
",scikit-learn
valuererror found input variables with inconsistent numbers of samples,"I wrote the following code to learn the score in the machine learning methods. but I get the following error. what would be the reason??
ValueError: Found input variables with inconsistent numbers of samples: [6396, 1599]

import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import numpy as np

df = pd.read_csv('Armenian Market Car Prices.csv')

df['Car Name'] = df['Car Name'].astype('category').cat.codes

df = df.join(pd.get_dummies(df.FuelType, dtype=int))
df = df.drop('FuelType', axis=1)

df['Region'] = df['Region'].astype('category').cat.codes

df['Price'] = df.pop('Price')

X = df.drop('Price', axis=1)
y = df['Price']

from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression

X_train, y_train, X_test, y_test = train_test_split(X, y, test_size=0.2)
model = LinearRegression()

model.fit(X_train, y_train)

---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
Cell In[358], line 1
----> 1 model.fit(X_train, y_train)

File ~\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\base.py:1473, in _fit_context.<locals>.decorator.<locals>.wrapper(estimator, *args, **kwargs)
   1466     estimator._validate_params()
   1468 with config_context(
   1469     skip_parameter_validation=(
   1470         prefer_skip_nested_validation or global_skip_validation
   1471     )
   1472 ):
-> 1473     return fit_method(estimator, *args, **kwargs)

File ~\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\linear_model\_base.py:609, in LinearRegression.fit(self, X, y, sample_weight)
    605 n_jobs_ = self.n_jobs
    607 accept_sparse = False if self.positive else [""csr"", ""csc"", ""coo""]
--> 609 X, y = self._validate_data(
    610     X, y, accept_sparse=accept_sparse, y_numeric=True, multi_output=True
    611 )
    613 has_sw = sample_weight is not None
    614 if has_sw:

File ~\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\base.py:650, in BaseEstimator._validate_data(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)
    648         y = check_array(y, input_name=""y"", **check_y_params)
    649     else:
--> 650         X, y = check_X_y(X, y, **check_params)
    651     out = X, y
    653 if not no_val_X and check_params.get(""ensure_2d"", True):

File ~\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\utils\validation.py:1291, in check_X_y(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)
   1273 X = check_array(
   1274     X,
   1275     accept_sparse=accept_sparse,
   (...)
   1286     input_name=""X"",
   1287 )
   1289 y = _check_y(y, multi_output=multi_output, y_numeric=y_numeric, estimator=estimator)
-> 1291 check_consistent_length(X, y)
   1293 return X, y

File ~\AppData\Local\Programs\Python\Python312\Lib\site-packages\sklearn\utils\validation.py:460, in check_consistent_length(*arrays)
    458 uniques = np.unique(lengths)
    459 if len(uniques) > 1:
--> 460     raise ValueError(
    461         ""Found input variables with inconsistent numbers of samples: %r""
    462         % [int(l) for l in lengths]
    463     )

ValueError: Found input variables with inconsistent numbers of samples: [6396, 1599]

I tried everything but nothing works or I don't know how to solve the problem.
Jupyternaut:

The error message you provided indicates that there is a problem with the input data. Specifically, it appears that there are two different versions of the input data, one with 6396 samples and another with 1599 samples. This can cause issues when trying to fit a model or perform other operations on the data.
To resolve this issue, you may need to check your code and ensure that you are using the correct version of the input data for each operation. You may also want to try cleaning up the input data by removing any duplicates or inconsistencies.

","The output order of train_test_split is not correct in your code. So your y_train is actualy the X_test, which has a different sample size than train set. Here is the correct order:
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

",scikit-learn
using ranking data in logistic regression,"I am trying to use some ranking data in a logistic regression. I want to use machine learning to make a simple classifier as to whether a webpage is ""good"" or not. It's just a learning exercise so I don't expect great results; just hoping to learn the ""process"" and coding techniques.
I have put my data in a .csv as follows :
URL WebsiteText AlexaRank GooglePageRank

In my Test CSV we have :
URL WebsiteText AlexaRank GooglePageRank Label

Label is a binary classification indicating ""good"" with 1 or ""bad"" with 0.
I currently have my LR running using only the website text; which I run a TF-IDF on.
I have a two questions which I need help with:

How can I normalize my ranking data for AlexaRank? I have a set of
10,000 webpages, for which I have the Alexa rank of all of them;
however they aren't ranked 1-10,000. They are ranked out of the
entire Internet, so while http://www.google.com may be ranked #1,
http://www.notasite.com may be ranked #83904803289480. How do I
normalize this in Scikit learn in order to get the best possible
results from my data?

I am running my Logistic Regression in this way; I am nearly sure I have done this incorrectly. I am trying to do the TF-IDF on the website text, then add the two other relevant columns and fit the Logistic Regression. I'd appreciate if someone could quickly verify that I am taking in the three columns I want to use in my LR correctly.
 loadData = lambda f: np.genfromtxt(open(f,'r'), delimiter=' ')

 print ""loading data..""
 traindata = list(np.array(p.read_table('train.tsv'))[:,2])#Reading WebsiteText column for TF-IDF.
 testdata = list(np.array(p.read_table('test.tsv'))[:,2])
 y = np.array(p.read_table('train.tsv'))[:,-1] #reading label

 tfv = TfidfVectorizer(min_df=3,  max_features=None, strip_accents='unicode', analyzer='word',

 token_pattern=r'\w{1,}', ngram_range=(1, 2), use_idf=1, smooth_idf=1,sublinear_tf=1)

 rd = lm.LogisticRegression(penalty='l2', dual=True, tol=0.0001, C=1, fit_intercept=True,    intercept_scaling=1.0, class_weight=None, random_state=None)

 X_all = traindata + testdata
 lentrain = len(traindata)

 print ""fitting pipeline""
 tfv.fit(X_all)
 print ""transforming data""
 X_all = tfv.transform(X_all)
 X = X_all[:lentrain]
 X_test = X_all[lentrain:]

 print ""20 Fold CV Score: "", np.mean(cross_validation.cross_val_score(rd, X, y, cv=20, scoring='roc_auc'))

 #Add Two Integer Columns
 AlexaAndGoogleTrainData = list(np.array(p.read_table('train.tsv'))[2:,3])#Not sure if I am doing this correctly. Expecting it to contain AlexaRank and GooglePageRank columns.
 AlexaAndGoogleTestData = list(np.array(p.read_table('test.tsv'))[2:,3])
 AllAlexaAndGoogleInfo = AlexaAndGoogleTestData + AlexaAndGoogleTrainData

 #Add two columns to X.
 X = np.append(X, AllAlexaAndGoogleInfo, 1) #Think I have done this incorrectly.

 print ""training on full data""
 rd.fit(X,y)
 pred = rd.predict_proba(X_test)[:,1]
 testfile = p.read_csv('test.tsv', sep=""\t"", na_values=['?'], index_col=1)
 pred_df = p.DataFrame(pred, index=testfile.index, columns=['label'])
 pred_df.to_csv('benchmark.csv')
     print ""submission file created..""`



","I guess sklearn.preprocessing.StandardScaler would be the first thing you want to try. StandardScaler transforms all of your features into Mean-0-Std-1 features. 

This definitely gets rid of your first problem. AlexaRank will be guaranteed to be spread around 0 and bounded. (Yes, even massive AlexaRank values like 83904803289480 are transformed to small floating point numbers). Of course, the results will not be integers between 1 and 10000 but they will maintain same order as the original ranks. And in this case, keeping the rank bounded and normalized will help solve your second problem like follows.
In order to understand why normalization would help in LR, let's revisit the logit formulation of LR. 

In your case, X1, X2, X3 are three TF-IDF features and X4, X5 are Alexa/Google rank related features. Now, the linear form of equation suggest that the coefficients represent the change in logit of y with one unit change in a variable. Think what happens when your X4 is kept fixed at a massive rank value, say 83904803289480. In that case, the Alexa Rank variable dominates your LR fit and a small change in TF-IDF value has almost no effect on the LR fit. Now one might think that the coefficient should be able to adjust to small/large values to account for differences between these features. Not in this case --- It's not only the magnitude of variables that matter but also their range. Alexa Rank definitely has a large range and should definitely dominate your LR fit in this case. Therefore, I guess normalizing all variables using StandardScaler to adjust their range will improve the fit. 

Here is how you can scale the X matrix. 
sc = proprocessing.StandardScaler().fit(X)
X = sc.transform(X)

Don't forget to use same scaler to transform X_test.
X_test = sc.transform(X_test)

Now you can use the fitting procedure etc.
rd.fit(X, y)
re.predict_proba(X_test)

Check this out for more on sklearn preprocessing: http://scikit-learn.org/stable/modules/preprocessing.html
Edit: Parsing and column merging part can be easily done using pandas, i.e., there is no need to convert the matrices into list and then append them. Moreover, pandas dataframes can be directly indexed by their column names. 
AlexaAndGoogleTrainData = p.read_table('train.tsv', header=0)[[""AlexaRank"", ""GooglePageRank""]]
AlexaAndGoogleTestData = p.read_table('test.tsv', header=0)[[""AlexaRank"", ""GooglePageRank""]]
AllAlexaAndGoogleInfo = AlexaAndGoogleTestData.append(AlexaAndGoogleTrainData)

Note that we are passing header=0 argument to read_table to maintain original header names from tsv file. And also note how we can index using entire set of columns. Finally, you can stack this new matrix with X using numpy.hstack.
X = np.hstack((X, AllAlexaAndGoogleInfo))

hstack horizontally combined two multi-dimensional array-like structures provided their lengths are same.
",scikit-learn
describing the data points with arcshaped graph,"I would like to fit a graph that closely follow the characteristics of a scatterplot using Python.
I have searched many curve-fitting techniques, and I have tried many different polynomial and linear regression approach powered by Scikit-Learn library.
However, the best I could get is the following
The result of polynomial regression
What I expect is somewhat like the following, denoted in green.
The ideal result desired
I have thought about simple connecting the dots, but I think that it would not give me a concrete mathematical expression of the graph. So I need to come up with a way to draw a graph that looks like one in the green above, which can be described in a certain mathematical expression.
Can you guys suggest me the ways that I can do it? I was also thinking if combining logarithmic or exponential functions are necessary for this purpose. If anyone has a reasonable idea. I would appreciate your reply and would be glad to upvote!
Just to give you an idea of what kind of code that I used, I am attaching my piece of python code that I used to produce the results displayed with the first picture.
import numpy as np
import pandas as pd
import plotly.express as px
from sklearn.preprocessing import PolynomialFeatures
from sklearn.linear_model import LinearRegression
import matplotlib.pyplot as plt

# Load the point cloud data
data = pd.read_csv('data/selected_2.csv')
points = data.to_numpy()
points_x = points[:, 0]
points_y = points[:, 1]
points_z = points[:, 2]

poly = PolynomialFeatures(degree=40, include_bias=False)
poly_features = poly.fit_transform(points_x.reshape(-1, 1))

poly_reg_model = LinearRegression()
poly_reg_model.fit(poly_features, points_z)
z_predicted = poly_reg_model.predict(poly_features)

plt.figure(figsize=(10, 6))
plt.title(""Your first polynomial regression – congrats! :)"", size=16)
plt.scatter(points_x, points_z)
plt.plot(points_x, z_predicted, c=""red"")
plt.show()

the data that I used is a point cloud data composed of 'X', 'Y', and 'Z' columns, with each row indicating the coordinates. The size of the original data is (23, 3). But as 'Y' components are negligible in my case, I am only concerned with columns 'X' and 'Z'. This is why I am trying to do this in 2D graph.
","Your code seems fine, but...
...as you formulated it, the polynomial curve is implicit (there are multiple z values for the same x).
Swapping the X, Z variables will make your curve explicit and then fittable as it looks like a reasonable polynomial.
",scikit-learn
why does plotting errors vs actual via predictionerrordisplay result in a value error,"I have trained a random forest regression model using sklearn, and used it to make some predictions on a test dataset. Naturally there are errors where the values predicted by the model are not the same as the actual values; in this case, the model's Mean Average Error and Mean Squared Error are quite high.
I want to visualise the errors, so that I can understand whether the errors are consistently large, or whether there are just a few unusually large errors driving up the averages.
I'm trying to use sklearn's PredictionErrorDisplay function to do this, but the following code returns the error message ""ValueError: Unable to coerce to Series, length must be 1"":
errors = PredictionErrorDisplay(y_true = test_targets, y_pred = test_predictions)
errors.plot()
plt.savefig('Output.png')
plt.clf()

Does anyone know how I can resolve this please? My reading of the error is that I need to convert the object PredictionErrorDisplay creates into a different format, but I'm not sure how to do that, or what the format needs to be exactly.
","The solution has been supplied by rayan2338.
The problem was that test_targets was a pandas dataframe and therefore incompatible with PredictionErrorDisplay.
The following code produces the required result:
test_targets_array = np.array(test_targets)
test_targets_array = test_targets_array.flatten()
errors = PredictionErrorDisplay(y_true = test_targets_array, y_pred = test_predictions)
errors.plot()
plt.savefig('Output.png')
plt.clf()

",scikit-learn
module 39numpy39 has no attribute 39dtype39,"When importing sklearn datasets eg. 
from sklearn.datasets import fetch_mldata
from sklearn.datasets import fetch_openml

I get the error
Traceback (most recent call last):
  File ""numbers.py"", line 1, in <module>
    from sklearn.datasets import fetch_openml
  File ""/anaconda2/envs/numbers/lib/python3.5/site-packages/sklearn/__init__.py"", line 64, in <module>
    from .base import clone
  File ""/anaconda2/envs/numbers/lib/python3.5/site-packages/sklearn/base.py"", line 11, in <module>
    import numpy as np
  File ""/anaconda2/envs/numbers/lib/python3.5/site-packages/numpy/__init__.py"", line 142, in <module>
    from . import core
  File ""/anaconda2/envs/numbers/lib/python3.5/site-packages/numpy/core/__init__.py"", line 93, in <module>
    from . import numerictypes as nt
  File ""/anaconda2/envs/numbers/lib/python3.5/site-packages/numpy/core/numerictypes.py"", line 86, in <module>
    import numbers
  File ""/Users/airocoop/repos/Numbers/numbers.py"", line 1, in <module>
    from sklearn.datasets import fetch_openml
  File ""/anaconda2/envs/numbers/lib/python3.5/site-packages/sklearn/datasets/__init__.py"", line 6, in <module>
    from .base import load_breast_cancer
  File ""/anaconda2/envs/numbers/lib/python3.5/site-packages/sklearn/datasets/base.py"", line 20, in <module>
    from ..utils import Bunch
  File ""/anaconda2/envs/numbers/lib/python3.5/site-packages/sklearn/utils/__init__.py"", line 10, in <module>
    from scipy.sparse import issparse
  File ""/anaconda2/envs/numbers/lib/python3.5/site-packages/scipy/__init__.py"", line 72, in <module>
    from numpy.random import rand, randn
  File ""/anaconda2/envs/numbers/lib/python3.5/site-packages/numpy/random/__init__.py"", line 143, in <module>
    from .mtrand import *
  File ""numpy.pxd"", line 87, in init mtrand
AttributeError: module 'numpy' has no attribute 'dtype'

I am not sure why I get this
I don't get this error when running things from a jupyter notebook, which is also weird. Any help on this issue would be greatly appreciated
","I figured this out. The answer is that the file I was running was named numbers.py. This screws the whole thing up. If you have this problem check to make sure you don't have a file in the directory called numbers.py
Easy way to check is move the file with the import statement to a different directory and try running it.
",scikit-learn
kmean get good group separation,"I have a basic code for use Kmeans with sklearn
    scaled = MinMaxScaler().fit_transform(points)

    kmeans = KMeans(n_clusters=nb_clusters) # , random_state=42 , init='random', algorithm='elkan'
    kmeans.fit(scaled)
    labels = kmeans.labels_
    centroids = kmeans.cluster_centers_

But i don't success to have my two groups well separated

Does someone know what's wrong or what is lacking please ?
Thank you in advance.
","The k-means algorithm is based on the assumption of spherically shaped clusters with a common diameter, e. g. isotropic Gaussian-distributed clusters. Your data does not fit this assumption.
First you should plot your data with a 1:1 aspect ratio to better understand what is going on. You will see that the separation of your two groups circled in blue is smaller as compared to their spread in y-direction.
Second, you should observe that the group to the right includes much, much less data points as compared to the group on the left. To the algorithm, the points on the right are mere outliers of the distribution it tries to model.
To sum it up, k-means is probably not a good fit for your data. Mean shift, although maybe overkill, should be a much better fit.
",scikit-learn
training linear models with mae using sklearn in python,"I'm currently trying to train a linear model using sklearn in python but not with mean squared error (MSE) as error measure - but with mean absolute error (MAE). I specificially need a linear model with MAE as requirement from my professor at university.
I've looked into sklearn.linear_model.LinearRegression which since it is an OLS regressor does not provide alternative error measures. 
Hence, I checked the other available regressors and stumbled upon sklearn.linear_model.HuberRegressor and sklearn.linear_model.SGDRegressor. They both mention MAE as part of their error measures - but do not seem to provide simple MAE. Is there a way to choose the parameters for one of those regressors so that the resulting error measure is a simple MAE? Or is there another regressor in sklearn which I've overlooked? 
Alternatively, is there another (easy to use) python 3.X package which provides what I need?
Thanks for your help!
","In SGD, if you use 'epsilon_insensitive' with epsilon=0 it should work as if you used MAE.
You could also take a look at statsmodels quantile regression (using MAE is also called median regression, and median is a quantile).
",scikit-learn
how to create a custom kernel for a gaussian process regressor in scikitlearn,"I am looking into using a GPR for a rather peculiar context, where I need to write my own Kernel. However I found out there's no documentation about how to do this. Trying to simply inherit from Kernel and implementing the methods __call__, get_params, diag and is_stationary is enough to get the fitting process to work, but then breaks down when I try to predict y values and standard deviations. What are the necessary steps to build a minimal but functional class that inherits from Kernel while using its own function? Thanks!
","Depending on how exotic your kernel will be, the answer to your question may be different.
I find the implementation of the RBF kernel quite self-documenting, so I use it as reference. Here is the gist:
class RBF(StationaryKernelMixin, NormalizedKernelMixin, Kernel):
    def __init__(self, length_scale=1.0, length_scale_bounds=(1e-5, 1e5)):
        self.length_scale = length_scale
        self.length_scale_bounds = length_scale_bounds

    @property
    def hyperparameter_length_scale(self):
        if self.anisotropic:
            return Hyperparameter(""length_scale"", ""numeric"",
                                  self.length_scale_bounds,
                                  len(self.length_scale))
        return Hyperparameter(
            ""length_scale"", ""numeric"", self.length_scale_bounds)

    def __call__(self, X, Y=None, eval_gradient=False):
        # ...

As you mentioned, your kernel should inherit from Kernel, which requires you to implement __call__, diag and is_stationary. Note, that sklearn.gaussian_process.kernels provides StationaryKernelMixin and NormalizedKernelMixin, which implement diag and is_stationary for you (cf. RBF class definition in the code).
You should not overwrite get_params! This is done for you by the Kernel class and it expects scikit-learn kernels to follow a convention, which your kernel should too: specify your parameters in the signature of your constructor as keyword arguments (see length_scale in the previous example of RBF kernel). This ensures that your kernel can be copied, which is done by GaussianProcessRegressor.fit(...) (this could be the reason that you could not predict the standard deviation).
At this point, you may notice the other parameter length_scale_bounds. That is only a constraint on the actual hyper parameter length_scale (cf. constrained optimization). This brings us to the fact, that you need to also declare your hyper parameters, that you want optimized and need to compute gradients for in your __call__ implementation. You do that by defining a property of your class that is prefixed by hyperparameter_ (cf. hyperparameter_length_scale in the code). Each hyper parameter that is not fixed (fixed = hyperparameter.fixed == True) is returned by Kernel.theta, which is used by GP on fit() and to compute the marginal log likelihood. So this is essential if you want to fit parameters to your data.
One last detail about Kernel.theta, the implementation states:

Returns the (flattened, log-transformed) non-fixed hyperparameters.

So you should be careful with 0 values in your hyper parameters as they can end up as np.nan and break stuff.
I hope this helps, even though this question is already a bit old. I have actually never implemented a kernel myself, but was eager to skim the sklearn code base. It is unfortunate that there are no official tutorials on that, the code base, however, is quite clean and commented.
",scikit-learn
classifiers in scikitlearn that handle nannull,"I was wondering if there are classifiers that handle nan/null values in scikit-learn.  I thought random forest regressor handles this but I got an error when I call predict.
X_train = np.array([[1, np.nan, 3],[np.nan, 5, 6]])
y_train = np.array([1, 2])
clf = RandomForestRegressor(X_train, y_train)
X_test = np.array([7, 8, np.nan])
y_pred = clf.predict(X_test) # Fails!

Can I not call predict with any scikit-learn algorithm with missing values?
Edit.
Now that I think about this, it makes sense.  It's not an issue during training but when you predict how do you branch when the variable is null?  maybe you could just split both ways and average the result?  It seems like k-NN should work fine as long as the distance function ignores nulls though.
Edit 2 (older and wiser me)
Some gbm libraries (such as xgboost) use a ternary tree instead of a binary tree precisely for this purpose: 2 children for the yes/no decision and 1 child for the missing decision. sklearn is using a binary tree
","I made an example that contains both missing values in training and the test sets
I just picked a strategy to replace missing data with the mean, using the SimpleImputer class. There are other strategies.
from __future__ import print_function

import numpy as np
from sklearn.ensemble import RandomForestClassifier
from sklearn.impute import SimpleImputer


X_train = [[0, 0, np.nan], [np.nan, 1, 1]]
Y_train = [0, 1]
X_test_1 = [0, 0, np.nan]
X_test_2 = [0, np.nan, np.nan]
X_test_3 = [np.nan, 1, 1]

# Create our imputer to replace missing values with the mean e.g.
imp = SimpleImputer(missing_values=np.nan, strategy='mean')
imp = imp.fit(X_train)

# Impute our data, then train
X_train_imp = imp.transform(X_train)
clf = RandomForestClassifier(n_estimators=10)
clf = clf.fit(X_train_imp, Y_train)

for X_test in [X_test_1, X_test_2, X_test_3]:
    # Impute each test item, then predict
    X_test_imp = imp.transform(X_test)
    print(X_test, '->', clf.predict(X_test_imp))

# Results
[0, 0, nan] -> [0]
[0, nan, nan] -> [0]
[nan, 1, 1] -> [1]

",scikit-learn
how can i make my sklearn prediction model better,"So basically, I have this model in sklearn that predicts the survival rate of titanic. its accuracy is around 0.77.
How can I make it better and more accurate?
import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler, LabelEncoder, MinMaxScaler
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.linear_model import LogisticRegression
from sklearn import metrics
from sklearn.tree import DecisionTreeClassifier

train_df = pd.read_csv(""train.csv"")
test_df = pd.read_csv(""test.csv"")

le = LabelEncoder()
sc = StandardScaler()

train_df['Age'].fillna(train_df['Age'].mean(), inplace=True)
train_df[""Embarked""].fillna(""N"", inplace=True)
train_df['Cabin'] = train_df['Cabin'].str[:1]
train_df['Cabin'].fillna('N', inplace=True)
train_df[""Cabin""]

for col in [""Sex"", ""Embarked"", ""Cabin""]:
    train_df[col] = LabelEncoder().fit_transform(train_df[col])

x = train_df.drop([""PassengerId"",""Name"",""Ticket"", ""Survived""], axis=1)
y = train_df[""Survived""]

x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2)

dt_clf = DecisionTreeClassifier(max_depth= 5, min_samples_leaf= 1,min_samples_split= 2)
dt_clf.fit(x_train, y_train)
pred = dt_clf.predict(x_test)

print(metrics.accuracy_score(y_test, pred))

I filled the na with mean and changed the scaler and the algorithm, but nothing happened.
","Handling Missing Values:
You've filled the missing values in 'Age' with the mean. To take it a step further, consider using another methods to fill in missing values, such as the median, mode, or even using predictive models.
Feature Engineering:
You can create some new features. For example:

Family Size: Combine the 'sibsp' (number of siblings/spouses aboard) and 'parch' (number of parents/children aboard) columns to create a new feature that represents the size of the family.
Title: Extract titles from the passenger names (like Mr, Mrs, Miss) and use them as a feature.
IsAlone: Create a binary feature that indicates whether the passenger was traveling alone.

Scaling and Encoding:
You're currently using LabelEncoder for the categorical variables. Consider using OneHotEncoder or pd.get_dummies for better handling of categorical variables. Also, make sure to apply appropriate scaling techniques.
Feature Selection:
You should select the most important features. You can use methods like recursive feature elimination (RFE), feature importance from models, or  correlation matrices to drop the less important features.
Model Selection and Hyperparameter Tuning:
You're using a DecisionTreeClassifier. Maybe you can try different algorithms and use cross-validation to fine-tune the hyperparameters. This will help you find the best combination for your model.
Cross-Validation and Hyperparameter Tuning:
To find the best hyperparameters for your model, consider using GridSearchCV or RandomizedSearchCV.
Look at my reconstructed code:
import seaborn as sns
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.impute import SimpleImputer
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from xgboost import XGBClassifier
from sklearn.metrics import accuracy_score

# Load Titanic dataset from Seaborn
titanic = sns.load_dataset('titanic')
print(""Dataset loaded."")

# Feature engineering
titanic['FamilySize'] = titanic['sibsp'] + titanic['parch']
titanic['IsAlone'] = (titanic['FamilySize'] == 0).astype(int)
titanic['deck'] = titanic['deck'].cat.add_categories('N').fillna('N')
print(""Feature engineering completed."")

# Dropping features
dropped_features = ['alive', 'adult_male', 'embark_town', 'alone']
titanic.drop(dropped_features, axis=1, inplace=True)
print(f""Dropped less useful features: {dropped_features}"")

# Preprocessing
numeric_features = ['age', 'fare', 'FamilySize']
numeric_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='mean')),
    ('scaler', StandardScaler())])

categorical_features = ['sex', 'deck', 'embarked', 'who', 'class']
categorical_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),
    ('onehot', OneHotEncoder(handle_unknown='ignore'))])

preprocessor = ColumnTransformer(
    transformers=[
        ('num', numeric_transformer, numeric_features),
        ('cat', categorical_transformer, categorical_features)])
print(""Preprocessing setup completed."")

# Splitting data
X = titanic.drop('survived', axis=1)
y = titanic['survived']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
print(""Data split into training and testing sets."")

# Define models and their hyperparameters
models = {
    'RandomForest': {
        'model': RandomForestClassifier(random_state=42),
        'params': {
            'classifier__n_estimators': [100, 200],
            'classifier__max_depth': [5, 10],
            'classifier__min_samples_split': [2, 5],
            'classifier__min_samples_leaf': [1, 2]
        }
    },
    'LogisticRegression': {
        'model': LogisticRegression(max_iter=1000),
        'params': {
            'classifier__C': [0.01, 0.1, 1, 10],
            'classifier__solver': ['lbfgs', 'liblinear']
        }
    },
    'SVM': {
        'model': SVC(),
        'params': {
            'classifier__C': [0.1, 1, 10],
            'classifier__gamma': ['scale', 'auto'],
            'classifier__kernel': ['linear', 'rbf']
        }
    },
    'GradientBoosting': {
        'model': GradientBoostingClassifier(random_state=42),
        'params': {
            'classifier__n_estimators': [100, 200],
            'classifier__learning_rate': [0.01, 0.1],
            'classifier__max_depth': [3, 5]
        }
    },
    'XGBoost': {
        'model': XGBClassifier(random_state=42),
        'params': {
            'classifier__n_estimators': [100, 200],
            'classifier__learning_rate': [0.01, 0.1],
            'classifier__max_depth': [3, 5]
        }
    }
}

# Function to perform grid search and return the best model
def perform_grid_search(X_train, y_train, model, params):
    pipeline = Pipeline(steps=[('preprocessor', preprocessor),
                               ('classifier', model)])
    grid_search = GridSearchCV(pipeline, params, cv=5, scoring='accuracy')
    grid_search.fit(X_train, y_train)
    return grid_search

best_models = {}
for model_name, model_info in models.items():
    print(f""Training {model_name}..."")
    best_models[model_name] = perform_grid_search(X_train, y_train, model_info['model'], model_info['params'])
    print(f""{model_name} training completed."")

# Evaluate models
for model_name, model in best_models.items():
    best_model = model.best_estimator_
    y_pred = best_model.predict(X_test)
    accuracy = accuracy_score(y_test, y_pred)
    print(f'{model_name} Best Hyperparameters: {model.best_params_}')
    print(f'{model_name} Accuracy: {accuracy}')

# Select the best model based on accuracy
best_model_name = max(best_models, key=lambda name: accuracy_score(y_test, best_models[name].best_estimator_.predict(X_test)))
print(f'Best model: {best_model_name} with accuracy {accuracy_score(y_test, best_models[best_model_name].best_estimator_.predict(X_test))}')

My accuracy is about 0.821
",scikit-learn
why do the sensitivity recall values differ between classification_report and precision_recall_fscore_support in a loop,"I am working with a synthetic dataset generated using make_classification from sklearn.datasets with 5 classes. I have trained a RandomForestClassifier on this data and am evaluating its performance using two different methods. However, I am observing differences in the sensitivity (recall) values between these two methods.
Here is the code I am using:
from sklearn.datasets import make_classification
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, precision_recall_fscore_support
import numpy as np
import pandas as pd

# Generate a synthetic dataset with 5 classes
X, y = make_classification(n_samples=1000, n_classes=5, n_informative=10, n_clusters_per_class=1, random_state=42)

# Split into train and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Train a classifier
clf = RandomForestClassifier(random_state=42)
clf.fit(X_train, y_train)

# Predict on the test set
y_pred = clf.predict(X_test)

# Method 1: classification_report
print(""Classification Report"")
print(classification_report(y_test, y_pred))

# Method 2: Loop with precision_recall_fscore_support
res = []

for l in range(5):
    prec, recall, _, _ = precision_recall_fscore_support(np.array(y_test) == l,
                                                         np.array(y_pred) == l,
                                                         pos_label=True, average=None)
    res.append([l, recall[0], recall[1]])

df = pd.DataFrame(res, columns=['class', 'sensitivity', 'specificity'])
print(""\nSensitivity and Specificity"")
print(df)

Outputs:
Classification Report
              precision    recall  f1-score   support

           0       0.76      0.71      0.74        35
           1       0.72      0.93      0.81        30
           2       0.72      0.81      0.76        32
           3       0.85      0.86      0.86        59
           4       0.88      0.64      0.74        44

    accuracy                           0.79       200
   macro avg       0.78      0.79      0.78       200
weighted avg       0.80      0.79      0.79       200


Sensitivity and Specificity
   class  sensitivity  specificity
0      0     0.951515     0.714286
1      1     0.935294     0.933333
2      2     0.940476     0.812500
3      3     0.936170     0.864407
4      4     0.974359     0.636364

Question:
Why do the sensitivity (recall) values differ between the classification_report and the loop using precision_recall_fscore_support? Specifically, why is there a discrepancy between the recall values reported by classification_report and the sensitivity values calculated in the loop method? If possible can u show it with a simple example (solved manually)
What did you try and what were you expecting?
I used two methods to evaluate the performance of my RandomForestClassifier. First, I used classification_report to get precision, recall, and F1-score for each class. Then, I calculated sensitivity and specificity for each class using a loop with precision_recall_fscore_support.
I expected the sensitivity values calculated in the loop method to match the recall values from the classification_report, as sensitivity and recall are often considered synonymous in classification tasks. However, I observed discrepancies between the two sets of values.
What actually resulted?
The recall values from the classification_report are different from the sensitivity values calculated in the loop method. The classification_report provides recall values for each class in a multi-class context, while the loop method treats each class as a binary classification problem, leading to different sensitivity and specificity values.
","You're incorrectly unpacking the results in the loop. With average=None, your recall is a pair:

the first one is recall of the ""negative class"" of this loop-iteration's one-vs-rest metric; this doesn't correspond to any single class metric.

The second one is recall of the positive class, which really is the recall for this class in the multiclass sense.


But then you've labeled them wrong in your dataframe: what you called specificity is really the sensitivity/recall (and indeed these match the classification report), and what you called sensitivity isn't any single-class metric.
",scikit-learn
choosing top k models using gridsearchcv in scikitlearn,"Is there an easy/pre-existing way to perform a Grid Search in scikit-learn and then automatically return the top k best performing models or automatically average their outputs? I intend to try and reduce overfitting this way. I have not yet found anything related to this.
EDIT: To clarify, I know about sklearn's GridSearch, I am looking for an option to perform a Grid Search and then return the top k best performing models or average over them, rather than just returning the best single model.
","If you have your fitted GridSearchCV object as grid, you can get the results for each parameter with grid.cv_results_. I usually load it as a pandas DataFrame.
import pandas as pd
results = pd.DataFrame(grid.cv_results_)
results.sort_values(by='rank_test_score', inplace=True)

Then you can get the parameters for each model from the params column. For example, If you want to set the 2nd best parameter:
params_2nd_best = results.iloc[1]['params']
clf_2nd_best = grid.best_estimator_.set_params(**params_2nd_best)

",scikit-learn
cannot cast array data from dtype39float6439 to dtype39int3239 according to 39safe39,"I've got a dataset with 6 columns 'Weight'(float), 'Gender'(0 or 1 (int)), 'Height'(float), 'Metabolism'(0,1,2,3 (int)), 'Psychology'(0,1,2,3,4,5,6 (int)) and the column we have to predict is 'Age'(int). I have to do it with sklearn's VotingClassifier. I've split the data this way after I applied one-hot-encoding.
X_train, X_test, y_train, y_test = train_test_split(X_hot, y, test_size=0.25, random_state=1)

I use these 4 algorithms for the classifier.
gbm = GradientBoostingRegressor(loss='huber',n_estimators=5000,max_features=""sqrt"",subsample=0.9)
gbm.fit(X = X_train,y = np.log1p(y_train))

ada = AdaBoostClassifier(n_estimators=2000)
ada.fit(X = X_train,y = y_train)

log_reg = LogisticRegression()
log_reg.fit(X_train, y_train)

and knn as well. Now, this part works perfectly
from sklearn.ensemble import VotingClassifier
estimators=[('knn', knn_best), ('ada', ada), ('log_reg', log_reg), ('gbm', gbm)]
new_ensemble = VotingClassifier(estimators, voting='hard')
new_ensemble.fit(X_train, y_train)

and this part below is where it shows the error
y_pred = new_ensemble.predict(X_test)

I tried converting everything to float from X_train, X_test, y_train, y_test but it didn't change anything. I changed everything to int but the same error happens as well. Why does that line show the error? I'm really confused.
---------------------------------------------------------------------------
TypeError                                 Traceback (most recent call last)
<ipython-input-37-86a04c2ceff1> in <module>
----> 1 y_pred = new_ensemble.predict(X_test)

~\AppData\Roaming\Python\Python37\site-packages\sklearn\ensemble\voting_classifier.py in predict(self, X)
    237                 lambda x: np.argmax(
    238                     np.bincount(x, weights=self._weights_not_none)),
--> 239                 axis=1, arr=predictions)
    240 
    241         maj = self.le_.inverse_transform(maj)

~\Anaconda3\lib\site-packages\numpy\lib\shape_base.py in apply_along_axis(func1d, axis, arr, *args, **kwargs)
    378     except StopIteration:
    379         raise ValueError('Cannot apply_along_axis when any iteration dimensions are 0')
--> 380     res = asanyarray(func1d(inarr_view[ind0], *args, **kwargs))
    381 
    382     # build a buffer for storing evaluations of func1d.

~\AppData\Roaming\Python\Python37\site-packages\sklearn\ensemble\voting_classifier.py in <lambda>(x)
    236             maj = np.apply_along_axis(
    237                 lambda x: np.argmax(
--> 238                     np.bincount(x, weights=self._weights_not_none)),
    239                 axis=1, arr=predictions)
    240 

TypeError: Cannot cast array data from dtype('float64') to dtype('int32') according to the rule 'safe'

","Try to use parameter voting='soft' for VotingClassifier. I think with voting='hard' it expects integer labels from all models, but gets some float values from regressors. With soft it takes models results as probabilities, and probabilities are float numbers, of course.
",scikit-learn
how to correctly apply layernorm after multiheadattention with different input shapes batch_first vs default in pytorch,"I’m working on an audio recognition task using a Transformer-based model in PyTorch. My input features are generated by a CNN-based embedding layer and have the shape [batch_size, d_model, n_token], where n_token is the sequence length and d_model is the feature dimension.
By default, nn.MultiheadAttention (when batch_first=False) expects input in the shape (seq, batch, feature). To make things more intuitive, I chose to set batch_first=True and then permute my data from [batch_size, d_model, n_token] to [batch_size, n_token, d_model] so that the time dimension comes before the feature dimension. Here’s a simplified code snippet:
# Original shape: [batch_size, d_model, n_token]
data = concat_cls_token(data)   # [batch_size, d_model, n_token+1]
data = data.permute(0, 2, 1)    # [batch_size, n_token+1, d_model]

multihead_att = nn.MultiheadAttention(d_model, num_heads, batch_first=True)
data, _ = multihead_att(data, data, data)
# Result shape: [batch_size, n_token+1, d_model]

After applying multi-head attention, I use LayerNorm(d_model) directly on this [batch_size, n_token+1, d_model] tensor. My understanding is that LayerNorm normalizes over the feature dimension, so as long as the feature dimension (d_model) is the last one, it should work fine. But I have two main questions:

If I had stuck with the default multi-head attention format (seq, batch, feature)—that is, using [n_token+1, batch_size, d_model]—would LayerNorm(d_model) still correctly normalize along the feature dimension without permuting the tensor again?
In practice, what’s the best approach for tasks like mine (audio sequence recognition)? Is it recommended to keep the data in [batch_size, seq_len, d_model] format before calling LayerNorm, or is it perfectly acceptable to use (seq, batch, feature) as long as the feature dimension is last?

Both my advisor and I are a bit uncertain. Below are more details from my forward method and the corresponding AttentionBlock implementation for reference:
def forward(self, x: torch.Tensor):
    # Initial: x is [batch_size, d_model, num_tokens]
    x = self.expand(x)
    x = self.concat_cls_token(x)   # [batch_size, d_model, num_tokens+1]
    x = x.permute(0, 2, 1)         # [batch_size, num_tokens+1, d_model]
    x = self.positional_encoder(x)
    x = self.attention_block(x)    # [batch_size, num_tokens+1, d_model]
    x = x.permute(0, 2, 1)         # [batch_size, d_model, num_tokens+1]

    x = self.get_cls_token(x)      # [batch_size, d_model, 1]
    y = self.class_mlp(x)          # [batch_size, n_classes]
    return y

and the implement of AttentionBlock:
class AttentionBlock(nn.Module):
    @staticmethod
    def make_ffn(hidden_dim: int) -> torch.nn.Module:
        return nn.Sequential(
            OrderedDict([
                (""ffn_linear1"", nn.Linear(in_features=hidden_dim, out_features=hidden_dim)),
                (""ffn_relu"", nn.ReLU()),
                (""ffn_linear2"", nn.Linear(in_features=hidden_dim, out_features=hidden_dim))
            ])
        )

    def __init__(self, embed_dim, n_head):
        super().__init__()
        self.attention = nn.MultiheadAttention(embed_dim, n_head, batch_first=True)
        self.layer_norm1 = nn.LayerNorm(embed_dim)
        self.feed_forward = self.make_ffn(embed_dim)
        self.layer_norm2 = nn.LayerNorm(embed_dim)

    def forward(self, x: torch.Tensor):
        attn_output, _ = self.attention(x, x, x)
        x = self.layer_norm1(x + attn_output)
        ff_output = self.feed_forward(x)
        x = self.layer_norm2(x + ff_output)
        return x

","From the layernorm documentation:
torch.nn.LayerNorm(normalized_shape, eps=1e-05, elementwise_affine=True, bias=True, device=None, dtype=None)

""""
Applies Layer Normalization over a mini-batch of inputs.

    This layer implements the operation as described in
    the paper `Layer Normalization <https://arxiv.org/abs/1607.06450>`__

    .. math::
        y = \frac{x - \mathrm{E}[x]}{ \sqrt{\mathrm{Var}[x] + \epsilon}} * \gamma + \beta

    The mean and standard-deviation are calculated over the last `D` dimensions, where `D`
    is the dimension of :attr:`normalized_shape`. For example, if :attr:`normalized_shape`
    is ``(3, 5)`` (a 2-dimensional shape), the mean and standard-deviation are computed over
    the last 2 dimensions of the input (i.e. ``input.mean((-2, -1))``).
    :math:`\gamma` and :math:`\beta` are learnable affine transform parameters of
    :attr:`normalized_shape` if :attr:`elementwise_affine` is ``True``.
    The standard-deviation is calculated via the biased estimator, equivalent to
    `torch.var(input, unbiased=False)`.
""""""

As the documentation says, The mean and standard-deviation are calculated over the last D dimensions. If you create a layer as nn.LayerNorm(d_model), it assumes the input will have a last dimension of shape d_model and apply layernorm over that dimension. The other dimensions of the tensor are not relevant.
",pytorch
debugging gan covergence,"Building a GAN to generate images. The images have 3 color channels, 96 x 96.
The images that are generated by the generator at the beginning are all black, which is an issue given that is statistically highly unlikely.
Also, the loss for both networks is not improving.
I have posted the entire code below, and commented to allow it to be easily read. This is my first time building a GAN and I am new to Pytorch so any help is very appreciated!
import torch
from torch.optim import Adam
from torch.utils.data import DataLoader
from torch.autograd import Variable

import numpy as np
import os
import cv2
from collections import deque

# training params
batch_size = 100
epochs = 1000

# loss function
loss_fx = torch.nn.BCELoss()

# processing images
X = deque()
for img in os.listdir('pokemon_images'):
    if img.endswith('.png'):
        pokemon_image = cv2.imread(r'./pokemon_images/{}'.format(img))
        if pokemon_image.shape != (96, 96, 3):
            pass
        else:
            X.append(pokemon_image)

# data loader for processing in batches
data_loader = DataLoader(X, batch_size=batch_size)

# covert output vectors to images if flag is true, else input images to vectors
def images_to_vectors(data, reverse=False):
    if reverse:
        return data.view(data.size(0), 3, 96, 96)
    else:
        return data.view(data.size(0), 27648)

# Generator model
class Generator(torch.nn.Module):
    def __init__(self):
        super(Generator, self).__init__()
        n_features = 1000
        n_out = 27648
        
        self.model = torch.nn.Sequential(
                torch.nn.Linear(n_features, 128),
                torch.nn.ReLU(),
                torch.nn.Linear(128, 256),
                torch.nn.ReLU(),
                torch.nn.Linear(256, 512),
                torch.nn.ReLU(),
                torch.nn.Linear(512, 1024),
                torch.nn.ReLU(),
                torch.nn.Linear(1024, n_out),
                torch.nn.Tanh()
        )


    def forward(self, x):
        img = self.model(x)
        return img

    def noise(self, s):
       x = Variable(torch.randn(s, 1000))
       return x


# Discriminator model
class Discriminator(torch.nn.Module):
    def __init__(self):
        super(Discriminator, self).__init__()
        n_features = 27648
        n_out = 1

        self.model = torch.nn.Sequential(
                torch.nn.Linear(n_features, 512),
                torch.nn.ReLU(),
                torch.nn.Linear(512, 256),
                torch.nn.ReLU(),
                torch.nn.Linear(256, n_out),
                torch.nn.Sigmoid()
        )


    def forward(self, img):
        output = self.model(img)
        return output


# discriminator training
def train_discriminator(discriminator, optimizer, real_data, fake_data):
    N = real_data.size(0)
    optimizer.zero_grad()

    # train on real
    # get prediction
    pred_real = discriminator(real_data)

    # calculate loss
    error_real = loss_fx(pred_real, Variable(torch.ones(N, 1)))
    
    # calculate gradients
    error_real.backward()

    # train on fake
    # get prediction
    pred_fake = discriminator(fake_data)

    # calculate loss
    error_fake = loss_fx(pred_fake, Variable(torch.ones(N, 0)))

    # calculate gradients
    error_fake.backward()

    # update weights
    optimizer.step()
    
    return error_real + error_fake, pred_real, pred_fake


# generator training
def train_generator(generator, optimizer, fake_data):
    N = fake_data.size(0)

    # zero gradients
    optimizer.zero_grad()

    # get prediction
    pred = discriminator(generator(fake_data))

    # get loss
    error = loss_fx(pred, Variable(torch.ones(N, 0)))

    # compute gradients
    error.backward()

    # update weights
    optimizer.step()

    return error


# Instance of generator and discriminator
generator = Generator()
discriminator = Discriminator()

# optimizers
g_optimizer = torch.optim.Adam(generator.parameters(), lr=0.001)
d_optimizer = torch.optim.Adam(discriminator.parameters(), lr=0.001)

# training loop
for epoch in range(epochs):
     for n_batch, batch in enumerate(data_loader, 0):
         N = batch.size(0)

         # Train Discriminator

         # REAL
         real_images = Variable(images_to_vectors(batch)).float()

         # FAKE
         fake_images = generator(generator.noise(N)).detach()

         # TRAIN
         d_error, d_pred_real, d_pred_fake = train_discriminator(
                 discriminator,
                 d_optimizer,
                 real_images,
                 fake_images
         )

         # Train Generator

         # generate noise
         fake_data = generator.noise(N)

         # get error based on discriminator
         g_error = train_generator(generator, g_optimizer, fake_data)

         # convert generator output to image and preprocess to show
         test_img = np.array(images_to_vectors(generator(fake_data), reverse=True).detach())
         test_img = test_img[0, :, :, :]
         test_img = test_img[..., ::-1]

         # show example of generated image
         cv2.imshow('GENERATED', test_img[0])
         if cv2.waitKey(1) & 0xFF == ord('q'):
             break

     print('EPOCH: {0}, D error: {1}, G error: {2}'.format(epoch, d_error, g_error))


cv2.destroyAllWindows()

# save weights
# torch.save('weights.pth')

","One can't really easily debug your training without the data and so on, but a possible problem is that your generator's last layer is a Tanh(), which means output values between -1 and 1. You probably want:

To have your real images normalized to the same range, e.g. in train_discriminator(): 
# train on real
pred_real = discriminator(real_data * 2. - 1.) # supposing real_data in [0, 1]

To re-normalize your generated data to [0, 1] before visualization/use.
# convert generator output to image and preprocess to show
test_img = np.array(
    images_to_vectors(generator(fake_data), reverse=True).detach())
test_img = test_img[0, :, :, :]
test_img = test_img[..., ::-1]
test_img = (test_img + 1.) / 2.


",pytorch
facing this error while classifying images containing 10 classes in pytorch in resnet50 my code is,"This is the code I am implementing: I am using a subset of the CalTech256 dataset to classify images of 10 different kinds of animals. We will go over the dataset preparation, data augmentation and then steps to build the classifier.
def train_and_validate(model, loss_criterion, optimizer, epochs=25):
    '''
    Function to train and validate
    Parameters
        :param model: Model to train and validate
        :param loss_criterion: Loss Criterion to minimize
        :param optimizer: Optimizer for computing gradients
        :param epochs: Number of epochs (default=25)
  
    Returns
        model: Trained Model with best validation accuracy
        history: (dict object): Having training loss, accuracy and validation loss, accuracy
    '''
    
    start = time.time()
    history = []
    best_acc = 0.0

    for epoch in range(epochs):
        epoch_start = time.time()
        print(""Epoch: {}/{}"".format(epoch+1, epochs))
        
        # Set to training mode
        model.train()
        
        # Loss and Accuracy within the epoch
        train_loss = 0.0
        train_acc = 0.0
        
        valid_loss = 0.0
        valid_acc = 0.0
        
        for i, (inputs, labels) in enumerate(train_data_loader):

            inputs = inputs.to(device)
            labels = labels.to(device)
            
            # Clean existing gradients
            optimizer.zero_grad()
            
            # Forward pass - compute outputs on input data using the model
            outputs = model(inputs)
            
            # Compute loss
            loss = loss_criterion(outputs, labels)
            
            # Backpropagate the gradients
            loss.backward()
            
            # Update the parameters
            optimizer.step()
            
            # Compute the total loss for the batch and add it to train_loss
            train_loss += loss.item() * inputs.size(0)
            
            # Compute the accuracy
            ret, predictions = torch.max(outputs.data, 1)
            correct_counts = predictions.eq(labels.data.view_as(predictions))
            
            # Convert correct_counts to float and then compute the mean
            acc = torch.mean(correct_counts.type(torch.FloatTensor))
            
            # Compute total accuracy in the whole batch and add to train_acc
            train_acc += acc.item() * inputs.size(0)
            
            #print(""Batch number: {:03d}, Training: Loss: {:.4f}, Accuracy: {:.4f}"".format(i, loss.item(), acc.item()))

            
        # Validation - No gradient tracking needed
        with torch.no_grad():

            # Set to evaluation mode
            model.eval()

            # Validation loop
            for j, (inputs, labels) in enumerate(valid_data_loader):
                inputs = inputs.to(device)
                labels = labels.to(device)

                # Forward pass - compute outputs on input data using the model
                outputs = model(inputs)

                # Compute loss
                loss = loss_criterion(outputs, labels)

                # Compute the total loss for the batch and add it to valid_loss
                valid_loss += loss.item() * inputs.size(0)

                # Calculate validation accuracy
                ret, predictions = torch.max(outputs.data, 1)
                correct_counts = predictions.eq(labels.data.view_as(predictions))

                # Convert correct_counts to float and then compute the mean
                acc = torch.mean(correct_counts.type(torch.FloatTensor))

                # Compute total accuracy in the whole batch and add to valid_acc
                valid_acc += acc.item() * inputs.size(0)

                #print(""Validation Batch number: {:03d}, Validation: Loss: {:.4f}, Accuracy: {:.4f}"".format(j, loss.item(), acc.item()))
            
        # Find average training loss and training accuracy
        avg_train_loss = train_loss/train_data_size 
        avg_train_acc = train_acc/train_data_size

        # Find average training loss and training accuracy
        avg_valid_loss = valid_loss/valid_data_size 
        avg_valid_acc = valid_acc/valid_data_size

        history.append([avg_train_loss, avg_valid_loss, avg_train_acc, avg_valid_acc])
                
        epoch_end = time.time()
    
        print(""Epoch : {:03d}, Training: Loss: {:.4f}, Accuracy: {:.4f}%, \n\t\tValidation : Loss : {:.4f}, Accuracy: {:.4f}%, Time: {:.4f}s"".format(epoch, avg_train_loss, avg_train_acc*100, avg_valid_loss, avg_valid_acc*100, epoch_end-epoch_start))
        
        # Save if the model has best accuracy till now
        torch.save(model, dataset+'_model_'+str(epoch)+'.pt')
            
    return model, history

# Load pretrained ResNet50 Model
resnet50 = models.resnet50(pretrained=True)
#resnet50 = resnet50.to('cuda:0')


# Freeze model parameters
for param in resnet50.parameters():
    param.requires_grad = False
# Change the final layer of ResNet50 Model for Transfer Learning
fc_inputs = resnet50.fc.in_features

resnet50.fc = nn.Sequential(
    nn.Linear(fc_inputs, 256),
    nn.ReLU(),
    nn.Dropout(0.4),
    nn.Linear(256, num_classes), # Since 10 possible outputs
    nn.LogSoftmax(dim=1) # For using NLLLoss()
)

# Convert model to be used on GPU
# resnet50 = resnet50.to('cuda:0')

# Change the final layer of ResNet50 Model for Transfer Learning
fc_inputs = resnet50.fc.in_features

resnet50.fc = nn.Sequential(
    nn.Linear(fc_inputs, 256),
    nn.ReLU(),
    nn.Dropout(0.4),
    nn.Linear(256, num_classes), # Since 10 possible outputs
    nn.LogSoftmax(dienter code herem=1) # For using NLLLoss()
)

# Convert model to be used on GPU
# resnet50 = resnet50.to('cuda:0')`enter code here`

Error is this:
--------------------------------------------------------------------------- 
RuntimeError                              Traceback (most recent call last) <ipython-input-30-c7a333acb694> in <module>()
      6 # Train the model for 25 epochs
      7 num_epochs = 30
----> 8 trained_model, history = train_and_validate(resnet50, loss_func, optimizer, num_epochs)
      9 
     10 torch.save(history, dataset+'_history.pt')

<ipython-input-29-239f590d090e> in train_and_validate(model, loss_criterion, optimizer, epochs)
     43 
     44             # Compute loss
---> 45             loss = loss_criterion(outputs, labels)
     46 
     47             # Backpropagate the gradients

~\Anaconda3\lib\site-packages\torch\nn\modules\module.py in
__call__(self, *input, **kwargs)
    539             result = self._slow_forward(*input, **kwargs)
    540         else:
--> 541             result = self.forward(*input, **kwargs)
    542         for hook in self._forward_hooks.values():
    543             hook_result = hook(self, input, result)

~\Anaconda3\lib\site-packages\torch\nn\modules\loss.py in forward(self, input, target)
    202 
    203     def forward(self, input, target):
--> 204         return F.nll_loss(input, target, weight=self.weight, ignore_index=self.ignore_index, reduction=self.reduction)
    205 
    206 

~\Anaconda3\lib\site-packages\torch\nn\functional.py in nll_loss(input, target, weight, size_average, ignore_index, reduce, reduction)    1836                          .format(input.size(0), target.size(0)))    1837     if dim == 2:
-> 1838         ret = torch._C._nn.nll_loss(input, target, weight, _Reduction.get_enum(reduction), ignore_index)    1839     elif dim == 4:    1840         ret = torch._C._nn.nll_loss2d(input, target, weight, _Reduction.get_enum(reduction), ignore_index)

RuntimeError: Assertion `cur_target >= 0 && cur_target < n_classes' failed.  at C:\Users\builder\AppData\Local\Temp\pip-req-build-0i480kur\aten\src\THNN/generic/ClassNLLCriterion.c:97

","This happens when there are either incorrect labels in your dataset, or the labels are 1-indexed (instead of 0-indexed). As from the error message, cur_target must be smaller than the total number of classes (10). To verify the issue, check the maximum and minimum label in your dataset. If the data is indeed 1-indexed, just minus one from all annotations and you should be fine.
Note, another possible reason is that there exists some -1 labels in the data. Some (esp older) datasets use -1 as indication of a wrong/dubious label. If you find such labels, just discard them.
",pytorch
how to solve the pytorch runtimeerror numpy is not available without upgrading numpy to the latest version because of other dependencies,"I am running a simple CNN using Pytorch for some audio classification on my Raspberry Pi 4 on Python 3.9.2 (64-bit). For the audio manipulation needed I am using librosa. librosa depends on the numba package which is only compatible with numpy version <= 1.20.
When running my code, the line
spect_tensor = torch.from_numpy(spect).double()

throws the RuntimeError:
RuntimeError: Numpy is not available

Searching the internet for solutions I found upgrading Numpy to the latest version to resolve that specific error, but throwing another error, because Numba only works with Numpy <= 1.20.
Is there a solution to this problem which does not include searching for an alternative to using librosa?
","Just wanted to give an update on my situation. I downgraded torch to version 0.9.1 which solved the original issue. Now OpenBLAS is throwing a warning because of an open MPLoop. But for now my code is up and running.
",pytorch
pytorch s3fd pga_attack problem in lossbackward to get graddata,"from detection.sfd import sfd_detector
def pgd_attack(model, input_data, eps=0.03, alpha=0.01, attack_steps=1, device='cpu'):
    ta = input_data.requires_grad_(True).to(device)
    perturbation = torch.zeros_like(input_data,requires_grad=True).to(device)

    for step in range(attack_steps):
        pred_result = model.detect_from_image((input_data + perturbation).clamp(0, 255))
        pred_box, pred_conf = [], []

        for result in pred_result:
            pred_box.append(result[:4])
            pred_conf.append(result[4])
        
        pred_conf = torch.tensor(pred_conf, dtype=torch.float32, device=device)
        potential_true_index,potential_false_index=classify_index_by_iou(pred_box,ground_truth_box,thresh_IOU=0.3)
        loss = loss_function(pred_conf,potential_true_index,potential_false_index)
        loss.backward()
        grad = perturbation.grad.data

have this error:
RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn

related code(sfd_detector.py)
models_urls = {
    's3fd': 'https://www.adrianbulat.com/downloads/python-fan/s3fd-619a316812.pth',
}


class SFDDetector(FaceDetector):
    def __init__(self, device, path_to_detector=None, verbose=False, filter_threshold=0.5):
        super(SFDDetector, self).__init__(device, verbose)

        # Initialise the face detector
        if path_to_detector is None:
            model_weights = load_url(models_urls['s3fd'])
        else:
            model_weights = torch.load(path_to_detector)

        self.fiter_threshold = filter_threshold
        self.face_detector = s3fd()
        self.face_detector.load_state_dict(model_weights)
        self.face_detector.to(device)
        self.face_detector.eval()

    def _filter_bboxes(self, bboxlist):
        if len(bboxlist) > 0:
            keep = nms(bboxlist, 0.3)
            bboxlist = bboxlist[keep, :]
            bboxlist = [x for x in bboxlist if x[-1] > self.fiter_threshold]

        return bboxlist

    # def detect_from_image(self, tensor_or_path):
    def detect_from_image(self, image):        
        # image = self.tensor_or_path_to_ndarray(tensor_or_path)

        # bboxlist = detect(self.face_detector, image, device=self.device)[0]
        bboxlist = batch_detect(self.face_detector, image, device=self.device)[0]

        bboxlist = self._filter_bboxes(bboxlist)

        return bboxlist

    def detect_from_batch(self, tensor):
        bboxlists = batch_detect(self.face_detector, tensor, device=self.device)

        new_bboxlists = []
        for i in range(bboxlists.shape[0]):
            bboxlist = bboxlists[i]
            bboxlist = self._filter_bboxes(bboxlist)
            new_bboxlists.append(bboxlist)

        return new_bboxlists

detect.py
def batch_detect(net, img_batch, device):
    """"""
    Inputs:
        - img_batch: a torch.Tensor of shape (Batch size, Channels, Height, Width)
    """"""

    if 'cuda' in device:
        torch.backends.cudnn.benchmark = True

    batch_size = img_batch.size(0)
    img_batch = img_batch.to(device, dtype=torch.float32)

    img_batch = img_batch.flip(-3)  # RGB to BGR
    img_batch = img_batch - torch.tensor([104.0, 117.0, 123.0], device=device).view(1, 3, 1, 1)

    # with torch.no_grad():
    olist = net(img_batch)  # patched uint8_t overflow error

    for i in range(len(olist) // 2):
        olist[i * 2] = F.softmax(olist[i * 2], dim=1)

    olist = [oelem.data.cpu().numpy() for oelem in olist]

    bboxlists = get_predictions(olist, batch_size)
    return bboxlists


def get_predictions(olist, batch_size):
    bboxlists = []
    variances = [0.1, 0.2]
    for i in range(len(olist) // 2):
        ocls, oreg = olist[i * 2], olist[i * 2 + 1]
        stride = 2**(i + 2)    # 4,8,16,32,64,128
        poss = zip(*np.where(ocls[:, 1, :, :] > 0.05))
        for Iindex, hindex, windex in poss:
            axc, ayc = stride / 2 + windex * stride, stride / 2 + hindex * stride
            priors = np.array([[axc / 1.0, ayc / 1.0, stride * 4 / 1.0, stride * 4 / 1.0]])
            score = ocls[:, 1, hindex, windex][:,None]
            loc = oreg[:, :, hindex, windex].copy()
            boxes = decode(loc, priors, variances)
            bboxlists.append(np.concatenate((boxes, score), axis=1))
    
    if len(bboxlists) == 0: # No candidates within given threshold
        bboxlists = np.array([[] for _ in range(batch_size)])
    else:
        bboxlists = np.stack(bboxlists, axis=1)
    return bboxlists

all of function code

def calculate_iou(box1, box2):
    x_left = max(box1[0], box2[0])
    y_top = max(box1[1], box2[1])
    x_right = min(box1[2], box2[2])
    y_bottom = min(box1[3], box2[3])

    if x_right < x_left or y_bottom < y_top:
        return 0.0  # 没有交集

    intersection_area = (x_right - x_left) * (y_bottom - y_top)
    box1_area = (box1[2] - box1[0]) * (box1[3] - box1[1])
    box2_area = (box2[2] - box2[0]) * (box2[3] - box2[1])
    # 并集面积
    union_area = box1_area + box2_area - intersection_area
    iou = intersection_area / union_area
    return iou

def classify_index_by_iou(pred_box,ground_truth_box,thresh_IOU = 0.3):
    potential_true_index = []
    potential_false_index = []

    for i,pred in enumerate(pred_box):    
        match_found=False
        for truth in ground_truth_box:
            if calculate_iou(pred,truth)>thresh_IOU:
                potential_true_index.append(i)
                match_found=True
                break
        if not match_found:
            potential_false_index.append(i)
    return potential_true_index,potential_false_index

def loss_function(pred_conf, potential_true_index, potential_false_index, threshold_false_box=1000):
    true_detection_loss = torch.tensor(0.0, dtype=torch.float32, device='cpu')
    false_detection_loss = torch.tensor(0.0, dtype=torch.float32, device='cpu')
    # true_detection_loss=torch.log(1-pred_conf[potential_true_index]).sum()
    # false_detection_loss=torch.log(pred_conf[potential_false_index[:threshold_false_box]]).sum()
    for i in potential_true_index:
        true_detection_loss -= torch.log(1 - pred_conf[i])
    for j in potential_false_index[:threshold_false_box]:
        false_detection_loss -= torch.log(pred_conf[j])
    loss = torch.add(true_detection_loss,false_detection_loss)
    print(loss)
    return loss

I just want to get grad.data information properly. I really don't know what is the problem. did I need to change all of numpy function to pytorch?
","check if the loss before loss.backward() requires grad by printing loss.requires_grad. If not you should check in the loss calculation function if:

Any of your for loop is called?
If yes, then check if any pred_conf[i] requires grad?

From what I see, your function in detect.py convert tensor to numpy and python, which break the gradient chain. That should be why your loss doesn't require grad.
",pytorch
loss is increasing from first epoch itself,"I am training my siamese network for nlp. I have used lstm in it. and BCELoss. My loss is increasing from the first epoch. The first 36 epoch loss is
error after  0 is 
 272.4357
[torch.FloatTensor of size 1]

error after  1 is 
 271.8972
[torch.FloatTensor of size 1]

error after  2 is 
 271.5598
[torch.FloatTensor of size 1]

error after  3 is 
 271.6979
[torch.FloatTensor of size 1]

error after  4 is 
 271.7315
[torch.FloatTensor of size 1]

error after  5 is 
 272.3965
[torch.FloatTensor of size 1]

error after  6 is 
 273.3982
[torch.FloatTensor of size 1]

error after  7 is 
 275.1197
[torch.FloatTensor of size 1]

error after  8 is 
 275.8228
[torch.FloatTensor of size 1]

error after  9 is 
 278.3311
[torch.FloatTensor of size 1]

error after  10 is 
 277.1054
[torch.FloatTensor of size 1]

error after  11 is 
 277.8418
[torch.FloatTensor of size 1]

error after  12 is 
 279.0189
[torch.FloatTensor of size 1]

error after  13 is 
 278.4090
[torch.FloatTensor of size 1]

error after  14 is 
 281.8813
[torch.FloatTensor of size 1]

error after  15 is 
 283.4077
[torch.FloatTensor of size 1]

error after  16 is 
 286.3093
[torch.FloatTensor of size 1]

error after  17 is 
 287.6292
[torch.FloatTensor of size 1]

error after  18 is 
 297.2318
[torch.FloatTensor of size 1]

error after  19 is 
 307.4176
[torch.FloatTensor of size 1]

error after  20 is 
 304.6649
[torch.FloatTensor of size 1]

error after  21 is 
 328.9772
[torch.FloatTensor of size 1]

error after  22 is 
 300.0669
[torch.FloatTensor of size 1]

error after  23 is 
 292.3902
[torch.FloatTensor of size 1]

error after  24 is 
 300.8633
[torch.FloatTensor of size 1]

error after  25 is 
 305.1822
[torch.FloatTensor of size 1]

error after  26 is 
 333.9984
[torch.FloatTensor of size 1]

error after  27 is 
 346.2062
[torch.FloatTensor of size 1]

error after  28 is 
 354.6148
[torch.FloatTensor of size 1]

error after  29 is 
 341.3568
[torch.FloatTensor of size 1]

error after  30 is 
 369.7580
[torch.FloatTensor of size 1]

error after  31 is 
 366.1615
[torch.FloatTensor of size 1]

error after  32 is 
 368.2455
[torch.FloatTensor of size 1]

error after  33 is 
 391.4102
[torch.FloatTensor of size 1]

error after  34 is 
 394.3190
[torch.FloatTensor of size 1]

error after  35 is 
 401.0990
[torch.FloatTensor of size 1]

error after  36 is 
 422.3723
[torch.FloatTensor of size 1]

","Probably your learning rate is too high. Try decreasing your learning rate. A too large learning rate is the most common reason for loss increasing from the first epoch. 
Also your loss is very high. It is unusual to have such a high lost. You probably have a sum in your loss function, it might be wiser to replace that sum with a mean. While this makes no difference if you use the Adam optimizer, if you use simple SGD with or without momentum using a sum instead of a mean, means that you will need to tune your learning rate differently if the dimensions (or the length of your sequence processed by your lstm) of your system changes.
",pytorch
does pytorch cnn care about image size,"I am playing with CNNs these days, and I have code like pasted below. My question is, would this work on any image size? It is not clear to me what parameter or channel, if any, cares about the image size? And if that's the case, how does the model know how many neurons it needs, isn't that a function of image size?
Related point on pretrained models - if I use pretrained models, do I need to reformat my images to be same as what the model was trained on in the first place, or how does that work?
class CNN(nn.Module):
    def __init__(self, num_classes, num_channels=1):
        super(CNN, self).__init__()
        self.num_classes = num_classes
        self.conv1 = nn.Conv2d(num_channels, 32, kernel_size=3, padding=1)
        self.relu1 = nn.ReLU()
        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)
        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)
        self.relu2 = nn.ReLU()
        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)
        self.fc = nn.Linear(64*7*7, num_classes)

","
No, but the Conv2D() layer can work on variable image sizes. The way the Conv2D() layer works is by applying the kernel on each set of kernel_size x kernel_size pixels. The padding adds to the dimensions before the convolution as well. So as long as the image dimensions are at least kernel_size x kernel_size after padding, the convolution will work. In this case, kernel_size is 3 and padding is 1 so even a single pixel will work because after padding, the image will be 3x3. The reason this CNN can't take variable image sizes is because of the linear layer. The linear layer requires a 7x7 image after max-pooling twice, so a 28x28 image to start with.

It depends on the type of layer, linear layers take in a set input size and a set output size, which determines the number of parameters (neurons). The number of parameters for convolution layers are determined by the kernel size and number of output channels so it doesn't rely on input size.

It depends on the model, some models allow variable image sizes even if they were trained on a specific resolution.


",pytorch
why there is 39unpickling error39 when using polars to read data for pytorch,"I have changed my data tool from xarray to polars in recent, and use pl.DataFrame.to_torch() to generate tensor for training my Pytorch model. Data source's format is parquet file.
For avoiding fork child processes, I use torch.multiprocessing.spawn to start my training process, however the process crashed with this:
/home/username/.conda/envs/torchhydro1/bin/python3.11 -X pycache_prefix=/home/username/.cache/JetBrains/IntelliJIdea2024.3/cpython-cache /home/username/.local/share/JetBrains/IntelliJIdea2024.3/python-ce/helpers/pydev/pydevd.py --multiprocess --qt-support=auto --port 29781 --file /home/username/torchhydro/experiments/train_with_era5land_gnn_ddp.py 
Console output is saving to: /home/username/torchhydro/experiments/results/train_gnn_ddp.txt
[20:38:51] DEBUG    No module named 'forge'                     signatures.py:43
           DEBUG    No module named 'forge'                     signatures.py:43
[20:38:52] DEBUG    Using selector: EpollSelector          selector_events.py:54
           ……
           DEBUG    Using fontManager instance from         font_manager.py:1580
                    /home/username/.cache/matplotlib/fontl                     
                    ist-v390.json                                               
update config file
!!!!!!NOTE!!!!!!!!
-------Please make sure the PRECIPITATION variable is in the 1st location in var_t setting!!---------
If you have POTENTIAL_EVAPOTRANSPIRATION, please set it the 2nd!!!-
!!!!!!NOTE!!!!!!!!
-------Please make sure the STREAMFLOW variable is in the 1st location in var_out setting!!---------
[20:39:04] DEBUG    No module named 'forge'                     signatures.py:43
           DEBUG    No module named 'forge'                     signatures.py:43
[20:39:06] DEBUG    Using selector: EpollSelector          selector_events.py:54
           ……
           DEBUG    Using fontManager instance from         font_manager.py:1580
                    /home/username/.cache/matplotlib/fontl                     
                    ist-v390.json                                               
……
Torch is using cuda:0
[2024-12-12 20:48:08,931] torch.distributed.distributed_c10d: [INFO] Using backend config: {'cuda': 'nccl'}
[W CUDAAllocatorConfig.h:30] Warning: expandable_segments not supported on this platform (function operator())
using 8 workers
Pin memory set to True
  0%|          | 0/22986 [00:00<?, ?it/s]
[20:48:40] DEBUG    No module named 'forge'                     signatures.py:43
           DEBUG    No module named 'forge'                     signatures.py:43
[20:48:41] DEBUG    Using selector: EpollSelector          selector_events.py:54
           ……
           DEBUG    Using fontManager instance from         font_manager.py:1580
                    /home/username/.cache/matplotlib/fontl                     
                    ist-v390.json                                               
[20:49:28] DEBUG    No module named 'forge'                     signatures.py:43
           DEBUG    No module named 'forge'                     signatures.py:43
[20:49:29] DEBUG    Using selector: EpollSelector          selector_events.py:54
           ……
           DEBUG    Using fontManager instance from         font_manager.py:1580
                    /home/username/.cache/matplotlib/fontl                     
                    ist-v390.json                                               
[20:50:19] DEBUG    No module named 'forge'                     signatures.py:43
           DEBUG    No module named 'forge'                     signatures.py:43
[20:50:20] DEBUG    Using selector: EpollSelector          selector_events.py:54
           ……
           DEBUG    Using fontManager instance from         font_manager.py:1580
                    /home/username/.cache/matplotlib/fontl                     
                    ist-v390.json                                               
[20:51:12] DEBUG    No module named 'forge'                     signatures.py:43
           DEBUG    No module named 'forge'                     signatures.py:43
[20:51:13] DEBUG    Using selector: EpollSelector          selector_events.py:54
           ……
           DEBUG    Using fontManager instance from         font_manager.py:1580
                    /home/username/.cache/matplotlib/fontl                     
                    ist-v390.json                                               
[20:52:07] DEBUG    No module named 'forge'                     signatures.py:43
           DEBUG    No module named 'forge'                     signatures.py:43
[20:52:09] DEBUG    Using selector: EpollSelector          selector_events.py:54
           ……
[20:52:13] DEBUG    CACHEDIR=/home/username/.cache/matplotlib   __init__.py:341
           DEBUG    Using fontManager instance from         font_manager.py:1580
                    /home/username/.cache/matplotlib/fontl                     
                    ist-v390.json                                               
[20:53:11] DEBUG    No module named 'forge'                     signatures.py:43
           DEBUG    No module named 'forge'                     signatures.py:43
[20:53:12] DEBUG    Using selector: EpollSelector          selector_events.py:54
           ……
           DEBUG    Using fontManager instance from         font_manager.py:1580
                    /home/username/.cache/matplotlib/fontl                     
                    ist-v390.json                                               
[20:55:12] DEBUG    No module named 'forge'                     signatures.py:43
           DEBUG    No module named 'forge'                     signatures.py:43
[20:55:14] DEBUG    Using selector: EpollSelector          selector_events.py:54
           ……
[20:55:19] DEBUG    CACHEDIR=/home/username/.cache/matplotlib   __init__.py:341
           DEBUG    Using fontManager instance from         font_manager.py:1580
                    /home/username/.cache/matplotlib/fontl                     
                    ist-v390.json                                               
Traceback (most recent call last):
  File ""/home/username/.local/share/JetBrains/IntelliJIdea2024.3/python-ce/helpers/pydev/pydevd.py"", line 1570, in _exec
    pydev_imports.execfile(file, globals, locals)  # execute the script
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/username/.local/share/JetBrains/IntelliJIdea2024.3/python-ce/helpers/pydev/_pydev_imps/_pydev_execfile.py"", line 18, in execfile
    exec(compile(contents+""\n"", file, 'exec'), glob, loc)
  File ""/home/username/torchhydro/experiments/train_with_era5land_gnn_ddp.py"", line 171, in <module>
    test_run_model()
  File ""/home/username/torchhydro/experiments/train_with_era5land_gnn_ddp.py"", line 56, in test_run_model
    mp.spawn(gnn_train_worker, args=(world_size, config_data, None), nprocs=world_size, join=True)
  File ""/home/username/.conda/envs/torchhydro1/lib/python3.11/site-packages/torch/multiprocessing/spawn.py"", line 241, in spawn
    return start_processes(fn, args, nprocs, join, daemon, start_method=""spawn"")
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File ""/home/username/.conda/envs/torchhydro1/lib/python3.11/site-packages/torch/multiprocessing/spawn.py"", line 197, in start_processes
    while not context.join():
              ^^^^^^^^^^^^^^
  File ""/home/username/.conda/envs/torchhydro1/lib/python3.11/site-packages/torch/multiprocessing/spawn.py"", line 140, in join
    raise ProcessExitedException(
torch.multiprocessing.spawn.ProcessExitedException: process 0 terminated with signal SIGKILL
python-BaseException
Traceback (most recent call last):
  File ""/home/username/.conda/envs/torchhydro1/lib/python3.11/multiprocessing/spawn.py"", line 132, in _main
    self = reduction.pickle.load(from_parent)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
_pickle.UnpicklingError: pickle data was truncated
python-BaseException
/home/username/.conda/envs/torchhydro1/lib/python3.11/multiprocessing/resource_tracker.py:254: UserWarning: resource_tracker: There appear to be 30 leaked semaphore objects to clean up at shutdown
  warnings.warn('resource_tracker: There appear to be %d '

Now I have 2 problems:
First, why it will appears _pickle.UnpicklingError?
Second, after executing 0%|          | 0/22986 [00:00<?, ?it/s], there is 7 ……s in my process log, means that this DEBUG process has been repeated for 8 or 9 times! I have set num_worker of pytorch DataLoader to 8, does this problem have connection with num_worker?
This problem occurs after I'm using polars, so I think problem comes from polars, or threads in polars and pytorch have some mistakes.
But how to know why there is UnpicklingError and solve it? Hope for your reply.
","It's mistake to filter polars.Dataframe and convert result to torch.Tensor in __get_item__ of torch.Dataset.
Convert the whole dataframe to tensor solved the problem.
",pytorch
how to convert a pytorch tensor into a numpy array,"How do I convert a torch tensor to numpy?
","copied from pytorch doc:
a = torch.ones(5)
print(a)


tensor([1., 1., 1., 1., 1.])

b = a.numpy()
print(b)


[1. 1. 1. 1. 1.]


Following from the below discussion with @John:
In case the tensor is (or can be) on GPU, or in case it (or it can) require grad, one can use
t.detach().cpu().numpy()

I recommend to uglify your code only as much as required.
",pytorch
what39s the difference between quothiddenquot and quotoutputquot in pytorch lstm,"I'm having trouble understanding the documentation for PyTorch's LSTM module (and also RNN and GRU, which are similar). Regarding the outputs, it says:

Outputs: output, (h_n, c_n)

output (seq_len, batch, hidden_size * num_directions): tensor containing the output features (h_t) from the last layer of the RNN, for each t. If a torch.nn.utils.rnn.PackedSequence has been given as the input, the output will also be a packed sequence.
h_n (num_layers * num_directions, batch, hidden_size): tensor containing the hidden state for t=seq_len
c_n (num_layers * num_directions, batch, hidden_size): tensor containing the cell state for t=seq_len


It seems that the variables output and h_n both give the values of the hidden state. Does h_n just redundantly provide the last time step that's already included in output, or is there something more to it than that?
","I made a diagram. The names follow the PyTorch docs, although I renamed num_layers to w.
output comprises all the hidden states in the last layer (""last"" depth-wise, not time-wise). (h_n, c_n) comprises the hidden states after the last timestep, t = n, so you could potentially feed them into another LSTM.

The batch dimension is not included.
",pytorch
pytorch torchvisiondatasetsimagefolder filenotfounderror found no valid file for the classes ipynb_checkpoints,"Tried to load training data with pytorch torch.datasets.ImageFolder in Colab.
transform = transforms.Compose([transforms.Resize(400),
                                transforms.ToTensor()])
dataset_path = 'ss/'
dataset = datasets.ImageFolder(root=dataset_path, transform=transform)
dataloader = torch.utils.data.DataLoader(dataset, batch_size=20)

I encountered the following error :
---------------------------------------------------------------------------
FileNotFoundError                         Traceback (most recent call last)
<ipython-input-27-7abcc1f434b1> in <module>()
      2                                 transforms.ToTensor()])
      3 dataset_path = 'ss/'
----> 4 dataset = datasets.ImageFolder(root=dataset_path, transform=transform)
      5 dataloader = torch.utils.data.DataLoader(dataset, batch_size=20)

3 frames
/usr/local/lib/python3.7/dist-packages/torchvision/datasets/folder.py in make_dataset(directory, class_to_idx, extensions, is_valid_file)
    100         if extensions is not None:
    101             msg += f""Supported extensions are: {', '.join(extensions)}""
--> 102         raise FileNotFoundError(msg)
    103 
    104     return instances

FileNotFoundError: Found no valid file for the classes .ipynb_checkpoints. Supported extensions are: .jpg, .jpeg, .png, .ppm, .bmp, .pgm, .tif, .tiff, .webp

My Dataset folder contains a subfolder with many training images in png format, still the ImageFolder can't access them.
","I encountered the same problem when I was using IPython notebook-like tools.
First please check if there is any hidden files under your dataset_path. Use ls -a if you are under a Linux environment.
The case happen to me is I found a hidden file called .ipynb_checkpoints which is located parallelly to image class subfolders. I think that file causes confusion to PyTorch dataset. I made sure it is not useful so I simply deleted it. Then the dataset works fine.
Or if you would like to simply ignore that file, you may also try this.
",pytorch
pytorch fails with cuda error deviceside assert triggered on colab,"I am trying to initialize a tensor on Google Colab with GPU enabled.
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

t = torch.tensor([1,2], device=device)

But I am getting this strange error.
RuntimeError: CUDA error: device-side assert triggered
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1

Even by setting that environment variable to 1 seems not showing any further details.
Anyone ever had this issue?
","While I tried your code, and it did not give me an error, I can say that usually the best practice to debug CUDA Runtime Errors: device-side assert like yours is to turn collab to CPU and recreate the error. It will give you a more useful traceback error.
Most of the time CUDA Runtime Errors can be the cause of some index mismatching so like you tried to train a network with 10 output nodes on a dataset with 15 labels. And the thing with this CUDA error is once you get this error once, you will recieve it for every operation you do with torch.tensors. This forces you to restart your notebook.
I suggest you restart your notebook, get a more accuracate traceback by moving to CPU, and check the rest of your code especially if you train a model on set of targets somewhere.
To gain a clearer insight into the typical utilization of GPUs in PyTorch applications, I recommend exploring deep learning projects on GitHub. Websites such as repo-rift.com can be particularly useful for this purpose. They allow you to perform text searches with queries like ""How does this paper use GPU"". This can help you pinpoint the exact usage of CUDA in specific lines of code within extensive repositories.
",pytorch
pytorch how to add l1 regularizer to activations,"I would like to add the L1 regularizer to the activations output from a ReLU.
More generally, how does one add a regularizer only to a particular layer in the network?


Related material:

This similar post refers to adding L2 regularization, but it appears to add the regularization penalty to all layers of the network.

nn.modules.loss.L1Loss() seems relevant, but I do not yet understand how to use this.

The legacy module L1Penalty seems relevant also, but why has it been deprecated?



","Here is how you do this:

In your Module's forward return final output and layers' output for which you want to apply L1 regularization
loss variable will be sum of cross entropy loss of output w.r.t. targets and L1 penalties.

Here's an example code
import torch
from torch.autograd import Variable
from torch.nn import functional as F


class MLP(torch.nn.Module):
    def __init__(self):
        super(MLP, self).__init__()
        self.linear1 = torch.nn.Linear(128, 32)
        self.linear2 = torch.nn.Linear(32, 16)
        self.linear3 = torch.nn.Linear(16, 2)

    def forward(self, x):
        layer1_out = F.relu(self.linear1(x))
        layer2_out = F.relu(self.linear2(layer1_out))
        out = self.linear3(layer2_out)
        return out, layer1_out, layer2_out

batchsize = 4
lambda1, lambda2 = 0.5, 0.01

model = MLP()
optimizer = torch.optim.SGD(model.parameters(), lr=1e-4)

# usually following code is looped over all batches 
# but let's just do a dummy batch for brevity

inputs = Variable(torch.rand(batchsize, 128))
targets = Variable(torch.ones(batchsize).long())

optimizer.zero_grad()
outputs, layer1_out, layer2_out = model(inputs)
cross_entropy_loss = F.cross_entropy(outputs, targets)

all_linear1_params = torch.cat([x.view(-1) for x in model.linear1.parameters()])
all_linear2_params = torch.cat([x.view(-1) for x in model.linear2.parameters()])
l1_regularization = lambda1 * torch.norm(all_linear1_params, 1)
l2_regularization = lambda2 * torch.norm(all_linear2_params, 2)

loss = cross_entropy_loss + l1_regularization + l2_regularization
loss.backward()
optimizer.step()

",pytorch
huggingface transformers for text generation with ctrl with google colab39s free gpu,"I wanted to test TextGeneration with CTRL using PyTorch-Transformers, before using it for fine-tuning. But it doesn't prompt anything like it does with GPT-2 and other similar language generation models. I'm very new for this and am stuck and can't figure out what's going on.
This is the procedure I followed in my Colab notebook,
!pip install transformers

!git clone https://github.com/huggingface/pytorch-transformers.git

!python pytorch-transformers/examples/run_generation.py \
    --model_type=ctrl \
    --length=100 \
    --model_name_or_path=ctrl \
    --temperature=0.2 \
    --repetition_penalty=1.2 \

And this is what I get after running the script
02/10/2020 01:02:31 - INFO - transformers.tokenization_utils -   loading file https://raw.githubusercontent.com/salesforce/ctrl/master/ctrl-vocab.json from cache at /root/.cache/torch/transformers/a858ad854d3847b02da3aac63555142de6a05f2a26d928bb49e881970514e186.285c96a541cf6719677cfb634929022b56b76a0c9a540186ba3d8bbdf02bca42
02/10/2020 01:02:31 - INFO - transformers.tokenization_utils -   loading file https://raw.githubusercontent.com/salesforce/ctrl/master/ctrl-merges.txt from cache at /root/.cache/torch/transformers/aa2c569e6648690484ade28535a8157aa415f15202e84a62e82cc36ea0c20fa9.26153bf569b71aaf15ae54be4c1b9254dbeff58ca6fc3e29468c4eed078ac142
02/10/2020 01:02:31 - INFO - transformers.configuration_utils -   loading configuration file https://storage.googleapis.com/sf-ctrl/pytorch/ctrl-config.json from cache at /root/.cache/torch/transformers/d6492ca334c2a4e079f43df30956acf935134081b2b3844dc97457be69b623d0.1ebc47eb44e70492e0c20494a084f108332d20fea7fe5ad408ef5e7a8f2baef4
02/10/2020 01:02:31 - INFO - transformers.configuration_utils -   Model config CTRLConfig {
  ""architectures"": null,
  ""attn_pdrop"": 0.1,
  ""bos_token_id"": 0,
  ""dff"": 8192,
  ""do_sample"": false,
  ""embd_pdrop"": 0.1,
  ""eos_token_ids"": 0,
  ""finetuning_task"": null,
  ""from_tf"": false,
  ""id2label"": {
    ""0"": ""LABEL_0""
  },
  ""initializer_range"": 0.02,
  ""is_decoder"": false,
  ""label2id"": {
    ""LABEL_0"": 0
  },
  ""layer_norm_epsilon"": 1e-06,
  ""length_penalty"": 1.0,
  ""max_length"": 20,
  ""model_type"": ""ctrl"",
  ""n_ctx"": 512,
  ""n_embd"": 1280,
  ""n_head"": 16,
  ""n_layer"": 48,
  ""n_positions"": 50000,
  ""num_beams"": 1,
  ""num_labels"": 1,
  ""num_return_sequences"": 1,
  ""output_attentions"": false,
  ""output_hidden_states"": false,
  ""output_past"": true,
  ""pad_token_id"": 0,
  ""pruned_heads"": {},
  ""repetition_penalty"": 1.0,
  ""resid_pdrop"": 0.1,
  ""summary_activation"": null,
  ""summary_first_dropout"": 0.1,
  ""summary_proj_to_labels"": true,
  ""summary_type"": ""cls_index"",
  ""summary_use_proj"": true,
  ""temperature"": 1.0,
  ""top_k"": 50,
  ""top_p"": 1.0,
  ""torchscript"": false,
  ""use_bfloat16"": false,
  ""vocab_size"": 246534
}

02/10/2020 01:02:31 - INFO - transformers.modeling_utils -   loading weights file https://storage.googleapis.com/sf-ctrl/pytorch/seqlen256_v1.bin from cache at /root/.cache/torch/transformers/c146cc96724f27295a0c3ada1fbb3632074adf87e9aef8269e44c9208787f8c8.b986347cbab65fa276683efbb9c2f7ee22552277bcf6e1f1166557ed0852fdf0
tcmalloc: large alloc 1262256128 bytes == 0x38b92000 @  0x7fe1900bdb6b 0x7fe1900dd379 0x7fe139843b4a 0x7fe1398455fa 0x7fe13bb7578a 0x7fe13bdbe30b 0x7fe13be05b37 0x7fe184c8cad5 0x7fe184c8d17b 0x7fe184c91160 0x7fe184ade496 0x551b15 0x5aa6ec 0x50abb3 0x50c5b9 0x508245 0x5096b7 0x595311 0x54a6ff 0x551b81 0x5aa6ec 0x50abb3 0x50c5b9 0x508245 0x509642 0x595311 0x54a6ff 0x551b81 0x5aa6ec 0x50abb3 0x50c5b9
tcmalloc: large alloc 1262256128 bytes == 0x19fdda000 @  0x7fe1900bdb6b 0x7fe1900dd379 0x7fe139843b4a 0x7fe1398455fa 0x7fe13bb7578a 0x7fe13bdbe30b 0x7fe13be05b37 0x7fe184c8cad5 0x7fe184c8d17b 0x7fe184c91160 0x7fe184ade496 0x551b15 0x5aa6ec 0x50abb3 0x50c5b9 0x508245 0x509642 0x595311 0x54a6ff 0x551b81 0x5aa6ec 0x50abb3 0x50d390 0x508245 0x509642 0x595311 0x54a6ff 0x551b81 0x5a067e 0x50d966 0x508245
^C

and then terminates. Could this be because of a GPU problem?
","The solution was to increase the RAM. Since I was using the Google Colab's free GPU, I was going through this: GitHub issue
and found this useful: Solution
The following piece of code will crash the session in Colab and select 'Get more RAM', which will increase the RAM up to 25.51GB
d=[]
while(1):
  d.append('1')

",pytorch
valueerror target size torchsize16 must be the same as input size torchsize16 1,"ValueError                                Traceback (most recent call last)
<ipython-input-30-33821ccddf5f> in <module>
     23         output = model(data)
     24         # calculate the batch loss
---> 25         loss = criterion(output, target)
     26         # backward pass: compute gradient of the loss with respect to model parameters
     27         loss.backward()

C:\Users\mnauf\Anaconda3\envs\federated_learning\lib\site-packages\torch\nn\modules\module.py in __call__(self, *input, **kwargs)
    487             result = self._slow_forward(*input, **kwargs)
    488         else:
--> 489             result = self.forward(*input, **kwargs)
    490         for hook in self._forward_hooks.values():
    491             hook_result = hook(self, input, result)

C:\Users\mnauf\Anaconda3\envs\federated_learning\lib\site-packages\torch\nn\modules\loss.py in forward(self, input, target)
    593                                                   self.weight,
    594                                                   pos_weight=self.pos_weight,
--> 595                                                   reduction=self.reduction)
    596 
    597 

C:\Users\mnauf\Anaconda3\envs\federated_learning\lib\site-packages\torch\nn\functional.py in binary_cross_entropy_with_logits(input, target, weight, size_average, reduce, reduction, pos_weight)
   2073 
   2074     if not (target.size() == input.size()):
-> 2075         raise ValueError(""Target size ({}) must be the same as input size ({})"".format(target.size(), input.size()))
   2076 
   2077     return torch.binary_cross_entropy_with_logits(input, target, weight, pos_weight, reduction_enum)

ValueError: Target size (torch.Size([16])) must be the same as input size (torch.Size([16, 1]))

I am training a CNN. Working on the Horses vs humans dataset. This is my code. I am using criterion = nn.BCEWithLogitsLoss() and optimizer = optim.RMSprop(model.parameters(), lr=0.01). My final layer is self.fc2 = nn.Linear(512, 1). Out last neuron, will output 1 for horse and 0 for human, right? or should I choose 2 neurons for output?
16 is the batch size. Since the error says ValueError: Target size (torch.Size([16])) must be the same as input size (torch.Size([16, 1])). I don't understand, where do I need to make change, to rectify the error.
","target = target.unsqueeze(1), before passing target to criterion, changed the target tensor size from [16] to [16,1]. Doing it solved the issue. Furthermore, I also needed to do target = target.float() before passing it to criterion, because our outputs are in float. Besides, there was another error in the code. I was using sigmoid activation function in the last layer, but I shouldn’t because the criterion I am using already comes with sigmoid builtin.
",pytorch
how to find the indexes of the first n maximum values of a tensor,"I know that torch.argmax(x, dim = 0) returns the index of the first maximum value in x along dimension 0. But is there an efficient way to return the indexes of the first n maximum values? If there are duplicate values I also want the index of those among the n indexes.
As a concrete example, say x=torch.tensor([2, 1, 4, 1, 4, 2, 1, 1]). I would like a function
generalized_argmax(xI torch.tensor, n: int)

such that
generalized_argmax(x, 4)
returns [0, 2, 4, 5] in this example.
","To acquire all you need you have to go over the whole tensor. The most efficient should therefore be to use argsort afterwards limited to n entries.
>>> x=torch.tensor([2, 1, 4, 1, 4, 2, 1, 1])
>>> x.argsort(dim=0, descending=True)[:n]
[2, 4, 0, 5]

Sort it again to get [0, 2, 4, 5] if you need the ascending order of indices.
",pytorch
detectron2 installation  no module named 39torch39,"I am trying to install detectron2 on ubuntu and face a weird python dependency problem. In short - pytorch is installed (with pip), torchvision is installed (with pip), but when I run
pip install 'git+https://github.com/facebookresearch/detectron2.git'

I get error ModuleNotFoundError: No module named 'torch'
as for dependencies
(detectron2_test) ubuntu@LAPTOP:~$ pip install torchvision
Requirement already satisfied: torchvision in ./detectron2_test/lib/python3.12/site-packages (0.19.1+cu118)
Requirement already satisfied: numpy in ./detectron2_test/lib/python3.12/site-packages (from torchvision) (1.26.3)
Requirement already satisfied: torch==2.4.1 in ./detectron2_test/lib/python3.12/site-packages (from torchvision) (2.4.1+cu118)
(...)

(detectron2_test) ubuntu@LAPTOP:~$ which pip
/home/ubuntu/detectron2_test/bin/pip
(detectron2_test) ubuntu@LAPTOP:~$ which python
/home/ubuntu/detectron2_test/bin/python
(detectron2_test) ubuntu@LAPTOP:~$ which python3
/home/ubuntu/detectron2_test/bin/python3

Any suggestions are appreciated!
","This is probably due to the isolation mechanism of the pip building process.
Basically, the installation requires torch to be installed to work, but recent versions of pip use some isolation that does not allow the build process to access installed packages.
You can disable that isolation by using this command:
$ pip install --no-build-isolation 'git+https://github.com/facebookresearch/detectron2.git'

This happens a lot for packages that need torch, probably because they tend to verify torch version and also import it to check for cuda and/or other capabilities, or to compile some kernels.
",pytorch
where do i get a cpuonly version of pytorch,"I'm trying to get a basic app running with Flask + PyTorch, and host it on Heroku. However, I run into the issue that the maximum slug size is 500mb on the free version, and PyTorch itself is ~500mb.
After some google searching, someone wrote about finding a cpu-only version of PyTorch, and using that, which is much smaller how-and-why-i-built-an-ml-based-python-api-hosted-on-heroku-j74qbfwn1.
However, I'm pretty lost as to how this is done, and the person didn't document this at all. Any advice is appreciated, thanks.
EDIT:
To be more specific about my problem, I tried installing torch by (as far as I understand), including a requirements.txt which listed torch as a dependency. Current I have: torch==0.4.1. However this doesn't work bc of size.
My question is, do you know what I could write in the requirements file to get the cpu-only version of torch that is smaller, or alternatively, if the requirements.txt doesn't work for this, what I would do instead, to get the cpu version.
","Per the Pytorch website, you can install pytorch-cpu with
conda install pytorch-cpu torchvision-cpu -c pytorch

You can see from the files on Anaconda cloud, that the size varies between 26 and 56MB depending on the OS where you want to install it.
You can get the wheel from http://download.pytorch.org/whl/cpu/.
The wheel is 87MB.
You can setup the installation by putting the link to the wheel in the requirements.txt file. If you use Python 3.6 on Heroku:
http://download.pytorch.org/whl/cpu/torch-0.4.1-cp36-cp36m-linux_x86_64.whl

otherwise, for Python 2.7:
http://download.pytorch.org/whl/cpu/torch-0.4.1-cp27-cp27mu-linux_x86_64.whl

For example if your requirements are pytorch-cpu, numpy and scipy and you're using Python 3.6, the requirements.txt would look like:
http://download.pytorch.org/whl/cpu/torch-0.4.1-cp36-cp36m-linux_x86_64.whl
numpy
scipy

",pytorch
how can one obtain the quotcorrectquot embedding layer in bert,"I want to utilize BERT to assess the similarity between two pieces of text:
from transformers import AutoTokenizer, AutoModel
import torch
import torch.nn.functional as F
import numpy as np

tokenizer = AutoTokenizer.from_pretrained(""bert-base-chinese"")
model = AutoModel.from_pretrained(""bert-classifier"")

def calc_similarity(s1, s2):
    inputs = tokenizer(s1, s2, return_tensors='pt', padding=True, truncation=True)

    with torch.no_grad():
        outputs = model(**inputs)
        embeddings = outputs.last_hidden_state[:, 0, :].cpu().numpy()

    cosine_similarity = F.cosine_similarity(embeddings[0], embeddings[1])
    return cosine_similarity

The similarity presented here is derived from a BERT sentiment classifier, which is a model fine-tuned based on the BERT architecture.
My inquiry primarily revolves around this line of code：
embeddings = outputs.last_hidden_state[:, 0, :].cpu().numpy()

I have observed at least three different implementations regarding this line; in addition to the aforementioned version that retrieves the first row, there are two other variations:
embeddings = outputs.last_hidden_state.mean(axis=1).cpu().numpy()

and
embeddings = model.bert.pooler(outputs.last_hidden_state.cpu().numpy())

In fact, vector outputs.last_hidden_state is a 9*768 tensor, and the three aforementioned methods can transform it into a 1*768 vector, thereby providing a basis for subsequent similarity calculations. From my perspective, the first approach is not appropriate within the semantic space defined by the classification task, as our objective is not to predict the next word. What perplexes me is the choice between the second and third methods, specifically whether to employ a simple average or to utilize the pooling layer of the model itself.
Any assistance would be greatly appreciated!
","1st approach is not a good choice because leveraging the [CLS] token embedding directly might not be the best approach, in case if the BERT  was fine tuned for a task other than similarity matching.

Task-Specific Embeddings: The [CLS] token embedding is affected by the task the bert model was trained on.
Averaging : Taking the mean of all token embeddings, we can get a more general representation of the input. This method balances out the representation by considering the contextual embeddings of all tokens.

Consider taking average or pooling (passing through another dense layer) will work.
",pytorch
error in getting captum text explanations for text classification,"I have the following code that I am using to identify the most influential words used to correctly predict the text in the test dataset
import pandas as pd
import torch
from torch.utils.data import DataLoader
from transformers import BertTokenizer, BertForSequenceClassification, AdamW
from sklearn.metrics import accuracy_score
from captum.attr import IntegratedGradients

# Loading data
train_df = pd.read_csv('train_dataset.csv')
test_df = pd.read_csv('test_dataset.csv')

# Tokenizer
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')

def preprocess_data(df, tokenizer, max_len=128):
    inputs = tokenizer(list(df['text']), padding=True, truncation=True, max_length=max_len, return_tensors=""pt"")
    labels = torch.tensor(df['label'].values)
    return inputs, labels

train_inputs, train_labels = preprocess_data(train_df, tokenizer)
test_inputs, test_labels = preprocess_data(test_df, tokenizer)

# DataLoader
train_dataset = torch.utils.data.TensorDataset(train_inputs['input_ids'], train_inputs['attention_mask'], train_labels)
train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)

test_dataset = torch.utils.data.TensorDataset(test_inputs['input_ids'], test_inputs['attention_mask'], test_labels)
test_loader = DataLoader(test_dataset, batch_size=16)

# Model setup
device = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")
model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2).to(device)

# Optimizer
optimizer = AdamW(model.parameters(), lr=5e-5)

# Training Loop
model.train()
for epoch in range(3):  # Train for 3 epochs
    for batch in train_loader:
        input_ids, attention_mask, labels = [x.to(device) for x in batch]
        optimizer.zero_grad()
        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)
        loss = outputs.loss
        loss.backward()
        optimizer.step()
    print(f""Epoch {epoch+1} loss: {loss.item()}"")

# Evaluation
model.eval()
correct_predictions = []
with torch.no_grad():
    for batch in test_loader:
        input_ids, attention_mask, labels = [x.to(device) for x in batch]
        outputs = model(input_ids, attention_mask=attention_mask)
        preds = torch.argmax(outputs.logits, dim=1)
        correct_predictions.extend(
            (preds == labels).cpu().numpy().tolist()
        )
accuracy = accuracy_score(test_labels.numpy(), correct_predictions)
print(f""Test Accuracy: {accuracy:.2f}"")

# Integrated Gradients
ig = IntegratedGradients(model)

def get_influential_words(input_text, model, tokenizer, ig, device):
    model.eval()
    # Tokenizing the input text
    inputs = tokenizer(input_text, return_tensors=""pt"", truncation=True, padding=True, max_length=128)
    input_ids = inputs['input_ids'].to(device, dtype=torch.long)  # Explicitly convert to LongTensor
    attention_mask = inputs['attention_mask'].to(device, dtype=torch.long)  # Explicitly convert to LongTensor

    print(""Input IDs shape:"", input_ids.shape, ""dtype:"", input_ids.dtype)
    print(""Attention mask shape:"", attention_mask.shape, ""dtype:"", attention_mask.dtype)
    # forward function for IG
    def forward_func(input_ids):
        outputs = model(input_ids, attention_mask=attention_mask)
        return outputs.logits

    # Applying Integrated Gradients
    attributions, delta = ig.attribute(input_ids, target=1, return_convergence_delta=True)
    tokens = tokenizer.convert_ids_to_tokens(input_ids[0].tolist())
    token_importances = attributions.sum(dim=2).squeeze(0).detach().cpu().numpy()

    return list(zip(tokens, token_importances))

# Analysing influential words for correctly predicted texts
for idx, correct in enumerate(correct_predictions):
    if correct:
        influential_words = get_influential_words(test_df['text'].iloc[idx], model, tokenizer, ig, device)
        print(f""Influential words for text: {test_df['text'].iloc[idx]}"")
        print(influential_words)

But I am getting the following error in running the above.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
Epoch 1 loss: 0.4719192385673523
Epoch 2 loss: 0.39585667848587036
Epoch 3 loss: 0.14659778773784637
Test Accuracy: 0.70
Input IDs shape: torch.Size([1, 8]) dtype: torch.int64
Attention mask shape: torch.Size([1, 8]) dtype: torch.int64
---------------------------------------------------------------------------
RuntimeError                              Traceback (most recent call last)
<ipython-input-9-f047b509c98d> in <cell line: 90>()
     90 for idx, correct in enumerate(correct_predictions):
     91     if correct:
---> 92         influential_words = get_influential_words(test_df['text'].iloc[idx], model, tokenizer, ig, device)
     93         print(f""Influential words for text: {test_df['text'].iloc[idx]}"")
     94         print(influential_words)

18 frames
/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py in embedding(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)
   2549         # remove once script supports set_grad_enabled
   2550         _no_grad_embedding_renorm_(weight, input, max_norm, norm_type)
-> 2551     return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)
   2552 
   2553 

RuntimeError: Expected tensor for argument #1 'indices' to have one of the following scalar types: Long, Int; but got torch.cuda.FloatTensor instead (while checking arguments for embedding)

","You need to slightly change the gradients calculation class. Also, you didn't include forward_func into the gradients class constructor, so the attribute method was not able to launch the stuff properly.
I think that using LayerIntegratedGradients is better for debugging BERT - in line with this tutorial https://captum.ai/tutorials/Bert_SQUAD_Interpret
Below please find snippet that works:
from captum.attr import LayerIntegratedGradients


def custom_forward(inputs):
    preds = predict(inputs)
    return torch.softmax(preds, dim = 1)[0][1].unsqueeze(-1)
lig = LayerIntegratedGradients(custom_forward, model.bert.embeddings)
def get_influential_words(input_text, model, tokenizer, ig, device):
    model.eval()
    # Tokenizing the input text
    inputs = tokenizer(input_text, return_tensors=""pt"", truncation=True, padding=True, max_length=128)
    input_ids = inputs['input_ids'].to(device)
    attention_mask = inputs['attention_mask'].to(device)
    # print(""Input IDs shape:"", input_ids.shape, ""dtype:"", input_ids.dtype)
    # print(""Attention mask shape:"", attention_mask.shape, ""dtype:"", attention_mask.dtype)

    attributions, delta = lig.attribute(input_ids, return_convergence_delta=True)
    
    tokens = tokenizer.convert_ids_to_tokens(input_ids[0].tolist())
    token_importances = attributions.sum(dim=2).squeeze(0).detach().cpu().numpy()

    return list(zip(tokens, token_importances))

results = []

for idx, correct in enumerate(correct_predictions):
    if correct:
        influential_words = get_influential_words(test_df['text'].iloc[idx], model, tokenizer, ig, device)
        print(f""Influential words for text: {test_df['text'].iloc[idx]}"")
        print(influential_words)

",pytorch
gather different pixels per image of an image stack with torch,"I have a batch of images and a batch of indices (x, y) for each image. The indices are different for each image, so I cant use simple indexing. What is the best or fastest way to get another batch with the colors of the selected pixels per image?
    n_images = 4
    width = 100
    height = 100
    channels = 3
    n_samples = 30
    
    images = torch.rand((n_images, height, width, channels))
    indices = (torch.rand((n_images, n_samples, 2)) * width).to(torch.int32)

    # preferred function
    # result = images[indices]
    # with result.shape = (n_images, n_samples, 3)



    

    # I just found this solution but I would rather like to call a general torch function
    xs = indices.reshape((-1, 2))[:, 0]
    ys = indices.reshape((-1, 2))[:, 1]
    ix = torch.arange(n_images, dtype=torch.int32)
    ix = ix[..., None].expand((-1, n_samples)).flatten()
    
    result = images[ix, ys, xs].reshape((n_images, n_samples, 3))

","You can use your indices tensor directly, you just need another tensor for the batch indexing:
n_images = 4
width = 100
height = 100
channels = 3
n_samples = 30

images = torch.rand((n_images, height, width, channels))
indices = (torch.rand((n_images, n_samples, 2)) * width).to(torch.int32)

batch_indices = torch.arange(n_images).view(-1, 1).expand(-1, n_samples)
result = images[batch_indices, indices[..., 1], indices[..., 0]]

This follows your convention of images[ix, ys, xs] where the ys index the height dimension of the tensor and the xs index the width
",pytorch
oserror cuda_home environment variable is not set please set it to your cuda install root,"I have a weird problem which only occurs since today on my github workflow. These are relevant commands.
pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu117
pip3 install mmengine==0.6.0 mmcv==2.0.0rc3 mmdet==3.0.0rc5 mmaction2==1.0rc3

The former succeeded. The latter stops with following error:
Collecting mmengine==0.6.0
  Using cached mmengine-0.6.0-py3-none-any.whl (360 kB)
Collecting mmcv==2.0.0rc3
  Using cached mmcv-2.0.0rc3.tar.gz (424 kB)
  Preparing metadata (setup.py) ... error
  error: subprocess-exited-with-error
  
  × python setup.py egg_info did not run successfully.
  │ exit code: 1
  ╰─> [18 lines of output]
      Traceback (most recent call last):
        File ""<string>"", line 2, in <module>
        File ""<pip-setuptools-caller>"", line 34, in <module>
        File ""/tmp/pip-install-uml22xq3/mmcv_89a43e000b91495e88399ffe3c493514/setup.py"", line 329, in <module>
          ext_modules=get_extensions(),
                      ^^^^^^^^^^^^^^^^
        File ""/tmp/pip-install-uml22xq3/mmcv_89a43e000b91495e88399ffe3c493514/setup.py"", line 290, in get_extensions
          ext_ops = extension(
                    ^^^^^^^^^^
        File ""/home/github/.pyenv/versions/miniconda3-3.10-22.11.1-1/envs/heavi-analytic/lib/python3.11/site-packages/torch/utils/cpp_extension.py"", line 1048, in CUDAExtension
          library_dirs += library_paths(cuda=True)
                          ^^^^^^^^^^^^^^^^^^^^^^^^
        File ""/home/github/.pyenv/versions/miniconda3-3.10-22.11.1-1/envs/heavi-analytic/lib/python3.11/site-packages/torch/utils/cpp_extension.py"", line 1179, in library_paths
          if (not os.path.exists(_join_cuda_home(lib_dir)) and
                                 ^^^^^^^^^^^^^^^^^^^^^^^^
        File ""/home/github/.pyenv/versions/miniconda3-3.10-22.11.1-1/envs/heavi-analytic/lib/python3.11/site-packages/torch/utils/cpp_extension.py"", line 2223, in _join_cuda_home
          raise EnvironmentError('CUDA_HOME environment variable is not set. '
      OSError: CUDA_HOME environment variable is not set. Please set it to your CUDA install root.
      [end of output]
  
  note: This error originates from a subprocess, and is likely not a problem with pip.
error: metadata-generation-failed

× Encountered error while generating package metadata.
╰─> See above for output.

note: This is an issue with the package mentioned above, not pip.
hint: See above for details.

Any idea?
UPDATE 1: So it turns out that pytorch version installed is 2.0.0 which is not desirable.
","It turns out that as torch 2 was released on March 15 yesterday, the continuous build automatically gets the latest version of torch.
This hardcoded torch version fix everything:
pip3 install torch==1.13.1+cu117 torchvision==0.14.1+cu117 \
  torchaudio==0.13.1 --extra-index-url https://download.pytorch.org/whl/cu117

It installs torch 1.13 with cuda 11.7.
Looking in indexes: https://pypi.org/simple, https://download.pytorch.org/whl/cu117
Collecting torch==1.13.1+cu117
  Using cached https://download.pytorch.org/whl/cu117/torch-1.13.1%2Bcu117-cp310-cp310-linux_x86_64.whl (1801.8 MB)
Collecting torchvision==0.14.1+cu117
  Using cached https://download.pytorch.org/whl/cu117/torchvision-0.14.1%2Bcu117-cp310-cp310-linux_x86_64.whl (24.3 MB)
Collecting torchaudio==0.13.1
  Using cached https://download.pytorch.org/whl/cu117/torchaudio-0.13.1%2Bcu117-cp310-cp310-linux_x86_64.whl (4.2 MB)
Collecting typing-extensions
  Using cached typing_extensions-4.5.0-py3-none-any.whl (27 kB)
Collecting pillow!=8.3.*,>=5.3.0
  Using cached Pillow-9.4.0-cp310-cp310-manylinux_2_28_x86_64.whl (3.4 MB)
Requirement already satisfied: requests in /home/github/.pyenv/versions/miniconda3-3.10-22.11.1-1/lib/python3.10/site-packages (from torchvision==0.14.1+cu117) (2.28.1)
Collecting numpy
  Using cached numpy-1.24.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.3 MB)
Requirement already satisfied: certifi>=2017.4.17 in /home/github/.pyenv/versions/miniconda3-3.10-22.11.1-1/lib/python3.10/site-packages (from requests->torchvision==0.14.1+cu117) (2022.12.7)
Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/github/.pyenv/versions/miniconda3-3.10-22.11.1-1/lib/python3.10/site-packages (from requests->torchvision==0.14.1+cu117) (1.26.13)
Requirement already satisfied: charset-normalizer<3,>=2 in /home/github/.pyenv/versions/miniconda3-3.10-22.11.1-1/lib/python3.10/site-packages (from requests->torchvision==0.14.1+cu117) (2.0.4)
Requirement already satisfied: idna<4,>=2.5 in /home/github/.pyenv/versions/miniconda3-3.10-22.11.1-1/lib/python3.10/site-packages (from requests->torchvision==0.14.1+cu117) (3.4)
Installing collected packages: typing-extensions, pillow, numpy, torch, torchvision, torchaudio
Successfully installed numpy-1.24.2 pillow-9.4.0 torch-1.13.1+cu117 torchaudio-0.13.1+cu117 torchvision-0.14.1+cu117 typing-extensions-4.5.0

EDIT 1:
Sometimes pip3 does not succeed. Use conda instead.
conda install pytorch==1.13.1 torchvision==0.14.1 \
  torchaudio==0.13.1 cudatoolkit=11.7 pytorch-cuda=11.7 -c pytorch -c nvidia

",pytorch
quotnotimplementederror could not run 39atenaddout39 with arguments from the 39quantizedcpu39 backendquot while implementing qat on resnet18 using pytorch,"I am trying to implement Quantization Aware Training(QAT) resnet18 model. While inferring I get this error
NotImplementedError: Could not run 'aten::add.out' with arguments from the 'QuantizedCPU' backend

I am trying to follow this documentation by pytorch for using their QAT API
Here is my code, I am also attaching a google collab notebook link
Block 1 - Importing the necessary libraries, defining training and evaluation functions
import torch
import torch.nn as nn
import torch.optim as optim
import torchvision
import torchvision.transforms as transforms
from torchvision.models import resnet18
import matplotlib.pyplot as plt
import copy
import numpy as np
import os

def evaluate_model(model, test_loader, device, criterion=None):
    model.eval()
    model.to(device)
    running_loss = 0
    running_corrects = 0
    for inputs, labels in test_loader:
        inputs = inputs.to(device)
        labels = labels.to(device)
        outputs = model(inputs)
        _, preds = torch.max(outputs, 1)
        if criterion is not None:
            loss = criterion(outputs, labels).item()
        else:
            loss = 0
        running_loss += loss * inputs.size(0)
        running_corrects += torch.sum(preds == labels.data)
    eval_loss = running_loss / len(test_loader.dataset)
    eval_accuracy = running_corrects / len(test_loader.dataset)
    return eval_loss, eval_accuracy



def train_model(model, train_loader, test_loader, device, learning_rate=1e-1, num_epochs=200):
    criterion = nn.CrossEntropyLoss()
    model.to(device)
    #optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9, weight_decay=1e-4)
    optimizer = optim.Adam(model.parameters(), lr=1e-4)
    scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[100, 150], gamma=0.1, last_epoch=-1)
    model.eval()
    eval_loss, eval_accuracy = evaluate_model(model=model, test_loader=test_loader, device=device, criterion=criterion)
    print(""Epoch: {:02d} Eval Loss: {:.3f} Eval Acc: {:.3f}"".format(-1, eval_loss, eval_accuracy))
    for epoch in range(num_epochs):
        model.train()
        running_loss = 0
        running_corrects = 0
        for inputs, labels in train_loader:
            inputs = inputs.to(device)
            labels = labels.to(device)
            optimizer.zero_grad()
            outputs = model(inputs)
            _, preds = torch.max(outputs, 1)
            loss = criterion(outputs, labels)
            if torch.isnan(loss):
                print(""NaN in Loss!"")
                return model
            loss.backward()
            optimizer.step()
            running_loss += loss.item() * inputs.size(0)
            running_corrects += torch.sum(preds == labels.data)

        train_loss = running_loss / len(train_loader.dataset)
        train_accuracy = running_corrects / len(train_loader.dataset)
        model.eval()
        eval_loss, eval_accuracy = evaluate_model(model=model, test_loader=test_loader, device=device, criterion=criterion)
        scheduler.step()
        print(""Epoch: {:03d} Train Loss: {:.3f} Train Acc: {:.3f} Eval Loss: {:.3f} Eval Acc: {:.3f}"".format(epoch, train_loss, train_accuracy, eval_loss, eval_accuracy))
    return model

Block 2 - Loading trainset and testset (CIFAR 100 resized to 224*224)
transform_train = transforms.Compose([
    transforms.Resize(224),
    transforms.RandomHorizontalFlip(),
    transforms.ToTensor(),
    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))
])
transform_test = transforms.Compose([
    transforms.Resize(224),
    transforms.ToTensor(),
    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))
])

trainset = torchvision.datasets.CIFAR100(root='./data', train=True, download=True, transform=transform_train)
trainloader = torch.utils.data.DataLoader(trainset, batch_size=128, shuffle=True, num_workers=2)

testset = torchvision.datasets.CIFAR100(root='./data', train=False, download=True, transform=transform_test)
testloader = torch.utils.data.DataLoader(testset, batch_size=128, shuffle=False, num_workers=2)

print(""Data loaded and transformed successfully!"")

Block 3 -
class QuantizedResNet18(nn.Module):
    def __init__(self, model_fp32):
        super().__init__()
        # QuantStub converts tensors from floating point to quantized.
        # This will only be used for inputs.
        self.quant = torch.ao.quantization.QuantStub()
        # DeQuantStub converts tensors from quantized to floating point.
        # This will only be used for outputs.
        self.dequant = torch.ao.quantization.DeQuantStub()
        # FP32 model
        self.model_fp32 = model_fp32

    def forward(self, x):
        # manually specify where tensors will be converted from floating
        # point to quantized in the quantized model
        print(f""Input shape before quant: {x.shape}, dtype: {x.dtype}"")
        x = self.quant(x)
        print(f""Input shape after quant: {x.shape}, dtype: {x.dtype}"")
        x = self.model_fp32(x)
        print(f""Input shape: {x.shape}, dtype: {x.dtype}"")
        # manually specify where tensors will be converted from quantized
        # to floating point in the quantized model
        x = self.dequant(x)
        print(f""Input shape: {x.shape}, dtype: {x.dtype}"")
        return x

model = resnet18(num_classes=100, pretrained=False)
fused_model = copy.deepcopy(model)
fused_model.eval()
qconfig = torch.ao.quantization.get_default_qconfig('fbgemm')
fused_model.qconfig = qconfig

# Fuse the model in place rather manually.
fused_model = torch.ao.quantization.fuse_modules(fused_model, [[""conv1"", ""bn1"", ""relu""]], inplace=True)
for module_name, module in fused_model.named_children():
    if ""layer"" in module_name:
        for basic_block_name, basic_block in module.named_children():
            torch.ao.quantization.fuse_modules(basic_block, [[""conv1"", ""bn1"", ""relu""], [""conv2"", ""bn2""]], inplace=True)
            for sub_block_name, sub_block in basic_block.named_children():
                if sub_block_name == ""downsample"":
                    torch.ao.quantization.fuse_modules(sub_block, [[""0"", ""1""]], inplace=True)

quantized_model_1 = QuantizedResNet18(model_fp32=fused_model)
quantized_model_1.qconfig = qconfig
cuda_device = torch.device(""cuda:0"")
quantized_model_1_prepared = torch.ao.quantization.prepare_qat(quantized_model_1.train())
trained_quantized_model_1_prepared = train_model(model=quantized_model_1_prepared, train_loader=trainloader, test_loader=testloader, device=cuda_device, learning_rate=1e-3, num_epochs=1)
cpu_device = torch.device(""cpu:0"")
trained_quantized_model_1_prepared.to(cpu_device)
trained_quantized_model_1_prepared.eval()
trained_quantized_model_1_prepared_int8 =  torch.ao.quantization.convert(trained_quantized_model_1_prepared)
print(evaluate_model(model=trained_quantized_model_1_prepared_int8, test_loader=testloader, device=cpu_device))

the issue is in the last line when I try to run evaluate_model function, particularly while inferring (outputs = model(inputs))
I get the following error
NotImplementedError: Could not run 'aten::add.out' with arguments from the 'QuantizedCPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::add.out' is only available for these backends: [CPU, CUDA, Meta, MkldnnCPU, SparseCPU, SparseCUDA, SparseMeta, SparseCsrCPU, SparseCsrCUDA, SparseCsrMeta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMTIA, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradMeta, AutogradNestedTensor, Tracer, AutocastCPU, AutocastXPU, AutocastMPS, AutocastCUDA, FuncTorchBatched, BatchedNestedTensor, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, 

","This tutorial tells that for torch 2.0 this feature is beta and you need to adjust original model with at least one change (https://pytorch.org/tutorials/advanced/static_quantization_tutorial.html#model-architecture) for residual addition:

Replacing addition with nn.quantized.FloatFunctional

You can see in your error trace that this line of code throws the error:
        out += identity


https://github.com/pytorch/vision/blob/229d8523bfa9a2696872d76b1cdb6815028f1e03/torchvision/models/resnet.py#L102
So we need to:

reimplement BasicBlock by replacing += operator with skip_add;
inject BasicBlock to the Resnet constructor.

Step 1
from functools import partial
from typing import Any, Callable, List, Optional, Type, Union

import torch
import torch.nn as nn
from torch import Tensor

from torchvision.transforms._presets import ImageClassification
from torchvision.utils import _log_api_usage_once
from torchvision.models.resnet import Bottleneck


from torchvision.models.resnet import conv3x3

class BasicBlock(nn.Module):
    expansion: int = 1

    def __init__(
        self,
        inplanes: int,
        planes: int,
        stride: int = 1,
        downsample: Optional[nn.Module] = None,
        groups: int = 1,
        base_width: int = 64,
        dilation: int = 1,
        norm_layer: Optional[Callable[..., nn.Module]] = None,
    ) -> None:
        super().__init__()
        if norm_layer is None:
            norm_layer = nn.BatchNorm2d
        if groups != 1 or base_width != 64:
            raise ValueError(""BasicBlock only supports groups=1 and base_width=64"")
        if dilation > 1:
            raise NotImplementedError(""Dilation > 1 not supported in BasicBlock"")
        # Both self.conv1 and self.downsample layers downsample the input when stride != 1
        self.conv1 = conv3x3(inplanes, planes, stride)
        self.bn1 = norm_layer(planes)
        self.relu = nn.ReLU(inplace=True)
        self.conv2 = conv3x3(planes, planes)
        self.bn2 = norm_layer(planes)
        self.downsample = downsample
        self.stride = stride
        self.skip_add = nn.quantized.FloatFunctional() 

    def forward(self, x: Tensor) -> Tensor:
        identity = x

        out = self.conv1(x)
        out = self.bn1(out)
        out = self.relu(out)

        out = self.conv2(out)
        out = self.bn2(out)

        if self.downsample is not None:
            identity = self.downsample(x)

        out = self.skip_add.add(out, identity)
        out = self.relu(out)

        return out

Step 2
Inject this by creating new constructor method for quantized model.
from torchvision.models.resnet import ResNet18_Weights, _resnet
from torchvision.models._api import register_model, Weights, WeightsEnum
from torchvision.models._utils import _make_divisible, _ovewrite_named_param, handle_legacy_interface

@register_model()
@handle_legacy_interface(weights=(""pretrained"", ResNet18_Weights.IMAGENET1K_V1))
def quantizedresnet18(*, weights: Optional[ResNet18_Weights] = None, progress: bool = True, **kwargs: Any):
    weights = ResNet18_Weights.verify(weights)

    return _resnet(BasicBlock, [2, 2, 2, 2], weights, progress, **kwargs)

Step 3
Go back to the cell with
model = resnet18(num_classes=100, pretrained=False)
fused_model = copy.deepcopy(model)
fused_model.eval()
qconfig = torch.ao.quantization.get_default_qconfig('fbgemm')
fused_model.qconfig = qconfig

And change it with this (see the first line is changed)
model = quantizedresnet18(num_classes=100, pretrained=False)
fused_model = copy.deepcopy(model)
fused_model.eval()
qconfig = torch.ao.quantization.get_default_qconfig('fbgemm')
fused_model.qconfig = qconfig

Then execute all cells below again.
",pytorch
how to efficiently implement forward fill in pytorch,"How can I efficiently implement the fill forward logic (inspired for pandas ffill) for a vector shaped NxLxC (batch, sequence dimension, channel). Because each channel sequence is independent this can be equivalent to working with a tensor shaped (N*C)xL.
The computation should keep the torch variable so that the actual output is differentiable.
I managed to make something with advanced indexing, but it is L**2 in the memory and number of operations, so not very great and gpu friendly.

Example:
Assuming you have the sequence [0,1,2,0,0,3,0,4,0,0,0,5,6,0] in a tensor shaped 1x14 the fill forward will give you the sequence [0,1,2,2,2,3,3,4,4,4,4,5,6,6].
An other example shaped 2x4 is [[0, 1, 0, 3], [1, 2, 0, 3]] which should be forward filled into [[0, 1, 1, 3], [1, 2, 2, 3]].

Method used today:
We use the following code that is highly unoptimized but still faster than non vectorized loops:
def last_zero_sequence_start_indices(t: torch.Tensor) -> torch.Tensor:
    """"""
    Given a 3D tensor `t`, this function returns a two-dimensional tensor where each entry represents
    the starting index of the last contiguous sequence of zeros up to and including the current index.
    If there's no zero at the current position, the value is the tensor's length.

    In essence, for each position in `t`, the function pinpoints the beginning of the last contiguous
    sequence of zeros up to that position.

    Args:
    - t (torch.Tensor): Input tensor with shape [Batch, Channel, Time].

    Returns:
    - torch.Tensor: Three-dimensional tensor with shape [Batch, Channel, Time] indicating the starting position of
        the last sequence of zeros up to each index in `t`.
    """"""

    # Create a mask indicating the start of each zero sequence
    start_of_zero_sequence = (t == 0) & torch.cat([
        torch.full(t.shape[:-1] + (1,), True, device=t.device),
        t[..., :-1] != 0,
    ], dim=2)

    # Duplicate this mask into a TxT matrix
    duplicated_mask = start_of_zero_sequence.unsqueeze(2).repeat(1, 1, t.size(-1), 1)

    # Extract the lower triangular part of this matrix (including the diagonal)
    lower_triangular = torch.tril(duplicated_mask)

    # For each row, identify the index of the rightmost '1' (start of the last zero sequence up to that row)
    indices = t.size(-1) - 1 - lower_triangular.int().flip(dims=[3]).argmax(dim=3)

    return indices

","Here is an approach to this problem, without creating TxT matrix:
import torch
def forward_fill(t: torch.Tensor) -> torch.Tensor:
    n_dim, t_dim = t.shape
    # Generate indices range
    rng = torch.arange(t_dim)
    
    rng_2d = rng.unsqueeze(0).repeat(n_dim, 1)
    # Replace indices to zero for elements that equal zero
    rng_2d[t == 0] = 0
    
    # Forward fill of indices range so all zero elements will be replaced with previous non-zero index.
    idx = rng_2d.cummax(1).values
    t = t[torch.arange(n_dim)[:, None], idx]
    return t

Note that this is a solution for 2D input but can be easily modified for more dimensions.
",pytorch
why do layernorm layers in bert base have 768 and not 512 weight and bias parameters,"The following will print 768 weight and bias parameters for each LayerNorm layer.
from transformers import BertModel
model = BertModel.from_pretrained('bert-base-uncased')
for name, param in model.named_parameters():
    if 'LayerNorm' in name:
        print(f""Layer: {name}, Parameters: {param.numel()}"")

As per this video, mean and std values are computed for each token in the input. And each mean, std pair has its own learned weight and bias in layer normalization. Since BERT takes in a max of 512 tokens, I'd expect a total of 512 weight and bias parameters in LayerNorm layers.
So why is it 768? Is the video incorrect? Is normalization performed for each of the 768 dimensions across all tokens, meaning mean and std values are computed across a max of 512 values?
","After quite a bit of research and code review, I was able to isolate details of layer normalization (LN), an aspect of transformers that's confusing a lot of people.
TL;DR: The assumption I made in my original question that each mean, std pair has its own weight, bias pair is incorrect. In LN, mean and std stats are computed across embedding dimensions of each token, i.e., there are as many mean, std pairs as there are tokens. But the weight and bias values are learned per embedding dimension, i.e., tokens share the weight and bias values during LN. This means, in the case of BERT base, there are a max of 512 mean and std values, and there are 768 weight and bias values.
For complete details, see my answer to this question.
",pytorch
how does one set the pad token correctly not to eos during finetuning to avoid model not predicting eos,"**tldr; what I really want to know is what is the official way to set pad token for fine tuning it wasn't set during original training, so that it doesn't not learn to predict EOS. **
colab: https://colab.research.google.com/drive/1poFdFYmkR_rDM5U5Z2WWjTepMQ8hvzNc?usp=sharing

The HF falcon tutorial has the following line:
tokenizer.pad_token = tokenizer.eos_token

it looks strange to me. It make sense pad and eos are the same but then why even make a difference between them in the first place in general?
Note its wrong to do pad = eos. This means during fine-tuning the model will never be trained to output eos (most likely) since eos is treated as pad token and no back propagated:
I just observed that when I set tokenizer.pad_token = tokenizer.eos_token during training, the model won't stop generating during inference, since it was trained to not output the eos token (per discussions above).

I saw this (here https://github.com/huggingface/transformers/issues/22794):
tokenizer.add_special_tokens({'pad_token': '[PAD]'})

But this assumes the model has a pad_token. I think an additional check has to be done that it does have an embedding for pad_token so that there are no run time errors (~type errors in the matrix extraction from the embedding ""table""/matrix).
But if one does that some care might be needed to initialize the new token so that it dominates the generation: https://nlp.stanford.edu/~johnhew/vocab-expansion.html

code:
def get_model_tokenizer_qlora_falcon7b(model_name: str = ""ybelkada/falcon-7b-sharded-bf16"",
                                       config: wand.Config,  # todo
                                       lora_alpha=16,  # todo
                                       lora_dropout=0.1,  # todo
                                       lora_r=64,  # todo
                                       bnb_4bit_compute_dtype=torch.float16,  # changed it from Guanaco hf
                                       ) -> tuple:
    """"""
    Load the Falcon 7B model, quantize it in 4bit and attach LoRA adapters on it.

    bf16 = 1S, 7Exp, 8Mantissa

    Do:
        pip install bitsandbytes
    ref:
        - https://colab.research.google.com/drive/1DOi8MFv4SWN9NImVornZ7t6BgmLoPQO-#scrollTo=AjB0WAqFSzlD
    """"""
    from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, AutoTokenizer

    # model_id = ""tiiuae/falcon-7b""
    # model_name: str = ""ybelkada/falcon-7b-sharded-bf16""

    # - get bnb config for bit-4 base model (bnb lib for using 4bit qlora quantization techniques by tim dettmers)
    bnb_config = BitsAndBytesConfig(
        load_in_4bit=True,  # load (usually huge) base model in 4 bits
        bnb_4bit_quant_type=""nf4"",  # normal float 4 for the (usually huge) base model. introduces error but fixed by ft
        # ref: https://gist.github.com/pacman100/1731b41f7a90a87b457e8c5415ff1c14
        bnb_4bit_compute_dtype=bnb_4bit_compute_dtype,
    )

    # - get falcon 4bit model
    model = AutoModelForCausalLM.from_pretrained(
        model_name,
        quantization_config=bnb_config,
        trust_remote_code=True  # allows to execute custom code you download from the uploaded model code you are using
    )
    model.config.use_cache = False  # todo: why? https://stackoverflow.com/questions/76633335/why-does-hugging-face-falcon-model-use-mode-config-use-cache-false-why-wouldn

    # get falcon tockenizer
    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)  # execs code downloaded from hf hub
    tokenizer.pad_token = tokenizer.eos_token


Modifying model gives issues
Darn this still not works:
 UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation)

code:
""""""
sfttrainer (likely using peft) best practices:
https://huggingface.co/docs/trl/main/en/sft_trainer#best-practices

Best practices

Pay attention to the following best practices when training a model with that trainer:

- SFTTrainer always pads by default the sequences to the max_seq_length argument of the SFTTrainer. If none is passed, the trainer will retrieve that value from the tokenizer. Some tokenizers do not provide default value, so there is a check to retrieve the minimum between 2048 and that value. Make sure to check it before training.
- For training adapters in 8bit, you might need to tweak the arguments of the prepare_model_for_int8_training method from PEFT, hence we advise users to use prepare_in_int8_kwargs field, or create the PeftModel outside the SFTTrainer and pass it.
- For a more memory-efficient training using adapters, you can load the base model in 8bit, for that simply add load_in_8bit argument when creating the SFTTrainer, or create a base model in 8bit outside the trainer and pass it.
- If you create a model outside the trainer, make sure to not pass to the trainer any additional keyword arguments that are relative to from_pretrained() method.

todo: why trust_remote_code? I want more details.
""""""
import sys

import torch
from peft import LoraConfig

from transformers.modeling_utils import PreTrainedModel

from pdb import set_trace as st


def test_bfloat16_int4(compute_dtype: torch.dtype,
                       use_4bit,
                       ):
    """"""
python -c ""import torch; print(torch.cuda.get_device_capability());""
    todo: check other code test_bfloat16() do we need use_4bit?
    """"""
    if compute_dtype == torch.float16 and use_4bit:
        major, _ = torch.cuda.get_device_capability()
        if major >= 8:
            print(""="" * 80)
            print(""Your GPU supports bfloat16, you can accelerate training with the argument --bfloat16"")
            print(""="" * 80)


def get_model_tokenizer_qlora_falcon7b(
        # -- mode args
        # model_id = ""tiiuae/falcon-7b""
        pretrained_model_name_or_path: str = ""ybelkada/falcon-7b-sharded-bf16"",
        use_cache: bool = True,
        # -- lora args
        lora_alpha=16,  # todo
        lora_dropout=0.1,  # todo, evidence drop out really help? google, crfm, gpt4
        lora_r=64,  # todo
        bnb_4bit_compute_dtype=torch.float16,  # changed it from Guanaco hf

        # -- training args
        output_dir=""./results"",
        per_device_train_batch_size=4,
        gradient_accumulation_steps=4,
        # paging so that the sudden mem gpu spikes don't cause the run to shut down
        # (I think usually caused by too long seqs)
        # todo: why 32 bit opt?
        # todo: paged nadamw opt?
        optim=""paged_adamw_32bit"",
        save_steps=10,
        logging_steps=10,
        learning_rate=2e-4,
        max_grad_norm=0.3,
        max_steps=500,
        warmup_ratio=0.03,
        lr_scheduler_type=""constant"",
        # -- quant. args (not recommended to be changed unless you know what your doing?)
        load_in_4bit=True,  # load (usually huge) base model in 4 bits
        bnb_4bit_quant_type=""nf4"",  # normal float 4 for the (large) base models qlora
) -> tuple:
    """"""
    Load the Falcon 7B model, quantize it in 4bit and attach LoRA adapters on it.

    bf16 = 1S, 7Exp, 8Mantissa
    hypothesis: 7b trained due to 6.7 emergence rumour, I still don't think emergence is real.
    Notes:
        - ft a model is very specific to the model, tokenizer and training scheme. Thus we return
            - model, tokenizer, ft config (peft config), training args

    ref:
        - https://colab.research.google.com/drive/1DOi8MFv4SWN9NImVornZ7t6BgmLoPQO-#scrollTo=AjB0WAqFSzlD
    """"""
    from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, AutoTokenizer

    # - Get bnb config for bit-4 base model (bnb lib for using 4bit qlora quantization techniques by tim dettmers)
    bnb_config = BitsAndBytesConfig(
        load_in_4bit=load_in_4bit,  # load (usually huge) base model in 4 bits
        bnb_4bit_quant_type=bnb_4bit_quant_type,  # normal float 4 for the (usually huge) base model
        bnb_4bit_compute_dtype=bnb_4bit_compute_dtype,  # if you can, during computation use bf16
    )

    # - Get falcon 4bit model
    # todo, where is this being saved & how to download quicker
    model = AutoModelForCausalLM.from_pretrained(
        pretrained_model_name_or_path=pretrained_model_name_or_path,
        quantization_config=bnb_config,
        trust_remote_code=True  # allows to execute custom code you download from the uploaded model code you are using
    )
    print(f'{type(model)=}')
    print(f'{model=}')
    # this is here to save gpu vram. Likely only needed when using 40b or when oom issues happen ref: https://stackoverflow.com/questions/76633335/why-does-hugging-face-falcon-model-use-mode-config-use-cache-false-why-wouldn
    model.config.use_cache = use_cache
    print(f'{type(model)=}')

    # - Get falcon tokenizer
    tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name_or_path,
                                              trust_remote_code=True)  # execs code downloaded from hf hub
    # tokenizer.pad_token = tokenizer.eos_token  # ref: https://stackoverflow.com/questions/76633368/why-does-the-falcon-qlora-tutorial-code-use-eos-token-as-pad-token
    # tokenizer.add_special_tokens({'pad_token': '[PAD]'})  # I think this is fine if during the training pad is ignored
    tokenizer.add_special_tokens({'pad_token': '<|pad|>'})  # I think this is fine if during the training pad is ignored

    # - Modify model
    # add pad token embed
    model.resize_token_embeddings(len(tokenizer))  # todo: I think this is fine if during the training pad is ignored
    model.transformer.word_embeddings.padding_idx = len(tokenizer) - 1
    model.config.max_new_tokens = len(tokenizer)
    # model.config.min_length = 1
    print(f'{model=}')
    print(f'{type(tokenizer)=}')
    print(f'{tokenizer.pad_token=}')
    # data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False) todo

    # - Get falcon lora config
    peft_config = LoraConfig(
        lora_alpha=lora_alpha,
        lora_dropout=lora_dropout,
        r=lora_r,
        bias=""none"",
        task_type=""CAUSAL_LM"",
        # model card for falcon tiiuae/falcon-7b: https://huggingface.co/tiiuae/falcon-7b/blob/main/modelling_RW.py
        # does seem to include all trainable params as done by qlora on their own paper
        target_modules=[
            # word_embeddings,
            ""query_key_value"",
            ""dense"",
            ""dense_h_to_4h"",
            ""dense_4h_to_h"",
            # ""lm_head""
        ]
    )
    print(f'{type(peft_config)=}')

    # todo: print the num params of the lora = D1*r + D2*r and num of bytes by prec. (bytes) * num params
    return model, tokenizer, peft_config


# -- tests

def example_test_model_already_has_pad_token():
    """"""
    if it already has pad token, it likely has a small prob, so we are done.

    compare it's norm with other tokens to verify this is true.

python ~/ultimate-utils/ultimate-utils-proj-src/uutils/hf_uu/model_tokenizer/falcon_uu_mdl_tok.py
    """"""
    # - the get datasets todo: preprocessing, padding, streaming
    from uutils.hf_uu.data_hf.common import get_guanaco_datsets_add_splits_train_test_only
    trainset, _, testset = get_guanaco_datsets_add_splits_train_test_only()

    # qlora flacon7b
    from uutils.hf_uu.model_tokenizer.falcon_uu_mdl_tok import get_model_tokenizer_qlora_falcon7b
    model, tokenizer, peft_config = get_model_tokenizer_qlora_falcon7b()
    model: PreTrainedModel = model
    print(f'{model=}')
    sent = 'Dogs are great because they are '
    print()

    # print to see if pad tokens are present and if it ignores the tokens at the end
    encoded_input = tokenizer(sent, padding='max_length', max_length=10, return_tensors='pt')
    print(f'{encoded_input=}')

    # Print all special tokens
    print('\n---- start Print all special tokens')
    for token_name, token in tokenizer.special_tokens_map.items():
        print(f""{token_name}: {token}"")
    print('\n---- end Print all special tokens')

    # Get the ID for the '[PAD]' token
    try:
        pad_token_id = tokenizer.convert_tokens_to_ids('[PAD]')
    except KeyError:
        raise ValueError(""Token [PAD] is not present in the tokenizer vocabulary."")

    # Index into the model's embedding table
    try:
        print(f'{model.get_input_embeddings().weight.size()=}')
        pad_embedding = model.get_input_embeddings().weight[pad_token_id]
    except IndexError:
        raise ValueError(f""Token ID {pad_token_id} is not present in the model's embedding matrix."")

    print(f'{pad_embedding=}')
    print('Success!\n')

    # check it generates something sensible
    # tokenizer.decode(model.generate(**tokenizer(sent, return_tensors='pt'), do_sample=True)[0])
    input_ids, attention_mask = encoded_input['input_ids'], encoded_input['attention_mask']
    predicted_tokens_ids_options = model.generate(input_ids=input_ids, attention_mask=attention_mask, do_sample=True)
    predicted_tokens_ids = predicted_tokens_ids_options[0]
    predicted_sent = tokenizer.decode(predicted_tokens_ids)
    print(f'original sentence: {sent=}')
    print(f'predicted sentence: {predicted_sent=}')
    print('Success2!')


if __name__ == '__main__':
    import time

    start_time = time.time()
    example_test_model_already_has_pad_token()
    print(f""The main function executed in {time.time() - start_time} seconds.\a"")

it doesn't like the modifications to the model:
    model.transformer.word_embeddings.padding_idx = len(tokenizer) - 1
    model.config.max_new_tokens = len(tokenizer)

How to fix?
Errors:
/lfs/hyperturing1/0/brando9/miniconda/envs/data_quality/lib/python3.10/site-packages/transformers/generation/utils.py:1259: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation)
  warnings.warn(
Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.
/lfs/hyperturing1/0/brando9/miniconda/envs/data_quality/lib/python3.10/site-packages/transformers/generation/utils.py:1452: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cpu, whereas the model is on cuda. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cuda') before running `.generate()`.
  warnings.warn(
Traceback (most recent call last):
  File ""/lfs/hyperturing1/0/brando9/ultimate-utils/ultimate-utils-proj-src/uutils/hf_uu/model_tokenizer/falcon_uu_mdl_tok.py"", line 211, in <module>
    example_test_model_already_has_pad_token()
  File ""/lfs/hyperturing1/0/brando9/ultimate-utils/ultimate-utils-proj-src/uutils/hf_uu/model_tokenizer/falcon_uu_mdl_tok.py"", line 199, in example_test_model_already_has_pad_token
    predicted_tokens_ids_options = model.generate(input_ids=input_ids, attention_mask=attention_mask, do_sample=True)
  File ""/lfs/hyperturing1/0/brando9/miniconda/envs/data_quality/lib/python3.10/site-packages/torch/utils/_contextlib.py"", line 115, in decorate_context
    return func(*args, **kwargs)
  File ""/lfs/hyperturing1/0/brando9/miniconda/envs/data_quality/lib/python3.10/site-packages/transformers/generation/utils.py"", line 1572, in generate
    return self.sample(
  File ""/lfs/hyperturing1/0/brando9/miniconda/envs/data_quality/lib/python3.10/site-packages/transformers/generation/utils.py"", line 2633, in sample
    next_token_scores = logits_warper(input_ids, next_token_scores)
  File ""/lfs/hyperturing1/0/brando9/miniconda/envs/data_quality/lib/python3.10/site-packages/transformers/generation/logits_process.py"", line 92, in __call__
    scores = processor(input_ids, scores)
  File ""/lfs/hyperturing1/0/brando9/miniconda/envs/data_quality/lib/python3.10/site-packages/transformers/generation/logits_process.py"", line 302, in __call__
    indices_to_remove = scores < torch.topk(scores, top_k)[0][..., -1, None]
RuntimeError: ""topk_cpu"" not implemented for 'Half'


Bounty Section: Small GPT2 code example
Yes I agree that pad is assigned to eos. Eos is still eos. But during fine-tuning now the weights wrt to eos are unchanged. This might be an issue since the probability of eos has not shifted to the fine-tuning regime. One possibility is that eos is outputed with less chance. Yes we can still halt production when we see eos but we've not shifted the probability to output eos according to our fine-tuning distribution -- but all other tokens have changed distribution. I think this could be an issue because it's not like the old probability of eos is conserved since all tokens probs have changed except eos + even if the old eos prob was conserved, it's wrt wrong distribution (not the fine tuning one).
e.g.,
    if tokenizer.pad_token_id is None:
        tokenizer.pad_token = tokenizer.eos_token
...
    raw_text_batch='a'
    tokenize_batch={'input_ids': tensor([[   64, 50256, 50256, 50256, 50256]]), 'attention_mask': tensor([[1, 0, 0, 0, 0]])}

but it would have been better to have
    tokenize_batch={'input_ids': tensor([[   64, 50256, 50256, 50256, 50256]]), 'attention_mask': tensor([[1, 1, 0, 0, 0]])}

code
def test_eos_pad():
    from datasets import load_dataset
    import torch
    from transformers import GPT2Tokenizer, GPT2LMHeadModel

    raw_text_batch = 'a'

    tokenizer = GPT2Tokenizer.from_pretrained(""gpt2"")
    # print(f'{tokenizer.eos_token=}')
    # print(f'{tokenizer.eos_token_id=}')
    # print(f'{tokenizer.pad_token=}')
    # print(f'{tokenizer.pad_token_id=}')

    # print(f'{raw_text_batch=}')
    # tokenize_batch = tokenizer(raw_text_batch, padding=""max_length"", max_length=5, truncation=True, return_tensors=""pt"")
    # print(f'{tokenize_batch=}')

    if tokenizer.pad_token_id is None:
        tokenizer.pad_token = tokenizer.eos_token
    probe_network = GPT2LMHeadModel.from_pretrained(""gpt2"")
    device = torch.device(f""cuda:{0}"" if torch.cuda.is_available() else ""cpu"")
    probe_network = probe_network.to(device)

    print(f'{tokenizer.eos_token=}')
    print(f'{tokenizer.eos_token_id=}')
    print(f'{tokenizer.pad_token=}')
    print(f'{tokenizer.pad_token_id=}')

    print(f'{raw_text_batch=}')
    tokenize_batch = tokenizer(raw_text_batch, padding=""max_length"", max_length=5, truncation=True, return_tensors=""pt"")
    print(f'{tokenize_batch=}')
    print('Done')


cross:

hf discuss forum: https://discuss.huggingface.co/t/why-does-the-falcon-qlora-tutorial-code-use-eos-token-as-pad-token/45954
pytorch forum discuss: https://discuss.pytorch.org/t/how-does-one-set-the-pad-token-correctly-not-to-eos-during-fine-tuning-to-avoid-model-not-predicting-eos/213619
https://discord.com/channels/879548962464493619/1126681170957045770/1126681170957045770
context peft pacman100 code: https://gist.github.com/pacman100/1731b41f7a90a87b457e8c5415ff1c14
twitter tweet of this: https://twitter.com/BrandoHablando/status/1693676898013061337?s=20

","Update August/8/2024
One more improvement. Some models like DeepSeekCoder base 7B do have a pad token already. So no need to set the pad token to eos. But the code that pads up to 1st occurence of eos + pads the rest has to pad the rest assuming they are pad tokens. So that's the diff:
def get_lm_examples_1st_eos_mask_remaining_eos(
        examples,
        tokenizer: AutoTokenizer, 
        
        # desired_dataset_column: str = 'text',
        # method_to_remove_columns: str = 'keys',

        remove_to_long_seqs: bool = False,
        # format: str = 'torch',
        debug: bool = False,
        ) -> dict[str, torch.Tensor]:
    """""" 
    Train only on first occurence of eos. The remaining eos are masked out. If 
    - train up to 1st ocurrence of eos token, mask out the rest of the eos tokens.
    - drop or not seqs that are too long, i.e., have no eos token.
    
    Assumes: pad == eos

    ref: https://stackoverflow.com/questions/76633368/how-does-one-set-the-pad-token-correctly-not-to-eos-during-fine-tuning-to-avoi
    """"""
    # - Get lm example
    seq_length: int = examples['input_ids'].size(0)
    print(f'{examples[""input_ids""].size()=}, {seq_length=}') if debug else None
    examples[""labels""] = examples[""input_ids""].clone()  # labels is hardcoded in HF so put it!
    eos_token_id = tokenizer.eos_token_id
    # assert eos_token_id == tokenizer.pad_token_id, 'Error: pad should be eos token'
    print(f'{tokenizer.pad_token_id=}, {tokenizer.eos_token_id=}') if debug else None
    seqs_to_drop: list[int] = [] # store idx to drop (to long), we don't want to modify the two lists at the same time as we are looping through them
    for idx, input_ids in enumerate(examples[""input_ids""]):
        # Find all occurrences of eos_token
        eos_positions = (input_ids == eos_token_id).nonzero(as_tuple=True)[0]
        if eos_positions.nelement() > 0:  # Check if eos_token is present --> if yes then make sure to trian on it then mask the remaining eos (assumes pad == eos)
            first_eos_position = eos_positions[0]
            examples[""attention_mask""][idx, first_eos_position] = 1  # Set the mask value to 1
            # Assert that the label for the first occurrence of eos_token is eos_token_id
            assert examples[""labels""][idx, first_eos_position] == eos_token_id, ""The label for the first eos_token is incorrect!""
            # # For all subsequent occurrences of eos_token, set their labels to -100
            # for subsequent_eos_position in eos_positions[1:]:
            #     examples[""labels""][idx, subsequent_eos_position] = -100
            #     assert examples[""labels""][idx, subsequent_eos_position] == -100, ""The label for the subsequent_eos_position incorrect! Should be -100.""
            # after first eos token mask everything (eos AND pad, hopefully that's all there but we can sanity check later)
            for desired_mask_idx in range(first_eos_position, seq_length):
                examples[""labels""][idx, desired_mask_idx] = -100
                assert examples[""labels""][idx, desired_mask_idx] == -100, ""The label for the desired_mask_idx incorrect! Should be -100.""
        elif remove_to_long_seqs:
            assert eos_positions.nelement() == 0, 'Error: there should be no eos if this if stmt is exexuted.'
            # record to drop this seq, has no eos so too long + flag says to drop it
            seqs_to_drop.append(idx)
        else:
            pass # nop: no eos in seq so too long, but keep it for training anyway
    # assert len(examples[""labels""]) == 0, 'Error: no labels were set'
    # -- Drop seqs with no eos
    if seqs_to_drop:
        examples[""input_ids""] = torch.stack([input_ids for idx, input_ids in enumerate(examples[""input_ids""]) if idx not in seqs_to_drop])
        examples[""attention_mask""] = torch.stack([mask for idx, mask in enumerate(examples[""attention_mask""]) if idx not in seqs_to_drop])
        examples[""labels""] = torch.stack([labels for idx, labels in enumerate(examples[""labels""]) if idx not in seqs_to_drop])
    return examples


Update, I made this better. Now if seq does not have eos at all you can remove that seq or chose to train on it
def raw_ds_2_lm_ds_mask_eos_pad_toks(
        raw_dataset, 
        tokenizer, 
        max_length: int,

        raw_str_2_desired_str: Optional[callable] = None, # either return {'text': examples['text']} or preprocess str to get what you need e.g. {'text': f""[ex['nl'] ex['fl'] {tok.eos_token}]"" for ex in examples}
        desired_dataset_column: str = 'text', # good val to use if hf str ds already pre-processed for you: 'text',

        method_to_remove_columns: str = 'keys',

        padding: str = 'max_length',
        truncation: bool = True, 
        return_tensors: str = 'pt',

        batched: bool = True, # Setting `batched=True` in the `dataset.map` function of Hugging Face's datasets library processes the data in batches rather than one item at a time, significantly speeding up the tokenization and preprocessing steps.
        streaming: bool = False,

        format: str = 'torch',
        # get_lm_examples_function = get_lm_examples_1st_eos_mask_remaining_eos,
        ):
    """""" """"""
    # - Get desired str dataset
    if raw_str_2_desired_str is None:
        get_desired_examples_str_function = lambda examples: {'text': examples[desired_dataset_column]} if raw_str_2_desired_str is not None else raw_str_2_desired_str 
    else:
        get_desired_examples_str_function = raw_str_2_desired_str
    desired_examples_str_dataset = raw_dataset.map(get_desired_examples_str_function, batched=batched) # note: we can't remove all str columns here or we will remove the ones we want to tokenize by accident

    # - Get tokenized data set
    desired_examples_str_dataset = desired_examples_str_dataset.with_format(format)  # annoying that return tensors in the tokenizer on it's own doesn't put it into a pt tensor, so for now we keep both.
    remove_str_columns = get_column_names(desired_examples_str_dataset, streaming, method_to_remove_columns)  # remove all keys that are not tensors to avoid bugs in collate function in task2vec's pytorch data loader
    tokenize_function = lambda examples: tokenizer(examples[desired_dataset_column], padding=padding, max_length=max_length, truncation=truncation, return_tensors=return_tensors)
    tokenized_datasets = desired_examples_str_dataset.map(tokenize_function, batched=batched, remove_columns=remove_str_columns)

    # - Get lm data set
    # get_lm_examples_function = lambda examples : group_texts(examples, block_size)
    get_lm_examples_function = lambda examples : get_lm_examples_1st_eos_mask_remaining_eos(examples, tokenizer)
    lm_dataset = tokenized_datasets.map(get_lm_examples_function, batched=batched)
    return lm_dataset

def get_lm_examples_1st_eos_mask_remaining_eos(
        examples,
        tokenizer: AutoTokenizer, 
        
        # desired_dataset_column: str = 'text',
        # method_to_remove_columns: str = 'keys',

        remove_to_long_seqs: bool = False,
        # format: str = 'torch',
        ) -> dict[str, torch.Tensor]:
    """""" 
    Train only on first occurence of eos. The remaining eos are masked out. If 
    - train up to 1st ocurrence of eos token, mask out the rest of the eos tokens.
    - drop or not seqs that are too long, i.e., have no eos token.
    
    Assumes: pad == eos

    ref: https://stackoverflow.com/questions/76633368/how-does-one-set-the-pad-token-correctly-not-to-eos-during-fine-tuning-to-avoi
    """"""
    # - Get lm example
    examples[""labels""] = examples[""input_ids""].clone()  # labels is hardcoded in HF so put it!
    eos_token_id = tokenizer.eos_token_id
    assert eos_token_id == tokenizer.pad_token_id, 'Error: pad should be eos token'
    seqs_to_drop: list[int] = [] # store idx to drop (to long), we don't want to modify the two lists at the same time as we are looping through them
    for idx, input_ids in enumerate(examples[""input_ids""]):
        # Find all occurrences of eos_token
        eos_positions = (input_ids == eos_token_id).nonzero(as_tuple=True)[0]
        if eos_positions.nelement() > 0:  # Check if eos_token is present --> if yes then make sure to trian on it then mask the remaining eos (assumes pad == eos)
            first_eos_position = eos_positions[0]
            examples[""attention_mask""][idx, first_eos_position] = 1  # Set the mask value to 1
            # Assert that the label for the first occurrence of eos_token is eos_token_id
            assert examples[""labels""][idx, first_eos_position] == eos_token_id, ""The label for the first eos_token is incorrect!""
            # For all subsequent occurrences of eos_token, set their labels to -100
            for subsequent_eos_position in eos_positions[1:]:
                examples[""labels""][idx, subsequent_eos_position] = -100
                assert examples[""labels""][idx, subsequent_eos_position] == -100, ""The label for the subsequent_eos_position incorrect! Should be -100.""
        elif remove_to_long_seqs:
            assert eos_positions.nelement() == 0, 'Error: there should be no eos if this if stmt is exexuted.'
            # record to drop this seq, has no eos so too long + flag says to drop it
            seqs_to_drop.append(idx)
        else:
            pass # nop: no eos in seq so too long, but keep it for training anyway
    # assert len(examples[""labels""]) == 0, 'Error: no labels were set'
    # -- Drop seqs with no eos
    if seqs_to_drop:
        examples[""input_ids""] = torch.stack([input_ids for idx, input_ids in enumerate(examples[""input_ids""]) if idx not in seqs_to_drop])
        examples[""attention_mask""] = torch.stack([mask for idx, mask in enumerate(examples[""attention_mask""]) if idx not in seqs_to_drop])
        examples[""labels""] = torch.stack([labels for idx, labels in enumerate(examples[""labels""]) if idx not in seqs_to_drop])
    return examples

train script:
""""""
Refs:
    - https://claude.ai/chat/ad5c9e18-beb4-48fb-9f43-a2ba463ce158
    - https://chatgpt.com/c/349f2c8a-949e-444d-ae3c-8ca60ba77831
""""""
import glob
import os
import numpy as np
import torch
from transformers import GPT2Tokenizer, GPT2LMHeadModel, TrainingArguments, Trainer
from transformers import AutoModelForCausalLM, AutoTokenizer
from datasets import load_dataset, load_metric
from typing import Dict, Tuple, Optional
from pathlib import Path
import evaluate

from utils import eval_hf
from utils import raw_ds_2_lm_ds_mask_eos_pad_toks

def compute_metrics(eval_pred: Tuple[np.ndarray, np.ndarray],
                    path: str = 'accuracy',
                    ) -> Dict[str, float]:
    """"""
    Compute the accuracy of the model.

    Args:
    eval_pred: A tuple containing the model predictions and labels.

    Returns:
    A dictionary with the accuracy score.
    
    TODO: document properly what accuracy is. Is it tfa, ara, exact string match, avg acc (wrt length etc.) ref: https://huggingface.co/spaces/evaluate-metric/accuracy
    """"""
    metric = evaluate.load(path=path)   # load metric from file or hf
    predictions, references = eval_pred
    predictions = np.argmax(predictions, axis=1)
    return metric.compute(predictions=predictions, references=references)

def preprocess_function_proofnet_simple(examples: Dict[str, list], tokenizer: GPT2Tokenizer, max_length: int = 512) -> Dict[str, torch.Tensor]:
    """"""
    Preprocess the input data for the proofnet dataset.

    Args:
    examples: The examples to preprocess.
    tokenizer: The tokenizer for encoding the texts.

    Returns:
    The processed model inputs.
    """"""
    # - Get raw string ins,outs (so deal with HF data set columns at str level)
    inputs: list[str] = [f""{examples['nl_statement'][i]}{tokenizer.eos_token}{examples['formal_statement'][i]}"" for i in range(len(examples['nl_statement']))]
    # - Get tokenized ins,outs (so remove irrelevant ""string"" columns to get only ""tensor"" relevant columns)
    model_inputs = tokenizer(inputs, max_length=max_length, padding=""max_length"", truncation=True, return_tensors=""pt"")
    # - Get lm ins,outs for training e.g., deal with padd, masks etc.
    labels = model_inputs.input_ids.clone()
    labels[labels == tokenizer.pad_token_id] = -100
    model_inputs[""labels""] = labels
    return model_inputs

def setup_and_train_proofnet(
        # pretrained_model_name_or_path: str = ""gpt2"", 
        # pretrained_model_name_or_path: str = ""openai-community/gpt2-xl"", 
        pretrained_model_name_or_path: str = ""meta-llama/Meta-Llama-3.1-8B"", 
        path: str = ""hoskinson-center/proofnet"",
        output_dir_train: str = '~/tmp/proofnet/train',
        output_dir_val: Optional[str] = None,  # we are training on the val set so no val set
        output_dir_test: str = '~/tmp/proofnet/test',
        path_to_save_model: Optional[str] = None,  # suggested path: '~/tmp/proofnet/model' then expanduser in py code
        num_train_epochs: int = 3,
        per_device_train_batch_size: Optional[int] = 2,
        per_device_eval_batch_size: Optional[int] = 2,
        learning_rate: float = 5e-5,
        weight_decay: float = 0.01,
        max_grad_norm: float = 1.0, 
        lr_scheduler_type = 'cosine', # https://discord.com/channels/879548962464493619/1227708244697284724/1227708244697284724
        warmup_ratio=0.01,  # copying alpaca for now, number of steps for a linear warmup,  https://discord.com/channels/879548962464493619/1227708244697284724/1227708244697284724
        optim='paged_adamw_32bit',
        gradient_accumulation_steps = 2, # Allows to process effective_batch_size = gradient_accumulation_steps * batch_size, num its to accumulate before opt update step
        gradient_checkpointing: Optional[bool] = True,
        report_to: str = 'none',  # recommended values 'wandb' or `none`
        ) -> None:
    """"""
    Set up the environment, preprocess the dataset, and train the model.

    export CUDA_VISIBLE_DEVICES=7

    Args:
    tokenizer_name: The name of the tokenizer.
    model_name: The name of the model.
    dataset_path: The path to the dataset.
    """"""
    # Clear CUDA cache to free up memory
    torch.cuda.empty_cache()

    # Load tokenizer and model
    if pretrained_model_name_or_path == ""gpt2"":
        tokenizer = GPT2Tokenizer.from_pretrained(pretrained_model_name_or_path, max_length=1024)
        if tokenizer.pad_token_id is None:
            tokenizer.pad_token = tokenizer.eos_token
            print(f'{tokenizer.pad_token=}')
        print(f'{tokenizer.eos_token=}\n{tokenizer.eos_token_id=}')
        model = GPT2LMHeadModel.from_pretrained(pretrained_model_name_or_path)
        device = torch.device(f""cuda:{0}"" if torch.cuda.is_available() else ""cpu"")
        model = model.to(device)
        max_length: int = tokenizer.model_max_length
        print(f'{max_length=}')
    elif pretrained_model_name_or_path == ""openai-community/gpt2-xl"":
        tokenizer = GPT2Tokenizer.from_pretrained(pretrained_model_name_or_path, max_length=1024)
        if tokenizer.pad_token_id is None:
            tokenizer.pad_token = tokenizer.eos_token
            print(f'{tokenizer.pad_token=}')
        print(f'{tokenizer.eos_token=}\n{tokenizer.eos_token_id=}')
        model = GPT2LMHeadModel.from_pretrained(pretrained_model_name_or_path)
        device = torch.device(f""cuda:{0}"" if torch.cuda.is_available() else ""cpu"")
        model = model.to(device)
        max_length: int = tokenizer.model_max_length
        print(f'{max_length=}') 
    elif pretrained_model_name_or_path == ""meta-llama/Meta-Llama-3.1-8B"":
        torch_dtype = torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float32 
        model = AutoModelForCausalLM.from_pretrained(pretrained_model_name_or_path, torch_dtype=torch_dtype)
        # tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name_or_path, padding_side=""right"", use_auth_token=True)
        tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name_or_path, padding_side=""right"")
        print(f'{tokenizer.pad_token=} {tokenizer.eos_token_id=}')
        tokenizer.pad_token = tokenizer.eos_token if tokenizer.pad_token_id is None else tokenizer.pad_token
        print(f'{tokenizer.pad_token=} {tokenizer.eos_token_id=}')
        # get context length for setting max length for training
        if hasattr(model.config, ""context_length""):
            # SEEMS IT IS NOT IN THE model.config
            print(""Context length:"", model.config.context_length)
            max_length: int = model.config.context_length
        else:
            print(f""Context length not found in model.config, so using your default or hardcoded value. Model is {pretrained_model_name_or_path=}."")
            # max_length: int = 4096
            max_length: int = 8192
            # max_length: int = 128  # for debugging
            # max_length: int = 128_000  # ref: https://huggingface.co/meta-llama/Meta-Llama-3.1-8B
            print(f'->{max_length=}')
    else:
        raise ValueError(f""Model {pretrained_model_name_or_path} not supported."")
    print(""Number of parameters:"", sum(p.numel() for p in model.parameters()))

    # - Load the dataset
    print(f'-Load the dataset')
    ## Proofnet
    # dataset_val = load_dataset(path, split='validation')
    # dataset_test = load_dataset(path, split='test')
    # # Preprocess the dataset
    # if path == ""hoskinson-center/proofnet"":
    #     preprocess_function = preprocess_function_proofnet_simple
    #     # note: text field is usually more common!
    #     val_dataset = dataset_val.map(lambda examples: preprocess_function(examples, tokenizer), batched=True, remove_columns=[""nl_statement"", ""formal_statement""])
    #     test_dataset = dataset_test.map(lambda examples: preprocess_function(examples, tokenizer), batched=True, remove_columns=[""nl_statement"", ""formal_statement""])
    ## C4
    # train_dataset = load_dataset(path='allenai/c4', name='en', split='train', streaming=True)
    # eval_dataset = load_dataset(path='allenai/c4', name='en', split='validation', streaming=True)
    # train_dataset = raw_ds_2_lm_ds_mask_eos_pad_toks(train_dataset, tokenizer, max_length)
    # eval_dataset = raw_ds_2_lm_ds_mask_eos_pad_toks(eval_dataset, tokenizer, max_length)

    # json files for putnam are not consistent and it seems they have to be: https://chatgpt.com/c/9cecca7d-d50d-42e2-b2d3-c1057bc21ef2 solve later
    # ~/putnam-math/data/Putnam_MATH_variations_static3/original/test
    # json_files = glob.glob(os.path.expanduser('~/putnam-math/data/Putnam_MATH_original_static3/test/**/*.json'), recursive=True)
    # train_dataset = load_dataset('json', data_files=json_files)
    # json_files = glob.glob(os.path.expanduser('~/putnam-math/data/Putnam_MATH_variations_static3/variations/test/**/*.json'), recursive=True)
    # eval_dataset = load_dataset('json', data_files=json_files)
    # train_dataset = raw_ds_2_lm_ds_mask_eos_pad_toks(train_dataset, tokenizer, max_length)
    # eval_dataset = raw_ds_2_lm_ds_mask_eos_pad_toks(eval_dataset, tokenizer, max_length)

    # Proofnet with 1st eos token train remaining eos not train
    from train.utils import raw_str_2_desired_af_str
    _raw_str_2_desired_af_str = lambda examples: raw_str_2_desired_af_str(examples, tokenizer)  # tokenizer needed to get eos tok to form right str to train on.
    train_dataset = load_dataset(path, split='validation')
    eval_dataset = load_dataset(path, split='test')
    train_dataset = raw_ds_2_lm_ds_mask_eos_pad_toks(train_dataset, tokenizer, max_length, raw_str_2_desired_str=_raw_str_2_desired_af_str)
    eval_dataset = train_dataset
    print(f'->{len(train_dataset)=} {len(eval_dataset)=}')
    # max_steps: int = (len(train_dataset) * num_train_epochs) // per_device_train_batch_size  # TODO: really?

    # Training arguments
    output_dir_train: Path = Path(output_dir_train).expanduser()
    output_dir_train.mkdir(parents=True, exist_ok=True)
    training_args = TrainingArguments(
        output_dir=output_dir_train,
        max_steps=2,  # TODO get rid of this in favour of 1 or 2 or 3 epochs
        # num_train_epochs=num_train_epochs, 
        gradient_accumulation_steps=gradient_accumulation_steps,  # based on alpaca https://github.com/tatsu-lab/stanford_alpaca, allows to process effective_batch_size = gradient_accumulation_steps * batch_size, num its to accumulate before opt update step
        gradient_checkpointing = gradient_checkpointing,  # TODO depending on hardware set to true?
        per_device_train_batch_size=per_device_train_batch_size,
        per_device_eval_batch_size=per_device_eval_batch_size,
        learning_rate=learning_rate,
        weight_decay=weight_decay, 
        max_grad_norm=max_grad_norm, # TODO once real training change?
        lr_scheduler_type=lr_scheduler_type,  # TODO once real training change? using what I've seen most in vision 
        warmup_ratio=warmup_ratio,
        optim=optim,
        # logging_strategy='epoch', # TODO
        save_steps=100, # Save checkpoint every 500 steps
        save_total_limit=3, # save last 3
        logging_steps=10,  # Frequency of logging steps
        logging_first_step=True,
        logging_dir=output_dir_train,
        evaluation_strategy='no',  # ""no""`: No evaluation is done during training. no can be good to avoid memory issues.
        # evaluation_strategy=""steps"",  # TODO Evaluate model at specified steps
        # eval_steps=110,  # TODO Evaluate every 100 steps
        # remove_unused_columns=False,  # TODO https://stackoverflow.com/questions/76879872/how-to-use-huggingface-hf-trainer-train-with-custom-collate-function/76929999#76929999 , https://claude.ai/chat/475a4638-cee3-4ce0-af64-c8b8d1dc0d90
        report_to=report_to,  # options I recommend: 'none', 'wandb'
        fp16=False,  # never ever set to True
        bf16=torch.cuda.is_bf16_supported(),
        # full_determinism=True,  # TODO periphery, Ensure reproducibility
        # torchdynamo=""nvfuser"",  # TODO periphery, Use NVFuser backend for optimized torch operations
        # dataloader_prefetch_factor=2,  # TODO periphery, Number of batches to prefetch
        # dataloader_pin_memory=True,  # TODO periphery, Pin memory in data loaders for faster transfer to GPU
        # dataloader_num_workers=16,  # TODO Number of subprocesses for data loading
    )

    # Initialize the Trainer 
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset,
        eval_dataset=eval_dataset,  # set to None if eval is giving you memory issues
        tokenizer=tokenizer,
        compute_metrics=compute_metrics
    )
    # Train the model
    trainer.train()

    # Evaluate the model
    if output_dir_test is not None:
        output_dir_test: Path = Path(output_dir_test).expanduser()
        output_dir_test.mkdir(parents=True, exist_ok=True)
        eval_args = TrainingArguments(output_dir=output_dir_test, per_device_eval_batch_size=per_device_eval_batch_size, fp16=False, bf16=torch.cuda.is_bf16_supported(), report_to=report_to)
        trainer = Trainer(model=model, args=eval_args, train_dataset=None, eval_dataset=eval_dataset)
        # results: dict[str, float] = trainer.evaluate(test_dataset)
        results: dict[str, float] = eval_hf(trainer, name='', path=path, split='test')
        print(f'{path=} split=test {results=}')

    # Save the trained model
    if path_to_save_model is not None:
        model.save_pretrained(path_to_save_model)

def main() -> None:
    """"""
    Main function to execute the model training and evaluation.
    """"""
    setup_and_train_proofnet()

if __name__ == ""__main__"":
    import time
    start_time = time.time()
    main()
    print(f""Time taken: {time.time() - start_time:.2f} seconds, or {(time.time() - start_time) / 60:.2f} minutes, or {(time.time() - start_time) / 3600:.2f} hours.\a"")

code repo: https://github.com/brando90/snap-cluster-setup/blob/9778140d8eb378f7c7873ec3fa906d0b01064031/py_src/train/simple_train2.py#L1

OK I think this is the code that train on first occurrence of eos and makes sure the rest are NOT trained on (feedback welcomed):
def collate_fn_train_only_first_eos_token_mask_everything_after_it(data: list[dict[str, str]], 
                                                                    tokenizer: PreTrainedTokenizer, 
                                                                    max_length: int=1024,  # GPT2 default, likely worth you change it! This default might cause bugs.
                                                                    ) -> dict[str, torch.Tensor]:
    """""" Train only on first occurence of eos. The remaining eos are masked out.

    Sometimes the model might not have a padding token. Sometimes people set the padding token to be the eos token.
    But sometimes this seems to lead to the model to predict eos token to much. 
    So instead of actually using the pad token that was set to the eos token, we instead mask out all excesive eos tokens that act as pads 
    and leave the first eos token at the end to be predicted -- since that is the only one that semantically means end of sequence 
    and therby by not training on random eos at the end by masking it not unncesserily shift/amplify the distribution of eos. 
    
    ref: https://discuss.huggingface.co/t/why-does-the-falcon-qlora-tutorial-code-use-eos-token-as-pad-token/45954/13?u=brando 
    ref: https://chat.openai.com/share/02d16770-a1f3-4bf4-8fc2-464286daa8a1
    ref: https://claude.ai/chat/80565d1f-ece3-4fad-87df-364ce57aec15 on when to call .clone()
    """"""
    # we are training full context length for llama so remove code bellow, if it tries to pad hopefully it throws an error
    # -- Ensure tokenizer has a padding token
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    # -- Extract sequences
    # sequences: list[str] = [example.get(""text"", """") or """" for example in data]
    sequences: list[str] = []
    for idx, example in enumerate(data):
        # Retrieve the value for ""text"" from the dictionary or default to an empty string if not present or falsy. ref: https://chat.openai.com/share/bead51fe-2acf-4f05-b8f7-b849134bbfd4
        text: str = example.get(""text"", """") or """"
        sequences.append(text)
    # -- Tokenize the sequences
    tokenized_data = tokenizer(sequences, padding=""max_length"", max_length=max_length, truncation=True, return_tensors=""pt"")
    tokenized_data[""labels""] = tokenized_data[""input_ids""].clone()  # labels is hardcoded in HF so put it!
    # -- Set the mask value for the first eos_token in each sequence to 1 and remaining to -100
    eos_token_id = tokenizer.eos_token_id
    for idx, input_ids in enumerate(tokenized_data[""input_ids""]):
        # Find all occurrences of eos_token
        eos_positions = (input_ids == eos_token_id).nonzero(as_tuple=True)[0]
        if eos_positions.nelement() > 0:  # Check if eos_token is present
            first_eos_position = eos_positions[0]
            tokenized_data[""attention_mask""][idx, first_eos_position] = 1  # Set the mask value to 1
            
            # Assert that the label for the first occurrence of eos_token is eos_token_id
            assert tokenized_data[""labels""][idx, first_eos_position] == eos_token_id, ""The label for the first eos_token is incorrect!""
            
            # For all subsequent occurrences of eos_token, set their labels to -100
            for subsequent_eos_position in eos_positions[1:]:
                tokenized_data[""labels""][idx, subsequent_eos_position] = -100
                assert tokenized_data[""labels""][idx, subsequent_eos_position] == -100, ""The label for the subsequent_eos_position incorrect! Should be -100.""
    return tokenized_data

reference: Why does the falcon QLoRA tutorial code use eos_token as pad_token?
",pytorch
on which device is a python dictionary containing pytorch tensors that are loaded on cuda,"I have a pytorch face detection model that returns bounding boxes and a few facial landmarks as a dictionary. The bounding boxes and the landmarks are pytorch tensors that where moved to the GPU. When I print the dictionary, it looks for example like this:
{'rects': tensor([[405.5453, 126.3100, 646.7983, 454.6852]], device='cuda:0'), 'points': tensor([[[477.5181, 247.7193],
         [591.4567, 247.1899],
         [544.2579, 328.7857],
         [494.3871, 371.9922],
         [583.4272, 370.3429]]], device='cuda:0'), 'scores': tensor([0.9999], device='cuda:0'), 'image_ids': tensor([0], device='cuda:0').

Does the dictionary lie on the CPU and only link to the GPU where the tensors are actually on? Or is the dictionary also loaded on the GPU now? Also, if the dictionary contained one list and one tensor like this:
{'rects': [405.5453, 126.3100, 646.7983, 454.6852] , 'points': tensor([[[477.5181, 247.7193],
         [591.4567, 247.1899],
         [544.2579, 328.7857],
         [494.3871, 371.9922],
         [583.4272, 370.3429]]], device='cuda:0')}.

Where is the dictionary then located?
Maybe I'm just confused on what device=""cuda:0"" actually means, so if someone could answer I would highly appreciate it!
","The dictionary is just a python object, it has no device associated with it.
The dictionary keys and values are pointers to other python objects. The dictionary itself has no idea what a device is and does not interact with that aspect of pytorch tensors. The dictionary only knows the pointers for the key/value objects. The device(s) in question are completely independent from the dictionary.
A pytorch tensor is an object that acts as a wrapper around some data. The tensor object has a reference on CPU memory used to interact with the tensor itself, as well as a .data parameter that references the underlying data (which may be on CPU or GPU).
For example, you can put tensors from multiple devices in the same dictionary:
x1 = torch.randn(5)
x2 = torch.randn(5).to('cuda:0')
x3 = torch.randn(5).to('cuda:1')

d = {
    'tensor1' : x1,
    'tensor2' : x2,
    'tensor3' : x3,
}

You can do this because the device aspect is only relevant to the pytorch tensors - it doesn't matter for the dictionary itself.
For your example of:
{'rects': [405.5453, 126.3100, 646.7983, 454.6852] , 'points': tensor([[[477.5181, 247.7193],
         [591.4567, 247.1899],
         [544.2579, 328.7857],
         [494.3871, 371.9922],
         [583.4272, 370.3429]]], device='cuda:0')}

The dictionary is stored in CPU memory. The dictionary stores pointers to the value objects in the dictionary.
The key rects references a python list stored in CPU memory.
They key points references the CPU pointer of a pytorch tensor object. This tensor has a .data parameter that references data on the GPU, but the dictionary itself doesn't know about that.
",pytorch
how to get a histogram of pytorch tensors in batches,"Is there a way to get the histograms of torch tensors in batches?
For Example:
x is a tensor of shape (64, 224, 224)
# x will have shape of (64, 256)
x = batch_histogram(x, bins=256, min=0, max=255)

","As suggested in the Pytorch Issues#99719, you can do this by torch.Tensor.scatter_add_. scatter_add_ is more memory efficient than torch.nn.functional.one_hot.
Similar to @user118967's answer:
# https://github.com/pytorch/pytorch/issues/99719#issuecomment-1664135524 
def batch_histogram(data_tensor, num_classes=-1):
    """"""
    Computes histograms, even if in batches (as opposed to torch.histc and torch.histogram).
    Arguments:
        data_tensor: a D1 x ... x D_n torch.LongTensor
        num_classes (optional): the number of classes present in data.
                                If not provided, tensor.max() + 1 is used (an error is thrown if tensor is empty).
    Returns:
        A D1 x ... x D_{n-1} x num_classes 'result' torch.LongTensor,
        containing histograms of the last dimension D_n of tensor,
        that is, result[d_1,...,d_{n-1}, c] = number of times c appears in tensor[d_1,...,d_{n-1}].
    """"""
    maxd = data_tensor.max()
    nc = (maxd+1) if num_classes <= 0 else num_classes
    hist = torch.zeros((*data_tensor.shape[:-1], nc), dtype=data_tensor.dtype, device=data_tensor.device)
    ones = torch.tensor(1, dtype=hist.dtype, device=hist.device).expand(data_tensor.shape)
    hist.scatter_add_(-1, ((data_tensor * nc) // (maxd+1)).long(), ones)
    return hist

with the test cases in Google colab here
",pytorch
autograd error caused by relu in pytorch,"I am using a residual neural network for a classification task. Somehow adding or omitting a ReLU activation causes the autograd to fail. I would be grateful for any insights on the reason for this? It cannot make any sense of it. ReLU is not an inplace operation, is it? Error message:
RuntimeError: one of the variables needed for gradient computation has been modified by an inplace operation
Here is the network architecture. The 3rd to last line is what causes the issue when not commented out.
class ResidualBlock(nn.Module):
    def __init__(self, num_filters, kernel_size):
        super(ResidualBlock, self).__init__()
        self.conv1 = nn.Conv1d(num_filters, num_filters, kernel_size=kernel_size, padding='same')
        self.bn1 = nn.BatchNorm1d(num_filters)
        self.conv2 = nn.Conv1d(num_filters, num_filters, kernel_size=kernel_size, padding='same')
        self.bn2 = nn.BatchNorm1d(num_filters)

    def forward(self, x):
        shortcut = x
        out = F.relu(self.bn1(self.conv1(x)))
        out = self.bn2(self.conv2(out))
        out = F.relu(out) # causes the issue when not commented out
        out += shortcut
        return out

Below is a minimal working example. I am using Python 3.12 and torch 2.5.1.
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import DataLoader, TensorDataset

# Define the ResidualBlock
class ResidualBlock(nn.Module):
    def __init__(self, num_filters, kernel_size):
        super(ResidualBlock, self).__init__()
        self.conv1 = nn.Conv1d(num_filters, num_filters, kernel_size=kernel_size, padding='same')
        self.bn1 = nn.BatchNorm1d(num_filters)
        self.conv2 = nn.Conv1d(num_filters, num_filters, kernel_size=kernel_size, padding='same')
        self.bn2 = nn.BatchNorm1d(num_filters)

    def forward(self, x):
        shortcut = x
        out = F.relu(self.bn1(self.conv1(x)))
        out = self.bn2(self.conv2(out))
        out = F.relu(out) # causes the issue
        out += shortcut
        return out


class SimpleModel(nn.Module):
    def __init__(self, num_filters, kernel_size):
        super(SimpleModel, self).__init__()
        self.res_block = ResidualBlock(num_filters, kernel_size)
        self.fc = nn.Linear(num_filters, 1)

    def forward(self, x):
        x = self.res_block(x)
        x = x.mean(dim=2)
        x = self.fc(x)
        return x

torch.manual_seed(42)
num_samples = 1000
sequence_length = 32
num_filters = 16

X = torch.randn(num_samples, num_filters, sequence_length)  # Random input
y = torch.sum(X, dim=(1, 2), keepdim=True)  # Simple target (sum of all values)


dataset = TensorDataset(X, y)
dataloader = DataLoader(dataset, batch_size=32, shuffle=True)


model = SimpleModel(num_filters=num_filters, kernel_size=3)
criterion = nn.MSELoss() 
optimizer = torch.optim.Adam(model.parameters(), lr=0.001)


epochs = 5
for epoch in range(epochs):
    model.train()
    epoch_loss = 0.0
    for batch_X, batch_y in dataloader:
        optimizer.zero_grad()
        outputs = model(batch_X)
        loss = criterion(outputs, batch_y)
        loss.backward()
        optimizer.step()
        epoch_loss += loss.item()
    print(f""Epoch {epoch+1}/{epochs}, Loss: {epoch_loss/len(dataloader):.4f}"")

print(""Training complete!"")

","The inplace operation is this:
out += shortcut

The relu needs its own output to compute its gradient! Thus you are doing an inplace operation on the output of the relu, which it needed to compute its gradient in the backwards pass.
replacing it with
out = out + shortcut

should solve your problem.
In general, try to avoid using inplace functions in pytorch (such as += unless you know what you are doing

more details:
if you look at the pytorch code, the backwards pass for the relu is auto-generated from the following bit of code in pytorch/tools/autograd/derivatives.yaml
- name: relu(Tensor self) -> Tensor
  self: threshold_backward(grad, result, 0)
  result: auto_element_wise

What this does is it

takes the gradient of the output as grad
and the result of the relu as result
and returns the contents of grad where result is bigger than 0 and 0 otherwise.

Thus it does need the output (technically, it could have stored its input instead, but this is how it is implemented)
",pytorch
getting model class labels from torchvision pretrained models,"I am using a pre-trained Alexnet model (without fine-tuning) from torchvision. The issue is that even though I am able to run the model on some data and get the output probability distribution, I am unable to find class labels to map it to.
Following this official documentation
import torch
model = torch.hub.load('pytorch/vision:v0.6.0', 'alexnet', pretrained=True)
model.eval()

AlexNet(
  (features): Sequential(
    (0): Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))
    (1): ReLU(inplace=True)
    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)
    (3): Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
    (4): ReLU(inplace=True)
    (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)
    (6): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (7): ReLU(inplace=True)
    (8): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (9): ReLU(inplace=True)
    (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (11): ReLU(inplace=True)
    (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)
  )
  (avgpool): AdaptiveAvgPool2d(output_size=(6, 6))
  (classifier): Sequential(
    (0): Dropout(p=0.5, inplace=False)
    (1): Linear(in_features=9216, out_features=4096, bias=True)
    (2): ReLU(inplace=True)
    (3): Dropout(p=0.5, inplace=False)
    (4): Linear(in_features=4096, out_features=4096, bias=True)
    (5): ReLU(inplace=True)
    (6): Linear(in_features=4096, out_features=1000, bias=True)
  )
)

Following some steps on processing the image, I am able to use it to get an output for a single image as a (1,1000) dim vector which I will use a softmax on to get a probability distribution -
#Output - 

tensor([-1.6531e+00, -4.3505e+00, -1.8172e+00, -4.2143e+00, -3.1914e+00,
         3.4163e-01,  1.0877e+00,  5.9350e+00,  8.0425e+00, -7.0242e-01,
        -9.4130e-01, -6.0822e-01, -2.4097e-01, -1.9946e+00, -1.5288e+00,
        -3.2656e+00, -5.5800e-01,  1.0524e+00,  1.9211e-01, -4.7202e+00,
        -3.3880e+00,  4.3048e+00, -1.0997e+00,  4.6132e+00, -5.7404e-03,
        -5.3437e+00, -4.7378e+00, -3.3974e+00, -4.1287e+00,  2.9064e-01,
        -3.2955e+00, -6.7051e+00, -4.7232e+00, -4.1778e+00, -2.1859e+00,
        -2.9469e+00,  3.0465e+00, -3.5882e+00, -6.3890e+00, -4.4203e+00,
        -3.3685e+00, -5.0983e+00, -4.9006e+00, -5.5235e+00, -3.7233e+00,
        -4.0204e+00,  2.6998e-01, -4.4702e+00, -5.6617e+00, -5.4880e+00,
        -2.6801e+00, -3.2129e+00, -1.6294e+00, -5.2289e+00, -2.7495e+00,
        -2.6286e+00, -1.8206e+00, -2.3196e+00, -5.2806e+00, -3.7652e+00,
        -3.0987e+00, -4.1421e+00, -5.2531e+00, -4.6505e+00, -3.5815e+00,
        -4.0189e+00, -4.0008e+00, -4.5512e+00, -3.2248e+00, -7.7903e+00,
        -1.4484e+00, -3.8347e+00, -4.5611e+00, -4.3681e+00,  2.7234e-01,
        -4.0162e+00, -4.2136e+00, -5.4524e+00,  1.1744e+00, -4.7785e+00,
        -1.8335e+00,  4.1288e-01,  2.2239e+00, -9.9919e-02,  4.8216e+00,
        -8.4304e-01,  5.6911e-01, -4.0484e+00, -3.3013e+00,  2.8698e+00,
        -1.1419e+00, -9.1690e-01, -2.9284e+00, -2.6097e+00, -1.8213e-01,
        -2.5429e+00, -2.1095e+00,  2.2419e+00, -1.6280e+00,  7.4458e+00,
         2.3184e+00, -5.7408e+00, -7.4332e-01, -5.4066e+00,  1.5177e+01,
        -4.4737e-02,  1.8237e+00, -3.7741e+00,  9.2271e-01, -4.3687e-01,
        -1.4003e+00, -4.3026e+00,  6.3782e-01, -1.0808e+00, -1.4173e+00,
         2.6194e+00, -3.8418e+00,  1.1598e+00, -2.6876e+00, -3.6103e+00,
        -4.9281e+00, -4.1411e+00, -3.3603e+00, -3.4296e+00, -1.4997e+00,
        -2.8381e+00, -1.2843e+00,  1.5745e+00, -1.7449e+00,  4.2903e-01,
         3.1234e-01, -2.8206e+00,  3.6688e-01, -2.1033e+00,  1.6481e+00,
         1.4222e+00, -2.7303e+00, -3.6292e+00,  1.2864e+00, -2.5541e+00,
        -2.9663e+00, -4.1575e+00, -3.1954e+00, -4.6487e-01,  1.8916e+00,
        -7.4721e-01,  4.5986e+00, -2.5443e+00, -6.2003e+00, -1.3215e+00,
        -2.6225e+00,  9.9639e+00,  9.7772e+00,  9.6715e+00,  9.0857e+00,...

Where do I get the class labels from? I couldn't find any method that let me get that from the model object.
","You cannot, unfortunately, get class label names directly from the torchvision models. However, these models are trained on the ImageNet dataset (hence the 1000 classes).
You have to get the class name mapping off the web as far as I know; there's no way to get it off torch. Previously, you could download ImageNet directly using torchvision.datasets.ImageNet, which had a built-in label to class name converter. Now the download link isn't publicly available and requires a manual download, before it can be used by datasets.ImageNet.
So you can simply search for the class to label mapping of ImageNet online, rather than downloading the data or attempting with torch. Try here for example.
",pytorch
celery worker exited prematurely signal 11 trying to run a python script on button click from django view,"I am working on a Django app whose partial process is transcribing audio with timestamps. When a user clicks on a button from a web interface, the Django server launches a Python script that helps with transcribing.
Now, here are a few approaches I have tried already:
I have a separate transcribe.py file. When a user clicks the transcribe button from the web page, it accesses a view from the project app. However, after partially running the script, the Django server terminates from the terminal.
Since the Python script is a long-running process, I figured I should run the program in the background so the Django server doesn't terminate. So, I implemented Celery and Redis. First, the transcribe.py script runs perfectly well when I run it from the Django shell. However, it terminates once again when I try to execute it from the view/web page.

python manage.py shell

Since I implemented the celery worker part, the server doesn't terminate but the worker throws the following error.
[tasks]
  . transcribeApp.tasks.run_transcription

[2024-11-25 03:26:04,500: INFO/MainProcess] Connected to redis://localhost:6379/0
[2024-11-25 03:26:04,514: INFO/MainProcess] mingle: searching for neighbors
[2024-11-25 03:26:05,520: INFO/MainProcess] mingle: all alone
[2024-11-25 03:26:05,544: INFO/MainProcess] celery@user.local ready.
[2024-11-25 03:26:16,253: INFO/MainProcess] Task searchApp.tasks.run_transcription[c684bdfa-ec21-4b4e-9542-0ca1f7729682] received
[2024-11-25 03:26:16,255: INFO/ForkPoolWorker-15] Starting transcription process.
[2024-11-25 03:26:16,509: WARNING/ForkPoolWorker-15] /Users/user/Desktop/project/django_app/django_venv/lib/python3.12/site-packages/whisper/__init__.py:150: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(fp, map_location=device)

[2024-11-25 03:26:16,670: ERROR/MainProcess] Process 'ForkPoolWorker-15' pid:38956 exited with 'signal 11 (SIGSEGV)'
[2024-11-25 03:26:16,683: ERROR/MainProcess] Task handler raised error: WorkerLostError('Worker exited prematurely: signal 11 (SIGSEGV) Job: 0.')
Traceback (most recent call last):
  File ""/Users/user/Desktop/project/django_app/django_venv/lib/python3.12/site-packages/billiard/pool.py"", line 1265, in mark_as_worker_lost
    raise WorkerLostError(
billiard.einfo.ExceptionWithTraceback: 
""""""
Traceback (most recent call last):
  File ""/Users/user/Desktop/project/django_app/django_venv/lib/python3.12/site-packages/billiard/pool.py"", line 1265, in mark_as_worker_lost
    raise WorkerLostError(
billiard.exceptions.WorkerLostError: Worker exited prematurely: signal 11 (SIGSEGV) Job: 0.
""""""

The implementation looks like this,
# Views.py
from . import tasks
from django.shortcuts import render
from django.http import HttpResponse, JsonResponse

def trainVideos(request):
    try:
        tasks.run_transcription.delay()
        return JsonResponse({""status"": ""success"", ""message"": ""Transcription has started check back later.""})
    # return render(request, 'embed.html', {'data': data})
    except Exception as e:
        JsonResponse({""status"": ""error"", ""message"": str(e)})

Here is what the transcribe function looks like, where the celery worker throws the worker exited prematurely error.
# Add one or two audios possibly .wav, .mp3 in a folder,
# and provide the file path here.
# transcribe.py 

import whisper_timestamped as whisper
import os
def transcribeTexts(model_id, filePath):
    result = []
    fileNames = os.listdir()
    
    model = whisper.load_model(model_id)

    for files in fileNames:
        audioPath = filePath + ""/"" + files

        audio = whisper.load_audio(audioPath)

        result.append(model.transcribe(audio, language=""en""))
    
    return result
 model_id = ""tiny""
 audioFilePath = path/to/audio
 transcribeTexts(model_id, audioFilePath)

Install the following libraries to reproduce the problem:
 pip install openai-whisper
 pip3 install whisper-timestamped
 pip install Django
 pip install celery redis
 pip install redis-server

The Celery Implementation: # celery.py from project main_app directory
from __future__ import absolute_import, unicode_literals
import os
from celery import Celery

os.environ.setdefault('DJANGO_SETTINGS_MODULE', 'main_app.settings')

app = Celery('main_app')

app.config_from_object('django.conf:settings', namespace='CELERY')

app.autodiscover_tasks()

def debug_tasks(self):
    print(f""Request: {self.request!r}"")

tasks.py from the transcribe_app directory:
from __future__ import absolute_import, unicode_literals
from . import transcribe
from celery import shared_task

@shared_task
def run_transcription():
    transcribe.transcribe()
    return ""Transcription Completed...""

The settings.py is also updated with the following:
CELERY_BROKER_URL = 'redis://localhost:6379/0'
CELERY_BROKER_CONNECTION_RETRY_ON_STARTUP = True

Also, modified the init.py file from django_app
from __future__ import absolute_import, unicode_literals

from .celery import app as celery_app

__all__ = ('celery_app',) 

For this application, some of the libraries are dependent on particular versions. All libraries and packages are listed below:
Package              Version
-------------------- -----------
amqp                 5.3.1
asgiref              3.8.1
billiard             4.2.1
celery               5.4.0
certifi              2024.8.30
charset-normalizer   3.3.2
click                8.1.7
click-didyoumean     0.3.1
click-plugins        1.1.1
click-repl           0.3.0
Cython               3.0.11
Django               5.1.2
django-widget-tweaks 1.5.0
dtw-python           1.5.3
faiss-cpu            1.9.0
ffmpeg               1.4
filelock             3.16.1
fsspec               2024.9.0
huggingface-hub      0.25.2
idna                 3.10
Jinja2               3.1.4
kombu                5.4.2
lfs                  0.2
llvmlite             0.43.0
MarkupSafe           3.0.1
more-itertools       10.5.0
mpmath               1.3.0
msgpack              1.1.0
networkx             3.3
numba                0.60.0
numpy                2.0.2
packaging            24.1
panda                0.3.1
pillow               10.4.0
pip                  24.3.1
prompt_toolkit       3.0.48
pydub                0.25.1
python-dateutil      2.9.0.post0
PyYAML               6.0.2
redis                5.2.0
regex                2024.9.11
requests             2.32.3
safetensors          0.4.5
scipy                1.14.1
semantic-version     2.10.0
setuptools           75.1.0
setuptools-rust      1.10.2
six                  1.16.0
sqlparse             0.5.1
sympy                1.13.3
tiktoken             0.8.0
tokenizers           0.20.1
torch                2.4.1
torchaudio           2.4.1
torchvision          0.19.1
tqdm                 4.66.5
transformers         4.45.2
txtai                7.4.0
typing_extensions    4.12.2
tzdata               2024.2
urllib3              2.2.3
vine                 5.1.0
wcwidth              0.2.13
whisper-timestamped  1.15.4

Overall, when I run the program independently, it works perfectly fine. But within Django, it just terminates however I execute it. I thought one of the reasons might be since I am loading long audios, so I chunked it and tried to run the transcribe.py program using the user interface; however, it's the same thing worker exited prematurely, signal 11 (SIGSEGV) Job: 0. I tried changing memory pool size to a higher level for a worker, didn't work. I am unsure exactly what needs to be done to run the transcribe.py file within Django since most known methods are not working for me. I may have missed something, so please help me figure this out. Thank you for your time.
","sigsegv often comes when you try to access memory that's not accessible by your program, see here. I could re-create the code and it worked completely fine on my end. Here are the probable reasons why this happened to you:

The pool type you specified in your celery command didn't workout successfully, --pool=solo seems to work since it doesn't fork the process.
Part of the code is executed as root and other parts aren't.
The file path you provided isn't correct, or it exists with wrong permissions.
Maybe you're executing this on a virtual machine with very limited ram, thus no memory is available since the AI model and libraries you've loaded are already heavy?
There's an actual problem with libc on your machine or Celery itself, but the problem isn't clear.


I'll walk you through how I re-created your code, and maybe you made a typo or a little mistake that resulted in the error you mentioned.
django-admin startproject project101
cd project101
python3 manage.py startapp app101

project101/urls.py:
from django.contrib import admin
from django.urls import path, include

urlpatterns = [
    path('admin/', admin.site.urls),
    path('', include(""app101.urls""))
]

project101/settings.py:
INSTALLED_APPS = [
    # ...
    
    'app101'
]

# put this at the end of settings.py
CELERY_BROKER_URL = 'redis://localhost:6379/0'
CELERY_BROKER_CONNECTION_RETRY_ON_STARTUP = True

project101/celery.py
from __future__ import absolute_import, unicode_literals
import os
from celery import Celery

os.environ.setdefault('DJANGO_SETTINGS_MODULE', 'project101.settings')

app = Celery('project101')

app.config_from_object('django.conf:settings', namespace='CELERY')

app.autodiscover_tasks()

def debug_tasks(self):
    print(f""Request: {self.request!r}"")


project101/init.py:
from __future__ import absolute_import, unicode_literals

from .celery import app as celery_app

__all__ = ('celery_app',) 

app101/views.py:
from . import tasks
from django.shortcuts import render
from django.http import HttpResponse, JsonResponse

def trainVideos(request):
    try:
        tasks.run_transcription.delay()
        return JsonResponse({""status"": ""success"", ""message"": ""Transcription has started check back later.""})
    # return render(request, 'embed.html', {'data': data})
    except Exception as e:
        JsonResponse({""status"": ""error"", ""message"": str(e)})


app101/urls.py:
from django.urls import path, include
from . import views

urlpatterns = [
    path('transcribe', views.trainVideos)
]


app101/tasks.py:
from __future__ import absolute_import, unicode_literals
from . import transcribe
from celery import shared_task

@shared_task
def run_transcription():
    transcribe.transcribe()
    return ""Transcription Completed...""


app101/transcribe.py:

import whisper_timestamped as whisper
import os

def transcribeTexts(model_id, audio_directory_path):
    result = []
    fileNames = os.listdir(audio_directory_path)
    
    model = whisper.load_model(model_id)

    for files in fileNames:
        print(files)
        audioPath = audio_directory_path + ""/"" + files

        audio = whisper.load_audio(audioPath)

        result.append(model.transcribe(audio, language=""en""))
    print(result)
    return result

def transcribe():
    model_id = ""tiny""
    audio_directory_path = 'audio_sample'
    transcribeTexts(model_id, audio_directory_path)


Note that audio_sample is a folder outside app101, it has the same level as app101 and project101. You could make it in another folder but make sure to specify the correct directory path. I've added directory structure below.
.
├── app101
│   ├── admin.py
│   ├── apps.py
│   ├── __init__.py
│   ├── migrations
│   ├── models.py
│   ├── __pycache__
│   ├── tasks.py
│   ├── tests.py
│   ├── transcribe.py
│   ├── urls.py
│   └── views.py
├── audio_sample
│   └── some_audio.mp3
├── db.sqlite3
├── manage.py
└── p101
    ├── asgi.py
    ├── celery.py
    ├── __init__.py
    ├── __pycache__
    ├── settings.py
    ├── urls.py
    └── wsgi.py


After this, run the following commands on separate terminals:
python3 manage.py runserver

celery -A project101 worker --pool=solo -l info

This should make your project up and running. To test, send a get request to http://localhost:8000/transcribe or simply open it in your browser.
Note the following:

This was just to walk you through how to successfully run celery, don't forget to implement the code in your project and make migrations accordingly.
You can run the Celery command with different arguments, such as changing pool from solo to gevent. --pool=solo seems to work fine.
Execute everything as the same user, either root (not really recommended) or normal user.
Make sure all files have correct permissions.

",pytorch
llm output repeating itself,"I am currently following this tutorial on making a basic LLM that spews Shakespeare like text(The full code for the transformer is at the the end). I am at the end but when I train it and get an output the output just keeps repeating itself with the same stuff. Here is my code
import tiktoken
import torch
import torch.nn as nn
from torch.nn import functional as F
from functions.encode import encode_chars
from functions.character_amount import character_amount
from functions.train_test_split import train_test_split
from functions.decoding import decoding
with open(r'example_shakespeare_text.txt') as file:
    file = file.read()
split = (file.split('\n'))
max_iters = 25
num_embed = 64
num_heads = 16
num_layers = 8
batch_size = 32
block_size = 128
dropout = 0.2
learning_rate = 1e-3

if torch.cuda.is_available():
    device = 'cuda'
else:
    device = 'cpu'

encode = tiktoken.get_encoding('gpt2')

characters = character_amount(encode=encode, split=split)
vocab_size = encode.n_vocab
    
encoded = encode_chars(split=split, encode=encode)

data = torch.tensor(encoded, dtype=torch.long)
train_data, test_data = train_test_split(data=data)

def array_creation(split):
    if split == 'train':
        data = train_data
    else:
        data = test_data

    ix = torch.randint(len(data) - block_size, (batch_size,))
    x = torch.stack([data[i:i + block_size] for i in ix])
    y = torch.stack([data[i+1:i+block_size + 1] for i in ix])
    x = x.to(device)
    y = y.to(device)
    return x, y
        
class Head(nn.Module):
    def __init__(self, head_size):
        super().__init__()
        self.key = nn.Linear(num_embed, head_size, bias=False)
        self.query = nn.Linear(num_embed, head_size, bias=False)
        self.value = nn.Linear(num_embed, head_size, bias=False)
        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))

        self.dropout = nn.Dropout(dropout)

    def forward(self,x):
        B, T, C = x.shape
        head_size = 16
        key = nn.Linear(C, head_size, bias=False)
        query = nn.Linear(C, head_size, bias=False)
        k = key(x)
        q = query(x)
        weight =  q @ k.transpose(-2,-1) * C **-0.5
        weight = weight.masked_fill(self.tril[:T,:T] == 0, float('-inf'))
        weight = F.softmax(weight, dim=-1)
        weight = self.dropout(weight)

        v = self.value(x)
        out = weight @ v
        return out

class MultiHead(nn.Module):
    def __init__(self, num_heads, head_size):
        super().__init__()
        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])
        self.prj = nn.Linear(num_embed, num_embed)
        self.dropout = nn.Dropout(dropout)
    def forward(self, x):
        out = torch.cat([h(x) for h in self.heads], dim=-1)
        out = self.dropout(self.prj(out))
        return out

class FeedForward(nn.Module):
    def __init__(self, num_embed):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(num_embed,  4 * num_embed),
            nn.ReLU(),
            nn.Linear(4 * num_embed, num_embed),
            nn.Dropout(dropout)
        )
    def forward(self, x):
        return self.net(x)
    
class Block(nn.Module):
    def __init__(self, num_embed,num_heads):
        super().__init__()
        head_size = num_embed // num_heads
        self.sa = MultiHead(num_heads, head_size)
        self.ffwd = FeedForward(num_embed)
        self.layernorm1 = nn.LayerNorm(num_embed)
        self.layernorm2 = nn.LayerNorm(num_embed)

    def forward(self, x):
        x = x + self.sa(self.layernorm1(x))
        x = x + self.ffwd(self.layernorm2(x))
        return x

class BigramLanguageModel(nn.Module):

    def __init__(self):
        super().__init__()
        self.token_embedding_table = nn.Embedding(vocab_size, num_embed)
        self.position_embedding_table = nn.Embedding(block_size, num_embed)
        self.blocks = nn.Sequential(*[Block(num_embed, num_heads=num_heads) for _ in range(num_layers)])
        self.ln_f = nn.LayerNorm(num_embed)
        self.lm_head = nn.Linear(num_embed, vocab_size)

    def forward(self, idx, targets=None):
        B, T = idx.shape
        token_emb = self.token_embedding_table(idx)
        position_embedding = self.position_embedding_table(torch.arange(T, device=device))
        x = token_emb + position_embedding
        x = self.blocks(x)
        x = self.ln_f(x)
        logits = self.lm_head(x)
        
        if targets != None:
            B, T, C = logits.shape
            logits = logits.view(B*T, C)
            targets = targets.view(B*T)
            
            loss = F.cross_entropy(logits, targets)
        else:
            loss = None
        return logits, loss
    def generate(self, idx, max_new_tokens):
        for _ in range(max_new_tokens):
            idx_cond = idx[:, -block_size:]
            logits, loss = self(idx_cond)
            logits = logits[:, -1, :]
            
            probs = F.softmax(logits, dim=1)
            idx_next = torch.multinomial(probs, num_samples=1)
            idx = torch.cat((idx, idx_next), dim=1)
        return idx

m = BigramLanguageModel()
model = m.to(device)

generated_list = model.generate(idx= torch.zeros((1,1), dtype=torch.long), max_new_tokens=100)[0].tolist()
decoded_list = decoding(generated_list=generated_list, encode=encode)
    
optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)

iteration = 0
for _ in range(max_iters):
    xy, yb = array_creation('train')
    
    logits, loss = model(xy, yb)
    optimizer.zero_grad(set_to_none=True)
    loss.backward()
    optimizer.step()
    iteration += 1

    print(iteration)
    print(loss.item())

context =  torch.zeros((1,1), dtype=torch.long, device=device)
print(decoding(generated_list=model.generate(context,max_new_tokens=100)[0].tolist(), encode=encode))

Here is the output
A', '! re al, we hear me speak.All:Speak.First Citizen:You are all resolved rather to die to than famish?A', '! re al, we hear me speak.All:Speak.First Citizen:You are all resolved rather to die to than famish?A', '! re al, we hear me speak.All:Speak.First Citizen:You are all resolved rather to die to than famish?A', '! re al, we hear me speak.All:Speak.First Citizen:You are all resolved rather to die to than famish?

It keeps repeating itself even farther than that.
I tried to increase the amount of data going in but that didn't help, I also tried to change the amount of iterations and batch size/block size. But it still didnt change the repetition.
Do I just need to do even more intense training?
","It was a problem with my decoding function, not sure what but I made my own tokenizer rather than using tiktoken and it fixed the problem.
",pytorch
torch installation results in not supported wheel on this platform,"Tried running pip3 install torch===1.4.0 torchvision===0.5.0 -f https://download.pytorch.org/whl/torch_stable.html first, taken from PyTorch website
which resulted in
No matching distribution found for torch===1.4.0
 and  Could not find a version that satisfies the requirement torch===1.4.0 (from versions: 0.1.2, 0.1.2.post1, 0.1.2.post2) 
Finally downloaded the .whl file from downloads page and tried installing locally like so 'C:\Users\Raf\AppData\Local\Programs\Python\Python38\Scripts\pip.exe' install torch-1.4.0+cpu-cp37-cp37m-win_amd64.whl
after which I get torch-1.4.0+cpu-cp37-cp37m-win_amd64.whl is not a supported wheel on this platform.
using 64 Python 3.8, on 64 bit Windows
","
using 64 Python 3.8

but you downloaded the cp37 whl which is for python 3.7. There is currently no whl file available for python 3.8. So either install from source (probably not recommended), install a different python version or create a virtual environment with python 3.7
Update
There is now: https://download.pytorch.org/whl/cpu/torch-1.4.0%2Bcpu-cp38-cp38-win_amd64.whl
",pytorch
how to write a raster plot,"I'm using pytorch on google colab.
I've got a tensor matrix below, this is the example, and actually the matrix size is about 50 neurons and 30,000~50,000 time.
a= torch.tensor([[0., 0., 0., 0., 0.],
                 [0., 0., 0., 0., 1.],
                 [0., 1., 0., 1., 0.]])

each of values are,
a= torch.tensor([[Neuron1(t=1), N2(t=1), N3(t=1), N4(t=1), N5(t=1)],
                 [N1(t=2), N2(t=2), N3(t=2), N4(t=2), N5(t=2)],
                 [N1(t=3), N2(t=3), N3(t=3), N4(t=3), N5(t=3)]])

and 1 means that neuron fire, 0 means not fire.
So Neuron5(t=2), Neuron2(t=3) and Neuron4(t=3) are firing.
Then, I want to make a raster plot or scatter plot like below using this matrix,
The dots show the firing neuron.

neuron number
1|
2|          *
3|
4|          *
5|__ *_____time
    1  2  3
What would be the best python code to do this?
I have no idea now.
Thank you for reading.
","You can do it easily as follows:
import matplotlib.pyplot as plt
a= torch.tensor([[0., 0., 0., 0., 0.],
                 [0., 0., 0., 0., 1.],
                 [0., 1., 0., 1., 0.]],device='cuda')
plt.scatter(*torch.where(a.cpu()))

",pytorch
understanding unusual yolo label formats and their impact on training,"I’m working on a dataset of stationary objects, where the data is divided into train, test, and validation folders with corresponding images and labels. The labels are in text files with the following format:
2 0.3832013609375 0 0 0.19411217812499998 0 0.614612228125 0.1995640296875 1 0.619265075 1 1 0.8055533171875 1 0.386728209375 0.798922646875 0 0.3832013609375 0

I’m confused because I expected each bounding box to have just 5 numbers:
class_id, x_center, y_center, width, height.

But here, I see significantly more numbers. Could it be that this format represents something else? Are there additional possibilities for YOLO label formats that I’m unaware of?
Additional Context
The data was sourced from this website, but I couldn’t find clear documentation about this label format.
Here’s the part I don’t understand: when I pass this dataset to YOLO for training using the following code, the training process works without any issues:
def train_yolo(weight_name):
    weight_path = os.path.join(weights_folder, weight_name)

    model = YOLO(weight_path)

    # Train model and save new weights
    results = model.train(data=data_yaml, epochs=100, imgsz=640, batch=16, name=f""yolo_{weight_name.split('.')[0]}"", save=True)

    return results

My data.yaml file contains:
train: ../train/images
val: ../valid/images
test: ../test/images

nc: 4
names: ['pencil', 'rubber', 'ruler', 'sharpner']

roboflow:
  workspace: waqas-hussain
  project: stationary-object-detector
  version: 8
  license: CC BY 4.0
  url: https://universe.roboflow.com/waqas-hussain/stationary-object-detector/dataset/8

There’s no direct reference to bounding box formats in this YAML file, yet YOLO processes the data correctly during training.
Questions:

How does YOLO handle these unusual label formats?
Could it be that
my training was incorrect due to this strange bounding box format?
Is there a way to confirm what this format represents and how it’s
parsed by YOLO?

Any insights or pointers would be greatly appreciated!
","From the picture on the website, I see that some of the annotations are not bounding boxes. They are polygons. A common way to encode a polygon is as a list of x/y pairs.
So I would guess that the format is
class_id x1 y1 x2 y2 x3 y3

etc.
To check this, I downloaded one of the pictures and its associated label. (Specifically, CamScanner-10-15-2023-14-29_86_jpg.rf.1042acb34a88542b82bbefa27b86569e.jpg I wrote a program which parsed this label and plotted it.
Code:
import numpy as np
import matplotlib.pyplot as plt


label_text = """"""1 0.3855721390625 0.17391304375 0.26533996718749997 0.1273291921875 0.10779436093749999 0.273291925 0.25290215625 0.3354037265625 0.3855721390625 0.17391304375
0 0.9618573796875 0.381987578125 0.8872305140625001 0.3540372671875 0.327529021875 0.9782608703125 0.45190713125000004 1 0.9618573796875 0.381987578125
2 0.970149253125 0.034161490625 0.8084577109375 0 0.0165837484375 0.9254658390625 0.0414593703125 0.9937888203125 0.178275290625 1 0.970149253125 0.034161490625""""""


lines = label_text.split('\n')
for line in lines:
    line = line.split(' ')
    class_id = line[0]
    label_without_id = np.array([float(s) for s in line[1:]])
    label_x = label_without_id[::2]
    label_y = label_without_id[1::2]
    plt.plot(label_x, label_y, label=class_id)
    # The convention when working with image coordinates is that Y-axis gets bigger as you move down the image
    plt.gca().invert_yaxis()
plt.legend()
plt.show()

Output:

That looks reasonably plausible, given the input. The aspect ratio is wrong, but they're likely expecting you to rescale the x/y coordinates by the image width/height. You can also compare this to the image labels on roboflow.
",pytorch
pytorch 040 there are three ways to create tensors on cuda device is there some difference between them,"I failed in the third way. t3 is still on CPU. No idea why.
a = np.random.randn(1, 1, 2, 3)

t1 = torch.tensor(a)
t1 = t3.to(torch.device('cuda'))

t2 = torch.tensor(a)
t2 = t2.cuda() 

t3 = torch.tensor(a, device=torch.device('cuda'))

","All three methods worked for me. 
In 1 and 2, you create a tensor on CPU and then move it to GPU when you use .to(device) or .cuda(). They are the same here.
However, when you use .to(device) method you can explicitly tell torch to move to specific GPU by setting device=torch.device(""cuda:<id>""). with .cuda() you have to do .cuda(<id>) to move to some particular GPU. 

Why do these two methods exist then?
.to(device) was introduced in 0.4 because it is easier to declare device variable at top of the code as 
device = torch.device(""cuda"") if torch.cuda.is_available() else torch.device(""cpu"")
and use .to(device) everywhere. This makes it quite easy to switch from CPU to GPU and vice-versa
Before this, we had to use .cuda() and your code will have if check for cuda.is_available() everywhere which made it cumbersome to switch between GPU/CPU.

The third method doesn't create a tensor on the CPU and directly copies data to GPU, which is more efficient. 
",pytorch
libtorch_cpuso undefined symbol ijit_isprofilingactive,"I have created this Conda environment:
conda env create -f environment.yml

The environment.yml file:
name: deep3d_pytorch
channels:
  - pytorch
  - conda-forge
  - defaults
dependencies:
  - python=3.6
  - pytorch=1.6.0
  - torchvision=0.7.0
  - numpy=1.18.1
  - scikit-image=0.16.2
  - scipy=1.4.1
  - pillow=6.2.1
  - pip
  - ipython=7.13.0
  - yaml=0.1.7
  - pip:
    - matplotlib==2.2.5
    - opencv-python==3.4.9.33
    - tensorboard==1.15.0
    - tensorflow==1.15.0
    - kornia==0.5.5
    - dominate==2.6.0
    - trimesh==3.9.20

I activate the Conda environment. But even a simple statement like python -c ""import torch; print(torch.__version__)"" to get the PyTorch version throws the undefined symbol error:
(deep3d_pytorch) m3@i7:~/repos/Deep3DFaceRecon_pytorch> python -c ""import torch; print(torch.__version__)""
Traceback (most recent call last):
  File ""<string>"", line 1, in <module>
  File ""/home/m3/anaconda3/envs/deep3d_pytorch/lib/python3.6/site-packages/torch/__init__.py"", line 189, in <module>
    from torch._C import *
ImportError: /home/m3/anaconda3/envs/deep3d_pytorch/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so: undefined symbol: iJIT_IsProfilingActive

I believe the PyTorch installed by Conda is broken. But the Conda logs are all fine. Does anyone have a clue or hint? I'm receiving the undefined symbol error on both my local machine and on Google Colab.
Update: minimal env
Even a minimal Environment like below, would throw similar errors:
conda create -n minimal_pytorch python=3.6 pytorch torchvision torchaudio -c pytorch
source activate minimal_pytorch && python -c ""import torch; print(torch.__version__)""

A similar undefined symbol error is thrown:
Traceback (most recent call last):
  File ""<string>"", line 1, in <module>
  File ""/usr/local/envs/minimal_pytorch/lib/python3.6/site-packages/torch/__init__.py"", line 197, in <module>
    from torch._C import *  # noqa: F403
ImportError: /usr/local/envs/minimal_pytorch/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so: undefined symbol: iJIT_NotifyEvent

Python version
When Python version is omitted while creating the environment:
conda create -n minimal_pytorch python pytorch torchvision torchaudio -c pytorch

The error is resolved:
source activate minimal_pytorch && python -c ""import torch; print(torch.__version__)""

PyTorch version is received without any error:
2.2.2

","To show how it was solved, I post all our code here. We were trying to test this repository:
https://github.com/sicxu/Deep3DFaceRecon_pytorch
Our final approach on Google Colab that works just fine is:
# Step 1: Press runtime on the top > Change runtime type > Select T4 GPU

# Step 2: Run this shell

!wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh -O miniconda.sh
!bash miniconda.sh -b -u -p /usr/local
!rm miniconda.sh
!conda update -y conda

%cd /content/
!rm -rf Deep3DFaceRecon_pytorch
!git clone https://github.com/sicxu/Deep3DFaceRecon_pytorch.git
%cd Deep3DFaceRecon_pytorch
!git clone https://github.com/NVlabs/nvdiffrast
!git clone https://github.com/deepinsight/insightface.git
!cp -r ./insightface/recognition/arcface_torch ./models/

# Step 3: Mount drive and copy the files mounted in google drive:

!mkdir /content/Deep3DFaceRecon_pytorch/checkpoints
!cp -r /content/drive/MyDrive/Deep3D/facerecon_20230425 /content/Deep3DFaceRecon_pytorch/checkpoints/facerecon_20230425
!cp /content/drive/MyDrive/Deep3D/01_MorphableModel.mat /content/Deep3DFaceRecon_pytorch/BFM/
!cp /content/drive/MyDrive/Deep3D/Exp_Pca.bin /content/Deep3DFaceRecon_pytorch/BFM/

# Step 4: Install the graphics related stuff

!sudo apt-get install libegl1-mesa-dev
!nvidia-smi
!sudo apt-get install libnvidia-gl-535

# Step 5: Run this shell to install everything

%%shell
eval ""$(conda shell.bash hook)""
conda create --name deep3d_pytorch python=3.6 -y
conda activate deep3d_pytorch
conda config --env --add channels pytorch
conda config --env --add channels conda-forge
conda config --env --add channels defaults
conda install pytorch==1.6.0 torchvision==0.7.0 cudatoolkit=10.2 -c pytorch -y
conda install numpy scikit-image=0.16.2 scipy=1.4.1 pillow=6.2.1 pip ipython=7.13.0 yaml=0.1.7 -y
pip install matplotlib==2.2.5 opencv-python==3.4.9.33 tensorboard==1.15.0 tensorflow==1.15.0 kornia==0.5.5 dominate==2.6.0 trimesh==3.9.20
pip install ./nvdiffrast/.

# Step 6: Run this shell to test the program

%%shell
eval ""$(conda shell.bash hook)""
conda activate deep3d_pytorch
python test.py --name=facerecon_20230425 --epoch=20 --img_folder=./datasets/examples

# Step 7: Copy test results to Google Drive

!ls /content/Deep3DFaceRecon_pytorch/checkpoints/facerecon_20230425/results/examples/epoch_20_000000/
!cp /content/Deep3DFaceRecon_pytorch/checkpoints/facerecon_20230425/results/examples/epoch_20_000000/* /content/drive/MyDrive/Deep3D/results/

# Step 8: prepare custom images along with their facial landmarks

!cp -r /content/drive/MyDrive/Deep3D/custom_images /content/Deep3DFaceRecon_pytorch/
%cd /content/Deep3DFaceRecon_pytorch/custom_images/
!python -m venv virtual_env
!source virtual_env/bin/activate && which pip
!source virtual_env/bin/activate && pip install mtcnn
!source virtual_env/bin/activate && pip install tensorflow
!source virtual_env/bin/activate && which python
!source virtual_env/bin/activate && python facial_landmarks.py # This Python script source code can be found here: https://github.com/sicxu/Deep3DFaceRecon_pytorch/issues/85#issuecomment-2069302718
%cd /content/Deep3DFaceRecon_pytorch/

# Step 9: run with custom images

%%shell
eval ""$(conda shell.bash hook)""
conda activate deep3d_pytorch
python test.py --name=facerecon_20230425 --epoch=20 --img_folder=./custom_images

# Step 10: copy the results back to Google Drive

!ls /content/Deep3DFaceRecon_pytorch/checkpoints/facerecon_20230425/results/custom_images/epoch_20_000000/
!cp /content/Deep3DFaceRecon_pytorch/checkpoints/facerecon_20230425/results/custom_images/epoch_20_000000/* /content/drive/MyDrive/Deep3D/results/


It's described here too: https://github.com/conda/conda/issues/13812#issuecomment-2071445372
",pytorch
pytorch tensor to numpy array,"I have a pytorch Tensor of shape [4, 3, 966, 1296]. I want to convert it to numpy array using the following code:
imgs = imgs.numpy()[:, ::-1, :, :]

How does that code work?
","There are 4 dimensions of the tensor you want to convert.
[:, ::-1, :, :] 

: means that the first dimension should be copied as it is and converted, same goes for the third and fourth dimension.
::-1 means that for the second axes it reverses the the axes
",pytorch
userwarning applied workaround for cudnn issue install nvrtcso,"I'm trainning a convolutional nueral network below.
def __init__(self, n_channels, n_classes):
    super().__init__()
    self.model = models.mobilenet_v3_large(pretrained=True)
    self.model.classifier[-1] = nn.Linear(1280, n_classes)
    self.model.features[0][0] = nn.Conv2d(
        n_channels,
        16,
        kernel_size=(3, 3),
        stride=(2, 2),
        padding=(1, 1),
        bias=False,
    )

there is only another thing in the warning that is

return F.conv2d(input, weight, bias, self.stride,

I don't know what is this warning and what can i do for it. if any information about the system is required, i can edit the question and add to it.
","This seems to be an issue with the CUDA library in pytorch 2.0.x+cu118. There're a couple of possible fixes.

Downgrade to the previous CUDA library, cu117. E.g. with pip:

pip3 uninstall pytorch
pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu117) 


To continue to use cu118, create a symlink in the torch package library.

cd venv/python3.10/site-packages/torch/lib  # (or the path to dist-packages/torch/lib, depending on how you installed pytorch)

ln -s libnvrtc-*.so.11.2 libnvrtc.so

Alternatively, you should be able to ignore the warning; it shouldn't have any side effects or impact.
I found the above solutions in this GitHub issue, along with more in-depth discussions into the issue.
",pytorch
how to make a dataloader with a directory of subfolders relevant to each class in pytorch,"I have a dataset that contains images of brain tumoursl. I want to make a CNN to classify these images.What I have seen is a directory of images which is separated in “train” , “test” folders.
However, in this case, the dataset directory structure is as follows.
dataset_dir
|_____tumor_type_1
|_____tumor_type_2
|_____tumor_type_3
|_____no_tumor

Now, I want to make 3 dataloaders. ( a train_dataloader,a validation_dataloader & a test_dataloader.)
Does anyone know how to do this in PyTorch without writing a custom script.
","You can use torchvision's ImageFolder class (docs here), but you should first split your data to train/test/val in separate directories beforehand in this format:
├── train
│   ├── class1
|      ├── image-1.jpg
│      ├── image-2.jpg
│   ├── class2
|      ├── image-1.jpg
│      ├── image-2.jpg
├── val
│   ├── class1
|      ├── image-1.jpg
│      ├── image-2.jpg
│   ├── class2
|      ├── image-1.jpg
│      ├── image-2.jpg
├── test
│   ├── ...
...

to split the images randomly:
import os
import shutil
import random

test_split = 0.2
valid_split = 0.2

if not os.path.exists('./new_dataset_dir'):
    os.mkdir('./new_dataset_dir')

os.mkdir('./new_dataset_dir/test')
os.mkdir('./new_dataset_dir/train')
os.mkdir('./new_dataset_dir/valid')

classes = os.listdir('./dataset_dir')

for c in classes:
    images = os.listdir('./dataset_dir/' + c)
    random.shuffle(images) # optional

    num_images = len(images)
    num_test = int(test_split * num_images)
    num_valid = int(valid_split * num_images)
    num_train = num_images - num_test - num_valid

    os.mkdir('./new_dataset_dir/test/' + c)
    os.mkdir('./new_dataset_dir/train/' + c)
    os.mkdir('./new_dataset_dir/valid/' + c)

    for idx, image in enumerate(images):
        split = 'train' if idx < num_train else 'valid' if idx < num_train + num_valid else 'test'
        shutil.move(f'./dataset_dir/{c}/{image}', f'./new_dataset_dir/{split}/{c}/{image}')
    
    os.rmdir('./dataset_dir/' + c)

Then you can easily create the dataloaders using ImageFolder:
from torchvision.datasets import ImageFolder
from torch.utils.data import DataLoader

train_dataset = ImageFolder(root='./new_dataset_dir/train')
val_dataset = ImageFolder(root='./new_dataset_dir/valid')
test_dataset = ImageFolder(root='./new_dataset_dir/test')

train_loader = DataLoader(train_dataset, ...)
valid_loader = DataLoader(val_dataset, ...)
test_loader = DataLoader(test_dataset, ...)

",pytorch
how to convert a tensor of booleans to ints in pytorch,"Suppose, we have a tensor 
t = torch.tensor([True, False, True, False])

How do we convert it to an integer tensor with values [1, 0, 1, 0]?
","The solution is just a single line of code.
To convert a tensor t with values [True, False, True, False] to an integer tensor, just do the following.
t = torch.tensor([True, False, True, False])
t_integer = t.long()
print(t_integer)
[1, 0, 1, 0]

",pytorch
training different stage of model with different loss,"I'm trying to train a two-stage model in an end-to-end way. However, I want to update the different stages of models with different losses. For example, suppose the end to end model is composed of two models:model1 and model2. The output is calculated through running
features = model1(inputs)
output = model2(features)

I want to update the parameters of model1 with loss1, while keeping the parameter of model2 unchanged. Next, I want to update the parameters of model2 with loss2, while keeping the parameter of model1 unchanged. My full implementation is something like:
import torch
import torch.nn as nn

# Define the first model
class Net(nn.Module):
    def __init__(self):
        super(Net, self).__init__()
        self.conv1 = nn.Linear(20, 10)
        self.conv2 = nn.Linear(10, 5)

    def forward(self, x):
        x = self.conv1(x)
        x = self.conv2(x)
        return x

# Define the second model
class Net1(nn.Module):
    def __init__(self):
        super(Net1, self).__init__()
        self.conv1 = nn.Linear(5, 1)

    def forward(self, x):
        x = self.conv1(x)
        return x

# Initialize models
model1 = Net()
model2 = Net1()

# Initialize separate optimizers for each model
optimizer = torch.optim.SGD(model1.parameters(), lr=0.1)
optimizer1 = torch.optim.SGD(model2.parameters(), lr=0.1)

optimizer.zero_grad() 
optimizer1.zero_grad()

criterion = nn.CrossEntropyLoss()

# Sample inputs and labels
inputs = torch.randn(2, 20)
labels = torch.randn(2,1)

features = model1(inputs)         
outputs_model = model2(features) 

loss1 = criterion(outputs_model[0], labels[0]) 
loss2 = criterion(outputs_model, labels) 
   
loss1.backward(retain_graph=True)  
optimizer.step()  
optimizer.zero_grad()       
optimizer1.zero_grad()  

 
loss2.backward()        

However, this will return
Traceback (most recent call last):
  File , line 55, in <module>
    loss2.backward()        
    ^^^^^^^^^^^^^^^^
  File ""/opt/homebrew/anaconda3/lib/python3.11/site-packages/torch/_tensor.py"", line 521, in backward
    torch.autograd.backward(
  File ""/opt/homebrew/anaconda3/lib/python3.11/site-packages/torch/autograd/__init__.py"", line 289, in backward
    _engine_run_backward(
  File ""/opt/homebrew/anaconda3/lib/python3.11/site-packages/torch/autograd/graph.py"", line 769, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
RuntimeError: one of the variables needed for gradient computation has been modified by an inplace operation: [torch.FloatTensor [10, 5]], which is output 0 of AsStridedBackward0, is at version 2; expected version 1 instead. Hint: enable anomaly detection to find the operation that failed to compute its gradient, with torch.autograd.set_detect_anomaly(True).

I kinda understand why this is happening, but is there a way to address this?
","I have a solution that requires an extra forward pass from model2. If someone can think of a solution that doesn't require this, feel free to chime in.
First, a bit about backprop. When you call backward on a tensor, pytorch computes gradients for all tensors in the computational graph. Since loss1 and loss2 are both output from model2, calling backward on either results in computing gradients for all parameters in both model1 and model2.
When these gradients are computed, pytorch uses current parameter values and activations stored from the forward pass. This means that the parameters used in the backward computation need to be unchanged from the forward pass.
When you call optimizer.step(), you update all parameters in the optimizer with an in-place update. This is why you get the error you see.
loss1.backward(retain_graph=True) # computes gradients
optimizer.step() # updates parameters in place
...
 
loss2.backward() # throws an error due to the in-place update from optimizer step

Because of this, the step calls must happen after all the backward calls. This leads to an issue of how to control gradients. The following does not work:
# trying to zero grads fails
loss1.backward() # backward first loss
optimizer1.zero_grad() # zero model2 grads
loss2.backward() # loss 2 grads computes for model1

# changing the loss order results in the same

I think there are two ways around this:

Cache grads outside pytorch parameters and manually re-add them to parameters before calling step (hacky, probably breaks a bunch of stuff)
Do an extra forward pass (inefficient from compute, but works within Pytorch's structure)

I opted for the second option
import torch
import torch.nn as nn

torch.manual_seed(42)

class Net1(nn.Module):
    def __init__(self):
        super(Net1, self).__init__()
        self.conv1 = nn.Linear(20, 10)
        self.conv2 = nn.Linear(10, 5)

    def forward(self, x):
        x = self.conv1(x)
        x = self.conv2(x)
        return x

# Define the second model
class Net2(nn.Module):
    def __init__(self):
        super(Net2, self).__init__()
        self.conv1 = nn.Linear(5, 1)

    def forward(self, x):
        x = self.conv1(x)
        return x

# Initialize models
model1 = Net1()
model2 = Net2()

# Initialize separate optimizers for each model
opt1 = torch.optim.SGD(model1.parameters(), lr=0.1)
opt2 = torch.optim.SGD(model2.parameters(), lr=0.1)

p1_1 = next(model1.parameters()).data.clone()
p2_1 = next(model2.parameters()).data.clone()

criterion = nn.MSELoss()

inputs = torch.randn(2, 20)
labels = torch.randn(2,1)

features = model1(inputs)
out1 = model2(features)
out2 = model2(features.detach()) # detach removes from the computational graph

opt1.zero_grad()
opt2.zero_grad()

# update only model1 with loss1
loss1 = criterion(out1[0], labels[0]) 
loss1.backward()
opt2.zero_grad()
opt1.step()

# check parameters after update
p1_2 = next(model1.parameters()).data.clone()
p2_2 = next(model2.parameters()).data.clone()
assert not (p1_1 == p1_2).any() # all parameters from model1 updated
assert (p2_1 == p2_2).all() # no parameters from model2 updated

# update only model2 with loss2
loss2 = criterion(out2, labels)
loss2.backward()
opt2.step()

# check parameters after update
p1_3 = next(model1.parameters()).data.clone()
p2_3 = next(model2.parameters()).data.clone()
assert (p1_2 == p1_3).all() # no parameters from model1 updated
assert not (p2_2 == p2_3).any() # all parameters from model2 updated

Note that I changed your loss from cross entropy to MSE. Your model produces an output of size (bs, 1) while cross entropy expects an output of shape (bs, num_classes). The output of shape (bs, 1) implies you are computing cross entropy of a single class, which will always return 0 loss
pred = torch.randn(64, 1)
labels = torch.randn(64, 1)
criterion = nn.CrossEntropyLoss()
criterion(pred, labels)
> tensor(-0.)

",pytorch
torchnnfunctional vs torchnn  pytorch,"While adding loss in Pytorch, I have the same function in torch.nn.Functional as well as in torch.nn. what is the difference ?
 torch.nn.CrossEntropyLoss() and torch.nn.functional.cross_entropy 
","Putting same text from PyTorch discussion forum @Alban D has given answer to similar question.
F.cross entropy vs torch.nn.Cross_Entropy_Loss

There isn’t much difference for losses.
The main difference between the nn.functional.xxx and the nn.Xxx is that one has a state and one does not.
This means that for a linear layer for example, if you use the functional version, you will need to handle the weights yourself (including passing them to the optimizer or moving them to the gpu) while the nn.Xxx version will do all of that for you with .parameters() or .to(device).
For loss functions, as no parameters are needed (in general), you won’t find much difference. Except for example, if you use cross entropy with some weighting between your classes, using the nn.CrossEntropyLoss() module, you will give your weights only once while creating the module and then use it. If you were using the functional version, you will need to pass the weights every single time you will use it.

",pytorch
why do we do batch matrixmatrix product,"I'm following Pytorch seq2seq tutorial and ittorch.bmm method is used like below:
attn_applied = torch.bmm(attn_weights.unsqueeze(0),
                         encoder_outputs.unsqueeze(0))

I understand why we need to multiply attention weight and encoder outputs.
What I don't quite understand is the reason why we need bmm method here.
torch.bmm document says 

Performs a batch matrix-matrix product of matrices stored in batch1 and batch2.
batch1 and batch2 must be 3-D tensors each containing the same number of matrices.
If batch1 is a (b×n×m) tensor, batch2 is a (b×m×p) tensor, out will be a (b×n×p) tensor.


","In the seq2seq model, the encoder encodes the input sequences given in as mini-batches. Say for example, the input is B x S x d where B is the batch size, S is the maximum sequence length and d is the word embedding dimension. Then the encoder's output is B x S x h where h is the hidden state size of the encoder (which is an RNN).
Now while decoding (during training) 
the input sequences are given one at a time, so the input is B x 1 x d and the decoder produces a tensor of shape B x 1 x h. Now to compute the context vector, we need to compare this decoder hidden state with the encoder's encoded states.
So, consider you have two tensors of shape T1 = B x S x h and T2 = B x 1 x h. So if you can do batch matrix multiplication as follows.
out = torch.bmm(T1, T2.transpose(1, 2))

Essentially you are multiplying a tensor of shape B x S x h with a tensor of shape B x h x 1 and it will result in B x S x 1 which is the attention weight for each batch.
Here, the attention weights B x S x 1 represent a similarity score between the decoder's current hidden state and encoder's all the hidden states. Now you can take the attention weights to multiply with the encoder's hidden state B x S x h by transposing first and it will result in a tensor of shape B x h x 1. And if you perform squeeze at dim=2, you will get a tensor of shape B x h which is your context vector.
This context vector (B x h) is usually concatenated to decoder's hidden state (B x 1 x h, squeeze dim=1) to predict the next token.
",pytorch
unable to update a latent vector using custom loss function in pytorch,"I am trying to implement this function but have had no luck. There is a VAE model that I am using, and along with it, there are encoder and decoder. I'm freezing the weights of the VAE decoder, and trying to change a latent vector which is updated using the function optimize_latent_vector(model, inp__, num_epochs=50, learning_rate=0.01). Now, there is some error regarding this piece of code: RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn

class VAE_GD_Loss(nn.Module):
    def __init__(self):
        super(VAE_GD_Loss, self).__init__()

    def forward(self, bad_seg, recons_mask, vector):
        # l2 normed squared and the soft dice loss are calculated
        loss = torch.sum(vector**2)+Soft_Dice_Loss(recons_mask, bad_seg)
        return loss

def optimize_latent_vector(model, inp__, num_epochs=50, learning_rate=0.01):
    inp__ = inp__.to(device).requires_grad_(True)
    # Encode and reparameterize to get initial latent vector
    with torch.no_grad():
        mu, log_var = model.encoder(inp__)
        z_latent_vect = model.reparameterize(mu, log_var)
    optimizer_lat = torch.optim.Adam([z_latent_vect], lr=learning_rate)
    dec_only = model.decoder
    
    for epoch in range(num_epochs):
        optimizer_lat.zero_grad()
        dec_only.eval()
        # Decode from latent vector
        recons_mask = dec_only(z_latent_vect)
        # Calculate loss
        VGLoss = VAE_GD_Loss()
        loss = VGLoss(inp__, recons_mask, z_latent_vect)
        # loss = Variable(loss, requires_grad=True)
        # Backpropagation
        loss.backward()
        optimizer_lat.step()
        print(f""Epoch {epoch}: Loss = {loss.item()}"")
    
    return z_latent_vect

If we uncomment the line loss = Variable(loss, requires_grad=True), then the code runs, but it doesn't minimize the loss whatsoever. I want to update the latent vector in such a way so that it follows the constraint set in the loss function. Any leads would help!
","I think your z_latent_vect is not enabled for gradient computation at all. It is initialised in a no_grad() block and is detached from the rest of the computation graph. Defining it as a torch.nn.Parameter should do the trick. At least I can see the loss decrease on a very simple VAE that I defined.
def optimize_latent_vector(model, inp__, num_epochs=50, learning_rate=0.01):
    inp__ = inp__.to(device)

    with torch.no_grad():
        mu, log_var = model.encoder(inp__)
        z_latent_vect = model.reparameterize(mu, log_var)

    z_latent_vect = torch.nn.Parameter(z_latent_vect.clone(), requires_grad=True)
    optimizer_lat = optim.Adam([z_latent_vect], lr=learning_rate)

    dec_only = model.decoder
    VGLoss = VAE_GD_Loss()

    for epoch in range(num_epochs):
        optimizer_lat.zero_grad()
        dec_only.eval()

        recons_mask = dec_only(z_latent_vect)
        loss = VGLoss(inp__, recons_mask, z_latent_vect)

        loss.backward()
        # print(loss.item()) you should see it reduce here
        optimizer_lat.step()

    return z_latent_vect

",pytorch
how is log_softmax implemented to compute its value and gradient with better speed and numerical stability,"Various frameworks and libraries (such as PyTorch and SciPy) provide special implementation for computing log(softmax()), which is faster and numerically more stable. However, I cannot find the actual Python implementation for this function, log_softmax(), in either of these packages.
Can anyone explain how this is implemented, or better, point me to the relevant source code?
","
The numerical error:

>>> x = np.array([1, -10, 1000])
>>> np.exp(x) / np.exp(x).sum()
RuntimeWarning: overflow encountered in exp
RuntimeWarning: invalid value encountered in true_divide
Out[4]: array([ 0.,  0., nan])

There are 2 methods to avoid the numerical error while compute the softmax:

Exp Normalization:


def exp_normalize(x):
    b = x.max()
    y = np.exp(x - b)
    return y / y.sum()

>>> exp_normalize(x)
array([0., 0., 1.])


Log Sum Exp


def log_softmax(x):
    c = x.max()
    logsumexp = np.log(np.exp(x - c).sum())
    return x - c - logsumexp


Please note that, a reasonable choice for both b, c in above formula is max(x). With this choice, overflow due to exp is impossible. The largest number exponentiated after shifting is 0.
",pytorch
torch matmul between 2d and 3d tensor,"Hi I have two tensors:
a = torch.randn(125, 128)    # Shape: (125, 128)
b = torch.randn(128, 8, 64)  # Shape: (128, 8, 64)

I want the result has a shape of (125, 8, 64)
My first observation is: last dimension of a match the first dimension of b then I do:
result = torch.matmul(a,b)

It gave me the error:
Expected size for first two dimensions of batch2 tensor to be: [128, 128] but got: [128, 8].
How can I do this.
Edit: I also dont' want to reshape into 2D and then reshape the result into 3D again.
","You can use an einsum
a = torch.randn(125, 128)    # Shape: (125, 128)
b = torch.randn(128, 8, 64)  # Shape: (128, 8, 64)
c = torch.einsum('ij,jkl->ikl', a, b)
print(c.shape)
> torch.Size([125, 8, 64])

",pytorch
pytorch lstm vs lstmcell,"What is the difference between LSTM and LSTMCell in Pytorch (currently version 1.1)? It seems that LSTMCell is a special case of LSTM (i.e. with only one layer, unidirectional, no dropout). 
Then, what's the purpose of having both implementations? Unless I'm missing something, it's trivial to use an LSTM object as an LSTMCell (or alternatively, it's pretty easy to use multiple LSTMCells to create the LSTM object)
","Yes, you can emulate one by another, the reason for having them separate is efficiency.
LSTMCell is a cell that takes arguments:

Input of shape batch × input dimension;
A tuple of LSTM hidden states of shape batch x hidden dimensions.

It is a straightforward implementation of the equations.
LSTM is a layer applying an LSTM cell (or multiple LSTM cells) in a ""for loop"", but the loop is heavily optimized using cuDNN. Its input is 

A three-dimensional tensor of inputs of shape batch × input length × input dimension;
Optionally, an initial state of the LSTM, i.e., a tuple of hidden states of shape batch × hidden dim (or tuple of such tuples if the LSTM is bidirectional)

You often might want to use the LSTM cell in a different context than apply it over a sequence, i.e. make an LSTM that operates over a tree-like structure. When you write a decoder in sequence-to-sequence models, you also call the cell in a loop and stop the loop when the end-of-sequence symbol is decoded.
",pytorch
how to manually specify checkpoint path in pytorchlightning,"Currently I am using TensorBoardLogger for all my needs and it's perfect, but i do not like how it handles checkpoint naming. I'd prefer to be able to specify the filename and the folder where to put the checkpoint manually, how should i do that?
","Yes, it is possible thanks to the ModelCheckPoint callback:
from pytorch_lightning.callbacks import ModelCheckpoint

checkpoint_callback = ModelCheckpoint(
    dirpath=""best_models"",
    filename='{epoch}-{val_loss:.2f}-{other_metric:.2f}'
)
trainer = Trainer(callbacks=[checkpoint_callback])

will create a checkpoint in the directory best_models/epoch=2-val_loss=0.02-other_metric=0.03.ckpt for example
",pytorch
get paths of saved checkpoints from pytorchlightning modelcheckpoint,"I am using PytorchLightning and a ModelCheckpoint which saves models with a formatted filename like filename=""model_{epoch}-{val_acc:.2f}""
I then want to load these checkpoints again, for simplicity I want the best from save_top_k=N.
As the filename is dynamic I wonder how can I retrieve the checkpoint files easily.
Is there a built-in attribute in the ModelCheckpoint or the trainer that gives me the saved checkpoints?
For example like
checkpoint_callback.get_top_k_paths()


I know I can do it with glob and model_dir, as I assume that callback keeps track of them anyway I wonder if there is a solution built in already and how to use it.
","-> All stored checkpoints can be found in ModelCheckpoint.best_k_models : Dict[str, Tensor] where the keys are the paths and the values the metric that is tracked.
Additionally does ModelCheckpoint have these attributes:
best_model_path
best_model_score, kth_best_model_path, kth_value, last_model_path and best_k_models.

Note: when loading a checkpoint
These values are only guaranteed when model_checkpoint.dirpath matches the one in in the checkpoints_state_dict[""dirpath""], i.e. you did not change the directory, otherwise only best_model_path is restored.
Otherwise as Aniket Maurya states you have to look at dirpath or the parallel files in best_model_path.
",pytorch
how to combine slice assignment mask assignment and broadcasting in pytorch,"To be more specific, I'm wondering how to assign a tensor by slice and by mask at different dimension(s) simultaneously in PyTorch.
Here's a small example about what I want to do:
With the tensors and masks below:
x = torch.zeros(2, 3, 4, 6)
mask = torch.tensor([[ True, True, False], [True, False, True]])
y = torch.rand(2, 3, 1, 3)

I want to achieve something like
x[mask, :, :3] = y[mask]


In dimension 0 and 1, only the 4x6/1x3 slices in x/y that whose corresponding element in mask is True are allowed to be assigned.
In dimension 2, I hope the 1-row tensor in y can be broadcast to all the 8 rows of x,
In dimension 3, only the first 3 elements in x are assigned with the 3-element tensor from y.

However, with code above, following error was caught:
RuntimeError: shape mismatch: value tensor of shape [4, 1, 3] cannot be broadcast to indexing result of shape [4, 3, 6]

It seems that PyTorch did [mask] indexing instead, and ignored the :3 indexing.
I've also tried
x[mask][:, :, :3] = y[mask]

No error occurred but the assignment still failed.
I know I can assign by slice and by mask step by step, but I hope to avoid any intermediate tensors if possible.
Tensors in neural networks may be extremely big, so may be an all-in-one assignment may take less time and less memory.
","You can do the following:
x = torch.zeros(2, 3, 4, 6)
mask = torch.tensor([[ True, True, False], [True, False, True]])
y = torch.rand(2, 3, 1, 3)
x[..., :3][mask] = y[mask]

This produces the same result as
i, j = mask.nonzero(as_tuple = True)
x[i, j, :, :3] = y[i, j]

For the 2D mask scenario. This method also works for additional dims:
x = torch.zeros(2, 3, 3, 4, 6)
y = torch.rand(2, 3, 3, 1, 3)
mask = torch.rand(2,3,3)>0.5
x[..., :3][mask] = y[mask]

For additional dims, the only constraint is that the first n=mask.ndim dims of x and y must match the shape of mask and the final dimension of y is 3 to match the :3.
",pytorch
creating a train test split for data nested in multiple folders,"I am preparing my data for training an image recognition model. I currently have one folder (the dataset) that contains multiple folders with the names of the labels and these folders have the images inside them.
I want to somehow split this dataset so that I have two main folders with the same subfolders, but the number of images inside these folders to be according to a preferred train/test split, so for instance 90% of the images in the train dataset and 10% in the test dataset.
I am struggling with finding the best way how to split my data. I have read a suggestion that pytorch torch.utils.Dataset class might be a way to do it but I can't seem to get it working as to preserve the folder hierarchy.
","If you have a folder structure like this:
folder
│     
│
└───class1
│   │   file011
│   │   file012
│   
└───class2
    │   file021
    │   file022

You can use simply the class torchvision.datasets.ImageFolder
As stated from the website of pytorch

A generic data loader where the images are arranged in this way:
root/dog/xxx.png
root/dog/xxy.png
root/dog/xxz.png

root/cat/123.png
root/cat/nsdf3.png
root/cat/asd932_.png


Then, after you have created your ImageFolder instance, like this for example
dataset = torchvision.datasets.Imagefolder(YOUR_PATH, ...)

you can split it in this way:
test_size = 0.1 * len(dataset)
test_set = torch.utils.data.Subset(dataset, range(test_size))  # take 10% for test
train_set = torch.utils.data.Subset(dataset, range(test_size, len(dataset)) # the last part for train

If you want to make a shuffle of the split, remember that the class subset uses the indexes for the split. So you can shuffle, and split them. Doing something like this
indexes = shuffle(range(len(dataset)))
indexes_train = indexes[:int(len(dataset)*0.9)]
indexes_test = = indexes[int(len(dataset)*0.9):]

",pytorch
load pytorch dataloader into gpu,"Is there a way to load a pytorch DataLoader (torch.utils.data.Dataloader) entirely into my GPU?
Now, I load every batch separately into my GPU.
CTX = torch.device('cuda')

train_loader = torch.utils.data.DataLoader(
    train_dataset,
    batch_size=BATCH_SIZE,
    shuffle=True,
    num_workers=0,
)

net = Net().to(CTX)
criterion = nn.CrossEntropyLoss()
optimizer = optim.SGD(net.parameters(), lr=LEARNING_RATE)

for epoch in range(EPOCHS):
    for inputs, labels in test_loader:
        inputs = inputs.to(CTX)        # this is where the data is loaded into GPU
        labels = labels.to(CTX)        

        optimizer.zero_grad()

        outputs = net(inputs)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()

    print(f'training accuracy: {net.validate(train_loader, device=CTX)}/{len(train_dataset)}')
    print(f'validation accuracy: {net.validate(test_loader, device=CTX)}/{len(test_dataset)}')

where the Net.validate() function is given by
def validate(self, val_loader, device=torch.device('cpu')):
    correct = 0
    for inputs, labels in val_loader:
        inputs = inputs.to(device)
        labels = labels.to(device)
        outputs = torch.argmax(self(inputs), dim=1)
        correct += int(torch.sum(outputs==labels))
    return correct


I would like to improve the speed by loading the entire dataset trainloader into my GPU, instead of loading every batch separately. So, I would like to do something like
train_loader.to(CTX)

Is there an equivalent function for this? Because torch.utils.data.DataLoader does not have this attribute .to().
I work with an NVIDIA GeForce RTX 2060 with CUDA Toolkit 10.2 installed.
","you can put your data of dataset in advance
train_dataset.train_data.to(CTX)  #train_dataset.train_data is a Tensor(input data)
train_dataset.train_labels.to(CTX)

for example of minst
import torch
from torch.utils.data import DataLoader
from torchvision import datasets
from torchvision import transforms
batch_size = 64
transform = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize((0.1307,), (0.3081,))
])
train_data = datasets.MNIST(
    root='./dataset/minst/',
    train=True,
    download=False,
    transform=transform
)
train_loader = DataLoader(
    dataset=train_data,
    shuffle=True,
    batch_size=batch_size
)
train_data.train_data = train_data.train_data.to(torch.device(""cuda:0""))  # put data into GPU entirely
train_data.train_labels = train_data.train_labels.to(torch.device(""cuda:0""))
# edit note for newer versions: use train_data.data and train_data.targets instead

I got this solution by using debugger...
",pytorch
how exactly the forward and backward hooks work in pytorch,"I am trying to understand how exactly code-wise the hooks operate in PyTorch. I have a model and I would like to set a forward and backward hook in my code. I would like to set a hook in my model after a specific layer and I guess the easiest way is to set a hook to this specific module. This introductory video warns that the backward module contains a bug, but I am not sure if that is still the case.
My code looks as follows:
def __init__(self, model, attention_layer_name='desired_name_module',discard_ratio=0.9):
  self.model = model
  self.discard_ratio = discard_ratio
  for name, module in self.model.named_modules():
    if attention_layer_name in name:
        module.register_forward_hook(self.get_attention)
        module.register_backward_hook(self.get_attention_gradient)

  self.attentions = []
  self.attention_gradients = []

def get_attention(self, module, input, output):
  self.attentions.append(output.cpu())

def get_attention_gradient(self, module, grad_input, grad_output):
  self.attention_gradients.append(grad_input[0].cpu())

def __call__(self, input_tensor, category_index):
  self.model.zero_grad()
  output = self.model(input_tensor)
  loss = ...
  loss.backward()

I am puzzled to understand how code-wise the following lines work:
module.register_forward_hook(self.get_attention)
module.register_backward_hook(self.get_attention_gradient)

I am registering a hook to my desired module, however, then, I am calling a function in each case without any input. My question is Python-wise, how does this call work exactly? How the arguments of the register_forward_hook and register_backward_hook operate when the function it's called?
","How does a hook work?
A hook allows you to execute a specific function - referred to as a ""callback"" - when a particular action has been performed. In this case, you are expecting self.get_attention to be called once the forward function of module has been accessed. To give a minimal example of how a hook would look like. I define a simple class on which you can register new callbacks through register_hook, then when the instance is called (via __call__), all hooks will be called with the provided arguments:
class Obj:
    def __init__(self):
        self.hooks = []
    
    def register_hook(self, hook):
        self.hooks.append(hook)

    def __call__(self, x, y):
        print('instance called')
        for hook in self.hooks:
            hook(x, y)

First, implement two hooks for demonstration purposes:
def foo(x, y):
    print(f'foo called with {x} and {y}')
def bar(x, _):
    print(f'bar called with {x}')

And initialize an instance of Obj:
obj = Obj()

You can register a hook and call the instance:
>>> obj.register_hook(foo)
>>> obj('yes', 'no')
instance called
foo called with yes and no

You can add hooks on top and call again to compare, here both hooks are triggered:
>>> obj.register_hook(bar)
>>> obj('yes', 'no')
instance called
foo called with yes and no
bar called with yes


Using hooks in PyTorch
There are two primary hooks in PyTorch: forward and backward. You also have pre- and post-hooks. Additionally there exists hooks on other actions such as load_state_dict...

To attach a hook on the forward process of a nn.Module, you should use register_forward_hook, the argument is a callback function that expects module, args, and output. This callback will be triggered on every forward execution.

For backward hooks, you should use register_full_backward_hook, the registered hook expects three arguments: module, grad_input, and grad_output. As of recent PyTorch versions, register_backward_hook has been deprecated and should not be used.


One side effect here is that you are registering the hook with self.get_attention and self.get_attention_gradient. The function passed to the register handler is not unbound to the class instance! In other words, on execution, these will be called without the self argument like:
self.get_attention(module, input, output)
self.get_attention_gradient(module, grad_input, grad_output)

This will fail. A simple way to fix this is to wrap the hook with a lambda when you register it:
module.register_forward_hook(
    lambda *args, **kwargs: Routine.get_attention(self, *args, **kwargs))

All in all, your class could look like this:
class Routine:
    def __init__(self, model, attention_layer_name):
        self.model = model

        for name, module in self.model.named_modules():
            if attention_layer_name in name:
                module.register_forward_hook(
                    lambda *args, **kwargs: Routine.get_attention(self, *args, **kwargs))
                module.register_full_backward_hook(
                    lambda *args, **kwargs: Routine.get_attention_gradient(self, *args, **kwargs))

        self.attentions = []
        self.attention_gradients = []

    def get_attention(self, module, input, output):
        self.attentions.append(output.cpu())

    def get_attention_gradient(self, module, grad_input, grad_output):
        self.attention_gradients.append(grad_input[0].cpu())

    def __call__(self, input_tensor):
        self.model.zero_grad()
        output = self.model(input_tensor)
        loss = output.mean()
        loss.backward()

When initialized with a single linear layer model:
routine = Routine(nn.Sequential(nn.Linear(10,10)), attention_layer_name='0')

You can call the instance, this will first trigger the forward hook with (because of self.model(input_tensor), and then the backward hook (because of loss.backward()).
>>> routine(torch.rand(1,10, requires_grad=True))

Following your implementation, your forward hook is caching the output of the ""attention_layer_name"" layer in self.attentions.
>>> routine.attentions
[tensor([[-0.3137, -0.2265, -0.2197,  0.2211, -0.6700, 
          -0.5034, -0.1878, -1.1334,  0.2025,  0.8679]], grad_fn=<...>)]

Similarly for the self.attention_gradients:
>>> routine.attentions_gradients
[tensor([[-0.0501,  0.0393,  0.0353, -0.0257,  0.0083,  
           0.0426, -0.0004, -0.0095, -0.0759, -0.0213]])] 


It is important to note that the cached outputs and gradients will remain in self.attentions and self.attentions_gradients and get appended on every execution of Routine.__call__.
",pytorch
attributeerror 39_multiprocessingdataloaderiter39 object has no attribute 39next39,"I am trying to load the dataset using Torch Dataset and DataLoader, but I got the following error:
AttributeError: '_MultiProcessingDataLoaderIter' object has no attribute 'next'

the code I use is:
class WineDataset(Dataset):

    def __init__(self):
        # Initialize data, download, etc.
        # read with numpy or pandas
        xy = np.loadtxt('./data/wine.csv', delimiter=',', dtype=np.float32, skiprows=1)
        self.n_samples = xy.shape[0]

        # here the first column is the class label, the rest are the features
        self.x_data = torch.from_numpy(xy[:, 1:]) # size [n_samples, n_features]
        self.y_data = torch.from_numpy(xy[:, [0]]) # size [n_samples, 1]

    # support indexing such that dataset[i] can be used to get i-th sample
    def __getitem__(self, index):
        return self.x_data[index], self.y_data[index]

    # we can call len(dataset) to return the size
    def __len__(self):
        return self.n_samples

    dataset = WineDataset()
        
    train_loader = DataLoader(dataset=dataset,
                              batch_size=4,
                              shuffle=True,
                              num_workers=2)

I tried to make the num_workers=0, still have the same error.
Python version 3.8.9
PyTorch version 1.13.0

","I too faced the same issue, when i tried to call the next() method as follows
dataiter = iter(dataloader)
data = dataiter.next()

You need to use the following instead and it works perfectly:
dataiter = iter(dataloader)
data = next(dataiter)

Finally your code should look like follows:
class WineDataset(Dataset):

    def __init__(self):
        # Initialize data, download, etc.
        # read with numpy or pandas
        xy = np.loadtxt('./data/wine.csv', delimiter=',', dtype=np.float32, skiprows=1)
        self.n_samples = xy.shape[0]

        # here the first column is the class label, the rest are the features
        self.x_data = torch.from_numpy(xy[:, 1:]) # size [n_samples, n_features]
        self.y_data = torch.from_numpy(xy[:, [0]]) # size [n_samples, 1]

    # support indexing such that dataset[i] can be used to get i-th sample
    def __getitem__(self, index):
        return self.x_data[index], self.y_data[index]

    # we can call len(dataset) to return the size
    def __len__(self):
        return self.n_samples

dataset = WineDataset()      
dataloader = DataLoader(dataset=dataset,
                              batch_size=4,
                              shuffle=True,
                              num_workers=2)

dataiter = iter(dataloader)
data = next(dataiter)

",pytorch
amd rocm with pytorch on navi10 rx 5700  rx 5700 xt,"I am one of those miserable creatures who own a AMD GPU (RX 5700, Navi10). I want to use up-to-date PyTorch libraries to do some Deep Learning on my local machine and stop using cloud instances.
I saw all over the internet that AMD is promising Navi10 support in the next 2-4 months(posts that were written 1-2 years back) however, I do not think they released an ""official"" support.
I installed ROCm on local machine and it actually detects my GPU and everything seems nice, here is rocminfo output.

I installed the necessary PyTorch ROCm version but when I try to run a code, I get the following error.

hipErrorNoBinaryForGpu: Unable to find code object for all current devices!

I suppose this is because ROCm still does not have a support for gfx1010 or I am lost at this point.
I would be happy if someone can provide a way to make ROCm work (preferable without compiling whole package for gfx1010 again) or provide way to use an AMD GPU just like a CUDA user.
","Set the HSA_OVERRIDE_GFX_VERSION=10.3.0 environment variable.
For example, in the terminal, input:
$ HSA_OVERRIDE_GFX_VERSION=10.3.0 python launch.py

I used 5700xt to run stable-diffusion for months, it works.
",pytorch
attributeerror  39tuple39 has no attribute 39to39,"I am writing this Image Classifier and I have defined the loaders but getting this mistake and I have no clue about it.
I have defined the train loader, for a better explanation I tried this
for ina,lab in train_loader:
    print(type(ina))
    print(type(lab)) 

and I got
<class 'torch.Tensor'>
<class 'tuple'>

Now, For training of the model, I did
def train_model(model,optimizer,n_epochs,criterion):
    start_time = time.time()
    for epoch in range(1,n_epochs-1):
        epoch_time = time.time()
        epoch_loss = 0
        correct = 0
        total = 0
        print( ""Epoch {}/{}"".format(epoch,n_epochs))
        
        model.train()
        
        for inputs,labels in train_loader:
            inputs = inputs.to(device)
            labels  = labels.to(device)
            optimizer.zero_grad()
            output = model(inputs)
            loss = criterion(output,labels)
            loss.backward()
            optimizer.step()
            epoch_loss +=loss.item()
            _,pred =torch.max(output,1)
            correct += (pred.cpu()==label.cpu()).sum().item()
            total +=labels.shape[0]
            
        acc = correct/total
      

and I got the error:
Epoch 1/15
---------------------------------------------------------------------------
AttributeError                            Traceback (most recent call last)
<ipython-input-36-fea243b3636a> in <module>
----> 1 train_model(model=arch, optimizer=optim, n_epochs=15, criterion=criterion)

<ipython-input-34-b53149a4bac0> in train_model(model, optimizer, n_epochs, criterion)
     12         for inputs,labels in train_loader:
     13             inputs = inputs.to(device)
---> 14             labels  = labels.to(device)
     15             optimizer.zero_grad()
     16             output = model(inputs)

AttributeError: 'tuple' object has no attribute 'to'

If you want anything more, please tell me!
Thanks
Edit: The label looks like this.
This was an Image Classification between Bee and Wasp. It also contains insects and non insects
('wasp', 'wasp', 'insect', 'insect', 'wasp', 'insect', 'insect', 'wasp', 'wasp', 'bee', 'insect', 'insect', 'other', 'bee', 'other', 'wasp', 'other', 'wasp', 'bee', 'bee', 'wasp', 'wasp', 'wasp', 'wasp', 'bee', 'wasp', 'wasp', 'other', 'bee', 'wasp', 'bee', 'bee')
('wasp', 'wasp', 'insect', 'bee', 'other', 'wasp', 'insect', 'wasp', 'insect', 'insect', 'insect', 'wasp', 'wasp', 'insect', 'wasp', 'wasp', 'wasp', 'bee', 'wasp', 'wasp', 'insect', 'insect', 'wasp', 'wasp', 'bee', 'wasp', 'insect', 'bee', 'bee', 'insect', 'insect', 'other')
","It literally means that the the tuple class in Python doesn't have a method called to. Since you're trying to put your labels onto your device, just do labels = torch.tensor(labels).to(device).
If you don't want to do this, you can change the way the DataLoader works by making it return your labels as a PyTorch tensor rather than a tuple.
Edit
Since the labels seem to be strings, I would convert them to one-hot encoded vectors first:
>>> import torch
>>> labels_unique = set(labels)
>>> keys = {key: value for key, value in zip(labels_unique, range(len(labels_unique)))}
>>> labels_onehot = torch.zeros(size=(len(labels), len(keys)))
>>> for idx, label in enumerate(labels_onehot):
...     labels_onehot[idx][keys[label]] = 1
...
>>> labels_onehot = labels.to(device)

I'm shooting a bit in the dark here because I don't know the details exactly, but yeah strings won't work with tensors.
",pytorch
runtimeerror weight tensor should be defined either for all 1000 classes or no classes but got weight tensor of shape 5,"I'm trying to use VGG16 for ** 5 classes data set**.
I've already added 5 new layers to adjust the output for logit as 5.
model = models.vgg16(pretrained=True) #Downloads the vgg16 model which is pretrained on Imagenet dataset.
#Replace the Final layer of pretrained vgg16 with 5 new layers.
model.fc = nn.Sequential(nn.Linear(1000,512),
                         nn.ReLU(inplace=True),
                         nn.Linear(512,256),
                         nn.ReLU(inplace=True),
                         nn.Linear(256,128),
                         nn.ReLU(inplace=True),
                         nn.Linear(128,64),
                         nn.ReLU(inplace=True),
                         nn.Linear(64,5))

And my loss function is as follows:
loss_fn = nn.CrossEntropyLoss(weight=class_weights) #CrossEntropyLoss with class_weights.

where class_weights is defined as such:
from sklearn.utils import class_weight #For calculating weights for each class.
class_weights = class_weight.compute_class_weight(class_weight='balanced',classes=np.array([0,1,2,3,4]),y=train_df['level'].values)
class_weights = torch.tensor(class_weights,dtype=torch.float).to(device)
 
print(class_weights) #Prints the calculated weights for the classes.

output: tensor([0.2556, 4.6000, 1.5333, 9.2000, 9.2000], device='cuda:0')
After first epoch I get the error given below:

RuntimeError: weight tensor should be defined either for all 1000 classes or no classes but got weight tensor of shape: [5]

","I faced the same problem as you. I started by changing the size of my final classifier layer (I copied the code from here):
model = models.mobilenet_v2(pretrained=True)

last_item_index = len(model.classifier) - 1
old_fc = model.classifier.__getitem__(last_item_index )
new_fc = nn.Linear(in_features=old_fc.in_features, 
                   out_features= 129, bias=True)
model.classifier.__setitem__(last_item_index , new_fc)

After changing this, I printed the model architecture using the following code:
from torchsummary import summary
summary(model, (3, 224, 224))

And it's working (number of classes in my dataset is 129):
(classifier): Sequential(
  (0): Dropout(p=0.2, inplace=False)
  (1): Linear(in_features=1280, out_features=129, bias=True)
)

",pytorch
pytorch model object has no attribute 39predict39 bert,"I had train a BertClassifier model using pytorch. After creating my best.pt I would like to make in production my model and using it to predict and classifier starting from a sample, so I resume them from the checkpoint. Otherwise after put it in evaluation and freeze model, I use .predict to make in work on my sample but I'm encountering this Attribute Error. I had also inizialize it before calling the checkpoint. When I am wrong? Thank you for your help!
def save_ckp(state, is_best, checkpoint_path, best_model_path):
    """"""
    function created to save checkpoint, the latest one and the best one. 
    This creates flexibility: either you are interested in the state of the latest checkpoint or the best checkpoint.
    state: checkpoint we want to save
    is_best: is this the best checkpoint; min validation loss
    checkpoint_path: path to save checkpoint
    best_model_path: path to save best model
    """"""
    f_path = checkpoint_path
    # save checkpoint data to the path given, checkpoint_path
    torch.save(state, f_path)
    # if it is a best model, min validation loss
    if is_best:
        best_fpath = best_model_path
        # copy that checkpoint file to best path given, best_model_path
        shutil.copyfile(f_path, best_fpath)

def load_ckp(checkpoint_fpath, model, optimizer):
    """"""
    checkpoint_path: path to save checkpoint
    model: model that we want to load checkpoint parameters into       
    optimizer: optimizer we defined in previous training
    """"""
    # load check point
    checkpoint = torch.load(checkpoint_fpath)
    # initialize state_dict from checkpoint to model
    model.load_state_dict(checkpoint['state_dict'])
    # initialize optimizer from checkpoint to optimizer
    optimizer.load_state_dict(checkpoint['optimizer'])
    # initialize valid_loss_min from checkpoint to valid_loss_min
    valid_loss_min = checkpoint['valid_loss_min']
    # return model, optimizer, epoch value, min validation loss 
    return model, optimizer, checkpoint['epoch'], valid_loss_min.item()

#Create the BertClassfier class
class BertClassifier(nn.Module):
    """"""Bert Model for Classification Tasks.""""""
    def __init__(self, freeze_bert=True):
        """"""
         @param    bert: a BertModel object
         @param    classifier: a torch.nn.Module classifier
         @param    freeze_bert (bool): Set `False` to fine-tune the BERT model
        """"""
        super(BertClassifier, self).__init__()
        
        .......
        
    def forward(self, input_ids, attention_mask):
        ''' Feed input to BERT and the classifier to compute logits.
         @param    input_ids (torch.Tensor): an input tensor with shape (batch_size,
                       max_length)
         @param    attention_mask (torch.Tensor): a tensor that hold attention mask
                       information with shape (batch_size, max_length)
         @return   logits (torch.Tensor): an output tensor with shape (batch_size,
                       num_labels) '''
         # Feed input to BERT
        outputs = self.bert(input_ids=input_ids,
                             attention_mask=attention_mask)
         
         # Extract the last hidden state of the token `[CLS]` for classification task
        last_hidden_state_cls = outputs[0][:, 0, :]
 
         # Feed input to classifier to compute logits
        logits = self.classifier(last_hidden_state_cls)
 
        return logits

def initialize_model(epochs):
    """""" Initialize the Bert Classifier, the optimizer and the learning rate scheduler.""""""
    # Instantiate Bert Classifier
    bert_classifier = BertClassifier(freeze_bert=False)

    # Tell PyTorch to run the model on GPU
    bert_classifier = bert_classifier.to(device)

    # Create the optimizer
    optimizer = AdamW(bert_classifier.parameters(),
                      lr=lr,    # Default learning rate
                      eps=1e-8    # Default epsilon value
                      )

    # Total number of training steps
    total_steps = len(train_dataloader) * epochs

    # Set up the learning rate scheduler
    scheduler = get_linear_schedule_with_warmup(optimizer,
                                                num_warmup_steps=0, # Default value
                                                num_training_steps=total_steps)
    return bert_classifier, optimizer, scheduler
    

def train(model, train_dataloader, val_dataloader, valid_loss_min_input, checkpoint_path, best_model_path, start_epochs, epochs, evaluation=True):

    """"""Train the BertClassifier model.""""""
    # Start training loop
    logging.info(""--Start training...\n"")

    # Initialize tracker for minimum validation loss
    valid_loss_min = valid_loss_min_input 


    for epoch_i in range(start_epochs, epochs):
        # =======================================
        #               Training
        # =======================================
        # Print the header of the result table
        logging.info((f""{'Epoch':^7} | {'Batch':^7} | {'Train Loss':^12} | {'Val Loss':^10} | {'Val Acc':^9} | {'Elapsed':^9}""))

        # Measure the elapsed time of each epoch
        t0_epoch, t0_batch = time.time(), time.time()

        # Reset tracking variables at the beginning of each epoch
        total_loss, batch_loss, batch_counts = 0, 0, 0

        # Put the model into the training mode
        model.train()

        # For each batch of training data...
        for step, batch in enumerate(train_dataloader):
            batch_counts +=1
            # Load batch to GPU
            b_input_ids, b_attn_mask, b_labels = tuple(t.to(device) for t in batch)

            # Zero out any previously calculated gradients
            model.zero_grad()

            # Perform a forward pass. This will return logits.
            logits = model(b_input_ids, b_attn_mask)

            # Compute loss and accumulate the loss values
            loss = loss_fn(logits, b_labels)
            batch_loss += loss.item()
            total_loss += loss.item()

            # Perform a backward pass to calculate gradients
            loss.backward()

            # Clip the norm of the gradients to 1.0 to prevent ""exploding gradients""
            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)

            # Update parameters and the learning rate
            optimizer.step()
            scheduler.step()

            # Print the loss values and time elapsed for every 20 batches
            if (step % 500 == 0 and step != 0) or (step == len(train_dataloader) - 1):
                # Calculate time elapsed for 20 batches
                time_elapsed = time.time() - t0_batch

                # Print training results
                logging.info(f""{epoch_i + 1:^7} | {step:^7} | {batch_loss / batch_counts:^12.6f} | {'-':^10} | {'-':^9} | {time_elapsed:^9.2f}"")

                # Reset batch tracking variables
                batch_loss, batch_counts = 0, 0
                t0_batch = time.time()

        # Calculate the average loss over the entire training data
        avg_train_loss = total_loss / len(train_dataloader)

        logging.info(""-""*70)
        # =======================================
        #               Evaluation
        # =======================================
        if evaluation == True:
            # After the completion of each training epoch, measure the model's performance
            # on our validation set.
            val_loss, val_accuracy = evaluate(model, val_dataloader)

            # Print performance over the entire training data
            time_elapsed = time.time() - t0_epoch
            
            logging.info(f""{epoch_i + 1:^7} | {'-':^7} | {avg_train_loss:^12.6f} | {val_loss:^10.6f} | {val_accuracy:^10.6f} | {time_elapsed:^9.2f}"")

            logging.info(""-""*70)
        logging.info(""\n"")


         # create checkpoint variable and add important data
        checkpoint = {
            'epoch': epoch_i + 1,
            'valid_loss_min': val_loss,
            'state_dict': model.state_dict(),
            'optimizer': optimizer.state_dict(),
        }
        
        # save checkpoint
        save_ckp(checkpoint, False, checkpoint_path, best_model_path)
        
        ## TODO: save the model if validation loss has decreased
        if val_loss <= valid_loss_min:
            print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(valid_loss_min,val_loss))
            # save checkpoint as best model
            save_ckp(checkpoint, True, checkpoint_path, best_model_path)
            valid_loss_min = val_loss
    
    logging.info(""-----------------Training complete--------------------------"")

def evaluate(model, val_dataloader):
    """"""After the completion of each training epoch, measure the model's performance on our validation set.""""""
    # Put the model into the evaluation mode. The dropout layers are disabled during the test time.
    model.eval()

    # Tracking variables
    val_accuracy = []
    val_loss = []

    # For each batch in our validation set...
    for batch in val_dataloader:
        # Load batch to GPU
        b_input_ids, b_attn_mask, b_labels = tuple(t.to(device) for t in batch)

        # Compute logits
        with torch.no_grad():
            logits = model(b_input_ids, b_attn_mask)

        # Compute loss
        loss = loss_fn(logits, b_labels)
        val_loss.append(loss.item())

        # Get the predictions
        preds = torch.argmax(logits, dim=1).flatten()

        # Calculate the accuracy rate
        accuracy = (preds == b_labels).cpu().numpy().mean() * 100
        val_accuracy.append(accuracy)

    # Compute the average accuracy and loss over the validation set.
    val_loss = np.mean(val_loss)
    val_accuracy = np.mean(val_accuracy)

    return val_loss, val_accuracy

bert_classifier, optimizer, scheduler = initialize_model(epochs=n_epochs)
train(model = bert_classifier ......)



bert_classifier, optimizer, scheduler = initialize_model(epochs=n_epochs)
model, optimizer, start_epoch, valid_loss_min = load_ckp(r""./best_model/best_model.pt"", bert_classifier, optimizer)

model.eval()
model.freeze()

sample = {
  ""seq"": ""ABCDE"",}

predictions = model.predict(sample)

AttributeError: 'BertClassifier' object has no attribute 'predict'

","Generally, people wrote the prediction function for you.
If not, you need to handle the low level stuff.
After this line, you loaded the trained parameters.
model, optimizer, start_epoch, valid_loss_min = load_ckp(r""./best_model/best_model.pt"", bert_classifier, optimizer)

After that, you need to do the model.forward(intput_seq,this_attention_mask_maybe_null).
You can see the forward method here is the: def forward(self, input_ids, attention_mask) in the model.
",pytorch
what is the difference between various backends in torchnnattentionsdpbackend and what do they mean,"In the pytorch docs on SDPBackend there are a few enums available to be used with the context manager,
ERROR: An error occurred when trying to determine the backend.
MATH: The math backend for scaled dot product attention.
FLASH_ATTENTION: The flash attention backend for scaled dot product attention.
EFFICIENT_ATTENTION: The efficient attention backend for scaled dot product attention.
CUDNN_ATTENTION: The cuDNN backend for scaled dot product attention.
What do they mean and how are they different?
What exactly is the EFFICIENT ATTENTION backend? And another is I checked with  torch.backends.cuda.flash_sdp_enabled() on a machine without GPU and it is true but isn't flash attention only supposed to be for GPU's and it is based on using GPU cache memory? Is efficient attention just flash attention 2?
","MATH is the pytorch C++ attention implementation
FLASH_ATTENTION is the attention implementation from the flash attention paper
EFFICIENT_ATTENTION is the implementation from the facebook xformers library
CUDNN_ATTENTION is the implementation from the Nvidia CuDNN library
You can read more about the differences here
",pytorch
how to implement contractive autoencoder in pytorch,"I'm trying to create a contractive autoencoder in Pytorch. I found this thread and tried according to that. This is the snippet I wrote based on the mentioned thread:
import datetime
import numpy as np 
import torch
import torchvision
from torchvision import datasets, transforms
from torchvision.utils import save_image, make_grid
import torch.nn as nn 
import torch.nn.functional as F 
import torch.optim as optim
import matplotlib.pyplot as plt 
%matplotlib inline

dataset_train = datasets.MNIST(root='MNIST',
                               train=True,
                               transform = transforms.ToTensor(),
                               download=True)
dataset_test  = datasets.MNIST(root='MNIST', 
                               train=False, 
                               transform = transforms.ToTensor(),
                               download=True)
batch_size = 128
num_workers = 2
dataloader_train = torch.utils.data.DataLoader(dataset_train,
                                               batch_size = batch_size,
                                               shuffle=True,
                                               num_workers = num_workers, 
                                               pin_memory=True)

dataloader_test = torch.utils.data.DataLoader(dataset_test,
                                               batch_size = batch_size,
                                               num_workers = num_workers,
                                               pin_memory=True)

def view_images(imgs, labels, rows = 4, cols =11):
    imgs = imgs.detach().cpu().numpy().transpose(0,2,3,1)
    fig = plt.figure(figsize=(8,4))
    for i in range(imgs.shape[0]):
        ax = fig.add_subplot(rows, cols, i+1, xticks=[], yticks=[])
        ax.imshow(imgs[i].squeeze(), cmap='Greys_r')
        ax.set_title(labels[i].item())


# now let's view some 
imgs, labels = next(iter(dataloader_train))
view_images(imgs, labels,13,10)

class Contractive_AutoEncoder(nn.Module):
    def __init__(self):
        super().__init__()
        self.encoder = nn.Linear(784, 512)
        self.decoder = nn.Linear(512, 784)

    def forward(self, input):
        # flatten the input
        shape = input.shape
        input = input.view(input.size(0), -1)
        output_e = F.relu(self.encoder(input))
        output = F.sigmoid(self.decoder(output_e))
        output = output.view(*shape)
        return output_e, output

def loss_function(output_e, outputs, imgs, device):
    output_e.backward(torch.ones(output_e.size()).to(device), retain_graph=True)
    criterion = nn.MSELoss()
    assert outputs.shape == imgs.shape ,f'outputs.shape : {outputs.shape} != imgs.shape : {imgs.shape}'
    
    imgs.grad.requires_grad = True 
    loss1 = criterion(outputs, imgs)
    print(imgs.grad)
    loss2 = torch.mean(pow(imgs.grad,2))
    loss = loss1 + loss2 
    return loss 

epochs = 50 
interval = 2000
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model = Contractive_AutoEncoder().to(device)
optimizer = optim.Adam(model.parameters(), lr =0.001)

for e in range(epochs):
    for i, (imgs, labels) in enumerate(dataloader_train):
        imgs = imgs.to(device)
        labels = labels.to(device)

        outputs_e, outputs = model(imgs)
        loss = loss_function(outputs_e, outputs, imgs,device)
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        if i%interval: 
            print('')

    print(f'epoch/epoechs: {e}/{epochs} loss : {loss.item():.4f} ')

For the sake of brevity I just used one layer for the encoder and the decoder. It should work regardless of number of layers in either of them obviously!
But the catch here is, aside from the fact that I don't know if this is the correct way of doing this, (calculating gradients with respect to the input), I get an error which makes the former solution wrong/not applicable.
That is:
    imgs.grad.requires_grad = True
produces the error :

AttributeError : 'NoneType' object has no attribute 'requires_grad'

I also tried the second method suggested in that thread which is as follows:
class Contractive_Encoder(nn.Module):
    def __init__(self):
        super().__init__()
        self.encoder = nn.Linear(784, 512)
        
    def forward(self, input):
        # flatten the input
        input = input.view(input.size(0), -1)
        output_e = F.relu(self.encoder(input))
        return output_e

class Contractive_Decoder(nn.Module):
    def __init__(self):
        super().__init__()
        self.decoder = nn.Linear(512, 784)

    def forward(self, input):
        # flatten the input
        output = F.sigmoid(self.decoder(input))
        output = output.view(-1,1,28,28)
        return output


epochs = 50 
interval = 2000
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

model_enc = Contractive_Encoder().to(device)
model_dec = Contractive_Decoder().to(device)

optimizer = optim.Adam([{""params"":model_enc.parameters()},
                        {""params"":model_dec.parameters()}], lr =0.001)

optimizer_cond = optim.Adam(model_enc.parameters(), lr = 0.001)

criterion = nn.MSELoss()

for e in range(epochs):
    for i, (imgs, labels) in enumerate(dataloader_train):
        imgs = imgs.to(device)
        labels = labels.to(device)

        outputs_e = model_enc(imgs)
        outputs = model_dec(outputs_e)
        loss_rec = criterion(outputs, imgs)
        optimizer.zero_grad()
        loss_rec.backward()
        optimizer.step()

        imgs.requires_grad_(True)
        y = model_enc(imgs)
        optimizer_cond.zero_grad()
        y.backward(torch.ones(imgs.view(-1,28*28).size()))

        imgs.grad.requires_grad = True
        loss = torch.mean([pow(imgs.grad,2)])
        optimizer_cond.zero_grad()
        loss.backward()
        optimizer_cond.step()
        
        if i%interval: 
            print('')

    print(f'epoch/epoechs: {e}/{epochs} loss : {loss.item():.4f} ')

but I face the error :
RuntimeError: invalid gradient at index 0 - got [128, 784] but expected shape compatible with [128, 512]

How should I go about this in Pytorch?
","Summary 
The final implementation for contractive loss that I wrote is as follows: 
def loss_function(output_e, outputs, imgs, lamda = 1e-4, device=torch.device('cuda')):

    criterion = nn.MSELoss()
    assert outputs.shape == imgs.shape ,f'outputs.shape : {outputs.shape} != imgs.shape : {imgs.shape}'
    loss1 = criterion(outputs, imgs)

    output_e.backward(torch.ones(outputs_e.size()).to(device), retain_graph=True)    
    # Frobenious norm, the square root of sum of all elements (square value)
    # in a jacobian matrix 
    loss2 = torch.sqrt(torch.sum(torch.pow(imgs.grad,2)))
    imgs.grad.data.zero_()
    loss = loss1 + (lamda*loss2) 
    return loss 

and inside training loop you need to do:
for e in range(epochs):
    for i, (imgs, labels) in enumerate(dataloader_train):
        imgs = imgs.to(device)
        labels = labels.to(device)

        imgs.retain_grad()
        imgs.requires_grad_(True)

        outputs_e, outputs = model(imgs)
        loss = loss_function(outputs_e, outputs, imgs, lam,device)

        imgs.requires_grad_(False)

        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
    print(f'epoch/epochs: {e}/{epochs} loss: {loss.item():.4f}')

Full explanation
As it turns out and rightfully @akshayk07 pointed out in the comments, the implementation found in Pytorch forum was wrong in multiple places. The notable thing, being it wasn't implementing the actual contractive loss that was introduced in Contractive Auto-Encoders:Explicit Invariance During Feature Extraction paper! and also aside from that, the implementation wouldn't work at all for obvious reasons that will be explained in a moment. 
The changes are obvious so I try to explain what's going on here. First of all note that imgs is not a leaf node, so the gradients would not be retained in the image .grad attribute.
In order to retain gradients for non leaf nodes, you should use retain_graph(). grad is only populated for leaf Tensors. Also imgs.retain_grad() should be called before doing forward() as it will       instruct the autograd to store grads into non-leaf nodes.
Update
Thanks to @Michael for pointing out that the correct calculation of Frobenius Norm is actually (from ScienceDirect):  

the square root of the sum of the squares of all the matrix entries

and not 

the the square root of the sum of the absolute values of all the
  matrix entries as explained here

",pytorch
view size is not compatible with input tensor39s size and stride,"I'm trying to training F-RCNN based on coco dataset on my images. Image size is 512X512
I've tested dataloader separately and it works and prints the batch images and BB details
Also i've tried to print the loss in NN and it does print the batch_mean as well and after that ERROR occurs.
img_process = v2.Compose(
    [
        v2.ToTensor(),
        v2.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
    ]
)

class SCocoDetection(datasets.CocoDetection):
    def __init__(
        self,
        image_directory_path: str,
        annotation_path : str,
        train: bool = True,
        image_processor = None
    ):
        super().__init__(image_directory_path, annotation_path)
        self.image_processor = image_processor

    def __getitem__(self, idx):

        image, annotations = super().__getitem__(idx)
        images, targets = [], []
        image_id = self.ids[idx]


        for ann in annotations:
            bbox = ann['bbox']
            #small = (bbox[:, 2] * bbox[:, 3]) <= (image.size[1] * image.size[0] * 0.001)
            small = (bbox[2] * bbox[3]) <= (512 * 512 * 0.001)
            #print(small)
            if small:
                bbox = torch.tensor(bbox).unsqueeze(0).float()
                boxes = ops.box_convert(bbox, in_fmt='xywh', out_fmt='xyxy')
                boxes = boxes.float()
                if (boxes[0][0] < boxes[0][2]) and (boxes[0][1] < boxes[0][3]):
                    output_dict = self.image_processor({""image"": image, ""boxes"": boxes})
                    images.append(output_dict['image'])
                    targets.append({
                        'boxes': output_dict['boxes'],
                        'labels': torch.ones(len(boxes), dtype=int)
                    })
                else:
                    print(f""Invalid box : {boxes}"")
        #print(f""image_id : {image_id} , idx : {idx} , targets :{targets}"")
        return images, targets


TRAIN_DATASET = SCocoDetection(
    
    image_directory_path='047/v2_coco_train/images',
    annotation_path='047/v2_coco_train/result.json',
    image_processor=img_process,
    train=True)
VAL_DATASET = SCocoDetection(
    image_directory_path='047/v2_coco_test/images',
    annotation_path= '047/v2_coco_test/result.json',
    image_processor=img_process,
    train=False)


print(""Number of training examples:"", len(TRAIN_DATASET))
print(""Number of validation examples:"", len(VAL_DATASET))
#print(""Number of test examples:"", len(TEST_DATASET))


def collate_fn(batch):
    return tuple(zip(*batch))

TRAIN_DATALOADER = DataLoader(dataset=TRAIN_DATASET,collate_fn = collate_fn, batch_size=2, shuffle=True)
VAL_DATALOADER = DataLoader(dataset=VAL_DATASET,collate_fn = collate_fn, batch_size=4, shuffle=True)

import numpy as np

class CocoDNN(L.LightningModule):
    def __init__(self):
        super().__init__()
        self.model = models.detection.fasterrcnn_mobilenet_v3_large_fpn(weights=""DEFAULT"")

    def forward(self, images, targets=None):
        return self.model(images, targets)


    def training_step(self, batch, batch_idx):
        imgs, annot = batch
        print(f""Batch :{batch_idx}"")

        batch_losses = []
        for img_b, annot_b in zip(imgs, annot):
            print(len(img_b), len(annot_b))
            if len(img_b) == 0:
                continue
            loss_dict = self.model(img_b, annot_b)
            losses = sum(loss for loss in loss_dict.values())
            #print(losses)
            batch_losses.append(losses)
        
        batch_mean  = torch.mean(torch.stack(batch_losses))
        #print(batch_mean)
        self.log('train_loss', batch_mean)

    def configure_optimizers(self):
        return optim.SGD(self.parameters(), lr=0.001, momentum=0.9, weight_decay=0.0005)

dnn = CocoDNN()
trainer = L.Trainer(limit_train_batches=100, max_epochs=1)
trainer.fit(model=dnn, train_dataloaders=TRAIN_DATALOADER)


### Error messages and logs

{
    ""name"": ""RuntimeError"",
    ""message"": ""view size is not compatible with input tensor's size and stride (at least one dimension spans across two contiguous subspaces). Use .reshape(...) instead."",
    ""stack"": ""---------------------------------------------------------------------------
RuntimeError                              Traceback (most recent call last)
Cell In[192], line 3
      1 dnn = CocoDNN()
      2 trainer = L.Trainer(limit_train_batches=100, max_epochs=1)
----> 3 trainer.fit(model=dnn, train_dataloaders=TRAIN_DATALOADER)

File site-packages/lightning/pytorch/trainer/trainer.py:538, in Trainer.fit(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)
    536 self.state.status = TrainerStatus.RUNNING
    537 self.training = True
--> 538 call._call_and_handle_interrupt(
    539     self, self._fit_impl, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path
    540 )

File site-packages/lightning/pytorch/trainer/call.py:47, in _call_and_handle_interrupt(trainer, trainer_fn, *args, **kwargs)
     45     if trainer.strategy.launcher is not None:
     46         return trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)
---> 47     return trainer_fn(*args, **kwargs)
     49 except _TunerExitException:
     50     _call_teardown_hook(trainer)

File site-packages/lightning/pytorch/trainer/trainer.py:574, in Trainer._fit_impl(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)
    567 assert self.state.fn is not None
    568 ckpt_path = self._checkpoint_connector._select_ckpt_path(
    569     self.state.fn,
    570     ckpt_path,
    571     model_provided=True,
    572     model_connected=self.lightning_module is not None,
    573 )
--> 574 self._run(model, ckpt_path=ckpt_path)
    576 assert self.state.stopped
    577 self.training = False

File site-packages/lightning/pytorch/trainer/trainer.py:981, in Trainer._run(self, model, ckpt_path)
    976 self._signal_connector.register_signal_handlers()
    978 # ----------------------------
    979 # RUN THE TRAINER
    980 # ----------------------------
--> 981 results = self._run_stage()
    983 # ----------------------------
    984 # POST-Training CLEAN UP
    985 # ----------------------------
    986 log.debug(f\""{self.__class__.__name__}: trainer tearing down\"")

File site-packages/lightning/pytorch/trainer/trainer.py:1025, in Trainer._run_stage(self)
   1023         self._run_sanity_check()
   1024     with torch.autograd.set_detect_anomaly(self._detect_anomaly):
-> 1025         self.fit_loop.run()
   1026     return None
   1027 raise RuntimeError(f\""Unexpected state {self.state}\"")

File site-packages/lightning/pytorch/loops/fit_loop.py:205, in _FitLoop.run(self)
    203 try:
    204     self.on_advance_start()
--> 205     self.advance()
    206     self.on_advance_end()
    207     self._restarting = False

File site-packages/lightning/pytorch/loops/fit_loop.py:363, in _FitLoop.advance(self)
    361 with self.trainer.profiler.profile(\""run_training_epoch\""):
    362     assert self._data_fetcher is not None
--> 363     self.epoch_loop.run(self._data_fetcher)

File site-packages/lightning/pytorch/loops/training_epoch_loop.py:140, in _TrainingEpochLoop.run(self, data_fetcher)
    138 while not self.done:
    139     try:
--> 140         self.advance(data_fetcher)
    141         self.on_advance_end(data_fetcher)
    142         self._restarting = False

File site-packages/lightning/pytorch/loops/training_epoch_loop.py:250, in _TrainingEpochLoop.advance(self, data_fetcher)
    247 with trainer.profiler.profile(\""run_training_batch\""):
    248     if trainer.lightning_module.automatic_optimization:
    249         # in automatic optimization, there can only be one optimizer
--> 250         batch_output = self.automatic_optimization.run(trainer.optimizers[0], batch_idx, kwargs)
    251     else:
    252         batch_output = self.manual_optimization.run(kwargs)

File site-packages/lightning/pytorch/loops/optimization/automatic.py:190, in _AutomaticOptimization.run(self, optimizer, batch_idx, kwargs)
    183         closure()
    185 # ------------------------------
    186 # BACKWARD PASS
    187 # ------------------------------
    188 # gradient update with accumulated gradients
    189 else:
--> 190     self._optimizer_step(batch_idx, closure)
    192 result = closure.consume_result()
    193 if result.loss is None:

File site-packages/lightning/pytorch/loops/optimization/automatic.py:268, in _AutomaticOptimization._optimizer_step(self, batch_idx, train_step_and_backward_closure)
    265     self.optim_progress.optimizer.step.increment_ready()
    267 # model hook
--> 268 call._call_lightning_module_hook(
    269     trainer,
    270     \""optimizer_step\"",
    271     trainer.current_epoch,
    272     batch_idx,
    273     optimizer,
    274     train_step_and_backward_closure,
    275 )
    277 if not should_accumulate:
    278     self.optim_progress.optimizer.step.increment_completed()

File site-packages/lightning/pytorch/trainer/call.py:167, in _call_lightning_module_hook(trainer, hook_name, pl_module, *args, **kwargs)
    164 pl_module._current_fx_name = hook_name
    166 with trainer.profiler.profile(f\""[LightningModule]{pl_module.__class__.__name__}.{hook_name}\""):
--> 167     output = fn(*args, **kwargs)
    169 # restore current_fx when nested context
    170 pl_module._current_fx_name = prev_fx_name

File site-packages/lightning/pytorch/core/module.py:1306, in LightningModule.optimizer_step(self, epoch, batch_idx, optimizer, optimizer_closure)
   1275 def optimizer_step(
   1276     self,
   1277     epoch: int,
   (...)
   1280     optimizer_closure: Optional[Callable[[], Any]] = None,
   1281 ) -> None:
   1282     r\""\""\""Override this method to adjust the default way the :class:`~lightning.pytorch.trainer.trainer.Trainer` calls
   1283     the optimizer.
   1284 
   (...)
   1304 
   1305     \""\""\""
-> 1306     optimizer.step(closure=optimizer_closure)

File site-packages/lightning/pytorch/core/optimizer.py:153, in LightningOptimizer.step(self, closure, **kwargs)
    150     raise MisconfigurationException(\""When `optimizer.step(closure)` is called, the closure should be callable\"")
    152 assert self._strategy is not None
--> 153 step_output = self._strategy.optimizer_step(self._optimizer, closure, **kwargs)
    155 self._on_after_step()
    157 return step_output

File site-packages/lightning/pytorch/strategies/strategy.py:238, in Strategy.optimizer_step(self, optimizer, closure, model, **kwargs)
    236 # TODO(fabric): remove assertion once strategy's optimizer_step typing is fixed
    237 assert isinstance(model, pl.LightningModule)
--> 238 return self.precision_plugin.optimizer_step(optimizer, model=model, closure=closure, **kwargs)

File site-packages/lightning/pytorch/plugins/precision/precision.py:122, in Precision.optimizer_step(self, optimizer, model, closure, **kwargs)
    120 \""\""\""Hook to run the optimizer step.\""\""\""
    121 closure = partial(self._wrap_closure, model, optimizer, closure)
--> 122 return optimizer.step(closure=closure, **kwargs)

File site-packages/torch/optim/optimizer.py:487, in Optimizer.profile_hook_step.<locals>.wrapper(*args, **kwargs)
    482         else:
    483             raise RuntimeError(
    484                 f\""{func} must return None or a tuple of (new_args, new_kwargs), but got {result}.\""
    485             )
--> 487 out = func(*args, **kwargs)
    488 self._optimizer_step_code()
    490 # call optimizer step post hooks

File site-packages/torch/optim/optimizer.py:91, in _use_grad_for_differentiable.<locals>._use_grad(self, *args, **kwargs)
     89     torch.set_grad_enabled(self.defaults[\""differentiable\""])
     90     torch._dynamo.graph_break()
---> 91     ret = func(self, *args, **kwargs)
     92 finally:
     93     torch._dynamo.graph_break()

File site-packages/torch/optim/sgd.py:112, in SGD.step(self, closure)
    110 if closure is not None:
    111     with torch.enable_grad():
--> 112         loss = closure()
    114 for group in self.param_groups:
    115     params: List[Tensor] = []

File site-packages/lightning/pytorch/plugins/precision/precision.py:108, in Precision._wrap_closure(self, model, optimizer, closure)
     95 def _wrap_closure(
     96     self,
     97     model: \""pl.LightningModule\"",
     98     optimizer: Steppable,
     99     closure: Callable[[], Any],
    100 ) -> Any:
    101     \""\""\""This double-closure allows makes sure the ``closure`` is executed before the ``on_before_optimizer_step``
    102     hook is called.
    103 
   (...)
    106 
    107     \""\""\""
--> 108     closure_result = closure()
    109     self._after_closure(model, optimizer)
    110     return closure_result

File site-packages/lightning/pytorch/loops/optimization/automatic.py:144, in Closure.__call__(self, *args, **kwargs)
    142 @override
    143 def __call__(self, *args: Any, **kwargs: Any) -> Optional[Tensor]:
--> 144     self._result = self.closure(*args, **kwargs)
    145     return self._result.loss

File site-packages/torch/utils/_contextlib.py:116, in context_decorator.<locals>.decorate_context(*args, **kwargs)
    113 @functools.wraps(func)
    114 def decorate_context(*args, **kwargs):
    115     with ctx_factory():
--> 116         return func(*args, **kwargs)

File site-packages/lightning/pytorch/loops/optimization/automatic.py:138, in Closure.closure(self, *args, **kwargs)
    135     self._zero_grad_fn()
    137 if self._backward_fn is not None and step_output.closure_loss is not None:
--> 138     self._backward_fn(step_output.closure_loss)
    140 return step_output

File site-packages/lightning/pytorch/loops/optimization/automatic.py:239, in _AutomaticOptimization._make_backward_fn.<locals>.backward_fn(loss)
    238 def backward_fn(loss: Tensor) -> None:
--> 239     call._call_strategy_hook(self.trainer, \""backward\"", loss, optimizer)

File site-packages/lightning/pytorch/trainer/call.py:319, in _call_strategy_hook(trainer, hook_name, *args, **kwargs)
    316     return None
    318 with trainer.profiler.profile(f\""[Strategy]{trainer.strategy.__class__.__name__}.{hook_name}\""):
--> 319     output = fn(*args, **kwargs)
    321 # restore current_fx when nested context
    322 pl_module._current_fx_name = prev_fx_name

File site-packages/lightning/pytorch/strategies/strategy.py:212, in Strategy.backward(self, closure_loss, optimizer, *args, **kwargs)
    209 assert self.lightning_module is not None
    210 closure_loss = self.precision_plugin.pre_backward(closure_loss, self.lightning_module)
--> 212 self.precision_plugin.backward(closure_loss, self.lightning_module, optimizer, *args, **kwargs)
    214 closure_loss = self.precision_plugin.post_backward(closure_loss, self.lightning_module)
    215 self.post_backward(closure_loss)

File site-packages/lightning/pytorch/plugins/precision/precision.py:72, in Precision.backward(self, tensor, model, optimizer, *args, **kwargs)
     52 @override
     53 def backward(  # type: ignore[override]
     54     self,
   (...)
     59     **kwargs: Any,
     60 ) -> None:
     61     r\""\""\""Performs the actual backpropagation.
     62 
     63     Args:
   (...)
     70 
     71     \""\""\""
---> 72     model.backward(tensor, *args, **kwargs)

File site-packages/lightning/pytorch/core/module.py:1101, in LightningModule.backward(self, loss, *args, **kwargs)
   1099     self._fabric.backward(loss, *args, **kwargs)
   1100 else:
-> 1101     loss.backward(*args, **kwargs)

File site-packages/torch/_tensor.py:581, in Tensor.backward(self, gradient, retain_graph, create_graph, inputs)
    571 if has_torch_function_unary(self):
    572     return handle_torch_function(
    573         Tensor.backward,
    574         (self,),
   (...)
    579         inputs=inputs,
    580     )
--> 581 torch.autograd.backward(
    582     self, gradient, retain_graph, create_graph, inputs=inputs
    583 )

File site-packages/torch/autograd/__init__.py:347, in backward(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)
    342     retain_graph = create_graph
    344 # The reason we repeat the same comment below is that
    345 # some Python versions print out the first line of a multi-line function
    346 # calls in the traceback and some print out the last line
--> 347 _engine_run_backward(
    348     tensors,
    349     grad_tensors_,
    350     retain_graph,
    351     create_graph,
    352     inputs,
    353     allow_unreachable=True,
    354     accumulate_grad=True,
    355 )

File site-packages/torch/autograd/graph.py:825, in _engine_run_backward(t_outputs, *args, **kwargs)
    823     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)
    824 try:
--> 825     return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
    826         t_outputs, *args, **kwargs
    827     )  # Calls into the C++ engine to run the backward pass
    828 finally:
    829     if attach_logging_hooks:

RuntimeError: view size is not compatible with input tensor's size and stride (at least one dimension spans across two contiguous subspaces). Use .reshape(...) instead.""
}


Environment

  Current environment
#- PyTorch Lightning Version (e.g., 2.4.0):  2.4.0
#- PyTorch Version (e.g., 2.4): 2.5.1
#- Python version (e.g., 3.12): 3.11
#- OS (e.g., Linux): MacOS
#- CUDA/cuDNN version:
#- GPU models and configuration: MPS
#- How you installed Lightning(`conda`, `pip`, source): pip

More info
No response
","For me, I was using Mac M3 and MPS acceleration. When i tested with CPU it worked fine.
Issue opened :  https://github.com/pytorch/vision/issues/8706#issuecomment-2455363423
",pytorch
using positional encoding in pytorch,"Is there any built-in positional encoding in pytorch? Basically, I want to be able to specify the dimension of the encoding, and then be able to get the i'th encoding for every i.
","There isn't, as far as I'm aware.
However, you can use an implementation from PyTorch's documentation:
class PositionalEncoding(nn.Module):

    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000):
        super().__init__()
        self.dropout = nn.Dropout(p=dropout)

        position = torch.arange(max_len).unsqueeze(1)
        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))
        pe = torch.zeros(max_len, 1, d_model)
        pe[:, 0, 0::2] = torch.sin(position * div_term)
        pe[:, 0, 1::2] = torch.cos(position * div_term)
        self.register_buffer('pe', pe)

    def forward(self, x: Tensor) -> Tensor:
        """"""
        Arguments:
            x: Tensor, shape ``[seq_len, batch_size, embedding_dim]``
        """"""
        x = x + self.pe[:x.size(0)]
        return self.dropout(x)

You can find it here:
",pytorch
random choice with pytorch,"I have a tensor of pictures, and would like to randomly select from it. I'm looking for the equivalent of np.random.choice(). 
import torch

pictures = torch.randint(0, 256, (1000, 28, 28, 3))

Let's say I want 10 of these pictures.
","torch has no equivalent implementation of np.random.choice(), see the discussion here. The alternative is indexing with a shuffled index or random integers.
To do it with replacement:

Generate n random indices
Index your original tensor with these indices 

pictures[torch.randint(len(pictures), (10,))]  

To do it without replacement:

Shuffle the index
Take the n first elements

indices = torch.randperm(len(pictures))[:10]

pictures[indices]

Read more about torch.randint and torch.randperm. Second code snippet is inspired by this post in PyTorch Forums.
",pytorch
generate array of positive integers that sum of to k,"My task is simple: I want to generate an (ideally numpy) array containing all combinations of m positive (>=0), but bounded (<= e) integers that sum exactly to k. Note that k and m might be relatively high, so generating all combinations and filtering will not work.
I have implemented it in plain, recursive python but this small functions takes most of my time and I need to replace it to perform better. I have tried to come up with numpy/pytorch code to generate this array but I didn't manage to do it so far.
I currently use numpy and pytorch in my project, but I am open to other libraries as long as I write python code and I get something I can convert to numpy arrays in the end.
Here's some code:
import timeit


def get_summing_up_to(max_degree, sum, length, current=0):
    assert sum >= 0
    assert length >= 1
    if length == 1:
        residual = sum - current
        if residual <= max_degree:
            return [(residual,)]
        else:
            return []
    max_element = min(max_degree, sum - current)
    return [
        (i,) + t
        for i in range(max_element + 1)
        for t in get_summing_up_to(
            max_degree, sum, length - 1,
            current=current + i
        )
    ]


if __name__ == '__main__':
    result = timeit.timeit('get_summing_up_to(60, 60, 6)', globals=globals(), number=1)
    print(f""Execution time: {result} for max_degree=60, sum=60, length=6"")

    result = timeit.timeit('get_summing_up_to(30, 30, 8)', globals=globals(), number=1)
    print(f""Execution time: {result} for max_degree=30, sum=30, length=8"")

","One thing to notice is your function generates redundant outputs. ie get_summing_up_to(30, 30, 8) would contain (30, 0, 0, 0, 0, 0, 0, 0), (0, 30, 0, 0, 0, 0, 0, 0), ....
One way to make this more efficient is to generate unique integer combinations excluding 0s from 1 to max_length. We can also add caching to the sub-problem of generating partitions for added efficiency.
from functools import lru_cache

def get_summing_up_to_minimal(max_value, target_sum, max_length):
    # optional caching - setting maxsize recommended 
    @lru_cache(maxsize=None)
    def generate_partitions(remaining_sum, max_val, length):
        # Early pruning conditions
        if remaining_sum < 0:
            return []
        if length == 0:
            return [()] if remaining_sum == 0 else []
        
        # Minimum possible sum with given length (using all 1's)
        if remaining_sum < length:
            return []
        
        # Maximum possible sum with given length (using max_val)
        if remaining_sum > max_val * length:
            return []
            
        # Base case for length 1
        if length == 1:
            return [(remaining_sum,)] if remaining_sum <= max_val else []

        results = []
        # Optimize the start value
        start = min(max_val, remaining_sum)
        # Calculate minimum value needed to achieve remaining_sum with remaining length
        min_required = (remaining_sum - 1) // length + 1
        
        # Iterate only through viable values
        for i in range(start, min_required - 1, -1):
            # Early pruning: check if remaining values can sum to target
            remaining_length = length - 1
            remaining_target = remaining_sum - i
            
            # If maximum possible sum with remaining length is too small, break
            if i * remaining_length < remaining_target:
                break
                
            # If minimum possible sum with remaining length is too large, continue
            if remaining_target < remaining_length:
                continue
                
            sub_partitions = generate_partitions(
                remaining_target,
                min(i, max_val),
                remaining_length
            )
            
            for sub_partition in sub_partitions:
                results.append((i,) + sub_partition)
        
        return results

    all_partitions = []
    # Only try lengths that could possibly work
    min_length = (target_sum - 1) // max_value + 1
    max_possible_length = min(max_length, target_sum)
    
    for length in range(min_length, max_possible_length + 1):
        partitions = generate_partitions(target_sum, max_value, length)
        all_partitions.extend(partitions)
    
    return all_partitions

If the full output with redundant results is required, we can generate them after the fact using the minimal set of outputs from get_summing_up_to_minimal:
from itertools import permutations

def expand_partitions(compact_partitions, max_length):
    result = []
    
    for partition in compact_partitions:
        # Calculate how many zeros we need to add
        zeros_needed = max_length - len(partition)
        if zeros_needed < 0:
            continue
            
        # Create the full partition with zeros
        full_partition = partition + (0,) * zeros_needed
        
        # Generate all unique permutations
        # Using a set to handle cases where partition contains duplicate numbers
        result.extend(set(permutations(full_partition)))
    
    return result

Note that expanding partitions would be the bulk of compute time.
I profiled the following:
# run 1, your original code
out = get_summing_up_to(30, 60, 6)

# run 2, generating just minimal outputs
out = get_summing_up_to_minimal(30, 60, 6)

# run 3, generate minimal outputs and expand to full outputs
out = get_summing_up_to_minimal(30, 60, 6)
out = expand_partitions(out, 8)

On my machine, your original code takes ~4.6 seconds. Generating the minimal outputs takes ~17.9 milliseconds. Generating the minimal outputs and expanding takes ~1.1 seconds.
If your downstream use case doesn't require the redundant combinations, you can save a lot of time just generating the minimal set. If you need to pack the outputs into a numpy array/torch tensor (requiring everything be the same length), you can pad the minimal outputs to max_length with zeros without generating all the combinations. ie:
out = get_summing_up_to_minimal(30, 60, 6)
out_array = np.zeros((len(out), 6))

for i, o in enumerate(out):
    out_array[i][:len(o)] = sorted(o)

",pytorch
dqn model either doesn39t work or it is extremely slow in training,"I'm trying to build a DQN model for my PhD progress, and before I implement it with the actual real data, I want to utilize dummy data.
Using the same process with simple Q Learning the approach was effective, but once I transitioned it to DQN to make it more advanced and adaptive, I starting facing issues with the training phase. I also implemented GPU acceleration but it doesn't help at all. I wonder if it's because of the size of the dummy dataset, or something else that I can't figure out.
Any help or guidance is appreciated.
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
import random
import pandas as pd
from collections import deque

# Set device
device = torch.device(""cuda"" if torch.cuda.is_available() else ""cpu"")
print(""Running on device:"", device)

# Dummy data setup
data = {
    'message_size': np.random.randint(1000, 70000, size=1000),
    'cpu_usage': np.random.uniform(40, 100, size=1000),
    'submission_time': np.random.uniform(0, 300, size=1000)
}
dummy_data = pd.DataFrame(data)

# Parameters
MAX_BLOCK_SIZE = 32768
ALPHA = 0.1
GAMMA = 0.9
EPSILON = 1.0
EPSILON_MIN = 0.01
EPSILON_DECAY = 0.99
BATCH_SIZE = 32
EPISODES = 1000

# DQN model
class DQN(nn.Module):
    def __init__(self, input_dim):
        super(DQN, self).__init__()
        self.fc1 = nn.Linear(input_dim, 64)
        self.fc2 = nn.Linear(64, 64)
        self.fc3 = nn.Linear(64, 1)
    
    def forward(self, x):
        x = torch.relu(self.fc1(x))
        x = torch.relu(self.fc2(x))
        return self.fc3(x)

# Initialize models and optimizer
dqn = DQN(input_dim=2).to(device)
target_model = DQN(input_dim=2).to(device)
target_model.load_state_dict(dqn.state_dict())
optimizer = optim.Adam(dqn.parameters(), lr=ALPHA)
memory = deque(maxlen=2000)

# Block choice function
def block_choice(state):
    if random.random() < EPSILON:
        return random.randint(1, int(state[0] // MAX_BLOCK_SIZE) + 1)
    else:
        state_tensor = torch.FloatTensor(state).to(device)
        return torch.argmax(dqn(state_tensor)).item() + 1

# Reward function based on utility
def utility_function_rewarding(total_latency, cpu_per_block, max_latency=300, max_cpu=100):
    latency_reward = max(0, 1 - (total_latency / max_latency))
    cpu_reward = max(0, 1 - (cpu_per_block / max_cpu))
    return latency_reward + cpu_reward

# Training function
def dqn_training(batch_size):
    if len(memory) < batch_size:
        return
    batch = random.sample(memory, batch_size)
    states, actions, rewards, next_states, dones = zip(*batch)
    
    # Move data to device
    states = torch.FloatTensor(states).to(device)
    rewards = torch.FloatTensor(rewards).to(device)
    next_states = torch.FloatTensor(next_states).to(device)
    dones = torch.FloatTensor(dones).to(device)

    state_action_values = dqn(states)
    next_state_values = target_model(next_states).max(1)[0]
    expected_values = rewards + (GAMMA * next_state_values * (1 - dones))

    loss = nn.functional.mse_loss(state_action_values, expected_values)
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()

# Store transitions in memory
def store_transition(state, action, reward, next_state, done):
    memory.append((state, action, reward, next_state, done))

# Main training loop
for episode in range(EPISODES):
    print(f""Starting Episode {episode + 1}/{EPISODES}"")
    row = dummy_data.sample().iloc[0]
    state = [row['submission_time'], row['cpu_usage']]
    total_reward = 0
    done = False

    while not done:
        action = block_choice(state)
        next_row = dummy_data.sample().iloc[0]
        next_latency = next_row['submission_time']
        next_cpu = next_row['cpu_usage'] / action
        next_state = [next_latency, next_cpu]

        reward = utility_function_rewarding(next_latency, next_cpu)
        total_reward += reward
        done = episode == EPISODES - 1
        store_transition(state, action, reward, next_state, done)
        
        state = next_state
        dqn_training(BATCH_SIZE)
    
    # Update epsilon for exploration-exploitation balance
    if EPSILON > EPSILON_MIN:
        EPSILON *= EPSILON_DECAY

    print(f""Episode {episode + 1}/{EPISODES} - Total Reward: {total_reward}"")

","Firstly, this code has a Mismatch Tensor Shape Error:
UserWarning: Using a target size (torch.Size([32])) that is different to the input size (torch.Size([32, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  loss = nn.functional.mse_loss(state_action_values, expected_values)

Solution:
Flatten state_action_values: Convert it from [32, 1] to [32].
or
Expand expected_values: Convert it from [32] to [32, 1].
Secondly, there is an infinite loop in episodes.
In your main training loop, the done flag is set based on the condition episode == EPISODES - 1. This means that for all episodes except the last one, done remains False, causing the inner while not done loop to run indefinitely.
Solution:
Implement something like MAX_STEPS_PER_EPISODE.
MAX_STEPS_PER_EPISODE = 100  # This can be a hyperparameter to tune.

",pytorch
adding batch normalization decreases the performance,"I'm using PyTorch to implement a classification network for skeleton-based action recognition. The model consists of three convolutional layers and two fully connected layers. This base model gave me an accuracy of around 70% in the NTU-RGB+D dataset. I wanted to learn more about batch normalization, so I added a batch normalization for all the layers except for the last one. To my surprise, the evaluation accuracy dropped to 60% rather than increasing But the training accuracy has increased from 80% to 90%. Can anyone say what am I doing wrong? or Adding batch normalization need not increase the accuracy? 


The model with batch normalization
class BaseModelV0p2(nn.Module):

    def __init__(self, num_person, num_joint, num_class, num_coords):
        super().__init__()
        self.name = 'BaseModelV0p2'
        self.num_person = num_person
        self.num_joint = num_joint
        self.num_class = num_class
        self.channels = num_coords
        self.out_channel = [32, 64, 128]
        self.loss = loss
        self.metric = metric
        self.bn_momentum = 0.01

        self.bn_cv1 = nn.BatchNorm2d(self.out_channel[0], momentum=self.bn_momentum)
        self.conv1 = nn.Sequential(nn.Conv2d(in_channels=self.channels, out_channels=self.out_channel[0],
                                             kernel_size=3, stride=1, padding=1),
                                   self.bn_cv1,
                                    nn.ReLU(),
                                    nn.MaxPool2d(kernel_size=2, stride=2))

        self.bn_cv2 = nn.BatchNorm2d(self.out_channel[1], momentum=self.bn_momentum)
        self.conv2 = nn.Sequential(nn.Conv2d(in_channels=self.out_channel[0], out_channels=self.out_channel[1],
                                            kernel_size=3, stride=1, padding=1),
                                   self.bn_cv2,
                                nn.ReLU(),
                                nn.MaxPool2d(kernel_size=2, stride=2))

        self.bn_cv3 = nn.BatchNorm2d(self.out_channel[2], momentum=self.bn_momentum)
        self.conv3 = nn.Sequential(nn.Conv2d(in_channels=self.out_channel[1], out_channels=self.out_channel[2],
                                            kernel_size=3, stride=1, padding=1),
                                   self.bn_cv3,
                                  nn.ReLU(),
                                  nn.MaxPool2d(kernel_size=2, stride=2))

        self.bn_fc1 = nn.BatchNorm1d(256 * 2, momentum=self.bn_momentum)
        self.fc1 = nn.Sequential(nn.Linear(self.out_channel[2]*8*3, 256*2),
                                 self.bn_fc1,
                                 nn.ReLU(),
                                 nn.Dropout2d(p=0.5))  # TO check

        self.fc2 = nn.Sequential(nn.Linear(256*2, self.num_class))

    def forward(self, input):
        list_bn_layers = [self.bn_fc1, self.bn_cv3, self.bn_cv2, self.bn_cv1]
        # set the momentum  of the batch norm layers to given momentum value during trianing and 0 during evaluation
        # ref: https://discuss.pytorch.org/t/model-eval-gives-incorrect-loss-for-model-with-batchnorm-layers/7561
        # ref: https://github.com/pytorch/pytorch/issues/4741
        for bn_layer in list_bn_layers:
            if self.training:
                bn_layer.momentum = self.bn_momentum
            else:
                bn_layer.momentum = 0

        logits = []
        for i in range(self.num_person):
            out = self.conv1(input[:, :, :, :, i])

            out = self.conv2(out)

            out = self.conv3(out)

            logits.append(out)

        out = torch.max(logits[0], logits[1])
        out = out.view(out.size(0), -1)
        out = self.fc1(out)
        out = self.fc2(out)

        t = out

        assert not ((t != t).any())  # find out nan in tensor
        assert not (t.abs().sum() == 0)  # find out 0 tensor

        return out

","My interpretation of the phenomenon you are observing,, is that instead of reducing the covariance shift, which is what the Batch Normalization is meant for, you are increasing it. In other words, instead of decrease the distribution differences between train and test, you are increasing it and that's what it is causing you to have a bigger difference in the accuracies between train and test. Batch Normalization does not assure better performance always, but for some problems it doesn't work well. I have several ideas that could lead to an improvement:

Increase the batch size if it is small, what would help the mean and std calculated in the Batch Norm layers to be more robust estimates of the population parameters.
Decrease the bn_momentum parameter a bit, to see if that also stabilizes the Batch Norm parameters.
I am not sure you should set bn_momentum to zero when test, I think you should just call model.train() when you want to train and model.eval() when you want to use your trained model to perform inference.
You could alternatively try Layer Normalization instead of Batch Normalization, cause it does not require accumulating any statistic and usually works well
Try regularizing a bit your model using dropout
Make sure you shuffle your training set in every epoch. Not shuffling the data set may lead to correlated batches that make the statistics in batch normalization cycle. That may impact your generalization
I hope any of these ideas work for you

",pytorch
howwhere does pytorch max documentation show that you can pass in 2 tensors for comparison,"I am learning pytorch and deep learning. The documentation for torch.max doesn't make sense in that it looks like we can compare 2 tensors but I don't see where in the documentation I could have determined this.
I had this code at first where I wanted to check ReLU values against the maximum. I thought that 0 could be broadcast for and h1.shape=torch.Size([10000, 128]).
h1 = torch.max(h1, 0)
y = h1 @ W2 + b2

However, I got this error:
TypeError: unsupported operand type(s) for @: 'torch.return_types.max' and 'Tensor'
I got to fix this when I changed the max equation to use a tensor instead of 0.
h1 = torch.max(h1, torch.tensor(0))
y = h1 @ W2 + b2

1. Why does this fix the error?
This is when I checked the documentation again and realized that there is nothing mentions a collection like a tuple or list for multiple tensors or even a *input for iterable unpacking.
Here are the 2 versions:
1st torch.max version:

torch.max(input) → Tensor Returns the maximum value of all elements in
the input tensor.
Warning
This function produces deterministic (sub)gradients unlike max(dim=0)

2nd version of torch.max

torch.max(input, dim, keepdim=False, *, out=None) Returns a namedtuple
(values, indices) where values is the maximum value of each row of the
input tensor in the given dimension dim. And indices is the index
location of each maximum value found (argmax).
If keepdim is True, the output tensors are of the same size as input
except in the dimension dim where they are of size 1. Otherwise, dim
is squeezed (see torch.squeeze()), resulting in the output tensors
having 1 fewer dimension than input.

2. What is tensor(0) according to this documentation?
","Check the bottom of the documentation page you linked where it says:
torch.max(input, other, *, out=None) → Tensor

See torch.maximum().

When the second argument is a tensor, torch.max computes torch.maximum
",pytorch
python package bug on installing via conda,"I attempted conda install on a remote pc from my university.
Attempted
conda install pytorch==2.2.0 torchvision==0.17.0 pytorch-cuda=12.1 -c pytorch -c nvidia -y
It is very slow and I am unable to install from it. It gives the following error.
Can anybody help on looking at the error? I installed miniconda from the official website by command line.
Here's the bug:
CondaVerificationError: The package for pytorch located at /home/wang.ziyue7/miniconda3/pkgs/pytorch-2.2.0-py3.10_cuda12.1_cudnn8.9.2_0
appears to be corrupted. The path 'lib/python3.10/site-packages/torchgen/packaged/ATen/templates/RegisterDispatchKey.cpp'
specified in the package manifest cannot be found.

CondaVerificationError: The package for pytorch located at /home/wang.ziyue7/miniconda3/pkgs/pytorch-2.2.0-py3.10_cuda12.1_cudnn8.9.2_0
appears to be corrupted. The path 'lib/python3.10/site-packages/torchgen/packaged/ATen/templates/RegisterFunctionalization.cpp'
specified in the package manifest cannot be found.

CondaVerificationError: The package for pytorch located at /home/wang.ziyue7/miniconda3/pkgs/pytorch-2.2.0-py3.10_cuda12.1_cudnn8.9.2_0
appears to be corrupted. The path 'lib/python3.10/site-packages/torchgen/packaged/ATen/templates/RegisterSchema.cpp'
specified in the package manifest cannot be found.

CondaVerificationError: The package for pytorch located at /home/wang.ziyue7/miniconda3/pkgs/pytorch-2.2.0-py3.10_cuda12.1_cudnn8.9.2_0
appears to be corrupted. The path 'lib/python3.10/site-packages/torchgen/packaged/ATen/templates/TensorMethods.cpp'
specified in the package manifest cannot be found.

CondaVerificationError: The package for pytorch located at /home/wang.ziyue7/miniconda3/pkgs/pytorch-2.2.0-py3.10_cuda12.1_cudnn8.9.2_0
appears to be corrupted. The path 'lib/python3.10/site-packages/torchgen/packaged/ATen/templates/UfuncCPU.cpp'
specified in the package manifest cannot be found.

CondaVerificationError: The package for pytorch located at /home/wang.ziyue7/miniconda3/pkgs/pytorch-2.2.0-py3.10_cuda12.1_cudnn8.9.2_0
appears to be corrupted. The path 'lib/python3.10/site-packages/torchgen/packaged/ATen/templates/UfuncCPUKernel.cpp'
specified in the package manifest cannot be found.

CondaVerificationError: The package for pytorch located at /home/wang.ziyue7/miniconda3/pkgs/pytorch-2.2.0-py3.10_cuda12.1_cudnn8.9.2_0
appears to be corrupted. The path 'lib/python3.10/site-packages/torchgen/packaged/ATen/templates/UfuncCUDA.cu'
specified in the package manifest cannot be found.

CondaVerificationError: The package for pytorch located at /home/wang.ziyue7/miniconda3/pkgs/pytorch-2.2.0-py3.10_cuda12.1_cudnn8.9.2_0
appears to be corrupted. The path 'lib/python3.10/site-packages/torchgen/packaged/ATen/templates/UnboxingFunctions.cpp'
specified in the package manifest cannot be found.

CondaVerificationError: The package for pytorch located at /home/wang.ziyue7/miniconda3/pkgs/pytorch-2.2.0-py3.10_cuda12.1_cudnn8.9.2_0
appears to be corrupted. The path 'lib/python3.10/site-packages/torchgen/packaged/autograd/README.md'
specified in the package manifest cannot be found.

CondaVerificationError: The package for pytorch located at /home/wang.ziyue7/miniconda3/pkgs/pytorch-2.2.0-py3.10_cuda12.1_cudnn8.9.2_0
appears to be corrupted. The path 'lib/python3.10/site-packages/torchgen/packaged/autograd/build.bzl'
specified in the package manifest cannot be found.

CondaVerificationError: The package for pytorch located at /home/wang.ziyue7/miniconda3/pkgs/pytorch-2.2.0-py3.10_cuda12.1_cudnn8.9.2_0
appears to be corrupted. The path 'lib/python3.10/site-packages/torchgen/packaged/autograd/templates/ADInplaceOrViewType.cpp'
specified in the package manifest cannot be found.

CondaVerificationError: The package for pytorch located at /home/wang.ziyue7/miniconda3/pkgs/pytorch-2.2.0-py3.10_cuda12.1_cudnn8.9.2_0
appears to be corrupted. The path 'lib/python3.10/site-packages/torchgen/packaged/autograd/templates/Functions.cpp'
specified in the package manifest cannot be found.

CondaVerificationError: The package for pytorch located at /home/wang.ziyue7/miniconda3/pkgs/pytorch-2.2.0-py3.10_cuda12.1_cudnn8.9.2_0
appears to be corrupted. The path 'lib/python3.10/site-packages/torchgen/packaged/autograd/templates/TraceType.cpp'
specified in the package manifest cannot be found.

CondaVerificationError: The package for pytorch located at /home/wang.ziyue7/miniconda3/pkgs/pytorch-2.2.0-py3.10_cuda12.1_cudnn8.9.2_0
appears to be corrupted. The path 'lib/python3.10/site-packages/torchgen/packaged/autograd/templates/VariableType.cpp'
specified in the package manifest cannot be found.

CondaVerificationError: The package for pytorch located at /home/wang.ziyue7/miniconda3/pkgs/pytorch-2.2.0-py3.10_cuda12.1_cudnn8.9.2_0
appears to be corrupted. The path 'lib/python3.10/site-packages/torchgen/packaged/autograd/templates/annotated_fn_args.py.in'
specified in the package manifest cannot be found.

CondaVerificationError: The package for pytorch located at /home/wang.ziyue7/miniconda3/pkgs/pytorch-2.2.0-py3.10_cuda12.1_cudnn8.9.2_0
appears to be corrupted. The path 'lib/python3.10/site-packages/torchgen/packaged/autograd/templates/python_enum_tag.cpp'
specified in the package manifest cannot be found.

CondaVerificationError: The package for pytorch located at /home/wang.ziyue7/miniconda3/pkgs/pytorch-2.2.0-py3.10_cuda12.1_cudnn8.9.2_0
appears to be corrupted. The path 'lib/python3.10/site-packages/torchgen/packaged/autograd/templates/python_fft_functions.cpp'
specified in the package manifest cannot be found.

CondaVerificationError: The package for pytorch located at /home/wang.ziyue7/miniconda3/pkgs/pytorch-2.2.0-py3.10_cuda12.1_cudnn8.9.2_0
appears to be corrupted. The path 'lib/python3.10/site-packages/torchgen/packaged/autograd/templates/python_functions.cpp'
specified in the package manifest cannot be found.

CondaVerificationError: The package for pytorch located at /home/wang.ziyue7/miniconda3/pkgs/pytorch-2.2.0-py3.10_cuda12.1_cudnn8.9.2_0
appears to be corrupted. The path 'lib/python3.10/site-packages/torchgen/packaged/autograd/templates/python_linalg_functions.cpp'
specified in the package manifest cannot be found.

CondaVerificationError: The package for pytorch located at /home/wang.ziyue7/miniconda3/pkgs/pytorch-2.2.0-py3.10_cuda12.1_cudnn8.9.2_0
appears to be corrupted. The path 'lib/python3.10/site-packages/torchgen/packaged/autograd/templates/python_nested_functions.cpp'
specified in the package manifest cannot be found.

CondaVerificationError: The package for pytorch located at /home/wang.ziyue7/miniconda3/pkgs/pytorch-2.2.0-py3.10_cuda12.1_cudnn8.9.2_0
appears to be corrupted. The path 'lib/python3.10/site-packages/torchgen/packaged/autograd/templates/python_nn_functions.cpp'
specified in the package manifest cannot be found.

CondaVerificationError: The package for pytorch located at /home/wang.ziyue7/miniconda3/pkgs/pytorch-2.2.0-py3.10_cuda12.1_cudnn8.9.2_0
appears to be corrupted. The path 'lib/python3.10/site-packages/torchgen/packaged/autograd/templates/python_return_types.cpp'
specified in the package manifest cannot be found.

CondaVerificationError: The package for pytorch located at /home/wang.ziyue7/miniconda3/pkgs/pytorch-2.2.0-py3.10_cuda12.1_cudnn8.9.2_0
appears to be corrupted. The path 'lib/python3.10/site-packages/torchgen/packaged/autograd/templates/python_sparse_functions.cpp'
specified in the package manifest cannot be found.

CondaVerificationError: The package for pytorch located at /home/wang.ziyue7/miniconda3/pkgs/pytorch-2.2.0-py3.10_cuda12.1_cudnn8.9.2_0
appears to be corrupted. The path 'lib/python3.10/site-packages/torchgen/packaged/autograd/templates/python_special_functions.cpp'
specified in the package manifest cannot be found.

CondaVerificationError: The package for pytorch located at /home/wang.ziyue7/miniconda3/pkgs/pytorch-2.2.0-py3.10_cuda12.1_cudnn8.9.2_0
appears to be corrupted. The path 'lib/python3.10/site-packages/torchgen/packaged/autograd/templates/python_torch_functions.cpp'
specified in the package manifest cannot be found.

CondaVerificationError: The package for pytorch located at /home/wang.ziyue7/miniconda3/pkgs/pytorch-2.2.0-py3.10_cuda12.1_cudnn8.9.2_0
appears to be corrupted. The path 'lib/python3.10/site-packages/torchgen/packaged/autograd/templates/python_variable_methods.cpp'
specified in the package manifest cannot be found.

ClobberError: This transaction has incompatible packages due to a shared path.
  packages: defaults/linux-64::jpeg-9e-h5eee18b_3, pytorch/linux-64::libjpeg-turbo-2.0.0-h9bf148f_0
  path: 'bin/cjpeg'


ClobberError: This transaction has incompatible packages due to a shared path.
  packages: defaults/linux-64::jpeg-9e-h5eee18b_3, pytorch/linux-64::libjpeg-turbo-2.0.0-h9bf148f_0
  path: 'bin/djpeg'


ClobberError: This transaction has incompatible packages due to a shared path.
  packages: defaults/linux-64::jpeg-9e-h5eee18b_3, pytorch/linux-64::libjpeg-turbo-2.0.0-h9bf148f_0
  path: 'bin/jpegtran'


ClobberError: This transaction has incompatible packages due to a shared path.
  packages: defaults/linux-64::jpeg-9e-h5eee18b_3, pytorch/linux-64::libjpeg-turbo-2.0.0-h9bf148f_0
  path: 'bin/rdjpgcom'


ClobberError: This transaction has incompatible packages due to a shared path.
  packages: defaults/linux-64::jpeg-9e-h5eee18b_3, pytorch/linux-64::libjpeg-turbo-2.0.0-h9bf148f_0
  path: 'bin/wrjpgcom'


ClobberError: This transaction has incompatible packages due to a shared path.
  packages: defaults/linux-64::jpeg-9e-h5eee18b_3, pytorch/linux-64::libjpeg-turbo-2.0.0-h9bf148f_0
  path: 'include/jconfig.h'


ClobberError: This transaction has incompatible packages due to a shared path.
  packages: defaults/linux-64::jpeg-9e-h5eee18b_3, pytorch/linux-64::libjpeg-turbo-2.0.0-h9bf148f_0
  path: 'include/jerror.h'


ClobberError: This transaction has incompatible packages due to a shared path.
  packages: defaults/linux-64::jpeg-9e-h5eee18b_3, pytorch/linux-64::libjpeg-turbo-2.0.0-h9bf148f_0
  path: 'include/jmorecfg.h'


ClobberError: This transaction has incompatible packages due to a shared path.
  packages: defaults/linux-64::jpeg-9e-h5eee18b_3, pytorch/linux-64::libjpeg-turbo-2.0.0-h9bf148f_0
  path: 'include/jpeglib.h'


ClobberError: This transaction has incompatible packages due to a shared path.
  packages: defaults/linux-64::jpeg-9e-h5eee18b_3, pytorch/linux-64::libjpeg-turbo-2.0.0-h9bf148f_0
  path: 'lib/libjpeg.a'


ClobberError: This transaction has incompatible packages due to a shared path.
  packages: defaults/linux-64::jpeg-9e-h5eee18b_3, pytorch/linux-64::libjpeg-turbo-2.0.0-h9bf148f_0
  path: 'lib/libjpeg.so'

","There are two issues associated with your post:

Corrupted Package Files: The CondaVerificationError suggests some files expected within Pytorch are either missing or corrupted. It is either the package installation process was interrupted or incorrectly downloaded.
Path Conflict: The error ClobberError suggest that there is an issue with the file path used. Such error occurs when two try to install files to the same location. In this case the conflict is between jpeg and lipjpeg-turbo, both of which want to use the bin and include folders or directories.

Please take the following steps to resolve the issues:

Clear conda package cache to remove potentially corrupted files.
Use the code (bash):
conda clean --all

To ensure that you using the latest version of conda, use the update Conda using this code (bash):
conda update conda
This is likely to fix the compatibility issues.

Attempt the installation again by reinstalling with the original command. This is the code: conda install pytorch==2.2.0 torchvision==0.17.0 pytorch-cuda=12.1 -c pytorch -c nvidia


",pytorch
runtimeerror cuda runtime error 35  cuda driver version is insufficient for cuda runtime version at torchcsrccudamodulecpp51,"When I try to load a pytorch checkpoint:
checkpoint = torch.load(pathname)

I see:

RuntimeError: cuda runtime error (35) : CUDA driver version is insufficient for CUDA runtime version at torch/csrc/cuda/Module.cpp:51

I created the checkpoint with a GPU available, but now only have CPU available.
How do I load the checkpoint?
","Load the checkpoint data to the best currently available location:
if torch.cuda.is_available():
    map_location=lambda storage, loc: storage.cuda()
else:
    map_location='cpu'

checkpoint = torch.load(pathname, map_location=map_location)

",pytorch
python multiprocessing on multiple cpus gpus,"I have 8 GPUs, 64 CPU cores (multiprocessing.cpu_count()=64)
I am trying to get inference of multiple video files using a deep learning model. I want some files to get processed on each of the 8 GPUs. For each GPU, I want a different 6 CPU cores utilized.
Below python filename: inference_{gpu_id}.py
Input1: GPU_id
Input2: Files to process for GPU_id
from torch.multiprocessing import Pool, Process, set_start_method
try:
     set_start_method('spawn', force=True)
except RuntimeError:
    pass

model = load_model(device='cuda:' + gpu_id) 

def pooling_func(file):
    preds = []
    cap = cv2.VideoCapture(file)
    while(cap.isOpened()):
          ret, frame = cap.read()
          count += 1
          if ret == True:
                frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
                pred = model(frame)[0]
                preds.append(pred)
          else:
                break
    cap.release()
    np.save(file[:-4]+'.npy', preds)

def process_files():

    # all files to process on gpu_id
    files = np.load(gpu_id + '_files.npy') 

    # I am hoping to use 6 cores for this gpu_id, 
    # and a different 6 cores for a different GPU id
    pool = Pool(6) 

    r = list(tqdm(pool.imap(pooling_func, files), total = len(files)))
    pool.close()
    pool.join()

if __name__ == '__main__':
    import multiprocessing
    multiprocessing.freeze_support()
    process_files()

I am hoping to run inference_{gpu_id}.py files on all GPUs simultaneously
Currently, I am able to successfully run it on one GPU, 6 cores, But when I try to run it on all GPUs together, only GPU 0 runs, all others stop giving below error message.
RuntimeError: CUDA error: invalid device ordinal.
The script I am running:
CUDA_VISIBLE_DEVICES=0 inference_0.py

CUDA_VISIBLE_DEVICES=1 inference_1.py

...

CUDA_VISIBLE_DEVICES=7 inference_7.py

","Consider this, if you are not using the CUDA_VISIBLE_DEVICES flag, then all GPUs will be available to your PyTorch process. This means torch.cuda.device_count will return 8 (assuming your version setup is valid). And you will be able to get access to each one of those 8 GPUs with torch.device, via torch.device('cuda:0'), torch.device('cuda:1'), ..., and torch.device('cuda:8').
Now if you are only planning on using one and want to restrict your process to one. then CUDA_VISIBLE_DEVICES=i (where i is the device ordinal) will make it so. In this case torch.cuda will only have access to a single device through torch.device('cuda:0'). It doesn't matter what the actual device ordinal is, the way you access it is through torch.device('cuda:0').
If you allow access to more than one device: let's say n°0, n°4, and n°2, then you would use CUDA_VISIBLE_DEVICES=0,4,2. Consequently you refer to your cuda devices via d0 = torch.device('cuda:0'), d1 = torch.device('cuda:1'), and d2 = torch.device('cuda:2'). In the same order as you defined them with the flag, i.e.:

d0 -> GPU n°0, d1 -> GPU n°4, and d2 -> GPU n°2.

This makes it so you can use the same code and run it on different GPUs without having to change the underlying code where you are referring to the device ordinal.
In summary, what you need to look at is the number of devices you need to run your code. In your case: 1 is enough. You will refer to it with torch.device('cuda:0'). When running your code, however, you will need to specify what that cuda:0 device is, with the flag:
> CUDA_VISIBLE_DEVICES=0 inference.py
> CUDA_VISIBLE_DEVICES=1 inference.py
  ...
> CUDA_VISIBLE_DEVICES=7 inference.py

Do note 'cuda' will default to 'cuda:0'.
",pytorch
how does tensor permutation work in pytorch,"I'm struggling with understanding the way torch.permute() works. In general, how exactly is an n-D tensor permuted? An example with explaination for a 4-D or higher dimension tensor is highly appreciated.
I've search across the web but did not find any clearly explaination.
","All tensors are contiguous 1D data lists in memory. What differs is the interface PyTorch provides us with to access them. This all revolves around the notion of stride, which is the way this data is navigated through. Indeed, on a higher level, we prefer to reason our data in higher dimensions by using tensor shapes. The following example and description are still valid for higher-dimensional tensors.
The permutation operator offers a way to change how you access the tensor data by seemingly changing the order of dimensions. Permutations return a view and do not require a copy of the original tensor (as long as you do not make the data contiguous), in other words, the permuted tensor shares the same underlying data.
At the user interface, permutation reorders the dimensions, which means the way this tensor is indexed changes depending on the order of dimensions supplied to the torch.Tensor.permute method.
Take a simple 3D tensor example: x shaped (I=3,J=2,K=2). Given i<I, j<J, and k<K, x could naturally be accessed via x[i,j,k]. Concerning the underlying data being accessed, since the stride of x is (JK=4, J=2, 1), then x[i,j,k] corresponds to _x[i*JK+j*J+1] where _x is the underlying data of x. By ""corresponds to"", it means the data array associated with tensor x is being accessed with the index i*JK+j*J+1.
If you now were to permute your dimensions, say y = x.permute(2,0,1), then the underlying data would remain the same (in fact data_ptr would yield the same pointer) but the interface to y is different! We have y with a shape of (K,I,J) and accessing y[i,j,k] translate to x[k,i,j] ie. dim=2 move to the front and dim=0,1 moved to the back... After permutation the stride is no longer the same, y has a stride of (IJ, I, 1) so y[i,j,k] corresponds to _x[i*IJ+j*I+1].
To read more about views and strides, refer to this.
",pytorch
switch functionclass implementation between numpy amp pytorch,"I have a function (actually a class, but for simplicity, let's pretend it's a function) that uses several NumPy operations that all exist in PyTorch e.g. np.add and I also want a PyTorch version of the function. I'm trying to avoid duplicating my code, so I want to know:
Is there a way for me to dynamically switch a function's execution back and forth between NumPy and PyTorch without needing duplicate implementations?
For a toy example, suppose my function is:
def foo_numpy(x: np.ndarray, y: np.ndarray) -> np.ndarray:
  return np.add(x, y)

I could define a PyTorch equivalent:
def foo_torch(x: torch.Tensor, y: torch.Tensor) -> torch.Tensor:
  return torch.add(x, y)

Could I somehow define a function like:
def foo(x, y, mode: str = 'numpy'):
  if mode == 'numpy':
    return np.add(x, y)
  elif mode == 'torch':
    return torch.add(x, y)
  else:
    raise ValueError

without needing the if-else statement?
Edit: what about something like the following?
def foo(x, y, mode: str = 'numpy'):
  if mode == 'numpy':
    lib = np
  elif mode == 'torch':
    lib = torch
  else:
    raise ValueError
  return lib.add(x, y)

","Instead of using a string, you can use a boolean (bool) value to represent the mode you want to use i.e. False (0) representing NumPy and True (1) representing PyTorch. One can then use ternary operators to further shrink the if statements.
def foo(x, y, mode: bool = 0):
    lib = torch if mode else np
    return lib.add(x, y) 

If you want to switch back and forth between the two in a class you can do something similar
class Example:

    def __init__(self):
        self._mode = True
    
    def switchMode(self):
        self._mode = !self._mode

    def foo(self, x, y):
        lib = torch if self._mode else np
        return lib.add(x, y) 

",pytorch
pytorch gpu memory keeps increasing with every batch,"I'm training a CNN model on images. Initially, I was training on image patches of size (256, 256) and everything was fine. Then I changed my dataloader to load full HD images (1080, 1920) and I was cropping the images after some processing. In this case, the GPU memory keeps increasing with every batch. Why is this happening?
PS: While tracking losses, I'm doing loss.detach().item() so that loss is not retained in the graph.
","As suggested here, deleting the input, output and loss data helped.
Additionally, I had the data as a dictionary. Just deleting the dictionary isn't sufficient. I had to iterate over the dict elements and delete all of them.
",pytorch
inference error after training an ipadapter plus model,"I downloaded packages from https://github.com/tencent-ailab/IP-Adapter
run the commands to train an IP-Adapter plus model (input: text + image, output: image):
accelerate launch --num_processes 2 --multi_gpu --mixed_precision ""fp16"" \
  tutorial_train_plus.py \
  --pretrained_model_name_or_path=""stable-diffusion-v1-5/"" \
  --image_encoder_path=""models/image_encoder/"" \
  --data_json_file=""assets/prompt_image.json"" \
  --data_root_path=""assets/train/"" \
  --mixed_precision=""fp16"" \
  --resolution=512 \
  --train_batch_size=2 \
  --dataloader_num_workers=4 \
  --learning_rate=1e-04 \
  --weight_decay=0.01 \
  --output_dir=""out_model/"" \
  --save_steps=3

During training, there is the message but the training can be continued:
Removed shared tensor {'adapter_modules.27.to_k_ip.weight', 'adapter_modules.1.to_v_ip.weight', 'adapter_modules.31.to_k_ip.weight', 'adapter_modules.15.to_k_ip.weight', 'adapter_modules.31.to_v_ip.weight', 'adapter_modules.11.to_k_ip.weight', 'adapter_modules.23.to_k_ip.weight', 'adapter_modules.3.to_k_ip.weight', 'adapter_modules.25.to_v_ip.weight', 'adapter_modules.21.to_k_ip.weight', 'adapter_modules.17.to_v_ip.weight', 'adapter_modules.13.to_k_ip.weight', 'adapter_modules.17.to_k_ip.weight', 'adapter_modules.19.to_v_ip.weight', 'adapter_modules.13.to_v_ip.weight', 'adapter_modules.7.to_v_ip.weight', 'adapter_modules.7.to_k_ip.weight', 'adapter_modules.29.to_k_ip.weight', 'adapter_modules.3.to_v_ip.weight', 'adapter_modules.5.to_v_ip.weight', 'adapter_modules.21.to_v_ip.weight', 'adapter_modules.5.to_k_ip.weight', 'adapter_modules.23.to_v_ip.weight', 'adapter_modules.25.to_k_ip.weight', 'adapter_modules.1.to_k_ip.weight', 'adapter_modules.9.to_v_ip.weight', 'adapter_modules.9.to_k_ip.weight', 'adapter_modules.15.to_v_ip.weight', 'adapter_modules.27.to_v_ip.weight', 'adapter_modules.29.to_v_ip.weight', 'adapter_modules.19.to_k_ip.weight', 'adapter_modules.11.to_v_ip.weight'} while saving. This should be OK, but check by verifying that you don't receive anywarning while reloading

After training is finished and convert the weight to generate ip_adapter.bin, then run the inference code ip_adapter-plus_demo.py with the following model paths in this file:
base_model_path = ""SG161222/Realistic_Vision_V4.0_noVAE""
vae_model_path = ""stabilityai/sd-vae-ft-mse""
image_encoder_path = ""models/image_encoder""
ip_ckpt = ""out_model/demo_plus_checkpoint/ip_adapter.bin""

It shows the error:
raise RuntimeError('Error(s) in loading state_dict for {}:\n\t{}'.format(
RuntimeError: Error(s) in loading state_dict for ModuleList:
        Missing key(s) in state_dict: ""1.to_k_ip.weight"", ""1.to_v_ip.weight"", ""3.to_k_ip.weight"", ""3.to_v_ip.weight"", ""5.to_k_ip.weight"", ""5.to_v_ip.weight"", ""7.to_k_ip.weight"", ""7.to_v_ip.weight"", ""9.to_k_ip.weight"", ""9.to_v_ip.weight"", ""11.to_k_ip.weight"", ""11.to_v_ip.weight"", ""13.to_k_ip.weight"", ""13.to_v_ip.weight"", ""15.to_k_ip.weight"", ""15.to_v_ip.weight"", ""17.to_k_ip.weight"", ""17.to_v_ip.weight"", ""19.to_k_ip.weight"", ""19.to_v_ip.weight"", ""21.to_k_ip.weight"", ""21.to_v_ip.weight"", ""23.to_k_ip.weight"", ""23.to_v_ip.weight"", ""25.to_k_ip.weight"", ""25.to_v_ip.weight"", ""27.to_k_ip.weight"", ""27.to_v_ip.weight"", ""29.to_k_ip.weight"", ""29.to_v_ip.weight"", ""31.to_k_ip.weight"", ""31.to_v_ip.weight"".

Any step wrong to cause this error?
","The model can be trained and inferenced successfully now:
Set safe_serialization to False in model training file tutorial_train_plus.py:
accelerator.save_state(save_path, safe_serialization=False)

It will generate pytorch_model.bin instead of model.safetensors during training.
Once training is complete, modify the model conversion code as below based on the original instructions in readme:
ckpt = ""pytorch_model.bin"" # set correct path
sd = torch.load(ckpt)

Model file ip_adapter.bin will be generated for inference.
",pytorch
how to simplify 3d tensor slicing,"I want to slice a 3D tensor in PyTorch. The shape of the 3D tensor src_tensor is (batch, max_len, hidden_dim), and I have a 1D index vector indices with the shape (batch,). I want to slice along the second dimension of src_tensor. I can achieve this functionality with the following codes:
import torch
nums = 30
l = [i for i in range(nums)]
src_tensor = torch.Tensor(l).reshape((3,5,2))
indices = [1,2,3]
slice_tensor = torch.zeros((3,2,2)) 
for i in range(3):
    p1,p2 = indices[i],indices[i]+1
    slice_tensor[i,:,:]=src_tensor[i,[p1,p2],:]
print(src_tensor)
print(indices)
print(slice_tensor)
""""""
tensor([[[ 0.,  1.],
         [ 2.,  3.],
         [ 4.,  5.],
         [ 6.,  7.],
         [ 8.,  9.]],

        [[10., 11.],
         [12., 13.],
         [14., 15.],
         [16., 17.],
         [18., 19.]],

        [[20., 21.],
         [22., 23.],
         [24., 25.],
         [26., 27.],
         [28., 29.]]])
[1, 2, 3]
tensor([[[ 2.,  3.],
         [ 4.,  5.]],

        [[14., 15.],
         [16., 17.]],

        [[26., 27.],
         [28., 29.]]])
""""""

My question is whether the above code can be simplified, for example, by eliminating the for loop.
","Since your indexing along the second dimension depends on the batch index, I think you need to leverage some weird more complex indexing schemes :
import torch
# arbitrary source tensor
src_tensor = torch.Tensor(range(30)).view((3,5,2))
B, N, D = src_tensor.shape
# arbitrary, non consecutive indices made into an index tensor for torch.gather
ids = [0, 1, 3]
index = torch.tensor(ids)
# slicing
sliced = torch.empty(B, 2, D) #empty instead of zero to avoid writing useless values
sliced[:, 0, :] = src_tensor[range(B), ids]
sliced[:, 1, :] = src_tensor[:,1:,:][range(B), ids]

The  first slicing on the last line serves to offset the tensor by 1 in the second dimension, which makes it possible to access the index+1 values without doing any addition or copying.
Note that if you know for sure that your indices are all consecutives (like in your example), then using narrow instead of slicing will be even better. Alternatively, for some even more complex indexing schemes, you can also try torch.gather
Note : as far as I'm aware, this slicing scheme is consistent with numpy's, so most of the documentation you can find for numpy slicing applies to torch
",pytorch
real time object detection with yolo model not working,"I have trained a custom yolo model to detect square slots on a board, and is working with more than 95 % accuracy on images.
But as soon as I switch to video detection it seems to not detect even a single thing
I am using the following code to run real time object detection
cap = cv2.VideoCapture('../video/1st/output.mp4')
while cap.isOpened():
    ret, frame = cap.read()

    results = model(frame)
    final_img = np.squeeze(results.render())


    cv2.imshow(""YOLO"", final_img)
    if cv2.waitKey(10) & 0XFF == ord(""q""):
        break
cap.release()
cv2.destroyAllWindows()


I load the model using this code
model = torch.hub.load(""ultralytics/yolov5"", ""custom"", path=""yolov5/runs/train/exp36/weights/best.pt"",force_reload=True)
model.conf = 0.6

I have even tried splitting the available video into jpegs and running the model on individual images, saving the output and then merging the output images into a new video file.
that works perfectly, so the model is detecting something.
but as soon as I switch to video it seems to go back to nothing.
","Yes, You cannot see anything due to the none type of object return in the results.renders()/
You can change the code script like this
cap = cv2.VideoCapture('../video/1st/output.mp4')
while cap.isOpened():
    ret, frame = cap.read()

    results = model(frame)
    bboxes = results.xyxy[0].cpu().tolist()
    for bbox in bboxes:
      conf = f'{bbox[4]:.4f}' #Confidance of that prediction
      bbox = list(map(lambda x: int(x), bbox)) #To convert float to integer
      class_id = bbox[5] #Class_id 
      bbox =bbox[:4] 
      cv2.rectangle(frame,(bbox[0],bbox[1]),(bbox[2],bbox[3]),color=(255,255,255),thickness=3)
    cv2.imshow(""YOLO"", frame)
    if cv2.waitKey(10) & 0XFF == ord(""q""):
        break
cap.release()
cv2.destroyAllWindows()

and write the frames in the video
the full code should look like this
input_video_path = #Enter your video path
cap = cv2.VideoCapture(input_video_path)
fps = int(cap.get(cv2.CAP_PROP_FPS))
frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))

output_video_path = 'output_video.mp4' # Output video path
fourcc = cv2.VideoWriter_fourcc(*'XVID')
out = cv2.VideoWriter(output_video_path, fourcc, fps, (frame_width, frame_height))

model = #Here load your model
while True:
    ret, frame = cap.read()
    if not ret:
      break
    results = model(frame)
    bboxes = results.xyxy[0].cpu().tolist()
    for bbox in bboxes:
      conf = f'{bbox[4]:.4f}' #Confidance of that prediction
      bbox = list(map(lambda x: int(x), bbox)) #To convert float to integer
      class_id = bbox[5] #Class_id 
      bbox =bbox[:4] 
      cv2.rectangle(frame,(bbox[0],bbox[1]),(bbox[2],bbox[3]),color=(255,255,255),thickness=3)
    out.write(frame)
    #cv2_imshow(frame)
    if cv2.waitKey(10) & 0XFF == ord(""q""):
        break

cap.release()
out.release()

# Close all OpenCV windows
cv2.destroyAllWindows()

References:
https://docs.ultralytics.com/ 
https://docs.ultralytics.com/yolov5/ 
https://docs.ultralytics.com/yolov5/tutorials/pytorch_hub_model_loading/
",pytorch
the training loss of vgg16 implemented in pytorch does not decrease,"I want to try some toy examples in pytorch, but the training loss does not decrease in the training.
Some info is provided here:

The model is vgg16, consisted of 13 conv layers and 3 dense layers.
The data is cifar100 in pytorch.
I choose cross entropy as the loss function.

The code is as follows
# encoding: utf-8

import torch
import torch.optim as optim
import torch.nn as nn
import torch.nn.functional as F
import torchvision.transforms as transforms
import torchvision

import numpy as np


class VGG16(torch.nn.Module):
    def __init__(self, n_classes):
        super(VGG16, self).__init__()

        # construct model
        self.conv1_1 = nn.Conv2d(3, 64, 3, padding=1)
        self.conv1_2 = nn.Conv2d(64, 64, 3, padding=1)
        self.conv2_1 = nn.Conv2d(64, 128, 3, padding=1)
        self.conv2_2 = nn.Conv2d(128, 128, 3, padding=1)
        self.conv3_1 = nn.Conv2d(128, 256, 3, padding=1)
        self.conv3_2 = nn.Conv2d(256, 256, 3, padding=1)
        self.conv3_3 = nn.Conv2d(256, 256, 3, padding=1)
        self.conv4_1 = nn.Conv2d(256, 512, 3, padding=1)
        self.conv4_2 = nn.Conv2d(512, 512, 3, padding=1)
        self.conv4_3 = nn.Conv2d(512, 512, 3, padding=1)
        self.conv5_1 = nn.Conv2d(512, 512, 3, padding=1)
        self.conv5_2 = nn.Conv2d(512, 512, 3, padding=1)
        self.conv5_3 = nn.Conv2d(512, 512, 3, padding=1)

        self.fc6 = nn.Linear(512, 512)
        self.fc7 = nn.Linear(512, 512)
        self.fc8 = nn.Linear(512, n_classes)

    def forward(self, x):
        x = F.relu(self.conv1_1(x))
        x = F.relu(self.conv1_2(x))
        x = F.max_pool2d(x, (2, 2))

        x = F.relu(self.conv2_1(x))
        x = F.relu(self.conv2_2(x))
        x = F.max_pool2d(x, (2, 2))

        x = F.relu(self.conv3_1(x))
        x = F.relu(self.conv3_2(x))
        x = F.relu(self.conv3_3(x))
        x = F.max_pool2d(x, (2, 2))

        x = F.relu(self.conv4_1(x))
        x = F.relu(self.conv4_2(x))
        x = F.relu(self.conv4_3(x))
        x = F.max_pool2d(x, (2, 2))

        x = F.relu(self.conv5_1(x))
        x = F.relu(self.conv5_2(x))
        x = F.relu(self.conv5_3(x))
        x = F.max_pool2d(x, (2, 2))

        x = x.view(-1, self.num_flat_features(x))

        x = F.relu(self.fc6(x))
        x = F.relu(self.fc7(x))
        x = self.fc8(x)
        return x

    def num_flat_features(self, x):
        size = x.size()[1:]

        num_features = 1
        for s in size:
            num_features *= s
        return num_features


if __name__ == '__main__':

    BATCH_SIZE = 128
    LOG_INTERVAL = 5

    # data
    transform = transforms.Compose([
        transforms.ToTensor()
    ])

    trainset = torchvision.datasets.CIFAR100(
        root='./data',
        train=True,
        download=True,
        transform=transform
    )

    testset = torchvision.datasets.CIFAR100(
        root='./data',
        train=False,
        download=True,
        transform=transform
    )

    trainloader = torch.utils.data.DataLoader(trainset, batch_size=BATCH_SIZE, shuffle=True)
    testloader = torch.utils.data.DataLoader(testset, batch_size=BATCH_SIZE, shuffle=False)

    # model
    vgg16 = VGG16(100)
    vgg16.cuda()

    # optimizer
    optimizer = optim.SGD(vgg16.parameters(), lr=0.01)

    # loss
    criterion = nn.CrossEntropyLoss()

    print('———— Train Start —————')
    for epoch in range(20):
        running_loss = 0.
        for step, (batch_x, batch_y) in enumerate(trainloader):
            batch_x, batch_y = batch_x.cuda(), batch_y.cuda()

            # 
            optimizer.zero_grad()

            output = vgg16(batch_x)
            loss = criterion(output, batch_y)
            loss.backward()
            optimizer.step()

            running_loss += loss.item()

            if step % LOG_INTERVAL == 0:
                print('[%d, %4d] loss: %.4f' % (epoch, step, running_loss / LOG_INTERVAL))
                running_loss = 0.

        def test():
            print('———— Test Start ————')
            correct = 0
            total = 0

            # 
            with torch.no_grad():
                for test_x, test_y in testloader:
                    images, labels = test_x.cuda(), test_y.cuda()
                    output = vgg16(images)
                    _, predicted = torch.max(output.data, 1)
                    total += labels.size(0)
                    correct += (predicted == labels).sum().item()

            accuracy = 100 * correct / total
            print('Accuracy of the network is: %.4f %%' % accuracy)
            print('———— Test Finish ————')

        test()

    print('———— Train Finish —————')


The loss stays around 4.6060 and never decrease. I have tried different learning rate but does not work.
","I have noticed that you are not using Batch normalization in between your convolution layers. I have added batch normalization layers and it seems to work. Following is the modified code:
class VGG16(torch.nn.Module):
def __init__(self, n_classes):
    super(VGG16, self).__init__()

    # construct model
    self.conv1_1 = nn.Conv2d(3, 64, 3, padding=1)
    self.conv11_bn = nn.BatchNorm2d(64)
    self.conv1_2 = nn.Conv2d(64, 64, 3, padding=1)
    self.conv12_bn = nn.BatchNorm2d(64)
    self.conv2_1 = nn.Conv2d(64, 128, 3, padding=1)
    self.conv21_bn = nn.BatchNorm2d(128)
    self.conv2_2 = nn.Conv2d(128, 128, 3, padding=1)
    self.conv22_bn = nn.BatchNorm2d(128)
    self.conv3_1 = nn.Conv2d(128, 256, 3, padding=1)
    self.conv31_bn = nn.BatchNorm2d(256)
    self.conv3_2 = nn.Conv2d(256, 256, 3, padding=1)
    self.conv32_bn = nn.BatchNorm2d(256)
    self.conv3_3 = nn.Conv2d(256, 256, 3, padding=1)
    self.conv33_bn = nn.BatchNorm2d(256)
    self.conv4_1 = nn.Conv2d(256, 512, 3, padding=1)
    self.conv41_bn = nn.BatchNorm2d(512)
    self.conv4_2 = nn.Conv2d(512, 512, 3, padding=1)
    self.conv42_bn = nn.BatchNorm2d(512)
    self.conv4_3 = nn.Conv2d(512, 512, 3, padding=1)
    self.conv43_bn = nn.BatchNorm2d(512)
    self.conv5_1 = nn.Conv2d(512, 512, 3, padding=1)
    self.conv51_bn = nn.BatchNorm2d(512)        
    self.conv5_2 = nn.Conv2d(512, 512, 3, padding=1)
    self.conv52_bn = nn.BatchNorm2d(512)
    self.conv5_3 = nn.Conv2d(512, 512, 3, padding=1)
    self.conv53_bn = nn.BatchNorm2d(512)

    self.fc6 = nn.Linear(512, 512)
    self.fc7 = nn.Linear(512, 512)
    self.fc8 = nn.Linear(512, n_classes)

def forward(self, x):
    x = F.relu(self.conv11_bn(self.conv1_1(x)))
    x = F.relu(self.conv12_bn(self.conv1_2(x)))
    x = F.max_pool2d(x, (2, 2))

    x = F.relu(self.conv22_bn(self.conv2_1(x)))
    x = F.relu(self.conv21_bn(self.conv2_2(x)))
    x = F.max_pool2d(x, (2, 2))

    x = F.relu(self.conv31_bn(self.conv3_1(x)))
    x = F.relu(self.conv32_bn(self.conv3_2(x)))
    x = F.relu(self.conv33_bn(self.conv3_3(x)))
    x = F.max_pool2d(x, (2, 2))

    x = F.relu(self.conv41_bn(self.conv4_1(x)))
    x = F.relu(self.conv42_bn(self.conv4_2(x)))
    x = F.relu(self.conv43_bn(self.conv4_3(x)))
    x = F.max_pool2d(x, (2, 2))

    x = F.relu(self.conv51_bn(self.conv5_1(x)))
    x = F.relu(self.conv52_bn(self.conv5_2(x)))
    x = F.relu(self.conv53_bn(self.conv5_3(x)))
    x = F.max_pool2d(x, (2, 2))

    x = x.view(-1, self.num_flat_features(x))

    x = F.relu(self.fc6(x))
    x = F.relu(self.fc7(x))
    x = self.fc8(x)
    return x

However, a more elegant version of the same could be found here 
",pytorch
pytorch optimizer adamw and adam with weight decay,"Is there any difference between torch.optim.Adam(weight_decay=0.01) and torch.optim.AdamW(weight_decay=0.01)?
Link to the docs: torch.optim.
","Yes, Adam and AdamW weight decay are different.

Hutter pointed out in their paper (Decoupled Weight Decay Regularization) that the way weight decay is implemented in Adam in every library seems to be wrong, and proposed a simple way (which they call AdamW) to fix it.

In Adam, the weight decay is usually implemented by adding wd*w (wd is weight decay here) to the gradients (Ist case), rather than actually subtracting from weights (IInd case).
# Ist: Adam weight decay implementation (L2 regularization)
final_loss = loss + wd * all_weights.pow(2).sum() / 2
# IInd: equivalent to this in SGD
w = w - lr * w.grad - lr * wd * w


These methods are same for vanilla SGD, but as soon as we add momentum, or use a more sophisticated optimizer like Adam, L2 regularization (first equation) and weight decay (second equation) become different.

AdamW follows the second equation for weight decay.
In Adam

weight_decay (float, optional) – weight decay (L2 penalty) (default: 0)

In AdamW

weight_decay (float, optional) – weight decay coefficient (default: 1e-2)

Read more on the fastai blog.
",pytorch
how to solve this error could not find a version that satisfies the requirement torch160,"I'm using a demo on Colab notebook called Deep Exemplar Based Video Colorization. https://colab.research.google.com/drive/1Y1XTlTdUG-2LzrH1Vnr_osg9BQavfYsz?usp=sharing
Worked swimmingly for four years until this monday. In the Run Environment cell - I get the error below. I'm not sure what the protocol is here, but I thought I'd just post the Run environment cell, after I run it, which displays the errors. As it was working fine till last sunday, does anyone know if there was a recent update which could have caused the issue?
Run Environment Command cell:
# seems to be a colab bug, need to install previous version for pytorch
!pip install torch==1.6.0 torchvision==0.7.0
!pip install -q moviepy
!apt install imagemagick
!pip install imageio==2.4.1

%cd Deep-Exemplar-based-Video-Colorization/
! pip install -r requirements.txt

ERROR: Could not find a version that satisfies the requirement torch==1.6.0 (from versions: 1.11.0, 1.12.0, 1.12.1, 1.13.0, 1.13.1, 2.0.0, 2.0.1, 2.1.0, 2.1.1, 2.1.2, 2.2.0, 2.2.1, 2.2.2, 2.3.0, 2.3.1, 2.4.0, 2.4.1, 2.5.0)
ERROR: No matching distribution found for torch==1.6.0

--------------------------------
ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.
moviepy 1.0.3 requires imageio<3.0,>=2.5; python_version >= ""3.4"", but you have imageio 2.4.1 which is incompatible.
scikit-image 0.24.0 requires imageio>=2.33, but you have imageio 2.4.1 which is incompatible.

------------------------------------------------
ERROR: Failed building wheel for scipy
  Running setup.py clean for scipy
  error: subprocess-exited-with-error
  
  × python setup.py clean did not run successfully.
  │ exit code: 1
  ╰─> See above for output.
  
  note: This error originates from a subprocess, and is likely not a problem with pip.
  ERROR: Failed cleaning build dir for scipy
---------------------------------------------

","I don't know of a specific update since alst sunday, but the versions specified in the requirements.txt (scikit==1.2) and also directly in the cell (torch==1.6) are not compatible with python 3.10, which is in colab for quite some time.
The best approach is to use a python version that supports all versions required in this demo. You could try to update the package versions required in requirements.txt to a higher version compatible with python 3.10, but then you have no guarantee that the module you are trying to use still works.
To achieve the downgrade of python, add a new cell to the very top containing
!sudo apt-get install python3.7
!sudo update-alternatives --install /usr/bin/python3 python3 /usr/bin/python3.7 1
!sudo apt install python3.7-distutils
!curl https://bootstrap.pypa.io/pip/3.7/get-pip.py | sudo python3.7

",pytorch
torch module causes model field variable to become none on reference right after assignment,"How is it possible that a field variable becomes None right after the line of assignment?
In particular, the following code prints
CLIPTokenizerFast NoneType CLIPTextModelWithProjection
when _setup_txt_encoder is being run
self.txt_encoder should be the same variable as txt_encoder but when being accessed, it is retreieved as a NoneType
It appears that inheriting from torch.nn.Module is causing the problem because if I remove it from the class inheritance, there is no problem.
from transformers import AutoTokenizer, PreTrainedTokenizer, PreTrainedTokenizerFast, CLIPTextModelWithProjection
from typing import Optional
from torch.nn import Module

class Foo:
    def _setup_txt_encoder(self, clip_txt_model_name: str):
        print('loading text tokenizer and encoder')
        tokenizer = AutoTokenizer.from_pretrained(clip_txt_model_name, clean_up_tokenization_spaces=True)
        txt_encoder = CLIPTextModelWithProjection.from_pretrained(clip_txt_model_name).requires_grad_(False)
        self.tokenizer, self.txt_encoder = tokenizer, txt_encoder
        self.txt_encoder = txt_encoder
        print(type(self.tokenizer).__name__, type(self.txt_encoder).__name__, type(txt_encoder).__name__)
        return tokenizer, txt_encoder
    tokenizer: Optional[PreTrainedTokenizer|PreTrainedTokenizerFast] = None
    txt_encoder: Optional[CLIPTextModelWithProjection] = None
class Bar(Module, Foo):
    def __init__(self, clip_txt_model_name='laion/CLIP-ViT-bigG-14-laion2B-39B-b160k'):
        super().__init__()
        if self.tokenizer is None or self.txt_encoder is None:
            self._setup_txt_encoder(clip_txt_model_name)

test = Bar()

EDIT: some more information. It's really weird the following returns different things
print(getattr(test,'txt_encoder'))
print(test.__getattr__('txt_encoder'))

outputs
None
CLIPTextModelWithProjection(
  (text_model): CLIPTextTransformer(
...

","torch.nn.Module overrides __setattr__ and __getattr__
When the attribute is set to a Module object, it gets added to the dict self.__dict__['_modules'] by the overridden __setattr__ but the base __setattr__ is not called to assign it to base object. But when you call getattr or reference the attribute using the syntax foo.bar, __getattr__ is not called because the attribute is present, even if it is none or deleted before. Therefore, getattr retrieves the original item assigned to the object rather than calling the __getattr__, which is overriden, to retrieve the value assigned to _modules. This is not limited to None. You can assign it to other values like an integer 1 and self.txt_encoder would still print 1 after doing self.txt_encoder = module
A solution is to call the base __setattr__ to assign the value to the object itself as well. For the example given in the question, we have
        self.tokenizer, self.txt_encoder = tokenizer, txt_encoder
        object.__setattr__(self,'txt_encoder',txt_encoder)
        print(type(self.tokenizer).__name__, type(self.txt_encoder).__name__, type(txt_encoder).__name__)

And now it should print out CLIPTokenizerFast CLIPTextModelWithProjection CLIPTextModelWithProjection
An alternative solution is to not assign None and make sure to assign it in init.
So, rather than
txt_encoder: Optional[CLIPTextModelWithProjection] = None

We instead have
txt_encoder: CLIPTextModelWithProjection

This will ensure that the overridden __getattr__ is called.
",pytorch
how to traverse and copy a cnn pretrained model by entering layer by layer,"I am trying to make a code that allows me to generalize the layers of a pre-trained model to generate a metamodel that is equivalent to twice the original model.
For this, I am using the convolutional layers that, using the group size concept, generate two outputs, and for now, I want the rest of the layers to be duplicated simply. In such a way, the first half of the convolution output will go to layer, for example, relu 1_1, and the second to relu1_2.
The main problem I have found is that many pre-trained networks are not defined using only simple layers but instead use groupings of layers in their topology.
In the following code:
    for name, layer in model.named_children():
        if (isinstance(layer, nn.Sequential) or list(layer.children())):
            for subname, sublayer in layer.named_children():

I have used it as a condition to try to differentiate between simple layers and groupings of layers, but even so, in most models, there are groupings of layers that the condition takes as if they were simple layers.
here, I leave the complete code:
import torch
import torch.nn as nn
import copy
from torchvision import models
from collections import OrderedDict

# Custom class for performing double convolutions with group_size
class ConvolutionalBlock(nn.Module):
    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0, groups=2, filter_values=None, bias=True):
        super(ConvolutionalBlock, self).__init__()
        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding, groups=groups, bias=bias)

        if filter_values is not None:
            self.initialize_manual_weights(filter_values)
        else:
            self.initialize_integer_weights()
            
        # Set the weight and bias to float16 after initialization
        self.conv.weight.data = self.conv.weight.data.to(torch.float16)
        if self.conv.bias is not None:
            self.conv.bias.data = self.conv.bias.data.to(torch.float16)

    def initialize_integer_weights(self):
        int_weights = torch.randint(-5, 5, self.conv.weight.shape, dtype=torch.int16, device=self.conv.weight.device)
        self.conv.weight.data = int_weights.to(dtype=torch.float16)  # Ensure weights are float16
        if self.conv.bias is not None:
            int_bias = torch.zeros(self.conv.bias.shape, dtype=torch.int16, device=self.conv.bias.device)
            self.conv.bias.data = int_bias.to(dtype=torch.float16)  # Ensure bias is also float16

    def initialize_manual_weights(self, filter_values):
        filter_tensor = torch.tensor(filter_values, dtype=torch.float16, device=self.conv.weight.device).view(self.conv.weight.shape)
        self.conv.weight.data = filter_tensor  # Weights are already float16
        if self.conv.bias is not None:
            self.conv.bias.data.fill_(0.0)
            self.conv.bias.data = self.conv.bias.data.to(dtype=torch.float16)  # Ensure bias is float16

    def forward(self, x):
        return self.conv(x)

# Function to copy and modify the model
def copy_and_modify_cnn_model(model):
    modified_layers = OrderedDict()
    
    for name, layer in model.named_children():
        if (isinstance(layer, nn.Sequential) or list(layer.children())):
            for subname, sublayer in layer.named_children():

                if isinstance(sublayer, nn.Conv2d):
                    in_channels = sublayer.in_channels
                    out_channels = sublayer.out_channels
                    kernel_size = sublayer.kernel_size
                    stride = sublayer.stride
                    padding = sublayer.padding
                    
                    modified_layers[subname] = ConvolutionalBlock(
                        in_channels=in_channels * 2,
                        out_channels=out_channels,
                        kernel_size=kernel_size,
                        stride=stride,
                        padding=padding,
                        groups=2
                    )
                else:
                        modified_layers[subname + ""_1""] = copy.deepcopy(sublayer)
                        modified_layers[subname + ""_2""] = copy.deepcopy(sublayer)

        else:
            if isinstance(layer, nn.Conv2d):
                in_channels = layer.in_channels
                out_channels = layer.out_channels
                kernel_size = layer.kernel_size
                stride = layer.stride
                padding = layer.padding
                
                modified_layers[name] = ConvolutionalBlock(
                    in_channels=in_channels * 2,
                    out_channels=out_channels,
                    kernel_size=kernel_size,
                    stride=stride,
                    padding=padding,
                    groups=2
                )
            else:
                    modified_layers[name + ""_1""] = copy.deepcopy(layer)
                    modified_layers[name + ""_2""] = copy.deepcopy(layer)
    
    new_model = nn.Sequential(modified_layers)
    return new_model

# Load a pre-trained model
original_model = models.vgg16(weights=models.VGG16_Weights.IMAGENET1K_V1)

# Set the random seed for reproducibility
torch.manual_seed(0)
torch.backends.cudnn.deterministic = True
torch.backends.cudnn.benchmark = False

# Move the model to the GPU if available
device = 'cuda' if torch.cuda.is_available() else 'cpu'
original_model.to(device)

# Create the modified model
new_model = copy_and_modify_cnn_model(original_model).to(device).half()  # Convert the model to float16


# Check the new model
print(""modelo original"")
print(original_model)
print(""nuevo modelo "")
print(new_model)

# Create a synthetic input tensor
input_tensor_1 = torch.randn(1, 3, 224, 224, device=device)  # Original input tensor with 3 channels
synthetic_input_tensor = torch.cat((input_tensor_1, input_tensor_1), dim=1).to(torch.float16)

# Test the output of the modified model
output_modified = new_model(synthetic_input_tensor)
print(""\nOutput of the modified model:"")
print(output_modified)

Finally, I leave examples when trying to execute code for a model such as resnet18 or vgg16 and where you can see that when trying to duplicate the model there are layers that it does not detect or groupings of layers that it is not able to detect as loose layers.
VGG16 original layers:
VGG(
  (features): Sequential(
    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): ReLU(inplace=True)
    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (3): ReLU(inplace=True)
    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (6): ReLU(inplace=True)
    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (8): ReLU(inplace=True)
    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (11): ReLU(inplace=True)
    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (13): ReLU(inplace=True)
    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (15): ReLU(inplace=True)
    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (18): ReLU(inplace=True)
    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (20): ReLU(inplace=True)
    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (22): ReLU(inplace=True)
    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (25): ReLU(inplace=True)
    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (27): ReLU(inplace=True)
    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (29): ReLU(inplace=True)
    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  )
  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))
  (classifier): Sequential(
    (0): Linear(in_features=25088, out_features=4096, bias=True)
    (1): ReLU(inplace=True)
    (2): Dropout(p=0.5, inplace=False)
    (3): Linear(in_features=4096, out_features=4096, bias=True)
    (4): ReLU(inplace=True)
    (5): Dropout(p=0.5, inplace=False)
    (6): Linear(in_features=4096, out_features=1000, bias=True)
  )
)


VGG16 duplicate:
Sequential(
  (0): ConvolutionalBlock(
    (conv): Conv2d(6, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2)
  )
  (1_1): ReLU(inplace=True)
  (1_2): ReLU(inplace=True)
  (2): ConvolutionalBlock(
    (conv): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2)
  )
  (3_1): Linear(in_features=4096, out_features=4096, bias=True)
  (3_2): Linear(in_features=4096, out_features=4096, bias=True)
  (4_1): ReLU(inplace=True)
  (4_2): ReLU(inplace=True)
  (5): ConvolutionalBlock(
    (conv): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2)
  )
  (6_1): Linear(in_features=4096, out_features=1000, bias=True)
  (6_2): Linear(in_features=4096, out_features=1000, bias=True)
  (7): ConvolutionalBlock(
    (conv): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2)
  )
  (8_1): ReLU(inplace=True)
  (8_2): ReLU(inplace=True)
  (9_1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (9_2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (10): ConvolutionalBlock(
    (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2)
  )
  (11_1): ReLU(inplace=True)
  (11_2): ReLU(inplace=True)
  (12): ConvolutionalBlock(
    (conv): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2)
  )
  (13_1): ReLU(inplace=True)
  (13_2): ReLU(inplace=True)
  (14): ConvolutionalBlock(
    (conv): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2)
  )
  (15_1): ReLU(inplace=True)
  (15_2): ReLU(inplace=True)
  (16_1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (16_2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (17): ConvolutionalBlock(
    (conv): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2)
  )
  (18_1): ReLU(inplace=True)
  (18_2): ReLU(inplace=True)
  (19): ConvolutionalBlock(
    (conv): Conv2d(1024, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2)
  )
  (20_1): ReLU(inplace=True)
  (20_2): ReLU(inplace=True)
  (21): ConvolutionalBlock(
    (conv): Conv2d(1024, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2)
  )
  (22_1): ReLU(inplace=True)
  (22_2): ReLU(inplace=True)
  (23_1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (23_2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (24): ConvolutionalBlock(
    (conv): Conv2d(1024, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2)
  )
  (25_1): ReLU(inplace=True)
  (25_2): ReLU(inplace=True)
  (26): ConvolutionalBlock(
    (conv): Conv2d(1024, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2)
  )
  (27_1): ReLU(inplace=True)
  (27_2): ReLU(inplace=True)
  (28): ConvolutionalBlock(
    (conv): Conv2d(1024, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2)
  )
  (29_1): ReLU(inplace=True)
  (29_2): ReLU(inplace=True)
  (30_1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (30_2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (avgpool_1): AdaptiveAvgPool2d(output_size=(7, 7))
  (avgpool_2): AdaptiveAvgPool2d(output_size=(7, 7))
  (0_1): Linear(in_features=25088, out_features=4096, bias=True)
  (0_2): Linear(in_features=25088, out_features=4096, bias=True)
  (2_1): Dropout(p=0.5, inplace=False)
  (2_2): Dropout(p=0.5, inplace=False)
  (5_1): Dropout(p=0.5, inplace=False)
  (5_2): Dropout(p=0.5, inplace=False)
)

In this case, layers 1,3,4,6 of the classifier are not detected
ResNet18 original layers :
ResNet(
  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu): ReLU(inplace=True)
  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
  (layer1): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (1): BasicBlock(
      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (layer2): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): BasicBlock(
      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, aoutput
  (layer4): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (downsample): Sequential(
        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): BasicBlock(
      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))
  (fc): Linear(in_features=512, out_features=1000, bias=True)
)

ResNet18 duplicate:
Sequential(
  (conv1): ConvolutionalBlock(
    (conv): Conv2d(6, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), groups=2)
  )
  (bn1_1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn1_2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu_1): ReLU(inplace=True)
  (relu_2): ReLU(inplace=True)
  (maxpool_1): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
  (maxpool_2): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
  (0_1): BasicBlock(
    (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
    (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (downsample): Sequential(
      (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (0_2): BasicBlock(
    (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
    (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
    (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (downsample): Sequential(
      (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (1_1): BasicBlock(
    (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
    (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  )
  (1_2): BasicBlock(
    (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
    (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
    (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  )
  (avgpool_1): AdaptiveAvgPool2d(output_size=(1, 1))
  (avgpool_2): AdaptiveAvgPool2d(output_size=(1, 1))
  (fc_1): Linear(in_features=512, out_features=1000, bias=True)
  (fc_2): Linear(in_features=512, out_features=1000, bias=True)

In this case, which is more complicated, layers are also lost, and others are copied together as blocks and not as individual layers.
My main question is, how can I copy the topology of any pre-trained model layer by layer, regardless of how it is defined?
","You can simply dive into the source code of your target pretrained model and directly manipulate the target layers anyway you want. They are no different from any other variables in a python class.
By the way, the module names printed on the terminal are not their actual names in the python code. They are synthesized based on a simple rule. For example, conv1 usually means the source code looks like self.conv = nn.ModuleList([nn.Conv2d for _ in range(n)]), and it's refering to the 2nd convolution layer in the list.
",pytorch
question about data_collator throwing a key error in hugging face,"I am trying to use data_collator function in hugging face using this code:
datasets = dataset.train_test_split(test_size=0.1)
train_dataset = datasets[""train""]
val_dataset = datasets[""test""]

print(type(train_dataset))

def data_collator(data):
# Initialize lists to store pixel values and input ids
   pixel_values_list = []
   input_ids_list = []

# Iterate over each sample in the data
   for item in data:      
      pixel_values_list.append(torch.tensor(item[""pixel_values""]))
      input_ids_list.append(torch.tensor(item[""input_ids""]))


return {
    ""pixel_values"": torch.stack(pixel_values_list),
    ""labels"": torch.stack(input_ids_list)
}

the train_data has 5 keys including input_ids. However, when I print(data[0]) inside the data_collator function, I only see 1 key, which is giving an error when running the trainer:
Traceback (most recent call last):
 File ""caption-code.py"", line 134, in <module>
trainer.train()
  File ""C:\Users\moham\anaconda3\envs\transformer\lib\site- 
 packages\transformers\trainer.py"", line 1321, in train
ignore_keys_for_eval=ignore_keys_for_eval,
 File ""C:\Users\moham\anaconda3\envs\transformer\lib\site- 
  packages\transformers\trainer.py"", line 1528, in _inner_training_loop
  for step, inputs in enumerate(epoch_iterator):
    File ""C:\Users\moham\anaconda3\envs\transformer\lib\site- 
  packages\torch\utils\data\dataloader.py"", line 521, in __next__
  data = self._next_data()
 File ""C:\Users\moham\anaconda3\envs\transformer\lib\site- 
  packages\torch\utils\data\dataloader.py"", line 561, in _next_data
data = self._dataset_fetcher.fetch(index)  # may raise StopIteration
 File ""C:\Users\moham\anaconda3\envs\transformer\lib\site- 
 packages\torch\utils\data\_utils\fetch.py"", line 52, in fetch
 return self.collate_fn(data)
 File ""caption-code.py"", line 102, in data_collator
 input_ids_list.append(item[""input_ids""])
  KeyError: 'input_ids'

I am using the trainer function as follows:
training_args = Seq2SeqTrainingArguments(
predict_with_generate=True,
evaluation_strategy=""epoch"",
per_device_train_batch_size=4,
per_device_eval_batch_size=4,
output_dir=""C:/Users/moham/Desktop/Euler/output"",
logging_dir=""./logs"",
logging_steps=10,
save_steps=10,
eval_steps=10,
warmup_steps=10,
max_steps=100,  # adjust as needed
overwrite_output_dir=True,
save_total_limit=3,
 )
trainer = Seq2SeqTrainer(
model=model,
args=training_args,
train_dataset=train_dataset,
eval_dataset=val_dataset,
data_collator=data_collator,
tokenizer=tokenizer,
compute_metrics=compute_exact_match
 )
trainer.train()

","The actual issue is in your Seq2SeqTrainingArguments which is leading the error in your data_collator().
Reason: The .trainer() is by default removing any unknown columns (not present in the model's forward method) from your data when you are providing a custom data_collator(). As a result even though each sample in your train_dataset has all the keys, when you send that to data_collator(), the .trainer() automatically removes the unknown columns.
Solution: You need to include an argument in your training arguments like the following:
training_args = Seq2SeqTrainingArguments(
predict_with_generate=True,
remove_unused_columns=False, 
...)

The remove_unused_columns=False, would prevent the default behaviour and you'd get the entire data in data_collator(). This issue would be useful for further reference.
",pytorch
programmatically change components of pytorch model,"I am training a model in pytorch and would like to be able to programmatically change some components of the model architecture to check which works best without any if-blocks in the forward(). Consider a toy example:
import torch

class Model(torch.nn.Model):
   def __init__(self, layers: str, d_in: int, d_out: int):
      super().__init__()
      self.layers = layers
      linears = torch.nn.ModuleList([
         torch.nn.Linear(d_in, d_out),
         torch.nn.Linear(d_in, d_out),
      ])

   def forward(x1: torch.Tensor, x2: torch.Tensor) -> torch.Tensor:
      if self.layers == ""parallel"":
         x1 = self.linears[0](x1)
         x2 = self.linears[1](x2)
         x = x1 + x2
      elif self.layers == ""sequential"":
         x = x1 + x2
         x = self.linears[0](x)
         x = self.linears[1](x)
      return x

My first intution was to provide external functions, e.g.
def parallel(x1, x2):
   x1 = self.linears[0](x1)
   x2 = self.linears[1](x2)
   return x1 + x2

and provide them to the model, like
class Model(torch.nn.Model):
   def __init__(self, layers: str, d_in: int, d_out: int, fn: Callable):
      super().__init__()
      self.layers = layers
      linears = torch.nn.ModuleList([
         torch.nn.Linear(d_in, d_out),
         torch.nn.Linear(d_in, d_out),
      ])
      self.fn = fn

   def forward(x1: torch.Tensor, x2: torch.Tensor) -> torch.Tensor:
      x = self.fn(x1, x2)

but of course the function's scope does not know self.linears and I would also like to avoid having to pass each and every architectural element to the function.
Do I wish for too much? Do I have to ""bite the sour apple"" as it says in German and either have larger function signatures, or use if-conditions, or something else? Or is there a solution to my problem?
","You could just use the if statement in the init function or in another function, for example:
from enum import Enum

class ModelType(Enum):
    Parallel = 1
    Sequential = 2
    
class Model(torch.nn.Model):
    def __init__(self, layers: str, d_in: int, d_out: int, model_type: ModelType):
        super().__init__()
        self.layers = layers
        linears = torch.nn.ModuleList([
         torch.nn.Linear(d_in, d_out),
         torch.nn.Linear(d_in, d_out),
        ])
        self.model_type = model_type
        self.initialize()

    def initialize(self):
        if self.model_type == ModelType.Parallel:
            self.fn = self.parallel
        else if self.model_type == ModelType.Sequential::
            self.fn = self.sequential

    def forward(self, x1: torch.Tensor, x2: torch.Tensor) -> torch.Tensor:
        x = self.fn(x1, x2)
        return x
    
    def parallel(self, x1, x2):
        x1 = self.linears[0](x1)
        x2 = self.linears[0](x2)
        x = x1 + x2
        return x
    
    def sequential(self, x1, x2):
        x = x1 + x2
        x = self.linears[0](x)
        x = self.linears[0](x)
        return x

I hope it helps.
",pytorch
resnet50to function on a nonnvidia gpu,"I am trying to convert the pretrained ResNet50 model to be used on GPU using the Pytorch function resnet50.to().
The problem is that I am using an Intel Iris Plus Graphics 655 1536 MB GPU on Mac, and I don't know what argument to pass to the function as I only found the one for NVIDIA GPUs (resnet50.to('cuda:0')). 
","PyTorch uses Nvidia's CUDA API for all GPU interactions. Other GPUs that don't utilise the CUDA API (such as AMD or Intel GPUs) are therefore not supported.
If you don't have an Nvidia GPU, you cannot run PyTorch on the GPU.
",pytorch
why doesnt the output shape multiply across channels in convolutional layers,"# First convolutional layer: input channels = 1, output channels = 32, kernel size = 5x5, padding = 2 (SAME)
self.conv1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=5, stride=1, padding=2)
# First pooling layer: max pooling, kernel size = 2x2, stride = 2
self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)

# Second convolutional layer: input channels = 32, output channels = 64, kernel size = 5x5, padding = 2 (SAME)
self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=5, stride=1, padding=2)
# Second pooling layer: max pooling, kernel size = 2x2, stride = 2
self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)

Why isn't the output after the second convolution layer
14 * 14 * 32 * 64? For the 32-channel input, each convolutional kernel operates on one channel, resulting in 64 different outcomes. Shouldn't the 32 channels be multiplied together?
I got answers like: for every 14 * 14 position of the input, a 5532 kernel dot product with 5532 input area will give you a 14*14 single chnnel output. Isn't the kernel size 5 * 5?
","Let's assume that your input shape is (N, 1, 32, 32) ((N,C,H,W) format).
Now, let us list the output shape after each layer.
self.conv1: (N, 32, 32, 32). 32 filters of shape 5x5x1 are convolved over the padded input (5x5 is the kernel size, 1 is the number of input channels). Hence, the output has 32 feature maps of shape 32x32.
self.pool1: (N, 32, 16, 16). Pooling layer downsamples feature maps by a factor of 2.
self.conv2: (N, 64, 16, 16). 64 filters of shape 5x5x32 are applied to the padded input (5x5 is the kernel size, 32 is the number of input channels). Output shape after performing convolution operation with one filter is 16x16. Hence, the output of the layer has 64 feature maps of shape 16x16.
self.pool2: (N, 64, 8, 8). Pooling layer downsamples feature maps by a factor of 2.
",pytorch
best practices for explorationexploitation in reinforcement learning,"My question follows my examination of the code in the PyTorch DQN tutorial, but then refers to Reinforcement Learning in general: what are the best practices for optimal exploration/exploitation in reinforcement learning?
In the DQN tutorial, the steps_done variable is a global variable, and the EPS_DECAY = 200. This means that: after 128 steps, the epsilon threshold = 0.500; after 889 steps, the epsilon threshold = 0.0600; and after 1500 steps, the epsilon threshold = 0.05047.
This might work for the CartPole problem featured in the tutorial – where the early episodes might be very short and the task fairly simple – but what about on more complex problems in which far more exploration is required? For example, if we had a problem with 40,000 episodes, each of which had 10,000 timesteps, how would we set up the epsilon greedy exploration policy? Is there some rule of thumb that’s used in RL work?
","well, for that I guess it is better to use the linear annealed epsilon-greedy policy which updates epsilon based on steps:

EXPLORE = 3000000   #how many time steps to play
FINAL_EPSILON = 0.001 # final value of epsilon
INITIAL_EPSILON = 1.0# # starting value of epsilon

if epsilon > FINAL_EPSILON:
            epsilon -= (INITIAL_EPSILON - FINAL_EPSILON) / EXPLORE



",pytorch
pytorch model accuracy test,"I'm using Pytorch to classify a series of images. 
The NN is defined as follows:
model = models.vgg16(pretrained=True)
model.cuda()
for param in model.parameters(): param.requires_grad = False

classifier = nn.Sequential(OrderedDict([
                           ('fc1', nn.Linear(25088, 4096)),
                           ('relu', nn.ReLU()),
                           ('fc2', nn.Linear(4096, 102)),
                           ('output', nn.LogSoftmax(dim=1))
                           ]))

model.classifier = classifier

The criterions and optimizers are as follows:
criterion = nn.NLLLoss()
optimizer = optim.Adam(model.classifier.parameters(), lr=0.001)

My validation function is as follows:
def validation(model, testloader, criterion):
    test_loss = 0
    accuracy = 0
    for images, labels in testloader:

        images.resize_(images.shape[0], 784)

        output = model.forward(images)
        test_loss += criterion(output, labels).item()

        ps = torch.exp(output)
        equality = (labels.data == ps.max(dim=1)[1])
        accuracy += equality.type(torch.FloatTensor).mean()

    return test_loss, accuracy

This is the piece of code that is throwing the following error:

RuntimeError: input has less dimensions than expected

epochs = 3
print_every = 40
steps = 0
running_loss = 0
testloader = dataloaders['test']

# change to cuda
model.to('cuda')

for e in range(epochs):
    running_loss = 0
    for ii, (inputs, labels) in enumerate(dataloaders['train']):
        steps += 1

        inputs, labels = inputs.to('cuda'), labels.to('cuda')

        optimizer.zero_grad()

        # Forward and backward passes
        outputs = model.forward(inputs)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()

        running_loss += loss.item()

        if steps % print_every == 0:
            model.eval()
            with torch.no_grad():
                test_loss, accuracy = validation(model, testloader, criterion)

            print(""Epoch: {}/{}.. "".format(e+1, epochs),
                  ""Training Loss: {:.3f}.. "".format(running_loss/print_every),
                  ""Test Loss: {:.3f}.. "".format(test_loss/len(testloader)),
                  ""Test Accuracy: {:.3f}"".format(accuracy/len(testloader)))

            running_loss = 0

Any help?
","If you don't have a GPU system (say you are developing on a laptop and will eventually test on a server with GPU) you can do the same using:
if torch.cuda.is_available():
        inputs =inputs.to('cuda')
    else:
        inputs = inputs.to('cuda')

Also, if you are wondering why there is a LogSoftmax, instead of Softmax that is because he is using NLLLoss as his loss function. You can read more about softmax here
",pytorch
is it possible to use nonpytoch augmentation in transformcompose,"I am working on a data classification problem that takes images as an input in Pytorch. I would like the use the imgaug library, but unfortunatly I keep on getting errors. Here is my code.
#import necessary libraries
from torch import nn
from torchvision import models
import imgaug as ia
import imgaug.augmenters as iaa
from torchvision import datasets
from torch.utils.data.dataloader import DataLoader
from torchvision import transforms
from torch import optim
import numpy as np
from PIL import Image
import glob
from matplotlib import image

#preprocess images
#create data transformers
seq = iaa.Sequential([iaa.Sometimes(0.5,iaa.GaussianBlur(sigma=(0,3.0))),
                      iaa.Sometimes(0.5,iaa.LinearContrast((0.75,1.5))),
                      iaa.AdditiveGaussianNoise(loc=0,scale=(0.0,0.05*255),per_channel=0.5),
                      iaa.Sometimes(0.5,iaa.Affine(
        scale={""x"": (0.8, 1.2), ""y"": (0.8, 1.2)},
        translate_percent={""x"": (-0.2, 0.2), ""y"": (-0.2, 0.2)},
        rotate=(-25, 25),
        shear=(-8, 8)))],random_order=True)
        

 

train_transformation = transforms.Compose([transforms.RandomResizedCrop(300),
                                           seq,
                                           transforms.ToTensor()])

train_data = datasets.ImageFolder(root = 'train')
train_loader = DataLoader(train_data,shuffle = True,batch_size = 32,num_workers = 0)
train_iter = iter(train_loader)
train_iter.next()


----
TypeError                                 Traceback (most recent call last)
 in 
     20 train_loader = DataLoader(train_data,shuffle = True,batch_size = 32,num_workers = 0)
     21 train_iter = iter(train_loader)
---> 22 train_iter.next()

TypeError(default_collate_err_msg_format.format(elem_type))

TypeError: default_collate: batch must contain tensors, numpy arrays, numbers, dicts or lists; found 

I am aware that the input to the imgaug transformer must be a numpy array, but I am not sure how to incorporate that into my transform.compose (if I can at all that is.). When the imgaug seq is not in the transform.compose it works properly.
Thank you for the help!
","Looking at the documentation of transforms in pytorch gives us a hint of how to do it: https://pytorch.org/docs/stable/torchvision/transforms.html#generic-transforms
I would try something like:
train_transformation = transforms.Compose([transforms.RandomResizedCrop(300),
                                           transforms.Lambda(lambda x: seq(x)),
                                           transforms.ToTensor()])

",pytorch
does batch normalisation work with a small batch size,"I'm using batch normalization with batch size 10 for face detection.
Does batch normalization works with such small batch sizes? If not, then what else can i use for normalization?
","Yes, it works for the smaller size, it will work even with the smallest possible size you set.
The trick is the bach size also adds to the regularization effect, not only the batch norm.
I will show you few pics:

We are on the same scale tracking the bach loss. The left-hand side is a module without the batch norm layer (black), the right-hand side is with the batch norm layer. 
Note how the regularization effect is evident even for the bs=10.

When we set the bs=64 the batch loss regularization is super evident. Note the y scale is always [0, 4].
My examination was purely on nn.BatchNorm1d(10, affine=False) without learnable parameters gamma and beta i.e. w and b.
This is why when you have low batch size, it has sense to use the BatchNorm layer.
",pytorch
does pytorch tensor type cast preserve information,"Consider the following simple operation,
>>> t
tensor([ 1.8750, -0.6875, -1.1250, -1.3750,  1.3750, -1.1250,  0.4688, -0.4062,
         0.8750, -1.7500], dtype=torch.float)
>>> t.view(torch.uint8)
tensor([ 63, 179, 185, 187,  59, 185,  47, 173,  54, 190], dtype=torch.uint8)
>>> t.view(torch.uint8).shape
torch.Size([10])
>>> t.view(torch.uint8).numpy()
array([ 63, 179, 185, 187,  59, 185,  47, 173,  54, 190], dtype=uint8)
>>> torch.as_tensor(t.view(torch.uint8).numpy())
tensor([ 63, 179, 185, 187,  59, 185,  47, 173,  54, 190], dtype=torch.uint8)
>>> torch.as_tensor(t.view(torch.uint8).numpy()).view(torch.float)
tensor([ 1.8750, -0.6875, -1.1250, -1.3750,  1.3750, -1.1250,  0.4688, -0.4062,
         0.8750, -1.7500], dtype=torch.float)


I am confused about how information is preserved across typecast conversions. The original tensor is of type float8, which is then converted to uint8 (0-255). The uint8 numpy array is then used to initialize a float8 tensor. Shouldn't this order of conversions result in loss of information?
","All the operations you are using, namely view, numpy and as_tensor affects only the ""metadata"" of the tensors, i.e how they should interpret the data that is stored inside. None of these actually change any bit in the underlying array of numbers (which can be interpreted however you want).
You can check in the documentations for these 3 operations (numpy, view, as_tensor) that they mention sharing the storage/data (with some excceptions that don't apply to your code, like gpu data etc)
So when you go full circle, no a single bit has changed and thus you can perfectly recover the initial tensor
",pytorch
unable to install torch using pip,"I using Windows
I installed Python 3.13
I am trying to pip install torch
However, when attempting to do so, I encounter this error.
F:\Kit>pip install torch
ERROR: Could not find a version that satisfies the requirement torch (from versions: none)
ERROR: No matching distribution found for torch

What is the issue and how can it be resolved?
","It is stated in the docs that:

PyTorch on Windows only supports Python 3.8-3.11.

However on PyPi, The stable release of torch (i.e. version 2.4.1)  supports Python 3.8 - 3.12. There are wheels for Python 3.12 Windows as can be seen here.
However, there is no support yet for Python 3.13. To use this package, downgrade to a supported version perhaps Python 3.12.
",pytorch
save to disk training dataset and validation dataset separately in pytorch,"I want to save train dataset, test dataset, and validation dataset in 3 separate folders.
Doing this for training and testing is easy
# Get training and testing data
all_training_data = getattr(datasets, config['Pytorch_Dataset']['dataset'])(
    root= path_to_data + ""/data_all_train_"" + config['Pytorch_Dataset']['dataset'],
    train=True,
    download=True, # If present it will not download the data again
    transform=ToTensor()
)
test_data = getattr(datasets, config['Pytorch_Dataset']['dataset'])(
    root= path_to_data + ""/data_test_"" + config['Pytorch_Dataset']['dataset'],
    train=False,
    download=True, # If present it will not download the data again
    transform=ToTensor()
)

This code makes use of torchvision.datasets to load and save to disk the dataset specified in config['Pytorch_Dataset']['dataset'] (e.g. MNIST). However there is no option to load a validation set this way, there is no validation=True option.
I could split the train dataset into train and validation with torch.utils.data.random.split, but there are two main problems with this approach:

I don't want to save the folder data_all_train, I want to save only 2 folders, one with the true training part and one with the validation part
I would like PyTorch to understand if  data_train and data_validation are present, and in this case it should not download again data_all_train, even if not present

","You don't have to save the split results on separate folders to maintain reproducibility, which is what I am assuming you really care for.
You could instead fix the seed before calling split like this:
torch.manual_seed(42)
data_train, data_val = torch.utils.data.random_split(data_all_train, (0.7, 0.3))

Then you get to maintain just the initial folders while also ensuring the train and val splits are consistent across trials.

But the caveat to the above is, you are fixing the global seed so you are also losing the randomness you might desire in the dataloader shuffling and such, which will end up identical per trial.
To avoid that, you can narrow down the scope you are fixing the seed by setting it only for the generator you pass to the split call:
split_gen = torch.Generator()
split_gen.manual_seed(42)

data_train, data_val = torch.utils.data.random_split(
                             data_all_train, 
                             (0.7, 0.3), 
                             generator=split_gen)

",pytorch
pytorch difference between reshape and view method,"What is the difference between reshape and view method and why do we need and I am using pytorch tensors and working on changing the shape of data then I came to know these two functions. what are the affects on memory which consumes the more memory and which is more expensive if we are working on large data with less resources.
x = torch.tensor([1, 2, 3, 4, 5], dtype = torch.float32)
x = x.reshape(-1, 1)

and the view method
x = x.view(x.shape[0], 1)

What is the difference and which should I use
","The short answer: When reshaping a contiguous tensor, both methods will do the same (namely, provide a new view of the given tensor) and can therefore be used interchangeably. When reshaping a non-contiguous tensor, reshape() will duplicate the necessary parts of memory from the given tensor for producing the resulting tensor, while view() will fail with a RuntimeError.
The long answer
The main difference is how torch.Tensor.reshape() and torch.Tensor.view() handle non-contiguous tensors.
To understand the difference, we need to understand what is a contiguous tensor, and what is a view of a tensor:

A contiguous tensor is a tensor whose values are stored in a single, uninterrupted – thus, ""contiguous"" – piece of memory. A non-contiguous tensor may have gaps in its memory layout.
Producing a view of a tensor means reinterpreting the arrangement of  values in its memory. Think of a piece of memory that stores 16 values: we can interpret it, for example, as a 16-element 1-d tensor or as a 4×4-element 2-d tensor. Both interpretations can use the same underlying memory. By using views and thus reinterpreting the memory layout, we can create differently shaped tensors from the same piece of memory, in this way avoiding duplication and saving memory.

Now back to the two methods:

If applied to a contiguous tensor, both reshape() and view() will produce a new view of the given tensor's memory, in this way avoiding duplication.
If applied to a non-contiguous tensor, the creation of a view will not be possible. The two methods handle this situation differently:

The reshape() method will duplicate the necessary piece of memory and will return a tensor whose memory will not be shared with the given tensor.
The view() method will produce a RuntimeError.



We can demonstrate this with the following piece of code:
from torch import arange

contiguous = arange(16).view(4, 4)             # Create contiguous 4×4 tensor
noncontiguous = arange(20).view(4, 5)[:, :4]   # Create non-contiguous 4×4 tensor

contiguous_r = contiguous.reshape(16)          # OK: produces a 1-d view
assert contiguous_r.data_ptr() == contiguous.data_ptr()  # Same memory used

contiguous_v = contiguous.view(16)             # OK: produces a 1-d view
assert contiguous_v.data_ptr() == contiguous.data_ptr()  # Same memory used

noncontiguous_r = noncontiguous.reshape(16)    # OK: produces a new 1-d array
assert noncontiguous_r.data_ptr() != noncontiguous.data_ptr()  # New memory used

noncontiguous_v = noncontiguous.view(16)       # ERROR: cannot produce view

The last line will produce RuntimeError: view size is not compatible with input tensor's size and stride (at least one dimension spans across two contiguous subspaces). Use .reshape(...) instead.
Maybe at this point, I should also mention what a tensor's stride is: in essence, it is the information that tells us how to map the tensor's indexes to its underlying memory. You will find more information on strides in particular and on contiguous vs. non-contiguous tensors in general, for example, in this discussion in the PyTorch forum.
As to your question, which should I use?:

I would recommend using reshape() if you just want to make sure that you will receive a result when reshaping a tensor: For a contiguous tensor, reshape() will do exactly the same as view() (namely, produce a view on the given tensor and not duplicate its memory). For a non-contiguous tensor, reshape() will be the only method that will produce a result, while view() will fail (see above).
I would recommend using view() if you are actually sure that your tensors to be reshaped are contiguous and want to catch the situation where they aren't. This may be meaningful, for example, if you work in a low-memory regime and thus rather prefer failing than duplicating memory. Another use case of view() is not reinterpreting the shape  but the data type of the underlying memory. I guess this is not your use case, but you will find more on this in the documentation of view() that I linked above.

",pytorch
no module named quottorchquot,"I successfully installed pytorch via conda:
conda install pytorch-cpu torchvision-cpu -c pytorch

I also successfully installed pytorch via pip:
pip3 install https://download.pytorch.org/whl/cpu/torch-1.0.1-cp36-cp36m-win_amd64.whl
pip3 install torchvision

But, it only works in a jupyter notebook. Whenever I try to execute a script from the console, I get the error message:
No module named ""torch""

","Try to install PyTorch using pip:
First create a Conda environment using:
conda create -n env_pytorch python=3.6

Activate the environment using:
conda activate env_pytorch

Now install PyTorch using pip:
pip install torchvision 

Note: This will install both torch and torchvision.
Now go to Python shell and import using the command:
import torch
import torchvision

",pytorch
pytorch running inference on multiple gpus,"I have a model that accepts two inputs. I want to run inference on multiple GPUs where one of the inputs is fixed, while the other changes. So, let’s say I use n GPUs, each of them has a copy of the model. First gpu processes the input pair (a_1, b), the second processes (a_2, b) and so on. All the outputs are saved as files, so I don’t need to do a join operation on the outputs. How can I do this with DDP or otherwise?
","I have figured out how to do this using torch.multiprocessing.Queue:
import torch
import torch.multiprocessing as mp
from absl import app, flags
from torchvision.models import AlexNet

FLAGS = flags.FLAGS

flags.DEFINE_integer(""num_processes"", 2, ""Number of subprocesses to use"")


def infer(rank, queue):
    """"""Each subprocess will run this function on a different GPU which is indicated by the parameter `rank`.""""""
    model = AlexNet()
    device = torch.device(f""cuda:{rank}"")
    model.to(device)
    while True:
        a, b = queue.get()
        if a is None:  # check for sentinel value
            break
        x = a + b
        x = x.to(device)
        model(x)
        del a, b  # free memory
        print(f""Inference on process {rank}"")


def main(argv):
    queue = mp.Queue()
    processes = []
    for rank in range(FLAGS.num_processes):
        p = mp.Process(target=infer, args=(rank, queue))
        p.start()
        processes.append(p)
    for _ in range(10):
        a_1 = torch.randn(1, 3, 224, 224)
        a_2 = torch.randn(1, 3, 224, 224)
        b = torch.randn(1, 3, 224, 224)
        queue.put((a_1, b))
        queue.put((a_2, b))
    for _ in range(FLAGS.num_processes):
        queue.put((None, None))  # sentinel value to signal subprocesses to exit
    for p in processes:
        p.join()  # wait for all subprocesses to finish


if __name__ == ""__main__"":
    app.run(main)

",pytorch
how to dump confusion matrix using tensorboard logger in pytorchlightning,"The official doc only states
>>> from pytorch_lightning.metrics import ConfusionMatrix
>>> target = torch.tensor([1, 1, 0, 0])
>>> preds = torch.tensor([0, 1, 0, 0])
>>> confmat = ConfusionMatrix(num_classes=2)
>>> confmat(preds, target)

This doesn't show how to use the metric with the framework.
My attempt (methods are not complete and only show relevant parts):
def __init__(...):
    self.val_confusion = pl.metrics.classification.ConfusionMatrix(num_classes=self._config.n_clusters)

def validation_step(self, batch, batch_index):
    ...
    log_probs = self.forward(orig_batch)
    loss = self._criterion(log_probs, label_batch)
   
    self.val_confusion.update(log_probs, label_batch)
    self.log('validation_confusion_step', self.val_confusion, on_step=True, on_epoch=False)

def validation_step_end(self, outputs):
    return outputs

def validation_epoch_end(self, outs):
    self.log('validation_confusion_epoch', self.val_confusion.compute())


After the 0th epoch, this gives
    Traceback (most recent call last):
      File ""C:\code\EPMD\Kodex\Templates\Testing\venv\lib\site-packages\pytorch_lightning\trainer\trainer.py"", line 521, in train
        self.train_loop.run_training_epoch()
      File ""C:\code\EPMD\Kodex\Templates\Testing\venv\lib\site-packages\pytorch_lightning\trainer\training_loop.py"", line 588, in run_training_epoch
        self.trainer.run_evaluation(test_mode=False)
      File ""C:\code\EPMD\Kodex\Templates\Testing\venv\lib\site-packages\pytorch_lightning\trainer\trainer.py"", line 613, in run_evaluation
        self.evaluation_loop.log_evaluation_step_metrics(output, batch_idx)
      File ""C:\code\EPMD\Kodex\Templates\Testing\venv\lib\site-packages\pytorch_lightning\trainer\evaluation_loop.py"", line 346, in log_evaluation_step_metrics
        self.__log_result_step_metrics(step_log_metrics, step_pbar_metrics, batch_idx)
      File ""C:\code\EPMD\Kodex\Templates\Testing\venv\lib\site-packages\pytorch_lightning\trainer\evaluation_loop.py"", line 350, in __log_result_step_metrics
        cached_batch_pbar_metrics, cached_batch_log_metrics = cached_results.update_logger_connector()
      File ""C:\code\EPMD\Kodex\Templates\Testing\venv\lib\site-packages\pytorch_lightning\trainer\connectors\logger_connector\epoch_result_store.py"", line 378, in update_logger_connector
        batch_log_metrics = self.get_latest_batch_log_metrics()
      File ""C:\code\EPMD\Kodex\Templates\Testing\venv\lib\site-packages\pytorch_lightning\trainer\connectors\logger_connector\epoch_result_store.py"", line 418, in get_latest_batch_log_metrics
        batch_log_metrics = self.run_batch_from_func_name(""get_batch_log_metrics"")
      File ""C:\code\EPMD\Kodex\Templates\Testing\venv\lib\site-packages\pytorch_lightning\trainer\connectors\logger_connector\epoch_result_store.py"", line 414, in run_batch_from_func_name
        results = [func(include_forked_originals=False) for func in results]
      File ""C:\code\EPMD\Kodex\Templates\Testing\venv\lib\site-packages\pytorch_lightning\trainer\connectors\logger_connector\epoch_result_store.py"", line 414, in <listcomp>
        results = [func(include_forked_originals=False) for func in results]
      File ""C:\code\EPMD\Kodex\Templates\Testing\venv\lib\site-packages\pytorch_lightning\trainer\connectors\logger_connector\epoch_result_store.py"", line 122, in get_batch_log_metrics
        return self.run_latest_batch_metrics_with_func_name(""get_batch_log_metrics"",
*args, **kwargs)
      File ""C:\code\EPMD\Kodex\Templates\Testing\venv\lib\site-packages\pytorch_lightning\trainer\connectors\logger_connector\epoch_result_store.py"", line 115, in run_latest_batch_metrics_with_func_name
        for dl_idx in range(self.num_dataloaders)
      File ""C:\code\EPMD\Kodex\Templates\Testing\venv\lib\site-packages\pytorch_lightning\trainer\connectors\logger_connector\epoch_result_store.py"", line 115, in <listcomp>
        for dl_idx in range(self.num_dataloaders)
      File ""C:\code\EPMD\Kodex\Templates\Testing\venv\lib\site-packages\pytorch_lightning\trainer\connectors\logger_connector\epoch_result_store.py"", line 100, in get_latest_from_func_name
        results.update(func(*args, add_dataloader_idx=add_dataloader_idx, **kwargs))
      File ""C:\code\EPMD\Kodex\Templates\Testing\venv\lib\site-packages\pytorch_lightning\core\step_result.py"", line 298, in get_batch_log_metrics
        result[dl_key] = self[k]._forward_cache.detach()
    AttributeError: 'NoneType' object has no attribute 'detach'

                                                      

It does pass the sanity validation check before training.
The failure happens on the return in validation_step_end. Makes little sense to me.
The exact same method of using mertics works fine with accuracy.
How to get a correct confusion matrix?
","Updated answer, August 2022

class IntHandler:
    def legend_artist(self, legend, orig_handle, fontsize, handlebox):
        x0, y0 = handlebox.xdescent, handlebox.ydescent
        text = plt.matplotlib.text.Text(x0, y0, str(orig_handle))
        handlebox.add_artist(text)
        return text



class LightningClassifier(LightningModule):
    ...

    def _common_step(self, batch, batch_nb, stage: str):
        assert stage in (""train"", ""val"", ""test"")
        logger = self._logger
        augmented_image, labels = batch

        outputs, aux_outputs = self(augmented_image)
        loss = self._criterion(outputs, labels)

        return outputs, labels, loss

    def validation_step(self, batch, batch_nb):
        stage = ""val""
        outputs, labels, loss = self._common_step(batch, batch_nb, stage=stage)
        self._common_log(loss, stage=stage)

        return {""loss"": loss, ""outputs"": outputs, ""labels"": labels}


    def validation_epoch_end(self, outs):
        # see https://github.com/Lightning-AI/metrics/blob/ff61c482e5157b43e647565fa0020a4ead6e9d61/docs/source/pages/lightning.rst
        # each forward pass, thus leading to wrong accumulation. In practice do the following:
        tb = self.logger.experiment  # noqa

        outputs = torch.cat([tmp['outputs'] for tmp in outs])
        labels = torch.cat([tmp['labels'] for tmp in outs])

        confusion = torchmetrics.ConfusionMatrix(num_classes=self.n_labels).to(outputs.get_device())
        confusion(outputs, labels)
        computed_confusion = confusion.compute().detach().cpu().numpy().astype(int)

        # confusion matrix
        df_cm = pd.DataFrame(
            computed_confusion,
            index=self._label_ind_by_names.values(),
            columns=self._label_ind_by_names.values(),
        )

        fig, ax = plt.subplots(figsize=(10, 5))
        fig.subplots_adjust(left=0.05, right=.65)
        sn.set(font_scale=1.2)
        sn.heatmap(df_cm, annot=True, annot_kws={""size"": 16}, fmt='d', ax=ax)
        ax.legend(
            self._label_ind_by_names.values(),
            self._label_ind_by_names.keys(),
            handler_map={int: IntHandler()},
            loc='upper left',
            bbox_to_anchor=(1.2, 1)
        )
        buf = io.BytesIO()

        plt.savefig(buf, format='jpeg', bbox_inches='tight')
        buf.seek(0)
        im = Image.open(buf)
        im = torchvision.transforms.ToTensor()(im)
        tb.add_image(""val_confusion_matrix"", im, global_step=self.current_epoch)


output:

Also based on this
",pytorch
is this benchmark valid tinygrad is impossibly fast vs torch or numpy for mediumsized 10000 by 10000 matrix multiplication cpu,"I ran the following benchmark code on google collab CPU with high ram enabled. Please point out any errors in the way I am benchmarking, (if any) as well as why there is a such a high performance boost with tinygrad.
# Set the size of the matrices
size = 10000

# Generate a random 10000x10000 matrix with NumPy
np_array = np.random.rand(size, size)

# Generate a random 10000x10000 matrix with PyTorch
torch_tensor = torch.rand(size, size)

# Generate a random 10000x10000 matrix with TinyGrad
tg_tensor = Tensor.rand(size, size)  

# Benchmark NumPy
start_np = time.time()
np_result = np_array @ np_array  # Matrix multiplication
np_time = time.time() - start_np
print(f""NumPy Time: {np_time:.6f} seconds"")

# Benchmark PyTorch
start_torch = time.time()
torch_result = torch_tensor @ torch_tensor  # Matrix multiplication
torch_time = time.time() - start_torch
print(f""PyTorch Time: {torch_time:.6f} seconds"")

# Benchmark TinyGrad
start_tg = time.time()
tg_result = tg_tensor @ tg_tensor  # Matrix multiplication
tg_time = time.time() - start_tg
print(f""TinyGrad Time: {tg_time:.6f} seconds"")


NumPy Time: 11.977072 seconds
PyTorch Time: 7.905509 seconds
TinyGrad Time: 0.000607 seconds

These were the results. After running the code many times, the results were very similar
","Tinygrad performs operations in a ""lazy"" way, so the matrix multiplication hasn't been performed yet. Change your matrix multiplication line to:
tg_result = (tg_tensor @ tg_tensor).realize()

or
tg_result = (tg_tensor @ tg_tensor).numpy()

",pytorch
usage of retain graph in pytorch,"I get error if I don't supply retain_graph=True in y1.backward()
   import torch
   x = torch.tensor([2.0], requires_grad=True)
   y = torch.tensor([3.0], requires_grad=True)
   f = x+y
   z = 2*f
   y1 = z**2
   y2 = z**3
   y1.backward()
   y2.backward()

Traceback (most recent call last):
  File ""/Users/a0m08er/pytorch/pytorch_tutorial/tensor.py"", line 58, in <module>
    y2.backward()
  File ""/Users/a0m08er/pytorch/lib/python3.11/site-packages/torch/_tensor.py"", line 521, in backward
    torch.autograd.backward(
  File ""/Users/a0m08er/pytorch/lib/python3.11/site-packages/torch/autograd/__init__.py"", line 289, in backward
    _engine_run_backward(
  File ""/Users/a0m08er/pytorch/lib/python3.11/site-packages/torch/autograd/graph.py"", line 769, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
RuntimeError: Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward.

But I don't get error when I do this:
   import torch

   x = torch.tensor([2.0], requires_grad=True)
   y = torch.tensor([3.0], requires_grad=True)
   z = x+y
   y1 = z**2
   y2 = z**3
   y1.backward()
   y2.backward()

Since z is a common node for y1 and y2 why it is not showing me error when I do y2.backward()
","basically the error

Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward.

Error comes when the backwards pass tries to access tensors that were saved for the backwards pass (using ctx.save_for_backward), and those are not present (usually because they were freed after doing the first backward pass witoutretain_graph=True).
So the computation graph is still there after the first backwards pass, only the tensors saved in context were freed.
But the thing is, addition operations do not need to save tensors for backwards pass (the gradient along each of the inputs is the same as the gradient over the sum — so the gradient is just passed along the graph without doing any operation, no need to save anything for backward). Thus the error doesn't happen if the only shared node is an addition node.
In comparison, multiplication needs to save the input values for the backward pass (since the gradient for a * b along b is a * grad(a * b)). Thus the exception gets raised when it tries to access them
",pytorch
setting padding token as eos token when using datacollatorforlanguagemodeling from huggingface,"In https://huggingface.co/learn/nlp-course/chapter7/6#preparing-the-dataset, there is
from transformers import DataCollatorForLanguageModeling

tokenizer.pad_token = tokenizer.eos_token
data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)

What the tutorial is doing is using a pretrained GPT2 model and its tokenizer and trying to create a dataset for causal language modeling pretraining task.
My question with the above line is that padding token is set to be the eos token. As a result even the original eos tokens will be ignored by the model during training since they will be perceived as padding tokens too.
This would prevent my model from learning to output eos tokens when its generation is over.
How come this is in the tutorials and it is a correct way ?
","TL;DR
Ignoring the EOS symbol when training a normal language model is okay. So padding the sequence with EOS instead of a dedicated PAD symbol is okay too.

In Long
When using DataCollatorForLanguageModeling(tokenizer, mlm=False), the ""masked-language modeling"" model is off and we are doing casual language modeling ,i.e. predicting the next word given the previous. Consider this:
['this', 'is', 'a', 'foobar', '.', 'EOS']

Now we pad the sequence until it's of length 10 tokens
['this', 'is', 'a', 'foobar', '.', 'EOS', 'EOS', 'EOS', 'EOS', 'EOS']

When the model learns with causal language model, it's predicting the next word given the previous, i.e.
>>> predict(next_token, given=[""BOS""])
'this'

>>> predict(next_token, given=[""BOS"", ""this""])
'is'

...

>>> predict(next_token, given=[""BOS"", ""this"", ""is"", ""a"", ""foobar"", "".""])
'EOS'

In most common inference routine, the model will stop once the first EOS is predicted, or all beams in the search during inference produced their first EOS.
During training, the model will learn:
ground_truth = [
 'this', 'is', 'a', 'foobar', '.', 'EOS', 'EOS', 'EOS', 'EOS', 'EOS', 
]

ground_prediction = [
 'this', 'is', 'foobar', '.', 'EOS', 'EOS', 'EOS', 'EOS', 'EOS', 'EOS', 
]

And when you compute the perplexity, all the PAD symbols are ignored, and in this case, when you treat the EOS as PAD, you are essentially tell the model even the first EOS is not necessary when computing perplexity.
Q: Is that the right thing to do to ignore even the first EOS token, when we use EOS as a padding token?
A: It depends on your task and what you want the 'EOS' to mean. For most natural language, we have punctuations before 'EOS', so EOS/PAD doesn't really matter. For programming language, we have '\n' and ';' or some end of sequence operator, so EOS isn't that necessary too.
Q: Then why do we bother to pad?
A: Actually that's a good question, we're padding so that the dot-products in transformer attentions can be ""easily"" computed.
But there are many cases where pad tokens can be efficiently packed, like in RNN https://pytorch.org/docs/stable/generated/torch.nn.utils.rnn.pad_packed_sequence.html (IIRC, not in transformers architecture though)
But I don't know how much of that is already in Pytorch/JAX underlying library for ""efficient"" transformers, which will allow us to avoid pre-padding inputs. From my experience in using Huggingface Pytorch models, if you don't pad the inputs, most probably the model will complain when you do a forward pass =(
If only, someone fix that mathematically. Maybe someone did try but it's not that common to be largely used by most transformers pre-trained model (yet).
",pytorch
how to disable neptune callback in transformers trainer runs,"After installing Neptune.ai for occasional ML experiments logging, it became included by default into the list of callbacks in all transformers.trainer runs. As a result, it requires proper initialisation with token or else throws NeptuneMissingConfiguration error, demanding token and project name.
This is really annoying, I'd prefer Neptune callback to limit itself to warning or just have it disabled if no token is provided.
Unfortunately there is no obvious way to disable this callback, short of uninstalling Neptune.ai altogether. The doc page at https://huggingface.co/docs/transformers/main_classes/callback states that this callback is enabled by default and gives no way to disable it (unlike some other callbacks that can be disabled by environment variable).
Question: how to disable Neptune callback on per run basis?
","The reason Neptune is included is because the default value of report_to in TrainingArguments is ""all"", which implicitly includes all installed loggers from the officially supported list of loggers. You should either uninstall Neptune from the environment you use for the project, or pass report_to=""none"" to the TrainingArguments instance you use to initialize the Trainer (n.b.: that's the string literal ""none"", NOT a Python None).
The other answers here, including the accepted answer, are either poor workarounds for this problem, or simply do not work at all. The proper way to handle this issue is as above.
",pytorch
how to do backpropagation in pytorch when training alphazero,"I'm trying to implement my version of AlphaZero for Connect Four. I have implemented a convolutional network using PyTorch and can get (random) value- and policy outputs from the model for given boardstates. Now I would like to simulate some games and train the model using them. However, I have encountered a problem:
As far as I understand, the training consists of basically two steps: a step in which selfplay is used to gather game data, and after that a step where the collected data is used to train the model using backpropagation.
In the selfplay step, the network is used to get an evaluation of a position and a policy on how to choose the next move. The policy is then improved upon using a version of the MCTS algorithm.
After a game is finished, all the moves and the result is saved.
For simplicity, assume that I only play a single game and then want to update the model. If I save the MCTS policies and the network policies I can now calculate the loss. But I can't backpropagate through the model, since the forward pass happened during the collection step. I could in theory forward the same position through the model again, but that sounds not only inefficient, but since my architecture uses dropout layers I would not even get the same results.
So how can I solve this problem in PyTorch? Can I somehow save a model together with the dropout configuration that was used to create a policy? Then I could at least just forward the position again and use backprop afterwards, even if that would be inefficient.
","In general it is not the practice to use gradients from self-play to backprop during training (for many reasons). It would be rather in-efficient to store gradients for later backprop. Plus there is exploration noise in self-play. Re-running is normal in RL training phase.
In self-play, you will likely use eval mode to be on-policy. Drop out is only used in training for regularization purpose. In a sense drop-out can be helpful for exploration, but I think more apt exploration is using parameter noise.
I don't know about AlphaZero, but IMHO it makes less sense to store dropout noise. If you want to do that, use replay buffer to store drop activation which you capture using register_forward_hook.
",pytorch
importerror using the trainer with pytorch requires accelerategt0260 but i have version already installed 101,"I'm trying to run traning this way but face import error, how to fix it:
args = TrainingArguments(output_dir=""finetuned"",
                          num_train_epochs=10,
                          per_device_train_batch_size=16,
                          save_steps=10000,
                          gradient_accumulation_steps = 2,
                          warmup_steps=500,
                         lr_scheduler_type=""polynomial"",
                         fp16=True,
                         )

trainer = Trainer(
    model = model,
    args = args,
    train_dataset = train_dataset,
    eval_dataset = test_dataset,
    tokenizer = tokenizer,

)


trainer.train()

However it gives me such error:
File ~\mambaforge\lib\site-packages\transformers\training_args.py:1750, in TrainingArguments.__post_init__(self)
   1748 # Initialize device before we proceed
   1749 if self.framework == ""pt"" and is_torch_available():
-> 1750     self.device
   1752 if self.torchdynamo is not None:
   1753     warnings.warn(
   1754         ""`torchdynamo` is deprecated and will be removed in version 5 of 🤗 Transformers. Use""
   1755         "" `torch_compile_backend` instead"",
   1756         FutureWarning,
   1757     )

File ~\mambaforge\lib\site-packages\transformers\training_args.py:2250, in TrainingArguments.device(self)
   2246 """"""
   2247 The device used by this process.
   2248 """"""
   2249 requires_backends(self, [""torch""])
-> 2250 return self._setup_devices

File ~\mambaforge\lib\site-packages\transformers\utils\generic.py:60, in cached_property.__get__(self, obj, objtype)
     58 cached = getattr(obj, attr, None)
     59 if cached is None:
---> 60     cached = self.fget(obj)
     61     setattr(obj, attr, cached)
     62 return cached

File ~\mambaforge\lib\site-packages\transformers\training_args.py:2123, in TrainingArguments._setup_devices(self)
   2121 if not is_sagemaker_mp_enabled():
   2122     if not is_accelerate_available():
-> 2123         raise ImportError(
   2124             f""Using the `Trainer` with `PyTorch` requires `accelerate>={ACCELERATE_MIN_VERSION}`: ""
   2125             ""Please run `pip install transformers[torch]` or `pip install 'accelerate>={ACCELERATE_MIN_VERSION}'`""
   2126         )
   2127 # We delay the init of `PartialState` to the end for clarity
   2128 accelerator_state_kwargs = {""enabled"": True, ""use_configured_state"": False}

ImportError: Using the `Trainer` with `PyTorch` requires `accelerate>=0.26.0`: Please run `pip install transformers[torch]` or `pip install 'accelerate>={ACCELERATE_MIN_VERSION}'`

but I have right version:
import accelerate    
accelerate.__version__

> '1.0.1'


transformers.__version__ 
> transformers

","Just create a new conda environment and install everything from scratch. As to the cause of your issue, it could be that you were using the wrong python environment by accident, it could also be a failed pip upgrade. A lot of mistakes can lead to this outcome. Creating a new environment step by step from scratch can fix anything.
",pytorch
how to build cnn in pytorch for rgb images,"I am building a CNN in Pytorch. Below is the code I would use for grayscale input images:
import torch.nn as nn
import torch.nn.functional as F

class Net(nn.Module):
  def __init__(self):
    super(Net, self).__init__()
    # 1x1x28x28 to 32x1x28x28
    self.conv1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, stride=1, padding=1)
    # 32x1x28x28 to 64x1x28x28
    self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1)
    # 64x1x28x28 to 64x1x14x14
    self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2) 
    # 64x1x14x14 to 128x1x14x14
    self.conv3 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=1)
    # 128x1x14x14 to 128x1x7x7
    self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)
    # 128x1x7x7 to 128
    self.fc1 = nn.Linear(in_features=128*7*7, out_features=128)
    # 128 to 27 (no. of classes)
    self.fc2 = nn.Linear(in_features=128, out_features=27)

  def forward(self, x):
    x = F.relu(self.conv1(x))
    x = F.relu(self.conv2(x))
    x = self.pool1(x)
    x = F.relu(self.conv3(x))
    x = self.pool2(x)
    x = x.view(-1, 128*7*7)
    x = F.relu(self.fc1(x))
    x = F.relu(self.fc2(x))
    return x

Do I have to change any of this code to adapt it to RGB images? If so, what are these changes?
FYI:

The input images have the shape of 28x28
They are RGB (3 color channels)

Thank you so much.
","CNN only accepts tensors of 4 dims. Such as your 1x1x28x28.
If you really have identical problem (classification, num of classes, image size are identical), basically, only the number of channels changes from 1 to 3. Therefore input dims should be 1x3x28x28.
Also, you can check out this two articles from Pytorch:

https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html

https://pytorch.org/tutorials/recipes/recipes/defining_a_neural_network.html


",pytorch
creating tensors on m1 gpu by default on pytorch using jupyter,"Right now, if I want to create a tensor on gpu, I have to do it manually. For context, I'm sure that GPU support is available since
print(torch.backends.mps.is_available())# this ensures that the current current PyTorch installation was built with MPS activated.
print(torch.backends.mps.is_built())

returns True.
I've been doing this every time:
device = torch.device(""mps"")
a = torch.randn((), device=device, dtype=dtype)

Is there a way to specify, for a jupyter notebook, that all my tensors are supposed to be run on the GPU?
","The convenient way
There is no convenient way to set default device to MPS as of 2022-12-22, per discussion on this issue.

The inconvenient way
You can accomplish the objective of 'I don't want to specify device= for tensor constructors, just use MPS' by intercepting calls to tensor constructors:
class MPSMode(torch.overrides.TorchFunctionMode):
    def __init__(self):
        # incomplete list; see link above for the full list
        self.constructors = {getattr(torch, x) for x in ""empty ones arange eye full fill linspace rand randn randint randperm range zeros tensor as_tensor"".split()}
    def __torch_function__(self, func, types, args=(), kwargs=None):
        if kwargs is None:
            kwargs = {}
        if func in self.constructors:
            if 'device' not in kwargs:
                kwargs['device'] = 'mps'
        return func(*args, **kwargs)

# sensible usage
with MPSMode():
    print(torch.empty(1).device) # prints mps:0

# sneaky usage
MPSMode().__enter__()
print(torch.empty(1).device) # prints mps:0


The recommended way:
I would lean towards just putting your device in a config at the top of your notebook and using it explicitly:
class Conf: dev = torch.device(""mps"")
# ...
a = torch.randn(1, device=Conf.dev)

This requires you to type device=Conf.dev throughout the code. But you can easily switch your code to different devices, and you don't have any implicit global state to worry about.
",pytorch
