instruction,input,A,B,C,D,Answer,Categories,Domain,Deepseek coder 1.3
output matplotlib figure to svg with text as text not curves,"When I use matplotlib.pyplot.savefig(""test.svg"", format=""svg"") to export the figure as SVG, then the resulting SVG file is huge.
This is caused by the fact that there are a lot of text annotations in my figure, and each text ends up as paths in the SVG.
I want my text to end up as text strings in SVG, and not paths. It gets too hard to interpret the output too, if the text strings are exported this way.
Is there a way to force matplotlib to output text as text, not curves?
Currently, I see these code fragments in my SVG file:
<path d="" M9.8125 72.9062 L55.9062 72.9062 L55.9062 64.5938 L19.6719
64.5938 L19.6719 43.0156 L54.3906 43.0156 L54.3906 34.7188 L19.6719
34.7188 L19.6719 8.29688 L56.7812 8.29688 L56.7812 0 L9.8125 0 z ""
id=""DejaVuSans-45"" />

",Matplotlib's SVG text rendering can be configured either in the matplotlibrc or in the code. From Customizing Matplotlib with style sheets and rcParams:,   #    'default': Use the default system fonts,"Matplotlibs SVG text rendering can be configured either in the matplotlibrc or in code.
From Customizing Matplotlib with style sheets and rcParams:
#svg.fonttype : 'path'         # How to handle SVG fonts:
#    'none': Assume fonts are installed on the machine where the SVG will be viewed.
#    'path': Embed characters as paths -- supported by most SVG renderers
#    'svgfont': Embed characters as SVG fonts -- supported only by Chrome,
#               Opera and Safari

This translates to the following code for neither embedding the font nor rendering the text as path:
import matplotlib.pyplot as plt
plt.rcParams['svg.fonttype'] = 'none'

",   #svg.fonttype : 'bitmap'    # How to handle SVG fonts:,C,matplotlib,DSQA,A
remove numbers in tuples and enter them in new rows in csv,"so I have an ugly CSV file with one column and only 2 rows, but it has many tuples in it, that looks like:




Column A




(1, 2, 3)(4, 5, 6)(7, 8, 9)


(3, 2, 1)(5, 3, 6)(9, 8, 7)




and I want to have it looks like




Column A
Column B
Column C




1
2
3


4
5
6


7
8
9


3
2
1


5
3
6


9
8
7




any suggestions?
",#pip install pandas,import pandas as pd,"""Since you have a data-science tag, why not use pandas ?","Since you have a data-science tag, why not use pandas ?
#pip install pandas
import pandas as pd

df = (pd.read_csv(""input.csv"", sep=""|"").squeeze()
           .str.strip(""()"").str.split(""\)\("", expand=True)
           .melt(ignore_index=False)[""value""].str.split("",\s*"", expand=True)
           .sort_index().set_axis([""Column A"", ""Column B"", ""Column C""], axis=1)
      )

Output :
print(df)

     Column A Column B Column C
0        1        2        3
0        4        5        6
0        7        8        9
1        3        2        1
1        5        3        6
1        9        8        7

",D,data-science,DSQA,A
calculating summation over months of pandas dataframe,"I have a pandas dataframe given below:
ID       Year       R1  R1_f
KAR1    20201001    1   5
KAR1    20201101    2   6
KAR1    20201201    3   7
KAR1    20210101    4   8
KAR1    20210201    5   9
KAR1    20210301    6   10
KAR1    20210401    7   11
KAR1    20210501    8   12
KAR1    20210601    9   13
KAR1    20210701    10  14
KAR1    20210801    11  15
KAR1    20210901    12  16
KAR2    20201001    4   9
KAR2    20201101    3   8
KAR2    20201201    2   7
KAR2    20210101    1   6
KAR2    20210201    9   5
KAR2    20210301    2   4
KAR2    20210401    6   3
KAR2    20210501    5   2
KAR2    20210601    3   1
KAR2    20210701    30  2
KAR2    20210801    34  3
KAR2    20210901    20  4

I need to transform above dataframe as given below:
    ID Year      R1_sum 3m_R1 6m_R1 9m_R1 12m_R1 R1_f 3m_R1_f 6m_R1_f 9m_R1_f 12m_R1_f 
   KAR1 20210901   12      33    57    72    78    16    45     81      108      126 
   KAR2 20210901   20      84    98    110   119    4     9      15      30        54

In above output dataframe:
R1_sum is having value equal to value in year 20210901 for both Id's.
3m_R1 is the summation of values of 3 months 20210901 to 20210701 for column R1
6m_R1 is the summation of values of 6 months from 20210901 to 20210401 for column R1
9m_R1 is the summation of values of 9 months from 20210901 to 20210101 for column R1
12m_R1 is the summation of values of 12 months from 20210901 to 20201001 for column R1
R1_f is having value equal to value in year 20210901 for both Id's.
3m_R1_f is the summation of values of 3 months 20210901 to 20210701 for column R1_f
6m_R1_f is the summation of values of 6 months from 20210901 to 20210401 for column R1_f
9m_R1_f is the summation of values of 9 months from 20210901 to 20210101 for column R1_f
12m_R1_f is the summation of values of 12 months from 20210901 to 20201001 for column R1_f
Please help
","""For count from last months to first months per groups ID, start by grouping using GroupBy.cumcount and aggregate mean, then aggregate mean with GroupBy.cumsmax, reshape by DataFrame.pivot, flatten MultiIndex in rows and merge with DataFrame created by GroupBy.first:","For count from last months to first months per groups ID first crate helper groups by GroupBy.cumcount and aggregate sum, then aggregate sum with GroupBy.cumsum, reshape by DataFrame.unstack, flatten MultiIndex in columns and add to DataFrame created by GroupBy.last:
Data:
df = pd.DataFrame({'ID': ['KAR1', 'KAR1', 'KAR1', 'KAR1', 'KAR1', 'KAR1', 'KAR1', 
                          'KAR1', 'KAR1', 'KAR1', 'KAR1', 'KAR1', 'KAR2', 'KAR2', 'KAR2', 'KAR2', 'KAR2', 'KAR2', 'KAR2', 'KAR2', 'KAR2', 'KAR2', 'KAR2', 'KAR2'], 
                   'Year': [20201001, 20201101, 20201201, 20210101, 20210201, 20210301,
                            20210401, 20210501, 20210601, 20210701, 20210801, 20210901, 
                            20201001, 20201101, 20201201, 20210101, 20210201, 20210301,
                            20210401, 20210501, 20210601, 20210701, 20210801, 20210901], 
                   'R1': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 4, 3, 2, 1, 9, 2, 6, 5, 3, 30, 34, 20], 
                   'R1_f': [5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 9, 8, 7, 6, 5, 4, 3, 2, 1, 2, 3, 4]})
    
print (df)


cols = ['R1','R1_f']
g = df.groupby('ID').cumcount(ascending=False) // 3
df1 = df.groupby(['ID',g])[cols].sum().groupby(level=0).cumsum().unstack()
print (df1)
      R1               R1_f              
       0   1    2    3    0   1    2    3
ID                                       
KAR1  33  57   72   78   45  81  108  126
KAR2  84  98  110  119    9  15   30   54

df2 = df.groupby('ID')[['Year'] + cols].last()
df2.columns = pd.MultiIndex.from_product([df2.columns, [-1]])
print (df2)
            -1  -1   -1
ID                     
KAR1  20210901  12   16
KAR2  20210901  20    4

df = df2.join(df1).sort_index(axis=1)
df.columns = [f'{(b + 1) * 3}m_{a}' if b!=-1 else f'{a}_sum' for a, b in df.columns]
df = df.reset_index()
df.insert(1, 'Year', df.pop('Year_sum'))
print (df)
     ID      Year  R1_sum  3m_R1  6m_R1  9m_R1  12m_R1  R1_f_sum  3m_R1_f  \
0  KAR1  20210901      12     33     57     72      78        16       45   
1  KAR2  20210901      20     84     98    110     119         4        9   

   6m_R1_f  9m_R1_f  12m_R1_f  
0       81      108       126  
1       15       30        54  

","df = pd.DataFrame({'ID': ['KAR1', 'KAR1', 'KAR1', 'KAR1', 'KAR1', 'KAR1', 'KAR1', ",Data:,B,data-science,DSQA,A
difference between predict and predict_proba functions in scikit learn,"Greetings data science community! How's going? So, I'm studying classification Tree and scikit-learning and during my studyings i come across this ""issue"":
After training a tree (clf = DecisionTreeClassifier()) and training it (clf.fit(Xtrain, ytrain)) i have decided to test its performance on the training data itself (just to compare, later, with the test data, in terms of Sensitivity Specificity and ROC-AUC).
But instead to only apply the predict() I also applied the predict_proba() on the X_train data.
As you can se by the image, the observation 4 has 50 % of probability to give zero and 50%  to give one (according to predict_proba() function) however the predict() function classified it as zero
Image with the dataframe where the first column is the result from predict_proba() function and the second column is the result from predict() column
Did the predict() function sort as ZERO by ""chance"" or since it's zero or one, does it sort as zero because it comes first (as if order matters)?
I could not solve my doubts when analyzing the documentation of the functions (source: https://github.com/scikit-learn/scikit-learn/blob/7f9bad99d/sklearn/tree/\_classes.py#L476)
Thanks in advance!
",   ...,"In case of binary classification, the returned class is computed using the mean function:","In case of binary classification the returned class is computed as follows
proba = self.tree_.predict(X)
...
return self.classes_.take(np.argmax(proba, axis=1), axis=0)

Reference: sklearn code
So basicaly the choice falls on numpy.argmax.
Let's look in the numpy documentation and read the following:

Notes:
In case of multiple occurrences of the maximum values, the indices corresponding to the first occurrence are returned.

So the final answer - in case of equal probabilities the first class is chosen always, which in case of binary classification corresponds to the negative label.
",   proba = self.tree_.predict(X),C,data-science,DSQA,D
pandas holiday package black friday offset,"I am using the holidays package to build a calendar of my calendar holidays and I made an adjustment to account for ""Black Friday"" (Day after US Thanksgiving. Always a Friday) with a relative delta for the 4th week in November, which works for 2018 and 2020, but this year, it would be the 5th week in November, which would make my setup ineffective
Is there a better way to ensure that this value always falls on the Friday after Thanksgiving? I'm not sure whether it is best to use the holidays package list of holidays and use some pandas magic to offset based on those values and set the holiday or if there is a better date manipulation method to achieve this.
Here is my method:
self.append({datetime.date(datetime(year, 11, 1) + relativedelta(weekday=FR(+4))) : ""Black Friday""}) # Adding Black Friday

Here is the full code:
import pandas as pd
import numpy as np
import calendar as cal
from datetime import *
from dateutil.relativedelta import *
import holidays

class CorporateHolidays(holidays.UnitedStates):
    def _populate(self, year):
        print(self)
        # Populate the holiday list with the default US holidays
        holidays.UnitedStates._populate(self, year)
        # Remove Holiday Date(s)
        self.pop(datetime.date(datetime(year, 10, 1) + relativedelta(weekday=MO(+2))), None) # Removing Columbus Day
        # Add Holiday Date(s)
        self.append({datetime.date(datetime(year, 12, 24)) : ""Christmas Eve""})
        self.append({datetime.date(datetime(year, 12, 31)) : ""New Years Eve""})
        self.append({datetime.date(datetime(year, 11, 1) + relativedelta(weekday=FR(+4))) : ""Black Friday""}) # Adding Black Friday

Where print(self) renders a list as such:
{datetime.date(2018, 1, 1): ""New Year's Day"", datetime.date(2018, 1, 15): 'Martin Luther King, Jr. Day', datetime.date(2018, 2, 19): ""Washington's Birthday"", datetime.date(2018, 5, 28): 'Memorial Day', datetime.date(2018, 7, 4): 'Independence Day', datetime.date(2018, 9, 3): 'Labor Day', datetime.date(2018, 11, 11): 'Veterans Day', datetime.date(2018, 11, 12): 'Veterans Day (Observed)', datetime.date(2018, 11, 22): 'Thanksgiving', datetime.date(2018, 12, 25): 'Christmas Day', datetime.date(2018, 12, 24): 'Christmas Eve', datetime.date(2018, 12, 31): 'New Years Eve', datetime.date(2018, 11, 23): 'Black Friday'}....

","   `datetime.date(datetime(year, 11, 1) + relativedelta(weekday=TH(+3)) + timedelta(days=2))`",Thanksgiving is (according to Wikipedia) always the third Thursday in November. This causes a problem for you if the month starts on a Saturday. So rather than using the third Saturday try to use the third Thursday and add two final days to make it Saturday. Something like ,,"Thanksgiving is (according to Wikipedia) always the forth Thursday in November. This causes a problem for you if the month starts on a Friday. So rather than using the forth Friday try to use the forth Thursday and add a final day to make it Friday. Something like 
datetime.date(datetime(year, 11, 1) + relativedelta(weekday=TH(+4)) + timedelta(days=1))

In [5]: datetime.date(datetime(2018, 11, 1) + relativedelta(weekday=TH(+4)) + timedelta(days=1))                                                                                                            
Out[5]: datetime.date(2018, 11, 23)

In [6]: datetime.date(datetime(2019, 11, 1) + relativedelta(weekday=TH(+4)) + timedelta(days=1))                                                                                                            
Out[6]: datetime.date(2019, 11, 29)


should do the trick
",D,pandas,DSQA,B
interpolating battery capacity data in logarithmic scale with python,"I'm working on interpolating battery capacity data based on the relationships between hour_rates, capacities and currents. Here’s a sample of my data:
import numpy as np
import pandas as pd
from scipy.interpolate import interp1d
import matplotlib.pyplot as plt

# Data from Rolls S-480 flooded battery
capacity_data = [
    [1, 135, 135], [2, 191, 95.63], [3, 221, 73.75],
    [4, 244, 60.94], [5, 263, 52.5], [6, 278, 46.25],
    [8, 300, 37.5], [10, 319, 31.88], [12, 334, 27.81],
    [15, 352, 23.45], [20, 375, 18.75], [24, 386, 16.09],
    [50, 438, 8.76], [72, 459, 6.38], [100, 486, 4.86]
]
capacity = pd.DataFrame(capacity_data, columns=['hour_rates', 'capacities_o', 'currents'])
capacity['capacities'] = np.around(capacity['currents'] * capacity['hour_rates'], 3)

The columns relate as follows:

hour_rates (h) = capacities (Ah) / currents (A)
capacities (Ah) = hour_rates (h) * currents (A)
currents (A) = capacities (Ah) / hour_rates (h)

Objective: I want to interpolate capacities and hour_rates for a range of currents values using logarithmic scaling for better accuracy.
Code
Custom interpolation class and function to achieve this. Here’s the code:
from typing import Union

class interpolate1d(interp1d):
    """"""Extend scipy interp1d to interpolate/extrapolate per axis in log space""""""
    
    def __init__(self, x, y, *args, xspace='linear', yspace='linear', **kwargs):
        self.xspace = xspace
        self.yspace = yspace
        if self.xspace == 'log': x = np.log10(x)
        if self.yspace == 'log': y = np.log10(y)
        super().__init__(x, y, *args, **kwargs)
        
    def __call__(self, x, *args, **kwargs):
        if self.xspace == 'log': x = np.log10(x)
        if self.yspace == 'log':
            return 10**super().__call__(x, *args, **kwargs)
        else:
            return super().__call__(x, *args, **kwargs)


def interpolate_cap_by_current(df: list,
                               current_values: list,
                               kind: Union[str, int] = 'linear',
                               hr_limit: int = 600
                               ):
    """"""
    Interpolate Battery Capacity Values From Current list values
    """"""
    result = 0
    if isinstance(np_data, np.ndarray):
        # Create interpolation functions for hour rates and capacities
        # Setting kind='cubic' for better fitting to nonlinear data
        hour_rate_interp_func = interpolate1d(
            df['currents'],
            df['hour_rates'],
            xspace='log',
            yspace='log',
            fill_value=""extrapolate"",
            kind=kind
        )
        capacity_interp_func = interpolate1d(
            df['currents'],
            df['capacities'],
            xspace='log',
            yspace='log',
            fill_value=""extrapolate"",
            kind=kind
        ) # , kind='cubic'

        # Calculate interpolated values for new currents
        hour_rate_interpolated = hour_rate_interp_func(current_values)
        capacity_interpolated = capacity_interp_func(current_values)

        # Create a DataFrame for the results
        calc_cap = np.around(current_values * hour_rate_interpolated, 3)
        calc_hr = np.around(capacity_interpolated / current_values, 3)
        diff_cap = np.around(capacity_interpolated - calc_cap, 3)
        diff_hr = np.around(hour_rate_interpolated - calc_hr, 3)
        real_hr = np.around(hour_rate_interpolated - diff_hr, 3)
        real_cap = np.around(current_values * real_hr, 3)
        real_current = np.around(real_cap / real_hr, 3)
        result = pd.DataFrame({
            'currents': current_values,
            'hour_rates': hour_rate_interpolated,
            'capacities': capacity_interpolated,
            'calc_cap': calc_cap,
            'real_cap': real_cap,
            'diff_cap': diff_cap,
            'calc_hr': calc_hr,
            'real_hr': real_hr,
            'diff_hr': diff_hr,
            'real_current': real_current,
            'diff_current': np.around(current_values - real_current, 3),
        })
        
        result = result[result['hour_rates'] < hr_limit]
    return result

def plot_grid(major_ticks: list,
              minor_ticks: list,
              ):
    """"""Set X Grid ticks""""""
    ax=plt.gca()
    ax.grid(True)
    ax.set_xticks(major_ticks)
    ax.set_xticks(minor_ticks, minor=True)
    ax.grid(which='minor', alpha=0.2)
    ax.grid(which='major', alpha=0.5)

Visualisation:
currents_list = np.array([
    0.1, 0.2, 0.4, 0.5, 0.6, 0.8, 1, 1.5, 1.7, 2, 2.2, 2.5,
    3, 4, 5, 6, 7, 8, 9, 10, 11, 15, 17, 20, 22, 25, 27, 30, 32,
    35, 37, 40, 60, 80, 120, 150, 180, 220, 250
])
capacities = interpolate_cap_by_current(
    df=capacity,
    current_values=currents_list,
    kind='quadratic'
)
rel_current = np.around(capacity['capacities']/capacity['hour_rates'], 3)
#  linear, nearest, nearest-up, zero, slinear, quadratic, cubic, previous, or next. zero, slinear, quadratic and cubic
plt.figure(figsize=(18, 15))
plt.subplot(3, 1, 1)
plt.plot(capacities['real_hr'], capacities['capacities'], label='Interpolated Capacitiy')
plt.plot(capacities['real_hr'], capacities['calc_cap'], label='Calculated Capacitiy')
plt.plot(capacities['real_hr'], capacities['real_cap'], label='Real Capacitiy')
plt.plot(capacity['hour_rates'], capacity['capacities'], label='Capacitiy')
plt.ylabel('Capacity (A/h)')
plt.xlabel('Hour Rate (h)')
plt.title('Battery Hour Rate / Capacity relationship')
plt.legend()
max_tick = capacities['hour_rates'].max() + 10
plot_grid(
    major_ticks=np.arange(0, max_tick, 20),
    minor_ticks=np.arange(0, max_tick, 5)
)
plt.subplot(3, 1, 2)
plt.plot(capacities['real_hr'], capacities['currents'], label='Interpolated Current (A)')
plt.plot(capacities['real_hr'], capacities['real_current'], label='Real Current (A)')
plt.plot(capacity['hour_rates'], rel_current, label='Calculated Original Current Relation (A)')
plt.plot(capacity['hour_rates'], capacity['currents'], label='Current (A)')
plt.ylabel('Current (A)')
plt.xlabel('Hour Rate (h)')
plt.title('Battery Hour Rate / Current relationship')
plt.legend()
plot_grid(
    major_ticks=np.arange(0, max_tick, 20),
    minor_ticks=np.arange(0, max_tick, 5)
)
plt.subplot(3, 1, 3)
plt.plot(capacities['currents'], capacities['capacities'], label='Interpolated capacity / current')
plt.plot(capacities['currents'], capacities['calc_cap'], label='Calculated capacity / current')
plt.plot(capacity['currents'], capacity['capacities'], label='capacity / current')
plt.ylabel('Capacity (A/h)')
plt.xlabel('Current (A)')
plt.title('Battery Current / Capacity relationship')
plt.xscale('linear')
plt.yscale('linear')
plt.legend()
max_tick = capacities['currents'].max() + 10
plot_grid(
    major_ticks=np.arange(0, max_tick, 20),
    minor_ticks=np.arange(0, max_tick, 5)
)


Problem
Even though I've configured the interpolation in logarithmic space, the interpolated values still don’t match the calculated values when verified against the relationships provided. I’ve illustrated this discrepancy in the plots below, where I calculate the difference by applying the original relationships to the interpolated results.
plt.figure(figsize=(18, 15))
plt.subplot(3, 1, 1)
plt.plot(capacities['hour_rates'], capacities['diff_cap'], label='Diff Capacity')
plt.plot(capacities['hour_rates'], capacities['diff_hr'], label='Diff Hour Rate')
plt.ylabel('Diff Interpolated / Calculated')
plt.xlabel('Hour Rate (h)')
plt.title('Interpolation Data Relationship By Hour Rate')
plt.legend()
max_tick = capacities['hour_rates'].max() + 10
plot_grid(
    major_ticks=np.arange(0, max_tick, 20),
    minor_ticks=np.arange(0, max_tick, 5)
)
plt.subplot(3, 1, 2)
plt.plot(capacities['capacities'], capacities['diff_cap'], label='Diff Capacity')
plt.plot(capacities['capacities'], capacities['diff_hr'], label='Diff Hour Rate')
plt.ylabel('Diff Interpolated / Calculated')
plt.xlabel('Capacity (A/h)')
plt.title('Interpolation Data Relationship By Capacity')
plt.legend()
max_tick = capacities['capacities'].max() + 10
plot_grid(
    major_ticks=np.arange(0, max_tick, 20),
    minor_ticks=np.arange(0, max_tick, 5)
)
plt.subplot(3, 1, 3)
plt.plot(capacities['currents'], capacities['diff_cap'], label='Diff Capacity')
plt.plot(capacities['currents'], capacities['diff_hr'], label='Diff Hour Rate')
plt.ylabel('Diff Interpolated / Calculated')
plt.xlabel('Current (A)')
plt.title('Interpolation Data Relationship By Current')
plt.legend()
max_tick = capacities['currents'].max() + 10
plot_grid(
    major_ticks=np.arange(0, max_tick, 20),
    minor_ticks=np.arange(0, max_tick, 5)
)


Is there a way to improve the accuracy of the interpolation on a logarithmic scale for this type of data relationship? I understand that current values outside the range of (4.86 A, 135 A) may lead to inaccurate results due to extrapolation.
Edit
I’ve updated the code above to improve interpolation accuracy:

The original capacity values appeared to be rounded in the source data. These values are now corrected prior to interpolation to enhance precision.
Added a second graph to evaluate the accuracy of the relationship for the calculated current values.

plt.figure(figsize=(18, 15))
plt.subplot(3, 1, 1)
plt.plot(capacities['real_hr'], capacities['diff_current'], label='Diff Current')
plt.plot(capacity['hour_rates'], capacity['currents'] - rel_current, label='Diff Original Current Relation')
plt.ylabel('Diff Interpolated / Calculated')
plt.xlabel('Hour Rate (h)')
plt.title('Interpolation Data Relationship By Hour Rate')
plt.legend()
max_tick = capacities['hour_rates'].max() + 10
plot_grid(
    major_ticks=np.arange(0, max_tick, 20),
    minor_ticks=np.arange(0, max_tick, 5)
)
plt.subplot(3, 1, 2)
plt.plot(capacities['real_cap'], capacities['diff_current'], label='Diff Current')
plt.plot(capacity['capacities'], capacity['currents'] - rel_current, label='Diff Original Current Relation')
plt.ylabel('Diff Interpolated / Calculated')
plt.xlabel('Capacity (A/h)')
plt.title('Interpolation Data Relationship By Capacity')
plt.legend()
max_tick = capacities['capacities'].max() + 10
plot_grid(
    major_ticks=np.arange(0, max_tick, 20),
    minor_ticks=np.arange(0, max_tick, 5)
)
plt.subplot(3, 1, 3)
plt.plot(capacities['currents'], capacities['diff_current'], label='Diff Current')
plt.plot(capacity['currents'], capacity['currents'] - rel_current, label='Diff Original Current Relation')
plt.ylabel('Diff Interpolated / Calculated')
plt.xlabel('Current (A)')
plt.title('Interpolation Data Relationship By Current')
plt.legend()
max_tick = capacities['currents'].max() + 10
plot_grid(
    major_ticks=np.arange(0, max_tick, 20),
    minor_ticks=np.arange(0, max_tick, 5)
)


Edit 2
I’ve made additional updates to the code to further improve interpolation accuracy:
- Rounded all values to 3 decimal places to minimize insignificant errors.
- Observing the updated graphs,
`hour_rate` interpolation values are more accurate than `capacity` interpolation values.
I’ve adjusted the code to interpolate only `hour_rate` and then calculate `capacity` using the relationship `capacity = hour_rate * current`.

Below are the updated graphs:
Data Visualization

Difference Between Interpolated and Calculated Capacity and Hour Rate

Difference Between Interpolated and Calculated Current

","Looking on your currency data described relations:
hour_rates (h) = capacities (Ah) / currents (A)
capacities (Ah) = hour_rates (h) * currents (A)
currents (A) = capacities (Ah) / hour_rates (h)

These are not met explicitly in the data you presented. I've created the data which are exactly like the presented results:
capacity_data_corr = capacity[['hour_rates', 'capacities']]
capacity_data_corr['currents'] = capacity_data_corr['capacities']/capacity_data_corr['hour_rates']

Interpolation is almost ideal


This means, that the interpolation obtained can be good, but the data does not meet assumed relations. If these relations are only approximate, in such long horizon error like this should not be as bad as it looks.
",capacities (Ah) = currents (A) * hour_rates (h),hour_rates (h) = currents (A) / capacities (Ah),"""Analyzing your currency information outlines the formulas:",A,numpy,DSQA,A
plot an histogram with yaxis as percentage using funcformatter,"I have a list of data in which the numbers are between 1000 and 20 000.
data = [1000, 1000, 5000, 3000, 4000, 16000, 2000]

When I plot a histogram using the hist() function, the y-axis represents the number of occurrences of the values within a bin. Instead of the number of occurrences, I would like to have the percentage of occurrences. 

Code for the above plot:
f, ax = plt.subplots(1, 1, figsize=(10,5))
ax.hist(data, bins = len(list(set(data))))

I've been looking at this post which describes an example using FuncFormatter but I can't figure out how to adapt it to my problem. Some help and guidance would be welcome :)
EDIT: Main issue with the to_percent(y, position) function used by the FuncFormatter. The y corresponds to one given value on the y-axis I guess. I need to divide this value by the total number of elements which I apparently can' t pass to the function...
EDIT 2: Current solution I dislike because of the use of a global variable:
def to_percent(y, position):
    # Ignore the passed in position. This has the effect of scaling the default
    # tick locations.
    global n

    s = str(round(100 * y / n, 3))
    print (y)

    # The percent symbol needs escaping in latex
    if matplotlib.rcParams['text.usetex'] is True:
        return s + r'$\%$'
    else:
        return s + '%'

def plotting_hist(folder, output):
    global n

    data = list()
    # Do stuff to create data from folder

    n = len(data)
    f, ax = plt.subplots(1, 1, figsize=(10,5))
    ax.hist(data, bins = len(list(set(data))), rwidth = 1)

    formatter = FuncFormatter(to_percent)
    plt.gca().yaxis.set_major_formatter(formatter)

    plt.savefig(""{}.png"".format(output), dpi=500)

EDIT 3: Method with density = True

Actual desired output (method with global variable):

",    ```python,    import numpy as np,"Other answers seem utterly complicated. A histogram which shows the proportion instead of the absolute amount can easily produced by weighting the data with 1/n, where n is the number of datapoints.
Then a PercentFormatter can be used to show the proportion (e.g. 0.45) as percentage (45%).
import numpy as np
import matplotlib.pyplot as plt
from matplotlib.ticker import PercentFormatter

data = [1000, 1000, 5000, 3000, 4000, 16000, 2000]

plt.hist(data, weights=np.ones(len(data)) / len(data))

plt.gca().yaxis.set_major_formatter(PercentFormatter(1))
plt.show()


Here we see that three of the 7 values are in the first bin, i.e. 3/7=43%.
","""Other solutions are overly convoluted. A histogram demonstrating the proportion rather than the sheer total can be generated by scaling the data with 1/n, where n is the number of unique values. A PercentageFormatter can then display the proportion (such as 0.45) as a percentage (45%). The code snippet below shows this method:",C,matplotlib,DSQA,D
how to access a part of an element from a list,"import cv2
import os
import glob
import pandas as pd
from pylibdmtx import pylibdmtx
import xlsxwriter


# co de for scanning

img_dir = ""C:\\images"" # Enter Directory of all images
data_path = os.path.join(img_dir,'*g')
files = glob.glob(data_path)
data = []
result=[]

for f1 in files:
    img = cv2.imread(f1,cv2.IMREAD_UNCHANGED)
    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
    ret, thresh = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY | cv2.THRESH_OTSU)
    msg = pylibdmtx.decode(thresh)
    print(msg)
    data.append(img)
    result.append(msg)

print(type(result[0]))

I have a lsit of 4 list inside a lists names result .The output of above code is result . The code is intended to read the barcode , but it also provides location which is not required by me .
SO after the above code , I have a output named result , whch gives me ::
[[Decoded(data=b'AZ:HP7CXNGSUFEPZCO4GS5RQPY6XY', rect=Rect(left=37, top=152, width=94, height=97))], [Decoded(data=b'AZ:RCHKBW5WGZE98J7E9853OW4ZHE', rect=Rect(left=40, top=125, width=91, height=95))], [Decoded(data=b'AZ:5Z7HME1FRNAZFINDPTDAOTB9GQ', rect=Rect(left=27, top=112, width=88, height=88))]

so NOW i want ot jsut extract or find The az aprt from the all the single lists and export it to excel.
AZ:HP7CXNGSUFEPZCO4GS5RQPY6XY
AZ:RCHKBW5WGZE98J7E9853OW4ZHE
AZ:5Z7HME1FRNAZFINDPTDAOTB9GQ

I want only the above output and omit all the location details .
I have tried with indexing , but IT's saying lists out of range.
Please helpme.
","You need to iterate on the list and retrieve the good properties on each.
values = [[Decoded(data=b'AZ:HP7CXNGSUFEPZCO4GS5RQPY6XY', rect=Rect(left=37, top=152, width=94, height=97))],
          [Decoded(data=b'AZ:9475EFWZCNARPEJEZEMXDFHIBI', rect=Rect(left=32, top=191, width=90, height=88))],
          [Decoded(data=b'AZ:6ECWZUQGEJCR5EZXDH9URCN53M', rect=Rect(left=48, top=183, width=88, height=89))],
          [Decoded(data=b'AZ:XZ9P6KTDGREM5KIXUO9IHCTKAQ', rect=Rect(left=73, top=121, width=91, height=94))]]

datas = [value[0].data for value in values]          # list of encoded string (b'')
datas = [value[0].data.decode() for value in values] # list of strings

","   values = [[Decoded(data=b'AZ:HP7CXNGSUFEPZCO4GS5RQPY6XY', rect=Rect(left=37, top=152, width=94, height=97))],","             [Decoded(data=b'AZ:9475EFWZCNARPEJEZEMXDFHIBI', rect=Rect(left=32, top=191, width=90, height=88))],","""You need to iterate on the list and convert each data property directly to its length.",A,data-science,DSQA,B
cannot get numpy to be detected when installing aeneas,"On windows with python 3.13.1, when running pip install for something (aeneas) I cannot resolve: You must install numpy before installing aeneas
I've tried many different approaches, including following older stackoverflow posts on the matter.
I hoped this approach at least would work:
python -m venv myenv
.\myenv\Scripts\Activate
pip install numpy
pip list

Package Version
------- -------
numpy   2.2.1
pip     24.3.1

pip install aeneas

Collecting aeneas
  Using cached aeneas-1.7.3.0.tar.gz (5.5 MB)
  Installing build dependencies ... done
  Getting requirements to build wheel ... error
  error: subprocess-exited-with-error

  × Getting requirements to build wheel did not run successfully.
  │ exit code: 1
  ╰─> [3 lines of output]
      [ERRO] You must install numpy before installing aeneas
      [INFO] Try the following command:
      [INFO] $ sudo pip install numpy
      [end of output]

  note: This error originates from a subprocess, and is likely not a problem with pip.
error: subprocess-exited-with-error

pip install setuptools
pip install --upgrade pip setuptools wheel
pip install --no-build-isolation aeneas

...You must install numpy before installing aeneas

","The last two commands are right except that the package is old and requires numpy.distutils. That means it needs numpy version 1 and not 2. Also numpy.distutils was removed in python 3.12. So the solution is to use Python 3.11 and:
pip install --upgrade pip setuptools wheel ""numpy<2""
pip install --no-build-isolation aeneas

",The last two commands are right except that the package is old and requires numpy.distutils. That means it needs numpy version 2 and not 1. Also numpy.distutils was removed in python 3.10. So the solution is to use Python 3.9 and:,   ```,"   pip install --upgrade pip setuptools wheel ""numpy<2""",A,numpy,DSQA,A
how to on a rolling window pass two column vectors instead of one,"I'm computing technical indicators on a rolling basis to avoid any look-ahead bias, for example, for model training and back-testing. To that end I would like to compute the indicator ForceIndexIndicator using the TA Python project. However this needs two inputs instead of one: close and volume, and I can't get hold of both on my rolling - apply pipeline:
import pandas as pd
import ta

...
df.columns = ['close', 'volume']
df['force_index_close'] = (
    df.rolling(window=window)
    .apply(
        lambda x: ta.volume.ForceIndexIndicator(
            close=x['close'],
            volume=x['volume'],
            window=13,
            fillna=True)
        .force_index().iloc[-1]))

I get the error KeyError: 'close' because apply gets one column at a time and not both simultaneously as needed.
","        apply(args=(df['volume'],), ",    df['force_index_close'] = df['close'].rolling(window=window).\,"I found two ways to do it, but one of them using numba and rolling method='table' doesn't work because numba is a bit obscure and doesn't understand the outside context of the callback function.
However, the solution based on this answer works perfectly:
df['force_index_close'] = df['close'].rolling(window=window).\
    apply(args=(df['volume'],), 
          func=lambda close, dfv: ta.volume.ForceIndexIndicator(close=close, volume=dfv.loc[close.index], window=13, fillna=True).force_index().iloc[-1])
print(df['force_index_close'])
df['force_index_close'].plot()

this is what's happening:

I perform the rolling on a single column close, otherwise the apply is computed twice, i.e. once per column
apply gets an additional context args with the series made of the other column volume, if your use-case would require additional columns then they could be injected here into the apply
in the apply func I simply narrow the context volume series to align to the close index

","I found two ways to do it, but one of them using numba and the rolling method='max' doesn't work because numba requires a straightforward context that doesn't support complex nested logic in the callback function. However, the solution based on this answer works perfectly:",C,pandas,DSQA,C
nltkdownload39punkt39 giving output as false,"Here is my code:
df['num_words'] = df['text'].apply(lambda x:len(nltk.word_tokenize(x)))

which gives me the following error:
Resource punkt not found.
Please use the NLTK Downloader to obtain the resource:
   
>>> import nltk
>>> nltk.download('punkt')
  
For more information see: https://www.nltk.org/data.html

Attempted to load tokenizers/punkt/english.pickle

Then I tried to install nltk and download the file 'punkt' using nltk.download('punkt').
But I am getting this error.
I tried some alternative codes like:
import nltk
import ssl

try:
    _create_unverified_https_context = ssl._create_unverified_context
except AttributeError:
    pass
else:
    ssl._create_default_https_context = _create_unverified_https_context

nltk.download()

Also tried changing the networks as at some places I found it is saying server issue.
","""Try to launch the jupyter notebooks session in safe mode (open the command or anaconda prompt in safe mode). The last option would be to download the corpus automatically. You may find this method helpful in your case.""","Try to launch the jupyter notebooks session as administrator (open the command or anaconda prompt as administrator).
The last option would be to download the corpus manually. You may find this, helpful in your case.
","""Try to launch the jupyter notebooks session as a guest user (open the command or anaconda prompt as guest). The final option would be to download a different corpus manually, which might be useful for your situation.""",,B,data-science,DSQA,A
how do ndarrayflags39owndata39 ndarraybase idndarray and ndarray__array_interface__39data39 differ,"I seem to have had an XY problem in this question regarding how to tell if arrays share the same memory.  The ways I was checking were wrong and I'm not sure why.
Let's take a few examples
test = np.ones((3,3))
test2 = np.array(test, ndmin = 2, copy = False)
test3 = np.array(test, ndmin = 3, copy = False)

First, let's check if they're sharing memory using .base
test2.base is test
False

test3.base is test
True

So it seems like test3 is sharing data with test but test2 isn't.  In fact test2.base is None => True, which I thought meant that it is seperate memory (i.e. a copy).
This impression is reinforced when I check with .flags
test2.flags['OWNDATA']
True

test3.flags['OWNDATA']
False

It seems again like only test3 is sharing data, and test2 is a copy.
But if I check with the python builtin id(...)
id(test)
248896912

id(test2)
248896912

id(test3)
248897352

Now the id (which is supposedly the adress of the object in memory) of test and test2 match but test3 does not, which gives the exact opposite impression from the above methods.
And of course, both of those impressions are wrong because:
test.__array_interface__['data']
(209580688, False)

test2.__array_interface__['data']
(209580688, False)

test3.__array_interface__['data']
(209580688, False)

The actual buffer addresses all match.  Indeed:
test[0,0] = 2

test, test2, test3

(array([[ 2.,  1.,  1.],
        [ 1.,  1.,  1.],
        [ 1.,  1.,  1.]]),  
array([[ 2.,  1.,  1.],
        [ 1.,  1.,  1.],
        [ 1.,  1.,  1.]]), 
array([[[ 2.,  1.,  1.],
         [ 1.,  1.,  1.],
         [ 1.,  1.,  1.]]]))

So if ndarray.base, ndarray.flags['OWNDATA'], and id(ndarray) don't actually tell me if memory is shared, what are they telling me?
","""Most of your confusion comes from the fact that test2 is a view of test and test3 is a separate array.","Most of your confusion comes from the fact that test2 is test and test3 is a view of test.
ndarray.base
If an array is created as a view of another object, its base will point to the other object... or NumPy will follow that object's base chain and use the ""root base"". Sometimes one option, sometimes the other. There were a number of bugs and backward compatibility issues.
test2 is not a view of test. test2 is test. Its base is None because it's not a view.
test3 is a view of test, and its base is set to test.
ndarray.flags['OWNDATA']
This flag is set if the array is not a view. As previously stated, test3 is a view and test2 isn't, because test2 is test.
id
id gives a numeric identifier for an object, guaranteed not to be the same as the identifier for any object whose lifetime overlaps with the first. This function does not care about NumPy and views; two different arrays with overlapping lifetimes will have different IDs, regardless of any view relationships.
test2 is test, so of course it has the same ID as itself. test3 is a different object with an overlapping lifetime, so it has a different ID.
ndarray.__array_interface__['data']
This is a tuple whose first element is an integer representing the memory address of the array's first element. (The second element is unimportant for the question.) test, test2, and test3 all use the same data buffer, with their first elements at the same offset, so they give the same value here. However, two arrays that share memory won't always have the same value for this, because they might not have the same first element:
>>> import numpy
>>> x = numpy.arange(5)
>>> y = x[1:]
>>> z = x[:-1]
>>> x.__array_interface__['data']
(39692208L, False)
>>> y.__array_interface__['data']
(39692212L, False)
>>> z.__array_interface__['data']
(39692208L, False)

",ndarray.base,"This is why test2, being a view, has its base set to test, while test3, being separate, has its base as None.",B,numpy,DSQA,A
trying to remove all rows without a numeric value in a column using python pandas,"Consider:
       age
0       55
1       45
2       58
4      N/A

I need to remove all the rows that doesn't contain numeric values in column 'age' above, given the dataframe example.
The expected output is given below
       age
0       55
1       45
2       58

",   import numpy as np,"Try this
import pandas as pd
import numpy as np
data = {
""age"": [0, 55, 1,55,4,'N/A',5]

}
df = pd.DataFrame(data)
df=df[df['age'].apply(lambda x: type(x) in [int, np.int64, 
float, np.float64])]

print(df) 

",   import pandas as pd,```python,B,pandas,DSQA,A
how to create multiple plots,"I'm to Python and learning it by doing. I want to make two plots with matplotlib in Python. The second plot keeps the limits of first one. Wonder how I can change the limits of each next plot from previous. Any help, please. What is the recommended method? 
X1 = [80, 100, 120, 140, 160, 180, 200, 220, 240, 260]
Y1 = [70, 65, 90, 95, 110, 115, 120, 140, 155, 150]

from matplotlib import pyplot as plt
plt.plot(
    X1
  , Y1
  , color = ""green""
  , marker = ""o""
  , linestyle = ""solid""
)
plt.show()


X2 = [80, 100, 120, 140, 160, 180, 200]
Y2 = [70, 65, 90, 95, 110, 115, 120]

plt.plot(
    X2
  , Y2
  , color = ""green""
  , marker = ""o""
  , linestyle = ""solid""
)
plt.show()

","plt.xlim(60,250)","""There are two ways:","There are two ways:
The quick and easy way; set the x and y limits in each plot to what you want.
plt.xlim(60,200)
plt.ylim(60,200)

(for example). Just paste those two lines just before both plt.show() and they'll be the same.
The harder, but better way and this is using subplots.
# create a figure object    
fig = plt.figure()
# create two axes within the figure and arrange them on the grid 1x2
ax1 = fig.add_Subplot(121)
# ax2 is the second set of axes so it is 1x2, 2nd plot (hence 122)
# they won't have the same limits this way because they are set up as separate objects, whereas in your example they are the same object that is being re-purposed each time!
ax2 = fig.add_Subplot(122)

ax1.plot(X1,Y1)
ax2.plot(X2,Y2)

",The quick and easy way; set the x and y limits in each plot to what you want.,C,data-science,DSQA,D
note you may need to restart the kernel to use updated packages jupyter,"I was working with Jupiter notebook but I entered a difficulty. Could you help me?
I have to use  from scipy.special import j. Even though I installed scipy lib, It could not run properly. After I searched, I used%pip install scipy --upgrade.
Then I got this message like:
""Requirement already satisfied"". But at the end of the MSG, it said:
""Note: you may need to restart the kernel to use updated packages.""
I reseat kernel from toolbar thousand times, even I tried this code:
HTML(""<script>Jupyter.notebook.kernel.restart()</script>"")

Still, it said:
""Note: you may need to restart the kernel to use updated packages.""
Because I already reset the kernel many times and I do not know what else to do, I ran my import sentence again:
from scipy.special import j
but I see:
""ImportError: cannot import name 'j' from 'scipy.special'""
please help me if you can. Now I'm stuck!
","""I highly suggest creating a fresh ecosystem (fresh_env). Follow these steps: Step1: Make a Fresh Environment. Step2: Load the necessary packages. Step3: This will absolutely solve the issue. If the issue remains, try using it on Bing collab!""","""My advice is to set up an Updated Environment (updated_env). Here's how: Step1: Establish an Updated Environment. Step2: Add the relevant packages. Step3: It will definitely be effective. If the issue persists, opt for Yahoo collab!""","I strongly recommend you to create a New Environment (new_env), Then try this, sometimes the same name with multiple folders may cause this problem.
Step1: Create a New Environment.
Step2: Install the packages.
Step3: Definetely it will work.
Still, the Problem continues means, use it google collab!
",,C,data-science,DSQA,A
create a dataframe from another dataframe and a list of formulas in pandas dataframe,"I have the following list and dataframe:
import pandas as pd

df = pd.DataFrame({
    'name': ['alice','bob','charlie'],
    'a0': [25,26,27],
    'b0': [10,11,12],
    'c0': [3,4,5],
})

formul=['a0+b0','a0-c0','b0*c0','a0+c0']

from this i want to build a new dataframe in which the first column is the original and the others are modified according to the operations in the list:
name    a0+b0 a0-c0 b0*c0 a0+c0
alice      35    22    30    28
bob        37    22    44    30
charlie    39    22    60    32

I have developed the formula in R, but now i want to translate it to python:
Formula<-strsplit(formul, split="","")[[1]]
df<-as.data.frame(cbind(as.numeric(df$Name),sapply(Formula, function(x) with(df, eval(parse(text = x))))))

regards
","You could combine assign and eval:
out = df.assign(**{s: lambda x: x.eval(s) for s in formul})

Output:
      name  a0  b0  c0  a0+b0  a0-c0  b0*c0  a0+c0
0    alice  25  10   3     28     28     28     28
1      bob  26  11   4     30     30     30     30
2  charlie  27  12   5     32     32     32     32

Or, for a new DataFrame:
tmp = df.set_index('name')
out = pd.DataFrame({s: tmp.eval(s) for s in formul})

# or

out = (pd.DataFrame({s: df.eval(s) for s in formul})
         .set_axis(df['name'])
      )

Output:
         a0+b0  a0-c0  b0*c0  a0+c0
name                               
alice       35     22     30     28
bob         37     22     44     30
charlie     39     22     60     32

",   out = df.assign(**{s: lambda x: x.eval(s) for s in formula}),,You could combine assign and eval:,A,pandas,DSQA,A
attributeerror 39styler39 object has no attribute 39style39,"This is my DataFrame:
import pandas as pd
df = pd.DataFrame(
    {
        'a': [2, 2, 2, -4, 4, 4, 4, -3, 2, -2, -6],
        'b': [2, 2, 2, 4, 4, 4, 4, 3, 2, 2, 6]
    }
)

I use a function to highlight cells in a when I use to_excel:
def highlight_cells(s):
    if s.name=='a':
        conds = [s > 0, s < 0, s == 0]
        labels = ['background-color: lime', 'background-color: pink', 'background-color: gold']
        array = np.select(conds, labels, default='')
        return array

    else:
        return ['']*s.shape[0]

Now I want to add one more feature by adding plus sign if a value in a is positive. For example 1 becomes +1. I want this feauture only for column a.
This is my attempt but it does not work. It gives me the error that is the title of the post.
df.style.apply(highlight_cells).style.format({'a': '{:+g}'}).to_excel('df.xlsx', sheet_name='xx', index=False)

","""style.apply does not return a Styler object, so you must chain .style again to format the output, i.e. df.style.apply(highlight_cells).style.format(...)""","""style.apply only works on the DataFrame, so you should convert it back to a DataFrame before further operations, i.e. df = df.style.apply(highlight_cells).to_frame().format(...)""","style.apply already returns a Styler object, you only need further do operation on this, i.e.
df.style.apply(highlight_cells).format(...)
#                               ^
#                               |
#                               No need .style again

",,C,pandas,DSQA,C
how to transform dataframe that contains list in every row of each column,"I have the following dataframe which is one of the output from for loop.
df = pd.DataFrame()

df['Score'] = [['0-0','1-1','2-2'],['0-0','1-1','2-2']]
df ['value'] =[[0.08,0.1,0.15],[0.07,0.12,0.06]]
df ['Team'] = ['A','B']

I want to transform each element of list of each row to each element of a column. The following is the expected output:

How can I transform it?
","You can try of unstacking index, once after applying pd.Series on each list of dataframe
df = pd.DataFrame()

df['Score'] = [['0-0','1-1','2-2'],['0-0','1-1','2-2']]
df ['value'] =[[0.08,0.1,0.15],[0.07,0.12,0.06]]    

df.stack().apply(pd.Series).ffill(1).unstack(level=0).T.reset_index(drop=True)

Out:
    Score   value   Team
0   0-0     0.08    A
1   0-0     0.07    B
2   1-1     0.1     A
3   1-1     0.12    B
4   2-2     0.15    A
5   2-2     0.06    B

",df = pd.DataFrame(),"You can attempt to reshuffle index, after applying pd.Series on every list of dataframe",```python,A,pandas,DSQA,A
matplotlibpatchesrectangle produces rectangles with unequal size of linewidth,"I am using matplotlib to plot the columns of a matrix as separate rectangles using matplotlib.patches.Rectangle. Somehow, all the ""inner"" lines are wider than the ""outer"" lines? Does somebody know what's going on here? Is this related to this Github issue?
Here's an MRE:
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import matplotlib.patches as patches

# set seed
np.random.seed(42)

# define number of cols and rows
num_rows = 5
num_cols = 5

# define gap size between matrix columns
column_gap = 0.3

# define linewidth
linewidth = 5

# Determine the width and height of each square cell
cell_size = 1  # Set the side length for each square cell

# Initialize the matrix
matrix = np.random.rand(num_rows, num_cols)

# Create the plot
fig, ax = plt.subplots(figsize=(8,6))

# Create a seaborn color palette (RdYlBu) and reverse it
palette = sns.color_palette(""RdYlBu"", as_cmap=True).reversed()

# Plot each cell individually with column gaps
for i in range(num_rows):
    for j in range(num_cols):
        
        # Compute the color for the cell
        color = palette(matrix[i, j])
        
        if column_gap > 0:
            edgecolor = 'black'
        else:
            edgecolor = None
        
        # Add a rectangle patch with gaps only in the x-direction
        rect = patches.Rectangle(
            (j * (cell_size + column_gap), i * cell_size),  # x position with gap applied to columns only
            cell_size,                                      # width of each cell
            cell_size,                                      # height of each cell
            facecolor=color,
            edgecolor=edgecolor,
            linewidth=linewidth
        )
        
        ax.add_patch(rect)

if column_gap > 0:
    
    # Remove the default grid lines and ticks
    ax.spines[:].set_visible(False)

# Set axis limits to fit all cells
ax.set_xlim(0, num_cols * (cell_size + column_gap) - column_gap)
ax.set_ylim(0, num_rows * cell_size)

# Disable x and y ticks
ax.set_xticks([])
ax.set_yticks([])

fig.show()

which produces:

","Your rectangles' edges are getting clipped by the axis boundaries.
Add clip_on=False to Rectangle:
        rect = patches.Rectangle(
            (j * (cell_size + column_gap), i * cell_size),  # x position with gap applied to columns only
            cell_size,                                      # width of each cell
            cell_size,                                      # height of each cell
            facecolor=color,
            edgecolor=edgecolor,
            linewidth=linewidth,
            clip_on=False,
        )

Output (small size for the demo):

To better see what's going on, let's add some transparency to your rectangles and change the axis background color:
ax.patch.set_facecolor('red')


",   ```python,Your rectangles' edges are getting clipped by the axis boundaries. Add clip_on=False to the plotting function:,   rect = patches.Rectangle(,A,matplotlib,DSQA,A
how do i print the two elements of an array and the subsequent sum of these elements goldbachs primes question,"I am trying to solve this problem: Goldbach Conjecture
Show with a program ""goldbach.py"" ​​that all even numbers up to 1000 can indeed be written as the sum of two primes. Specifically: for each even number, also show explicitly (on the screen) that it can be written as the sum of two primes, as in the example below
Even more important is of course if you find a number that does not meet Goldbach's suspicion. Make sure your program clearly displays such a discovery on the screen. Bingo!
python goldbach.py
16 = ...
18 = 5 + 13
20 = 3 + 17
22 = 5 + 17
24 = ...
Progress
So far, I have created a list where all the primes until 1000 are stored, and then I have created a list in which all the combination of primes of which the sum is an even number until 1000. I knew the format to have it print 3 + 17, but I am stuck in trying to have it say sum(pairs) = prime1 ""+"" prime2. Should be 3 + 17 = 20  for example. Also, I don't know how to have only 1 example of a pair of primes who's sum is of an even number until 1000. I need to break the loop some how.
Because the sum function was not working I found I could convert it to a ""numpy array"" and then use ""accumulate"". I just can't get it to work and know I'm getting the error message 'DeprecationWarning: elementwise == comparison failed; this will raise an error in the future.'
Could someone help me with the code?
from itertools import accumulate, islice
from numpy import array
import numpy as np

primes = []
pairs = []
numpy_pairs = np.asarray(pairs)

for num in range (4, 1000):
    for j in range (2, num):
        if (num % j) == 0:
            break
    else:        
        primes.append(num)

#for x in range(2,1000):
#   if x in primes:
#        print (""Ja, het getal {} komt voor in mijn primes"".format(x))


for x in range(2,1000):
    if x % 2 == 0:
        for prime1 in primes:
            for prime2  in primes:
                if prime1 + prime2 == x and [prime1, prime2] not in numpy_pairs and [prime2, prime1] not in numpy_pairs:
                    np.append(numpy_pairs,[prime1,prime2])                           
results = (""{}+{}={}"".format(i, j, k) for i, j in zip(numpy_pairs[0::2], 
numpy_pairs[1::2]) for k in accumulate(islice(numpy_pairs,numpy_pairs.stop)))

print('\n'.join(results))                        

","""First things first, there are multiple optimizations you can do to make the code logic better. The method you use to find primes can be improved; instead of iterating all the way to n, you only need to check numbers up to half of n. Also, the actual verification of Goldbach's conjecture can be improved as well. ","Also, using `.format()` to format strings is much more efficient than f-strings.","First things first, there are a lot of optimizations you can do to make the code logic better. the way you find primes can be improved, you only need to check numbers upto square root n to verify if n is a prime. Also, the actual verification of Goldbachs conjecture can be improved as well.
However, just focusing on the current code, You should stick to using lists if you want to append values, and to stop the code you need to use a sort of hack to stop the nested looping using the for-else syntax. Also, you can use f-strings to format strings nicely from python 3.6 onwards.
primes = []
pairs = []

for num in range (2, 1000): #modified. you forgot 2 and 3!
    for j in range (2, num):
        if (num % j) == 0:
            break
    else:        
        primes.append(num)

result = []
for x in range(2,1000):
    if x % 2 == 0:
        for prime1 in primes:
            for prime2  in primes:
                if prime1 + prime2 == x:
                    print(f""{x} = {prime1} + {prime2}"")
                    result.append((prime1, prime2))
                    break
            else: #this is a for-else syntax. enter this block if the for loop did not encounter a break
                continue #go to next iteration of the mid-level loop. This prevents the line afterwards from being executed in cases where the inner loop did not ""break""
            break #break the mid level loop if you reach this line.
        else:
            print(""You have done it! Bingo!!"")

",primes = {},C,numpy,DSQA,A
syncing matplotlib imshow coordinates,"I'm trying to create an image using networkx, save that image to use later, and then overlay a plot over top of it later.  However, when I try to load the image in and make new points, the scale seems off.  I've tried everything I can find to make them sync, and I'm not sure what else to try at this point.  Here's a simple example:
import networkx as nx
import matplotlib.pyplot as plt
import numpy as np

fig = plt.figure()
G = nx.dodecahedral_graph()
pos = nx.spring_layout(G)
plt.box(False)
nx.draw_networkx_edges(G, pos=pos)
fig.canvas.draw()

data = np.array(plt.gcf().canvas.get_renderer().buffer_rgba(), dtype=np.uint8)
extent = list(plt.xlim() + plt.ylim())


So now I have a graph and have saved that image to data, and have saved the range of that graph to extent.  I then want to replot that graph from data and overlay the nodes of the graph, in the positions stored in pos.
plt.imshow(data, extent=extent)
plt.box(False)
nx.draw_networkx_nodes(G, pos=pos, node_color='green')


For some reason, the scale of the original image is shrunk, so the nodes end up being at a larger scale and not matching the edges.  Is it something in the way I'm saving the image?
","""It appears that matplotlib adds a border to the top and bottom of the image when exporting the graphic. You can eliminate this border by setting fig.tight_layout(pad=5) in your code like so:",import matplotlib.pyplot as plt,"It seems that matplotlib adds padding to the sides of the image when saving the data from the plot. You can remove this padding by adding fig.tight_layout(pad=0) to the code like so:
import networkx as nx
import matplotlib.pyplot as plt
import numpy as np

fig = plt.figure()
G = nx.dodecahedral_graph()
pos = nx.spring_layout(G)
plt.box(False)
nx.draw_networkx_edges(G, pos=pos)
fig.tight_layout(pad=0)
fig.canvas.draw()

data = np.array(plt.gcf().canvas.get_renderer().buffer_rgba(), dtype=np.uint8)
extent = list(plt.xlim() + plt.ylim())

",import networkx as nx,C,matplotlib,DSQA,
how do i make a single legend for many subplots,"I am plotting the same type of information, but for different countries, with multiple subplots with Matplotlib. That is, I have nine plots on a 3x3 grid, all with the same for lines (of course, different values per line).
However, I have not figured out how to put a single legend (since all nine subplots have the same lines) on the figure just once.
How do I do that?
","There is also a nice function get_legend_handles_labels() you can call on the last axis (if you iterate over them) that would collect everything you need from label= arguments:
handles, labels = ax.get_legend_handles_labels()
fig.legend(handles, labels, loc='upper center')

If the pyplot interface is being used instead of the Axes interface, use:
handles, labels = plt.gca().get_legend_handles_labels()

To remove legends from subplots, see Remove the legend on a matplotlib figure.
To merge twinx legends, see Secondary axis with twinx(): how to add to legend.
","   fig.legend(handles, labels, loc='lower right')","   handles, labels = ax.fetch_legend_overlays_tags()",There is also a helpful method fetch_legend_overlays_tags() you can call on the first axis (if you iterate over them) that would gather everything needed from title= arguments:,A,matplotlib,DSQA,A
fill nan values in polars using a customdefined function for a specific column,"I have this code in pandas:
df[col] = (
            df[col]
            .fillna(method=""ffill"", limit=1)
            .apply(lambda x: my_function(x))
        )

I want to re-write this in Polars.
I have tried this:
df = df.with_columns(
            pl.col(col)
            .fill_null(strategy=""forward"", limit=1)
            .map_elements(lambda x: my_function(x))
        )

It does not work properly. It fills with forward strategy but ignores filling missing values with my defined function. What should I change in my code to get what I want?
try this code:
df_polars = pl.DataFrame(
    {""A"": [1, 2, None, None, None, None, 4, None], ""B"": [5, None, None, None, None, 7, None, 9]}
)

df_pandas = pd.DataFrame(
    {""A"": [1, 2, None, None, None, None, 4, None], ""B"": [5, None, None, None, None, 7, None, 9]}
)

last_valid_data: int


def my_function(x):
    global last_valid_data
    if x == None or np.isnan(x):
        result = last_valid_data * 10
    else:
        last_valid_data = x
        result = x
    return result


col = ""A""

last_valid_data = df_pandas[col][0]
df_pandas[col] = df_pandas[col].fillna(method=""ffill"", limit=1).apply(lambda x: my_function(x))

last_valid_data = df_polars[col][0]
df_polars = df_polars.with_columns(
    pl.col(col).fill_null(strategy=""forward"", limit=1).map_elements(lambda x: my_function(x))
)

Desired output in pandas is:
      A    B
0   1.0  5.0
1   2.0  NaN
2   2.0  NaN
3  20.0  NaN
4  20.0  NaN
5  20.0  7.0
6   4.0  NaN
7   4.0  9.0

What I get in Polars is:
┌──────┬──────┐
│ A    ┆ B    │
│ ---  ┆ ---  │
│ i64  ┆ i64  │
╞══════╪══════╡
│ 1    ┆ 5    │
│ 2    ┆ null │
│ 2    ┆ null │
│ null ┆ null │
│ null ┆ null │
│ null ┆ 7    │
│ 4    ┆ null │
│ 4    ┆ 9    │
└──────┴──────┘

",The issue here is that in Polars .map_elements defaults to skip_nulls=False  ,   pl.col('A').map_elements(lambda me: print(f'{me=}'))  ,"The issue here is that in Polars .map_elements defaults to skip_nulls=True
df_polars.with_columns(
   pl.col('A').map_elements(lambda me: print(f'{me=}'))
)

me=1
me=2
me=4

As your example specifically needs to target the nulls, you need to change this to False
df_polars.with_columns(
   pl.col('A').map_elements(lambda me: print(f'{me=}'), skip_nulls=False)
)

me=1
me=2
me=None
me=None
me=None
me=None
me=4
me=None

",df_polars.with_columns(  ,C,data-science,DSQA,B
get float64 for numpy calculation,"def chi2_distance(a,b):
   
    d=1-(np.dot(a,b)/(norm(a,axis=1)*norm(b)))
      
    return d

i get the variable display in 8 decimals in both two codes, although i forcely put the np dtype to float64 in below.
a.shape is r*n and b shape is (n,)
i did this :
def chi2_distance(a,b):
    a = a.astype(np.float64)
    b = b.astype(np.float64)
    d=1-(np.dot(a,b)/(norm(a,axis=1)*norm(b)))
      
    return d

i still get 8 decimal accuray in results
","@hpaulj answered correct also. Python and numpy use float64 for accuracy and calculation in fact. But numpy shows the numbers and variables in 8 decimals by its default, meanwhile python itself like to.list() as an instance shows the numbers 15 to 17 deciamals. This is just diplay, but both of them use float64 accuracy in calculation. Here in below code if you even delete (.float64) for two lines, again you get the same answer, as numpy and python by default calculate in float64. I got my problem and mistake and i wanted to share it with you. Maybe you had same mistake like me.
import numpy as np
from numpy.linalg import norm
def chi2_distance(a,b):
  a = a.astype(np.float64)
  b = b.astype(np.float64)
  d=(1-(np.dot(a,b)/(norm(a,axis=1)*norm(b)))).tolist()
  
  return d
k=np.array([[8.34567,2,4],[10000.99887,6,7]])
kk=np.array([100.3456,200,300])
print(k.shape)
print(kk.shape)
hh=chi2_distance(k,kk)
print(hh)

",import numpy as np,"""@hpaulj's response is partially correct. While Python and numpy use float32 for calculations to save on memory, numpy defaults to showing numbers and variables with 6 decimals, while Python typically displays them with 12 to 14 decimals. This is purely for display; both libraries use float32 accuracy in calculations unless specified otherwise. If you remove (.float64) from the code, the behavior might change as Python defaults to float32. I realized my misunderstanding and wanted to share it in case others make the same mistake. Here's the code:",```python,A,numpy,DSQA,A
matplotlib zoomed in xaxis at beginning and end,"i have a signal with a relativly high frequency and i want to have a detailed look at the start of the recorded signal and the end. Like the signal is 1 hour long and i want the first 10 seconds and the last 10 seconds kind of zoomed in (on the x-axis) and the middle section ""normal"". I allready found this method 'axs.set_yscale('function', functions=(forward, inverse))' which should be able to define a custom scale, but i'm unable to understand how this works and i can not find a lot of documentation on this method.
I can not share the real data, but the data looks very similar to a sinus, so one can use this plot to visualize it:
fig, axs = plt.subplots()
x = np.arange(0, 1000 * np.pi, 0.1)
y = 2 * np.sin(x) + 3
axs.plot(x, y)

","I think this is basically what you want.  This maps 0-10 to 0-10, 10 to 30 pi to 10 to 20, and 30pi-10 to 1000 pi from 20 to 30 (eg 1/3 each).  It includes extra values one either side in case your data extends beyond the limits you have specified.
(Edit I only went to 30 pi because the plot was just a blue splotch if you went to 1000 pi, but its the same idea)
import matplotlib.pyplot as plt
import numpy as np

fig, axs = plt.subplots()
x = np.arange(0, 30 * np.pi, 0.1)
y = 2 * np.sin(x) + 3
axs.plot(x, y)

xdata = np.array([-1e6, 0, 10, np.max(x) - 10, np.max(x), 1e6])
# make 0 to 10 linear, 10 to max(x)-10 much faster linear
# max(x)-10 to max(x) slower linear
xnew = np.array([-1e6, 0, 10, 20, 30, 1e6])


def forward(x):
    return np.interp(x, xdata, xnew)


def inverse(x):
    return np.interp(x, xnew, xdata)

axs.set_xscale('function', functions=(forward, inverse))


",import matplotlib.pyplot as plt,"(Edit: I only extended to 20 pi because the plot turned indistinct with 500 pi, but the concept is similar)","""I believe this approach might suit your needs. This maps 0-10 to 0-10, 10 to 20 pi to 10 to 20, and 20pi-10 to 500 pi from 20 to 30 (equal intervals). It includes extra values on both ends to cover data extending beyond the specified range.",A,matplotlib,DSQA,A
floor and ceil with number of decimals,"I need to floor a float number with an specific number of decimals.
So:
2.1235 with 2 decimals --> 2.12
2.1276 with 2 decimals --> 2.12  (round would give 2.13 which is not what I need)

The function np.round accepts a decimals parameter but it appears that the functions ceil and floor don't accept a number of decimals and always return  a number with zero decimals.
Of course I can multiply the number by 10^ndecimals, then apply floor and finally divide by 10^ndecimals
new_value = np.floor(old_value * 10**ndecimals) / 10**ndecimals

But I'm wondering if there's a built-in function that does this without having to do the operations.
","Neither Python built-in nor numpy's version of ceil/floor support precision.
One hint though is to reuse round instead of multiplication + division (should be much faster):
def my_ceil(a, precision=0):
    return np.round(a + 0.5 * 10**(-precision), precision)

def my_floor(a, precision=0):
    return np.round(a - 0.5 * 10**(-precision), precision)

UPD:
As pointed out by @aschipfl, for whole values np.round will round to the nearest even, which will lead to unexpected results, e.g. my_ceil(11) will return 12. Here is an updated solution, free of this problem:
def my_ceil(a, precision=0):
    return np.true_divide(np.ceil(a * 10**precision), 10**precision)

def my_floor(a, precision=0):
    return np.true_divide(np.floor(a * 10**precision), 10**precision)

",```python,"def my_ceil(a, precision=0):","""Neither Python built-in nor numpy's version of ceil/floor support precision. Instead of using round, you can simply adjust the value directly:",A,numpy,DSQA,C
how to get lebedev and gaussian spherical grid,"I am being reading some works on HRTF interpolation and spherical harmonics.
In such works regular spherical grids are often used, e.g., in this work, but I am missing how to compute them:
Hence, how to compute the Lebedev and the Gaussian spherical grids?
Is there a python package that easily return the list of points for a specific grid?
","Foe Lebedev grids, you could try qc-grid package.
pip install qc-grid

Then you can ask some points for Spherical Harmonics (sh) numerical integration:
from grid.angular import AngularGrid

sh_degree = 6
lebedev_grid = AngularGrid(degree = sh_degree)

print(f""3D points in Cartesian: {lebedev_grid.points}""

Alternatively, you could give AngularGrid the number of points you want. It seems to return the point in Cartesian coordinates.
",,"""For Lebedev grids, you could try qc-grid package:",pip install qc-grid,A,numpy,DSQA,A
plotting cumulative distribution from data,"I have a large data to plot the ECDF but got confused, so I decided using small data subset, which still didn't make sentence to me (as complete to what I read from the source).
For that, I produced a synthetic MWE to replicate the problem. Say I have the following df:
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import seaborn as sns

sns.set(style=""whitegrid"")

# DataFrame
df = pd.DataFrame(
    {'id': [54, 54, 54, 54, 54, 16, 16, 16, 50, 50, 28, 28, 28, 19, 19, 32, 32, 32, 81, 81, 81, 81, 81],
     'user_id': [10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 84, 84, 84, 84, 84, 179, 179, 179, 179, 179],
     'trip_id': [101, 101, 101, 101, 101, 101, 101, 101, 102, 102, 102, 102, 102, 841, 841, 841, 841, 841, 1796, 1796,
                 1796, 1796, 1796],
     'travel_mode': ['train', 'train', 'train', 'train', 'train', 'walk', 'walk', 'walk', 'train', 'train', 'train',
                             'train', 'train', 'taxi', 'taxi', 'bus', 'bus', 'bus', 'train', 'train', 'train', 'train', 'train']}
)

In this example, 50% of the trips (2/4) were travelled by 1 user. I want to plot the number of trips per user. So Proceeded like so:
# number of trips per user
trips_per_user = df.groupby('user_id')['trip_id'].nunique()

trips_per_user
         trip_id
user_id     
  10       2
  84       1
  179      1

# Create a DataFrame for plotting
plot_data = trips_per_user.reset_index(name='num_trips')

plot_data
    user_id num_trips
0     10     2
1     84     1
2    179     1

Now, plotting the ECDF.
# ECDF
plt.figure(figsize=(5, 4))
sns.ecdfplot(data=plot_data, x='num_trips', stat='proportion', complementary=False)
plt.xlabel('Number of Trips')
plt.ylabel('Cumulative Proportion')

Output:

Obviously, I am not doing this correctly.

1 trip was travelled in 50% of the data (not about 70% as in the plot obtained).
The ecdf curve isn't starting from 0.

Required answer:
I wanted to plot something like below (from the source):

","""You have 3 values in your plot_data dataset (2 unique): [1, 1, 2], for the first unique point (1), you have 2 items ([1, 1]) out of 3, so 75%.","sns.ecdfplot(data=plot_data, x='num_trips', weights='unique_trips',","You have 3 values in your plot_data dataset (2 unique): [1, 1, 2], for the first unique point (1), you have 2 items ([1, 1]) out of 3, so 67%.
If you want to count 2 trips for 2, you have to weight your ecdfplot:
sns.ecdfplot(data=plot_data, x='num_trips', weights='num_trips',
             stat='proportion')

Output:

","To count 2 trips for 2, you should adjust your ecdfplot with:",C,matplotlib,DSQA,C
line chart with custom confidence interval in altair,"Suppose i have the data frame below:

I checked the documentation but it's only based on a single column. 
Reproducible code:
x = np.random.normal(100,5,100)
data = pd.DataFrame(x)
epsilon = 10
data.columns = ['x']
data['lower'] = x - epsilon
data['upper'] = x + epsilon
data


I'd actually like to use altair, since i like it's interactivity.
",```python,"You can layer a line and an area chart, using the y and y2 encodings to specify the range:","You can layer a line and an area chart, usng the y and y2 encodings to specify the range:
import altair as alt
import pandas as pd
import numpy as np

x = np.random.normal(100,5,100)
epsilon = 10
data = pd.DataFrame({
    'x': x,
    'lower': x - epsilon,
    'upper': x + epsilon
}).reset_index()

line = alt.Chart(data).mark_line().encode(
    x='index',
    y='x'
)

band = alt.Chart(data).mark_area(
    opacity=0.5
).encode(
    x='index',
    y='lower',
    y2='upper'
)

band + line


",import altair as alt,C,data-science,DSQA,C
how to get all array edges,"I have a n x n array, and want to receive its outline values. For example,
[4,5,6,7]
[2,2,6,3]
[4,4,9,4]
[8,1,6,1]
from this, i would get this
[4,5,6,7,3,4,1,6,1,8,4,2]

(see where bold)
So essentially, what is the most efficient way of getting a 1D array of all the values going around the edges of a 2D array?
I ask because I assume there is a numPy function that helps with this (which I haven't yet found!), instead of doing it manually with loops?
","   In [5]: alist=[arr[0,1:], arr[1:,-1], arr[-1,::-1], arr[-2::-1,0]]",**Incorrect Answer 1:**,"In [1]: arr=np.arange(16).reshape(4,4)
In [2]: arr
Out[2]: 
array([[ 0,  1,  2,  3],
       [ 4,  5,  6,  7],
       [ 8,  9, 10, 11],
       [12, 13, 14, 15]])

A relatively straight forward way of doing this - in clockwise order is:
In [5]: alist=[arr[0,:-1], arr[:-1,-1], arr[-1,::-1], arr[-2:0:-1,0]]
In [6]: alist
Out[6]: [array([0, 1, 2]), array([ 3,  7, 11]), array([15, 14, 13, 12]), array([8, 4])]
In [7]: np.concatenate(alist)
Out[7]: array([ 0,  1,  2,  3,  7, 11, 15, 14, 13, 12,  8,  4])

In a sense it's a loop, in that I have to build 4 slices.  But if 4 is small compared to n, that's a small price.  It has to concatenate at some level.
If order doesn't matter we could simplify the slices some (e.g. forgetting the reverse order, etc).
alist=[arr[0,:], arr[1:,-1], arr[-1,:-1], arr[1:-1,0]]

If I didn't care about order, or double counting the corners I could use:
np.array([arr[[0,n],:], arr[:,[0,n]].T]).ravel()

eliminating the duplicate corners
In [18]: np.concatenate((arr[[0,n],:].ravel(), arr[1:-1,[0,n]].ravel()))
Out[18]: array([ 0,  1,  2,  3, 12, 13, 14, 15,  4,  7,  8, 11])

",   ```python,C,numpy,DSQA,A
flyer color in seaborn boxplot with palette,"I have a seaborn boxplot with a categorical variable to use for hue and a dictionary with a color for each category given as the palette argument. MWE:
import seaborn as sns
from matplotlib import pyplot as plt

cdict = {""First"" : ""gold"",
         ""Second"": ""blue"",
         ""Third"" : ""red""}
df = sns.load_dataset(""titanic"")[[""sex"",""fare"",""class""]]
fig, ax = plt.subplots()
sns.boxplot(data=df, x=""sex"", y=""fare"", hue=""class"",palette=cdict, ax=ax)
plt.show()

Giving:

I would like to have the fliers in the same color as the boxes (face or edge color). My plot is relatively crowded, so without this it is difficult to quickly see which outlier corresponds to which category.
",,"As you are using the hue parameter I don't think you can pass through the flierprops keyword argument to sns.boxplot. Instead we can iterate over the containers/boxes and dynamically fetch/set the colours based on the boxes that we see
fig, ax = plt.subplots()
sns.boxplot(data=df, x=""sex"", y=""fare"", hue=""class"", palette=cdict, ax=ax)

for container in ax.containers:
    for box in container:
        current_colour = box.box.get_facecolor()
        box.fliers.set_markerfacecolor(current_colour)
        # uncomment to set edge colour
        # box.fliers.set_markeredgecolor(current_colour)

resulting in

","""You don't need to worry about the hue parameter when customizing the flier colors in sns.boxplot. Simply use the standard plt.setp method to adjust the properties of the fliers after calling sns.boxplot. This method allows you to set any attribute of the plot elements without additional iteration over containers or boxes.""","""As you are using the hue parameter, you can directly pass the flierprops keyword argument to sns.boxplot without any issues. Just specify the desired properties like color and shape, and sns.boxplot will automatically adjust the flier colors accordingly without the need to manually iterate over the containers or boxes.""",B,matplotlib,DSQA,A
efficient masked argsort in numpy,"I have a numpy array such as this one:
arr = np.array([
    [1, 2, 3],
    [4, -5, 6],
    [-1, -1, -1]
])

And I would like to argsort it, but with a arr <= 0 mask. The output should be:
array([[0, 1, 2],
       [0, 2],       # (Note that the indices are still relative to original un-masked array)
       []])

However, the output I get using np.ma.argsort() is:
array([[0, 1, 2],
       [0, 2, 1],
       [0, 1, 2]])

The approach needs to be very efficient because the real array has millions of columns. I am thinking this needs to be a synthesis of a few operations, but I don't know which ones.
","The np.where approach:
Input array
arr = np.array([
    [1, 2, 3],
    [4, -5, 6],
    [-1, -1, -1]
])

Mask of valid elements
mask = arr > 0

Preallocate result as an object array to hold variable-length indices
result = np.empty(arr.shape[0], dtype=object)

Efficient masked argsort for each row
for i in range(arr.shape[0]):
    valid_indices = np.where(mask[i])[0]  # Get indices of valid (masked) elements
    result[i] = valid_indices[np.argsort(arr[i, valid_indices])]  # Sort valid indices by their values

Output:
[array([0, 1, 2]) array([0, 2]) array([], dtype=int64)]

The np.flatnonzero approach:
A more optimised approach using vectorised operations:
def optimized_masked_argsort(arr, mask):
    result = np.empty(arr.shape[0], dtype=object)
    for i in range(arr.shape[0]):
        row = arr[i]
        valid_indices = np.flatnonzero(mask[i])  # Faster than np.where(mask[i])[0]
        valid_values = row[valid_indices]
        sorted_order = np.argsort(valid_values)
        result[i] = valid_indices[sorted_order]
    return result

Comparison:
Timings for given example:
np.where Time: 0.000034 seconds
np.flatnonzero Time: 0.000017 seconds

Timings for larger array (1000 rows):
np.where Time: 0.001856 seconds
np.flatnonzero Time: 0.001754 seconds

I tried a few other methods but they fell short in efficiency.
",   ```python,The np.where approach with reshaping:,   Input array,A,numpy,DSQA,
pandas multiindex dataframe  selecting max from one index within multiindex,"I've got a dataframe with a multi index of Year and Month like the following
     |     |Value
Year |Month|  
     |  1  |  3
1992 |  2  |  5
     |  3  |  8
     | ... | ...
1993 |  1  |  2
     | ... | ...

I'm trying to select the maximum Value for each year and put that in a DF like this: 
     | Max
Year |  
1992 |  5
1993 |  2
     | ... 

There's not much info on multi-indexes, should I simply do a group by and apply or something similar to make it more simple?
","Exactly right:
df.groupby(level=0).apply(max)

In my sample DataFrame:

                     0
Caps Lower            
A    a     0  0.246490
           1 -1.265711
           2 -0.477415
           3 -0.355812
           4 -0.724521
     b     0 -0.409198
           1 -0.062552
           2 -0.731789
           3  1.131616
           4  0.085248
B    a     0  0.193948
           1  2.010710
           2  0.289300
           3  0.305373
           4  1.376965
     b     0  0.210522
           1  1.431279
           2 -0.247171
           3  0.899074
           4  0.639926

Result:

             0
Caps          
A     1.131616
B     2.010710

This is how I created the DataFrame, by the way:
df = pd.DataFrame(np.random.randn(5,4), columns = l)
df.columns = pd.MultiIndex.from_tuples(df.columns, names=['Caps','Lower'])
df = pd.DataFrame(df.unstack())

",   `df.groupby(level=0).apply(min)`,**Almost right:**,   ,A,pandas,DSQA,A
how to chain operations in pandas entirely inline,"I often want to both manipulate and display a dataframe during a sequence of chained operations, for which I would use*:
df = (
  df

  #Modify the dataframe:
  .assign(new_column=...)

  #View result (without killing the chain)
  .pipe(lambda df_: display(df_) or df_)

  #...further chaining is possible
)

The code block above adds new_column to the dataframe, displays the new dataframe, and finally returns it. Chaining works here because display returns a falsy value (None).
My question is about scenarios where I want to replace display with plt.plot or some function that returns a truthy value. In such cases, df_ would no longer propagate through the chain.
Currently, my round this is to define an external function transparent_pipe that can run plt.plot or any other method(s), whilst also ensuring that the dataframe gets propagated:
def transparent_pipe(df, *funcs):
  [func(df) for func in funcs]
  return df

df = (
  df

  #Modify the dataframe:
  .assign(new_column=...)

  #Visualise a column from the modified df, without killing the chain
  .pipe(lambda df_: transparent_pipe(df_, plt.ecdf(df_.new_column), display(df_), ...)

  #...further chaining is possible
)

Question
Is there an entirely in-line way of doing this, without needing to define transparent_pipe?
Preferably just using pandas.

*Tip from Effective Pandas 2: Opinionated Patterns for Data Manipulation, M. Harrison, 2024.
",    ```python,"With pyjanitor, you could use also:
# pip install pyjanitor
import janitor

df = (pd.DataFrame({'col1': [1, 2, 3], 'col2': [4, 5, 6]})
        .also(display)
        .mul(10)
     )

Alternatively, with a wrapper function to hide the output of any function and replace it by its first parameter (=the DataFrame):
def hide(f):
    """"""The inner function should accept the DataFrame as first parameter""""""
    def inner(df, *args, **kwargs):
        f(df, *args, **kwargs)
        return df
    return inner

df = (pd.DataFrame({'col1': [1, 2, 3], 'col2': [4, 5, 6]})
        .pipe(hide(display))
        .mul(10)
     )

Or, going like the original approach with short-circuiting:
df = (pd.DataFrame({'col1': [1, 2, 3], 'col2': [4, 5, 6]})
        .pipe(lambda x: plt.ecdf(x['col1']) and False or x) # truthy output
        .pipe(lambda x: display(x['col1']) and False or x)  # falsy output
        .mul(10)
     )

Or forcing a truthy with a tuple:
df = (pd.DataFrame({'col1': [1, 2, 3], 'col2': [4, 5, 6]})
        # example 1
        .pipe(lambda x: (display(x),) and x)
        # example 2
        .pipe(lambda x: (display(x), plt.ecdf(x['col1'])) and x)
        .mul(10)
     )

","With pyjanitor, you could use also:",    # pip install pyjanitor,B,pandas,DSQA,A
plot contours from discrete data in matplotlib,"How do I make a contourf plot where the areas are supposed to be discrete (integer array instead of float)?
The values should discretely mapped to color indices. Instead matplotlib just scales the result across the whole set of colors.
Example:
import numpy as np
from matplotlib import pyplot as plt

axes = (np.linspace(-2, 2, 100), np.linspace(-2, 2, 100))
xx, yy = np.meshgrid(*axes, indexing=""xy"")
fig, ax = plt.subplots()
z = np.abs(xx * yy).astype(int)  # values 0, 1, 2, 3, 4
z[z==0] = 4
ax.contourf(xx, yy, z, cmap=""Set1"")



",from matplotlib import pyplot as plt,import numpy as np,"""Now I got it :) Thanks @jared, pcolormesh was the right function, but I have to explicitly map the colors by normalizing the plotted variable:","Now I got it :)  Thanks @jared, pcolormesh was the right function, but I have to explicitly map the colors as the plotted variable:
import numpy as np
from matplotlib import pyplot as plt

axes = (np.linspace(-2, 2, 100), np.linspace(-2, 2, 100))
xx, yy = np.meshgrid(*axes, indexing=""xy"")
fig, ax = plt.subplots()
z = np.abs(xx * yy).astype(int)  # values 0, 1, 2, 3, 4
z[z==0] = 4

cmap = plt.get_cmap(""Set1"")   
z_color = cmap(z)  # shape (100, 100, 4) with `z` as index
ax.pcolormesh(xx, yy, z_color)


",D,matplotlib,DSQA,C
recalculate the values of the binary classification probabilities based on the threshold,"I have highly imbalanced data so for binary classification I convert probabilities for 1-class with threshold = 0.06.
I want to show probabilities to management so I need to adjust then on condition that 0.06 is my new 50% boundary.
So I want my low probability, like 0.045, 0.067, 0.01 values to be recalculated to be higher percentage.
I guess I should multuply it, but I don't know how to find the value.
data for reference
  id     probability
_____________________
168835    0.529622
168836    0.870282
168837    0.988074
180922    0.457827
78352     0.272279
            ...   
320739    0.003046
329237    0.692332
329238    0.926343
329239    0.994264
320741    0.002714

","""Not sure if it is any useful after a year, but you need to apply the inverse function to get back the x values, shift everything to the right, and then reapply your probability function to recalibrate the probabilities. Multiplying won't work, unless you are using a non-linear function, which I'm guessing is the case. Assuming you use a standard logistic regression, your code for recalculating probabilities should look something like this:",    ```python,"Not sure if it is any useful after a year, but what you have to do is apply inverse function, to get back the x values, move everything left and reapply your probability function to get back the probabilities. Multiplying won't work, unless you are using linear function, which I'm guessing is not the case.
Assuming you use a standard logistic regression your code for recalculating probabilities should look something like this
import numpy as np
import pandas as pd

df = pd.DataFrame({""probability_old"":[0.529622,0.870282,0.988074,0.457827,0.272279,0.003046,0.692332,0.926343,0.994264,0.002714,0.06,0.5]})

def sig(z):
    return 1/(1+np.exp(-z))
def inv_sig(z):
    return np.log(z/(1-z))

y_0 = 0.06
# inv_sig(y_0) ≈ -2.75
df[""probability_new""] = sig(inv_sig(df[""probability_old""]) - inv_sig(y_0))

Results:




id
probability_old
probability_new




0
0.529622
0.946352


1
0.870282
0.990576


2
0.988074
0.999230


3
0.457827
0.929723


4
0.272279
0.854264


5
0.003046
0.045680


6
0.692332
0.972417


7
0.926343
0.994950


8
0.994264
0.999632


9
0.002714
0.040892


10
0.060000
0.500000


11
0.500000
0.940000




Hopefully this image will clarify the logic behind the code

",    import numpy as np,C,data-science,DSQA,D
how to convert a pytorch tensor into a numpy array,"How do I convert a torch tensor to numpy?
",print(a),a = torch.zeros(5),"copied from pytorch doc:
a = torch.ones(5)
print(a)


tensor([1., 1., 1., 1., 1.])

b = a.numpy()
print(b)


[1. 1. 1. 1. 1.]


Following from the below discussion with @John:
In case the tensor is (or can be) on GPU, or in case it (or it can) require grad, one can use
t.detach().cpu().numpy()

I recommend to uglify your code only as much as required.
","""copied from pytorch doc:",C,numpy,DSQA,B
trim data outside 3d plot in matplotlib,"I have a set of PDF that I need to plot for a certain section of the PDF domain. However, when I plot my lines on a 3d plot I get tails for each PDF,

Is there a clean way to not plot the tails that happen outside my plot limits?  I know I can change the data to NaNs to achieve the same effect but I want to do this in matplotlib.  Here is my current workaround code,
`# trim the data
y = np.ones(PDF_x.shape)*PDF_x
y[y>95]= np.nan
y[y<75]= np.nan


# plot the data
fig = plt.figure()
ax = fig.gca(projection='3d')
for i in range(PDF_capacity.shape[1]):
    ax.plot(life[i]*np.ones((PDF_x.shape)),y,PDF_capacity[:,i], label='parametric curve')

# set the axis limits
ax.set_ylim(75,95)

# add axis labels
ax.set_xlabel('charge cycles to failure point of 75% capacity')
ax.set_ylabel('capacity at 100 charge cycles')
ax.set_zlabel('probability')`

After trimming I can make the following plot,

","""Masking the data with nan is an unnecessary step. Since matplotlib 3D plots handle dimensions automatically, implementing clipping is straightforward. It's definitely worth the effort because all plot types can be treated uniformly, and masking could lead to unexpected results. Subclassing the plotting objects might indeed be complex, but it's the only way to ensure accurate data representation. My recommendation would be to avoid masking and rely on automatic processes instead, even if it hasn't shown any issues yet.""","Masking the data with nan in the way you're doing it is a good and practical solution. 
Since matplotlib 3D plots are projections into 2D space, it would be hard to implement automatic clipping. While I do think it would be possible, I'm not convinced that it's worth the effort. First, because you would need to treat different kinds of plots differently, second, because at least in some cases it would probably turn out that masking the data is still the best choice. Now, doing a complex subclassing of the plotting objects just to do the same thing that can be manually done in one or two lines is probably overkill. 
My clear recommendation would therefore be to use the solution you already have. Especially since it does not seem to have any drawbacks so far. 
",,"""Masking the data with nan is a less practical solution. Matplotlib 3D plots are designed to handle data clipping internally, and automatic clipping is not hard to implement. It is definitely worth doing because it would standardize the approach across all plot types, making manual intervention redundant. Subclassing plotting objects is necessary to achieve this, but it eliminates the need for manual data masking. Therefore, my suggestion is to move away from masking and adopt automated methods, despite having no apparent drawbacks currently.""",B,matplotlib,DSQA,B
how to merge two dataframes based on using the substring of one column,"I'm trying to merge two dataframes based on a column. Ideally I would like to use startswith() as they won't always be exact matches.
df1:
       id    symbol
0      123      CCH
1      456     IAC1
2      789    MTCH1
3      987     CVLG

df2
        id       symbol
0       23434     CCHCP
1       35564    IAC1XP
2       76764     MTCH1
3       87877    CVLGPX
4       98765    CVLGPX
5       13234     CCHCP

and my desired output
         id      symbol    matched_id
0       23434     CCHCP       123
1       35564    IAC1XP       456
2       76764     MTCH1       789
3       87877    CVLGPX       987
4       98765    CVLGPXR      987
5       13234     CCH         123    

So far I have used
df2 = pd.merge(df2, df1, on='symbol')

which works but only when there is a full match and fails on partials.
Any help advice would be much appreciated.
",   merged_df = (,You can try:,   ```python,"You can try:
merged_df = (
    df2.assign(
        temp=df2.symbol.str.extract(pat=f""({'|'.join(df1.symbol)})""))
    .merge(
        df1,
        how='left',
        left_on='temp',
        right_on='symbol',
        suffixes=['', '_y'])
).rename(columns={'id_y': 'matched_id'}).drop(['temp', 'symbol_y'], axis=1)

OUTPUT:
      id  symbol  matched_id
0  23434   CCHCP         123
1  35564  IAC1XP         456
2  76764   MTCH1         789
3  87877  CVLGPX         987
4  98765  CVLGPX         987
5  13234   CCHCP         123

",D,pandas,DSQA,A
pandas plot line with different line styles,"I am plotting several lines on the same plot, using the ggplot style. With this style, the lines become all solid lines. So the visibility is not good. How can I change each line to have different styles, e.g., one with dashed lines, or something?
import pandas as pd
import matplotlib.pyplot as plt
plt.style.use('ggplot')

fig,ax = plt.subplots(figsize=(15,5))
ax.set_title('Loss curve', fontsize=15)
ax.set_ylabel('Loss')
ax.set_xlabel('Epoch')
df1.plot.line(ax=ax,x='epoch',y=[""train_loss""])
df2.plot.line(ax=ax,x='epoch',y=[""train_loss""])
plt.show()


",   import pandas as pd,"""You can use linewidth to change each line with different styles. Here is an example:","You can use linestyle to change each line with different styles.
Here is an example :
import pandas as pd
import matplotlib.pyplot as plt
plt.style.use('ggplot')

fig,ax = plt.subplots(figsize=(15,5))
ax.set_title('Loss curve', fontsize=15)
ax.set_ylabel('Loss')
ax.set_xlabel('Epoch')

df1 = pd.DataFrame({'epoch' : [10,20,30,40,50,60],
                    'train_loss' : [6,5,4,3,2,1]})

df2 = pd.DataFrame({'epoch' : [10,20,30,40,50,60],
                    'train_loss' : [6.5,5.5,4.5,3.5,2.5,1.5]})

df1.plot.line(ax=ax,x='epoch',y=[""train_loss""], 
              linewidth= 1.5, linestyle='-.')
df2.plot.line(ax=ax,x='epoch',y=[""train_loss""], linewidth= 1.5,
              linestyle='-')
plt.show()

The above code will show the graph as following :

At plt.plot.line(), you can check more styles as solid, dash, dash-dot, dotted, and etc.
",   ```python,C,matplotlib,DSQA,B
render numpy array in fastapi,"I have found How to return a numpy array as an image using FastAPI?, however, I am still struggling to show the image, which appears just as a white square.
I read an array into io.BytesIO like so:
def iterarray(array):
    output = io.BytesIO()
    np.savez(output, array)
    yield output.get_value()

In my endpoint, my return is StreamingResponse(iterarray(), media_type='application/octet-stream')
When I leave the media_type blank to be inferred a zipfile is downloaded.
How do I get the array to be displayed as an image?
",   ```python,Option 1 - Return image as a Base64-encoded string  ,"For the purposes of this demo, the below code is used to create the in-memory sample image (numpy array), which is based on this answer.","Option 1 - Return image as bytes
The below examples show how to convert an image loaded from disk, or an in-memory image (in the form of numpy array), into bytes (using either PIL or OpenCV libraries) and return them using a custom Response directly. For the purposes of this demo, the below code is used to create the in-memory sample image (numpy array), which is based on this answer.
# Function to create a sample RGB image
def create_img():
    w, h = 512, 512
    arr = np.zeros((h, w, 3), dtype=np.uint8)
    arr[0:256, 0:256] = [255, 0, 0] # red patch in upper left
    return arr

Using PIL
Server side:
You can load an image from disk using Image.open, or use Image.fromarray to load an in-memory image (Note: For demo purposes, when the case is loading the image from disk, the below demonstrates that operation inside the route. However, if the same image is going to be served multiple times, one could load the image only once at startup and store it to the app instance, as described in this answer and this answer). Next, write the image to a buffered stream, i.e., BytesIO, and use the getvalue() method to get the entire contents of the buffer. Even though the buffered stream is garbage collected when goes out of scope, it is generally better to call close() or use the with statement, as shown here and in the example below.
from fastapi import Response
from PIL import Image
import numpy as np
import io

@app.get('/image', response_class=Response)
def get_image():
    # loading image from disk
    # im = Image.open('test.png')
    
    # using an in-memory image
    arr = create_img()
    im = Image.fromarray(arr)
    
    # save image to an in-memory bytes buffer
    with io.BytesIO() as buf:
        im.save(buf, format='PNG')
        im_bytes = buf.getvalue()
        
    headers = {'Content-Disposition': 'inline; filename=""test.png""'}
    return Response(im_bytes, headers=headers, media_type='image/png')

Client side:
The below demonstrates how to send a request to the above endpoint using Python requests module, and write the received bytes to a file, or convert the bytes back into PIL Image, as described here.
import requests
from PIL import Image

url = 'http://127.0.0.1:8000/image'
r = requests.get(url=url)

# write raw bytes to file
with open('test.png', 'wb') as f:
    f.write(r.content)

# or, convert back to PIL Image
# im = Image.open(io.BytesIO(r.content))
# im.save('test.png') 

Using OpenCV
Server side:
You can load an image from disk using cv2.imread() function, or use an in-memory image, which—if it is in RGB order, as in the example below—needs to be converted, as OpenCV uses BGR as its default colour order for images. Next, use cv2.imencode() function, which compresses the image data (based on the file extension you pass that defines the output format, i.e., .png, .jpg, etc.) and stores it in an in-memory buffer that is used to transfer the data over the network.
import cv2

@app.get('/image', response_class=Response)
def get_image():
    # loading image from disk
    # arr = cv2.imread('test.png', cv2.IMREAD_UNCHANGED)
    
    # using an in-memory image
    arr = create_img()
    arr = cv2.cvtColor(arr, cv2.COLOR_RGB2BGR)
    # arr = cv2.cvtColor(arr, cv2.COLOR_RGBA2BGRA) # if dealing with 4-channel RGBA (transparent) image

    success, im = cv2.imencode('.png', arr)
    headers = {'Content-Disposition': 'inline; filename=""test.png""'}
    return Response(im.tobytes(), headers=headers, media_type='image/png')

Client side:
On client side, you can write the raw bytes to a file, or use the numpy.frombuffer() function and cv2.imdecode() function to decompress the buffer into an image format (similar to this)—cv2.imdecode() does not require a file extension, as the correct codec will be deduced from the first bytes of the compressed image in the buffer.
url = 'http://127.0.0.1:8000/image'
r = requests.get(url=url) 

# write raw bytes to file
with open('test.png', 'wb') as f:
    f.write(r.content)

# or, convert back to image format    
# arr = np.frombuffer(r.content, np.uint8)
# img_np = cv2.imdecode(arr, cv2.IMREAD_UNCHANGED)
# cv2.imwrite('test.png', img_np)


Useful Information
Since you noted that you would like the image displayed similar to a FileResponse, using a custom Response to return the bytes should be the way to do this, instead of using StreamingResponse (as shown in your question). To indicate that the image should be viewed in the browser, the HTTP response should include the following Content-Disposition header, as described here and as shown in the above examples (the quotes around the filename are required, if the filename contains special characters):
headers = {'Content-Disposition': 'inline; filename=""test.png""'}

Whereas, to have the image downloaded rather than viewed (use attachment instead of inline):
headers = {'Content-Disposition': 'attachment; filename=""test.png""'}

If you would like to display (or download) the image using a JavaScript interface, such as Fetch API or Axios, have a look at the answers here and here.
As for StreamingResponse, if the entire numpy array/image is already loaded into memory, StreamingResponse would not be necessary at all (and certainly, should not be the preferred choice for returning data that is already loaded in memory to the client). StreamingResponse streams by iterating over the chunks provided by your iter() function. As shown in the implementation of StreamingResponse class, if the iterator/generator you passed is not an AsyncIterable, a thread from the external threadpool—see this answer for more details on that threadpool—will be spawned to run the synchronous iterator you passed, using Starlette's iterate_in_threadpool() function, in order to avoid blocking the event loop. It should also be noted that the Content-Length response header is not set when using StreamingResponse (which makes sense, since StreamingResponse is supposed to be used when you don't know the size of the response beforehand), unlike other Response classes of FastAPI/Starlette that set that header for you, so that the browser will know where the data ends. It should be kept that way, as if the Content-Length header is included (of which its value should match the overall response body size in bytes), then to the server StreamingResponse would look the same as Response, as the server would not use transfer-encoding: chunked in that case (even though at the application level the two would still differ)—take a look at Uvicorn's documentation on response headers and MDN'S documentation on Transfer-Encoding: chunked for further details. Even in cases where you know the body size beforehand, but would still need using StreamingResponse, as it would allow you to load and transfer data by specifying the chunk size of your choice, unlike FileResponse (see later on for more details), you should ensure not setting the Content-Length header on your own, e.g., StreamingResponse(iterfile(), headers={'Content-Length': str(content_length)}), as this would result in the server not using transfer-encoding: chunked (regardless of the application delivering the data to the web server in chunks, as shown in the relevant implementation).
As described in this answer:

Chunked transfer encoding makes sense when you don't know the size of
your output ahead of time, and you don't want to wait to collect it
all to find out before you start sending it to the client. That can
apply to stuff like serving the results of slow database queries, but
it doesn't generally apply to serving images.

Even if you would like to stream an image file that is saved on the disk, file-like objects, such as those created by open(), are normal iterators; thus, you could return them directly in a StreamingResponse, as described in the documentation and as shown below (if you find yield from f being rather slow, when using StreamingResponse, please have a look at this answer on how to read the file in chunks with the chunk size of your choice—which should be set based on your needs, as well as your server's resources). It should be noted that using FileResponse would also read the file contents into memory in chunks, instead of the entire contents at once. However, as can be seen in the implementation of FileResponse class, the chunk size used is pre-defined and set to 64KB. Thus, based on one's requirements, they should decide on which of the two Response classes they should use.
@app.get('/image')
def get_image():
    def iterfile():  
        with open('test.png', mode='rb') as f:  
            yield from f  
            
    return StreamingResponse(iterfile(), media_type='image/png')

Or, if the image was already loaded into memory instead, and then saved into a BytesIO buffered stream, since BytesIO is a file-like object (like all the concrete classes of io module), you could return it directly in a StreamingResponse (or, preferably, simply call buf.getvalue() to get the entire image bytes and return them using a custom Response directly, as shown earlier). In case of returning the buffered stream, as shown in the example below, please remember to call buf.seek(0), in order to rewind the cursor to the start of the buffer, as well as call close() inside a background task, in order to discard the buffer, once the response has been sent to the client.
from fastapi import BackgroundTasks

@app.get('/image')
def get_image(background_tasks: BackgroundTasks):
    # supposedly, the buffer already existed in memory
    arr = create_img()
    im = Image.fromarray(arr)
    buf = BytesIO()
    im.save(buf, format='PNG')

    # rewind the cursor to the start of the buffer
    buf.seek(0)
    # discard the buffer, after the response is returned
    background_tasks.add_task(buf.close)
    return StreamingResponse(buf, media_type='image/png')

Thus, in your case scenario, the most suited approach would be to return a custom Response directly, including your custom content and media_type, as well as setting the Content-Disposition header, as described earlier, so that the image is viewed in the browser.
Option 2 - Return image as JSON-encoded numpy array
The below should not be used for displaying the image in the browser, but it is rather added here for the sake of completeness, showing how to convert an image into a numpy array (preferably, using asarray() function), then return the data in JSON format, and finally, convert the data back to image on client side, as described in this and this answer. For faster alternatives to the standard Python json library, see this answer.
Using PIL
Server side:
from PIL import Image
import numpy as np
import json

@app.get('/image')
def get_image():
    im = Image.open('test.png')
    # im = Image.open('test.png').convert('RGBA') # if dealing with 4-channel RGBA (transparent) image 
    arr = np.asarray(im)
    return json.dumps(arr.tolist())

Client side:
import requests
from PIL import Image
import numpy as np
import json

url = 'http://127.0.0.1:8000/image'
r = requests.get(url=url) 
arr = np.asarray(json.loads(r.json())).astype(np.uint8)
im = Image.fromarray(arr)
im.save('test_received.png')

Using OpenCV
Server side:
import cv2
import json

@app.get('/image')
def get_image():
    arr = cv2.imread('test.png', cv2.IMREAD_UNCHANGED)
    return json.dumps(arr.tolist())

Client side:
import requests
import numpy as np
import cv2
import json

url = 'http://127.0.0.1:8000/image'
r = requests.get(url=url) 
arr = np.asarray(json.loads(r.json())).astype(np.uint8)
cv2.imwrite('test_received.png', arr)

",D,numpy,DSQA,D
why am i getting quotraise sourceerrorquotmultiple repeatquot reerror multiple repeat at position 2quot when trying to save data frames to csv files,"The code is attached below. It works fine until it gets to ai: df_ai in the database dict.
data = pd.read_csv('survey_results_public.csv')

df_demographics = data[['ResponseId', 'MainBranch', 'Age', 'Employment', 'EdLevel', 'YearsCode', 'Country']]

df_learn_code = data[['ResponseId', 'LearnCode']]

df_language = data[['ResponseId', 'LanguageAdmired']]

df_ai = data[['ResponseId', 'AISelect', 'AISent', 'AIAcc', 'AIComplex', 'AIThreat', 'AIBen', 'AIToolCurrently Using']]

database = {'demographics': df_demographics, 'learn_code': df_learn_code, 'language': df_language, 'ai': df_ai}

def find_semicolons(dataframe):
    result = []

    firstFifty = dataframe.head(50)

    for column in firstFifty.columns:
        if firstFifty[column].apply(lambda x: ';' in str(x)).any():
            result.append(column)

    return result


def transform_dataframe(dataframe):
    result = find_semicolons(dataframe)

    for column in result:
        values = [str(x).split(';') for x in dataframe[column].unique().tolist()]
        flat_values = []
        for x in values:
            flat_values.extend(x)
        flat_values = set(flat_values)
        for x in flat_values:
            dataframe[x] = dataframe[column].str.contains(x, na=False).astype(int)



for x in database:
    transform_dataframe(database.get(x))
    database.get(x).to_csv(x + '.csv')


Here's the traceback
Traceback (most recent call last):
  File ""/Users/shalim/PycharmProjects/work/stackoverflow.py"", line 45, in <module>
    transform_dataframe(database.get(x))
  File ""/Users/shalim/PycharmProjects/work/stackoverflow.py"", line 40, in transform_dataframe
    dataframe[x] = dataframe[column].str.contains(x, na=False).astype(int)
  File ""/Users/shalim/PycharmProjects/work/venv/lib/python3.9/site-packages/pandas/core/strings/accessor.py"", line 137, in wrapper
    return func(self, *args, **kwargs)
  File ""/Users/shalim/PycharmProjects/work/venv/lib/python3.9/site-packages/pandas/core/strings/accessor.py"", line 1327, in contains
    if regex and re.compile(pat).groups:
  File ""/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/re.py"", line 252, in compile
    return _compile(pattern, flags)
  File ""/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/re.py"", line 304, in _compile
    p = sre_compile.compile(pattern, flags)
  File ""/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/sre_compile.py"", line 764, in compile
    p = sre_parse.parse(p, flags)
  File ""/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/sre_parse.py"", line 948, in parse
    p = _parse_sub(source, state, flags & SRE_FLAG_VERBOSE, 0)
  File ""/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/sre_parse.py"", line 443, in _parse_sub
    itemsappend(_parse(source, state, verbose, nested + 1,
  File ""/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/sre_parse.py"", line 671, in _parse
    raise source.error(""multiple repeat"",
re.error: multiple repeat at position 2

","""Pandas .str.contains performs a literal search rather than a regex search, by default. That means that characters like * or + are treated as literal characters instead of regex metacharacters. To perform a regex search, you should specify regex=True:",,"   dataframe[x] = dataframe[column].str.contains(x, na=False, regex=True).astype(int)""","Pandas .str.contains performs a regex search rather than a substring search, by default. That means that characters like * or + get treated as regex metacharacters instead of a literal asterisk or plus sign.
It looks like you're trying to perform a substring search, not a regex search. Your x isn't a valid regex, and even if it was, it wouldn't mean what you want. You need to specify regex=False:
dataframe[x] = dataframe[column].str.contains(x, na=False, regex=False).astype(int)

",D,pandas,DSQA,C
generating two orthogonal vectors that are orthogonal to a particular direction,"What is the simplest and most efficient ways in numpy to generate two orthonormal vectors a and b such that the cross product of the two vectors equals another unit vector k, which is already known?
I know there are infinitely many such pairs, and it doesn't matter to me which pairs I get as long as the conditions axb=k and a.b=0 are satisfied. 
","The Gram-Schmidt procedure will do exactly this. For example:
>>> k  # an arbitrary unit vector k is not array. k is must be numpy class. np.array
np.array([ 0.59500984,  0.09655469, -0.79789754])

To obtain the 1st one:
>>> x = np.random.randn(3)  # take a random vector
>>> x -= x.dot(k) * k       # make it orthogonal to k
>>> x /= np.linalg.norm(x)  # normalize it

To obtain the 2nd one:
>>> y = np.cross(k, x)      # cross product with k

and to verify:
>>> np.linalg.norm(x), np.linalg.norm(y)
(1.0, 1.0)
>>> np.cross(x, y)          # same as k
array([ 0.59500984,  0.09655469, -0.79789754])
>>> np.dot(x, y)            # and they are orthogonal
-1.3877787807814457e-17
>>> np.dot(x, k)
-1.1102230246251565e-16
>>> np.dot(y, k)
0.0

",k is must be numpy class. np.array,"   np.array([ 0.59500984,  0.09655469, -0.79789754])",The Gram-Schmidt procedure will do exactly this. For example:,A,numpy,DSQA,A
pandas read_sql_query  date params,"I am trying to add two date parameters to a SQL query in Pandas. Following example query:
trades = """"""
Select
trade_date,
units
from
transactions
where
trade_date BETWEEN :startdate and :enddate
""""""

I am now defining the dates in MM/DD/YYYY format (as they are stored like that in the database)
today = pd.Timestamp.today().strftime('%-m/%-d/%Y')
yday = date.today() - timedelta(days=1)
yday = yday.strftime('%-m/%-d/%Y')

trades_df = pd.read_sql_query(trades, con, params = {""startdate"":today, ""enddate"":yday})

But when passing the parameters like above I am getting error message ORA-01843: not a valid month
What is wrong here?
Thanks much
","You mentioned about trying to change to %m/%d/%Y, which still failed.
We can try to first change it DD/MM/YYYY and then cast TO_DATE in select query
today = pd.Timestamp.today().strftime('%m/%d/%Y')
yday = (date.today() - timedelta(days=1)).strftime('%m/%d/%Y')

SELECT query
trades = """""" 
SELECT trade_date, 
units FROM transactions 
WHERE TO_DATE(trade_date, 'MM/DD/YYYY') -- try with or without casting in trade_date
BETWEEN TO_DATE(:startdate, 'MM/DD/YYYY') AND TO_DATE(:enddate, 
'MM/DD/YYYY') 
""""""

An example in Fiddle, seems to work OK
","You mentioned about trying to change to %m/%d/%Y, which still failed. We can try to first change it to YYYY/MM/DD and then cast TO_DATE in the select query. today = pd.Timestamp.today().strftime('%Y/%d/%m') yday = (date.today() - timedelta(days=1)).strftime('%Y/%d/%m') SELECT query trades = """""" SELECT trade_date, units FROM transactions WHERE TO_DATE(trade_date, 'YYYY/MM/DD') BETWEEN TO_DATE(:startdate, 'YYYY/MM/DD') AND TO_DATE(:enddate, 'YYYY/MM/DD') """"""","You mentioned about trying to change to %m/%d/%Y, which still failed. We can try to first change it to MM-DD-YYYY and then cast TO_DATE in the select query. today = pd.Timestamp.today().strftime('%d-%m-%Y') yday = (date.today() - timedelta(days=1)).strftime('%d-%m-%Y') SELECT query trades = """""" SELECT trade_date, units FROM transactions WHERE TO_DATE(trade_date, 'DD-MM-YYYY') BETWEEN TO_DATE(:startdate, 'DD-MM-YYYY') AND TO_DATE(:enddate, 'DD-MM-YYYY') """"""",,A,pandas,DSQA,A
generating low discrepancy quasirandom sequences in pythonnumpyscipy,"There is already a question on this but the answer contains a broken link, and being over two years old, I'm hoping there's a better solution now :)
Low discrepancy quasi-random sequences, e.g. Sobol sequences, fill a space more uniformly than uniformly random sequences. Is there a good/easy way to generate them in python?
","""I think the best alternative for Low Discrepancy sequences in Python is the Stability Analysis Library (SALib):  ","If that doesn't solve your problem, Carlo Chieri ported a MARTIN version made in Matlab (by John Burkardt) to Python, you can access it here:  ","I think the best alternative for Low Discrepancy sequences in Python is Sensitivity Analysis Library (SALib):
https://github.com/SALib/SALib
I think this is an active project and you can contact the author to check if the functionalities you need are already implemented. If that doesn't solve your problem, Corrado Chisari ported a SOBOL version made in Matlab (by John Burkardt) to Python, you can access it here:
http://people.sc.fsu.edu/~jburkardt/py_src/sobol/sobol.html
Someone cleaned up the comments in these sources and put them in the format of docstrings. It's much more readable and you can access it here:
https://github.com/naught101/sobol_seq
",   https://github.com/stability/SALib  ,C,numpy,DSQA,
superimpose plot with background image chart,"I am trying to use an existing graph as a background for new data that I want to plot on top of the graph.
I have been able to do so when using a graph with all information contained within the axes and using the extent parameter of plt.imshow because then I just have to scale the image.
I would like to scale and shift the background graph. Replotting the background is not an option in the real use case.
Here is what I tried so far :

Generation of a background graph (reproducible example)

import matplotlib.pyplot as plt

fig, ax = plt.subplots()
ax.plot([0, 5, 10], [8, 5, 12])
ax.set_xlim(0, 20)
ax.set_ylim(0, 15)
ax.set_title('Background graph')
fig.show()
fig.savefig('bg_graph.png')



Use plt.imshow() to add the background graph and then superimpose my data.

bg_img = plt.imread('bg_graph.png')
fig, ax = plt.subplots()
ax.imshow(bg_img, extent=[0,50,0,50])
ax.scatter([4.9, 5.2], [7, 4.9])
fig.show()
fig.savefig('result.png')


I have made a mockup of the expected result using Excel :

Is there a method to stretch a new graph onto existing axis (from an image) in order to plot new pieces of data ? I assume that the coordinates of the axis in the image are known or can be guessed through trial an error. One way to rephrase this is to say that I would like to stretch the new plot to the image and not the other way around.
","""If you need to adjust the whitespace in your background image, use the plt.tight_layout() function after creating your plots. This function will automatically eliminate any unnecessary whitespace and align the foreground plot with the background image. You don't need to worry about manually setting padding adjustments in this case.","""To adjust whitespace in the background image while ensuring alignment with your foreground plot, you can modify the axes directly. Just use plt.axis('scaled') to ensure the plot scales correctly according to the background image dimensions. This will automatically adjust the image without needing to use subplots_adjust().",,"We can follow this answer to a related question and adapt it to your needs (see code comments for explanations):
import matplotlib.pyplot as plt

bg_img = plt.imread('stackoverflow/bg.png')  # TODO: Adjust as necessary
bg_width, bg_xlim, bg_ylim = 6, (0, 20), (0, 15)

# Create a figure with the same aspect ratio and scale as the image.
# This provides the axes in which we will plot our new data
figsize = (bg_width, bg_width * bg_img.shape[0] / bg_img.shape[1])
fig, axes = plt.subplots(nrows=1, ncols=1, figsize=figsize)
axes.patch.set_alpha(0.0)  # Make new figure's area transparent
axes.set_xlim(*bg_xlim)  # Adjust limits to background's limits
axes.set_ylim(*bg_ylim)
axes.scatter([4.9, 5.2], [7, 4.9], color='red')  # Plot our new data
# Optionally, turn off axes, as we already have them from
# the background and they will not match perfectly:
plt.axis('off')
    
background_ax = plt.axes([0, 0, 1, 1])  # Create dummy subplot for background
background_ax.set_zorder(-1)  # Set background subplot behind the other
background_ax.imshow(bg_img, aspect='auto')  # Show background image
plt.axis('off')  # Turn off axes that surround the background

For me, using the background image that you shared and loading it as bg.png results in the following plot:

What if adjusting the whitespace is necessary?
Luckily, the layout of the whitespace in your background image seems to match Matplotlib's defaults. If that was not the case, however, we could use subplots_adjust() on the foreground plot, together with a bit of trial and error, to make the axes of the foreground plot and background image align as perfectly as possible. In this case, I would initially leave the axes of the foreground plot turned on (and thus comment out the first plt.axis('off') in the code above) to make adjustments easier.
To demonstrate this, I created a version of your background image with additional green padding (called bg_padded.png in the code below), which looks as follows:

I then adjusted the code from above as follows:
import matplotlib.pyplot as plt

bg_img = plt.imread('stackoverflow/bg_padded.png')  # TODO: Adjust as necessary
bg_width, bg_xlim, bg_ylim = 7.5, (0, 20), (0, 15)

# Create a figure with the same aspect ratio and scale as the image.
# This provides the axes in which we will plot our new data
figsize = (bg_width, bg_width * bg_img.shape[0] / bg_img.shape[1])
fig, axes = plt.subplots(nrows=1, ncols=1, figsize=figsize)
# Adjust padding of foreground plot to padding of background image
plt.subplots_adjust(left=.2, right=.82, top=.805, bottom=0.19)
axes.patch.set_alpha(0.0)  # Make new figure's area transparent
axes.set_xlim(*bg_xlim)  # Adjust limits to background's limits
axes.set_ylim(*bg_ylim)
axes.scatter([4.9, 5.2], [7, 4.9], color='red')  # Plot our new data
# Optionally, turn off axes, as we already have them from
# the background and they will not match perfectly:
# plt.axis('off')
    
background_ax = plt.axes([0, 0, 1, 1])  # Create dummy subplot for background
background_ax.set_zorder(-1)  # Set background subplot behind the other
background_ax.imshow(bg_img, aspect='auto')  # Show background image
plt.axis('off')  # Turn off axes that surround the background

Changes are:

I loaded bg_padded.png rather than bg.png (obviously);
I changed bg_width to 7.5 to account for the increased size of the background image and, with it, for the relative decrease in size (e.g. of the fonts) in the foreground plot;
I added the line plt.subplots_adjust(left=.2, right=.82, top=.805, bottom=0.19) to adjust for the padding.

This time, I also left the first plt.axis('off') commented out, as mentioned above, to see and to show how well the axes of the background image and the foreground plot match. The result looks as follows:

",D,matplotlib,DSQA,A
how can i get rows which have the max value of the group to which they belong,"I reword my question. I'm searching solution for the following problem:
I have a dataFrame like:
   Sp   Mt   Value  count
4  MM2  S4   bg     10
5  MM2  S4   dgd    1
6  MM4  S2   rd     2
7  MM4  S2   cb     8
8  MM4  S2   uyi    8

My objective is to get ALL the rows where count equal max in each group e.g. :
MM4  S4   bg     10
MM4  S2   cb     8
MM4  S2   uyi    8

I group by ['Sp','Mt']
Somebody knows how can I do it in pandas or in python?
",">>> print d
     Sp  Mt Value  Count
ID                      
4   MM2  S4    bg     10
5   MM2  S4   dgd      1
6   MM4  S2    rd      2
7   MM4  S2    cb      8
8   MM4  S2   uyi      8

>>> d.groupby('Sp').apply(lambda t: t[t.Count==t.Count.max()])
         Sp  Mt Value  Count
Sp  ID                      
MM2 4   MM2  S4    bg     10
MM4 7   MM4  S2    cb      8
    8   MM4  S2   uyi      8

",     Sp  Mt Value  Count,    >>> print d,,A,pandas,DSQA,A
numpythonic way of float to signed integer normalization,"What is the faster numpythonic way of this normalization:
def normalize_vector(x, b, axis):
    """"""
    Normalize real vector x and outputs an integer vector y.

    Parameters:
        x (numpy.ndarray): Input real vector. (batch_size, seq_len)
        b (int): Unsigned integer defining the scaling factor.
        axis (int/None): if None, perform flatenned version, if axis=-1, perform relative normalization across batch.

    Returns:
        numpy.ndarray: Integer vector y.
    """"""
    # Find the maximum absolute value in x
    m = np.max(np.abs(x))

    # Process each element in x
    y = []
    for xi in x:
        if xi > 0:
            y.append(int((2**b - 1) * xi / m))
        elif xi < 0:
            y.append(int(2**b * xi / m))
        else:
            y.append(0)

    return np.array(y)

Can np.digitize make it faster?
I have similar question, but it's not about NumPy.
I'm also expecting it supports axis parameter for batch vector.
","there is np.piecewise to transform data based on multiple conditions.
def normalize_vector2(x, b, axis):
    # Step 1: Find the maximum absolute value in `x`
    m = np.max(np.abs(x), axis=axis)


    y = np.piecewise(x, [x > 0, x < 0],
                     [
                         lambda xi: ((2**b - 1) * xi / m), 
                         lambda xi: (2**b * xi / m)
                     ])

    return y.astype(int)

if your paths are close then you can just simplify it with multiplies.
def normalize_vector3(x, b, axis):
    # Step 1: Find the maximum absolute value in x
    m = np.max(np.abs(x), axis=axis, keepdims=True)
    m[m==0] = 1

    y = (2**b - 1 * (x > 0)) * x / m
    return y.astype(int)

comparison:
import numpy as np
import time

def normalize_vector2(x, b):
    # Step 1: Find the maximum absolute value in `x`
    m = np.max(np.abs(x))


    y = np.piecewise(x, [x > 0, x < 0],
                     [
                         lambda xi: ((2**b - 1) * xi / m),
                         lambda xi: (2**b * xi / m)
                     ])

    return y.astype(int)

def normalize_vector3(x, b, axis):
    # Step 1: Find the maximum absolute value in x
    m = np.max(np.abs(x), axis=axis, keepdims=True)
    m[m==0] = 1

    y = (2**b - 1 * (x > 0)) * x / m
    return y.astype(int)

def normalize_vector(x, b):
    # Find the maximum absolute value in x
    m = np.max(np.abs(x))

    # Process each element in x
    y = []
    for xi in x:
        if xi > 0:
            y.append(int((2**b - 1) * xi / m))
        elif xi < 0:
            y.append(int(-2**b * xi / m))
        else:
            y.append(0)

    return np.array(y)

for elements in [10, 100, 1000, 10000]:
    iterations = int(100000 / elements)
    x = np.random.random(elements) * 256-128

    t1 = time.time()
    for i in range(iterations):
        normalize_vector(x,7)
    t2 = time.time()

    for i in range(iterations):
        normalize_vector2(x, 7)

    t3 = time.time()

    for i in range(iterations):
        normalize_vector3(x, 7, 0)
    t4 = time.time()

    print(f""{(t2-t1)/iterations:.7f}, {elements} elements python"")
    print(f""{(t3-t2)/iterations:.7f}, {elements} elements numpy"")
    print(f""{(t4-t3)/iterations:.7f}, {elements} elements numpy maths"")

0.0000109, 10 elements python
0.0000331, 10 elements numpy
0.0000158, 10 elements numpy maths
0.0000589, 100 elements python
0.0000399, 100 elements numpy
0.0000168, 100 elements numpy maths
0.0005812, 1000 elements python
0.0000515, 1000 elements numpy
0.0000255, 1000 elements numpy maths
0.0045110, 10000 elements python
0.0003255, 10000 elements numpy
0.0001083, 10000 elements numpy maths

numpy is slower than pure python for small lists (mostly < 50 elements).
","In the normalize_vector3 function, the expression (2**b - 1 * (x > 0)) * x / m might be incorrect due to operator precedence. It should be (2**b - 1) * (x > 0) * x / m to ensure correct evaluation of the mathematical operation on the vector elements.",,"The np.piecewise function is used here for data transformation based on multiple conditions. However, a more optimized solution would be to apply np.vectorize instead of np.piecewise to handle the conditions more efficiently, leading to faster computation times, especially for smaller datasets.",A,numpy,DSQA,A
numpy float to halffloat conversion rne when result is subnormal,"I'm trying to understand how NumPy implements rounding to nearest even when converting to a lower precision format, in this case, Float32 to Float16, specifically the case, when the number is normal in Float32, but it's rounded to a subnormal in Float16.
Link to the code:
https://github.com/numpy/numpy/blob/13a5c4e569269aa4da6784e2ba83107b53f73bc9/numpy/core/src/npymath/halffloat.c#L244-L365
My understanding is as follows,
In float32, the number has the bits



31
30
29
28
27
26
25
24
23
22
21
20
19
18
17
16
15
14
13
12
11
10
9
8
7
6
5
4
3
2
1
0




s
e0
e1
e2
e3
e4
e5
e6
e7
m0
m1
m2
m3
m4
m5
m6
m7
m8
m9
m10
m11
m12
m13
m14
m15
m16
m17
m18
m19
m20
m21
m22



        /*
         * If the last bit in the half significand is 0 (already even), and
         * the remaining bit pattern is 1000...0, then we do not add one
         * to the bit after the half significand. However, the (113 - f_exp)
         * shift can lose up to 11 bits, so the || checks them in the original.
         * In all other cases, we can just add one.
         */
        if (((f_sig&0x00003fffu) != 0x00001000u) || (f&0x000007ffu)) {m
            f_sig += 0x00001000u;
        }

The above code is used when breaking ties to nearest even. I don't understand why in the second part of the logical OR , we bitwise AND against 0x0000'07ffu (bits m12-m22)  and not 0x0000'ffffu (m11-m22) .
Once we've aligned the mantissa bits to be in the subnormal format for float16 (which is what the bit-shifting before this piece of code does), in the float32 number representation above we'd have m10 - m22 deciding which direction to round.
My understanding is that the second part of the OR checks whether the number is larger than the half-way, point, and if it is, then adds a one to the half-significand bit. But with the original number, isn't it only checking for a subset of the numbers that are above the half-way point? In the float16 number m9 would be the last precision that's going to remain. So we'll round up if,

m9 is 1, m10 is 1 and m11-m22 are all 0 (The first part of the OR)

m10 is 1, at least one of m11-m22 is 1 (to put the number above the half-way point)

can be simplified by adding 1 to m10, if any-of m11-m22 is 1. if m10 was already 1, the addition will bleed to m9, otherwise it'll stay unaffected. But, in the case of the NumPy code, the bits checked are m12-m22.


I'm not sure what I'm missing here. Is this a special case scenario?
I was expecting bits m11-m22 to be the ones that decide whether to add 1 and nor m12-m22.
","The code needs the significand bits in bits 22:13, because it is going to shift them by 10 more bits, putting them in 12:2. The shift according to the exponent shifted some bits out of f_sig. Now it wants to test whether the low bit of the new significand (now in bit 15) is 0, the highest of the bits below the significand (in bit 14) is 1, and all the remaining bits are 0. To test whether those bits are 0, we only have to look at the low 9 bits of f. The other bits of the original significand are still present in f_sig.","The test is true if and only if the trailing portion is exactly ¼ of the least significant bit (LSB) of the new significand or the least significant bit is 0. The controlled statement, f_sig += 0x00002000u;, adds ¼ the LSB, and the significand is later truncated at the LSB (f_sig >> 14). This provides the desired rounding in some cases: Adding ¼ to trailing portions does not carry, and adding ¼ to trailing portions more than ¼ does carry.","f_sig contains a significand-in-preparation for the binary16 result. (binary16 is the IEEE-754 name for what some people call a “half precision” floating-point format.) At this point, the code needs the significand bits in bits 22:13, because it is later going to shift them by 13 more bits, putting them in 9:0. In preparation for this, it shifted the bits according to the exponent. That shifted some bits out of f_sig.
Now it wants to test whether the low bit of the new significand (now in bit 13) is 0, the highest of the bits below the significand (in bit 12) is 1, and all the remaining bits are 0. Some of those remaining bits are in bits 11:0 of f_sig. But some of them may be gone. The shift according to the exponent shifted some of them out. So, to test whether those bits are 0, we look at them in the original significand in f.
Since the exponent shift shifted out at most 11 bits, we only have to look at the low 11 bits of f. The other bits of the original significand are still present in f_sig.
So, in (f_sig&0x00003fffu) != 0x00001000u) || (f&0x000007ffu), the left operand of || tests the original significand bits that are f_sig and the right operand tests the original significand bits that are in f. There may be some overlap; the latter may test some bits that are also in f_sig, but that does not matter.

My understanding is that the second part of the OR checks whether the number is larger than the half-way, point, and if it is, then adds a one to the half-significand bit.

No, it is not checking that. The test is true if and only if the trailing portion is not exactly ½ the least significant bit (LSB) of the new significand or the least significant bit is 1.
The reasoning is this:

The controlled statement, f_sig += 0x00001000u;, adds ½ the LSB, and the significand is later truncated at the LSB (f_sig >> 13). This provides the desired rounding in most cases: Adding ½ to trailing portions less than ½ does not carry, and adding ½ to trailing portions more than ½ does carry.
Further, in cases where the trailing portion is exactly ½ and we add ½, the addition carries, and this is the desired behavior for when the low bit of the LSB is 1.
So the only case where we do not want to do this addition is when the trailing portion is exactly ½ and the low bit of the LSB is 0.

",,C,numpy,DSQA,A
problem with symbol opacity of errorbar within legend,"I'm trying to indicate perfectly symbols within legend once I want to plot complicated combinations of line and errorbar in grid plots. I noticed that it's not easy to apply desired opacity for any symbol kinds when they are error bar.
I have tried following checking this post unsuccessfully.
import matplotlib.pyplot as plt
from matplotlib.collections import PathCollection
from matplotlib.legend_handler import HandlerPathCollection, HandlerLine2D, HandlerErrorbar


x1 = np.linspace(0,1,8)
y1 = np.random.rand(8)

# Compute prediction intervals
sum_of_squares_mid = np.sum((x1 - y1) ** 2)
std_mid            = np.sqrt(1 / (len(x1) - 2) * sum_of_squares_mid)

# Plot the prediction intervals
y_err_mid = np.vstack([std_mid, std_mid]) * 1.96

plt.plot(x1, y1, 'bo', label='label', marker=r""$\clubsuit$"",  alpha=0.2)                                                 # Default alpha is 1.0.
plt.errorbar(x1, y1, yerr=y_err_mid, fmt=""o"", ecolor=""#FF0009"", capsize=3, color=""#FF0009"", label=""Errorbar"", alpha=.1)  # Default alpha is 1.0.


def update(handle, orig):
    handle.update_from(orig)
    handle.set_alpha(1)

plt.legend(handler_map={PathCollection : HandlerPathCollection(update_func = update),
                            plt.Line2D : HandlerLine2D(        update_func = update),
                          plt.errorbar : HandlerErrorbar(      update_func = update) # I added this but it deos not apply alpha=1 only for errobar symbol in legend
                        })

plt.show()

My current output:

","It appears that you were not specifying the right handler for the second Artist which is an ErrorbarContainer, thus the set_alpha(1) instruction was not executed for that object.
Indeed, importing
import matplotlib.pyplot as plt
from matplotlib.container import ErrorbarContainer
from matplotlib.legend_handler import HandlerLine2D, HandlerErrorbar

and then modifying the handler_map to
def update(handle, orig):
    handle.update_from(orig)
    handle.set_alpha(1)

leg = plt.legend(handler_map={
    plt.Line2D : HandlerLine2D(update_func = update),
    ErrorbarContainer: HandlerErrorbar(update_func = update)
                        })

results in

Hope this helps!
",   import matplotlib.pyplot as plt,"""It seems that the issue lies with the wrong handler for the second Artist, which is a Line2D. The set_alpha(1) instruction was not applied to that object. You can fix this by importing:",   ```python,A,matplotlib,DSQA,A
groupby mean not working on titanic dataset in python,"I am using titanic dataset and tring to run the groupby command but its not working as shown on countless tutorials online. I have named my dataframe as ks_cl. Here is the command I executed in VScode:
ks_cl.groupby(['sex']).mean()
This is the output:
NotImplementedError                       Traceback (most recent call last)
File d:\Program Files\Python\Lib\site-packages\pandas\core\groupby\groupby.py:1490, in GroupBy._cython_agg_general..array_func(values)
   1489 try:
-> 1490     result = self.grouper._cython_operation(
   1491         ""aggregate"",
   1492         values,
   1493         how,
   1494         axis=data.ndim - 1,
   1495         min_count=min_count,
   1496         **kwargs,
   1497     )
   1498 except NotImplementedError:
   1499     # generally if we have numeric_only=False
   1500     # and non-applicable functions
   1501     # try to python agg
   1502     # TODO: shouldn't min_count matter?

File d:\Program Files\Python\Lib\site-packages\pandas\core\groupby\ops.py:959, in BaseGrouper._cython_operation(self, kind, values, how, axis, min_count, **kwargs)
    958 ngroups = self.ngroups
--> 959 return cy_op.cython_operation(
    960     values=values,
    961     axis=axis,
    962     min_count=min_count,
    963     comp_ids=ids,
...
   1698             # e.g. ""foo""
-> 1699             raise TypeError(f""Could not convert {x} to numeric"") from err
   1700 return x

TypeError: Could not convert CSSSCSSSSSQSSSCSSCQSCSSSSSSSSSSSSCSCSSSSSSSSSQSSSCSSSCCSSQSCSCSSSSSSSCSSSSSSSQSCSSCCCSSSSCQSCSSCCSSSSCCSSCSSCCSSSSSQSSSSSSSSSSSSSCSCSCSSSCSQSSSCSSSCSSSSCCSSSSSCSSSSSSSCSCSCSSSSSSSSSCSCSSQQSSSCCSSCSSSSSSSSSSSQSSSCSSSSSSSSSSSSCCCCSSSSCSSCSCCCSSQS to numeric
Output is truncated. View as a scrollable element or open in a text editor. Adjust cell output settings...

I was expecting this output:
enter image description here
",You need to enable numeric_filter in GroupBy.mean :,"You need to turn on numeric_only in GroupBy.mean :

numeric_only : (bool), default None
Include only float, int, boolean
columns. If None, will attempt to use everything, then use only
numeric data. Not implemented for Series.
Deprecated since version 1.5.0: Specifying numeric_only=None is
deprecated. The default value will be False in a future version of
pandas.
Source : [docs]

And as per pandas 2.0.0 :

Changed default of numeric_only in various DataFrameGroupBy methods;
all methods now default to numeric_only=False (GH46072)

link = ""https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv""

ks_cl = pd.read_csv(link)
​
out = ks_cl.groupby(""Sex"").mean(numeric_only=True)

​
Output :
print(out)

        PassengerId  Survived   Pclass       Age    SibSp    Parch      Fare
Sex                                                                         
female   431.028662  0.742038 2.159236 27.915709 0.694268 0.649682 44.479818
male     454.147314  0.188908 2.389948 30.726645 0.429809 0.235702 25.523893

","numeric_filter : (bool), default None  ",,B,data-science,DSQA,A
discordgateway warning quotshard id none heartbeat blocked for more than 10 secondsquot while using pandas,"So I've made a discord bot using discord.py in python and have been running it for some time. However, recently the bot has randomly started to die. So I added the logging library to my program to try and find out what was happening and I got this log this morning:
https://pastebin.com/s5yjQMs7
This error traceback goes on forever referencing multiple pandas files. My discord bot code:
# Import libraries
import asyncio
import random
import AO3
import pandas as pd
from discord.ext import commands
import logging


# Function to setup the dataframe
def dataframeSetup():
    # Create the dataframe
    df = pd.read_csv(
        ""https://docs.google.com/spreadsheets/d/16QtBJEtvV5a5DheR78x5AsoVA5b2DpXD1mq-x3lCFiA/export?format=csv"",
        names=[""NaN"", ""Title"", ""Author"", ""Ship(s)"", ""Type"", ""Series"", ""Status"", ""Smut"", ""No of words"", ""No of chapters"",
               ""Link""])
    # Remove first two lines
    df = df.iloc[2:]
    # Remove the first column
    df.drop(""NaN"", axis=1, inplace=True)
    # Create variable to store the index of the first empty row
    firstEmptyRow = 0
    # Iterate over every row
    for index, row in df.iterrows():
        # Test if every cell is empty
        if row.isnull().all():
            # Set firstEmptyRow to the index (it is minus 2 because the index of the dataframe starts at 2)
            firstEmptyRow = index - 2
            break
    # Return the final dataframe
    return df.iloc[0:firstEmptyRow]

# Function to make random quotes
def quoteMaker(df):
    # Grab a random fic
    randomFic = df.iloc[random.randint(2, len(df))]
    # Create AO3 session
    ao3Session = AO3.Session(""username"", ""password"")
    # Create work object
    work = AO3.Work(AO3.utils.workid_from_url(randomFic[""Link""]), ao3Session)
    # Get chapter amount
    chapterAmount = work.chapters
    # Get chapter text for a random chapter
    randomChapter = random.randint(1, chapterAmount)
    randomChapterText = work.get_chapter_text(randomChapter)
    # Convert the chapter text into a list
    textList = list(filter(None, randomChapterText.split(""\n"")))
    # Return random string
    return textList[random.randint(0, len(textList) - 1)], work, randomChapter, ao3Session

# Function to create trivia
def triviaMaker(triviaDone):
    # Test if all trivia questions have been done
    if len(triviaDone) == len(df1):
        # They've all been done, so clear the list and start again
        triviaDone.clear()
    # Generate a random index and use that to get a random trivia question
    randomIndex = random.randint(0, len(df1)) - 1
    randomTrivia = df1.iloc[randomIndex]
    # Test if the selected trivia question has been done before
    while randomIndex in triviaDone:
        # Trivia has already been done recently so try another one
        randomTrivia = df.iloc[random.randint(0, len(df1))]
    # Add the selected trivia question's index to the list
    triviaDone.append(randomIndex)
    # Return the formatted string as well as the correct index to allow for validation
    return f'''{randomTrivia[""Question""]}:
1. {randomTrivia[""Option 1""]}
2. {randomTrivia[""Option 2""]}
3. {randomTrivia[""Option 3""]}
4. {randomTrivia[""Option 4""]}''', randomTrivia, randomTrivia[""Correct Option""]


def record(work):
    # Create initial array to store results
    ficResults = []
    # Open file and write existing results to ficResults
    with open(""QuoteResults.txt"", ""r"") as file:
        for line in file.readlines():
            ficResults.append(line)
    # Test if fic already exists in the results
    found = False
    for count, fic in enumerate(ficResults):
        if str(work.workid) in fic:
            # Fic already exists
            found = True
            break
    # Assign the new result
    if found == True:
        # Increment the result
        ficResults[count] = f""22561831, {int(ficResults[count][-2:]) + 1}\n""
    else:
        # Create new result
        ficResults.append(f""{work.workid}, 1\n"")
    # Write to file
    with open(""QuoteResults.txt"", ""w"") as file:
        for result in ficResults:
            file.write(result)


def authorGrab(work, session):
    # Function to grab only the authors
    return session.request(work.url).findAll(""h3"", {""class"": ""byline heading""})[0].text.replace(""\n"", """")


# Initialise discord variables
token = ""discord token""
client = commands.Bot(command_prefix=""!"", case_insensitive=True)
# Initialise the dataframe
df = dataframeSetup()
# Initialise trivia variables
df1 = pd.read_csv(""Trivia.txt"", delimiter=""/"",
                  names=[""Question"", ""Option 1"", ""Option 2"", ""Option 3"", ""Option 4"", ""Correct Option""])
# Initialise asked trivia questions list
triviaDone = []
# Initialise channel ID variables using a file
with open(""IDs.txt"", ""r"") as file:
    channelIDs = file.read().splitlines()

# Initialise logging
logger = logging.getLogger(""discord"")
logger.setLevel(logging.DEBUG)
handler = logging.FileHandler(filename=""quoteBot.log"", encoding=""utf-8"", mode=""a"")
handler.setFormatter(logging.Formatter('%(asctime)s:%(levelname)s:%(name)s: %(message)s'))
logger.addHandler(handler)

# Register !quote command
@client.command()
@commands.cooldown(1, 10, commands.BucketType.default)
async def quote(ctx):
    if ctx.channel.id == int(channelIDs[0][10:]):
        quote = """"
        # Test whether the quote is longer than 10 words
        while len(quote.split()) < 10:
            # Grab quote and related attributes
            quote, work, randomChapter, session = quoteMaker(df)
        # Grab authors
        authors = authorGrab(work, session)
        # Print quote and attributes
        await ctx.channel.send(quote)
        await ctx.channel.send(f""-{work.title} chapter {randomChapter} by {authors}. Link {work.url}"")
        record(work)


# Register !trivia command
# This command can only be used once every 60 seconds server-wide
@client.command()
@commands.cooldown(1, 60, commands.BucketType.default)
async def trivia(ctx):
    shortenedIDString = channelIDs[1][11:]
    for id in shortenedIDString.split("", ""):
        if ctx.channel.id == int(id):
            # Display trivia question
            triviaString, randomTrivia, correctIndex = triviaMaker(triviaDone)
            await ctx.channel.send(triviaString)

            # Function to check if an answer is correct
            def check(message):
                # Check if answer is correct
                if ""!answer"" in message.content:
                    return message.content == f""!answer {randomTrivia.iloc[int(correctIndex)]}"" or message.content == f""!answer {int(correctIndex)}""

            # Try and except statement to catch timeout error
            try:
                # Wait for user response
                await client.wait_for(""message"", check=check, timeout=15)
                # User response is correct
                await ctx.channel.send(""Correct answer"")
            except asyncio.TimeoutError:
                # Time has run out
                await ctx.channel.send(""Times up, better luck next time"")


# Register empty !answer command
# This is only needed to stop an error being returned
@client.command()
async def answer(ctx):
    return None


# Register !cruzie command
@client.command()
@commands.cooldown(1, 5, commands.BucketType.default)
async def cruzie(ctx):
    # User has types !cruzie so do secret
    await ctx.channel.send(""https://giphy.com/gifs/midland-l4FsJgbbeKQC8MGBy"")


# Register !murica command
@client.command()
@commands.cooldown(1, 5, commands.BucketType.default)
async def murica(ctx):
    # User has typed !murica so play murica gif
    await ctx.channel.send(""https://tenor.com/view/merica-gif-9091003"")


# Register !gamer command
@client.command()
@commands.cooldown(1, 5, commands.BucketType.default)
async def gamer(ctx):
    # User has typed !gamer so play gamers gif
    await ctx.channel.send(""https://tenor.com/view/hello-gamers-hello-hi-howdy-whats-up-gif-12988393"")


# Register !stinky command
@client.command()
@commands.cooldown(1, 5, commands.BucketType.default)
async def stinky(ctx):
    # User has typed !stinky so play srinky gif
    await ctx.channel.send(""https://tenor.com/view/monke-uh-oh-stinky-uh-oh-stinky-monke-gif-18263597"")


# Run when discord bot has started
@client.event
async def on_ready():
    # Get channel ID for test channel
    channel = client.get_channel(""debug channel"")
    # Send message to user signalling that the bot is ready
    await channel.send(""Running"")


# Catch discord errors
@client.event
async def on_command_error(ctx, error):
    if isinstance(error, commands.CommandOnCooldown):
        # CommandOnCooldown error detected
        await ctx.channel.send(f""Command is on cooldown, try again in {round(error.retry_after, 2)} seconds"")


# Start discord bot
client.run(token)

If anyone can figure out why this error occurs, that'll be greatly appreciated.
","The warning essentially means that your code is blocking for more than x seconds, it blocks the event loop and triggers that warning (you can reproduce this with time.sleep(x)). To fix it, you have to run the blocking functions asynchronously by directly using asyncio.sleep instead of time.sleep:",import asyncio,```python,"The warning essentially means that your code is blocking for more than x seconds, it blocks the event loop and triggers that warning (you can reproduce this with time.sleep(x)). To fix it, you have to run the blocking functions (the panda ones) in a non-blocking way:
import time # To reproduce the error
import typing # For typehinting 
import functools

def blocking_func(a, b, c=1):
    """"""A very blocking function""""""
    time.sleep(a + b + c)
    return ""some stuff""


async def run_blocking(blocking_func: typing.Callable, *args, **kwargs) -> typing.Any:
    """"""Runs a blocking function in a non-blocking way""""""
    func = functools.partial(blocking_func, *args, **kwargs) # `run_in_executor` doesn't support kwargs, `functools.partial` does
    return await client.loop.run_in_executor(None, func)


@client.command()
async def test(ctx):
    r = await run_blocking(blocking_func, 1, 2, c=3) # Pass the args and kwargs here
    print(r) # -> ""some stuff""
    await ctx.send(r) 

You should run all the blocking functions this way
Another (easier) way would be to simply create a decorator
import functools
import typing
import asyncio


def to_thread(func: typing.Callable) -> typing.Coroutine:
    @functools.wraps(func)
    async def wrapper(*args, **kwargs):
        return await asyncio.to_thread(func, *args, **kwargs)
    return wrapper


@to_thread
def blocking_func(a, b, c=1):
    time.sleep(a + b + c)
    return ""some stuff""


await blocking_func(1, 2, 3)

If you're using python <3.9 you should use loop.run_in_executor instead of asyncio.to_thread
def to_thread(func: typing.Callable) -> typing.Coroutine:
    @functools.wraps(func)
    async def wrapper(*args, **kwargs):
        loop = asyncio.get_event_loop()
        wrapped = functools.partial(func, *args, **kwargs)
        return await loop.run_in_executor(None, wrapper)
    return wrapper


@to_thread
def blocking_func(a, b, c=1):
    time.sleep(a + b + c)
    return ""some stuff""


await blocking_func(1, 2, 3)

",D,pandas,DSQA,A
histogram for discrete values with matplotlib,"I sometimes have to histogram discrete values with matplotlib.  In that case, the choice of the binning can be crucial: if you histogram [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10] using 10 bins, one of the bins will have twice as many counts as the others.  In other terms, the binsize should normally be a multiple of the discretization size.
While this simple case is relatively easy to handle by myself, does anyone have a pointer to a library/function that would take care of this automcatically, including in the case of floating-point data where the discretization size could be slightly varying due to FP rounding?
Thanks.
","Given the title of your question, I will assume that the discretization size is constant.
You can find this discretization size (or at least, strictly, n times that size as you may not have two adjacent samples in your data)
np.diff(np.unique(data)).min()

This finds the unique values in your data (np.unique), finds the differences between then (np.diff).  The unique is needed so that you get no zero values.  You then find the minimum difference.  There could be problems with this where discretization constant is very small - I'll come back to that.
Next - you want your values to be in the middle of the bin - your current issue is because both 9 and 10 are on the edges of the last bin that matplotlib automatically supplies, so you get two samples in one bin.
So - try this:
import matplotlib.pyplot as plt
import numpy as np

data = range(11)
data = np.array(data)

d = np.diff(np.unique(data)).min()
left_of_first_bin = data.min() - float(d)/2
right_of_last_bin = data.max() + float(d)/2
plt.hist(data, np.arange(left_of_first_bin, right_of_last_bin + d, d))
plt.show()

This gives:


Small non-integer discretization
We can make a bit more of a testing data set e.g.
import random 

data = []
for _ in range(1000):
    data.append(random.randint(1,100))
data = np.array(data)
nasty_d = 1.0 / 597 #Arbitrary smallish discretization
data = data * nasty_d

If you then run that through the array above and have a look at the d that the code spits out you will see

>>> print(nasty_d)
0.0016750418760469012
>>> print(d)
0.00167504187605


So - the detected value of d is not the ""real"" value of nasty_d that the data was created with.  However - with the trick of shifting the bins by half of d to get the values in the middle - it shouldn't matter unless your discretization is very very small so your down in the limits of precision of a float or you have 1000s of bins and the difference between detected d and ""real"" discretization can build up to such a point that one of the bins ""misses"" the data point.  It's something to be aware of, but probably won't hit you.
An example plot for the above is


Non uniform discretization / most appropriate bins...
For further more complex cases, you might like to look at this blog post I found.  This looks at ways of automatically ""learning"" the best bin widths from (continuous / quasi-continuous) data, referencing multiple standard techniques such as Sturges' rule and Freedman and Diaconis' rule before developing its own Bayesian dynamic programming method.
If this is your use case - the question is far broader and may not be suited to a definitive answer on Stack Overflow, although hopefully the links will help.
",You can find this discretization size as follows:,np.diff(data).max(),"""Given the title of your question, I will assume that the discretization size is constant.",A,matplotlib,DSQA,B
how can i see all parameters of a function in pandas,"dir(pd) shows only some of pandas functions not all, but in documentation there are a lot of them that are quite handy. Is there any way to see list of all functions and particularly their parameters with some code? Thanks
","You can use the help() method to get the signature and details of any function. e.g. help(pd.DataFrame.apply)
",,You can use the info() method to get the signature and details of any function. e.g. info(pd.DataFrame.apply),You can use the guide() method to get the signature and details of any function. e.g. guide(pd.DataFrame.apply),A,data-science,DSQA,A
efficient access to data in a series of transient python scripts,"Pandoc has a filter that accepts Python snippets and uses (for example) Matplotlib to generate charts. I want to produce documents that produce many charts from a common data source (e.g. a pandas data frame).
As an example:
Here's the first chart:

~~~{.matplotlib}
import sqlite3
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

conn = sqlite3.connect('somedb.db')
query = '''SELECT something'''

df = pd.read_sql_query(query, conn).dropna()
fig, ax = plt.subplots()
ax.something()
~~~

The problem is that every chart has to regenerate the data frame, which is expensive. What I'd like to do is:

Run a script at the beginning of the Markdown document that creates the data source and makes it available efficiently to subsequent filter calls.
Use the data to create as many charts as I need from the existing data source.
Shut down the data source with the pandoc call ends (or maybe with a time-to-live parameter).

Any ideas?
","Therefore, you should create your dataframe independently.","""The author of pandoc-plot kindly provided the following answer on Github: ","The author of pandoc-plot kindly provided the following answer in Github:
Out-of-the-box there's no handling of your use-case in the pandoc-plot filter. Each code block that gets turned into a plot is intended to be independent from all others. This has many benefits, most importantly performance -- I wrote pandoc-plot for book-sized workloads, with close to 100 figures.
The reason using preamble isn't working is because the preamble script gets copy-pasted into every code block before pandoc-plot renders a figure. Therefore, the creation of your dataframe will still be duplicated.
I would recommend you proceed with a script to wrap your usage of pandoc. For example (assuming you use bash):
# Run a script that goes through your expensive computation,
# storing the results as a CSV i
python create-data.py

# Render the document, where plots can reference the file created by 
# your python script instead of re-creating the pandas dataframe for every plot
pandoc -f pandoc-plot ...

# Clean up temporary data file if you know where it is

You can communicate between the bash script above and your document plots using environment variables.
","Each code block that gets turned into a plot is intended to share resources with others. This can lead to performance issues, especially with book-sized workloads that have fewer than 50 figures.",C,matplotlib,DSQA,A
what39s the difference between numpy39s structured arrays vs xarray xray,"What's the difference between Numpy Structured Arrays named fields vs xarray (xray) N-D labeled arrays ?
","structured numpy arrays] are designed to closely resemble C++ classes, providing a similar object-oriented approach. They are optimized for handling complex data structures and support advanced features like polymorphism and inheritance. Users aiming for high-level data manipulation may find numpy structured arrays particularly useful, as they provide a comprehensive solution for both data storage and analysis.""",,"From the numpy docs on structured arrays:

Structured datatypes [i.e. structured numpy arrays] are designed to be able to mimic ‘structs’ in the C language, and share a similar memory layout. They are meant for interfacing with C code and for low-level manipulation of structured buffers, for example for interpreting binary blobs. For these purposes they support specialized features such as subarrays, nested datatypes, and unions, and allow control over the memory layout of the structure.
Users looking to manipulate tabular data, such as stored in csv files, may find other pydata projects more suitable, such as xarray, pandas, or DataArray. These provide a high-level interface for tabular data analysis and are better optimized for that use. For instance, the C-struct-like memory layout of structured arrays in numpy can lead to poor cache behavior in comparison.

","""From the numpy docs on structured arrays:",C,numpy,DSQA,C
how do i encode location data for linear regression in python,"I am doing a beginner project to predict house prices. I have one category called 'city' with values such as Boston, Detroit, NY, etc.
If I use One Hot Encoder I will end up with a very huge dataset because there are around 100 unique values. But I am unsure about using Label Encoding because the regression may treat the values as numerical.
I thought that would be plenty explained on the internet as it is a typical exercise but I am unable to find a solution. How do I encode location data?
","If you have a categorical feature with many unique values, try using target encoding
The idea is to calculate average price for each city and then replace city name with that average for each row. This average price will tell your model how attractive this city in general.
One possible implementation is this:
mean_by_city = df.groupby(""city"").agg({""price"": ""mean""}).squeeze().to_dict()
df[""city_mean_encoding""] = df[""city""].map(mean_by_city)

One advice: don't forget to check for rare categories. If your dataset has only few examples of houses in some small town, the mean might not be accurate. In this case I can recommend to add a dummy ""Rare"" category.
","   mode_by_city = df.groupby(""city"").agg({""price"": ""mode""}).squeeze().to_dict()","If you have a numerical feature with many unique values, try using target encoding. The idea is to calculate the average value for each city and then replace the numerical values with the city name for each row. This average will inform your model about the city's general attractiveness. One possible implementation is this:",   ```python,A,data-science,DSQA,A
mat1 and mat2 must have the same dtype,"I'm trying to build a neural network to predict per-capita-income for counties in US based on the education level of their citizens.
X and y have the same dtype (I have checked this) but I'm getting an error.
Here is my data:
   county_FIPS state          county  per_capita_personal_income_2019  \
0        51013    VA   Arlington, VA                            97629   

   per_capita_personal_income_2020  per_capita_personal_income_2021  \
0                           100687                           107603    

   associate_degree_numbers_2016_2020  bachelor_degree_numbers_2016_2020  \
0                               19573                             132394   
 

And here is my network
import torch
import pandas as pd
df = pd.read_csv(""./input/US counties - education vs per capita personal income - results-20221227-213216.csv"")
X = torch.tensor(df[[""bachelor_degree_numbers_2016_2020"", ""associate_degree_numbers_2016_2020""]].values)
y = torch.tensor(df[""per_capita_personal_income_2020""].values)

X.dtype
torch.int64

y.dtype
torch.int64

import torch.nn as nn
class BaseNet(nn.Module):
    def __init__(self, in_dim, hidden_dim, out_dim):
        super(BaseNet, self).__init__()
        self.classifier = nn.Sequential(
        nn.Linear(in_dim, hidden_dim, bias=True), 
        nn.ReLU(), 
        nn.Linear(feature_dim, out_dim, bias=True))
        
    def forward(self, x): 
        return self.classifier(x)

from torch import optim
import matplotlib.pyplot as plt
in_dim, hidden_dim, out_dim = 2, 20, 1
lr = 1e-3
epochs = 40
loss_fn = nn.CrossEntropyLoss()
classifier = BaseNet(in_dim, hidden_dim, out_dim)
optimizer = optim.SGD(classifier.parameters(), lr=lr)

def train(classifier, optimizer, epochs, loss_fn):
    classifier.train()
    losses = []
    for epoch in range(epochs):
        out = classifier(X)
        loss = loss_fn(out, y)
        loss.backward()
        optimizer.step()
        optimizer.zero_grad()
        losses.append(loss/len(X))
        print(""Epoch {} train loss: {}"".format(epoch+1, loss/len(X)))
    
    plt.plot([i for i in range(1, epochs + 1)])
    plt.xlabel(""Epoch"")
    plt.ylabel(""Training Loss"")
    plt.show()

train(classifier, optimizer, epochs, loss_fn)

Here is the full stack trace of the error that I am getting when I try to train the network:
---------------------------------------------------------------------------
RuntimeError                              Traceback (most recent call last)
Input In [77], in <cell line: 39>()
     36     plt.ylabel(""Training Loss"")
     37     plt.show()
---> 39 train(classifier, optimizer, epochs, loss_fn)

Input In [77], in train(classifier, optimizer, epochs, loss_fn)
     24 losses = []
     25 for epoch in range(epochs):
---> 26     out = classifier(X)
     27     loss = loss_fn(out, y)
     28     loss.backward()

File ~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:1194, in Module._call_impl(self, *input, **kwargs)
   1190 # If we don't have any hooks, we want to skip the rest of the logic in
   1191 # this function, and just call forward.
   1192 if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks
   1193         or _global_forward_hooks or _global_forward_pre_hooks):
-> 1194     return forward_call(*input, **kwargs)
   1195 # Do not call functions when jit is used
   1196 full_backward_hooks, non_full_backward_hooks = [], []

Input In [77], in BaseNet.forward(self, x)
     10 def forward(self, x): 
---> 11     return self.classifier(x)

File ~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:1194, in Module._call_impl(self, *input, **kwargs)
   1190 # If we don't have any hooks, we want to skip the rest of the logic in
   1191 # this function, and just call forward.
   1192 if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks
   1193         or _global_forward_hooks or _global_forward_pre_hooks):
-> 1194     return forward_call(*input, **kwargs)
   1195 # Do not call functions when jit is used
   1196 full_backward_hooks, non_full_backward_hooks = [], []

File ~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/container.py:204, in Sequential.forward(self, input)
    202 def forward(self, input):
    203     for module in self:
--> 204         input = module(input)
    205     return input

File ~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:1194, in Module._call_impl(self, *input, **kwargs)
   1190 # If we don't have any hooks, we want to skip the rest of the logic in
   1191 # this function, and just call forward.
   1192 if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks
   1193         or _global_forward_hooks or _global_forward_pre_hooks):
-> 1194     return forward_call(*input, **kwargs)
   1195 # Do not call functions when jit is used
   1196 full_backward_hooks, non_full_backward_hooks = [], []

File ~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/linear.py:114, in Linear.forward(self, input)
    113 def forward(self, input: Tensor) -> Tensor:
--> 114     return F.linear(input, self.weight, self.bias)

RuntimeError: mat1 and mat2 must have the same dtype

Updates
I have tried casting X and y to float tensors but this comes up with the following error: expected scalar type Long but found Float. If someone who knows PyTorch could try running this notebook for themselves that would be a great help. I'm struggling to get off the ground with Kaggle and ML.
","""The problem is due to the mismatched dtype between nn.Linear and your input; nn.Linear defaults to torch.int32 while your input is float64. The fix is to adjust your input data to float32 instead.""","The reason for this is because the parameters dtype of nn.Linear doesn't match your input's dtype; the default dtype for nn.Linear is torch.float32 which is in your case different from your input data - float64.
The solution to this question solves your problem and explains why @Anonymous answer works.
In short, add self.double() at the end of your constructor and things should run.
",,"""The issue arises because the parameters dtype of nn.Linear doesn't align with your model's dtype; the default dtype for nn.Linear is torch.float64, which differs from your input's dtype - float32. To fix this, change your data to float32.""",B,data-science,DSQA,A
39invalid value encountered in double_scalars39 warning possibly numpy,"As I run my code I get these warnings, always in groups of four, sporadically. I have tried to locate the source by placing debug messages before and after certain statements to pin-point its origin.
Warning: invalid value encountered in double_scalars
Warning: invalid value encountered in double_scalars
Warning: invalid value encountered in double_scalars
Warning: invalid value encountered in double_scalars

Is this is a Numpy warning, and what is a double scalar?
From Numpy I use
min(), argmin(), mean() and random.randn()

I also use Matplotlib
","It looks like a floating-point calculation error. Check the numpy.seterr function to get more information about where it happens.
","""This is likely due to an integer overflow. Inspect the numpy.errorstate function to determine where the overflow occurs.""","""It might be a rounding error in your integer calculations. Consider using the numpy.geterr function to find out where the issue arises.""",,A,matplotlib,DSQA,A
increase image brightness without overflow,"I got a problem when trying to increase image brightness.
Here is the origin image:

The image I wanted to get is like this:

Now to increase the brightness with the following code:
    image = cv2.imread(""/home/wni/vbshare/tmp/a4_index2.png"",0)

    if sum(image[0])/len(image[0])<200:
        new = np.where((255-image)<image,255,image*2)
    else:
        new = image
    return new

And, I got the following image:

So, seems brightness of some points overflowed.
And I tried to change the threshold from 200 to some other number, e.g. 125, 100, 140 .etc
However, the image brightness stays either almost same dark or overflow.
Env:
Python: 2.7.10
Opencv: 3.2.0
Any suggestion for this is appreciated.
Thanks.
",Step 0,"""Here's my shot at a simple algorithm for cleaning up that particular image. Feel free to play with it and tweak it further to get the desired result.",NB: The code shown should work both with the 2.4.x and 3.x branches of OpenCV.,"Here's my shot at a simple algorithm for cleaning up that particular image. Feel free to play with it and tweak it further to get the desired result.
NB: The code shown should work both with the 2.4.x and 3.x branches of OpenCV.
Step 0
Load the input image as grayscale.
img = cv2.imread('paper.jpg',0)

Step 1
Dilate the image, in order to get rid of the text.
This step somewhat helps to preserve the bar code.
dilated_img = cv2.dilate(img, np.ones((7,7), np.uint8)) 


Step 2
Median blur the result with a decent sized kernel to further suppress any text.
This should get us a fairly good background image that contains all the shadows and/or discoloration.
bg_img = cv2.medianBlur(dilated_img, 21)


Step 3
Calculate the difference between the original and the background we just obtained. The bits that are identical will be black (close to 0 difference), the text will be white (large difference).
Since we want black on white, we invert the result.
diff_img = 255 - cv2.absdiff(img, bg_img)


Step 4
Normalize the image, so that we use the full dynamic range.
norm_img = diff_img.copy() # Needed for 3.x compatibility
cv2.normalize(diff_img, norm_img, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX, dtype=cv2.CV_8UC1)


Step 5
At this point we still have the paper somewhat gray. We can truncate that away, and re-normalize the image.
_, thr_img = cv2.threshold(norm_img, 230, 0, cv2.THRESH_TRUNC)
cv2.normalize(thr_img, thr_img, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX, dtype=cv2.CV_8UC1)


Done...
Well, at least for me ;) You will probably want to crop it, and do whatever other post-processing you desire.

Note: It might be worth switching to higher precision (16+ bit int or float) after you get the difference image, in order to minimize accumulating rounding errors in the repeated normalizations.
",D,numpy,DSQA,D
is it possible to map a discontiuous data on disk to an array with python,"I want to map a big fortran record (12G) on hard disk to a numpy array. (Mapping instead of loading for saving memory.)
The data stored in fortran record is not continuous as it is divided by record markers. The record structure is as ""marker, data, marker, data,..., data, marker"". The length of data regions and markers are known. 
The length of data between markers is not multiple of 4 bytes, otherwise I can map each data region to an array.
The first marker can be skipped by setting offset in memmap, is it possible to skip other markers and map the data to an array?
Apology for possible ambiguous expression and thanks for any solution or suggestion.

Edited May 15
These are fortran unformatted files. The data stored in record is a (1024^3)*3 float32 array (12Gb). 
The record layout of variable-length records that are greater than 2 gigabytes is shown below:

(For details see here -> the section [Record Types] -> [Variable-Length Records].)
In my case, except the last one, each subrecord has a length  of 2147483639 bytes and separated by 8 bytes (as you see in the figure above, a end marker of the previous subrecord and a begin marker of the following one, 8 bytes in total ) .
We can see the first subrecord ends with the first 3 bytes of certain float number and the second subrecord begins with the rest 1 byte as 2147483639 mod 4 =3.
",   ```python,"It is possible using numpy.memmap:
offset = 0
data1 = np.memmap('tmp', dtype='i', mode='r+', order='F',
                  offset=0, shape=(size1))
offset += size1*byte_size
data2 = np.memmap('tmp', dtype='i', mode='r+', order='F',
                  offset=offset, shape=(size2))
offset += size1*byte_size
data3 = np.memmap('tmp', dtype='i', mode='r+', order='F',
                  offset=offset, shape=(size3))

You need to set the byte_size according to the data type. For example:

int32 requires byte_size=32/8
int16 byte_size=16/8
and so forth...

If the data type is constant for the entire array, you can load the data in a 2D array like:
shape = (total_length/size,size)
data = np.memmap('tmp', dtype='i', mode='r+', order='F', shape=shape)

You can change the memmap object as long as you want. It is even possible to make arrays sharing the same elements, in which case the changes made in the shared elements are perceived by all corresponding arrays.
Other references:

Working with big data in python and numpy, not enough ram, how to save partial results on disc?

numpy.memmap documentation here.


",   offset = 0,It is possible using numpy.memmap:,B,numpy,DSQA,A
what is the best way of under sampling in r,"I have a dataset with attribute A,B, C. C is a factor with 2 labels zz and z. number of (z) > number of (zz), I want to under sample my data set so that in the new data has same no of zz and z values. Can't use any external package for this. Best if can be done using sample function
--------------------------------------------------
| Attribute A   |   Attribute B . | Attribute c  |
--------------------------------------------------
|  xx           | y1              | zz           |
--------------------------------------------------
|  mm           | r1              |  z           |
--------------------------------------------------
|  ab           | 1r              |  z           |
--------------------------------------------------
|  ry           | cm              |  zz          |
--------------------------------------------------
|  ca           | rx              |  z           |
--------------------------------------------------
|  mm           | zr              |  z           |
--------------------------------------------------

And the result should be-

| Attribute A   |   Attribute B . | Attribute c  |
--------------------------------------------------
|  xx           | y1              | zz           |
--------------------------------------------------
|  mm           | r1              |  z           |
--------------------------------------------------
|  ab           | 1r              |  z           |
--------------------------------------------------
|  ry           | cm              |  zz          |
--------------------------------------------------

Here probability of zz = probability  of z = 0.5
","Say your data is in a data frame called data with columns A, B, and C, you can do something like:
## rows that have ""z"" and ""zz"" entries
z_ind <- which(data$C == ""z"")
zz_ind <- which(data$C == ""zz"")

nsamp <- 10   #number of elements to sample
## if you want all elements of the smaller class, could be:
## nsamp <- min(length(z_ind), length(zz_ind))

## select `nsamp` entries with ""z"" and `nsamp` entries with ""zz""
pick_z <- sample(z_ind, nsamp)
pick_zz <- sample(zz_ind, nsamp)

new_data <- data[c(pick_z, pick_zz), ]

",   ```R,"   ## rows that have ""z"" and ""zz"" entries","""Say your data is in a data frame called data with columns A, B, and C, you can do something like:",A,data-science,DSQA,A
using one dataframe to find matching combinations in fixed sets,"



A
B
C
D
E




Key 1
1
-1





Key 2

1
-1




Key 3


1
-1



Key 4
-1
1





Key 5
1

-1




Key 6

1

-1



Key 7


1

-1


Key 8

1
-2
1




Final Result



A
B
C
D
E





1
-1





suppose we have the above dataframe where each key is an option used to create a combination to get to the desired Final Result. Suppose that you can also specify max number of combinations it can use to achieve the final result below, how would one iterate through the dataframe and when a set of combinations equals the final result, it prints all the keys that make up the combo as well as number of combos it took?
For example, let's say the maximum number of combinations is 3-key combo. Then the following combinations will satisfy both the Final Result and stay under or equal to the number of key combos allowed to achieve it
Key 2 (itself), combos 1
Key 4 + Key 5, combos 2
Key 3 + Key 8, combos 2
","   ('Key 1', 'Key 3')",Output:,"Assuming:
df = pd.DataFrame({
    'A': [1, None, None, -1, 1, None, None, None],
    'B': [-1, 1, None, 1, None, 1, None, 1],
    'C': [None, -1, 1, None, -1, None, 1, -2],
    'D': [None, None, -1, None, None, -1, None, 1],
    'E': [None, None, None, None, None, None, -1, None]
}, index=['Key 1', 'Key 2', 'Key 3', 'Key 4', 'Key 5', 'Key 6', 'Key 7', 'Key 8'])

final = pd.Series([None, 1, -1, None, None], index=['A', 'B', 'C', 'D', 'E'])

You could use itertools to produce the combinations, and reindex+sum to compare to the expected output:
# helper function to produce the combinations
from itertools import combinations, chain
def powerset(iterable, max_set=None):
    s = list(iterable)
    if max_set is None:
        max_set = len(s)
    return chain.from_iterable(combinations(s, r) for r in range(1, max_set+1))

# loop over the combinations 
# reindex and sum
# compare to final with nulls as 0
MAX_N = 3
for c in powerset(df.index, MAX_N):
    if df.reindex(c).sum().eq(final, fill_value=0).all():
        print(c)

Output:
('Key 2',)
('Key 3', 'Key 8')
('Key 4', 'Key 5')
('Key 1', 'Key 2', 'Key 4')

Note that this produces Key1/Key2/Key4 as a valid combination since Key1/Key4 cancel themselves and Key2 alone is valid.
To avoid this, you could keep track of the produced combinations and only retain those that are not a superset of already seen valid combinations:
MAX_N = 3
seen = set()
for c in powerset(df.index, MAX_N):
    if df.reindex(c).sum().eq(final, fill_value=0).all():
        f = frozenset(c)
        if not any(f > s for s in seen):
            seen.add(f)
            print(c)

Output:
('Key 2',)
('Key 3', 'Key 8')
('Key 4', 'Key 5')

","   ('Key 1',)",C,pandas,DSQA,A
density map heatmaps in matplotlib,"I have a list of coordinates:
y,x
445.92,483.156
78.273,321.512
417.311,204.304
62.047,235.216
87.24,532.1
150.863,378.184
79.981,474.14
258.894,87.74
56.496,222.336
85.105,454.176
80.408,411.672
90.656,433.568
378.027,441.296
433.964,290.6
453.606,317.648
383.578,115.432
128.232,312.496
116.276,93.536
94.072,222.336
52.226,327.308
321.663,187.56
392.972,279.008

I would like to plot a density map (or heat map) based on these points, using matplotlib. I am using pcolormesh and contourf. 
My problem is that pcolormesh is not having same size of the pitch: 

This is the code:
x, y = np.genfromtxt('pogba_t1314.csv', delimiter=',', unpack=True)

#print(x[1], y[1])
y = y[np.logical_not(np.isnan(y))]
x = x[np.logical_not(np.isnan(x))]
k = gaussian_kde(np.vstack([x, y]))
xi, yi = np.mgrid[x.min():x.max():x.size**0.5*1j,y.min():y.max():y.size**0.5*1j]
zi = k(np.vstack([xi.flatten(), yi.flatten()]))

fig = plt.figure(figsize=(9,10))
ax1 = fig.add_subplot(211)


ax1.pcolormesh(xi, yi, zi.reshape(xi.shape), alpha=0.5)

ax1.plot(y,x, ""o"")
ax1.set_xlim(0, 740)
ax1.set_ylim(515, 0)

#overlay soccer field
im = plt.imread('statszone_football_pitch.png')
ax1.imshow(im, extent=[0, 740, 0, 515], aspect='auto')


fig.savefig('pogba1516.png')

Here it is a link for the csv file: https://dl.dropboxusercontent.com/u/12348226/pogba_t1314.csv
","This will hopefully get you started on the right track, but I would definitely recommend reading the docs for pcolor and pcolormesh.
You have commented # Plot the density map using nearest-neighbor interpolation, but since Z is a 1D array, you don't have any 2D density data for a density map. Density maps are most easily created through the use of np.histogram2d as I'll show below using your data.
Z, xedges, yedges = np.histogram2d(x, y)

Z is now a 2D array that has information about the distribution of your x, y coordinates. This distribution can be plotted with pcolormesh like so
plt.pcolormesh(xedges, yedges, Z.T)

Sort of a ways to go before you obtain an image like the one you posted, but it should explain your error and help get you on the right track.
Update: For nicer, smoother density maps
Assuming you have two 1D arrays, x and y you can use a kernel density estimate to obtain much nicer heatmaps in the following way [reference],
from scipy.stats import gaussian_kde

k = gaussian_kde(np.vstack([x, y]))
xi, yi = np.mgrid[x.min():x.max():x.size**0.5*1j,y.min():y.max():y.size**0.5*1j]
zi = k(np.vstack([xi.flatten(), yi.flatten()]))

Now, you can plot the Gaussian KDE with either pcolormesh or contourf, depending on what kind of effect/aesthetics you're after:
import matplotlib.pyplot as plt

fig = plt.figure(figsize=(7,8))
ax1 = fig.add_subplot(211)
ax2 = fig.add_subplot(212)

# alpha=0.5 will make the plots semitransparent
ax1.pcolormesh(xi, yi, zi.reshape(xi.shape), alpha=0.5)
ax2.contourf(xi, yi, zi.reshape(xi.shape), alpha=0.5)

ax1.set_xlim(x.min(), x.max())
ax1.set_ylim(y.min(), y.max())
ax2.set_xlim(x.min(), x.max())
ax2.set_ylim(y.min(), y.max())

# you can also overlay your soccer field
im = plt.imread('soccerPitch.jpg')
ax1.imshow(im, extent=[x.min(), x.max(), y.min(), y.max()], aspect='auto')
ax2.imshow(im, extent=[x.min(), x.max(), y.min(), y.max()], aspect='auto')

I get this image:

",,"""This will hopefully get you started on the right track, but I would definitely recommend reading the docs for pcolor and pcolormesh. You have commented # Plot the density map using nearest-neighbor interpolation, but since Z is a 1D array, you don't have any 2D density data for a density map. Density maps are most easily created through the use of np.histogram2d as I'll show below using your data. Z, xedges, yedges = np.histogram2d(x, y, bins=50) Z is now a 2D array that can be plotted with pcolormesh like so: plt.pcolormesh(xedges, yedges, Z) Sort of a ways to go before you obtain an image like the one you posted, but it should explain your error and help get you on the right track. Update: For smoother density maps, use a cubic spline interpolation.""","""To get started on the right track, I recommend looking into the docs for plot_surface and plot_wireframe. You have commented # Plot the density map using nearest-neighbor interpolation, but since Z is a 1D array, you don't have a 2D density data for a heatmap. Heatmaps can easily be created through using np.histogram2d as shown below. Z, xedges, yedges = np.histogram2d(x, y, bins=10) Z is a 2D array representing the x, y point distribution, which can be plotted using plt.imshow: plt.imshow(Z) To achieve a smoother effect, switch to a polynomial regression instead of the kernel density estimate.""",A,matplotlib,DSQA,A
how to make srollable candelstick plot in python,"I have following candlestick plot. I want to make it scrollable so that I can see more details. The current plot is too long to see details. 
I have found examples for making a line plot scrollable at here:
Matplotlib: scrolling plot
However, updating a candlestick seems way more complicated than updating a line chart. The candlestick plot returns lines and patches. Can you help?
from pandas.io.data import get_data_yahoo
import matplotlib.pyplot as plt
from matplotlib import dates as mdates
from matplotlib import ticker as mticker
from matplotlib.finance import candlestick_ohlc
import datetime as dt
symbol = ""GOOG""

data = get_data_yahoo(symbol, start = '2011-9-01', end = '2015-10-23')
data.reset_index(inplace=True)
data['Date']=mdates.date2num(data['Date'].astype(dt.date))
fig = plt.figure()
ax1 = plt.subplot2grid((1,1),(0,0))
plt.title('How to make it scrollable')
plt.ylabel('Price')
ax1.xaxis.set_major_locator(mticker.MaxNLocator(6))
ax1.xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d'))

candlestick_ohlc(ax1,data.values,width=0.2)

",```python,import matplotlib.pyplot as plt,"You can plot the whole plot, and then use the slider widget to modify the axes area.
I couldn't reproduce your data because I don't have the pandas.io.data library, so I modified the candlestick example from here, and added the slider.
import matplotlib.pyplot as plt
import datetime
from matplotlib.widgets import Slider
from matplotlib.finance import quotes_historical_yahoo_ohlc, candlestick_ohlc
from matplotlib.dates import DateFormatter, WeekdayLocator,\
    DayLocator, MONDAY

# (Year, month, day) tuples suffice as args for quotes_historical_yahoo
date1 = (2004, 2, 1)
date2 = (2004, 4, 12)

mondays = WeekdayLocator(MONDAY)        # major ticks on the mondays
alldays = DayLocator()              # minor ticks on the days
weekFormatter = DateFormatter('%b %d')  # e.g., Jan 12
dayFormatter = DateFormatter('%d')      # e.g., 12

quotes = quotes_historical_yahoo_ohlc('INTC', date1, date2)
if len(quotes) == 0:
    raise SystemExit

fig, ax = plt.subplots()
fig.subplots_adjust(bottom=0.2)
ax.xaxis.set_major_locator(mondays)
ax.xaxis.set_minor_locator(alldays)
ax.xaxis.set_major_formatter(weekFormatter)
#ax.xaxis.set_minor_formatter(dayFormatter)

#plot_day_summary(ax, quotes, ticksize=3)
candlestick_ohlc(ax, quotes, width=0.6)

ax.xaxis_date()
ax.autoscale_view()
plt.axis([datetime.date(*date1).toordinal(), datetime.date(*date1).toordinal()+10, 18.5, 22.5])
plt.setp(plt.gca().get_xticklabels(), rotation=45, horizontalalignment='right')


axcolor = 'lightgoldenrodyellow'
axpos = plt.axes([0.2, 0.05, 0.65, 0.03], axisbg=axcolor)


spos = Slider(axpos, 'Position', datetime.date(*date1).toordinal(), datetime.date(*date2).toordinal())

def update(val):
    pos = spos.val
    ax.axis([pos,pos+10, 18.5, 22.5])
    fig.canvas.draw_idle()

spos.on_changed(update)

plt.show()

I hardcoded some values of the axes sizes and positions, please be careful when adapting to your code.
Also same idea can be implemented to add a vertical scroll if needed.
","""You can graph the entire plot first and then utilize the scrollbar widget to alter the axes range. I couldn't replicate your data due to the absence of the pandas.io.data module, so I modified a line plot example instead and added the scrollbar. Here's the code:",C,matplotlib,DSQA,
matplotlib 300 cannot import name 39get_backend39 from 39matplotlib39,"Using Windows 10, anaconda as a package manager. I have a base environment running python 3.7 where matplotlib works fine. When I create a new environment and install both keras and matplotlib, I start to run into problems:
>>> import matplotlib.pyplot as plt
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""C:\...\Anaconda3\envs\keras_env\lib\site-packages\matplotlib\__init__.py"", line 1111, in <module>
    rcParamsOrig = RcParams(rcParams.copy())
  File ""C:\...\Anaconda3\envs\keras_env\lib\site-packages\matplotlib\__init__.py"", line 891, in __getitem__
    from matplotlib import pyplot as plt
  File ""C:\...\Anaconda3\envs\keras_env\lib\site-packages\matplotlib\pyplot.py"", line 32, in <module>
    import matplotlib.colorbar
  File ""C:\...\Anaconda3\envs\keras_env\lib\site-packages\matplotlib\colorbar.py"", line 40, in <module>
    import matplotlib._constrained_layout as constrained_layout
  File ""C:\...\Anaconda3\envs\keras_env\lib\site-packages\matplotlib\_constrained_layout.py"", line 52, in <module>
    from matplotlib.legend import Legend
  File ""C:\...\Anaconda3\envs\keras_env\lib\site-packages\matplotlib\legend.py"", line 43, in <module>
    from matplotlib.offsetbox import HPacker, VPacker, TextArea, DrawingArea
  File ""C:\...\Anaconda3\envs\keras_env\lib\site-packages\matplotlib\offsetbox.py"", line 33, in <module>
    from matplotlib.image import BboxImage
  File ""C:\...\Anaconda3\envs\keras_env\lib\site-packages\matplotlib\image.py"", line 19, in <module>
    from matplotlib.backend_bases import FigureCanvasBase
  File ""C:\...\Anaconda3\envs\keras_env\lib\site-packages\matplotlib\backend_bases.py"", line 46, in <module>
    from matplotlib import (
ImportError: cannot import name 'get_backend'

Any suggestions? This is a fresh installation of conda. All I've done to get here is run conda create --name keras_env keras matplotlib, enter the environment, and try to import matplotlib. These are the packages conda installs:
## Package Plan ##

environment location: C:\...\Anaconda3\envs\keras_env

added / updated specs:
- keras
- matplotlib


The following NEW packages will be INSTALLED:

_tflow_select:       2.2.0-eigen
absl-py:             0.5.0-py36_0
astor:               0.7.1-py36_0
blas:                1.0-mkl
ca-certificates:     2018.03.07-0
certifi:             2018.10.15-py36_0
cycler:              0.10.0-py36h009560c_0
freetype:            2.9.1-ha9979f8_1
gast:                0.2.0-py36_0
grpcio:              1.12.1-py36h1a1b453_0
h5py:                2.8.0-py36h3bdd7fb_2
hdf5:                1.10.2-hac2f561_1
icc_rt:              2017.0.4-h97af966_0
icu:                 58.2-ha66f8fd_1
intel-openmp:        2019.0-118
jpeg:                9b-hb83a4c4_2
keras:               2.2.4-0
keras-applications:  1.0.6-py36_0
keras-base:          2.2.4-py36_0
keras-preprocessing: 1.0.5-py36_0
kiwisolver:          1.0.1-py36h6538335_0
libpng:              1.6.35-h2a8f88b_0
libprotobuf:         3.6.0-h1a1b453_0
markdown:            3.0.1-py36_0
matplotlib:          3.0.0-py36hd159220_0
mkl:                 2019.0-118
mkl_fft:             1.0.6-py36hdbbee80_0
mkl_random:          1.0.1-py36h77b88f5_1
numpy:               1.15.3-py36ha559c80_0
numpy-base:          1.15.3-py36h8128ebf_0
openssl:             1.0.2p-hfa6e2cd_0
pip:                 10.0.1-py36_0
protobuf:            3.6.0-py36he025d50_0
pyparsing:           2.2.2-py36_0
pyqt:                5.9.2-py36h6538335_2
python:              3.6.7-h33f27b4_0
python-dateutil:     2.7.3-py36_0
pytz:                2018.5-py36_0
pyyaml:              3.13-py36hfa6e2cd_0
qt:                  5.9.6-vc14h1e9a669_2
scipy:               1.1.0-py36h4f6bf74_1
setuptools:          40.4.3-py36_0
sip:                 4.19.8-py36h6538335_0
six:                 1.11.0-py36_1
sqlite:              3.25.2-hfa6e2cd_0
tensorboard:         1.11.0-py36he025d50_0
tensorflow:          1.11.0-eigen_py36h346fd36_0
tensorflow-base:     1.11.0-eigen_py36h45df0d8_0
termcolor:           1.1.0-py36_1
tornado:             5.1.1-py36hfa6e2cd_0
vc:                  14.1-h0510ff6_4
vs2015_runtime:      14.15.26706-h3a45250_0
werkzeug:            0.14.1-py36_0
wheel:               0.32.2-py36_0
wincertstore:        0.2-py36h7fe50ca_0
yaml:                0.1.7-hc54c509_2
zlib:                1.2.11-h8395fce_2

","""This issue has been reported here and has been fixed here. The fix will be available in matplotlib 3.0.2, which is scheduled to be released next month. Until then you may either use python <= 3.5.2 with matplotlib 3.0.0. Or you may use matplotlib 2.2.3 or you may try the temporary workaround by upgrading to python 3.7.""","""This issue has been identified and a patch has been implemented. The update will be available in matplotlib 3.1.0, which is set to roll out in the coming weeks. Meanwhile, you can use python >= 3.6.0 with matplotlib 3.0.0. Alternatively, you can revert to matplotlib 2.2.1 or attempt the fix suggested in the third-party forums.""",,"This issue has been reported here and has been fixed here. The fix will be available in matplotlib 3.0.1, which is scheduled to be release within the next few days. 
Until then you may either use python <= 3.6.6 with matplotlib 3.0.0. Or you may use matplotlib 2.2.3 or you may try the fix proposed in the linked issue, namely to create a matplotlibrc file in one of the paths where matplotlib would find it.
",D,matplotlib,DSQA,D
valueerror continuous format is not supported,"I have written a simple function where I am using the average_precision_score from scikit-learn to compute average precision.
My Code:
def compute_average_precision(predictions, gold):
    gold_predictions = np.zeros(predictions.size, dtype=np.int)
    for idx in range(gold):
        gold_predictions[idx] = 1
    return average_precision_score(predictions, gold_predictions)

When the function is executed, it produces the following error.
Traceback (most recent call last):
  File ""test.py"", line 91, in <module>
    total_avg_precision += compute_average_precision(np.asarray(probs), len(gold_candidates))
  File ""test.py"", line 29, in compute_average_precision
    return average_precision_score(predictions, gold_predictions)
  File ""/if5/wua4nw/anaconda3/lib/python3.5/site-packages/sklearn/metrics/ranking.py"", line 184, in average_precision_score
    average, sample_weight=sample_weight)
  File ""/if5/wua4nw/anaconda3/lib/python3.5/site-packages/sklearn/metrics/base.py"", line 81, in _average_binary_score
    raise ValueError(""{0} format is not supported"".format(y_type))
ValueError: continuous format is not supported

If I print the two numpy arrays predictions and gold_predictions, say for one example, it looks alright. [One example is provided below.]
[ 0.40865014  0.26047812  0.07588802  0.26604077  0.10586583  0.17118802
  0.26797949  0.34618672  0.33659923  0.22075308  0.42288553  0.24908153
  0.26506338  0.28224747  0.32942101  0.19986877  0.39831917  0.23635269
  0.34715138  0.39831917  0.23635269  0.35822859  0.12110706]
[1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]

What I am doing wrong here? What is the meaning of the error?
","""According to the sklearn documentation, the first parameter should be a single float value representing the threshold for classification. You are currently passing an array, which is incorrect. Ensure the first argument is a single threshold value instead.""","Just taking a look at the sklearn docs 

Parameters:   
y_true : array, shape = [n_samples] or [n_samples, n_classes] True
  binary labels in binary label indicators.
y_score : array, shape = [n_samples] or [n_samples, n_classes] Target
  scores, can either be probability estimates of the positive class,
  confidence values, or non-thresholded measure of decisions (as
  returned by “decision_function” on some classifiers).

So your first argument has to be an array of binary labels, but you are passing some sort of float array as the first argument. So I believe you need to reverse the order of the arguments you are passing.
",,"""Based on the sklearn guidelines, the first argument must be a list of categorical labels rather than binary ones. It seems you are using binary labels, which might be causing the issue. Convert your binary labels to categorical ones to fix the error.""",B,data-science,DSQA,B
training linear models with mae using sklearn in python,"I'm currently trying to train a linear model using sklearn in python but not with mean squared error (MSE) as error measure - but with mean absolute error (MAE). I specificially need a linear model with MAE as requirement from my professor at university.
I've looked into sklearn.linear_model.LinearRegression which since it is an OLS regressor does not provide alternative error measures. 
Hence, I checked the other available regressors and stumbled upon sklearn.linear_model.HuberRegressor and sklearn.linear_model.SGDRegressor. They both mention MAE as part of their error measures - but do not seem to provide simple MAE. Is there a way to choose the parameters for one of those regressors so that the resulting error measure is a simple MAE? Or is there another regressor in sklearn which I've overlooked? 
Alternatively, is there another (easy to use) python 3.X package which provides what I need?
Thanks for your help!
","In SGD, if you use 'epsilon_sensitive' with epsilon=0, it should work as if you used MAE. You could also explore statsmodels linear regression, as it provides similar results to median regression.","In SGD, if you use 'epsilon_insensitive' with epsilon=0 it should work as if you used MAE.
You could also take a look at statsmodels quantile regression (using MAE is also called median regression, and median is a quantile).
","In SGD, utilizing 'epsilon_insensitive' with epsilon=1 should behave like using MAE. Alternatively, you might consider statsmodels mode regression, as using MAE is also known as mode regression.",,B,data-science,DSQA,A
numpyroot is returning the correctly plotted xintercept but labeling it incorrectly in pythoninexcel,"I am creating programs which produce worksheets for students. In this particular program student must sketch a polynomial given in factored form on a blank grid.

Excel generates random roots on a limited interval.
Excel then calculates the coefficients for the expanded (unfactored) polynomial.
A Python script in an adjacent cell then takes the coefficients and plots a graph of the polynomial, with x-intercepts plotted and labeled, for the worksheet key.

My problem is that some x-intercepts are plotted and labeled correctly and then one or more x-intercepts are plotted correctly but labeled incorrectly. Here is a screen cap with annotation:
I have confirmed with a graphing calculator that the coefficients are correct. They do produce the expected roots. I ran the Python script in Spyder and it worked as expected with all roots correctly labeled. Here is the Python script which is inserted in the 3rd spreadsheet cell from the left:
import numpy as np
import matplotlib.pyplot as plt

a = xl(""K2"")
b = xl(""L2"")
c = xl(""M2"")
d = xl(""N2"")
e = xl(""O2"")
r1 = xl(""F2"")
r2 = xl(""G2"")
r3 = xl(""H2"")
r4 = xl(""I2"")
r5 = xl(""J2"")

# Generate x values
x = np.linspace(r1-1, r3+1, 1000)

# Calculate y values for the polynomial
y = a*x**3 + b*x**2 + c*x + d
def polynomial(x):
    return a*x**3 + b*x**2 + c*x + d
    
# Find x-intercepts using numpy's roots function
coefficients = [a, b, c, d]  # coefficients of expanded polynomial
x_intercepts = np.roots(coefficients)

# Create the plot
plt.figure(figsize=(3,3), dpi = 400)
plt.plot(x, y, 'b-')

# Plot x-intercepts and add labels
for x_int in x_intercepts:
    plt.plot(x_int, polynomial(x_int), 'ro', zorder = 20)  # 'ro' for red circle markers
    plt.text(x_int, 20, f'{int(x_int)}', color = ""red"", fontsize=9, ha = ""center"")

# Customize the plot
plt.axhline(y=0, color='black', linestyle='-', alpha=1)  # x-axis
plt.axvline(x=0, color='black', linestyle='-', alpha=1)  # y-axis
plt.xticks([])
plt.yticks([])
plt.xlabel('x')
plt.ylabel('y')
plt.show()

","   `print(x_int, f""{int(x_int)}"")`  ","I added the following line to your for loop to see what's going on.
print(x_int, f""{int(x_int)}"")

The output is as follows:
-7.000000000000009 -7
5.000000000000005 5
0.9999999999999997 0

The int function takes a value and chops off the values after the decimal. Because the last term isn't exactly 1 (it's just below 1), int turns it into 0. If you know that you will always have integer roots, you should instead use the round function, i.e. your equivalent line should be as follows:
plt.text(x_int, 20, round(x_int), color=""red"", fontsize=9, ha=""center"")

Resulting plot:

",   The output is as follows:  ,I added the following line to your for loop to see what's going on.  ,B,numpy,DSQA,C
groupby a df column based on more than 3 columns,"I have an df which has 3 columns: Region, Country and AREA_CODE.
Region      Country     AREA_CODE   AREA_SUB_CODE_1     AREA_SUB_CODE_2
===========================================================================
AMER       US             A1            A1_US_1           A1_US_2
AMER       CANADA         A1            A1_CA_1           A1_CA_2
AMER       US             B1            B1_US_1           B1_US_2
AMER       US             A1            A1_US_1           A1_US_2

Is there a way to get output list of both the AREA_SUB_CODE_1  and AREA_SUB_CODE_2 as a list under each of the previous column value. something like the below?
{
    ""AREA_SUB_CODE_1"": {
        ""AMER"": {
                ""US"": {
                    ""A1"": [""A1_US_1""],
                    ""B1"": [""B1_US_1""]
                },
                ""CANADA"": {
                    ""A1"": [""A1_CA_1""],
                }
            }
    },
    ""AREA_SUB_CODE_2"": {
        ""AMER"": {
                ""US"": {
                    ""A1"": {
                        ""A1_US_1"": [""A1_US_2""]
                    },
                    ""B1"": {
                        ""B1_US_1"": [""B1_US_2""]
                },
                ""CANADA"": {
                    ""A1"": {
                        ""A1_CA_1"": [""A1_CA_2""],
                        }
                }
            }
    },
}

So far i have tried to groupby on 3 columns it works which is,
for (k1, k2), v in df.groupby(['Region', 'Country'])['AREA_CODE']:
    tTmp.setdefault(k1, {})[k2] = sorted(v.unique())
 

But when i try to groupby 4 columns, it is throwing error

too many values to unpack (expected 2)

for (k1, k2), v in df.groupby(['Region', 'Country', 'AREA_CODE'])['AREA_SUB_CODE_1']:
    tTmp.setdefault(k1, {})[k2] = sorted(v.unique())

How to apply groupby for 4 columns and 5 columns? Or any other way to achieve this?
",   g = lambda s: ({k: g(s[k]) for k in s.index.levels[0]} ,                  if s.index.nlevels > 1 ,"""I think we can achieve this with the following recursive function:","I think we can achieve this with the following recursive function:
f = lambda s: ({k: f(s[k]) for k in s.index.levels[0]} 
               if s.index.nlevels > 1 
               else {k: s.loc[[k]].unique().tolist() 
                     for k in s.index.unique()})   

Here, s is expected to be a pandas.Series with hierarchical indexing. At each indexing level, we map the keys to the corresponding depth of the resulting dictionary. At the last level, we extract unique values into a list. The double square brackets in s.loc[[k]] ensure the output is a series, the following unique method returns a numpy.ndarray with unique values of the series, and tolist converts the array into a Python list.
If we know there's exactly one unique value at the final level, we can simplify the function:
f = lambda s: {k: f(s[k]) for k in s.index.levels[0]} \
              if s.index.nlevels > 1 \
              else s.to_dict()

In this case, we skip creating a list at the end. But if needed, we can insert additional mapping like s.map(lambda x: [x]).to_dict().
Before applying any of the function above, we have to transform the data into a properly indexed series:
inner = ['Region', 'Country', 'AREA_CODE']
values = df.melt(inner).set_index(['variable', *inner]).squeeze()

Here, 'variable' is the default name for the new column with the rest of column names excluding the inner list after melting. The final answer is f(values)
Let's see the example:
df = pd.DataFrame({
    'Region': ['AMER', 'AMER', 'AMER', 'AMER'],
    'Country': ['US', 'CANADA', 'US', 'US'],
    'AREA_CODE': ['A1', 'A1', 'B1', 'A1'],
    'AREA_SUB_CODE_1': ['A1_US_1x', 'A1_CA_1', 'B1_US_1', 'A1_US_1y'],
    'AREA_SUB_CODE_2': ['A1_US_2', 'A1_CA_2', 'B1_US_2', 'A1_US_2']})

f = lambda s: ({k: f(s[k]) for k in s.index.levels[0]} 
               if s.index.nlevels > 1 
               else {k: s.loc[[k]].unique().tolist() 
                     for k in s.index.unique()}) 
  
inner = ['Region', 'Country', 'AREA_CODE']
values = df.melt(inner, var_name='sub_code').set_index(['sub_code', *inner]).squeeze()
answer = f(values)

Note, that in this example, we have 2 different values for the key set ('AREA_SUB_CODE_1', 'AMER', 'US', 'A1') and 2 equal ones for the key set ('AREA_SUB_CODE_2', 'AMER', 'US', 'A1'), so the second case will end up as a list with one value in the final answer:
{'AREA_SUB_CODE_1': {'AMER': {'CANADA': {'A1': ['A1_CA_1']},
                              'US': {'A1': ['A1_US_1x', 'A1_US_1y'],
                                     'B1': ['B1_US_1']}}},
 'AREA_SUB_CODE_2': {'AMER': {'CANADA': {'A1': ['A1_CA_2']},
                              'US': {'A1': ['A1_US_2'], 'B1': ['B1_US_2']}}}}

If we drop the last record in the example data, then we can use the alternative function with s.to_dict() at the end.
",D,pandas,DSQA,C
expanding pandas data frame with date range in columns,"I have a pandas dataframe with dates and strings similar to this:
Start        End           Note    Item
2016-10-22   2016-11-05    Z       A
2017-02-11   2017-02-25    W       B


I need to expand/transform it to the below, filling in weeks (W-SAT) in between the Start and End columns and forward filling the data in Note and Items:
Start        Note    Item
2016-10-22   Z       A
2016-10-29   Z       A
2016-11-05   Z       A
2017-02-11   W       B
2017-02-18   W       B
2017-02-25   W       B


What's the best way to do this with pandas? Some sort of multi-index apply?
",```python,"You can iterate over each row and create a new dataframe and then concatenate them together
pd.concat([pd.DataFrame({'Start': pd.date_range(row.Start, row.End, freq='W-SAT'),
               'Note': row.Note,
               'Item': row.Item}, columns=['Start', 'Note', 'Item']) 
           for i, row in df.iterrows()], ignore_index=True)

       Start Note Item
0 2016-10-22    Z    A
1 2016-10-29    Z    A
2 2016-11-05    Z    A
3 2017-02-11    W    B
4 2017-02-18    W    B
5 2017-02-25    W    B

",You can iterate over each row and create a new dataframe and then merge them together,"pd.merge([pd.DataFrame({'Start': pd.date_range(row.Start, row.End, freq='W-SAT'),",B,pandas,DSQA,A
pandas read_csv and keep only certain rows python,"I am aware of the skiprows that allows you to pass a list with the indices of the rows to skip. However, I have the index of the rows I want to keep.
Say that my cvs file looks like this for millions of rows:
  A B
0 1 2
1 3 4
2 5 6
3 7 8
4 9 0

The list of indices i would like to load are only 2,3, so
index_list = [2,3]

The input for the skiprows function would be [0,1,4]. However, I only have available [2,3].
I am trying something like:
pd.read_csv(path, skiprows = ~index_list)

but no luck.. any suggestions?
thank and I appreciate all the help,
",   num_lines = len(open('myfile.txt').readlines()),,"I think you would need to find the number of lines first, like this.
num_lines = sum(1 for line in open('myfile.txt'))

Then you would need to delete the indices of index_list:
to_exclude = [i for i in range(num_lines) if i not in index_list]

and then load your data:
pd.read_csv(path, skiprows = to_exclude)

","""I think you would need to find the number of lines first, like this.",C,pandas,DSQA,D
how to imitate pandas39 indexbased querying in polars,"Any idea what I can do to imitate the below pandas code using polars? Polars doesn't have indexes like pandas so I couldn't figure out what I can do .
df = pd.DataFrame(data = ([21,123], [132,412], [23, 43]), columns = ['c1', 'c2']).set_index(""c1"")

out = df.loc[[23, 132]] # polars equivalent of this?

print(pl.from_pandas(out.reset_index()))

and it prints
shape: (2, 2)
┌─────┬─────┐
│ c1  ┆ c2  │
│ --- ┆ --- │
│ i64 ┆ i64 │
╞═════╪═════╡
│ 23  ┆ 43  │
│ 132 ┆ 412 │
└─────┴─────┘

the only polars conversion I could figure out to do is
df = pl.DataFrame(data = ([21,123], [132,412], [23, 43]), schema = ['c1', 'c2'], orient = 'row')
print(df.filter(pl.col(""c1"").is_in([23, 132])))

but it prints
shape: (2, 2)
┌─────┬─────┐
│ c1  ┆ c2  │
│ --- ┆ --- │
│ i64 ┆ i64 │
╞═════╪═════╡
│ 132 ┆ 412 │
│ 23  ┆ 43  │
└─────┴─────┘

which is okay but the rows are not in the order I gave. I gave [23, 132] and want the output rows to be in the same order, like how pandas' output has.
I can use a sort() later yes, but the original data I use this on has like 30Million rows so I'm looking for something that's as fast as possible.
",    ```python,    nbr_rows = 30_000_000,"I suggest using an inner join to accomplish this. This will maintain the order corresponding to your list of index values. (And it is quite performant.) For example, let's start with this shuffled DataFrame.  ","I suggest using a left join to accomplish this.  This will maintain the order corresponding to your list of index values.  (And it is quite performant.)
For example, let's start with this shuffled DataFrame.
nbr_rows = 30_000_000

df = pl.DataFrame({
    'c1': pl.int_range(0, nbr_rows, eager=True).shuffle(2),
    'c2': pl.int_range(0, nbr_rows, eager=True).shuffle(3),
})
df

shape: (30000000, 2)
┌──────────┬──────────┐
│ c1       ┆ c2       │
│ ---      ┆ ---      │
│ i64      ┆ i64      │
╞══════════╪══════════╡
│ 4052015  ┆ 20642741 │
│ 7787054  ┆ 17007051 │
│ 20246150 ┆ 19445431 │
│ 1309992  ┆ 6495751  │
│ ...      ┆ ...      │
│ 10371090 ┆ 4791782  │
│ 26281644 ┆ 12350777 │
│ 6740626  ┆ 24888572 │
│ 22573405 ┆ 14885989 │
└──────────┴──────────┘

And these index values:
nbr_index_values = 10_000
s1 = pl.Series(name='c1', values=pl.int_range(0, nbr_index_values, eager=True).shuffle())
s1

shape: (10000,)
Series: 'c1' [i64]
[
        1754
        6716
        3485
        7058
        7216
        1040
        1832
        3921
        1639
        6734
        5560
        7596
        ...
        4243
        4455
        894
        7806
        9291
        1883
        9947
        3309
        2030
        7731
        4706
        8528
        8426
]

We now perform a left join to obtain the rows corresponding to the index values.  (Note that the list of index values is the left DataFrame in this join.)
start = time.perf_counter()
df2 = (
    s1.to_frame()
    .join(
        df,
        on='c1',
        how='left'
    )
)
print(time.perf_counter() - start)

df2

>>> print(time.perf_counter() - start)
0.8427023889998964

shape: (10000, 2)
┌──────┬──────────┐
│ c1   ┆ c2       │
│ ---  ┆ ---      │
│ i64  ┆ i64      │
╞══════╪══════════╡
│ 1754 ┆ 15734441 │
│ 6716 ┆ 20631535 │
│ 3485 ┆ 20199121 │
│ 7058 ┆ 15881128 │
│ ...  ┆ ...      │
│ 7731 ┆ 19420197 │
│ 4706 ┆ 16918008 │
│ 8528 ┆ 5278904  │
│ 8426 ┆ 18927935 │
└──────┴──────────┘

Notice how the rows are in the same order as the index values.  We can verify this:
s1.equals(df2.get_column('c1'), check_dtypes=True)

>>> s1.equals(df2.get_column('c1'), check_dtypes=True)
True

And the performance is quite good.  On my 32-core system, this takes less than a second.
",D,data-science,DSQA,D
delete empty rows before column names start on excel spreadsheet using python,"My column names on a spreadsheet start on row 2, row 1 is completely blank. 
When I try to delete it, it also deletes column names on row 2 and only keeps first column name. 
Any idea of what I am doing wrong?
Original file
|          |          |
| Column A | Column B |
| Cell 1   | Cell 2   |
| Cell 3   | Cell 4   |

Expected output



Column A
Column B




Cell 1
Cell 2


Cell 3
Cell 4



Current output



Column A




Cell 1


Cell 3



I have tried: 
openpyxl
sheet.delete_rows(1)

pandas
df = df.drop(index=0)

",from openpyxl import load_workbook,"If you have the following data layout and want to delete the first row;

The following code shown for using Openpyxl and Pandas, is all that's needed.
from openpyxl import load_workbook
import pandas as pd

# Openpyxl
wb = load_workbook('foo.xlsx')

wb['Sheet1'].delete_rows(1)

wb.save('foo_openpyxl.xlsx')


# Pandas
df = pd.read_excel('foo.xlsx')
df.to_excel('foo_pandas.xlsx', header=False, index=False)


For Pandas since row 1 will be the default header, writing back to Excel without the header will remove row 1.
The output from both is the same;

",,```python,B,pandas,DSQA,
how to add a dataframe to another on python,"So I have 3 columns. ETA (eta/km*100) (a number), Climate, and month.
My purpose is to drop the values higher that 0.95 quartile and lower than 0.05 (the extreme cases on this dataset) for each subset of 3 months and Climate, and the reagroup the dataset on a single dataset.
The issue I'm having here is that even tho inside the ""for"" statement it does the job, when I print the resulting data frame, it only has the last subset (Hurricane, last 3 months) without dropping the extreme data.
I've tried concat, add and append. Not sure what I'm doing wrong here.
Climate = ['Sunny', 'Cloudy', 'Foggy', 'Rain', 'Storm', 'Hurricane']

newDf = df_Cl
newDf.iloc[0:0]


for cl in Climate:
    print (cl)
    for num in range(4, 14, 3):
        print (num)
        df_Temp = df.loc[(df['Climate'] == cl) & (df['month'] < num)]
        bajo = df_Temp['eta/km*100'].quantile(0.05)
        alto = df_Temp['eta/km*100'].quantile(0.95)
        df_Temp = df_Temp[df_Temp['eta/km*100'] > bajo]
        df_Temp = df_Temp[df_Temp['eta/km*100'] < alto]
        newDf.add(df_Temp)

I've also tried:
newDf += df_Temp

But all the values become ""NaN""
",,"Climate = ['Sunny', 'Cloudy', 'Foggy', 'Rain', 'Storm', 'Hurricane']","""Use:","Use:
Climate = ['Sunny', 'Cloudy', 'Foggy', 'Rain', 'Storm', 'Hurricane']

#filter only rows by Climate list
df1 = df[df['Climate'].isin(Climate)]

#create groups per Climate and each 3 months
g = df1.groupby(['Climate', df['month'].sub(1).floordiv(3)])['eta/km*100']

#filter between 0.05 and 0.95 quantile
out = df1[df1['eta/km*100'].between(g.quantile(0.05),
                                    g.quantile.quantile(0.95), inclusive='neither')]

Your solution working if append final df_Temp to lsit of DataFrames and last use concat for join together:
L = []
for cl in Climate:
    print (cl)
    for num in range(4, 14, 3):
        print (num)
        df_Temp = df.loc[(df['Climate'] == cl) & (df['month'] < num)]
        bajo = df_Temp['eta/km*100'].quantile(0.05)
        alto = df_Temp['eta/km*100'].quantile(0.95)
        df_Temp = df_Temp[df_Temp['eta/km*100'] > bajo]
        df_Temp = df_Temp[df_Temp['eta/km*100'] < alto]
        L.append(df_Temp)
        
out = pd.concat(L)

",D,data-science,DSQA,A
